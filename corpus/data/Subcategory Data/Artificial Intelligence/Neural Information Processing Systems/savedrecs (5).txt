PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Ernoult, M; Grollier, J; Querlioz, D; Bengio, Y; Scellier, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ernoult, Maxence; Grollier, Julie; Querlioz, Damien; Bengio, Yoshua; Scellier, Benjamin			Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LEARNING ALGORITHM	Equilibrium Propagation (EP) is a biologically inspired learning algorithm for convergent recurrent neural networks, i.e. RNNs that are fed by a static input x and settle to a steady state. Training convergent RNNs consists in adjusting the weights until the steady state of output neurons coincides with a target y. Convergent RNNs can also be trained with the more conventional Backpropagation Through Time (BPTT) algorithm. In its original formulation EP was described in the case of real-time neuronal dynamics, which is computationally costly. In this work, we introduce a discrete -time version of EP with simplified equations and with reduced simulation time, bringing EP closer to practical machine learning tasks. We first prove theoretically, as well as numerically that the neural and weight updates of EP, computed by forward-time dynamics, are step-by-step equal to the ones obtained by BPTT, with gradients computed backward in time. The equality is strict when the transition function of the dynamics derives from a primitive function and the steady state is maintained long enough. We then show for more standard discrete -time neural network dynamics that the same property is approximately respected and we subsequently demonstrate training with EP with equivalent performance to BPTT. In particular, we define the first convolutional architecture trained with EP achieving similar to 1% test error on MNIST, which is the lowest error reported with ER. These results can guide the development of deep neural networks trained with EP.	[Ernoult, Maxence; Querlioz, Damien] Univ Paris Saclay, Univ Paris Sud, Ctr Nanosci & Nanotechnol, St Aubin, France; [Ernoult, Maxence; Grollier, Julie] Univ Paris Saclay, Univ Paris Sud, Unite Mixte Phys, CNRS,Thales, St Aubin, France; [Bengio, Yoshua; Scellier, Benjamin] Univ Montreal, Mila, Montreal, PQ, Canada; [Bengio, Yoshua] Canadian Inst Adv Res, Toronto, ON, Canada	UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; Universite de Montreal; Canadian Institute for Advanced Research (CIFAR)	Ernoult, M (corresponding author), Univ Paris Saclay, Univ Paris Sud, Ctr Nanosci & Nanotechnol, St Aubin, France.; Ernoult, M (corresponding author), Univ Paris Saclay, Univ Paris Sud, Unite Mixte Phys, CNRS,Thales, St Aubin, France.				NSERC; CIFAR; Canada Research Chairs; European Research Council [682955, 715872]; Samsung	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Canada Research Chairs(Canada Research ChairsCGIAR); European Research Council(European Research Council (ERC)European Commission); Samsung(Samsung)	The authors would like to thank Joao Sacramento for feedback and discussions, as well as NSERC, CIFAR, Samsung and Canada Research Chairs for funding. Julie Grollier and Damien Querlioz acknowledge funding from the European Research Council, respectively under grants bioSPINspired (682955) and NANOINFER (715872).	ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Alderton A, 2019, INT J ENV RES PUB HE, V16, DOI 10.3390/ijerph16091516; Almeida L. B., 1987, IEEE First International Conference on Neural Networks, P609; Ambrogio S, 2018, NATURE, V558, P60, DOI 10.1038/s41586-018-0180-5; [Anonymous], 2018, NATURE, V554, P145, DOI DOI 10.1038/D41586-018-01683-1; CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0; Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Movellan J., 1991, CONNECTIONIST MODELS, P10, DOI 10.1016/B978-1-4832-1448-1.50007-X; O'Connor P., 2018, INT C MACH LEARN WOR; PINEDA FJ, 1987, PHYS REV LETT, V59, P2229, DOI 10.1103/PhysRevLett.59.2229; Romera M, 2018, NATURE, V563, P230, DOI 10.1038/s41586-018-0632-y; Rumelhart D. E., 1985, TECHNICAL REPORT; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Scellier B., 2016, ARXIV160205179V2, P914; Scellier B., 2018, ARXIV180804873; Scellier B, 2019, NEURAL COMPUT, V31, P312, DOI 10.1162/neco_a_01160; Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024; Tallec C., 2017, ARXIV170205043; Torrejon J, 2017, NATURE, V547, P428, DOI 10.1038/nature23011; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270	21	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307013
C	Ghasemipour, SKS; Gu, SX; Zemel, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ghasemipour, Seyed Kamyar Seyed; Gu, Shixiang; Zemel, Richard			SMILe : Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Imitation Learning (IL) has been successfully applied to complex sequential decision-making problems where standard Reinforcement Learning (RL) algorithms fail. A number of recent methods extend IL to few-shot learning scenarios, where a meta-trained policy learns to quickly master new tasks using limited demonstrations. However, although Inverse Reinforcement Learning (IRL) often outperforms Behavioral Cloning (BC) in terms of imitation quality, most of these approaches build on BC due to its simple optimization objective. In this work, we propose SMILe, a scalable framework for Meta Inverse Reinforcement Learning (Meta-IRL) based on maximum entropy IRL, which can learn high-quality policies from few demonstrations. We examine the efficacy of our method on a variety of high-dimensional simulated continuous control tasks and observe that SMILe significantly outperforms Meta-BC. Furthermore, we observe that SMILe performs comparably or outperforms Meta-DAgger, while being applicable in the state-only setting and not requiring online experts. To our knowledge, our approach is the first efficient method for Meta-IRL that scales to the function approximator setting. For datasets and reproducing results please refer to https://github.com/KamyarGh/rl_swiss/blob/master/reproducing/smile_paper.md.	[Ghasemipour, Seyed Kamyar Seyed; Zemel, Richard] Univ Toronto, Vector Inst, Toronto, ON, Canada; [Gu, Shixiang] Google Brain, Mountain View, CA USA	University of Toronto; Google Incorporated	Ghasemipour, SKS (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	kamyar@cs.toronto.edu; shanegu@google.com; zemel@cs.toronto.edu						Abbeel P., 2004, P 21 INT C MACHINE L, P1; BI JS, 2017, ADV NEURAL INFORM PR, P108; Brockman G., 2016, OPENAI GYM; Finn C., 2016, ABS161103852 CORR; Finn C, 2017, PR MACH LEARN RES, V70; Finn C, 2016, PR MACH LEARN RES, V48; Fox R., 2015, ARXIV PREPRINT ARXIV; Fu Justin, 2018, INT C LEARN REPR; Garnelo M, 2018, ARXIV180701622; Gidel G., 2018, ARXIV180704740; Gleave A., 2018, MULTITASK MAXIMUM EN; Gordon Jonathan, 2018, METALEARNING PROBABI; Gulrajani I, 2017, P NIPS 2017; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kostrikov I., 2018, ARXIV180902925; Nichol Alex, 2018, ARXIV180302999; Nowozin S, 2016, ADV NEUR IN, V29; Oord A.V.D., 2016, SSW; Rakelly K, 2019, PR MACH LEARN RES, V97; Rawlik Konrad, 2013, 2012 Robotics: Science and Systems, P353; Ross St<prime>ephane, 2011, AISTATS; Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964; Schaal S, 1997, ADV NEUR IN, V9, P1040; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Todorov E, 2008, IEEE DECIS CONTR P, P4286, DOI 10.1109/CDC.2008.4739438; Toussaint M., 2009, P 26 ANN INT C MACHI, P1049, DOI [10.1145%2F1553374.1553508, DOI 10.1145/1553374.1553508, 10.1145/1553374.1553508]; Xu Kelvin, 2018, FEW SHOT INTENT INFE; YAO YM, 2017, ADV NEURAL INFORM PR, P320, DOI DOI 10.1109/QRS-C.2017.61; Yu TH, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Ziebart B. D., 2008, AAAI, V8, P1433; Ziebart B. D., 2010, THESIS	37	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307085
C	Gigante, S; Charles, AS; Krishnaswamy, S; Mishne, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gigante, Scott; Charles, Adam S.; Krishnaswamy, Smita; Mishne, Gal			Visualizing the PHATE of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIFFUSION; MULTISCALE	Understanding why and how certain neural networks outperform others is key to guiding future development of network architectures and optimization methods. To this end, we introduce a novel visualization algorithm that reveals the internal geometry of such networks: Multislice PHATE (M-PHATE), the first method designed explicitly to visualize how a neural network's hidden representations of data evolve throughout the course of training. We demonstrate that our visualization provides intuitive, detailed summaries of the learning dynamics beyond simple global measures (i.e., validation loss and accuracy), without the need to access validation data. Furthermore, M-PHATE better captures both the dynamics and community structure of the hidden units as compared to visualization based on standard dimensionality reduction methods (e.g., ISOMAP, t-SNE). We demonstrate M-PHATE with two vignettes: continual learning and generalization. In the former, the M-PHATE visualizations display the mechanism of "catastrophic forgetting" which is a major challenge for learning in task-switching contexts. In the latter, our visualizations reveal how increased heterogeneity among hidden units correlates with improved generalization performance. An implementation of M-PHATE, along with scripts to reproduce the figures in this paper, is available at https://github.com/scottgigante/M-PHATE.	[Gigante, Scott] Yale Univ, Comp Biol & Bioinf Program, New Haven, CT 06511 USA; [Charles, Adam S.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Krishnaswamy, Smita] Yale Univ, Dept Genet, New Haven, CT 06520 USA; [Krishnaswamy, Smita] Yale Univ, Depts Comp Sci, New Haven, CT 06520 USA; [Mishne, Gal] Univ Calif San Diego, Halicioglu Data Sci Inst, La Jolla, CA 92093 USA	Yale University; Princeton University; Yale University; Yale University; University of California System; University of California San Diego	Gigante, S (corresponding author), Yale Univ, Comp Biol & Bioinf Program, New Haven, CT 06511 USA.	scott.gigante@yale.edu; adamsc@princeton.edu; smita.krishnaswamy@yale.edu; gmishne@ucsd.edu		Mishne, Gal/0000-0002-5287-3626; Charles, Adam/0000-0002-9045-3489; Gigante, Scott/0000-0002-4544-2764	Gruber Foundation; Chan-Zuckerberg Initiative [182702]; National Institute of General Medical Sciences of the National Institutes of Health [R01GM130847]; National Institute of Neurological Disorders and Stroke of the National Institutes of Health [R01EB026936]	Gruber Foundation; Chan-Zuckerberg Initiative; National Institute of General Medical Sciences of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of General Medical Sciences (NIGMS)); National Institute of Neurological Disorders and Stroke of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS))	This work was partially supported by the Gruber Foundation [S.G.]; the Chan-Zuckerberg Initiative (grant ID: 182702) and the National Institute of General Medical Sciences of the National Institutes of Health (grant ID: R01GM130847) [S.K.]; and the National Institute of Neurological Disorders and Stroke of the National Institutes of Health (grant ID: R01EB026936) [G.M.].	Allen-Zhu Z., 2018, ABS181104918 CORR; Arpit D, 2017, PR MACH LEARN RES, V70; Baldi P., 2013, ADV NEURAL INFORM PR, V26, P2814, DOI DOI 10.17744/MEHC.25.2.XHYREGGXDCD0Q4NY; Banisch R, 2017, CHAOS, V27, DOI 10.1063/1.4971788; Bengio Y., 2005, ADV NEURAL INFORM PR, V18, P107; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Coifman RR, 2014, APPL COMPUT HARMON A, V36, P79, DOI 10.1016/j.acha.2013.03.001; Cox T., 2000, MULTIDIMENSIONAL SCA; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171; Gal Y, 2016, PR MACH LEARN RES, V48; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; He J, 2009, IEEE GEOSCI REMOTE S, V6, P767, DOI 10.1109/LGRS.2009.2025058; Hsu Yen-Chang, 2018, ABS181012488 CORR; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lederman RR, 2018, APPL COMPUT HARMON A, V44, P509, DOI 10.1016/j.acha.2015.09.002; Li H, 2018, ADV NEUR IN, V31; Lindenbaum Ofir, 2019, INFORM FUSION; Marshall NF, 2018, APPL COMPUT HARMON A, V45, P709, DOI 10.1016/j.acha.2017.11.003; Mishne G, 2013, IEEE J-STSP, V7, P111, DOI 10.1109/JSTSP.2012.2232279; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Montufar GF, 2015, SIAM J DISCRETE MATH, V29, P321, DOI 10.1137/140957081; Moon Kevin R, 2017, VISUALIZING TRANSITI; Mucha PJ, 2010, SCIENCE, V328, P876, DOI 10.1126/science.1184819; Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012; Salimans T, 2016, ADV NEUR IN, V29; Santos C, 2009, ADVANCES IN TOURISM ECONOMICS: NEW DEVELOPMENTS, P175, DOI 10.1007/978-3-7908-2124-6_11; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Talmon R, 2013, IEEE T AUDIO SPEECH, V21, P130, DOI 10.1109/TASL.2012.2215593; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414	38	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301079
C	Guan, H; Ning, L; Lin, Z; Shen, XP; Zhou, HY; Lim, SH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Guan, Hui; Ning, Lin; Lin, Zhen; Shen, Xipeng; Zhou, Huiyang; Lim, Seung-Hwan			In-Place Zero-Space Memory Protection for CNN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FAULT-TOLERANCE; NEURAL-NETWORKS; PERFORMANCE	Convolutional Neural Networks (CNN) are being actively explored for safety-critical applications such as autonomous vehicles and aerospace, where it is essential to ensure the reliability of inference results in the presence of possible memory faults. Traditional methods such as error correction codes (ECC) and Triple Modular Redundancy (TMR) are CNN-oblivious and incur substantial memory overhead and energy cost. This paper introduces in-place zero-space ECC assisted with a new training scheme weight distribution-oriented training. The new method provides the first known zero space cost memory protection for CNNs without compromising the reliability offered by traditional ECC.	[Guan, Hui; Ning, Lin; Lin, Zhen; Shen, Xipeng; Zhou, Huiyang] North Carolina State Univ, Raleigh, NC 27695 USA; [Lim, Seung-Hwan] Oak Ridge Natl Lab, Oak Ridge, TN 37831 USA	University of North Carolina; North Carolina State University; United States Department of Energy (DOE); Oak Ridge National Laboratory	Guan, H (corresponding author), North Carolina State Univ, Raleigh, NC 27695 USA.	hguan2@ncsu.edu; lning@ncsu.edu; zlin4@ncsu.edu; xshen5@ncsu.edu; hzhou@ncsu.edu; lims1@ornl.gov			National Science Foundation (NSF) [CCF-1525609, CCF-1703487, CCF-1717550, CCF-1908406]; US Department of Energy (DOE) [DE-AC05-00OR22725]	National Science Foundation (NSF)(National Science Foundation (NSF)); US Department of Energy (DOE)(United States Department of Energy (DOE))	We would like to thank the anonymous reviews for their helpful feedbacks. This material is based upon work supported by the National Science Foundation (NSF) under Grant No. CCF-1525609, CCF-1703487, CCF-1717550, and CCF-1908406. Any opinions, comments, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF. This manuscript has been authored by UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of Energy (DOE). The US government retains and the publisher, by accepting the article for publication, acknowledges that the US government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for US government purposes. DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 1983, ERROR CONTROL CODING; Arechiga Austin P, 2018, 2018 IEEE 8 ANN COMP; Azizimazreah A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE AND STORAGE (NAS); Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fa-Xin Yu, 2010, Information Technology Journal, V9, P1068, DOI 10.3923/itj.2010.1068.1080; Hacene Ghouthi Boukli, 2019, TRAINING MODERN DEEP; Iandola F.N., 2016, ARXIV PREPRINT ARXIV; Jacob B., 2017, GEMMLOWP SMALL SELF; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kim S., 2018, IEEE T CIRCUITS-I, P1; Li GP, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126964; LYONS RE, 1962, IBM J RES DEV, V6, P200, DOI 10.1147/rd.62.0200; Migacz S, 2017, P GPU TECHN C; PHATAK DS, 1995, IEEE T NEURAL NETWOR, V6, P446, DOI 10.1109/72.363479; PROTZEL PW, 1993, IEEE T NEURAL NETWOR, V4, P600, DOI 10.1109/72.238315; Qin M., 2017, ARXIV170906173; Reagen B., 2018, 2018 55 ACM ESDA IEE, P1, DOI DOI 10.1109/DAC.2018.8465834; Reagen B, 2016, CONF PROC INT SYMP C, P267, DOI 10.1109/ISCA.2016.32; Ren A, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P925, DOI 10.1145/3297858.3304076; Salami B, 2018, INT SYM COMP ARCHIT, P322, DOI [10.1109/CAHPC.2018.8645906, 10.1109/SBAC-PAD.2018.00059]; Sridharan V, 2015, ACM SIGPLAN NOTICES, V50, P297, DOI [10.1145/2775054.2694348, 10.1145/2694344.2694348]; Temam O, 2012, CONF PROC INT SYMP C, P356, DOI 10.1109/ISCA.2012.6237031; Torres-Huitzil C, 2017, IEEE ACCESS, V5, P17322, DOI 10.1109/ACCESS.2017.2742698; Whatmough PN, 2017, ISSCC DIG TECH PAP I, P242, DOI 10.1109/ISSCC.2017.7870351; Zhang J., 2018, P 55 ANN DES AUT C, P19; Zhang J. J., 2018, 2018 IEEE 36 VLSI TE, P1; Zhang T., 2018, P EUROPEAN C COMPUTE, P184	29	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305070
C	Han, C; Mao, JY; Gan, C; Tenenbaum, JB; Wu, JJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Han, Chi; Mao, Jiayuan; Gan, Chuang; Tenenbaum, Joshua B.; Wu, Jiajun			Visual Concept-Metaconcept Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Humans reason with concepts and metaconcepts: we recognize red and green from visual input; we also understand that they describe the same property of objects (i.e., the color). In this paper, we propose the visual concept-metaconcept learner (VCML) for joint learning of concepts and metaconcepts from images and associated question-answer pairs. The key is to exploit the bidirectional connection between visual concepts and metaconcepts. Visual representations provide grounding cues for predicting relations between unseen pairs of concepts. Knowing that red and green describe the same property of objects, we generalize to the fact that cube and sphere also describe the same property of objects, since they both categorize the shape of objects. Meanwhile, knowledge about metaconcepts empowers visual concept learning from limited, noisy, and even biased data. From just a few examples of purple cubes we can understand a new color purple, which resembles the hue of the cubes instead of the shape of them. Evaluation on both synthetic and real-world datasets validates our claims.	[Han, Chi; Mao, Jiayuan; Wu, Jiajun] MIT CSAIL, Cambridge, MA 02139 USA; [Han, Chi] IIS, Cambridge, MA 02139 USA; [Han, Chi] Tsinghua Univ, Beijing, Peoples R China; [Gan, Chuang] IBM Watson Lab, Ossining, NY 10562 USA; [Tenenbaum, Joshua B.] CSAIL, CBMM, MIT BCS, Cambridge, MA USA	Massachusetts Institute of Technology (MIT); Tsinghua University	Han, C (corresponding author), MIT CSAIL, Cambridge, MA 02139 USA.; Han, C (corresponding author), IIS, Cambridge, MA 02139 USA.; Han, C (corresponding author), Tsinghua Univ, Beijing, Peoples R China.		Wu, JiaJun/GQH-7885-2022		Center for Brains, Minds and Machines (CBMM, NSF STC) [CCF-1231216]; ONR MURI [N00014-16-1-2007]; MIT-IBM Watson AI Lab; Facebook	Center for Brains, Minds and Machines (CBMM, NSF STC); ONR MURI(MURIOffice of Naval Research); MIT-IBM Watson AI Lab(International Business Machines (IBM)); Facebook(Facebook Inc)	We thank Jon Gauthier for helpful discussions and suggestions. This work was supported in part by the Center for Brains, Minds and Machines (CBMM, NSF STC award CCF-1231216), ONR MURI N00014-16-1-2007, MIT-IBM Watson AI Lab, and Facebook.	Andreas Jacob, 2016, NAACL HLT; [Anonymous], 2013, NEURIPS; Cho Kyunghyun, 2014, EMNLP; Deng J., 2009, CVPR; Faghri F., 2018, P BMVC, P12; Fidler Sanja, 2016, ICLR; Gan C., 2017, ICCV; Ganju Siddha, 2017, CVPR; He K., 2017, ICCV; Hu Ronghang, 2018, ECCV; Hudson D. A., 2019, GQA NEW DATASET COMP; Hudson Drew A., 2018, ABS180303067 CORR; Johnson J., 2017, CVPR; Johnson J., 2016, CVPR; Karpathy A., 2015, CVPR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kiros Jamie Ryan, 2014, NEURIPS WORKSH; Lai A, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P721; Lee Kenton, 2019, NAACL HLT; Lin Yankai, 2017, ACL; Malinowski M., 2015, CVPR; Mao J., 2019, P INT C LEARN REPR; Mascharka David, 2018, CVPR; McRae K, 2005, BEHAV RES METHODS, V37, P547, DOI 10.3758/BF03192726; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Misra I., 2017, CVPR; Nagarajan Tushar, 2018, ECCV; Pennington Jeffrey, 2014, EMNLP; Purushwalkam Senthil, 2019, ARXIV190505908; Ren Zhou, 2016, ACM MM; Shi Haoyue, 2018, COLING; Socher R., 2013, NEURIPS; Speer Robyn, 2017, AAAI; Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006; Sun J, 2015, CVPR; Thoma Steffen, 2017, ISWC; Vendrov I., 2016, P INT C LEARN REPR; Vilnis L., 2018, ACL; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang Z., 2014, AAAI; Wu Jiajun, 2017, CVPR; Xu H., 2016, P EUR C COMP VIS ECC; Yang Fan, 2017, NEURIPS; Yang Z., 2016, CVPR; Yi K, 2019, INT C LEARN REPR; Yi K., 2018, NEURIPS; Zhou Bolei, 2015, ARXIV151202167	47	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305005
C	Hanin, B; Rolnick, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hanin, Boris; Rolnick, David			Deep ReLU Networks Have Surprisingly Few Activation Patterns	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The success of deep networks has been attributed in part to their expressivity: per parameter, deep networks can approximate a richer class of functions than shallow networks. In ReLU networks, the number of activation patterns is one measure of expressivity; and the maximum number of patterns grows exponentially with the depth. However, recent work has showed that the practical expressivity of deep networks - the functions they can learn rather than express - is often far from the theoretical maximum. In this paper, we show that the average number of activation patterns for ReLU networks at initialization is bounded by the total number of neurons raised to the input dimension. We show empirically that this bound, which is independent of the depth, is tight both at initialization and during training, even on memorization tasks that should maximize the number of activation patterns. Our work suggests that realizing the full expressivity of deep networks may not be possible in practice, at least with current methods.	[Hanin, Boris] Facebook AI Res, New York, NY 10003 USA; [Hanin, Boris] Texas A&M Univ, College Stn, TX 77843 USA; [Rolnick, David] Univ Penn, Philadelphia, PA 19104 USA	Facebook Inc; Texas A&M University System; Texas A&M University College Station; University of Pennsylvania	Hanin, B (corresponding author), Facebook AI Res, New York, NY 10003 USA.; Hanin, B (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.	bhanin@math.tamu.edu; drolnick@seas.upenn.edu						Arpit D., 2017, ICML; Ba Jimmy, 2014, NEURIPS; BARRON AR, 1994, MACH LEARN, V14, P115, DOI 10.1023/A:1022650905902; Bianchini M, 2014, IEEE T NEUR NET LEAR, V25, P1553, DOI 10.1109/TNNLS.2013.2293637; Cohen N., 2016, C LEARNING THEORY, V49, P698; Croce Francesco, 2018, AISTATS; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; De Palma Giacomo, 2018, ARXIV181210156; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; Hanin B., 2019, ICML; Hanin B., 2017, ARXIV170802691; Hanin B., 2018, ARXIV181205994; Hanin Boris, 2018, NEURIPS; He K., 2015, ICCV; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Lee J., 2018, ICLR; Lin HW, 2017, J STAT PHYS, V168, P1223, DOI 10.1007/s10955-017-1836-5; Montufar Guido F, 2014, NEURIPS; Novak R., 2018, ICLR; Poole Ben, 2016, NEURIPS; Raghu M, 2017, PR MACH LEARN RES, V70; Rolnick David, 2018, ICLR; Serra T., 2018, ARXIV181003370; Serra Thiago, 2018, ICML; Stanley R. P, 2004, LECT NOTES, V13, P389; Telgarsky M, 2015, ARXIV150908101V2CSLG; Zhang C., 2017, ICLR	28	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300033
C	Henaff, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Henaff, Mikael			Explicit Explore-Exploit Algorithms in Continuous State Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a new model-based algorithm for reinforcement learning (RL) which consists of explicit exploration and exploitation phases, and is applicable in large or infinite state spaces. The algorithm maintains a set of dynamics models consistent with current experience and explores by finding policies which induce high disagreement between their state predictions. It then exploits using the refined set of models or experience gathered during exploration. We show that under realizability and optimal planning assumptions, our algorithm provably finds a near-optimal policy with a number of samples that is polynomial in a structural complexity measure which we show to be low in several natural settings. We then give a practical approximation using neural networks and demonstrate its performance and sample efficiency in practice.	[Henaff, Mikael] Microsoft Res, Albuquerque, NM 87109 USA	Microsoft	Henaff, M (corresponding author), Microsoft Res, Albuquerque, NM 87109 USA.	mihenaff@microsoft.com						Abbasi-Yadkori Y., 2011, P 24 ANN C LEARNING, P1; Atkeson CG, 1997, IEEE INT CONF ROBOT, P3557, DOI 10.1109/ROBOT.1997.606886; Bellemare M., 2016, NEURIPS; Bishop C.M., 1994, MIXTURE DENSITY NETW; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Brockman G., 2016, OPENAI GYM; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Coulom R., 2006, EFFICIENT SELECTIVIT, V4630; Dean S., 2017, CORR; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Du SS, 2019, PR MACH LEARN RES, V97; Finn, 2018, ARXIV180400645, P4732; Foster DJ, 2018, PR MACH LEARN RES, V80; Ha D, 2018, ADV NEUR IN, V31; Hafner D., 2018, ARXIV181104551; Henaff Mikael, 2019, ARXIV190102705; Hesse C., 2017, OPENAI BASELINES; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Jiang N, 2017, PR MACH LEARN RES, V70; Kakade S. M., 2003, ICML; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kearns M, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P740; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Klimov O, 2019, INT C LEARNING REPRE; Kolter J. Z., 2009, P 26 ANN INT C MACHI, P513, DOI DOI 10.1145/1553374.1553441; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Ma T., 2019, INT C LEARN REPR, P1; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moore A.W., 1990, TECHNICAL REPORT; Mueller A., 1997, INTEGRAL PROBABILITY; Nagabandi A., 2018, NEURAL NETWORK DYNAM, P7559; Oh J, 2017, ADV NEUR IN, V30; Osband I., 2017, CORR; Ostrovski G, 2017, PR MACH LEARN RES, V70; Pathak D, 2017, IEEE COMPUT SOC CONF, P488, DOI 10.1109/CVPRW.2017.70; Pathak D, 2019, PR MACH LEARN RES, V97; Quan J., 2016, P 4 ICLR, P1; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Shangtong Z., 2018, MODULARIZED IMPLEMEN; Shyam P., 2018, CORR; Sorg J, 2010, P UNC ART INT, P564; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl Alexander L., 2005, P 22 INT C MACH LEAR, P856, DOI [DOI 10.1145/1102351.1102459, 10.1145/1102351.1102459]; Sun W., 2018, CORR; Sutton R. S., 2008, P 24 C UNC ART INT, P528; Sutton RS, 1996, ADV NEUR IN, V8, P1038; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094	51	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901005
C	Hendrikx, H; Bach, F; Massoulie, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hendrikx, Hadrien; Bach, Francis; Massoulie, Laurent			An Accelerated Decentralized Stochastic Proximal Algorithm for Finite Sums	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DISTRIBUTED OPTIMIZATION; COORDINATE; PARALLEL	Modern large-scale finite-sum optimization relies on two key aspects: distribution and stochastic updates. For smooth and strongly convex problems, existing decentralized algorithms are slower than modern accelerated variance-reduced stochastic algorithms when run on a single machine, and are therefore not efficient. Centralized algorithms are fast, but their scaling is limited by global aggregation steps that result in communication bottlenecks. In this work, we propose an efficient Accelerated Decentralized stochastic algorithm for Finite Sums named ADFS, which uses local stochastic proximal updates and randomized pairwise communications between nodes. On n machines, ADFS learns from nm samples in the same time it takes optimal algorithms to learn from m samples on one machine. This scaling holds until a critical network size is reached, which depends on communication delays, on the number of samples m, and on the network topology. We provide a theoretical analysis based on a novel augmented graph approach combined with a precise evaluation of synchronization times and an extension of the accelerated proximal coordinate gradient algorithm to arbitrary sampling. We illustrate the improvement of ADFS over state-of-the-art decentralized approaches with experiments.	[Hendrikx, Hadrien; Bach, Francis; Massoulie, Laurent] PSL Res Univ, DIENS, INRIA, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris	Hendrikx, H (corresponding author), PSL Res Univ, DIENS, INRIA, Paris, France.	hadrien.hendrikx@inria.fr; francis.bach@inria.fr; laurent.massoulie@inria.fr			European Research Council [SEQUOIA 724063]	European Research Council(European Research Council (ERC)European Commission)	We acknowledge support from the European Research Council (grant SEQUOIA 724063).	Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; [Anonymous], 1992, SYNCHRONIZATION LINE; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Bubeck S., 2015, FDN TRENDS MACHINE L; Chen J., 2016, ABS160400981 CORR; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A, 2016, ADV NEUR IN, V29; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; He Lie, 2018, ADV NEURAL INFORM PR, P4536; Hendrikx Hadrien, 2019, ARTIFICIAL INTELLIGE; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Johansson B, 2009, SIAM J OPTIMIZ, V20, P1157, DOI 10.1137/08073038X; Koloskova Anastasia, 2019, INT C MACH LEARN; Kshitij Patel K, 2019, ARXIV190411325; Leblond R, 2017, PR MACH LEARN RES, V54, P46; Lee YT, 2013, ANN IEEE SYMP FOUND, P147, DOI 10.1109/FOCS.2013.24; Lian X, 2017, P ADV NEUR INF PROC, P5330; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Lin QH, 2015, SIAM J OPTIMIZ, V25, P2244, DOI 10.1137/141000270; Lin T., 2018, P 8 INT C LEARN REPR, P1; Mokhtari Aryan, 2016, J MACHINE LEARNING R, V17, P2165; Nedic A, 2017, SIAM J OPTIMIZ, V27, P2597, DOI 10.1137/16M1084316; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Scaman K, 2017, PR MACH LEARN RES, V70; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shen Zebang, 2018, INT C MACH LEARN, P4631; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Tang H., 2018, P INT C MACH LEARN, P4855; Zeng Y, 2018, OPER RES, V66, P1728, DOI 10.1287/opre.2018.1748	39	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300086
C	Ho, J; Lohn, E; Abbeel, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ho, Jonathan; Lohn, Evan; Abbeel, Pieter			Compression with Flows via Local Bits-Back Coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Likelihood-based generative models are the backbones of lossless compression due to the guaranteed existence of codes with lengths close to negative log likelihood. However, there is no guaranteed existence of computationally efficient codes that achieve these lengths, and coding algorithms must be hand-tailored to specific types of generative models to ensure computational efficiency. Such coding algorithms are known for autoregressive models and variational autoencoders, but not for general types of flow models. To fill in this gap, we introduce local bits-back coding, a new compression technique for flow models. We present efficient algorithms that instantiate our technique for many popular types of flows, and we demonstrate that our algorithms closely achieve theoretical codelengths for state-of-the-art flow models on high-dimensional data.	[Ho, Jonathan; Lohn, Evan; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ho, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	jonathanho@berkeley.edu; evan.lohn@berkeley.edu; pabbeel@cs.berkeley.edu			UC Berkeley EECS Departmental Fellowship	UC Berkeley EECS Departmental Fellowship	We thank Peter Chen and Stas Tiomkin for discussions and feedback. This work was supported by a UC Berkeley EECS Departmental Fellowship.	Ball Johannes, 2018, INT C LEARN REPR ICL; Balle Johannes, 2016, ARXIV161101704; Behrmann J., 2018, ARXIV181100995; Chen Xi, 2016, ARXIV161102731; Child R., 2019, ARXIV190410509; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Deco G., 1995, ADV NEURAL INFORM PR, P247; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Duda Jarek, 2013, ARXIV13112540; Frey BJ, 1997, COMPUT J, V40, P157, DOI 10.1093/comjnl/40.2_and_3.157; Giesen F, 2014, ARXIV14023392; Grathwohl Will, 2018, ARXIV181001367; Gritsenko Alexey A, 2019, RELATIONSHIP NORMALI; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Ho Jonathan, 2019, ICML; Honkela A, 2004, IEEE T NEURAL NETWOR, V15, P800, DOI 10.1109/TNN.2004.828762; HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898; Jiang S, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P863, DOI 10.1145/3178876.3186134; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kingma FH, 2019, PR MACH LEARN RES, V97; Kraft L. G., 1949, THESIS; Kumar M., 2019, ARXIV190301434; Maaloe L, 2019, ADV NEUR IN, V32; MCMILLAN B, 1956, IRE T INFORM THEOR, V2, P115, DOI 10.1109/TIT.1956.1056818; Menick Jacob, 2018, ARXIV181201608; Mentzer Fabian, 2018, ARXIV181112817; Oord A.V.D., 2016, SSW; Papamakarios George, 2017, ARXIV170507057; Parmar Niki, 2018, ARXIV180205751, P2; Prenger R, 2019, INT CONF ACOUST SPEE, P3617, DOI 10.1109/ICASSP.2019.8683143; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rippel O, 2017, PR MACH LEARN RES, V70; RISSANEN JJ, 1976, IBM J RES DEV, V20, P198, DOI 10.1147/rd.203.0198; Salimans Tim, 2017, ARXIV170105517; Theis Lucas, 2017, INT C LEARN REPR; Theis Lucas, 2016, ICLR; Townsend James, 2019, ARXIV190104866; Uria  B., 2016, J MACH LEARN RES, V17, P7184; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; WALLACE CS, 1968, COMPUT J, V11, P185, DOI 10.1093/comjnl/11.2.185	47	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303082
C	Hu, SY; Leung, CW; Leung, HF		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Shuyue; Leung, Chin-Wing; Leung, Ho-Fung			Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modelling the dynamics of multi-agent learning has long been an important research topic, but all of the previous works focus on 2-agent settings and mostly use evolutionary game theoretic approaches. In this paper, we study an n-agent setting with n tends to infinity, such that agents learn their policies concurrently over repeated symmetric bimatrix games with some other agents. Using the mean field theory, we approximate the effects of other agents on a single agent by an averaged effect. A Fokker-Planck equation that describes the evolution of the probability distribution of Q-values in the agent population is derived. To the best of our knowledge, this is the first time to show the Q-learning dynamics under an n-agent setting can be described by a system of only three equations. We validate our model through comparisons with agent-based simulations on typical symmetric bimatrix games and different initial settings of Q-values.	[Hu, Shuyue; Leung, Chin-Wing; Leung, Ho-Fung] Chinese Univ Hong Kong, Hong Kong, Peoples R China	Chinese University of Hong Kong	Hu, SY (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	syhu@cse.cuhk.edu.hk; cwleung@cse.cuhk.edu.hk; lhf@cse.cuhk.edu.hk						Agogino AK, 2012, AUTON AGENT MULTI-AG, V24, P1, DOI 10.1007/s10458-010-9142-5; Bloembergen D, 2015, J ARTIF INTELL RES, V53, P659, DOI 10.1613/jair.4818; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Cheng Shih-Fen, 2004, NOTES EQUILIBRIA SYM; Claus C., 1998, P AAAI; Fokker AD, 1914, ANN PHYS-BERLIN, V43, P810; Huang MY, 2006, COMMUN INF SYST, V6, P221; Junling Hu, 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P242; Kaisers M, 2012, P 11 INT C AUT AG MU, V3, P1393; Kaisers M., 2010, P 9 INT C AUTONOMOUS, V1, P309; Kianercy A, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.041145; Klos T, 2010, LECT NOTES ARTIF INT, V6322, P82, DOI 10.1007/978-3-642-15883-4_6; Lanctot M, 2017, ADV NEUR IN, V30; Lasry JM, 2007, JAP J MATH, V2, P229, DOI 10.1007/s11537-007-0657-8; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Lesser MP, 2012, OXIDATIVE STRESS IN AQUATIC ECOSYSTEMS, P9; Mguni D, 2018, AAAI CONF ARTIF INTE, P4686; Panait L, 2008, J MACH LEARN RES, V9, P423; Parker LE, 2000, DISTRIBUTED AUTONOMOUS ROBOTIC SYSTEMS, P3; Pipattanasomporn M., 2009, IEEEPES POWER SYSTEM, P1, DOI DOI 10.1109/PSCE.2009.4840087; Planck V., 1917, SATZ STAT DYNAMIK SE; Rodrigues Gomes E., 2009, P 26 ANN INT C MACHI, P369; Sen Sandip, 2007, IJCAI, P1512; Shoham Y, 2007, ARTIF INTELL, V171, P365, DOI 10.1016/j.artint.2006.02.006; Subramanian J, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P251; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tuyls K, 2006, AUTON AGENT MULTI-AG, V12, P115, DOI 10.1007/s10458-005-3783-9; Tuyls K., 2003, P 2 INT JOINT C AUTO, P693; Tuyls K, 2007, ARTIF INTELL, V171, P406, DOI 10.1016/j.artint.2007.01.004; Watkins C J, 1992, MACHINE LEARNING, V8; Weiss P., 1907, J PHYS THEOR APPL, V6, P661, DOI [10.1051/jphystap:019070060066100, DOI 10.1051/JPHYSTAP:019070060066100]; Wunder M., 2010, P 27 INT C MACH LEAR; Yang Yaodong, 2018, ARXIV PREPRINT ARXIV, P5571, DOI DOI 10.1115/FEDSM2018-83038	33	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903071
C	Jothimurugan, K; Alur, R; Bastani, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jothimurugan, Kishor; Alur, Rajeev; Bastani, Osbert			A Composable Specification Language for Reinforcement Learning Tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reinforcement learning is a promising approach for learning control policies for robot tasks. However, specifying complex tasks (e.g., with multiple objectives and safety constraints) can be challenging, since the user must design a reward function that encodes the entire task. Furthermore, the user often needs to manually shape the reward to ensure convergence of the learning algorithm. We propose a language for specifying complex control tasks, along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. We implement our approach in a tool called SPECTRL, and show that it outperforms several state-of-the-art baselines.	[Jothimurugan, Kishor; Alur, Rajeev; Bastani, Osbert] Univ Penn, Philadelphia, PA 19104 USA	University of Pennsylvania	Jothimurugan, K (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	kishor@cis.upenn.edu; alur@cis.upenn.edu; obastani@cis.upenn.edu	Jothimurugan, Kishor/GXW-0689-2022	Jothimurugan, Kishor/0000-0003-1448-2947	NSF [CCF 1723567]; AFRL [FA8750-18-C-0090]; DARPA [FA8750-18-C-0090]	NSF(National Science Foundation (NSF)); AFRL(United States Department of DefenseUS Air Force Research Laboratory); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank the reviewers for their insightful comments. This work was partially supported by NSF by grant CCF 1723567 and by AFRL and DARPA under Contract No. FA8750-18-C-0090.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Andreas J, 2017, PR MACH LEARN RES, V70; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; [Anonymous], [No title captured]; Clarke E.M.T., 2018, HDB MODEL CHECKING, V10; Collins S, 2005, SCIENCE, V307, P1082, DOI 10.1126/science.1107799; Coogan S, 2016, IEEE T CONTROL NETW, V3, P162, DOI 10.1109/TCNS.2015.2428471; Deshmukh JV, 2017, FORM METHOD SYST DES, V51, P5, DOI 10.1007/s10703-017-0286-7; Fainekos GE, 2009, THEOR COMPUT SCI, V410, P4262, DOI 10.1016/j.tcs.2009.06.021; He KL, 2017, IEEE INT C INT ROBOT, P5326; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kress-Gazit H, 2009, IEEE T ROBOT, V25, P1370, DOI 10.1109/TRO.2009.2030225; Li X, 2017, IEEE INT C INT ROBOT, P3834; Maler Oded, 2013, International Journal on Software Tools for Technology Transfer, V15, P247, DOI 10.1007/s10009-012-0247-9; Mania H., 2018, ARXIV180307055; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Pnueli A., 1977, 18th Annual Symposium on Foundations of Computer Science, P46, DOI 10.1109/SFCS.1977.32; Ross St<prime>ephane, 2011, AISTATS; Silver D, 2014, ICML ICML 14, P387; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Topcu U., 2018, ADV NEURAL INFORM PR, P7450; VARDI MY, 1994, INFORM COMPUT, V115, P1, DOI 10.1006/inco.1994.1092; Wongpiromsarn T, 2012, IEEE T AUTOMAT CONTR, V57, P2817, DOI 10.1109/TAC.2012.2195811; Ziebart B. D., 2008, AAAI, V8, P1433	26	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904065
C	Kallus, N; Uehara, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kallus, Nathan; Uehara, Masatoshi			Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ROBUSTNESS	Off-policy evaluation (OPE) in both contextual bandits and reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. The problem's importance has attracted many proposed solutions, including importance sampling (IS), self-normalized IS (SNIS), and doubly robust (DR) estimates. DR and its variants ensure semiparametric local efficiency if Q-functions are well-specified, but if they are not they can be worse than both IS and SNIS. It also does not enjoy SNIS's inherent stability and boundedness. We propose new estimators for OPE based on empirical likelihood that are always more efficient than IS, SNIS, and DR and satisfy the same stability and boundedness properties as SNIS. On the way, we categorize various properties and classify existing estimators by them. Besides the theoretical guarantees, empirical studies suggest the new estimators provide advantages.	[Kallus, Nathan] Cornell Univ, New York, NY 10021 USA; [Uehara, Masatoshi] Harvard Univ, Cambrdige, MA 02138 USA	Cornell University; Harvard University	Uehara, M (corresponding author), Harvard Univ, Cambrdige, MA 02138 USA.	kallus@cornell.edu; uehara_m@g.harvard.edu			National Science Foundation [1846210]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1846210.	[Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Bennett A., 2019, ADV NEURAL INFORM PR; Bowsher C. G., 2012, P NATL ACAD SCI, V109; Brockman G., 2016, OPENAI GYM; Farajtabar M, 2018, P 35 INT C MACH LEAR, P1447; Jiang N, 2016, PR MACH LEARN RES, V48; Kallus N., 2019, ARXIV190808526; Kallus N., 2019, STAT SINICA, V29, P1697; Kallus N., 2018, ADV NEURAL INFORM PR, P8895; Kallus Nathan, 2019, ARXIV190905850; Kang JDY, 2007, STAT SCI, V22, P523, DOI 10.1214/07-STS227; Li LH, 2015, JMLR WORKSH CONF PRO, V38, P608; Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077; Munos R, 2016, P 30 INT C NEUR INF; Murphy SA, 2003, J R STAT SOC B, V65, P331, DOI 10.1111/1467-9868.00389; Narita Yusuke, 2019, AAAI; Newey W.K., 1994, HDB ECONOMETRICS, VIV, DOI DOI 10.1016/S1573-4412(05)80005-4; Owen A, 2000, J AM STAT ASSOC, V95, P135, DOI 10.2307/2669533; Owen A., 2001, MONOGRAPHS STAT APPL, V92; Precup D., 2000, INT C MACH LEARN, P759; Robins J.M., 2007, STAT SCI, V22, P544, DOI [https://doi.org/10.1214/07-STS227D, DOI 10.1214/07-STS227D]; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Rubin D., 2008, INT J BIOSTAT, V4, P5, DOI [10.2202/1557-4679.1084, DOI 10.2202/1557-4679.1084]; Swaminathan A, 2015, ADV NEUR IN, V28; Tan ZQ, 2006, J AM STAT ASSOC, V101, P1619, DOI 10.1198/016214506000000023; Tan ZQ, 2010, BIOMETRIKA, V97, P661, DOI 10.1093/biomet/asq035; Tan ZQ, 2004, J AM STAT ASSOC, V99, P1027, DOI 10.1198/016214504000001664; Thomas P., 2016, INT C MACH LEARN, P2139; TSIATIS A, 2006, SPR S STAT, pR7; Vaart A. W., 1998, ASYMPTOTIC STAT; Wang Y. -X., 2017, P 34 INT C MACH LEAR, P3589; Zhou D, 2018, ADV NEURAL INFORM PR, V31, P5356	35	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303033
C	Korshunova, I; Xiong, H; Fedoryszak, M; Theis, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Korshunova, Iryna; Xiong, Hanchen; Fedoryszak, Mateusz; Theis, Lucas			Discriminative Topic Modeling with Logistic LDA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite many years of research into latent Dirichlet allocation (LDA), applying LDA to collections of non-categorical items is still challenging. Yet many problems with much richer data share a similar structure and could benefit from the vast literature on LDA. We propose logistic LDA, a novel discriminative variant of latent Dirichlet allocation which is easy to apply to arbitrary inputs. In particular, our model can easily be applied to groups of images, arbitrary text embeddings, and integrates well with deep neural networks. Although it is a discriminative model, we show that logistic LDA can learn from unlabeled data in an unsupervised manner by exploiting the group structure present in the data. In contrast to other recent topic models designed to handle arbitrary inputs, our model does not sacrifice the interpretability and principled motivation of LDA.	[Korshunova, Iryna] Univ Ghent, Ghent, Belgium; [Xiong, Hanchen; Fedoryszak, Mateusz; Theis, Lucas] Twitter, San Francisco, CA USA	Ghent University; Twitter, Inc.	Korshunova, I (corresponding author), Univ Ghent, Ghent, Belgium.	iryna.korshunova@ugent.be; hxiong@twitter.com; mfedoryszak@twitter.com; ltheis@twitter.com						Bernardo J. M., 2007, BAYESIAN STAT, V8, P3, DOI DOI 10.1007/978-3-642-93220-5_6; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI [10.1145/860435.860460, DOI 10.1145/860435.860460]; Blei D. M., 2008, ADV NEURAL INFORM PR, V20; Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blunsom, 2017, P 34 INT C MACH LEAR, V70, P2410; Cao Z., 2015, AAAI; CARDOSOCACHOPO A, 2007, THESIS; Dai AM, 2015, ADV NEUR IN, V28; Dieng AB, 2017, INT C LEARN REPR; Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hinton G.E., 2009, ADV NEURAL INFORM PR, V22; Hoffman M. D., 2010, ADV NEURAL INFORM PR, V23; Hu D., 2009, LATENT DIRICHLET ALL; Jo Yohan, 2011, P 4 ACM INT C WEB SE, P815, DOI DOI 10.1145/1935826.1935932; Johnson R, 2016, PR MACH LEARN RES, V48; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lacoste-Julien S, 2011, P 14 INT C ARTIFICIA, P416; Lacoste-Julien S., 2009, ADV NEURAL INFORM PR, V21; Lang K., 1995, P 12 INT C MACHINE L; Larochelle H., 2012, ADV NEURAL INFORM PR, V25; Lau J. H., 2014, P 14 C EUR CHAPT ASS; Miao YS, 2016, PR MACH LEARN RES, V48; Mikolov T., 2013, P INT C LEARN REPR I; Mimno D., 2008, P 24 C UNC ART INT U; Minka T., 2002, P 18 C UNCERTAINTY A, P352; Mnih A., 2014, P 31 INT C INT C MAC; Ng AY, 2002, ADV NEURAL INFORM PR, V14; Pandey G., 2017, ABS170106796 ARXIV; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; Ramage D., 2009, P 2009 C EMP METH NA, V1, P248, DOI DOI 10.3115/1699510.1699543; Rosen-Zvi Michal, 2012, ARXIV12074169; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Srivastava A., 2016, NEURIPS; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Theis L., 2015, TRUST REGION METHOD; Wang C., 2009, IEEE C COMP VIS PATT; Wang X., 2006, P 12 ACM SIGKDD INT, P424; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	41	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306074
C	Li, P; Chien, E; Milenkovic, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Pan; Chien, Eli; Milenkovic, Olgica			Optimizing Generalized PageRank Methods for Seed-Expansion Community Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LAW	Landing probabilities (LP) of random walks (RW) over graphs encode rich information regarding graph topology. Generalized PageRanks (GPR), which represent weighted sums of LPs of RWs, utilize the discriminative power of LP features to enable many graph-based learning studies. Previous work in the area has mostly focused on evaluating suitable weights for GPRs, and only a few studies so far have attempted to derive the optimal weights of GPRs for a given application. We take a fundamental step forward in this direction by using random graph models to better our understanding of the behavior of GPRs. In this context, we provide a rigorous non-asymptotic analysis for the convergence of LPs and GPRs to their mean-field values on edge-independent random graphs. Although our theoretical results apply to many problem settings, we focus on the task of seed-expansion community detection over stochastic block models. There, we find that the predictive power of LPs decreases significantly slower than previously reported based on asymptotic findings. Given this result, we propose a new GPR, termed Inverse PR (IPR), with LP weights that increase for the initial few steps of the walks. Extensive experiments on both synthetic and real, large-scale networks illustrate the superiority of IPR compared to other GPRs for seeded community detection.(1)	[Li, Pan; Chien, Eli; Milenkovic, Olgica] UIUC, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Li, P (corresponding author), UIUC, Champaign, IL 61820 USA.	panli2@illinois.edu; ichien3@illinois.edu; milenkov@illinois.edu	Chien, Eli/AHA-3502-2022					Abbassi Z., 2007, P 9 WEBKDD 1 SNA KDD, P102; Abbe E., 2015, ARXIV151209080; Aiello W, 2001, EXP MATH, V10, P53, DOI 10.1080/10586458.2001.10504428; Andersen R, 2006, ANN IEEE SYMP FOUND, P475; Avrachenkov K., 2018, ARXIV180607640; Avrachenkov K, 2015, LECT NOTES COMPUT SC, V9479, P151, DOI 10.1007/978-3-319-26784-5_12; Avron H, 2015, PR MACH LEARN RES, V37, P1795; Baeza-Yates R., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P308, DOI 10.1145/1148170.1148225; Berberidis D., 2018, MIN LEARN GRAPH WORK, V8, P1; Bojchevski A., 2019, ACM KDD MLG WORKSH; Bruna J, 2013, PROC INT C LEARN REP; Chen NY, 2017, RANDOM STRUCT ALGOR, V51, P237, DOI 10.1002/rsa.20700; Chien E., 2019, LANDING PROBABILITIE; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Chung F, 2007, P NATL ACAD SCI USA, V104, P19735, DOI 10.1073/pnas.0708838104; Chung F, 2011, ELECTRON J COMB, V18; Chung F, 2009, LECT NOTES COMPUT SC, V5427, P62, DOI 10.1007/978-3-540-95995-3_6; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Giles C. L., 1998, Digital 98 Libraries. Third ACM Conference on Digital Libraries, P89, DOI 10.1145/276675.276685; Gleich DF, 2014, INTERNET MATH, V10, P188, DOI 10.1080/15427951.2013.814092; Gleich DF, 2015, SIAM REV, V57, P321, DOI 10.1137/140976649; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; He K, 2015, IEEE DATA MINING, P769, DOI 10.1109/ICDM.2015.89; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Ikeda M., 2018, ARXIV180904396; Jeh G., 2003, P 12 INT C WORLD WID, P271, DOI DOI 10.1145/775152.775191; Jiang BB, 2017, BIOINFORMATICS, V33, P1829, DOI 10.1093/bioinformatics/btx029; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kelley K, 2012, PSYCHOL METHODS, V17, P137, DOI 10.1037/a0028086; Kipf TN, 2016, P INT C LEARN REPR; Kleinberg J., 2018, WEB C 2018 BIGNET; Klicpera J., 2019, PROC ANN C NEURAL IN, P13333; Klicpera Johannes, 2019, INT C LEARN REPR ICL; Kloster K, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1386, DOI 10.1145/2623330.2623706; Kloumann IM, 2017, P NATL ACAD SCI USA, V114, P33, DOI 10.1073/pnas.1611275114; Kloumann IM, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1366, DOI 10.1145/2623330.2623621; Leskovec J, 2016, ACM T INTEL SYST TEC, V8, DOI 10.1145/2898361; Leskovec J, 2009, INTERNET MATH, V6, P29, DOI 10.1080/15427951.2009.10129177; Li P., 2019, ARXIV190210132; Li P, 2018, ADV NEUR IN, V31; Li YX, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P658, DOI 10.1145/2736277.2741676; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Massa P, 2007, RECSYS 07: PROCEEDINGS OF THE 2007 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P17; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988; Mossel E., 2014, P MACHINE LEARNING R, P356; Namata G., 2012, P INT WORKSH MIN LEA; Page L., 1999, PAGERANK CITATION RA; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Tao T, 2012, RANDOM MATRICES-THEO, V1, DOI 10.1142/S2010326311500018; Velickovi Petar, 2017, ARXIV171010903; Weyl H, 1912, MATH ANN, V71, P441, DOI 10.1007/BF01456804; Whang JJ, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2099, DOI 10.1145/2505515.2505535; Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927; Yin H, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P555, DOI 10.1145/3097983.3098069; Yin ZJ, 2010, 2010 INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2010), P152, DOI 10.1109/ASONAM.2010.27; Zhang P, 2014, P NATL ACAD SCI USA, V111, P18144, DOI 10.1073/pnas.1409770111	59	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903035
C	Li, YY; Chen, X; Li, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Yingying; Chen, Xin; Li, Na			Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MPC; STABILITY	This paper studies the online optimal control problem with time-varying convex stage costs for a time-invariant linear dynamical system, where a finite lookahead window of accurate predictions of the stage costs are available at each time. We design online algorithms, Receding Horizon Gradient-based Control (RHGC), that utilize the predictions through finite steps of gradient computations. We study the algorithm performance measured by dynamic regret: the online performance minus the optimal performance in hindsight. It is shown that the dynamic regret of RHGC decays exponentially with the size of the lookahead window. In addition, we provide a fundamental limit of the dynamic regret for any online algorithms by considering linear quadratic tracking problems. The regret upper bound of one RHGC method almost reaches the fundamental limit, demonstrating the effectiveness of the algorithm. Finally, we numerically test our algorithms for both linear and nonlinear systems to show the effectiveness and generality of our RHGC.	[Li, Yingying; Chen, Xin; Li, Na] Harvard Univ, SEAS, Cambridge, MA 02138 USA	Harvard University	Li, YY (corresponding author), Harvard Univ, SEAS, Cambridge, MA 02138 USA.	yingyingli@g.harvard.edu; chen_xin@g.harvard.edu; nali@seas.harvard.edu	Chen, Xin/GPK-2599-2022		NSF Career [1553407]; ARPA-E NODES; AFOSR YIP program; ONR YIP program	NSF Career(National Science Foundation (NSF)NSF - Office of the Director (OD)); ARPA-E NODES; AFOSR YIP program(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR YIP program(Office of Naval Research)	This work was supported by NSF Career 1553407, ARPA-E NODES, AFOSR YIP and ONR YIP programs.	Abbasi-Yadkori Y, 2014, PR MACH LEARN RES, V32; Agarwal Naman, 2019, P 36 INT C MACH LEAR, P111; Allan DA, 2017, SYST CONTROL LETT, V106, P68, DOI 10.1016/j.sysconle.2017.03.005; Amrit R, 2011, ANNU REV CONTROL, V35, P178, DOI 10.1016/j.arcontrol.2011.10.011; Andrew L.H., 2013, J MACHINE LEARNING R, P741; Angeli D, 2016, ANNU REV CONTROL, V41, P218, DOI 10.1016/j.arcontrol.2016.04.003; Angeli D, 2012, IEEE T AUTOMAT CONTR, V57, P1615, DOI 10.1109/TAC.2011.2179349; Angeli D, 2009, IEEE DECIS CONTR P, P7972, DOI 10.1109/CDC.2009.5400707; [Anonymous], 2013, INTRO LECT CONVEX OP; Baca T, 2018, IEEE INT C INT ROBOT, P6753, DOI 10.1109/IROS.2018.8594266; Badici M, 2015, IEEE DECIS CONTR P, P6730, DOI 10.1109/CDC.2015.7403279; Bang H, 2005, BIOMETRICS, V61, P962, DOI 10.1111/j.1541-0420.2005.00377.x; Bertsekas DP., 2011, DYNAMIC PROGRAMMING, V1; BORODIN A, 1992, J ACM, V39, P745, DOI 10.1145/146585.146588; Chen NJ, 2016, SIGMETRICS/PERFORMANCE 2016: PROCEEDINGS OF THE SIGMETRICS/PERFORMANCE JOINT INTERNATIONAL CONFERENCE ON MEASUREMENT AND MODELING OF COMPUTER SCIENCE, P193, DOI [10.1145/2896377.2901464, 10.1145/2964791.2901464]; Cohen A, 2018, PR MACH LEARN RES, V80; Damm T, 2014, SIAM J CONTROL OPTIM, V52, P1935, DOI 10.1137/120888934; Dean S., 2017, ARXIV171001688; Dean S., 2018, ARXIV180509388; Diehl M, 2011, IEEE T AUTOMAT CONTR, V56, P703, DOI 10.1109/TAC.2010.2101291; Ellis M, 2014, J PROCESS CONTR, V24, P1156, DOI 10.1016/j.jprocont.2014.03.010; Ellis M, 2014, AICHE J, V60, P507, DOI 10.1002/aic.14274; Ferramosca A, 2010, IEEE DECIS CONTR P, P6131, DOI 10.1109/CDC.2010.5717482; Ferramosca A, 2014, IEEE T AUTOMAT CONTR, V59, P2657, DOI 10.1109/TAC.2014.2326013; Goel G, 2019, PR MACH LEARN RES, V89; Graichen K, 2010, IEEE T AUTOMAT CONTR, V55, P2576, DOI 10.1109/TAC.2010.2057912; Grune L, 2017, IEEE DECIS CONTR P; Grune L, 2015, IEEE DECIS CONTR P, P4332, DOI 10.1109/CDC.2015.7402895; Grune L, 2014, J PROCESS CONTR, V24, P1187, DOI 10.1016/j.jprocont.2014.05.003; Grune L, 2013, AUTOMATICA, V49, P725, DOI 10.1016/j.automatica.2012.12.003; Grune Lars, 2018, OPTIMAL CONTROL APPL; Hazan E., 2016, FDN TRENDSR OPTIMIZA; Hespanha J. P., 2018, LINEAR SYSTEMS THEOR; Imwalle G., 2018, ADV NEURAL INFORM PR, P3814; Jadbabaie A, 2015, JMLR WORKSH CONF PRO, V38, P398; Kim KD, 2014, IEEE T AUTOMAT CONTR, V59, P3341, DOI 10.1109/TAC.2014.2351911; Klancar G, 2005, 2005 IEEE INTERNATIONAL SYMPOSIUM ON INTELLIGENT CONTROL & 13TH MEDITERRANEAN CONFERENCE ON CONTROL AND AUTOMATION, VOLS 1 AND 2, P1349; Kouro S, 2009, IEEE T IND ELECTRON, V56, P1826, DOI 10.1109/TIE.2008.2008349; Li Y., 2018, ARXIV180107780; Lin M., 2012, GREEN COMPUTING C IG, P1, DOI DOI 10.1109/IGCC.2012.6322266; Lin MH, 2013, IEEE ACM T NETWORK, V21, P1378, DOI 10.1109/TNET.2012.2226216; Lu  L., 2013, ONLINE ENERGY GENERA, V41; LUENBERGER DG, 1967, IEEE T AUTOMAT CONTR, VAC12, P290, DOI 10.1109/TAC.1967.1098584; Mokhtari A, 2016, IEEE DECIS CONTR P, P7195, DOI 10.1109/CDC.2016.7799379; Muller M.A., 2017, SICE J CONTROL MEAS, V10, P39; Niangjun Chen, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P191, DOI 10.1145/2745844.2745854; Perea-Lopez E, 2003, COMPUT CHEM ENG, V27, P1201, DOI 10.1016/S0098-1354(03)00047-4; Pololu Corporation, POL M3PI US GUID; Puterman M.L., 2014, MARKOV DECISION PROC; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Rawlings JB, 2012, NOB HILL PUB, P155; Richter S, 2012, IEEE T AUTOMAT CONTR, V57, P1391, DOI 10.1109/TAC.2011.2176389; Rios-Torres J, 2017, IEEE T INTELL TRANSP, V18, P1066, DOI 10.1109/TITS.2016.2600504; Shalev-Shwartz S., 2012, FDN TRENDSR MACHINE; Tu Stephen, 2017, ARXIV171208642; Vamvoudakis KG, 2010, AUTOMATICA, V46, P878, DOI 10.1016/j.automatica.2010.02.018; Van Scoy B., 2017, IEEE CONTROL SYSTEMS, V2, P49; Wang WL, 2007, INT J PROD ECON, V107, P56, DOI 10.1016/j.ijpe.2006.05.013; Wang Y, 2010, IEEE T CONTR SYST T, V18, P267, DOI 10.1109/TCST.2009.2017934; Xu W, 2006, IEEE IFIP NETW OPER, P115, DOI 10.1109/NOMS.2006.1687544; Yi Ouyang, 2017, ARXIV170904047; Zeilinger MN, 2011, IEEE T AUTOMAT CONTR, V56, P1524, DOI 10.1109/TAC.2011.2108450	62	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906053
C	Li, Z; Brendel, W; Walker, EY; Cobos, E; Muhammad, T; Reimer, J; Bethge, M; Sinz, FH; Pitkow, X; Tolias, AS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Zhe; Brendel, Wieland; Walker, Edgar Y.; Cobos, Erick; Muhammad, Taliah; Reimer, Jacob; Bethge, Matthias; Sinz, Fabian H.; Pitkow, Xaq; Tolias, Andreas S.			Learning From Brains How to Regularize Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS	Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) - unlike brains - are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.	[Li, Zhe; Walker, Edgar Y.; Cobos, Erick; Muhammad, Taliah; Reimer, Jacob; Pitkow, Xaq; Tolias, Andreas S.] Baylor Coll Med, Dept Neurosci, Houston, TX 77030 USA; [Li, Zhe; Walker, Edgar Y.; Cobos, Erick; Muhammad, Taliah; Reimer, Jacob; Sinz, Fabian H.; Pitkow, Xaq; Tolias, Andreas S.] Baylor Coll Med, Ctr Neurosci & Artificial Intelligence, Houston, TX 77030 USA; [Pitkow, Xaq; Tolias, Andreas S.] Rice Univ, Dept Elect & Comp Engn, Houston, TX 77251 USA; [Brendel, Wieland; Bethge, Matthias] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany; [Brendel, Wieland; Sinz, Fabian H.] Univ Tubingen, Bernstein Ctr Computat Neurosci, Tubingen, Germany; [Bethge, Matthias] Univ Tubingen, Inst Theoret Phys, Tubingen, Germany; [Sinz, Fabian H.] Univ Tubingen, Inst Bioinformat & Med Informat, Tubingen, Germany	Baylor College of Medicine; Baylor College of Medicine; Rice University; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen	Li, Z (corresponding author), Baylor Coll Med, Dept Neurosci, Houston, TX 77030 USA.; Li, Z (corresponding author), Baylor Coll Med, Ctr Neurosci & Artificial Intelligence, Houston, TX 77030 USA.	zhel@bcm.edu		Li, Zhe/0000-0003-4176-9687	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]; Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft) [ZUK 63]; Carl-Zeiss-Stiftung; German Federal Ministry of Education and Research (BMBF) through the Tubingen AI Center [FKZ: 01IS18039A]; DFG Cluster of Excellence "Machine Learning - New Perspectives for Science" [EXC 2064/1, 390727645]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC); Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft); Carl-Zeiss-Stiftung; German Federal Ministry of Education and Research (BMBF) through the Tubingen AI Center(Federal Ministry of Education & Research (BMBF)); DFG Cluster of Excellence "Machine Learning - New Perspectives for Science"(German Research Foundation (DFG))	This work is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. FS is supported by the Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, ZUK 63) and the Carl-Zeiss-Stiftung. FS and MB acknowledges the support from the German Federal Ministry of Education and Research (BMBF) through the Tubingen AI Center (FKZ: 01IS18039A) and the DFG Cluster of Excellence "Machine Learning - New Perspectives for Science", EXC 2064/1, project number 390727645.	Anselmi F, 2016, INF INFERENCE, V5, P134, DOI 10.1093/imaiai/iaw009; Blanchard Nathaniel, 2018, ARXIV180510726; Brendel W., 2019, INT C LEARN REPR; Brendel Wieland, 2017, DECISION BASED ADVER; Cadena SA, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006897; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cortes C, 2012, J MACH LEARN RES, V13, P795; Geirhos R., 2018, ADV NEURAL INFORM PR, P7538; Geirhos R., 2019, P INT C LEARNING REP, P1; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Hu J., 2017, CORR; Ilyas Andrew, 2019, ARXIV E PRINTS; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kornblith S., 2019, ARXIV190500414; Kriegeskorte Nikolaus, 2008, Front Syst Neurosci, V2, P4, DOI 10.3389/neuro.06.004.2008; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Madry Aleksander, 2018, ICLR; McClure P, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00131; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Poggio TA, 2016, COMPUT NEUROSCI-MIT, P1; Rauber Jonas, 2017, REL MACH LEARN WILD, P6, DOI DOI 10.21105/JOSS.02607; Schott Lukas, 2018, 1 ADVERSARIALLY ROBU; Sinz F., 2018, ADV NEURAL INFORM PR, V31, P7199; Szegedy Christian, 2013, ARXIV E PRINTS; Tacchetti A, 2018, ANNU REV VIS SCI, V4, P403, DOI 10.1146/annurev-vision-091517-034103; Talwalkar A, 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587670; Walker EY, 2018, BIORXIV; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	31	5	5	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901019
C	Lin, JX; Jain, U; Schwing, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lin, Jingxiang; Jain, Unnat; Schwing, Alexander G.			TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning Baselines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reasoning is an important ability that we learn from a very early age. Yet, reasoning is extremely hard for algorithms. Despite impressive recent progress that has been reported on tasks that necessitate reasoning, such as visual question answering and visual dialog, models often exploit biases in datasets. To develop models with better reasoning abilities, recently, the new visual commonsense reasoning (VCR) task has been introduced. Not only do models have to answer questions, but also do they have to provide a reason for the given answer. The proposed baseline achieved compelling results, leveraging a meticulously designed model composed of LSTM modules and attention nets. Here we show that a much simpler model obtained by ablating and pruning the existing intricate baseline can perform better with half the number of trainable parameters. By associating visual features with attribute information and better text to image grounding, we obtain further improvements for our simpler & effective baseline, TAB-VCR. We show that this approach results in a 5.3%, 4.4% and 6.5% absolute improvement over the previous state-of-the-art [103] on question answering, answer justification and holistic VCR.	[Lin, Jingxiang; Jain, Unnat; Schwing, Alexander G.] Univ Illinois, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Lin, JX (corresponding author), Univ Illinois, Urbana, IL 61801 USA.				NSF [1718221]; MRI [1725729]; UIUC; Samsung; 3M; Cisco Systems Inc. [CG 1377144]; Adobe	NSF(National Science Foundation (NSF)); MRI; UIUC; Samsung(Samsung); 3M(3M); Cisco Systems Inc.; Adobe	This work is supported in part by NSF under Grant No. 1718221 and MRI #1725729, UIUC, Samsung, 3M, Cisco Systems Inc. (Gift Award CG 1377144) and Adobe. We thank NVIDIA for providing GPUs used for this work and Cisco for access to the Arcetri cluster. The authors thank Prof. Svetlana Lazebnik for insightful discussions and Rowan Zellers for releasing and helping us navigate the VCR dataset & evaluation.	Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Alayrac JB, 2016, PROC CVPR IEEE, P4575, DOI 10.1109/CVPR.2016.495; Alberti C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2131; ANDERSON P, 2018, CVPR, V3, P6, DOI DOI 10.1109/CVPR.2018.00636; Andreas J., 2016, ANN C N AM CHAPT ASS; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Bau D., 2019, ICLR; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Bird Steven, 2004, ACL, DOI DOI 10.3115/1118108.1118117; Chen Kan, 2015, ARXIV151105960; Chen Yen-Chun, 2019, ARXIV190911740; CHI MTH, 1989, COGNITIVE SCI, V13, P145, DOI 10.1207/s15516709cog1302_1; Chuang C.-Y., 2018, CVPR; Core MG, 2006, P AAAI, V2, P1766, DOI DOI 10.21236/ADA459166; Crowley K., 1999, CHILD DEV; Das A., 2016, EMNLP; Das A., 2018, P SER P MACHINE LEAR; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; Das Abhishek, 2017, ICCV; Das Abhishek, 2018, CVPR; de Vries Harm, 2017, CVPR; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Ehsani K., 2018, CVPR; Farhadi Ali, 2009, CVPR; Felsen P., 2017, CVPR; Fukui Akira, 2016, ARXIV160601847; Gao HY, 2015, ADV NEUR IN, V28; Gordon Daniel, 2018, IEEE C COMP VIS PATT; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Holyoak K. J., 2012, OXFORD HDB THINKING; Hudson D. A., 2019, GQA NEW DATASET COMP; Ilievski I., 2016, FOCUSED DYNAMIC ATTE; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jabri A., 2016, ECCV; Jain U., 2017, P IEEE C COMP VIS PA, P6485; Jain U., 2018, ARXIV180311186; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kim J, 2017, ICLR; Kim Jinkyu, 2018, ECCV, P2; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lane H Chad, 2005, TECHNICAL REPORT; Legare C. H., 2014, J EXPT CHILD PSYCHOL; Lei J., 2020, P 58 ANN M ASS COMP; Lei Jie, 2018, EMNLP, P1369, DOI DOI 10.18653/V1/D18-1167; Li Gen, 2019, ARXIV190806066; Li YK, 2018, PROC CVPR IEEE, P6116, DOI 10.1109/CVPR.2018.00640; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lu JS, 2019, ADV NEUR IN, V32; Lu JS, 2016, ADV NEUR IN, V29; Lu Jiasen, 2017, ADV NEURAL INFORM PR, P314; Ma C.-Y, 2018, CVPR; Ma L., 2016, AAAI; Maharaj Tegan, 2017, CVPR; Malinowski M., 2015, P INT C COMP VIS; Malinowski M., 2014, NIPS; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Mostafazadeh N., 2016, ARXIV160306059; Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Narasimhan M., 2018, P NIPS; Narasimhan M., 2018, ARXIV180901124; Park Dong Huk, 2018, P IEEE C COMP VIS PA; Ren M., 2015, P 28 INT C NEUR INF, V2, P2953, DOI [10.5555/2969442.2969570, DOI 10.5555/2969442.2969570]; Rhinehart N., 2017, ICCV; Rohrbach Anna, 2017, INT J COMPUTER VISIO; Roscoe RD, 2008, INSTR SCI, V36, P321, DOI 10.1007/s11251-007-9034-5; Ross J. A., 1995, ALBERTA J EDUC RES; Russakovsky O., 2010, ECCV; Schwartz I., 2017, NEURIPS; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; Shortliffe E. H., 1975, Mathematical Biosciences, V23, P351, DOI 10.1016/0025-5564(75)90047-4; Singh K. K., 2016, WACV; Su Weijie, 2019, VL BERT PRETRAINING; Tapaswi M., 2016, IEEE C COMP VIS PATT; Tommasi T, 2019, INT J COMPUT VISION, V127, P38, DOI 10.1007/s11263-018-1096-0; van Lent M, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P900; Vicol Paul, 2018, CVPR; Wang P., 2017, IJCAI; Williams J. J., 2013, COGNITIVE PSYCHOL; Williams J. J., 2010, COGNITIVE SCI; Wu Q., 2016, P CVPR; Wu Z., 1994, P C ASS COMP LING; Xiong C., 2016, P INT C MACHINE LEAR; Xu H., 2016, P EUR C COMP VIS ECC; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Ye T, 2018, LECT NOTES COMPUT SC, V11216, P89, DOI 10.1007/978-3-030-01258-8_6; Yoshikawa Yuya, 2018, ARXIV180404326; Yu L, 2015, IEEE INT C COMP VIS; Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202; Zellers Rowan, 2019, P CVPR, P2839; Zhang P., 2016, CVPR; Zhang Q, 2018, ABS180200121 CORR; Zhang Quanshi, 2019, CVPR; Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590; Zhou YP, 2015, IEEE I CONF COMP VIS, P4498, DOI 10.1109/ICCV.2015.511; Zhu Y., 2016, P IEEE C COMP VIS PA	108	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907029
C	Locatello, F; Yurtsevert, A; Fercoq, O; Cevhert, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Locatello, Francesco; Yurtsevert, Alp; Fercoq, Olivier; Cevhert, Volkan			Stochastic Frank-Wolfe for Composite Convex Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEMIDEFINITE; MATRIX	A broad class of convex optimization problems can be formulated as a semidefinite program (SDP), minimization of a convex function over the positive-semidefinite cone subject to some affine constraints. The majority of classical SDP solvers are designed for the deterministic setting where problem data is readily available. In this setting, generalized conditional gradient methods (aka Frank-Wolfe-type methods) provide scalable solutions by leveraging the so-called linear minimization oracle instead of the projection onto the semidefinite cone. Most problems in machine learning and modern engineering applications, however, contain some degree of stochasticity. In this work, we propose the first conditional-gradient-type method for solving stochastic optimization problems under affine constraints. Our method guarantees O(k(-1/3)) convergence rate in expectation on the objective residual and O(k (5/12)) on the feasibility gap.	[Locatello, Francesco] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Yurtsevert, Alp; Cevhert, Volkan] Ecole Polytech Fed Lausanne, LIONS, Lausanne, Switzerland; [Fercoq, Olivier] Univ Paris Saclay, Telecom Paris, LTCI, Paris, France	Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Locatello, F (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	francesco.locatello@inf.ethz.ch; alp.yurtsever@epf1.ch; olivier.fercoq@telecom-paristech.fr; volkan.cevher@epf1.ch	Locatello, Francesco/GQY-6025-2022	Locatello, Francesco/0000-0002-4850-0683	Max Planck ETH Center for Learning Systems; ETH Core Grant; Google Ph.D. Fellowship; Swiss National Science Foundation (SNSF) [200021_178865/1]; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program [725594 - time-data]	Max Planck ETH Center for Learning Systems; ETH Core Grant; Google Ph.D. Fellowship(Google Incorporated); Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF)); European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program(European Research Council (ERC))	Francesco Locatello has received funding from the Max Planck ETH Center for Learning Systems, by an ETH Core Grant (to Gunnar Ratsch) and by a Google Ph.D. Fellowship. Volkan Cevher and Alp Yurtsever have received funding from the Swiss National Science Foundation (SNSF) under grant number 200021_178865/1, and the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement no 725594 - time-data).	Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; [Anonymous], 2012, ICML; Cevher V, 2018, LECT NOTES MATH, V2227, P149, DOI 10.1007/978-3-319-97478-1_7; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Dias PA, 2019, IEEE WINT CONF APPL, P21, DOI 10.1109/WACV.2019.00010; Dunner C, 2016, PR MACH LEARN RES, V48; Fan JQ, 2016, ECONOMET J, V19, pC1, DOI 10.1111/ectj.12061; Fan JQ, 2014, NATL SCI REV, V1, P293, DOI 10.1093/nsr/nwt032; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Garber D., 2018, ARXIV180910477; Gidel G, 2018, PR MACH LEARN RES, V84; Goldfarb D, 2017, PR MACH LEARN RES, V54, P1066; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Hazan E, 2008, LECT NOTES COMPUT SC, V4957, P306, DOI 10.1007/978-3-540-78773-0_27; Hazan E, 2016, PR MACH LEARN RES, V48; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Lacoste-Julien S., 2013, P 30 INT C MACH LEAR, P53; Lacoste-Julien S., 2016, ARXIV PREPRINT ARXIV; Lan G., 2014, ARXIV13095550V2; Lan GH, 2017, PR MACH LEARN RES, V70; Lan GG, 2016, SIAM J OPTIMIZ, V26, P1379, DOI 10.1137/140992382; Locatello F, 2018, PR MACH LEARN RES, V80; Locatello F, 2017, ADV NEUR IN, V30; Locatello F, 2017, PR MACH LEARN RES, V54, P860; Lu H., 2018, ARXIV180707680; Madani R, 2015, IEEE T POWER SYST, V30, P199, DOI 10.1109/TPWRS.2014.2322051; Mixon DG, 2017, INF INFERENCE, V6, P389, DOI 10.1093/imaiai/iax001; Mokhtari A., 2018, ARXIV180409554; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983; Tran-Dinh Q, 2018, SIAM J OPTIMIZ, V28, P96, DOI 10.1137/16M1093094; Reddi S. J., 2016, ARXIV160708254; Richard E., 2012, P 29 INT C MACHINE L, P51; Schafer J, 2005, STAT APPL GENET MO B, V4, DOI 10.2202/1544-6115.1175; Yu Y., 2014, GEN CONDITIONAL GRAD; Yurtsever A., 2016, ADV NEUR IN, V29, P4322; Yurtsever A, 2018, PR MACH LEARN RES, V80; Yurtsever A, 2019, PR MACH LEARN RES, V97	41	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905087
C	Lubars, B; Tan, CH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lubars, Brian; Tan, Chenhao			Ask not what AI can do, but what AI should do: Towards a framework of task delegability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MOTIVATION; TRUST; HUMANS	While artificial intelligence (AI) holds promise for addressing societal challenges, issues of exactly which tasks to automate and to what extent to do so remain understudied. We approach this problem of task delegability from a human-centered perspective by developing a framework on human perception of task delegation to AI. We consider four high-level factors that can contribute to a delegation decision: motivation, difficulty, risk, and trust. To obtain an empirical understanding of human preferences in different tasks, we build a dataset of 100 tasks from academic papers, popular media portrayal of AI, and everyday life, and administer a survey based on our proposed framework. We find little preference for full AI control and a strong preference for machine-in-the-loop designs, in which humans play the leading role. Among the four factors, trust is the most correlated with human preferences of optimal human-machine delegation. This framework represents a first step towards characterizing human preferences of AI automation across tasks. We hope this work encourages future efforts towards understanding such individual attitudes; our goal is to inform the public and the AI research community rather than dictating any direction in technology development.	[Lubars, Brian; Tan, Chenhao] Univ Colorado Boulder, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Lubars, B (corresponding author), Univ Colorado Boulder, Boulder, CO 80309 USA.	brian.lubars@colorado.edu; chenhao.tan@colorado.edu						Anderson Ashton, 2016, P KDD; Anderson Drew, 2017, GLAAD HRC CALL STANF; [Anonymous], 1999, P SIGCHI C HUM FACT, DOI DOI 10.1145/302979.303030; BANDURA A, 1989, AM PSYCHOL, V44, P1175, DOI 10.1037/0003-066X.44.9.1175; Bradshaw JM, 2012, IEEE INTELL SYST, V27, P8, DOI 10.1109/MIS.2012.37; Branson S., 2010, P ECCV; Castelfranchi C, 1998, ROBOT AUTON SYST, V24, P141, DOI 10.1016/S0921-8890(98)00028-1; Cheng JZ, 2016, SCI REP-UK, V6, DOI 10.1038/srep24454; Clark Elizabeth, 2018, P IUI; Erel Isil, 2018, TECHNICAL REPORT; Fails Jerry Alan, 2003, P IUI; Fakoor Rasool, 2013, P ICML, V28; Finale Doshi-Velez, 2017, RIGOROUS SCI INTERPR; Fitts Paul M, 1951, TECHNICAL REPORT; Frey CB, 2017, TECHNOL FORECAST SOC, V114, P254, DOI 10.1016/j.techfore.2016.08.019; Gombolay Matthew, 2016, INT J ROBOT RES; Green Ben, 2019, P CSCW; Green Ben, 2019, P FAT; Green Spence, 2015, QUEUE, V13, DOI 10.1145/2791301.2798086; Hayes AF, 2007, COMMUN METHODS MEAS, V1, P77, DOI 10.1080/19312450709336664; Johnson M, 2014, J HUM-ROBOT INTERACT, V3, P43, DOI 10.5898/JHRI.3.1.Johnson; Kim Been, 2016, P NIPS; Kleinberg J, 2018, Q J ECON, V133, P237, DOI 10.1093/qje/qjx032; Lai Vivian, 2019, P FAT, P5819, DOI [10.1006/ijhc.1994.1007, DOI 10.1006/IJHC.1994.1007]; Latham GP, 2005, ANNU REV PSYCHOL, V56, P485, DOI 10.1146/annurev.psych.55.090902.142105; Lee JD, 2004, HUM FACTORS, V46, P50, DOI 10.1518/hfes.46.1.50.30392; LEE JD, 1994, INT J HUM-COMPUT ST, V40, P153, DOI 10.1006/ijhc.1994.1007; Lewandowsky S, 2000, J EXP PSYCHOL-APPL, V6, P104, DOI 10.1037//1076-898X.6.2.104; Litjens G, 2016, SCI REP-UK, V6, DOI 10.1038/srep26286; Locke EA, 2002, AM PSYCHOL, V57, P705, DOI 10.1037//0003-066X.57.9.705; Locke EA, 2000, APPL PSYCHOL-INT REV, V49, P408, DOI 10.1111/1464-0597.00023; Mezzofiore Gianluca, 2017, AI STUDY WHICH CLAIM; Milewski Allen, 2014, PEOPLE DELEGATE, V11; Murphy H., 2017, WHY STANFORD RES TRI; Ng Andrew, 2017, STANF MSX FUT FOR; Parasuraman R, 2000, IEEE T SYST MAN CY A, V30, P286, DOI 10.1109/3468.844354; Perkins LeeAnn, 2010, DESIGNING HUMAN CTR; PRICE HE, 1985, HUM FACTORS, V27, P33, DOI 10.1177/001872088502700104; Ribeiro M. T., 2016, P KDD; Ribeiro Marco Tulio, 2018, P AAAI; Roemmele Melissa, 2015, P ICDIS; Schwab K., 2017, 4 IND REVOLUTION; Stout Nathan, 2014, AIS T HUMAN COMPUTER, V6; Susskind R., 2015, FUTURE PROFESSIONS T; Wang Y, 2018, J PERS SOC PSYCHOL, V114, P246, DOI 10.1037/pspa0000098; Yang Qian, 2019, P CHI	46	5	5	2	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300006
C	Ma, XZ; Kong, X; Zhang, SH; Hovy, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ma, Xuezhe; Kong, Xiang; Zhang, Shanghang; Hovy, Eduard			MaCow: Masked Convolutional Generative Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Flow-based generative models, conceptually attractive due to tractability of the exact log-likelihood computation and latent-variable inference as well as efficiency in training and sampling, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. Despite computational efficiency, the density estimation performance of flow-based generative models significantly falls behind those of state-of-the-art autoregressive models. In this work, we introduce masked convolutional generative flow (MACOW), a simple yet effective architecture for generative flow using masked convolution. By restricting the local connectivity to a small kernel, MACOW features fast and stable training along with efficient sampling while achieving significant improvements over Glow for density estimation on standard image benchmarks, considerably narrowing the gap with autoregressive models.	[Ma, Xuezhe; Kong, Xiang; Zhang, Shanghang; Hovy, Eduard] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Ma, XZ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	xuezhem@cs.cmu.edu; xiangk@cs.cmu.edu; shanghaz@andrew.cmu.edu; hovy@cmu.edu	Zhang, Lisa/AAW-9795-2021; Ma, Xuezhe/GWN-1885-2022	Ma, Xuezhe/0000-0001-7582-1653; Hovy, Eduard/0000-0002-3270-7903				Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Brock A., 2019, INT C LEARNING REPRE; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Djork-Arn, ICLR 2016; Germain M, 2015, PR MACH LEARN RES, V37, P881; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ho Jonathan, 2019, ICML; Hoogeboom E, 2019, PR MACH LEARN RES, V97; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Larochelle H., 2011, INT C ART INT STAT; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma Xuezhe, 2019, INT C LEARN REPR ICL; Menick Jacob, 2019, GENERATING HIGH FIDE; Oord A.V.D., 2016, SSW; Papamakarios George, 2017, ARXIV170507057; Parmar Niki, 2018, ARXIV180205751, P2; Paszke Adam, 2017, NIPS AUT WORKSH, DOI DOI 10.1017/CBO9781107707221.009; Prenger R., 2018, ARXIV181100002; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2017, PR MACH LEARN RES, V70; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Salimans Tim, 2017, ARXIV170105517; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Theis Lucas, 2016, ICLR; Uria Benigno, 2013, P 26 INT C NEURAL IN; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Xi Chen, 2017, ARXIV171209763; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zheng Guoqing, 2017, CORR; Ziegler ZM, 2019, PR MACH LEARN RES, V97	36	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305084
C	Manek, G; Kolter, JZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Manek, Gaurav; Kolter, J. Zico			Learning Stable Deep Dynamics Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep networks are commonly used to model dynamical systems, predicting how the state of a system will evolve over time (either autonomously or in response to control inputs). Despite the predictive power of these systems, it has been difficult to make formal claims about the basic properties of the learned systems. In this paper, we propose an approach for learning dynamical systems that are guaranteed to be stable over the entire state space. The approach works by jointly learning a dynamics model and Lyapunov function that guarantees non-expansiveness of the dynamics under the learned Lyapunov function. We show that such learning systems are able to model simple dynamical systems and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion.	[Manek, Gaurav; Kolter, J. Zico] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; [Kolter, J. Zico] Bosch Ctr AI, Chennai, Tamil Nadu, India	Carnegie Mellon University	Manek, G (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	gmanek@cs.cmu.edu; zkolter@cs.cmu.edu						Amos B, 2017, PR MACH LEARN RES, V70; [Anonymous], 2008, NEURAL INFORM PROCES; Blocher C, 2017, INT CONF UBIQ ROBOT, P124; Chen Yize, 2018, ARXIV180511835; Chow Y., 2018, ADV NEURAL INFORM PR, P8092; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gu SX, 2016, PR MACH LEARN RES, V48; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Khalil HK., 2002, NONLINEAR SYSTEMS, Vthird; Khansari-Zadeh SM, 2011, IEEE T ROBOT, V27, P943, DOI 10.1109/TRO.2011.2159412; Kingma D. P., 2013, AUTO ENCODING VARIAT; LaSalle Joseph, 2012, STABILITY LIAPUNOVS, V4; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Papachristodoulou A, 2002, IEEE DECIS CONTR P, P3482, DOI 10.1109/CDC.2002.1184414; Parrilo P.A., 2000, THESIS; Richards S. M., 2018, ARXIV180800924, P466; Schodl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012; Taylor Andrew J, 2019, ARXIV190301577; Umlauft J, 2017, PR MACH LEARN RES, V70; VanderPlas Jake, 2017, TRIPLE PENDULUM CHAO	21	5	5	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902072
C	Mason, B; Tripathy, A; Nowak, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mason, Blake; Tripathy, Ardhendu; Nowak, Robert			Learning Nearest Neighbor Graphs from Noisy Distance Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM	We consider the problem of learning the nearest neighbor graph of a dataset of n items. The metric is unknown, but we can query an oracle to obtain a noisy estimate of the distance between any pair of items. This framework applies to problem domains where one wants to learn people's preferences from responses commonly modeled as noisy distance judgments. In this paper, we propose an active algorithm to find the graph with high probability and analyze its query complexity. In contrast to existing work that forces Euclidean structure, our method is valid for general metrics, assuming only symmetry and the triangle inequality. Furthermore, we demonstrate efficiency of our method empirically and theoretically, needing only O(n log(n) Delta(-2)) queries in favorable settings, where Delta(-2) accounts for the effect of noise. Using crowd-sourced data collected for a subset of the UT Zappos50K dataset, we apply our algorithm to learn which shoes people believe are most similar and show that it beats both an active baseline and ordinal embedding.	[Mason, Blake; Tripathy, Ardhendu; Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Mason, B (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	bmason3@wisc.edu; astripathy@wisc.edu; rdnowak@wisc.edu	Tripathy, Ardhendu Shekhar/U-2887-2019	Tripathy, Ardhendu Shekhar/0000-0003-1893-4891	AFOSR/AFRL grants [FA8750-17-2-0262, FA9550-18-1-0166]	AFOSR/AFRL grants	The authors wish to thank Lalit Jain for many helpful discussions over the course of this work for which the paper is better and the reviewers for their helpful suggestions. This work was partially supported by AFOSR/AFRL grants FA8750-17-2-0262 and FA9550-18-1-0166.	Bagaria Vivek, 2017, ARXIV171100817; Bagaria Vivek, 2018, ARXIV180508321; Bhatia N., 2010, INT J COMPUT SCI INF; Booth BM, 2019, INT CONF ACOUST SPEE, P7630, DOI 10.1109/ICASSP.2019.8683730; Brickell J, 2008, SIAM J MATRIX ANAL A, V30, P375, DOI 10.1137/060653391; Clarkson K., 1983, S FDN COMP SCI, P26, DOI DOI 10.1109/SFCS.1983.16; Cormen T.H., 2009, INTRO ALGORITHMS; Cvetkovski A, 2009, IEEE INFOCOM SER, P1647, DOI 10.1109/INFCOM.2009.5062083; Eriksson B, 2010, LECT NOTES COMPUT SC, V6032, P171, DOI 10.1007/978-3-642-12334-4_18; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; FLOYD RW, 1962, COMMUN ACM, V5, P345, DOI 10.1145/367766.368168; Garivier A, 2013, 2013 IEEE INFORMATION THEORY WORKSHOP (ITW); Gilbert AC, 2017, ANN ALLERTON CONF, P612; Goyal A, 2008, 2008 IEEE 14TH INTERNATIONAL MIXED-SIGNALS, SENSORS, AND SYSTEMS TEST WORKSHOP, P25; Heim E., 2015, ARXIV151102254; Houle ME, 2015, IEEE T PATTERN ANAL, V37, P136, DOI 10.1109/TPAMI.2014.2343223; Jain Lalit, 2016, ADV NEURAL INFORM PR, V29, P2711; Jamieson K., 2013, ARXIV13063917; Jamieson KG., 2015, ADV NEURAL INFORM PR, V28, P2656; Kaufmann E., 2013, C LEARNING THEORY, P228; Kleindessner M., 2017, J MACHINE LEARNING R, V18, P1889; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; LeJeune Daniel, 2019, ARXIV190209465; Leunis E., 2012, IEEE INT WORKSH MACH, P1, DOI DOI 10.1109/MLSP.2012.6349720; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; Sankaranarayanan J, 2007, COMPUT GRAPH-UK, V31, P157, DOI 10.1016/j.cag.2006.11.011; SHEPARD RN, 1962, PSYCHOMETRIKA, V27, P125, DOI 10.1007/BF02289630; Singla A., 2016, P 33 INT C INT C MAC, V48, P412; Tamuz Omer, 2011, P 28 INT C INT C MAC, P673; Tschopp D., 2011, ADV NEURAL INFORM PR, V24, P2231; VAIDYA PM, 1989, DISCRETE COMPUT GEOM, V4, P101, DOI 10.1007/BF02187718; Yu A., 2014, COMPUTER VISION PATT; Yu A., 2017, INT C COMP VIS ICCV	33	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901024
C	Masrani, V; Le, TA; Wood, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Masrani, Vaden; Le, Tuan Anh; Wood, Frank			The Thermodynamic Variational Objective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MARGINAL LIKELIHOOD ESTIMATION; MODELS	We introduce the thermodynamic variational objective (TVO) for learning in both continuous and discrete deep generative models. The TVO arises from a key connection between variational inference and thermodynamic integration that results in a tighter lower bound to the log marginal likelihood than the standard variational evidence lower bound (ELBO) while remaining as broadly applicable. We provide a computationally efficient gradient estimator for the TVO that applies to continuous, discrete, and non-reparameterizable distributions and show that the objective functions used in variational inference, variational autoencoders, wake sleep, and inference compilation are all special cases of the TVO. We use the TVO to learn both discrete and continuous deep generative models and empirically demonstrate state of the art model and inference network learning.	[Masrani, Vaden; Wood, Frank] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada; [Le, Tuan Anh] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	University of British Columbia; Massachusetts Institute of Technology (MIT)	Masrani, V (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.				EPSRC DTA; Google [DF6700]; Natural Sciences and Engineering Research Council of Canada (NSERC); Canada CIFAR AI Chairs Program; Compute Canada; Intel; DARPA under its D3M program; DARPA under LWLL program	EPSRC DTA(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google(Google Incorporated); Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada CIFAR AI Chairs Program; Compute Canada; Intel(Intel Corporation); DARPA under its D3M program; DARPA under LWLL program	We would like to thank Trevor Campbell, Adam Scibior, Boyan Beronov, and Saifuddin Syed for their helpful comments on early drafts of this manuscript. Tuan Anh Le's research leading to these results is supported by EPSRC DTA and Google (project code DF6700) studentships. We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada CIFAR AI Chairs Program, Compute Canada, Intel, and DARPA under its D3M and LWLL programs.	Blei David, VARIATIONAL INFERENC; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Bornschein J., 2015, ICLR; Bornschein J, 2016, PR MACH LEARN RES, V48; Burda Yuri, 2016, 4 INT C LEARN REPR I; Eguchi S., 1985, HIROSHIMA MATH J, V15, P341; Evans DJ, 1986, INT SCH PHYS E FERMI; Fan Y, 2011, MOL BIOL EVOL, V28, P523, DOI 10.1093/molbev/msq224; Figurnov Mikhail, 2018, ADV NEURAL INFORM PR, P441; Friel N, 2008, J R STAT SOC B, V70, P589, DOI 10.1111/j.1467-9868.2007.00650.x; Grathwohl Will, 2018, ICLR; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Grosse R., 2013, ADV NEURAL INFORM PR, P2769; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Jang E., 2017, ICLR; Kingma D. P, 2014, ARXIV13126114; Lartillot N, 2006, SYST BIOL, V55, P195, DOI 10.1080/10635150500433722; Le T. A., 2016, ARXIV161009900; Le Tuan Anh, 2018, ICLR; LEcuyer P, 1993, MANAGE SCI; Maddison Chris J, 2017, ICLR; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Naesseth CA, 2017, PR MACH LEARN RES, V54, P489; Naesseth CA, 2018, PR MACH LEARN RES, V84; Neal RM, 1993, CRGTR931 U TOR DEP C; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Osborne M., 2012, ADV NEURAL INF PROCE, V25, P1; Rainforth Tom, 2018, ARXIV180204537; Rasmussen CE, 2003, ADV NEURAL INF PROCE, P505; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rodrigue N, 2011, SYST BIOL, V60, P881, DOI 10.1093/sysbio/syr065; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Teh Yee Whye, 2018, REVISITING REWEIGHTE; Titsias Michalis K., 2018, UNBIASED IMPLICIT VA; Tucker G, 2017, ADV NEUR IN, V30; Tucker George, 2018, ARXIV181004152; Weaver Lex., 2001, P 17 C UNC ART INT U, P538; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xie WG, 2011, SYST BIOL, V60, P150, DOI 10.1093/sysbio/syq085	43	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903019
C	Neverova, N; Novotny, D; Vedaldi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Neverova, Natalia; Novotny, David; Vedaldi, Andrea			Correlated Uncertainty for Learning Dense Correspondences from Noisy Labels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many machine learning methods depend on human supervision to achieve optimal performance. However, in tasks such as DensePose, where the goal is to establish dense visual correspondences between images, the quality of manual annotations is intrinsically limited. We address this issue by augmenting neural network predictors with the ability to output a distribution over labels, thus explicitly and introspectively capturing the aleatoric uncertainty in the annotations. Compared to previous works, we show that correlated error fields arise naturally in applications such as DensePose and these fields can be modelled by deep networks, leading to a better understanding of the annotation errors. We show that these models, by understanding uncertainty better, can solve the original DensePose task more accurately, thus setting the new state-of-the-art accuracy in this benchmark. Finally, we demonstrate the utility of the uncertainty estimates in fusing the predictions produced by multiple models, resulting in a better and more principled approach to model ensembling which can further improve accuracy.	[Neverova, Natalia; Novotny, David; Vedaldi, Andrea] Facebook AI Res, New York, NY 10003 USA	Facebook Inc	Neverova, N (corresponding author), Facebook AI Res, New York, NY 10003 USA.	nneverova@fb.com; dnovotny@fb.com; vedaldi@fb.com						Blundell C., 2015, ICML; Bogo F, 2016, ECCV; BREIMAN L, 1996, MACHINE LEARNING, V24; Frimmer M, 2017, NATO SCI PEACE SEC B, P3, DOI 10.1007/978-94-024-0850-8_1; Gal Y., 2016, ICML; Guler R.A., 2018, CVPR; Hernandez-Lobato J. M., 2015, ICML; KENDALL A., 2017, NIPS; Khan M. E., 2018, ICML; Kiureghian A. Der, 2007, SPEC WORKSH RISK ACC; Lakshminarayanan B., 2017, NIPS; Liu G., 2018, ECCV; Neverova Natalia, 2019, CVPR; Newell A., 2016, ECCV; Osband I., 2016, NIPS; Pearce T., 2018, ICML; Su Ken, 2019, ARXIV190404514V1; Tagasovska N., 2019, ARXIV181100908; Varol G., 2017, CVPR	19	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300083
C	Pierrot, T; Ligner, G; Reed, S; Sigaud, O; Perrin, N; Laterre, A; Kas, D; Beguir, K; de Freitas, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pierrot, Thomas; Ligner, Guillaume; Reed, Scott; Sigaud, Olivier; Perrin, Nicolas; Laterre, Alexandre; Kas, David; Beguir, Karim; de Freitas, Nando			Learning Compositional Neural Programs with Recursive Tree Search and Planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel reinforcement learning algorithm, AlphaNPI, that incorporates the strengths of Neural Programmer-Interpreters (NPI) and AlphaZero. NPI contributes structural biases in the form of modularity, hierarchy and recursion, which are helpful to reduce sample complexity, improve generalization and increase interpretability. AlphaZero contributes powerful neural network guided search algorithms, which we augment with recursion. AlphaNPI only assumes a hierarchical program specification with sparse rewards: 1 when the program execution satisfies the specification, and 0 otherwise. This specification enables us to overcome the need for strong supervision in the form of execution traces and consequently train NPI models effectively with reinforcement learning. The experiments show that AlphaNPI can sort as well as previous strongly supervised NPI variants. The AlphaNPI agent is also trained on a Tower of Hanoi puzzle with two disks and is shown to generalize to puzzles with an arbitrary number of disks. The experiments also show that when deploying our neural network policies, it is advantageous to do planning with guided Monte Carlo tree search.	[Pierrot, Thomas; Ligner, Guillaume; Laterre, Alexandre; Kas, David; Beguir, Karim] InstaDeep, London, England; [Reed, Scott; de Freitas, Nando] DeepMind, London, England; [Sigaud, Olivier] Sorbonne Univ, Paris, France; [Perrin, Nicolas] Sorbonne Univ, CNRS, Paris, France	UDICE-French Research Universities; Sorbonne Universite; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Pierrot, T (corresponding author), InstaDeep, London, England.	t.pierrot@instadeep.com; g.ligner@instadeep.com; reedscot@google.com; olivier.sigaud@upmc.fr; perrin@isir.upmc.fr; a.laterre@instadeep.com; d.kas@instadeep.com; kb@instadeep.com; nandodefreitas@google.com			French National Research Agency (ANR) [ANR-18-CE33-0005 HUSKI]	French National Research Agency (ANR)(French National Research Agency (ANR))	Work by Nicolas Perrin was partially supported by the French National Research Agency (ANR), Project ANR-18-CE33-0005 HUSKI.	Andreas J, 2017, PR MACH LEARN RES, V70; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; [Anonymous], 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2017, ICLR; Bai A., 2016, P 25 INT JOINT C ART, P3029; Balog Matej, 2016, INT C LEARN REPR; Bengio Yoshua, 2019, ARXIV190110912; Bosnjak M, 2017, PR MACH LEARN RES, V70; Bunel R. R., 2016, C NEURAL INFORM PROC, V29, P1444; Chen X., 2017, P ICLR; Chen Yutian, 2019, ICLR; Denil M., 2017, ARXIV170606383; Devlin J, 2017, PR MACH LEARN RES, V70; Devlin Jacob, 2017, ADV NEURAL INFORM PR, P2080; Edwards Ashley D, 2018, ARXIV180310227; Ellis K, 2018, ADV NEUR IN, V31; Evans R, 2018, J ARTIF INTELL RES, V61, P1; Fox R., 2018, INT C LEARN REPR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kaiser L., 2015, COMPUTER SCI; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Laterre A, 2018, RANKED REWARD ENABLI; Levy A., 2018, ARXIV180508180; Li Chengtao, 2017, INT C LEARN REPR; Nachum O., 2018, ARXIV181001257, P1; Nachum O, 2018, ADV NEUR IN, V31; Nye Maxwell, 2019, ARXIV190206349; Osa Takayuki, 2019, INT C LEARN REPR; Reed S., 2016, INT C LEARN REPR; Shin R., 2018, PROC 32 INT C NEURAL, P8931; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sun S.-H., 2018, INT C MACH LEARN ICM, P4797; Verma A, 2018, PR MACH LEARN RES, V80; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Vien Ngo Anh, 2015, NAT C ART INT AAAI; Xiao Da, 2018, ARXIV180202696; Xu DD, 2018, INT J DISAST RISK SC, V9, P167, DOI 10.1007/s13753-018-0170-0; Zintgraf L, 2019, PR MACH LEARN RES, V97	39	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906034
C	Seo, PH; Kim, G; Han, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Seo, Paul Hongsuck; Kim, Geeho; Han, Bohyung			Combinatorial Inference against Label Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Label noise is one of the critical sources that degrade generalization performance of deep neural networks significantly. To handle the label noise issue in a principled way, we propose a unique classification framework of constructing multiple models in heterogeneous coarse-grained meta-class spaces and making joint inference of the trained models for the final predictions in the original (base) class space. Our approach reduces noise level by simply constructing meta-classes and improves accuracy via combinatorial inferences over multiple constituent classifiers. Since the proposed framework has distinct and complementary properties for the given problem, we can even incorporate additional off-the-shelf learning algorithms to improve accuracy further. We also introduce techniques to organize multiple heterogeneous meta-class sets using k-means clustering and identify a desirable subset leading to learn compact models. Our extensive experiments demonstrate outstanding performance in terms of accuracy and efficiency compared to the state-of-the-art methods under various synthetic noise configurations and in a real-world noisy dataset.	[Seo, Paul Hongsuck] POSTECH, Comp Vis Lab, Pohang, South Korea; [Seo, Paul Hongsuck; Kim, Geeho; Han, Bohyung] Seoul Natl Univ, Comp Vis Lab, Seoul, South Korea; [Seo, Paul Hongsuck; Kim, Geeho; Han, Bohyung] Seoul Natl Univ, ASRI, Seoul, South Korea	Pohang University of Science & Technology (POSTECH); Seoul National University (SNU); Seoul National University (SNU)	Seo, PH (corresponding author), POSTECH, Comp Vis Lab, Pohang, South Korea.; Seo, PH (corresponding author), Seoul Natl Univ, Comp Vis Lab, Seoul, South Korea.; Seo, PH (corresponding author), Seoul Natl Univ, ASRI, Seoul, South Korea.	hsseo@postech.ac.kr; snow1234@snu.ac.kr; bhhan@snu.ac.kr			Google AI Focused Research Award; Korean ICT RAMP;D programs of the MSIP/IITP grant [2017-0-01778, 2017-0-01780]	Google AI Focused Research Award(Google Incorporated); Korean ICT RAMP;D programs of the MSIP/IITP grant	This work is partly supported by Google AI Focused Research Award and Korean ICT R&D programs of the MSIP/IITP grant [2017-0-01778, 2017-0-01780].	Azadi Samaneh, 2015, ARXIV151107069; Blum A, 2003, J ACM, V50, P506, DOI 10.1145/792538.792543; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Domingos P., 2000, ICML; Freund Y, 1996, ICML; Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240; Ghosh Aritra, 2017, AAAI; Gold JR, 2017, PLAN HIST ENVIRON SE, P1; Han B, 2018, ADVANCES IN NEURAL I; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; Hendrycks Dan, 2018, ADV NEURAL INFORM PR, P10456; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jenni S, 2018, LECT NOTES COMPUT SC, V11214, P632, DOI 10.1007/978-3-030-01249-6_38; Jiang L., 2018, ICML; Jindal I., 2016, ICDM, DOI [10.1109/ICDM.2016.0121, DOI 10.1109/ICDM.2016.0121]; Li W, 2017, IEEE INT CONF AUTOMA, P103, DOI 10.1109/FG.2017.136; Li YH, 2017, IEEE I CONF COMP VIS, P2098, DOI 10.1109/ICCV.2017.229; Northcutt CG, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Opitz D., 1999, J ARTIF INTELLIG RES, V11, P169, DOI [DOI 10.1613/JAIR.614, 10.1613/jair.614]; Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240; Reed Scott, 2014, ARXIV14126596; Rokach L, 2010, ARTIF INTELL REV, V33, P1, DOI 10.1007/s10462-009-9124-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Seo PH, 2018, LECT NOTES COMPUT SC, V11214, P544, DOI 10.1007/978-3-030-01249-6_33; Sukhbaatar Sainbayar, 2014, ICLR; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696; Wang YL, 2018, PROC CVPR IEEE, P8906, DOI 10.1109/CVPR.2018.00928; Welinder P., 2010, CNSTR2010001 CALTECH; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xuan H, 2018, ECCV; Yan Y, 2014, MACH LEARN, V95, P291, DOI 10.1007/s10994-013-5412-1; Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14; Yu XR, 2019, PR MACH LEARN RES, V97; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhang ZL, 2018, ADV NEUR IN, V31	41	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301020
C	Sharma, P; Pathak, D; Gupta, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sharma, Pratyusha; Pathak, Deepak; Gupta, Abhinav			Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study a generalized setup for learning from demonstration to build an agent that can manipulate novel objects in unseen scenarios by looking at only a single video of human demonstration from a third-person perspective. To accomplish this goal, our agent should not only learn to understand the intent of the demonstrated third-person video in its context but also perform the intended task in its environment configuration. Our central insight is to enforce this structure explicitly during learning by decoupling what to achieve (intended task) from how to perform it (controller). We propose a hierarchical setup where a high-level module learns to generate a series of first-person sub-goals conditioned on the third-person video demonstration, and a low-level controller predicts the actions to achieve those sub-goals. Our agent acts from raw image observations without any access to the full state information. We show results on a real robotic platform using Baxter for the manipulation tasks of pouring and placing objects in a box. Project video is available at https://pathak22.github.io/hierarchical-imitation/.	[Sharma, Pratyusha] MIT, Cambridge, MA 02139 USA; [Pathak, Deepak] Facebook AI Res, New York, NY USA; [Gupta, Abhinav] CMU, Pittsburgh, PA USA; Univ Calif Berkeley, Berkeley, CA USA	Massachusetts Institute of Technology (MIT); Facebook Inc; Carnegie Mellon University; University of California System; University of California Berkeley	Sharma, P (corresponding author), MIT, Cambridge, MA 02139 USA.	pratyuss@csail.mit.edu			ONR MURI [N000141612007]; ONR Young Investigator Award; Facebook graduate fellowship	ONR MURI(MURIOffice of Naval Research); ONR Young Investigator Award; Facebook graduate fellowship(Facebook Inc)	We would like to thank David Held, Aayush Bansal, members of the CMU visual learning lab and Berkeley AI Research lab for fruitful discussions. The work was carried out when PS was at CMU and DP was at UC Berkeley. This work was supported by ONR MURI N000141612007 and ONR Young Investigator Award to AG. DP is supported by the Facebook graduate fellowship.	Abbeel P., 2004, ICML; Akgun B., 2012, HRI; [Anonymous], 2017, ICCV; Argall B. D., 2009, RAS; Carreira J., 2017, CVPR; Dosovitskiy A., 2017, CORL; Ebert F., 2017, CORR; Ebert Frederik, 2017, P P MACHINE LEARNING; Finn Chelsea, 2016, NEURIPS WORKSH ADV T; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Isola P., 2017, CVPR; Kundu D, 2018, HEALTH ECON REV, V8, DOI 10.1186/s13561-018-0187-5; Mirza M., 2014, ARXIV PREPRINT ARXIV; Mulling K., 2013, INT J ROB RES; OpenAI, 2018, CORR; Pathak D., 2018, ICLR; Pathak D., 2016, CVPR; Pomerleau D. A., 1989, NIPS; Rhinehart N., 2017, ICCV; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sadeghi F., 2016, CORR; SCHAAL S, 1999, TRENDS COGNITIVE SCI; Sermanet P., 2018, ICRA; Sermanet P., 2017, RSS; Tzeng E., 2014, CORR; Watter M., 2015, CORR; Yu T., 2018, ARXIV180201557; Zhang Ting, 2017, CORR; Zhou T., 2016, P CVPR	31	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302058
C	Shen, ZY; Vialard, FX; Niethammer, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shen, Zhengyang; Vialard, Francois-Xavier; Niethammer, Marc			Region-specific Diffeomorphic Metric Mapping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMAGE REGISTRATION; FLOWS	We introduce a region-specific diffeomorphic metric mapping (RDMM) registration approach. RDMM is non-parametric, estimating spatio-temporal velocity fields which parameterize the sought-for spatial transformation. Regularization of these velocity fields is necessary. In contrast to existing non-parametric registration approaches using a fixed spatially-invariant regularization, for example, the large displacement diffeomorphic metric mapping (LDDMM) model, our approach allows for spatially-varying regularization which is advected via the estimated spatio-temporal velocity field. Hence, not only can our model capture large displacements, it does so with a spatio-temporal regularizer that keeps track of how regions deform, which is a more natural mathematical formulation. We explore a family of RDMM registration approaches: 1) a registration model where regions with separate regularizations are pre-defined (e.g., in an atlas space or for distinct foreground and background regions), 2) a registration model where a general spatially-varying regularizer is estimated, and 3) a registration model where the spatially-varying regularizer is obtained via an end-to-end trained deep learning (DL) model. We provide a variational derivation of RDMM, showing that the model can assure diffeomorphic transformations in the continuum, and that LDDMM is a particular instance of RDMM. To evaluate RDMM performance we experiment 1) on synthetic 2D data and 2) on two 3D datasets: knee magnetic resonance images (MRIs) of the Osteoarthritis Initiative (OAI) and computed tomography images (CT) of the lung. Results show that our framework achieves comparable performance to state-of-the-art image registration approaches, while providing additional information via a learned spatio-temporal regularizer. Further, our deep learning approach allows for very fast RDMM and LDDMM estimations. Code is available at https://github.com/uncbiag/registration.	[Shen, Zhengyang; Niethammer, Marc] Univ N Carolina, Chapel Hill, NC 27515 USA; [Vialard, Francois-Xavier] UPEM, LIGM, Champs Sur Marne, France	University of North Carolina; University of North Carolina Chapel Hill; Ecole des Ponts ParisTech; Universite Gustave-Eiffel; ESIEE Paris	Shen, ZY (corresponding author), Univ N Carolina, Chapel Hill, NC 27515 USA.	zyshen@cs.unc.edu; francois-xavier.vialard@u-pem.fr; mn@cs.unc.edu			National Institutes of Health (NIH) [NSF EECS-1711776, NIH 1R01AR072013]; National Science Foundation (NSF) [NSF EECS-1711776, NIH 1R01AR072013]	National Institutes of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation (NSF)(National Science Foundation (NSF))	Research reported in this work was supported by the National Institutes of Health (NIH) and the National Science Foundation (NSF) under award numbers NSF EECS-1711776 and NIH 1R01AR072013. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH or the NSF. We would also like to thank Dr. Raul San Jose Estepar for providing the lung data.	[Anonymous], 2014, ICLR; Avants BB, 2008, MED IMAGE ANAL, V12, P26, DOI 10.1016/j.media.2007.06.004; Avants BB., 2009, INSIGHT J, V2, P1, DOI [DOI 10.54294/UVNHIN, 10.54294/uvnhin]; BAJCSY R, 1989, COMPUT VISION GRAPH, V46, P1, DOI 10.1016/S0734-189X(89)80014-3; Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Cao XH, 2018, IEEE T BIO-MED ENG, V65, P1900, DOI 10.1109/TBME.2018.2822826; Chen R. T., 2018, ADV NEURAL INFORM PR, P6571; Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316; Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49; Dalca Adrian V., 2018, MICCAI; de Vos BD, 2017, LECT NOTES COMPUT SC, V10553, P204, DOI 10.1007/978-3-319-67558-9_24; Delon J, 2012, SIAM J DISCRETE MATH, V26, P801, DOI 10.1137/110823304; Dupuis P, 1998, Q APPL MATH, V56, P587, DOI 10.1090/qam/1632326; Haber E, 2007, INT J COMPUT VISION, V71, P361, DOI 10.1007/s11263-006-8984-4; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Li H., 2018, ISBI; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Modat M, 2014, J MED IMAGING, V1, DOI 10.1117/1.JMI.1.2.024003; Modat M, 2010, COMPUT METH PROG BIO, V98, P278, DOI 10.1016/j.cmpb.2009.09.002; Modersitzki J, 2004, NUMER MATH SCI COMP; Niethammer Marc, 2019, CVPR; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Ourselin S, 2001, IMAGE VISION COMPUT, V19, P25, DOI 10.1016/S0262-8856(00)00052-4; Pace DF, 2013, IEEE T MED IMAGING, V32, P2114, DOI 10.1109/TMI.2013.2274777; RISSER L, 2010, MICCAI, V6362, P610; Risser L, 2013, MED IMAGE ANAL, V17, P182, DOI 10.1016/j.media.2012.10.001; Roh M.-M., 2017, INT C MEDICAL IMAGE, P266, DOI DOI 10.1007/978-3-319-66182-7_31; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Schmah T, 2013, LECT NOTES COMPUT SC, V8149, P203, DOI 10.1007/978-3-642-40811-3_26; Shen DG, 2002, IEEE T MED IMAGING, V21, P1421, DOI 10.1109/TMI.2002.803111; Shen Z., 2019, CVPR; Simpson IJA, 2015, MED IMAGE ANAL, V26, P203, DOI 10.1016/j.media.2015.08.006; Singh N, 2013, I S BIOMED IMAGING, P1219; Stefanescu R, 2004, MED IMAGE ANAL, V8, P325, DOI 10.1016/j.media.2004.06.010; Vercauteren T, 2008, LECT NOTES COMPUT SC, V5241, P754, DOI 10.1007/978-3-540-85988-8_90; Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040; Vialard FX, 2014, LECT NOTES COMPUT SC, V8673, P227, DOI 10.1007/978-3-319-10404-1_29; Vialard FX, 2012, INT J COMPUT VISION, V97, P229, DOI 10.1007/s11263-011-0481-8; Wulff J, 2015, PROC CVPR IEEE, P120, DOI 10.1109/CVPR.2015.7298607; Yang X, 2017, NEUROIMAGE, V158, P378, DOI 10.1016/j.neuroimage.2017.07.008; Yang X, 2016, LECT NOTES COMPUT SC, V10008, P48, DOI 10.1007/978-3-319-46976-8_6; Younes L, 2009, NEUROIMAGE, V45, pS40, DOI 10.1016/j.neuroimage.2008.10.050	44	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID	36081637				2022-12-19	WOS:000534424301013
C	Nguyen, TH; Simsekli, U; Gurbuzbalaban, M; Richard, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Thanh Huy Nguyen; Simsekli, Umut; Gurbuzbalaban, Mert; Richard, Gael			First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SDES DRIVEN; DIFFERENTIAL-EQUATIONS; LEVY; UNIQUENESS	Stochastic gradient descent (SGD) has been widely used in machine learning due to its computational efficiency and favorable generalization properties. Recently, it has been empirically demonstrated that the gradient noise in several deep learning settings admits a non-Gaussian, heavy-tailed behavior. This suggests that the gradient noise can be modeled by using alpha-stable distributions, a family of heavy-tailed distributions that appear in the generalized central limit theorem. In this context, SGD can be viewed as a discretization of a stochastic differential equation (SDE) driven by a Levy motion, and the metastability results for this SDE can then be used for illuminating the behavior of SGD, especially in terms of 'preferring wide minima'. While this approach brings a new perspective for analyzing SGD, it is limited in the sense that, due to the time discretization, SGD might admit a significantly different behavior than its continuous-time limit. Intuitively, the behaviors of these two systems are expected to be similar to each other only when the discretization step is sufficiently small; however, to the best of our knowledge, there is no theoretical understanding on how small the step-size should be chosen in order to guarantee that the discretized system inherits the properties of the continuous-time system. In this study, we provide formal theoretical analysis where we derive explicit conditions for the step-size such that the metastability behavior of the discrete-time system is similar to its continuous-time limit. We show that the behaviors of the two systems are indeed similar for small step-sizes and we identify how the error depends on the algorithm and problem parameters. We illustrate our results with simulations on a synthetic model and neural networks.	[Thanh Huy Nguyen; Simsekli, Umut; Richard, Gael] Telecom Paris, Inst Polytech Paris, LTCI, Paris, France; [Simsekli, Umut] Univ Oxford, Dept Stat, Oxford, England; [Gurbuzbalaban, Mert] Rutgers Business Sch, Dept Management Sci & Informat Syst, New Brunswick, NJ USA	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; University of Oxford; Rutgers State University New Brunswick	Nguyen, TH (corresponding author), Telecom Paris, Inst Polytech Paris, LTCI, Paris, France.				French National Research Agency (ANR) [ANR-16-CE23-0014]; industrial chair Data science & Artificial Intelligence from Telecom Paris; NSF [CCF-1814888, DMS-1723085]	French National Research Agency (ANR)(French National Research Agency (ANR)); industrial chair Data science & Artificial Intelligence from Telecom Paris; NSF(National Science Foundation (NSF))	We are grateful to Peter Tankov for providing us the derivations for the Girsanov-like change of measures. This work is partly supported by the French National Research Agency (ANR) as a part of the FBIMATRIX (ANR-16-CE23-0014) project, and by the industrial chair Data science & Artificial Intelligence from Telecom Paris. Mert Gurbuzbalaban acknowledges support from the grants NSF DMS-1723085 and NSF CCF-1814888.	Bayraktar E, 2015, ANN APPL PROBAB, V25, P3251, DOI 10.1214/14-AAP1073; BERGLUND N., 2011, ARXIV11065799; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bottou Leon, 2012, NEURAL NETWORKS TRIC, P421, DOI [10.1007/978-3-642-35289-8_25, DOI 10.1007/978-3-642-35289-8_25]; Burghoff T, 2015, J STAT PHYS, V161, P171, DOI 10.1007/s10955-015-1313-y; Chaudhari P, 2016, ARXIV161101838; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Debussche A, 2013, J FUNCT ANAL, V264, P1757, DOI 10.1016/j.jfa.2013.01.009; Duan J., 2015, INTRO STOCHASTIC DYN, V51; Erdogdu MA., 2018, ADV NEURAL INFORM PR, Vvol 31, P9671; Gao Xuefeng, 2018, ARXIV E PRINTS; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hu Wenqing, 2017, ARXIV170507562; Imkeller P, 2006, STOCH PROC APPL, V116, P611, DOI 10.1016/j.spa.2005.11.006; Imkeller P, 2010, J STAT PHYS, V141, P94, DOI 10.1007/s10955-010-0041-6; Jastrzebski Stanislaw, 2017, ARXIV171104623; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Kulik AM, 2019, STOCH PROC APPL, V129, P473, DOI 10.1016/j.spa.2018.03.010; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Liang Mingjie, 2018, ARXIV180105936; Lindvall T., 2002, LECT COUPLING METHOD; Liutkus A, 2019, PR MACH LEARN RES, V97; Mandt S, 2016, PR MACH LEARN RES, V48; Mikulevicius R, 2018, STOCHASTICS, V90, P569, DOI 10.1080/17442508.2017.1381095; Oksendal B.K., 2005, APPL STOCHASTIC CONT, V498; Papyan V., 2018, ARXIV181107062; Pavlyukevich I, 2011, STOCH DYNAM, V11, P495, DOI 10.1142/S0219493711003413; Priola E, 2012, OSAKA J MATH, V49, P421; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Sagun L, 2016, EIGENVALUES HESSIAN; Simsekli U, 2018, PR MACH LEARN RES, V80; Simsekli U, 2019, PR MACH LEARN RES, V97; Simsekli U, 2017, PR MACH LEARN RES, V70; Tankov P., 2003, FINANCIAL MODELLING; Nguyen TH, 2019, PR MACH LEARN RES, V97; Tzen B., 2018, P 2018 C LEARN THEOR, P857; Xu P., 2018, ADV NEURAL INFORM PR, V31, P3122; Yaida S., 2019, INT C LEARN REPR; Zhu Z., 2018, ARXIV180300195	45	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300025
C	Tian, JJ; Ramdas, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tian, Jinjin; Ramdas, Aaditya			ADDIS: an adaptive discarding algorithm for online FDR control with conservative nulls	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FALSE DISCOVERY RATE	Major internet companies routinely perform tens of thousands of A/B tests each year. Such large-scale sequential experimentation has resulted in a recent spurt of new algorithms that can provably control the false discovery rate (FDR) in a fully online fashion. However, current state-of-the-art adaptive algorithms can suffer from a significant loss in power if null p-values are conservative (stochastically larger than the uniform distribution), a situation that occurs frequently in practice. In this work, we introduce a new adaptive discarding method called ADDIS that provably controls the FDR and achieves the best of both worlds: it enjoys appreciable power increase over all existing methods if nulls are conservative (the practical case), and rarely loses power if nulls are exactly uniformly distributed (the ideal case). We provide several practical insights on robust choices of tuning parameters, and extend the idea to asynchronous and offline settings as well.	[Tian, Jinjin; Ramdas, Aaditya] Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Tian, JJ (corresponding author), Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA.	jinjint@andrew.cmu.edu; aramdas@cmu.edu						Aharoni E, 2014, J R STAT SOC B, V76, P771, DOI 10.1111/rssb.12048; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Ellis Jules L, 2017, ARXIV180100141; Foster DP, 2008, J R STAT SOC B, V70, P429, DOI 10.1111/j.1467-9868.2007.00643.x; Javanmard A., 2015, ARXIV150206197; Javanmard A, 2018, ANN STAT, V46, P526, DOI 10.1214/17-AOS1559; Ramdas Aaditya, 2017, ADV NEURAL INFORM PR, P5655; Ramdas Aaditya, 2018, P 35 INT C MACH LEAR, P4286; Ramdas AK, 2019, ANN STAT, V47, P2790, DOI 10.1214/18-AOS1765; Storey JD, 2004, J R STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x; Storey JD, 2002, J ROY STAT SOC B, V64, P479, DOI 10.1111/1467-9868.00346; Zhao QW, 2018, J ADDICT DIS, V37, P64, DOI 10.1080/10550887.2018.1545555; Zrnic T., 2018, ARXIV181205068	13	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901006
C	Wang, CC; Wang, YZ; Wang, YX; Wu, CT; Yu, GQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Congchao; Wang, Yizhi; Wang, Yinxue; Wu, Chiung-Ting; Yu, Guoqiang			muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Min-cost flow has been a widely used paradigm for solving data association problems in multi-object tracking (MOT). However, most existing methods of solving min-cost flow problems in MOT are either direct adoption or slight modifications of generic min-cost flow algorithms, yielding sub-optimal computation efficiency and holding the applications back from larger scale of problems. In this paper, by exploiting the special structures and properties of the graphs formulated in MOT problems, we develop an efficient min-cost flow algorithm, namely, minimum-update Successive Shortest Path (muSSP). muSSP is proved to provide exact optimal solution and we demonstrated its efficiency through 40 experiments on five MOT datasets with various object detection results and a number of graph designs. muSSP is always the most efficient in each experiment compared to the three peer solvers, improving the efficiency by 5 to 337 folds relative to the best competing algorithm and averagely 109 to 4089 folds to each of the three peer methods.	[Wang, Congchao; Wang, Yizhi; Wang, Yinxue; Wu, Chiung-Ting; Yu, Guoqiang] Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA 24061 USA	Virginia Polytechnic Institute & State University	Yu, GQ (corresponding author), Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.	ccwang@vt.edu; yzwang@vt.edu; yxwang90@vt.edu; ctwu@vt.edu; yug@vt.edu						Ahuja R., 2008, NETWORK FLOWS THEORY; Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21; Chenouard N, 2014, NAT METHODS, V11, P281, DOI 10.1038/nmeth.2808; Dendorfer P., 2019, CVPR19 TRACKING DETE; Emami P., 2018, ARXIV PREPRINT ARXIV; Ess A., 2008, IEEE C COMP VIS PATT; Frigioni D, 2000, J ALGORITHMS, V34, P251, DOI 10.1006/jagm.1999.1048; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Goldberg AV, 1997, J ALGORITHM, V22, P1, DOI 10.1006/jagm.1995.0805; Henschel Roberto, 2018, IEEE C COMP VIS PATT; Lenz P, 2015, IEEE I CONF COMP VIS, P4364, DOI 10.1109/ICCV.2015.496; Ramalingam G, 1996, J ALGORITHM, V21, P267, DOI 10.1006/jagm.1996.0046; Roditty L, 2004, LECT NOTES COMPUT SC, V3221, P580; Sharma S, 2018, IEEE INT CONF ROBOT, P3508; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xing J., 2014, ARXIV PREPRINT ARXIV; Zhang L, 2008, INT C WAVEL ANAL PAT, P11, DOI 10.1109/ICWAPR.2008.4635742	21	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300039
C	Wang, JJ; Sun, S; Yu, YL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Jingjing; Sun, Sun; Yu, Yaoliang			Multivariate Triangular Quantile Maps for Novelty Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ANOMALY DETECTION; SUPPORT; CLASSIFICATION	Novelty detection, a fundamental task in machine learning, has drawn a lot of recent attention due to its wide-ranging applications and the rise of neural approaches. In this work, we present a general framework for neural novelty detection that centers around a multivariate extension of the univariate quantile function. Our framework unifies and extends many classical and recent novelty detection algorithms, and opens the way to exploit recent advances in flow-based neural density estimation. We adapt the multiple gradient descent algorithm to obtain the first efficient end-to-end implementation of our framework that is free of tuning hyperparameters. Extensive experiments over a number of real datasets confirm the efficacy of our proposed method against state-of-the-art alternatives.	[Wang, Jingjing; Yu, Yaoliang] Univ Waterloo, Waterloo, ON, Canada; [Sun, Sun] Natl Res Council Canada, Ottawa, ON, Canada	University of Waterloo; National Research Council Canada	Wang, JJ (corresponding author), Univ Waterloo, Waterloo, ON, Canada.	jingjing.wang@uwaterloo.ca; sun.sun@uwaterloo.ca; yaoliang.yu@uwaterloo.ca	wang, jing/GRS-7509-2022; WANG, JINGYI/GSJ-1241-2022		NSERC	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	We thank the reviewers for their constructive comments. We thank Priyank Jaini for bringing Decurninge's work to our attention. This work is supported by NSERC.	Abati Davide, 2019, IEEE CVF C COMP VIS; Arjas E., 1978, Mathematics of Operations Research, V3, P205, DOI 10.1287/moor.3.3.205; Barto A, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00907; BenDavid S, 1997, J COMPUT SYST SCI, V55, P171, DOI 10.1006/jcss.1997.1507; Bogachev VI, 2005, SB MATH+, V196, P309, DOI 10.1070/SM2005v196n03ABEH000882; Chalapathy R., 2018, ARXIV PREPRINT ARXIV; Chernozhukov V, 2017, ANN STAT, V45, P223, DOI 10.1214/16-AOS1450; Dasgupta Sanjoy, 2018, P NATL ACAD SCI; Decurninge Alexis, 2015, THESIS; Deecke L, 2019, LECT NOTES ARTIF INT, V11051, P3, DOI 10.1007/978-3-030-10925-7_1; Desideri JA, 2012, CR MATH, V350, P313, DOI 10.1016/j.crma.2012.03.014; Ekeland I, 2012, MATH FINANC, V22, P109, DOI 10.1111/j.1467-9965.2010.00453.x; Fan W, 2004, KNOWL INF SYST, V6, P507, DOI 10.1007/s10115-003-0132-7; Fliege J, 2000, MATH METHOD OPER RES, V51, P479, DOI 10.1007/s001860000043; Galichon A, 2012, J ECON THEORY, V147, P1501, DOI 10.1016/j.jet.2011.06.002; Gardner AB, 2006, J MACH LEARN RES, V7, P1025; Golan I, 2018, ADV NEUR IN, V31; Hayton P, 2001, ADV NEUR IN, V13, P946; Huang CW, 2018, PR MACH LEARN RES, V80; Inouye DI, 2018, PR MACH LEARN RES, V80; Jaini P, 2019, PR MACH LEARN RES, V97; Kingma D.P, P 3 INT C LEARNING R; Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111; Liang Shiyu, 2018, INT C LEARN REPR; Manevitz L, 2007, NEUROCOMPUTING, V70, P1466, DOI 10.1016/j.neucom.2006.05.013; Manevitz LM, 2002, J MACH LEARN RES, V2, P139, DOI 10.1162/15324430260185574; Markou M, 2003, SIGNAL PROCESS, V83, P2499, DOI 10.1016/j.sigpro.2003.07.019; Markou M, 2003, SIGNAL PROCESS, V83, P2481, DOI 10.1016/j.sigpro.2003.07.018; MARZOUK Y., 2016, HDB UNCERTAINTY QUAN, P1; Menon AK, 2018, ADV NEUR IN, V31; Moya MM, 1996, NEURAL NETWORKS, V9, P463, DOI 10.1016/0893-6080(95)00120-4; MUKAI H, 1980, IEEE T AUTOMAT CONTR, V25, P177, DOI 10.1109/TAC.1980.1102298; OBRIEN GL, 1975, ANN PROBAB, V3, P80, DOI 10.1214/aop/1176996450; Papamakarios George, 2017, ARXIV170507057; Pathak D., 2017, ICML 17, P2778; Pidhorskyi S, 2018, ADV NEUR IN, V31; Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Ravanbakhsh M, 2019, IEEE WINT CONF APPL, P1896, DOI 10.1109/WACV.2019.00206; Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577; Ruff L, 2018, PR MACH LEARN RES, V80; Ruschendorf L., 1981, Mathematische Operationsforschung und Statistik, Series Statistics, V12, P327, DOI 10.1080/02331888108801593; Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356; Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006; Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Song Q., 2018, INT C LEARN REPR; Spantini A, 2018, J MACH LEARN RES, V19; Steinwart I, 2005, J MACH LEARN RES, V6, P211; Takeda Akiko, 2008, P 25 INT C MACH LEAR, P1056; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558; Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177; Xiao H., 2017, FASHION MNIST NOVEL; Zhai SF, 2016, PR MACH LEARN RES, V48	56	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305010
C	Wang, YX; Blei, DM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Yixin; Blei, David M.			Variational Bayes under Model Misspecification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VON-MISES THEOREM; ASYMPTOTIC NORMALITY; INFERENCE	Variational Bayes (VB) is a scalable alternative to Markov chain Monte Carlo (MCMC) for Bayesian posterior inference. Though popular, VB comes with few theoretical guarantees, most of which focus on well-specified models. However, models are rarely well-specified in practice. In this work, we study VB under model misspecification. We prove the VB posterior is asymptotically normal and centers at the value that minimizes the Kullback-Leibler (KL) divergence to the true data-generating distribution. Moreover, the VB posterior mean centers at the same value and is also asymptotically normal. These results generalize the variational Bernstein-von Mises theorem [31] to misspecified models. As a consequence of these results, we find that the model misspecification error dominates the variational approximation error in VB posterior predictive distributions. It explains the widely observed phenomenon that VB achieves comparable predictive accuracy with MCMC even though VB uses an approximating family. As illustrations, we study VB under three forms of model misspecification, ranging from model over-/under-dispersion to latent dimensionality misspecification. We conduct two simulation studies that demonstrate the theoretical results.	[Wang, Yixin; Blei, David M.] Columbia Univ, New York, NY 10027 USA	Columbia University	Wang, YX (corresponding author), Columbia Univ, New York, NY 10027 USA.		Wang, Yixin/ABF-4336-2021	Wang, Yixin/0000-0002-6617-4842	ONR [N00014-17-1-2131, N00014-15-1-2209]; NIH [1U01MH11572701]; NSF [CCF-1740833]; DARPA [SD2 FA8750-18-C-0130]; IBM; 2Sigma; Amazon; NVIDIA; Simons Foundation	ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); IBM(International Business Machines (IBM)); 2Sigma; Amazon; NVIDIA; Simons Foundation	We thank Victor Veitch and Jackson Loper for helpful comments on this article. This work is supported by ONR N00014-17-1-2131, ONR N00014-15-1-2209, NIH 1U01MH11572701, NSF CCF-1740833, DARPA SD2 FA8750-18-C-0130, IBM, 2Sigma, Amazon, NVIDIA, and Simons Foundation.	Alquier P., 2017, ARXIV170609293; Alquier P, 2016, J MACH LEARN RES, V17; [Anonymous], 2017, ARXIV171011268; Bickel P, 2013, ANN STAT, V41, P1922, DOI 10.1214/13-AOS1124; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Carpenter B., 2015, J STAT SOFTWARE; Cherief-Abdellatif BE, 2018, ELECTRON J STAT, V12, P2995, DOI 10.1214/18-EJS1475; Faes C, 2011, J AM STAT ASSOC, V106, P959, DOI 10.1198/jasa.2011.tm10301; Fan Z., 2018, ARXIV180807890; Freedman D, 1999, ANN STAT, V27, P1119; Ghorbani Behrooz, 2018, ARXIV180200568; Hall P, 2011, ANN STAT, V39, P2502, DOI 10.1214/11-AOS908; Hall P, 2011, STAT SINICA, V21, P369; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Jaiswal P., 2019, ARXIV190201902; Kleun BJK, 2006, ANN STAT, V34, P837, DOI 10.1214/009053606000000029; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; MCCULLAGH P, 1984, EUR J OPER RES, V16, P285, DOI 10.1016/0377-2217(84)90282-0; Ormerod JT, 2012, J COMPUT GRAPH STAT, V21, P2, DOI 10.1198/jcgs.2011.09118; Ormerod J. T., 2014, TECHNICAL REPORT; Pati Debdeep, 2017, ARXIV171208983; Sheth Rishit, 2019, PSEUDO BAYESIAN LEAR; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; WANG B, 2006, CONVERGENCE PROPERTI, V1, P625; Wang B., 2005, AISTATS; Wang B., 2004, P 20 C UNC ART INT B, p577 584; Wang Y, 2019, J LOW FREQ NOISE V A, V38, P1008, DOI 10.1177/1461348418795813; Westling Ted, 2015, ARXIV151008151; You C, 2014, AUST NZ J STAT, V56, P73, DOI 10.1111/anzs.12063; [No title captured]; [No title captured]; [No title captured]	36	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905007
C	Wu, LW; Li, SQ; Hsieh, CJ; Sharpnack, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Liwei; Li, Shuqing; Hsieh, Cho-Jui; Sharpnack, James			Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEURAL-NETWORKS; REGRESSION	In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastic shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.	[Wu, Liwei; Sharpnack, James] Univ Calif Davis, Dept Stat, Davis, CA 95616 USA; [Li, Shuqing] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA; [Hsieh, Cho-Jui] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Davis; University of California System; University of California Davis; University of California System; University of California Los Angeles	Wu, LW (corresponding author), Univ Calif Davis, Dept Stat, Davis, CA 95616 USA.	liwu@ucdavis.edu; qshli@ucdavis.edu; chohsieh@cs.ucla.edu; jsharpna@ucdavis.edu	WU, LIWEI/ABG-4582-2021		NSF [IIS-1719097]; Intel faculty award; Google Cloud; Nvidia	NSF(National Science Foundation (NSF)); Intel faculty award; Google Cloud(Google Incorporated); Nvidia	Hsieh acknowledges the support of NSF IIS-1719097, Intel faculty award, Google Cloud and Nvidia.	Bennett J., 2007, P KDD CUP WORKSH, P35, DOI DOI 10.1145/1345448.1345459; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Kang W.-C., 2018, ARXIV180809781; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lehmann J, 2015, SEMANT WEB, V6, P167, DOI 10.3233/SW-140134; Matt Post, 2018, WMT, DOI DOI 10.18653/V1/W18-6319; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Mnih A., 2007, ADV NEURAL INFORM PR, V20, P1257; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Rao N., 2015, PROC 28 INT C NEURAL, P2107, DOI DOI 10.5555/2969442.2969475; Rendle Steffen, 2009, UAI; Srebro N., 2005, P ADV NEURAL INFORM; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wu LW, 2018, PR MACH LEARN RES, V80; Wu LW, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P515, DOI 10.1145/3097983.3098071	32	5	5	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300003
C	Yang, JW; Ren, ZL; Gan, C; Zhu, HY; Parikh, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Jianwei; Ren, Zhile; Gan, Chuang; Zhu, Hongyuan; Parikh, Devi			Cross-channel Communication Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Convolutional neural networks (CNNs) process input data by feed-forwarding responses to subsequent layers. While a lot of progress has been made by making networks deeper, filters at each layer independently generate responses given the input and do not communicate with each other. In this paper, we introduce a novel network unit called Cross-channel Communication (C3) block, a simple yet effective module to encourage the communication across filters within the same layer. The C3 block enables filters to exchange information through a micro neural network, which consists of a feature encoder, a message passer, and a feature decoder, before sending the information to the next layer. With C3 block, each channel response is modulated by accounting for the responses at other channels. Extensive experiments on multiple vision tasks show that our proposed block brings improvements for different CNN architectures, and learns more diverse and complementary representations.	[Yang, Jianwei; Ren, Zhile; Parikh, Devi] Georgia Inst Technol, Atlanta, GA 30332 USA; [Parikh, Devi] Facebook AI Res, Menlo Pk, CA USA; [Gan, Chuang] MIT IBM Watson AI Lab, Cambridge, MA USA; [Zhu, Hongyuan] ASTAR, Inst Infocomm Res, Singapore, Singapore	University System of Georgia; Georgia Institute of Technology; Facebook Inc; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)	Yang, JW (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.		Ren, Zhile/AAD-4844-2019	Ren, Zhile/0000-0002-0302-795X				Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K, 2017, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.2017.322; Hinton, 2016, ARXIV PREPRINT ARXIV; Hu J., 2018, P IEEE C COMP VIS PA; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kipf Thomas N, 2017, INT C LEFV REPR ICLR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Velickovic Petar, 2018, INT C LEFV REPR ICLR; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Yang J., 2017, FASTER PYTORCH IMPLE; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319	30	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301030
C	Yeh, RA; Hu, YT; Schwing, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yeh, Raymond A.; Hu, Yuan-Ting; Schwing, Alexander G.			Chirality Nets for Human Pose Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose Chirality Nets, a family of deep nets that is equivariant to the "chirality transform," i.e., the transformation to create a chiral pair. Through parameter sharing, odd and even symmetry, we propose and prove variants of standard building blocks of deep nets that satisfy the equivariance property, including fully connected layers, convolutional layers, batch-normalization, and LSTM/GRU cells. The proposed layers lead to a more data efficient representation and a reduction in computation by exploiting symmetry. We evaluate chirality nets on the task of human pose regression, which naturally exploits the left/right mirroring of the human body. We study three pose regression tasks: 3D pose estimation from video, 2D pose forecasting, and skeleton based activity recognition. Our approach achieves/matches state-of-the-art results, with more significant gains on small datasets and limited-data settings.	[Yeh, Raymond A.; Hu, Yuan-Ting; Schwing, Alexander G.] Univ Illinois, Dept Elect Engn, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Yeh, RA (corresponding author), Univ Illinois, Dept Elect Engn, Champaign, IL 61820 USA.	yeh17@illinois.edu; ythu2@illinois.edu; aschwing@illinois.edu		Yeh, Raymond/0000-0003-4375-0680	NSF [1718221]; MRI [1725729]; UIUC; Samsung; 3M; Cisco Systems Inc. [CG 1377144]; Adobe; Google	NSF(National Science Foundation (NSF)); MRI; UIUC; Samsung(Samsung); 3M(3M); Cisco Systems Inc.; Adobe; Google(Google Incorporated)	This work is supported in part by NSF under Grant No. 1718221 and MRI #1725729, UIUC, Samsung, 3M, Cisco Systems Inc. (Gift Award CG 1377144) and Adobe. We thank NVIDIA for providing GPUs used for this work and Cisco for access to the Arcetri cluster. RY is supported by a Google PhD Fellowship.	[Anonymous], 1998, IMPL FIR FILT FLEX D; [Anonymous], 2017, IEEE DATA ENG B; Battaglia Peter W, 2018, ARXIV 180601261; Bengio Y., 1999, SHAPE CONTOUR GROUPI; Cao Z., 2018, ARXIV181208008CS; Chao Y.-W., 2017, P CVPR; Chen Yuhua, 2018, P CVPR; Chiu H.-k., 2019, P WACV; Cho Kyunghyun, 2014, P 2014 C EMP METH NA, P1724; Cohen T., 2016, P ICML; Cohen T. S., 2018, P ICLR; Dalal N., 2005, P CVPR; Fang H.-S., 2018, P AAAI; Gens R., 2014, P NEURIPS; Gilmer J., 2017, P ICML; Goyal V.K., 2014, FDN SIGNAL PROCESSIN; He K., 2017, P ICCV; Hochreiter S, 1997, NEURAL COMPUTATION; Hossain K, 2018, ROUTL EXPL ENV STUD, P3; Hu Y.-T., 2017, P NEURIPS; Hu Y.-T., 2018, P ECCV; Hussein Noureldien, 2019, P CVPR; Ioffe S., 2015, P ICML; Ionescu C., 2014, PAMI; Kim T. S., 2017, P CVPRW; Kingma Diederik P, 2015, ICLR 2015; Kipf T., 2018, P ICML; Kipf TN, 2017, P 5 INT C LEARN REPR; Lee K., 2018, P ECCV; Li C., 2018, P IJCAI; Liu I.-J., 2019, P CORL; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Luvizon D. C., 2018, P CVPR; Martinez J., 2017, P ICCV; Martinez J., 2017, P CVPR; Mikolajczyk K., 2004, IJCV; Minderer M., 2019, P NEURIPS; Pascanu R., 2014, P ICLR; Pavlakos G., P CVPR; Pavlakos Georgios, 2018, P CVPR; Pavllo D., 2019, P CVPR; Qi C. R., 2017, P CVPR; Ravanbakhsh S., 2017, P ICML; Reddi S., 2018, P ICLR; Scarselli F., 2009, IEEE T NEURAL NETW; Shahroudy Amir, 2016, P CVPR; Si C., 2018, P ECCV; Sigal L., 2010, IJCV; Srivastava N., 2014, JMLR; Sun X., 2017, ICCV; Tekin B., 2017, P ICCV; Tran D., 2018, P CVPR; Tuytelaars T., 2015, P CVPR; Waibel A., 1995, BACKPROPAGATION THEO; Williams R. J., 1989, NEURAL COMPUTATION; Worrall D. E., 2017, P CVPR; Yan S., 2018, P AAAI; Yang W., 2018, P CVPR; Yeh R. A., 2019, P CVPR; Yeh R. A., 2016, P ICASSP; Zaheer M., 2017, P NEURIPS; Zhang P., 2018, P ECCV; Zhang W., 2013, P ICCV	64	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308021
C	Yurochkin, M; Claici, S; Chien, E; Mirzazadeh, F; Solomon, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yurochkin, Mikhail; Claici, Sebastian; Chien, Edward; Mirzazadeh, Farzaneh; Solomon, Justin			Hierarchical Optimal Transport for Document Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The ability to measure similarity between documents enables intelligent summarization and analysis of large corpora. Past distances between documents suffer from either an inability to incorporate semantic similarities between words or from scalability issues. As an alternative, we introduce hierarchical optimal transport as a meta-distance between documents, where documents are modeled as distributions over topics, which themselves are modeled as distributions over words. We then solve an optimal transport problem on the smaller topic space to compute a similarity score. We give conditions on the topics under which this construction defines a distance, and we relate it to the word mover's distance. We evaluate our technique for k-NN classification and show better interpretability and scalability with comparable performance to current methods at a fraction of the cost.(1)	[Yurochkin, Mikhail; Mirzazadeh, Farzaneh] IBM Res, Yorktown Hts, NY 10598 USA; [Claici, Sebastian; Chien, Edward; Solomon, Justin] MIT, CSAIL, Cambridge, MA 02139 USA; [Yurochkin, Mikhail; Claici, Sebastian; Chien, Edward; Mirzazadeh, Farzaneh; Solomon, Justin] MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA	International Business Machines (IBM); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Yurochkin, M (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.; Yurochkin, M (corresponding author), MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA.	mikhail.yurochkin@ibm.com; sclaici@mit.edu; edchien@mit.edu; farzaneh@ibm.com; jsolomon@mit.edu			Army Research Office [W911NF1710068]; Air Force Office of Scientific Research [FA9550-19-1-031]; National Science Foundation [IIS-1838071]; Amazon Research Award; MIT-IBM Watson AI Laboratory; Toyota-CSAIL Joint Research Center; QCRI-CSAIL Computer Science Research Program	Army Research Office; Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Science Foundation(National Science Foundation (NSF)); Amazon Research Award; MIT-IBM Watson AI Laboratory(International Business Machines (IBM)); Toyota-CSAIL Joint Research Center; QCRI-CSAIL Computer Science Research Program	J. Solomon acknowledges the generous support of Army Research Office grant W911NF1710068, Air Force Office of Scientific Research award FA9550-19-1-031, of National Science Foundation grant IIS-1838071, from an Amazon Research Award, from the MIT-IBM Watson AI Laboratory, from the Toyota-CSAIL Joint Research Center, from the QCRI-CSAIL Computer Science Research Program, and from a gift from Adobe Systems. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of these organizations.	Arora Sanjeev, 2016, SIMPLE TOUGH BEAT BA; Atasu K, 2019, PR MACH LEARN RES, V97; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Bryant M, 2012, P ADV NEUR INF PROC, V25, P2699; Cachopo A.M.J.C., 2007, IMPROVING METHODS SI; CUTURI M., 2013, P INT C ADV NEURAL I, V26; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Frakes WB, 1992, INFORM RETRIEVAL DAT, V331; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Gurobi Optimization, 2022, GUROBI OPTIMIZER REF; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Huang G., 2016, PROC NEURAL INF PROC, P4869; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; LUHN HP, 1957, IBM J RES DEV, V1, P309, DOI 10.1147/rd.14.0309; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Newman D., 2010, HUMANLANGUAGETECHNOL, P100, DOI DOI 10.5555/1857999.1858011; Otto F, 2000, J FUNCT ANAL, V173, P361, DOI 10.1006/jfan.1999.3557; Pan MM, 2018, CONFERENCE PROCEEDINGS OF THE 6TH INTERNATIONAL SYMPOSIUM ON PROJECT MANAGEMENT (ISPM2018), P524; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Peyre G., 2018, COMPUTATIONAL UNPUB; Sanders N. J, 2011, SANDERS TWITTER SENT; Santambrogio F, 2015, PROG NONLINEAR DIFFE, V87, P1, DOI 10.1007/978-3-319-20828-2_1; Solomon J., 2018, OPTIMAL TRANSPORT DI; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wan XJ, 2007, INFORM SCIENCES, V177, P3718, DOI 10.1016/j.ins.2007.02.045; Wang L, 2011, ACTA POLYM SIN, P752, DOI 10.3724/SP.J.1105.2011.10220; Williamson S., 2010, ICML; Wu XH, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P1998; Xu HT, 2018, ADV NEUR IN, V31; Yurochkin M., 2017, ADV NEURAL INFORM PR, P3881; Yurochkin M., 2016, ADV NEURAL INFORM PR, P2505; Yurochkin M, 2019, PR MACH LEARN RES, V97	38	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301057
C	Zhang, XY; Zhang, KQ; Miehling, E; Basar, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Xiangyuan; Zhang, Kaiqing; Miehling, Erik; Basar, Tamer			Non-Cooperative Inverse Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GAMES; INFORMATION; DUALITY	Making decisions in the presence of a strategic opponent requires one to take into account the opponent's ability to actively mask its intended objective. To describe such strategic situations, we introduce the non-cooperative inverse reinforcement learning (N-CIRL) formalism. The N-CIRL formalism consists of two agents with completely misaligned objectives, where only one of the agents knows the true objective function. Formally, we model the N-CIRL formalism as a zero-sum Markov game with one-sided incomplete information. Through interacting with the more informed player, the less informed player attempts to both infer, and act according to, the true objective function. As a result of the one-sided incomplete information, the multi-stage game can be decomposed into a sequence of single-stage games expressed by a recursive formula. Solving this recursive formula yields the value of the N-CIRL game and the more informed player's equilibrium strategy. Another recursive formula, constructed by forming an auxiliary game, termed the dual game, yields the less informed player's strategy. Building upon these two recursive formulas, we develop a computationally tractable algorithm to approximately solve for the equilibrium strategies. Finally, we demonstrate the benefits of our N-CIRL formalism over the existing multi-agent IRL formalism via extensive numerical simulation in a novel cyber security setting.	[Zhang, Xiangyuan; Zhang, Kaiqing; Miehling, Erik; Basar, Tamer] Univ Illinois, Coordinated Sci Lab, 1101 W Springfield Ave, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Zhang, XY (corresponding author), Univ Illinois, Coordinated Sci Lab, 1101 W Springfield Ave, Urbana, IL 61801 USA.	xz7@illinois.edu; kzhang66@illinois.edu; miehling@illinois.edu; basar1@illinois.edu			US Army Research Laboratory (ARL) [W911NF-17-2-0196]; Office of Naval Research (ONR) MURI [N00014-16-1-2710]	US Army Research Laboratory (ARL)(United States Department of DefenseUS Army Research Laboratory (ARL)); Office of Naval Research (ONR) MURI(MURIOffice of Naval Research)	This work was supported in part by the US Army Research Laboratory (ARL) Cooperative Agreement W911NF-17-2-0196, and in part by the Office of Naval Research (ONR) MURI Grant N00014-16-1-2710.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Ammann P., 2002, ACM C COMP COMM SEC; [Anonymous], 2003, STOCHASTIC GAMES APP; Arnold T., 2017, AAAI C ART INT; Basar T., 2014, DYNAMIC GAMES EC DYN, V16, P23; Blackwell D., 1965, ANN MATH STAT, V36, P226, DOI DOI 10.1214/AOMS/1177700285; Cardaliaguet P, 2009, APPL MATH OPT, V59, P1, DOI 10.1007/s00245-008-9042-0; DeMeyer B, 1996, MATH OPER RES, V21, P237, DOI 10.1287/moor.21.1.237; Finn C, 2016, PR MACH LEARN RES, V48; Hadfield-Menell D., 2016, ADV NEURAL INFORM PR, P3916; Hansen E. A., 2004, AAAI C ART INT; Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678; Hendricks K, 2006, J ECON MANAGE STRAT, V15, P431, DOI 10.1111/j.1530-9134.2006.00106.x; Horak K, 2017, AAAI CONF ARTIF INTE, P558; Horner J, 2010, OPER RES, V58, P1107, DOI 10.1287/opre.1100.0829; Iannucci S., 2016, IEEE INT C AUT COMP; Ji Y., 2017, ACM SIGSAC C COMP CO; Joelle Pineau, 2003, IJCAI, P1025; Kalman R. E, 1964, J ENG MATER-T ASME, V86, P51, DOI [10.1115/1.3653115, DOI 10.1115/1.3653115]; Kuhn H., 1953, EXTENSIVE GAMES PROB, V2; Lanctot M, 2017, ADV NEUR IN, V30; Lin XM, 2019, J ARTIF INTELL RES, V66, P473, DOI 10.1613/jair.1.11541; Lin XM, 2018, IEEE T GAMES, V10, P56, DOI 10.1109/TCIAIG.2017.2679115; Littman ML, 1994, ICML 1994, P157; Malik D, 2018, PR MACH LEARN RES, V80; Marschak J., 1972, EC THEORY TEAMS; Mertens J.-F., 1985, International Journal of Game Theory, V14, P1, DOI 10.1007/BF01770224; Miehling E., 2015, 2 ACM WORKSH MOV TAR; Miehling E, 2018, IEEE T INF FOREN SEC, V13, P2490, DOI 10.1109/TIFS.2018.2819967; Nayyar A., 2012, IEEE C DEC CONTR; Nayyar A, 2014, IEEE T AUTOMAT CONTR, V59, P555, DOI 10.1109/TAC.2013.2283743; Nayyar A, 2013, IEEE T AUTOMAT CONTR, V58, P1644, DOI 10.1109/TAC.2013.2239000; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Renault J, 2006, MATH OPER RES, V31, P490, DOI 10.1287/moor.1060.0199; Rosenberg D, 2000, MATH OPER RES, V25, P23, DOI 10.1287/moor.25.1.23.15206; Rosenberg D, 1998, INT J GAME THEORY, V27, P577, DOI 10.1007/s001820050091; Russell SJ, 1995, ARTIF INTELL, V4th; Russell S, 2015, AI MAG, V36, P105, DOI 10.1609/aimag.v36i4.2577; Srinivasan S., 2018, ADV NEURAL INFORM PR, P3426; Wang Xiaojie, 2018, INT C MACH LEARN; WIENER N, 1960, SCIENCE, V131, P1355, DOI 10.1126/science.131.3410.1355; Zhang K., 2019, ARXIV190600729; Zhang K., 2018, ARXIV181202783; Zhang KQ, 2018, PR MACH LEARN RES, V80	45	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901015
C	Zhang, Y; Hare, J; Prugel-Bennett, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Yan; Hare, Jonathon; Prugel-Bennett, Adam			Deep Set Prediction Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Current approaches for predicting sets from feature vectors ignore the unordered nature of sets and suffer from discontinuity issues as a result. We propose a general model for predicting sets that properly respects the structure of sets and avoids this problem. With a single feature vector as input, we show that our model is able to auto-encode point sets, predict the set of bounding boxes of objects in an image, and predict the set of attributes of these objects.	[Zhang, Yan; Hare, Jonathon; Prugel-Bennett, Adam] Univ Southampton, Southampton, Hants, England	University of Southampton	Zhang, Y (corresponding author), Univ Southampton, Southampton, Hants, England.	yz5n12@ecs.soton.ac.uk; jsh2@ecs.soton.ac.uk; apb@ecs.soton.ac.uk		Zhang, Yan/0000-0003-3470-3663				Achlioptas P, 2018, PR MACH LEARN RES, V80; Balles L, 2019, HOLOGRAPHIC OTHER PO; Belanger D, 2017, PR MACH LEARN RES, V70; Belanger D, 2016, PR MACH LEARN RES, V48; Bojanowski P, 2018, PR MACH LEARN RES, V80; Cao N. D, 2018, ICML DEEP GEN MOD WO; Desta M. T, 2018, IEEE WINT C APPL COM; Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275; Greff K, 2019, PR MACH LEARN RES, V97; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Lazarow J, 2017, IEEE I CONF COMP VIS, P2793, DOI 10.1109/ICCV.2017.302; Li Chun-Liang, 2018, ARXIV181005795; Li Yujia, 2018, ARXIV180303324; Mena G, 2018, INT C LEARN REPR ICL; Mordatch I, 2018, ARXIV181102486; Paszke Adam, 2017, NEURIPS WORKSH AUT; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezatofighi S. H., 2018, ARXIV PREPRINT ARXIV; Santoro A, 2017, ADV NEUR IN, V30; Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41; Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255; Vaswani A, 2017, ADV NEUR IN, V30; Vinyals Oriol, 2015, ARXIV151106391; Yang C, 2019, ADV NEURAL INFORM PR, V32; Yang YZ, 2018, IEEE CONF COMPUT; You JX, 2018, PR MACH LEARN RES, V80; Zaheer Manzil, 2017, ADV NEURAL INFORM PR, V2, P8; Zhang Y, 2019, ARXIV190602795; Zhang Yulun, 2019, ARXIV190310082	31	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303023
C	Anderson, P; Gould, S; Johnson, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Anderson, Peter; Gould, Stephen; Johnson, Mark			Partially-Supervised Image Captioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild - for example, as assistants for people with impaired vision - a much larger number and variety of visual concepts must be understood. To address this problem, we teach image captioning models new visual concepts from labeled images and object detection datasets. Since image labels and object classes can be interpreted as partial captions, we formulate this problem as learning from partially specified sequence data. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores.	[Anderson, Peter; Johnson, Mark] Macquarie Univ, Sydney, NSW, Australia; [Gould, Stephen] Australian Natl Univ, Canberra, ACT, Australia; [Anderson, Peter] Georgia Tech, Atlanta, GA 30332 USA	Macquarie University; Australian National University; University System of Georgia; Georgia Institute of Technology	Anderson, P (corresponding author), Macquarie Univ, Sydney, NSW, Australia.; Anderson, P (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	p.anderson@mq.edu.au; stephen.gould@anu.edu.au; mark.johnson@mq.edu.au		Johnson, Mark/0000-0003-4809-8441; Anderson, Peter/0000-0002-6359-8586	Google award through the Natural Language Understanding Focused Program; Australian Research Council [DP160102156]; Data61/CSIRO [CRP 8201800363]	Google award through the Natural Language Understanding Focused Program(Google Incorporated); Australian Research Council(Australian Research Council); Data61/CSIRO	This research was supported by a Google award through the Natural Language Understanding Focused Program, CRP 8201800363 from Data61/CSIRO, and under the Australian Research Council's Discovery Projects funding scheme (project number DP160102156). We also thank the anonymous reviewers for their valuable comments that helped to improve the paper.	ANDERSON P, 2018, CVPR, V3, P6, DOI DOI 10.1109/CVPR.2018.00636; Anderson P., 2017, EMNLP; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Chen X, 2015, CORR, V1504, P325; Dai Andrew M., 2015, ADV NEURAL INFORM PR; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Devlin J., 2015, COMPUTER SCI; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fang Hao, 2015, CVPR; Ghahramani Z., 1994, P ADV NEUR INF PROC, P120; Graves Alex, 2013, ARXIV13080850 CORR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hendricks L. A., 2016, CVPR; Hill F, 2016, ARXIV160203483; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hokamp C., 2017, ACL; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Koehn Philipp, 2010, STAT MACHINE TRANSLA; Krasin I., 2017, OPENIMAGES PUBLIC DA; Krishna Ranjay, 2016, ARXIV160207332; Lavie Alon, 2007, P 2 WORKSH STAT MACH, P228, DOI DOI 10.3115/1626355.1626389; Levy O, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P302, DOI 10.3115/v1/p14-2050; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lipton Z, 2016, MACHINE LEARNING HEA, P6776; Lu J., 2018, CVPR; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; MacLeod Haley, 2017, P 2017 CHI C HUM FAC; Mao J, 2015, 3 INT C LEARN REPR I; McLachlan GJ, 2008, WILEY SER PROBAB ST, P365; Mikolov T., 2013, 9 INT C LEARN REPR, P1; Papadopoulos D. P., 2016, COMPUTER VISION PATT; Papadopoulos DP, 2017, IEEE I CONF COMP VIS, pCP38, DOI 10.1109/ICCV.2017.528; Parveen Shahla, 2002, NIPS; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Post Matt, 2018, ARXIV180406609; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Richardson Kyle, 2018, NAACL; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sipser M, 2013, CENGAGE LEARNING, V3rd; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Tran Kenneth, 2016, IEEE C COMP VIS PATT; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wiseman S, 2016, EMNLP; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z., 2016, ADV NEURAL INF PROCE, V29; Yao T., 2017, CVPR; Young Peter, 2014, T ASSOC COMPUT LING, V2, P67	55	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301083
C	Bajpai, A; Garg, S; Mausam		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bajpai, Aniket; Garg, Sankalp; Mausam			Transfer of Deep Reactive Policies for MDP Planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Domain-independent probabilistic planners input an MDP description in a factored representation language such as PPDDL or RDDL, and exploit the specifics of the representation for faster planning. Traditional algorithms operate on each problem instance independently, and good methods for transferring experience from policies of other instances of a domain to a new instance do not exist. Recently, researchers have begun exploring the use of deep reactive policies, trained via deep reinforcement learning (RL), for MDP planning domains. One advantage of deep reactive policies is that they are more amenable to transfer learning. In this paper, we present the first domain-independent transfer algorithm for MDP planning domains expressed in an RDDL representation. Our architecture exploits the symbolic state configuration and transition function of the domain (available via RDDL) to learn a shared embedding space for states and state-action pairs for all problem instances of a domain. We then learn an RL agent in the embedding space, making a near zero-shot transfer possible, i.e., without much training on the new instance, and without using the domain simulator at all. Experiments on three different benchmark domains underscore the value of our transfer algorithm. Compared against planning from scratch, and a state-of-the-art RL transfer algorithm, our transfer solution has significantly superior learning curves.	[Bajpai, Aniket; Garg, Sankalp; Mausam] Indian Inst Technol Delhi, New Delhi, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Bajpai, A (corresponding author), Indian Inst Technol Delhi, New Delhi, India.	quantum.computing96@gmail.com; sankalp2621998@gmail.com; mausam@cse.iitd.ac.in		Mausam, ./0000-0003-4088-4296	Google; Bloomberg award; IBM SUR award; 1MG award; Visvesvaraya faculty award by Govt. of India; Microsoft Azure	Google(Google Incorporated); Bloomberg award; IBM SUR award(International Business Machines (IBM)); 1MG award; Visvesvaraya faculty award by Govt. of India; Microsoft Azure(Microsoft)	We thank Ankit Anand and the anonymous reviewers for their insightful comments on an earlier draft of the paper. We also thank Alan Fern, Scott Sanner, Akshay Gupta and Arindam Bhattacharya for initial discussions on the research. This work is supported by research grants from Google, a Bloomberg award, an IBM SUR award, a 1MG award, and a Visvesvaraya faculty award by Govt. of India. We thank Microsoft Azure sponsorships, and the IIT Delhi HPC facility for computational resources.	Anand A, 2016, P I C AUTOMAT PLAN S, P29; Anand A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1509; Bellman R., 1957, INDIANA U MATH J; Clevert D.A., 2015, ABS151107289 CORR; Fern  Alan, 2018, ICAPS; Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10; Garnelo M., 2016, ABS160905518 CORR; Goyal Palash, 2017, ABS170502801 CORR; Groshev  Edward, 2018, ICAPS; Grzes M., 2014, ICAPS; Guestrin C., 2001, P 17 INT JOINT C ART, V17, P673; Higgins I, 2017, PR MACH LEARN RES, V70; Kipf Thomas N., 2017, INT C LEARNING REPRE; Kolobov A., 2012, UAI, P438; Littman M. L., 1997, P 14 NAT C ART INT, P748; Matiisen  Tambet, 2017, ABS170700183 CORR; Mausam, 2012, PLANNING MARKOV DECI; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Parisotto  Emilio, 2015, ABS151106342 CORR; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rajendran Janarthanan, 2017, ICLR; Ravindran B., 2004, THESIS; Sanner S., 2010, RELATIONAL DYNAMIC I; Sutton R. S., 1998, ADAPTIVE COMPUTATION; Tamar Aviv, 2017, P 26 INT JOINT C ART; Toyer  Sam, 2018, P 32 AAAI C ART INT; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Younes HLS, 2005, J ARTIF INTELL RES, V24, P851, DOI 10.1613/jair.1880	29	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005054
C	Bassily, R; Thakkar, O; Thakurta, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bassily, Raef; Thakkar, Om; Thakurta, Abhradeep			Model-Agnostic Private Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We design differentially private learning algorithms that are agnostic to the learning model assuming access to a limited amount of unlabeled public data. First, we provide a new differentially private algorithm for answering a sequence of m online classification queries (given by a sequence of m unlabeled public feature vectors) based on a private training set. Our algorithm follows the paradigm of subsample-and-aggregate, in which any generic non-private learner is trained on disjoint subsets of the private training set, and then for each classification query, the votes of the resulting classifiers ensemble are aggregated in a differentially private fashion. Our private aggregation is based on a novel combination of the distance-to-instability framework [26], and the sparse-vector technique [15, 18]. We show that our algorithm makes a conservative use of the privacy budget. In particular, if the underlying non-private learner yields a classification error of at most alpha is an element of (0, 1), then our construction answers more queries, by at least a factor of 1/alpha in some cases, than what is implied by a straightforward application of the advanced composition theorem for differential privacy. Next, we apply the knowledge transfer technique to construct a private learner that outputs a classifier, which can be used to answer an unlimited number of queries. In the PAC model, we analyze our construction and prove upper bounds on the sample complexity for both the realizable and the non-realizable cases Similar to non-private sample complexity, our bounds are completely characterized by the VC dimension of the concept class.	[Bassily, Raef] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA; [Thakkar, Om] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA; [Thakurta, Abhradeep] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA	University System of Ohio; Ohio State University; Boston University; University of California System; University of California Santa Cruz	Bassily, R (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.	bassily.1@osu.edu; omthkkr@bu.edu; aguhatha@ucsc.edu			NSF [TRIPODS-1740850, TRIPODS+X-1839317, IIS-1447700]; Sloan foundation; OSU; UC Santa Cruz	NSF(National Science Foundation (NSF)); Sloan foundation(Alfred P. Sloan Foundation); OSU; UC Santa Cruz	The authors would like to thank Vitaly Feldman, and Adam Smith for helpful discussions during the course of this project. In particular, the authors are grateful for Vitaly's ideas about the possible extensions of the results in Section 4, which we outlined in Remarks 2 and 3. This work is supported by NSF grants TRIPODS-1740850, TRIPODS+X-1839317, and IIS-1447700, a grant from the Sloan foundation, and start-up supports from OSU and UC Santa Cruz.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; [Anonymous], 1994, INTRO COMPUTATIONAL; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Bassily Raef, 2018, ARXIV180305101; Beimel A, 2016, THEOR COMPUT, V12, DOI 10.4086/toc.2016.v012a001; Beimel A, 2010, LECT NOTES COMPUT SC, V5978, P437, DOI 10.1007/978-3-642-11799-2_26; Beimel Amos, 2013, ITCS; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Chaudhuri Kamalika, 2011, JMLR Workshop Conf Proc, V2011, P155; Chaudhuri Kamalika, 2011, JMLR; Dwork C., 2018, P 31 C LEARN THEOR P, P1693; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2009, ACM S THEORY COMPUT, P381; Hamm J, 2016, PR MACH LEARN RES, V48; Hardt M., 2010, FOCS; Kasiviswanathan SP, 2008, ANN IEEE SYMP FOUND, P531, DOI 10.1109/FOCS.2008.27; Kifer D., 2012, J MACH LEARN RES WOR, V23; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Papernot Nicolas, 2017, P INT C LEARN REPR, P2; Papernot Nicolas, 2018, INT C LEARN REPR ICL, P2; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Talwar K., 2015, NIPS; Thakurta Abhradeep Guha, 2013, P 26 ANN C LEARN THE, P819	27	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001063
C	Ben-Porat, O; Tennenholtz, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ben-Porat, Omer; Tennenholtz, Moshe			A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a game-theoretic approach to the study of recommendation systems with strategic content providers. Such systems should be fair and stable. Showing that traditional approaches fail to satisfy these requirements, we propose the Shapley mediator. We show that the Shapley mediator fulfills the fairness and stability requirements, runs in linear time, and is the only economically efficient mechanism satisfying these properties.	[Ben-Porat, Omer; Tennenholtz, Moshe] Technion Israel Inst Technol, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Ben-Porat, O (corresponding author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.	omerbp@campus.technion.ac.il; moshe@ie.technion.ac.il			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [740435]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 740435).	Adamopoulos P, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P153, DOI 10.1145/2645710.2645752; Ben-Basat Ran, 2015, P ICTIR, P51; Ben-Porat  O., 2018, ACCEPTED MATH OPERAT; Ben-Porat O., 2017, P 31 INT C NEUR INF, P1498; Ben-Porat  O., 2018, ARXIV180600955; Bozdag E, 2013, ETHICS INF TECHNOL, V15, P209, DOI 10.1007/s10676-013-9321-6; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Burke R. D., 2016, UMAP P; Burke Robin, 2017, ARXIV170700093; Cary Matthew, 2014, ACM Transactions on Economics and Computation, V2, DOI 10.1145/2632226; Chierichetti F., 2017, NEURIPS, P5029; Cohen J., 2017, P 31 INT C NEUR INF; DENG XT, 1994, MATH OPER RES, V19, P257, DOI 10.1287/moor.19.2.257; Dubey P., 1975, International Journal of Game Theory, V4, P131, DOI 10.1007/BF01780630; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Garg V., 2016, ADV NEURAL INFORM PR, P1552; Ieong S., 2005, PROC 6 ACM C ELECT C, P193, DOI [10.1145/1064009.1064030, DOI 10.1145/1064009.1064030]; Kamiran Faisal, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P869, DOI 10.1109/ICDM.2010.50; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Kamishima T, 2014, 8 ACM C REC SYST RES; Kamishima  T., 2015, ICML2015 WORKSH FAIR, P51; Kamishima Toshihiro, 2012, P WORKSHOP HUMAN DEC, P8; Koutsoupias E, 1999, LECT NOTES COMPUT SC, V1563, P404; Luce R. D., 2005, INDIVIDUAL CHOICE BE; Modani N, 2017, LECT NOTES ARTIF INT, V10235, P144, DOI 10.1007/978-3-319-57529-2_12; Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044; Myerson R. B., 1977, Mathematics of Operations Research, V2, P225, DOI 10.1287/moor.2.3.225; Pariser E, 2011, FILTER BUBBLE WHAT I; Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; Pizzato L., 2010, P 4 ACM C REC SYST, P207, DOI DOI 10.1145/1864708.1864747; Pleiss G., 2017, ADV NEURAL INFORM PR, P5684; Raifer N, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P465, DOI 10.1145/3077136.3080785; Roughgarden T, 2009, ACM S THEORY COMPUT, P513; Shapley LS, 1952, TECHNICAL REPORT; Wooldridge M, 2011, LECT NOTES ARTIF INT, V6682, P1, DOI 10.1007/978-3-642-22000-5_1; Young H. P., 1985, International Journal of Game Theory, V14, P65, DOI 10.1007/BF01769885; Yu H., 2011, J INFORM COMPUTATION, V8, P4061; Zemel R., 2013, P INT C MACH LEARN, P325; Zheng Yong, 2017, ARXIV170708913	39	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301013
C	Bernstein, G; Sheldon, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bernstein, Garrett; Sheldon, Daniel			Differentially Private Bayesian Inference for Exponential Families	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods, it gives properly calibrated posterior beliefs in the non-asymptotic data regime.	[Bernstein, Garrett; Sheldon, Daniel] Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA	University of Massachusetts System; University of Massachusetts Amherst	Bernstein, G (corresponding author), Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA.	gbernstein@cs.umass.edu; sheldon@cs.umass.edu			National Science Foundation [1522054, 1617533]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant Nos. 1522054 and 1617533.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Bernstein G, 2017, PR MACH LEARN RES, V70; Bickel P.J., 2015, MATH STAT BASIC IDEA, VI; Bickel PJ, 2015, MATH STAT BASIC IDEA, VI (Vol 117); Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Cook SR, 2006, J COMPUT GRAPH STAT, V15, P675, DOI 10.1198/106186006X136976; DIACONIS P, 1979, ANN STAT, V7, P269, DOI 10.1214/aos/1176344611; Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21; Dwork C., 2014, FDN TRENDS THEORETIC; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Fisher R.A., 1922, PHILOS T R SOC LON A, V222, P309, DOI [DOI 10.1098/RSTA.1922.0009, 10.1098/rsta.1922.0009]; Foulds J., 2016, P 32 C UNC ART INT U, P192; Geumlek J., 2017, INT C NEUR INF PROC, P5295; Gretton A, 2012, J MACH LEARN RES, V13, P723; Jain P., 2013, P 13 INT C MACH LEAR, P118; Karwa V, 2016, ANN STAT, V44, P87, DOI 10.1214/15-AOS1358; Karwa V, 2014, LECT NOTES COMPUT SC, V8744, P143, DOI 10.1007/978-3-319-11257-2_12; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Kifer D., 2011, P 2011 ACM SIGMOD IN, P193, DOI DOI 10.1145/1989323.1989345; Kifer D., 2012, J MACHINE LEARNING R, V1, P3; MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Park T, 2008, J AM STAT ASSOC, V103, P681, DOI 10.1198/016214508000000337; Petersen Kaare Brandt, 2008, MATRIX COOKBOOK, V7, P510; ROBBINS H, 1948, B AM MATH SOC, V54, P1151, DOI 10.1090/S0002-9904-1948-09142-X; Rubinstein B.I.P., 2010, J PRIVACY CONFIDENTI, DOI [10.29012/jpc.v4i1.612, DOI 10.29012/JPC.V4I1.612]; Schein Aaron, 2018, NIPS 2017 WORKSH ADV; Smith A, 2011, ACM S THEORY COMPUT, P813; Wang YX, 2015, PR MACH LEARN RES, V37, P2493; Williams O., 2010, P 23 INT C NEURAL IN, P2451; Zhang Z., 2016, 30 AAAI C ART INT	33	5	5	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302090
C	Cheng, R; Wang, ZY; Fragkiadaki, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cheng, Ricson; Wang, Ziyan; Fragkiadaki, Katerina			Geometry-Aware Recurrent Neural Networks for Active Visual Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present recurrent geometry-aware neural networks that integrate visual information across multiple views of a scene into 3D latent feature tensors, while maintaining an one-to-one mapping between 3D physical locations in the world scene and latent feature locations. Object detection, object segmentation, and 3D reconstruction is then carried out directly using the constructed 3D feature memory, as opposed to any of the input 2D images. The proposed models are equipped with differentiable egomotion-aware feature warping and (learned) depth-aware unprojection operations to achieve geometrically consistent mapping between the features in the input frame and the constructed latent model of the scene. We empirically show the proposed model generalizes much better than geometry unaware LSTM/GRU networks, especially under the presence of multiple objects and cross-object occlusions. Combined with active view selection policies, our model learns to select informative viewpoints to integrate information from by "undoing" cross-object occlusions, seamlessly combining geometry with learning from experience.	[Cheng, Ricson; Wang, Ziyan; Fragkiadaki, Katerina] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Cheng, R (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ricsonc@andrew.cmu.edu; ziyanw1@andrew.cmu.edu; katef@cs.cmu.edu						Aloimonos J., 1988, IJCV; Ammirato P., 2017, ICRA; Bajcsy Ruzena, 1988, P IEEE; Caicedo J., 2015, ICCV; Chang Angel X., 2015, SHAPENET INFORM RICH; Cho K., 2014, CORR; Denzler J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P400; Godard C., 2016, CORR; Gonzalez-Garcia A, 2015, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2015.7298921; Gordon D., 2017, CORR; Gupta S., 2017, P IEEE C COMP VIS PA, P2616; Hadsell R., 2006, P CVPR; Harley A. W., 2015, CORR; Held R., 1963, J COMP PHYSL PSYCHOL; Henriques J. F., 2018, P IEEE C COMP VIS PA; Hoffman J., 2017, CORR; JAYARAMAN D, 2016, ECCV, V9909, P489, DOI DOI 10.1007/978-3-319-46454-1_30; Jayaraman Dinesh, 2017, CORR, V3; Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414; Kar A, 2017, CORR; Kerl C., 2013, IROS; Malmir M., 2015, BMVC; Mathe S., 2016, CVPR; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V., 2014, CORR; Parisotto E., 2017, CORR; Qi Charles Ruizhongtai, 2016, CORR; Ranzato M. A., 2014, ARXIV14055488; Ren S., 2015, CORR; Rivlin E., 2000, IJCV; Sch6ps T., 2014, ISMAR; Soatto S., 2009, ICCV; Song S., 2017, IEEE C COMP VIS PATT; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tatler BW, 2011, J VISION, V11, DOI 10.1167/11.5.5; Tulsiani Shubham, 2017, CORR; Tung H.Y.F., 2017, ICCV; Vijayanarasimhan Sudheendra, 2017, COMPUTER VISION PATT; Wilkes D., 1992, CVPR; Wong L. L., 2013, ROB SCI SYST RSS WOR; Wu J., 2017, ABS171106475 ARXIV, P1; Wu J., 2018, INT J COMPUTER VISIO; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Zhou T., 2017, UNSUPERVISED LEARNIN; Zhu Y., 2017, ICRA	45	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305012
C	Cheron, G; Alayrac, JB; Laptev, I; Schmid, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cheron, Guilhem; Alayrac, Jean-Baptiste; Laptev, Ivan; Schmid, Cordelia			A flexible model for training action localization with varying levels of supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame. Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less-demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos.	[Cheron, Guilhem; Alayrac, Jean-Baptiste; Laptev, Ivan] PSL Res Univ, CNRS, Ecole Normale Super, INRIA, F-75005 Paris, France; [Cheron, Guilhem; Schmid, Cordelia] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France	Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Cheron, G (corresponding author), PSL Res Univ, CNRS, Ecole Normale Super, INRIA, F-75005 Paris, France.; Cheron, G (corresponding author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.				ERC; MSR-Inria joint lab; Louis Vuitton ENS Chair on Artificial Intelligence; Amazon academic research award; Intel gift; DGA project DRAAF	ERC(European Research Council (ERC)European Commission); MSR-Inria joint lab; Louis Vuitton ENS Chair on Artificial Intelligence; Amazon academic research award; Intel gift; DGA project DRAAF	We thank Remi Leblond for his thoughtful comments and help with the paper. This work was supported in part by ERC grants ACTIVIA and ALLEGRO, the MSR-Inria joint lab, the Louis Vuitton ENS Chair on Artificial Intelligence, an Amazon academic research award, the Intel gift and the DGA project DRAAF.	Alayrac J.-B., 2016, CVPR; Arbelaez Pablo, 2014, CVPR; Bach F., 2007, NIPS; Bojanowski P., 2014, ECCV; Bojanowski P., 2013, ICCV; Carreira J., 2017, CVPR; Chen W., 2015, ICCV; Chunhui Gu, 2018, CVPR; Duchenne O., 2009, CVPR; Frank M., 1956, NAVAL RES LOGISTICS; Gkioxari G., 2015, CVPR; Gorban A., 2015, THUMOS CHALLENGE ACT; He Kaiming, 2016, CVPR; Huang D.-A., 2016, ECCV; Huang Rui, 2017, ICCV; Jiong Yang, 2017, ICCV; Joulin A., 2010, CVPR; Kalogeiton V., 2017, ICCV; Kay W., 2017, CORR; Lacoste-Julien S., 2013, INT C MACH LEARN PML, P53; Laptev I., 2007, ICCV; Limin Wang, 2017, CVPR; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Linli Xu, 2004, NIPS; Lucas B. D., 1981, IJCAI; Mettes P., 2017, BMVC; Mettes Pascal, 2016, ECCV; Miech A., 2017, ICCV; Oneata D., 2014, ECCV; Osokin A., 2016, ICML; Peng Xiaojiang, 2016, ECCV; Richard Alexander, 2017, P IEEE C COMP VIS PA, V2, P6; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Saha S., 2017, ICCV; Saha S., 2016, BMVC; Shaoqing R, 2015, NIPS; Singh Gurkirt, 2017, ICCV; Singh K. K., 2017, ICCV; Siva P., 2011, BMVC; Soomro K., 2012, ARXIVPREPRINT, P2556; Soomro Khurram, 2017, ICCV; Uijlings J., 2013, IJCV; van Gemert J., 2015, BMVC; Weinzaepfel P., 2015, ICCV; Weinzaepfel Philippe, 2016, CORR; Yan Ke, 2005, ICCV; Zolfaghari M., 2017, ICCV	47	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300087
C	Chien, I; Pan, C; Milenkovic, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chien, I. (Eli); Pan, Chao; Milenkovic, Olgica			Query K-means Clustering and the Double Dixie Cup Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHM	We consider the problem of approximate K-means clustering with outliers and side information provided by same-cluster queries and possibly noisy answers. Our solution shows that, under some mild assumptions on the smallest cluster size, one can obtain an (1 + epsilon)-approximation for the optimal potential with probability at least 1 - delta, where epsilon > 0 and (delta is an element of (0, 1), using an expected number of O(K-3/epsilon delta) noiseless same-cluster queries and comparison-based clustering of complexity O(ndK + K-3/epsilon delta); here, n denotes the number of points and d the dimension of space. Compared to a handful of other known approaches that perform importance sampling to account for small cluster sizes, the proposed query technique reduces the number of queries by a factor of roughly O(K-6/epsilon(3)), at the cost of possibly missing very small clusters. We extend this settings to the case where some queries to the oracle produce erroneous information, and where certain points, termed outliers, do not belong to any clusters. Our proof techniques differ from previous methods used for K-means clustering analysis, as they rely on estimating the sizes of the clusters and the number of points needed for accurate centroid estimation and subsequent nontrivial generalizations of the double Dixie cup problem. We illustrate the performance of the proposed algorithm both on synthetic and real datasets, including MNIST and CIFAR 10.	[Chien, I. (Eli); Pan, Chao; Milenkovic, Olgica] UIUC, Dept ECE, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Chien, I (corresponding author), UIUC, Dept ECE, Urbana, IL 61801 USA.	ichien3@illinois.edu; chaopan2@illinois.edu; milenkov@illinois.edu	Chien, Eli/AHA-3502-2022		grants 239 SBC Purdue STC Center for Science of Information [4101-38050]; NSF [CCF 15-27636]	grants 239 SBC Purdue STC Center for Science of Information; NSF(National Science Foundation (NSF))	This work was supported in part by the grants 239 SBC Purdue 4101-38050 STC Center for Science of Information and NSF CCF 15-27636.	Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15; Ailon Nir, 2017, ARXIV170401862; [Anonymous], 1999, P 4 INT C ADV PATTER; [Anonymous], 2013, ARXIV13115939; Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216; Awasthi P., 2015, P 31 INT S COMP GEOM; Bradley PS., 2000, RES REDMOND, P1; Dasarathy G., 2015, P C LEARN THEOR, P503; Doumas AV, 2016, ESAIM-PROBAB STAT, V20, P367, DOI 10.1051/ps/2016016; Gamlath B., 2018, ARXIV180300926; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Hui S, 2014, COMMUN STAT-THEOR M, V43, P4103, DOI 10.1080/03610926.2012.705941; Inaba M., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P332, DOI 10.1145/177424.178042; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kim Taewan, 2017, ARXIV170903202; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lee E, 2017, INFORM PROCESS LETT, V120, P40, DOI 10.1016/j.ipl.2016.11.009; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274, DOI 10.1007/978-3-642-00202-1_24; Mazumdar A., 2017, P 31 INT C NEURAL IN, P5790; Mazumdar A, 2016, ANN ALLERTON CONF, P738; Newman D.J., 1960, AM MATH MON, V67, P58, DOI 10.2307/2308930; Shank N. B., 2013, ELECTRON J COMB, V20, P33; Szpankowski W., 2001, AVERAGE CASE ANAL AL, P442; Wikipedia contributors, 2018, CHERN BOUND WIK FREE; Woo KG, 2004, INFORM SOFTWARE TECH, V46, P255, DOI 10.1016/j.infsof.2003.07.003; Yun S.-Y., 2014, COLT, P138	32	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001021
C	Cummings, R; Krehbiel, S; Lai, KA; Tantipongpipat, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cummings, Rachel; Krehbiel, Sara; Lai, Kevin A.; Tantipongpipat, Uthaipon			Differential Privacy for Growing Databases	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The large majority of differentially private algorithms focus on the static setting, where queries are made on an unchanging database. This is unsuitable for the myriad applications involving databases that grow over time. To address this gap in the literature, we consider the dynamic setting, in which new data arrive over time. Previous results in this setting have been limited to answering a single non-adaptive query repeatedly as the database grows [DNPR10, CSS11]. In contrast, we provide tools for richer and more adaptive analysis of growing databases. Our first contribution is a novel modification of the private multiplicative weights algorithm of [HR10], which provides accurate analysis of exponentially many adaptive linear queries (an expressive query class including all counting queries) for a static database. Our modification maintains the accuracy guarantee of the static setting even as the database grows without bound. Our second contribution is a set of general results which show that many other private and accurate algorithms can be immediately extended to the dynamic setting by rerunning them at appropriate points of data growth with minimal loss of accuracy, even when data growth is unbounded.	[Cummings, Rachel; Lai, Kevin A.; Tantipongpipat, Uthaipon] Georgia Inst Technol, Atlanta, GA 30332 USA; [Krehbiel, Sara] Univ Richmond, Richmond, VA 23173 USA	University System of Georgia; Georgia Institute of Technology; University of Richmond	Cummings, R (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	rachelc@gatech.edu; krehbiel@richmond.edu; kevinlai@gatech.edu; tao@gatech.edu			Mozilla Research Grant; NSF [CCF-24067E5, CCF-1740776, IIS-1453304]; Georgia Institute of Technology ARC fellowship	Mozilla Research Grant; NSF(National Science Foundation (NSF)); Georgia Institute of Technology ARC fellowship	R.C. and S.K. supported in part by a Mozilla Research Grant. K.L. supported in part by NSF grant IIS-1453304. U.T. supported in part by NSF grants CCF-24067E5 and CCF-1740776, and by a Georgia Institute of Technology ARC fellowship.	Agarwal  Naman, 2017, INT C MACH LEARN ICM; [Anonymous], 2010, P 42 ACM S THEOR COM; Bassily R., 2016, P 48 ANN ACM S THEOR; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Blum A, 2008, ACM S THEORY COMPUT, P609; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626; Cummings R., 2016, P 29 C LEARN THEOR C, P772; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, SCIENCE, V349, P636, DOI 10.1126/science.aaa9375; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2009, ACM S THEORY COMPUT, P381; Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85; Jain P., 2012, COLT, V24, P1; Jameson GJO, 2016, MATH GAZ, V100, P298, DOI 10.1017/mag.2016.67; Ji  Zhanglong, 2014, 14127584 ARXIV; KIFER D, 2012, J MACH LEARN RES, V23, P1; Shalev-Shwartz S., 2009, C LEARNING THEORY; Thakurta A., 2013, ADV NEURAL INFORM PR, P2733	19	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003042
C	Ghassami, A; Kiyavash, N; Huang, BW; Zhang, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ghassami, AmirEmad; Kiyavash, Negar; Huang, Biwei; Zhang, Kun			Multi-domain Causal Structure Learning in Linear Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE	We study the problem of causal structure learning in linear systems from observational data given in multiple domains, across which the causal coefficients and/or the distribution of the exogenous noises may vary. The main tool used in our approach is the principle that in a causally sufficient system, the causal modules, as well as their included parameters, change independently across domains. We first introduce our approach for finding causal direction in a system comprising two variables and propose efficient methods for identifying causal direction. Then we generalize our methods to causal structure learning in networks of variables. Most of previous work in structure learning from multi-domain data assume that certain types of invariance are held in causal modules across domains. Our approach unifies the idea in those works and generalizes to the case that there is no such invariance across the domains. Our proposed methods are generally capable of identifying causal direction from fewer than ten domains. When the invariance property holds, two domains are generally sufficient.	[Ghassami, AmirEmad] Univ Illinois, Dept ECE, Urbana, IL 61801 USA; [Kiyavash, Negar] Georgia Inst Technol, Sch ISyE & ECE, Atlanta, GA 30332 USA; [Huang, Biwei; Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA	University of Illinois System; University of Illinois Urbana-Champaign; University System of Georgia; Georgia Institute of Technology; Carnegie Mellon University	Ghassami, A (corresponding author), Univ Illinois, Dept ECE, Urbana, IL 61801 USA.	ghassam2@illinois.edu			Army grant [W911NF-15-1-0281]; NSF [NSF CCF 1065022]; United States Air Force [FA8650-17-C-7715]; National Science Foundation under EAGER Grant [IIS-1829681]; National Institutes of Health [NIH-1R01EB022858-01, FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, FAIN-U54HG008540]; Department of Defense [FA8702-15-D-0002]	Army grant; NSF(National Science Foundation (NSF)); United States Air Force(United States Department of Defense); National Science Foundation under EAGER Grant; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Department of Defense(United States Department of Defense)	This work was supported in part by Army grant W911NF-15-1-0281 and NSF grant NSF CCF 1065022. This material is partially based upon work supported by United States Air Force under Contract No. FA8650-17-C-7715, by National Science Foundation under EAGER Grant No. IIS-1829681, and National Institutes of Health under Contract No. NIH-1R01EB022858-01, FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, and FAIN-U54HG008540, and work funded and supported by the Department of Defense under Contract No. FA8702-15-D-0002 with Carnegie Mellon University for the operation of the Software Engineering Institute, a federally funded research and development center. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Air Force or the National Institutes of Health or the National Science Foundation. We thank Clark Glymour and Malcolm Forster for helpful discussions, and appreciate the comments from anonymous reviewers, which greatly helped to improve the paper.	Andersson SA, 1997, ANN STAT, V25, P505; Bird CM, 2008, NAT REV NEUROSCI, V9, P182, DOI 10.1038/nrn2335; Bollen K. A., 1989, WILEY SERIES PROBABI; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Daniusis P., 2010, P 26 C UNC ART INT U; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Ghassami A. E., 2017, P NIPS, P3015; Gretton A., 2008, ADV NEURAL INFORM PR, P585; HOOVER KD, 1990, ECON PHILOS, V6, P207, DOI 10.1017/S026626710000122X; Hoyer P. O., 2009, ADV NEURAL INFORM PR, V21, P689; Huang B, 2017, P IEEE 17 INT C DAT; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Pourahmadi M, 2011, STAT SCI, V26, P369, DOI 10.1214/11-STS358; Reichenbach Hans, 1991, DIRECTION TIME, V65; Sch_olkopf B., 2012, P ICML, P1255; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Shimizu S, 2011, J MACH LEARN RES, V12, P1225; Spirtes P., 2000, CAUSATION PREDICTION; Tian J., 2001, P 17 C UNC ART INT, P512; Wang  Yuhao, 2018, ARXIV180205631; Xu P., 2016, ADV NEURAL INFORM PR, P1064; Zhang  K., 2015, P 15 C THEOR ASP RAT; Zhang K., 2009, P 25 C UNC ART INT U; Zhang K., 2017, P INT JOINT C ART IN; Zhang K, 2006, P 13 INT C NEUR INF; Zhang Kun, 2013, ICML	27	5	5	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000074
C	Goetz, J; Tewari, A; Zimmerman, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Goetz, Jack; Tewari, Ambuj; Zimmerman, Paul			Active Learning for Non-Parametric Regression Using Purely Random Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Active learning is the task of using labelled data to select additional points to label, with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods were previously needed to obtain theoretically superior performance. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free, consistent and minimax optimal for Lipschitz functions.	[Goetz, Jack; Tewari, Ambuj; Zimmerman, Paul] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Goetz, J (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	jrgoetz@umich.edu; tewaria@umich.edu; paulzim@umich.edu			NSF [DMS-1646108]; Sloan Research Fellowship	NSF(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	JG acknowledges the support of NSF via grant DMS-1646108. AT acknowledges the support of a Sloan Research Fellowship.	[Anonymous], 2006, ADV NEURAL INFORM PR; Attenberg Josh, 2011, SIGKDD EXPLOR NEWSL, V12, P36, DOI [DOI 10.1145/1964897.1964906, 10.1145/1964897.1964906]; Awasthi P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P449, DOI 10.1145/2591796.2591839; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Breiman L., 2017, CLASSIFICATION REGRE, DOI DOI 10.1201/9781315139470; Breiman L, 2000, 579 UCB STAT DEP; Bull AD, 2013, ANN STAT, V41, P41, DOI 10.1214/12-AOS1064; Chaudhuri K, 2017, PR MACH LEARN RES, V70; Chaudhuri Kamalika, 2015, ADV NEURAL INFORM PR, P1090; Cramer C.J., 2013, ESSENTIALS COMPUTATI; Dasgupta S., 2008, ADV NEURAL INFORM PR, P353; Efromovich S, 2008, SCAND J STAT, V35, P266, DOI 10.1111/j.1467-9469.2007.00582.x; Genuer R, 2012, J NONPARAMETR STAT, V24, P543, DOI 10.1080/10485252.2012.677843; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Hanneke S, 2015, J MACH LEARN RES, V16, P3487; Lakshminarayanan B., 2014, P ADV NEUR INF PROC, P3140; Liu H., 2018, STRUCT MULTIDISCIP O, V57, P393; Mourtada J., 2018, ARXIV180305784; Mourtada J., 2017, ADV NEURAL INFORM PR, P3761; Munos R., 2014, ADV NEURAL INFORM PR, P469; Sourati J, 2017, J MACH LEARN RES, V18; Hoang TN, 2014, PR MACH LEARN RES, V32, P739	23	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302054
C	Graber, C; Meshi, O; Schwing, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Graber, Colin; Meshi, Ofer; Schwing, Alexander			Deep Structured Prediction with Nonlinear Output Transformations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep structured models are widely used for tasks like semantic segmentation, where explicit correlations between variables provide important prior information which generally helps to reduce the data needs of deep nets. However, current deep structured models are restricted by oftentimes very local neighborhood structure, which cannot be increased for computational complexity reasons, and by the fact that the output configuration, or a representation thereof, cannot be transformed further. Very recent approaches which address those issues include graphical model inference inside deep nets so as to permit subsequent non-linear output space transformations. However, optimization of those formulations is challenging and not well understood. Here, we develop a novel model which generalizes existing approaches, such as structured prediction energy networks, and discuss a formulation which maintains applicability of existing inference techniques.	[Graber, Colin; Schwing, Alexander] Univ Illinois, Urbana, IL 61801 USA; [Meshi, Ofer] Google, Menlo Pk, CA USA	University of Illinois System; University of Illinois Urbana-Champaign; Google Incorporated	Graber, C (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	cgraber2@illinois.edu; meshi@google.com; aschwing@illinois.edu			National Science Foundation [1718221]; Samsung; 3M; IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR)	National Science Foundation(National Science Foundation (NSF)); Samsung(Samsung); 3M(3M); IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR)(International Business Machines (IBM))	This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221, Samsung, 3M, and the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR). We thank NVIDIA for providing the GPUs used for this research.	Alvarez J. M., 2012, P ECCV; Amos B, 2017, PR MACH LEARN RES, V70; Belanger D, 2017, PR MACH LEARN RES, V70; Belanger D, 2016, PR MACH LEARN RES, V48; Borenstein E, 2002, LECT NOTES COMPUT SC, V2351, P109; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chen Liang-Chich, 2015, ABS14127062 CORR; Chen LC, 2015, PR MACH LEARN RES, V37, P1785; Ciliberto Carlo, 2018, ARXIV180602402; De Campos T.E., 2009, CHARACTER RECOGNITIO; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Finley T., 2008, P ICML; GLOBERSON A, 2006, P NIPS; Gygli M, 2017, PR MACH LEARN RES, V70; Hazan T., 2016, JMLR; Hazan T., 2010, P NIPS; Hazan T., 2010, T INFORM THEORY; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huiskes Mark J, 2008, P 1 ACM INT C MULTIM, P39, DOI DOI 10.1145/1460096.1460104; Jegelka S, 2011, ADV NEURAL INFORM PR, V24, P460; Kappes J.H., 2015, IJCV; Nguyen K, 2017, IEEE WINT CONF APPL, P56, DOI 10.1109/WACV.2017.14; Komodakis N., 2011, P CVPR; KOMODAKIS N, 2009, P CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulesza A., 2008, P NIPS; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Leblond Remi<prime>, 2018, ICLR; Lin G., 2015, P NIPS; McCormick S. T., 2008, SUBMODULAR FUNCTION, P321; Meltzer T, 2009, P UAI; Meshi O., 2017, P NIPS; Meshi O., 2010, INT C MACH LEARN; Meshi O, 2016, PR MACH LEARN RES, V48; Meshi Ofer, 2015, P NIPS; Nam J., 2017, ADV NEURAL INFORM PR, P5413; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Pletscher P., 2010, P ECML PKDD; Schrijver A., 2004, COMBINATORIAL OPTIMI; Schwing A., 2011, P CVPR; Schwing A. G., 2015, ARXIV PREPRINT ARXIV; Schwing A. G., 2014, P ICML; Schwing A. G., 2012, P NIPS; Shelhamer E., 2016, IEEE TPAMI; Shimony S. E., 1994, ARTIF INTELL; Song Y, 2016, PR MACH LEARN RES, V48; Sontag D., 2008, P NIPS; Stobbe P., 2010, ADV NEURAL INFORM PR, P2208; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Taskar B., 2003, P NIPS; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Tsochantaridis I., 2005, JMLR; Tu Lifu, 2018, ICLR; van den Oord A, 2016, PR MACH LEARN RES, V48; Wainwright M, 2008, GRAPHICAL MODELS EXP; Wainwright M. J., 2003, P C CONTR COMM COMP; Werner T., 2007, PAMI; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	59	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000079
C	Guo, XY; Li, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Guo, Xiangyu; Li, Shi			Distributed k-Clustering for Data with Heavy Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we consider the k-center/median/means clustering with outliers problems (or the (k, z)-center/median/means problems) in the distributed setting. Most previous distributed algorithms have their communication costs linearly depending on z, the number of outliers. Recently Guha et al. [10] overcame this dependence issue by considering bi-criteria approximation algorithms that output solutions with 2z outliers. For the case where z is large, the extra z outliers discarded by the algorithms might be too large, considering that the data gathering process might be costly. In this paper, we improve the number of outliers to the best possible (1 + epsilon)z, while maintaining the O(1)-approximation ratio and independence of communication cost on z. The problems we consider include the (k, z)-center problem, and (k, z)-median/means problems in Euclidean metrics. Implementation of the our algorithm for (k, z)-center shows that it outperforms many previous algorithms, both in terms of the communication cost and quality of the output solution.	[Guo, Xiangyu; Li, Shi] SUNY Buffalo, Buffalo, NY 14260 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Guo, XY (corresponding author), SUNY Buffalo, Buffalo, NY 14260 USA.	xiangyug@buffalo.edu; shil@buffalo.edu			NSF [CCF-1566356, CCF-1717134]	NSF(National Science Foundation (NSF))	This research was supported by NSF grants CCF-1566356 and CCF-1717134.	Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15; [Anonymous], 2018, ARXIVABS180904684; Balcan M.F., 2013, P 26 INT C NEURAL IN, P1995; Byrka J, 2017, ACM T ALGORITHMS, V13, DOI 10.1145/2981561; Charikar M, 2001, SIAM PROC S, P642; Chen J., 2016, NIPS; Chen K, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P826; Ding H, 2016, PR MACH LEARN RES, V48; Ene A., 2011, SIGKDD, DOI DOI 10.1145/2020408.2020515; Guha S, 2017, PROCEEDINGS OF THE 29TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'17), P143, DOI 10.1145/3087556.3087568; Im S, 2015, SPAA'15: PROCEEDINGS OF THE 27TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P65, DOI 10.1145/2755573.2755607; Krishnaswamy R, 2018, ACM S THEORY COMPUT, P646, DOI 10.1145/3188745.3188882; Malkomes G., 2015, NIPS, V28, P1063	13	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002039
C	Han, YJ; Jiao, JT; Lee, CZ; Weissman, T; Wu, YH; Yu, TC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Han, Yanjun; Jiao, Jiantao; Lee, Chuan-Zheng; Weissman, Tsachy; Wu, Yihong; Yu, Tiancheng			Entropy Rate Estimation for Markov Chains with Large State Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				UNSEEN	Entropy estimation is one of the prototypical problems in distribution property testing. To consistently estimate the Shannon entropy of a distribution on S elements with independent samples, the optimal sample complexity scales sublinearly with S as Theta(S/log S) as shown by Valiant and Valiant [4]. Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with S states from a sample path of n observations. We show that Provided the Markov chain mixes not too slowly, i.e., the relaxation time is at most O(S/ln(3)S), consistent estimation is achievable when n >> S-2/log S. Provided the Markov chain has some slight dependency, i.e., the relaxation time is at least 1+Omega(ln(2) S/root S), consistent estimation is impossible when n less than or similar to S-2/logS. Under both assumptions, the optimal estimation accuracy is shown to be Theta(S-2/n log S). In comparison, the empirical entropy rate requires at least Omega(S-2) samples to be consistent, even when the Markov chain is memoryless. In addition to synthetic experiments, we also apply the estimators that achieve the optimal sample complexity to estimate the entropy rate of the English language in the Penn Treebank and the Google One Billion Words corpora, which provides a natural benchmark for language modeling and relates it directly to the widely used perplexity measure.	[Han, Yanjun; Lee, Chuan-Zheng; Weissman, Tsachy] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Jiao, Jiantao] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Wu, Yihong] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06511 USA; [Yu, Tiancheng] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China	Stanford University; University of California System; University of California Berkeley; Yale University; Tsinghua University	Han, YJ (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	yjhan@stanford.edu; jiantao@berkeley.edu; czlee@stanford.edu; tsachy@stanford.edu; yihong.wu@yale.edu; thueeyutc14@foxmail.com						Acharya J, 2017, PR MACH LEARN RES, V70; [Anonymous], 2017, ARXIV171102141; Antos A, 2001, RANDOM STRUCT ALGOR, V19, P163, DOI 10.1002/rsa.10019; BILLINGSLEY P, 1961, ANN MATH STAT, V32, P12, DOI 10.1214/aoms/1177705136; Bordenave C, 2010, ALEA-LAT AM J PROBAB, V7, P41; Brown P. F., 1992, Computational Linguistics, V18, P31; Cai HX, 2004, IEEE T INFORM THEORY, V50, P1551, DOI 10.1109/TIT.2004.830771; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Ciuperca G., 2005, APPL STOCH MODEL BUS, P1109; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; COVER TM, 1978, IEEE T INFORM THEORY, V24, P413, DOI 10.1109/TIT.1978.1055912; Daskalakis Constantinos, 2017, ARXIV170406850; Effros M, 2002, IEEE T INFORM THEORY, V48, P1061, DOI 10.1109/18.995542; Falahatgar M, 2016, IEEE INT SYMP INFO, P2689, DOI 10.1109/ISIT.2016.7541787; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Hsu D. J., 2015, ADV NEURAL INFORM PR, P1459; Jiang Qian, 2009, CONSTRUCTION TRANSIT; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Jiao JT, 2013, IEEE T INFORM THEORY, V59, P6220, DOI 10.1109/TIT.2013.2267934; Kamath S, 2016, IEEE INT SYMP INFO, P685, DOI 10.1109/ISIT.2016.7541386; KIEFFER JC, 1991, IEEE T INFORM THEORY, V37, P263, DOI 10.1109/18.75241; Kontoyiannis I, 1998, IEEE T INFORM THEORY, V44, P1319, DOI 10.1109/18.669425; Krumme C, 2013, SCI REP-UK, V3, DOI 10.1038/srep01645; Kuchaiev Oleksii, 2017, ARXIV170310722; Lanctot JK, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P409; Levin D. A., 2016, ARXIV161205330; Mitzenmacher M., 2005, PROBABILITY COMPUTIN; Montenegro R, 2006, FOUND TRENDS THEOR C, V1, P237, DOI 10.1561/0400000003; Paninski L, 2004, IEEE T INFORM THEORY, V50, P2200, DOI 10.1109/TIT.2004.833360; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; SHIELDS PC, 1996, GRADUATE STUDIES MAT, V13; Song CM, 2010, SCIENCE, V327, P1018, DOI 10.1126/science.1177170; Takaguchi T, 2011, PHYS REV X, V1, DOI 10.1103/PhysRevX.1.011008; Tao T, 2012, RANDOM MATRICES-THEO, V1, DOI 10.1142/S2010326311500018; Tatwawadi Kedar, 2018, ARXIV180501355; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Valiant G, 2017, J ACM, V64, DOI 10.1145/3125643; Valiant G, 2011, ACM S THEORY COMPUT, P685; Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81; Wang C., 2012, SCI REPORTS, V2; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; WYNER AD, 1989, IEEE T INFORM THEORY, V35, P1250, DOI 10.1109/18.45281; ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934; Zoph Barret, 2016, P INT C LEARN REPR	48	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004035
C	Hoshen, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hoshen, Yedid			Non-Adversarial Mapping with VAEs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The study of cross-domain mapping without supervision has recently attracted much attention. Much of the recent progress was enabled by the use of adversarial training as well as cycle constraints. The practical difficulty of adversarial training motivates research into non-adversarial methods. In a recent paper, it was shown that cross-domain mapping is possible without the use of cycles or GANs. Although promising, this approach suffers from several drawbacks including costly inference and an optimization variable for every training example preventing the method from using large training sets. We present an alternative approach which is able to achieve non-adversarial mapping using a novel form of Variational Auto-Encoder. Our method is much faster at inference time, is able to leverage large datasets and has a simple interpretation.	[Hoshen, Yedid] Facebook AI Res, Tel Aviv, Israel	Facebook Inc	Hoshen, Y (corresponding author), Facebook AI Res, Tel Aviv, Israel.							[Anonymous], 2017, ICCV; [Anonymous], 2014, ICLR; [Anonymous], 2017, ICCV; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Bojanowski Piotr, 2018, ICML; Choi Y., 2018, CVPR; Fabius Otto, 2014, ICLR WORKSH; Gulrajani Ishaan, 2017, NIPS; Hoshen Y., 2018, ECCV; Hoshen Yedid, 2018, ICLR WORKSH; Hoshen Yedid, 2018, ARXIV180106126; Karras T, 2017, ARXIV171010196; Kim T, 2017, ICML; Liu M.Y., 2016, NIPS; Liu M. -Y., 2017, NIPS; Miller E.G., 2000, CVPR; Miyato Takeru, 2018, ICLR; Radford A., 2016, ICLR; Tolstikhin Ilya, 2018, ICLR; Wolf L., 2018, INT C LEARN REPR; Wolf L., 2017, NIPS; Yi Z., 2017, ICCV; Yu A., 2014, CVPR; Zhang H., 2017, ARXIV171010916	24	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002011
C	Huang, QY; Zhang, PC; Wu, DP; Zhang, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huang, Qiuyuan; Zhang, Pengchuan; Wu, Dapeng; Zhang, Lei			Turbo Learning for CaptionBot and DrawingBot	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study in this paper the problems of both image captioning and text-to-image generation, and present a novel turbo learning approach to jointly training an image-to-text generator (a.k.a. CaptionBot) and a text-to-image generator (a.k.a. DrawingBot). The key idea behind the joint training is that image-to-text generation and text-to-image generation as dual problems can form a closed loop to provide informative feedback to each other. Based on such feedback, we introduce a new loss metric by comparing the original input with the output produced by the closed loop. In addition to the old loss metrics used in CaptionBot and DrawingBot, this extra loss metric makes the jointly trained CaptionBot and DrawingBot better than the separately trained CaptionBot and DrawingBot. Furthermore, the turbo-learning approach enables semi-supervised learning since the closed loop can provide pseudo-labels for unlabeled samples. Experimental results on the COCO dataset demonstrate that the proposed turbo learning can significantly improve the performance of both CaptionBot and DrawingBot by a large margin.	[Huang, Qiuyuan; Zhang, Pengchuan; Zhang, Lei] Microsoft Res, Redmond, WA 98052 USA; [Wu, Dapeng] Univ Florida, Gainesville, FL USA	Microsoft; State University System of Florida; University of Florida	Huang, QY (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	qihua@microsoft.com; penzhan@microsoft.com; dpwu@ieee.org; leizhang@microsoft.com	Zhang, Pengchuan/AAR-3769-2021	Wu, Dapeng/0000-0003-1755-0183				Abadi M, 2015, P 12 USENIX S OPERAT; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Banerjee S., 2005, P ACL WORKSH INTR EX, P65; Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856; COCO, 2017, COCO DAT IM CAPT; Denton Emily L, 2015, NEURIPS, V2, P4; Devlin J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P100; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Gan Z., 2017, P IEEE C COMP VIS PA; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; He Di, 2016, NEURAL INFORM PROCES, P2; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kiros R, 2014, PR MACH LEARN RES, V32, P595; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Mansimov E., 2016, ICLR, P1; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pasunuru R., 2017, EMNLP; Pennington Jeffrey, 2017, STANFORD GLOVE GLOBA; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2017, PR MACH LEARN RES, V70; Reed S, 2016, PR MACH LEARN RES, V48; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Salimans T, 2016, ADV NEUR IN, V29; van den Oord Aaron, 2016, ARXIV160605328; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29; Xu KS, 2017, IEEE INT CON MULTI, P361, DOI 10.1109/ICME.2017.8019408; Xu T., 2018, ATTNGAN FINE GRAINED; Yao T., 2017, P INT C COMP VIS; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Zhang H., 2017, ARXIV PREPRINT ARXIV; Zhang H., 2017, ICCV; Zhu Jun-Yan, 2017, ICCV	41	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001003
C	Huggins, JH; Mackey, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huggins, Jonathan H.; Mackey, Lester			Random Feature Stein Discrepancies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Computable Stein discrepancies have been deployed for a variety of applications, ranging from sampler selection in posterior inference to approximate Bayesian inference to goodness-of-fit testing. Existing convergence-determining Stein discrepancies admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time Stein discrepancies have been proposed for goodness-of-fit testing, they exhibit avoidable degradations in testing power-even when power is explicitly optimized To address these shortcomings, we introduce feature Stein discrepancies (Phi)SDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct (Phi)SDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations-random (Phi)SDs (R Phi SDs)-which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, R(Phi)SDs perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.	[Huggins, Jonathan H.] Harvard, Dept Biostat, Cambridge, MA 02138 USA; [Mackey, Lester] Microsoft Res New England, Cambridge, MA USA	Harvard University; Microsoft	Huggins, JH (corresponding author), Harvard, Dept Biostat, Cambridge, MA 02138 USA.	jhuggins@mit.edu; lmackey@microsoft.com						Abramowitz M., 1970, HDB MATH FUNCTIONS F; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Briol FX, 2017, PR MACH LEARN RES, V70; Carmeli C, 2010, ANAL APPL, V8, P19, DOI 10.1142/S0219530510001503; Chung F., 2006, COMPLEX GRAPHS NETWO, V107; Chwialkowski K.P., 2015, P 28 INT C NEUR INF, P1981; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Eberle A, 2016, PROBAB THEORY REL, V166, P851, DOI 10.1007/s00440-015-0673-1; GEYER CJ, 1991, COMPUTING SCIENCE AND STATISTICS, P156; Gorham  J., 2016, 161106972V3 ARXIV; Gorham J, 2017, PR MACH LEARN RES, V70; Gorham J, 2015, ADV NEUR IN, V28; Gretton A, 2012, J MACH LEARN RES, V13, P723; Herb RA, 2011, CONTEMP MATH, V557, P3; Honorio  J., 2017, 171009953V1 ARXIV; Jitkrittum  W., 2017, ADV NEURAL INFORM PR; Liu  Q., 2017, INT C ART INT STAT; Liu Q., 2016, NEURIPS; Liu Q, 2016, PR MACH LEARN RES, V48; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Serfling R., 1980, APPROXIMATION THEORE; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Sutherland DJ, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P862; Wang D., 2016, ARXIV161101722; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wendland H., 2005, SCATTERED DATA APPRO; Zhao J, 2015, NEURAL COMPUT, V27, P1345, DOI 10.1162/NECO_a_00732	30	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301085
C	Jitkrittum, W; Kanagawa, H; Sangkloy, P; Hays, J; Scholkopf, B; Gretton, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jitkrittum, Wittawat; Kanagawa, Heishiro; Sangkloy, Patsorn; Hays, James; Schoelkopf, Bernhard; Gretton, Arthur			Informative Features for Model Comparison	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISTRIBUTIONS	Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing GAN models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.	[Jitkrittum, Wittawat; Schoelkopf, Bernhard] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Kanagawa, Heishiro; Gretton, Arthur] UCL, Gatsby Unit, London, England; [Sangkloy, Patsorn; Hays, James] Georgia Inst Technol, Atlanta, GA 30332 USA	Max Planck Society; University of London; University College London; University System of Georgia; Georgia Institute of Technology	Jitkrittum, W (corresponding author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.	wittawat@tuebingen.mpg.de; heishirok@gatsby.ucl.ac.uk; patsorn_sangkloy@gatech.edu; hays@gatech.edu; bernhard.schoelkopf@tuebingen.mpg.de; arthur.gretton@gmail.com	Jitkrittum, Wittawat/U-6881-2019	Gretton, Arthur/0000-0003-3169-7624	Gatsby Charitable Foundation	Gatsby Charitable Foundation	HK and AG thank the Gatsby Charitable Foundation for the financial support.	Amos Brandon, 2016, CMUCS16118; Arjovsky M, 2017, PR MACH LEARN RES, V70; Baringhaus L., 1988, METRIKA, V35, P339, DOI DOI 10.1007/BF02613322; Bounliphone W., 2015, ARXIV151104581; BOX GEP, 1976, J AM STAT ASSOC, V71, P791, DOI 10.2307/2286841; Casella G, 2002, DUXBURY; Chwialkowski K.P., 2015, P 28 INT C NEUR INF, P1981; Chwialkowski K, 2016, PR MACH LEARN RES, V48; FRIEDMAN JH, 1979, ANN STAT, V7, P697, DOI 10.1214/aos/1176344722; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gorham J, 2015, ADV NEUR IN, V28; Gretton A, 2012, ADV NEURAL INF PROCE; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS 2017; Hall P, 2002, BIOMETRIKA, V89, P359, DOI 10.1093/biomet/89.2.359; Harchaoui Z., 2008, TESTING HOMOGENEITY, P609; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196; Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181; Jitkrittum W., 2017, ADV NEURAL INFORM PR, P262; Jitkrittum W, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Liu Q, 2016, PR MACH LEARN RES, V48; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Nowozin S, 2016, ADV NEUR IN, V29; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rosenbaum PR, 2005, J ROY STAT SOC B, V67, P515, DOI 10.1111/j.1467-9868.2005.00513.x; Salimans T, 2016, ADV NEUR IN, V29; Serfling R., 1980, APPROXIMATION THEORE; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Srivastava A., 2017, ARXIV E PRINTS; Sutherland D.J., 2016, ARXIV161104488; Sutherland Danica J, 2018, ICLR; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szekely GJ, 2005, J MULTIVARIATE ANAL, V93, P58, DOI 10.1016/j.jmva.2003.12.002; Yamada M., 2018, ARXIV180206226	43	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300075
C	Kannan, S; Morgenstern, J; Roth, A; Waggoner, B; Wu, ZS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kannan, Sampath; Morgenstern, Jamie; Roth, Aaron; Waggoner, Bo; Wu, Zhiwei Steven			A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Bandit learning is characterized by the tension between long-term exploration and short-term exploitation. However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. In such settings, one might like to run a "greedy" algorithm, which always makes the optimal decision for the individuals at hand - but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm. We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve "no regret", perhaps (depending on the specifics of the setting) with a constant amount of initial training data. This suggests that in slightly perturbed environments, exploration and exploitation need not be in conflict in the linear setting.(1)	[Kannan, Sampath; Roth, Aaron] Univ Penn, Philadelphia, PA 19104 USA; [Morgenstern, Jamie] Georgia Tech, Atlanta, GA USA; [Waggoner, Bo] Microsoft Res, New York, NY USA; [Wu, Zhiwei Steven] Univ Minnesota, Minneapolis, MN 55455 USA	University of Pennsylvania; University System of Georgia; Georgia Institute of Technology; Microsoft; University of Minnesota System; University of Minnesota Twin Cities	Kannan, S (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.							Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Barry-Jester Anna Maria, 2015, NEW SCI; Bastani H., 2017, ARXIV E PRINTS; Bietti A., 2018, ARXIV E PRINTS; Bird Sarah, 2016, WORKSH FAIRN ACC TRA; Byrnes Nanette, 2016, MIT TECHNOLOGY REV; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Ensign Danielle, 2017, WORKSH FAIRN ACC TRA; Li L., 2011, PROC 4 ACM INT C WEB, P297, DOI DOI 10.1145/1935826.1935878; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Raghavan Manish, 2018, P 31 C LEARN THEOR C; Rudin C., 2013, WIRED MAGAZINE; Syrgkanis V, 2016, PR MACH LEARN RES, V48	14	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302025
C	Kaufmann, E; Koolen, WM; Garivier, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kaufmann, Emilie; Koolen, Wouter M.; Garivier, Aurelien			Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTIARMED BANDIT	Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-task in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.	[Kaufmann, Emilie] CNRS, Paris, France; [Kaufmann, Emilie] Univ Lille, CRIStAL SequeL Inria Lille, Lille, France; [Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands; [Garivier, Aurelien] Ecole Normale Super Lyon, UMPA, Lyon, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; Universite de Lille - ISITE; Universite de Lille; Ecole Normale Superieure de Lyon (ENS de LYON)	Kaufmann, E (corresponding author), CNRS, Paris, France.; Kaufmann, E (corresponding author), Univ Lille, CRIStAL SequeL Inria Lille, Lille, France.	emilie.kaufmann@univ-lille.fr; wmkoolen@cwi.nl; aurelien.garivier@ens-lyon.fr						Agrawal S., 2012, P 25 C LEARN THEOR; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Chen L., 2017, P 30 C LEARN THEOR C; CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205; de la Pena VH, 2009, PROBAB APPL SER, P1; DEramo C., 2017, AAAI; DEramo C., 2016, INT C MACH LEARN ICM; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Garivier A., 2017, ARXIV171104454; Garivier A., 2018, MATH OPERATIONS  JUN; Garivier A., 2016, P 29 C LEARN THEOR C; Goldsman D., 1998, COMP SYSTEMS VIA SIM; Grill J.-B., 2016, ADV NEURAL INFORM PR, P4680; Grunwald P. D., 2007, MINIMUM DESCRIPTION; Huang R., 2017, INT C ALG LEARN THEO; Imagawa T, 2017, CONF TECHNOL APPL, P202; Jamieson K., 2014, P 27 C LEARN THEOR; Kalyanakrishnan S., 2012, P ICML; Kaufmann E., 2012, P 23 C ALG LEARN THE; Kaufmann E, 2018, PREPRINT; Kaufmann E., 2017, ADV NEURAL INFORM PR; Kaufmann E, 2016, J MACH LEARN RES, V17; Kim S.-H., 2005, ACM Transactions on Modeling and Computer Simulation, V15, P155, DOI 10.1145/1060576.1060579; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Locatelli A., 2016, INT C MACH LEARN ICM; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Russo D., 2016, CORR; Simchowitz M., 2017, P 30 C LEARN THEOR C; Teraoka K, 2014, IEICE T INF SYST, VE97D, P392, DOI 10.1587/transinf.E97.D.392; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; van Hasselt H., 2013, CORR	33	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000080
C	Ke, CY; Honorio, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ke, Chuyang; Honorio, Jean			Information-theoretic Limits for Community Detection in Network Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We analyze the information-theoretic limits for the recovery of node labels in several network models. This includes the Stochastic Block Model, the Exponential Random Graph Model, the Latent Space Model, the Directed Preferential Attachment Model, and the Directed Small-world Model. For the Stochastic Block Model, the non-recoverability condition depends on the probabilities of having edges inside a community, and between different communities. For the Latent Space Model, the non-recoverability condition depends on the dimension of the latent space, and how far and spread are the communities in the latent space. For the Directed Preferential Attachment Model and the Directed Small-world Model, the non-recoverability condition depends on the ratio between homophily and neighborhood size. We also consider dynamic versions of the Stochastic Block Model and the Latent Space Model.	[Ke, Chuyang; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Ke, CY (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	cke@purdue.edu; jhonorio@purdue.edu						Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Ball B, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036103; Bandeira AS, 2018, FOUND COMPUT MATH, V18, P345, DOI 10.1007/s10208-016-9341-9; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Bin Yu, 1997, FESTSCHRIFT L LECAM, V423, P435; Boykov Y, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P79, DOI 10.1007/0-387-28831-7_5; Cabreros Irineo, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P584, DOI 10.1109/CISS.2016.7460568; CHENG Y, 2014, INT C MACH LEARN, P244; Cline MS, 2007, NAT PROTOC, V2, P2366, DOI 10.1038/nprot.2007.324; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Deshpande Y, 2016, IEEE INT SYMP INFO, P185, DOI 10.1109/ISIT.2016.7541286; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Goldenberg A, 2010, FOUND TRENDS MACH LE, V2, P129, DOI 10.1561/2200000005; Hajek B, 2016, IEEE T INFORM THEORY, V62, P2788, DOI 10.1109/TIT.2016.2546280; Heimlicher S., 2012, ARXIV12092910; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; Jog V., 2015, ARXIV150906418; Jung D, 2017, INT CONF BIG DATA, P363, DOI 10.1109/BIGCOMP.2017.7881694; Leskovec J., 2010, P 19 INT C WORLD WID, P631; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Mathai A.M., 1992, QUADRATIC FORMS RAND; Mossel E., 2012, ARXIV12021499; Newman MEJ, 2002, P NATL ACAD SCI USA, V99, P2566, DOI 10.1073/pnas.012582999; Rui Wu, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P449, DOI 10.1145/2745844.2745887; Saad H, 2017, ANN ALLERTON CONF, P822; Sarkar P., 2006, ADV NEURAL INFORM PR, P1145; Tang M, 2013, ANN STAT, V41, P1406, DOI 10.1214/13-AOS1112; Wang W, 2010, IEEE INT SYMP INFO, P1373, DOI 10.1109/ISIT.2010.5513573; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Xu J., 2014, C LEARNING THEORY, P903; Yun S.Y., 2016, ADV NEURAL INFORM PR, V29, P965	36	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002083
C	Korshunova, I; Degrave, J; Huszar, F; Gal, Y; Gretton, A; Dambre, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Korshunova, Iryna; Degrave, Jonas; Huszar, Ferenc; Gal, Yarin; Gretton, Arthur; Dambre, Joni			BRUNO: A Deep Recurrent Model for Exchangeable Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.	[Korshunova, Iryna; Degrave, Jonas; Dambre, Joni] Univ Ghent, Ghent, Belgium; [Huszar, Ferenc] Twitter, San Francisco, CA USA; [Gal, Yarin] Univ Oxford, Oxford, England; [Gretton, Arthur] UCL, Gatsby Unit, London, England; [Degrave, Jonas] DeepMind, London, England	Ghent University; Twitter, Inc.; University of Oxford; University of London; University College London	Korshunova, I (corresponding author), Univ Ghent, Ghent, Belgium.	iryna.korshunova@ugent.be; jonas.degrave@ugent.be; fhuszar@twitter.com; yarin@cs.ox.ac.uk; arthur.gretton@gmail.com; joni.dambre@ugent.be		Gretton, Arthur/0000-0003-3169-7624; Dambre, Joni/0000-0002-9373-1210				Aldous D., 1985, LECT NOTES MATH; BAILEY RW, 1994, MATH COMPUT, V62, P779, DOI 10.2307/2153537; Burges, 1998, MNIST DATABASE HANDW; Chen Fei, 2018, ARXIV180207876; Dinh L, 2017, 5 INT C LEARN REPR I; Edwards H., 2017, P 5 INT C LEARN REPR; Ghahramani Z, 2006, CVPR, P2110; Ghahramani Zoubin, 2006, ADV NEURAL INFORM PR, P435; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Papamakarios George, 2017, ARXIV170507057; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T, 2016, ADV NEUR IN, V29; Shah A, 2014, JMLR WORKSH CONF PRO, V33, P877; Szabo Z, 2016, J MACH LEARN RES, V17; Theis Lucas, 2016, ICLR; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Vinyals Oriol, 2016, 4 INT C LEARN REPR I, DOI DOI 10.48550/ARXIV.1511.01844; Xiao H., 2017, FASHION MNIST NOVEL; Zaheer Manzil, 2017, P ADV NEUR INF PROC, P3394	23	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001071
C	Lee, SW; Heo, YJ; Zhang, BT		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Sang-Woo; Heo, Yu-Jung; Zhang, Byoung-Tak			Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence. Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. To ask the adequate question, deep learning and reinforcement learning have been recently applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose "Answerer in Questioner's Mind" (AQM), a novel information theoretic algorithm for goal-oriented dialog. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer's intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialog tasks: "MNIST Counting Dialog" and "GuessWhat?!". In our experiments, AQM outperforms comparative algorithms by a large margin.	[Lee, Sang-Woo] Naver Corp, Clova AI Res, Seongnam, South Korea; [Lee, Sang-Woo; Heo, Yu-Jung; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea; [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea	Seoul National University (SNU)	Lee, SW (corresponding author), Naver Corp, Clova AI Res, Seongnam, South Korea.; Lee, SW (corresponding author), Seoul Natl Univ, Seoul, South Korea.				Institute for Information & Communications Technology Promotion [R0126-16-1072-SW.StarLab, 2017-0-01772-VTT, 2018-0-00622-RMI]; Korea Evaluation Institute of Industrial Technology - Korea government (MSIP, DAPA) [10060086-RISF]	Institute for Information & Communications Technology Promotion; Korea Evaluation Institute of Industrial Technology - Korea government (MSIP, DAPA)	The authors would like to thank Jin-Hwa Kim, Tong Gao, Cheolho Han, Wooyoung Kang, Jaehyun Jun, Hwiyeol Jo, Byoung-Hee Kim, Kyoung Woon On, Sungjae Cho, Joonho Kim, Seungjae Jung, Hanock Kwak, Donghyun Kwak, Christina Baek, Minjoon Seo, Marco Baroni, and Jung-Woo Ha for helpful comments and editing. This work was supported by the Institute for Information & Communications Technology Promotion (R0126-16-1072-SW.StarLab, 2017-0-01772-VTT, 2018-0-00622-RMI) and Korea Evaluation Institute of Industrial Technology (10060086-RISF) grant funded by the Korea government (MSIP, DAPA).	Andreas J., 2016, P 2016 C EMP METH NA, P1173, DOI DOI 10.18653/V1/D16-1125; [Anonymous], 2017, ARXIV170306585; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Batali John, 1998, APPROACHES EVOLUTION, V405, P426; Bordes Antoine, 2017, ICLR; Bruner J., 1981, ADV INFANCY RES; Chandrasekaran Arjun, 2017, ARXIV170400717; Chattopadhyay Prithvijit, 2017, ARXIV170805122; Cho K., 2014, P 2014 C EMP METH NA, P1724; Choi Edward, 2018, ICLR; Das A, 2017, IEEE INT C ELECTR TA; de Vries H., 2017, P IEEE C COMP VIS PA; Evtimova Katrina, 2018, ICLR; Finn C, 2017, PR MACH LEARN RES, V70; Foerster J. N, 2017, ARXIV170904326; Fried Daniel, 2018, P 2018 C N AM CHAPT, V1, P1951, DOI DOI 10.18653/V1/N18-1177; Han Cheolho, 2017, 2017 IJCAI WORKSH LI; Hernandez-Leal P, 2017, L N INST COMP SCI SO, V179, P3, DOI 10.1007/978-3-319-49622-1_1; Kendall A., 2017, ADV NEURAL INF PROCE, V30; Kim Jin-Hwa, 2017, ARXIV171205558; Kottur Satwik, 2017, P 2017 C EMP METH NA, P2962, DOI DOI 10.18653/V1/D17-1321.URL; Lazaridou Angeliki, 2018, ICLR; Lehrmann Andreas, 2017, ADV NEURAL INFORM PR; Lemon Oliver, 2006, P 11 C EUR CHAPT ASS, P119; Li X, 2017, INT JOINT C NAT LANG; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Mordatch I., 2017, ARXIV170304908; Polifroni Joseph, 2006, 5 INT C LANG RES EV; Potts C., 2017, T ASS COMPUTATIONAL, DOI [10.1162/tacl_a_00064, DOI 10.1162/TACL_A_00064]; PREMACK D, 1978, BEHAV BRAIN SCI, V1, P515, DOI 10.1017/S0140525X00076512; Redmon J., 2017, P IEEE C COMPUTER VI, P7263, DOI DOI 10.1109/CVPR.2017.690; Rothe A, 2017, ADV NEUR IN, V30; Serban I. V., 2015, ARXIV150704808; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Strub F., 2017, ARXIV170305423; Vinyals O., 2015, ICML DEEP LEARN WORK; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wen T.H, 2016, ARXIV PREPRINT ARXIV; Williams JD, 2007, COMPUT SPEECH LANG, V21, P393, DOI 10.1016/j.csl.2006.06.008; Yu Licheng, 2017, COMPUTER VISION PATT, V2; Zhang B.-T., 2013, AAAI SPRING S LIF MA, P62; Zhao T., 2016, SIGDIAL, P1	46	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302058
C	Li, Y; Cheng, MH; Fujii, K; Hsieh, FS; Hsieh, CJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Yao; Cheng, Minhao; Fujii, Kevin; Hsieh, Fushing; Hsieh, Cho Jui			Learning from Group Comparisons: Exploiting Higher Order Interactions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of learning from group comparisons, with applications in predicting outcomes of sports and online games. Most of the previous works in this area focus on learning individual effects-they assume each player has an underlying score, and the "ability" of the team is modeled by the sum of team members' scores. Therefore, current approaches cannot model deeper interaction between team members: some players perform much better if they play together, while some players perform poorly together. In this paper, we propose a new model that takes the player-interaction effects into consideration. However, under certain circumstances, the total number of individuals can be very large, and number of player interactions grows quadratically, which makes learning intractable. In this case, we propose a latent factor model, and show that the sample complexity of our model is bounded under mild assumptions. Finally, we show that our proposed models have much better prediction power on several E-sports datasets, and furthermore can be used to reveal interesting patterns that cannot be discovered by previous methods.	[Li, Yao; Fujii, Kevin; Hsieh, Fushing] Univ Calif Davis, Dept Stat, Davis, CA 95616 USA; [Cheng, Minhao; Hsieh, Cho Jui] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA	University of California System; University of California Davis; University of California System; University of California Los Angeles	Li, Y (corresponding author), Univ Calif Davis, Dept Stat, Davis, CA 95616 USA.	yaoli@ucdavis.edu; mhcheng@ucla.edu; kmfujii@ucdavis.edu; fhsieh@ucdavis.edu		Li, Yao/0000-0002-7195-5774	NSF [IIS-1719097]; Intel Faculty Award; Google Cloud and Nvidia	NSF(National Science Foundation (NSF)); Intel Faculty Award; Google Cloud and Nvidia(Google Incorporated)	The paper is partially supported by the support of NSF via IIS-1719097, Intel Faculty Award, Google Cloud and Nvidia.	[Anonymous], 2016, ADV NEURAL INFORM PR; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Chen  S., 2016, PREDICTING MATCHUPS; Chen S, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P227, DOI 10.1145/2835776.2835787; Chiang KY, 2017, PR MACH LEARN RES, V54, P748; Fujii  K., 2017, TECHNICAL REPORT; Fushing H, 2011, P ROY SOC A-MATH PHY, V467, P3590, DOI 10.1098/rspa.2011.0268; Fushing H, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.061110; Gleich David F, 2011, P ACM SIGKDD C KNOWL, P60; Glickman M. E., 1995, AM CHESS J; Herbrich R., 2006, NIPS; Huang  T.-K., 2008, JMLR; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Purushotham S, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P552, DOI 10.1145/2623330.2623747; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Shamir O, 2014, J MACH LEARN RES, V15, P3401; Wauthier Fabian, 2013, INT C MACH LEARN, P109; Xiao  C., 2016, AAAI; Zhang L, 2010, PROC INT C TOOLS ART, P249, DOI 10.1109/ICTAI.2010.108	21	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305003
C	Liu, H; Jin, S; Zhang, CS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Hu; Jin, Sheng; Zhang, Changshui			Connectionist Temporal Classification with Maximum Entropy Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Connectionist Temporal Classification (CTC) is an objective function for end-to-end sequence learning, which adopts dynamic programming algorithms to directly learn the mapping between sequences. CTC has shown promising results in many sequence learning applications including speech recognition and scene text recognition. However, CTC tends to produce highly peaky and overconfident distributions, which is a symptom of overfitting. To remedy this, we propose a regularization method based on maximum conditional entropy which penalizes peaky distributions and encourages exploration. We also introduce an entropy-based pruning method to dramatically reduce the number of CTC feasible paths by ruling out unreasonable alignments. Experiments on scene text recognition show that our proposed methods consistently improve over the CTC baseline without the need to adjust training settings. Code has been made publicly available at: https://github.com/liuhu-bigeye/enctc.crnn.	[Liu, Hu; Jin, Sheng; Zhang, Changshui] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence,Dept Automat,Tsinghu, Beijing, Peoples R China	Tsinghua University	Liu, H (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence,Dept Automat,Tsinghu, Beijing, Peoples R China.	liuhu15@mails.tsinghua.edu.cn; js17@mails.tsinghua.edu.cn; zcs@mail.tsinghua.edu.cn		Jin, Sheng/0000-0001-5736-7434	NSFC [61876095, 61751308, 61473167]; Beijing Natural Science Foundation [L172037]	NSFC(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work is supported by NSFC (Grant No. 61876095, No. 61751308 and No. 61473167) and Beijing Natural Science Foundation (Grant No. L172037).	Amodei D, 2016, PR MACH LEARN RES, V48; Battenberg E, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P206; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chorowski Jan, 2016, ARXIV161202695; Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175; Graves A., 2006, P 23 INT C MACH LEAR, P369; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; Hannun A.Y., 2014, ARXIV14125567, P1; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Huang DA, 2016, LECT NOTES COMPUT SC, V9908, P137, DOI 10.1007/978-3-319-46493-0_9; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jaderberg  Max, 2014, ARXIV14062227; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Kim Suyoun, 2017, ARXIV171102212; KROGH A, 1992, ADV NEUR IN, V4, P950; Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245; Lin MX, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P393, DOI 10.1145/3126686.3126755; Liu WB, 2016, J RESIDUALS SCI TECH, V13, P1; Lucas S. M., 2005, International Journal on Document Analysis and Recognition, V7, P105, DOI 10.1007/s10032-004-0134-3; Maron O, 1998, ADV NEUR IN, V10, P570; Miao YJ, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P167, DOI 10.1109/ASRU.2015.7404790; Miller D, 1996, IEEE T SIGNAL PROCES, V44, P3108, DOI 10.1109/78.553484; Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127; Mnih V, 2016, PR MACH LEARN RES, V48; Pereyra Gabriel, 2017, ARXIV170106548; Senior A, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P604, DOI 10.1109/ASRU.2015.7404851; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Variani E, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4959; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402; Wang T, 2012, INT C PATT RECOG, P3304; Zeiler MD, 2013, ARXIV13013557, DOI DOI 10.1007/978-3-319-26532-2_6	37	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300077
C	Lu, T; Schuurmans, D; Boutilier, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lu, Tyler; Schuurmans, Dale; Boutilier, Craig			Non-delusional Q-learning and Value Iteration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Since standard Q-updates make globally uncoordinated action choices with respect to the expressible policy class, inconsistent or even conflicting Q-value estimates can result, leading to pathological behaviour such as over/under-estimation, instability and even divergence. To solve this problem, we introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets-sets that record constraints on policies consistent with backed-up Q-values. We prove that both the model-based and model-free algorithms using this backup remove delusional bias, yielding the first known algorithms that guarantee optimal results under general conditions. These algorithms furthermore only require polynomially many information sets (from a potentially exponential support). Finally, we suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias.	[Lu, Tyler; Schuurmans, Dale; Boutilier, Craig] Google AI, Mountain View, CA 94043 USA		Lu, T (corresponding author), Google AI, Mountain View, CA 94043 USA.	tylerlu@google.com; schuurmans@google.com; cboutilier@google.com						Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Bartlett P. L., 2017, ARXIV; Bellemare MG, 2016, AAAI CONF ARTIF INTE, P1476; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellemare Marc G., 2017, P INT C MACH LEARN I; BLUM A, 1988, P 1 ANN WORKSH COMP, P00009; Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369; De Farias DP, 2003, OPER RES, V51, P850, DOI 10.1287/opre.51.6.850.24925; Durrett R., 2013, PROBABILITY THEORY E; Geist Matthieu, 2017, ADV NEURAL INFORM PR, P3208; Gordon G. J., 1999, THESIS; Hessel M., 2017, AAAI; Jiang N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1181; Lehnert L., 2018, P 32 AAAI C ART INT; Maei H.R., 2010, 27 ICML, P719; Melo FS, 2007, LECT NOTES COMPUT SC, V4539, P308, DOI 10.1007/978-3-540-72927-3_23; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384; Nachum O., 2017, ADV NEURAL INFORM PR, P1476; Petrik M, 2010, OPTIMIZATION BASED A; Sauer N., 1972, J COMB THEORY A, V13, P145, DOI [10.1016/0097-3165(72)90019-2, DOI 10.1016/0097-3165(72)90019-2]; SHELAH S, 1972, PAC J MATH, V41, P247, DOI 10.2140/pjm.1972.41.247; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szepesvari C., 2004, P INT C MACH LEARN I; TESAURO G, 1992, MACH LEARN, V8, P257, DOI 10.1007/BF00992697; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Van Hasselt H, 2010, ADV NEURAL INFORM PR, P2613; Vapnik V.N, 1998, STAT LEARNING THEORY; Wang Z., 2016, P INT C MACH LEARN I; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Watkins CJCH., 1989, THESIS	34	5	5	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004050
C	Magill, M; Qureshi, FZ; de Haan, HW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Magill, Martin; Qureshi, Faisal Z.; de Haan, Hendrick W.			Neural Networks Trained to Solve Differential Equations Learn General Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a technique based on the singular vector canonical correlation analysis (SVCCA) for measuring the generality of neural network layers across a continuously-parametrized set of tasks. We illustrate this method by studying generality in neural networks trained to solve parametrized boundary value problems based on the Poisson partial differential equation. We find that the first hidden layers are general, and that they learn generalized coordinates over the input domain. Deeper layers are successively more specific. Next, we validate our method against an existing technique that measures layer generality using transfer learning experiments. We find excellent agreement between the two methods, and note that our method is much faster, particularly for continuously-parametrized problems. Finally, we also apply our method to networks trained on MNIST, and show it is consistent with, and complimentary to, another study of intrinsic dimensionality.	[Magill, Martin; Qureshi, Faisal Z.; de Haan, Hendrick W.] Univ Ontario, Inst Tech, Oshawa, ON, Canada	Ontario Tech University	Magill, M (corresponding author), Univ Ontario, Inst Tech, Oshawa, ON, Canada.	martin.magill1@uoit.net; faisal.qureshi@uoit.ca; hendrick.dehaan@uoit.ca			Ontario Graduate Scholarship (OGS); Natural Sciences and Engineering Research Council (NSERC) [2014-06091]	Ontario Graduate Scholarship (OGS)(Ontario Graduate Scholarship); Natural Sciences and Engineering Research Council (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	MM gratefully acknowledges funding from the Ontario Graduate Scholarship (OGS). FZQ gratefully acknowledges funding from the Natural Sciences and Engineering Research Council (NSERC) in the form of Discovery Grant 2015-04533. HWdH gratefully acknowledges funding from the Natural Sciences and Engineering Research Council (NSERC) in the form of Discovery Grant 2014-06091.	Abadi M, 2015, P 12 USENIX S OPERAT; Berg  Jens, 2017, ARXIV171106464; Burges, 1998, MNIST DATABASE HANDW; DISSANAYAKE MWMG, 1994, COMMUN NUMER METH EN, V10, P195, DOI 10.1002/cnm.1640100303; Grohs P., 2018, ARXIV PREPRINT ARXIV; Han J, 2018, P NATL ACAD SCI USA, V115, P8505, DOI 10.1073/pnas.1718942115; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lagaris IE, 1998, IEEE T NEURAL NETWOR, V9, P987, DOI 10.1109/72.712178; Le Q.V., 2011, NEURIPS, P1017; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Li C., 2018, P 6 INT C LEARN REPR; Li Y., 2015, FE NIPS, P196; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Sinno J.P., 2009, IEEE T KNOWL DATA EN, V22, P1345, DOI [10.1109/TKDE.2009.191, DOI 10.1109/TKDE.2009.191]; Sirignano  Justin, 2017, ARXIV170807469; VANMILLIGEN BP, 1995, PHYS REV LETT, V75, P3594, DOI 10.1103/PhysRevLett.75.3594; Yadav N, 2015, SPRINGER BRIEFS APPL, DOI [10.1007/978-94-017-9816-7, DOI 10.1007/978-94-017-9816-7]; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519	19	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304011
C	Mokhtari, A; Ozdaglar, A; Jadbabaie, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mokhtari, Aryan; Ozdaglar, Asuman; Jadbabaie, Ali			Escaping Saddle Points in Constrained Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				2ND-ORDER STATIONARY-POINTS; NONLINEAR LEAST-SQUARES; CUBIC-REGULARIZATION; EVALUATION COMPLEXITY; CONVERGENCE; ALGORITHMS; OPTIMALITY	In this paper, we study the problem of escaping from saddle points in smooth nonconvex optimization problems subject to a convex set C. We propose a generic framework that yields convergence to a second-order stationary point of the problem, if the convex set C is simple for a quadratic objective function. Specifically, our results hold if one can find a rho-approximate solution of a quadratic program subject to C in polynomial time, where rho < 1 is a positive constant that depends on the structure of the set C. Under this condition, we show that the sequence of iterates generated by the proposed framework reaches an (epsilon, gamma)-second order stationary point (SOSP) in at most O(max{epsilon(-2), rho(-3) gamma(-3)}) iterations. We further characterize the overall complexity of reaching an SOSP when the convex set C can be written as a set of quadratic constraints and the objective function Hessian has a specific structure over the convex set C. Finally, we extend our results to the stochastic setting and characterize the number of stochastic gradient and Hessian evaluations to reach an (epsilon, gamma)-SOSP.	[Mokhtari, Aryan; Ozdaglar, Asuman; Jadbabaie, Ali] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mokhtari, A (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	aryanm@mit.edu; asuman@mit.edu; jadbabai@mit.edu			DARPA Lagrange; ONR BRC Program	DARPA Lagrange; ONR BRC Program	This work was supported by DARPA Lagrange and ONR BRC Program. The authors would like to thank Yue Sun for pointing out a missing condition in the first draft of the paper.	Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; Allen-Zhu Z., 2017, ABS170808694 CORR; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; [Anonymous], 2016, ADV NEURAL INFORM PR; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bian W, 2015, MATH PROGRAM, V149, P301, DOI 10.1007/s10107-014-0753-5; BURKE JV, 1990, MATH PROGRAM, V47, P305, DOI 10.1007/BF01580867; Carmon Y., 2016, ABS161100756 CORR; Carmon Y, 2017, PR MACH LEARN RES, V70; Carmon Yair, 2017, ARXIV171100841; Cartis C, 2012, IMA J NUMER ANAL, V32, P1662, DOI 10.1093/imanum/drr035; Cartis C., 2017, FDN COMPUTATIONAL MA, P1; Cartis C, 2015, SIAM J NUMER ANAL, V53, P836, DOI 10.1137/130915546; Cartis C, 2013, SIAM J OPTIMIZ, V23, P1553, DOI 10.1137/120869687; Cartis C, 2011, MATH PROGRAM, V130, P295, DOI 10.1007/s10107-009-0337-y; Cartis C, 2011, MATH PROGRAM, V127, P245, DOI 10.1007/s10107-009-0286-5; Conn AR, 1993, SIAM J OPTIMIZ, V3, P164, DOI 10.1137/0803009; Curtis FE, 2017, MATH PROGRAM, V162, P1, DOI 10.1007/s10107-016-1026-2; Di Pillo G, 2005, MATH OPER RES, V30, P897, DOI 10.1287/moor.1050.0150; Facchinei F, 1998, MATH OPER RES, V23, P746, DOI 10.1287/moor.23.3.746; Fu MY, 1998, J COMB OPTIM, V2, P29, DOI 10.1023/A:1009739827008; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Haeser G., 2017, MATH PROGRAM, P1; Hinder O., 2017, ARXIV171011606; Jeyakumar V, 2014, MATH PROGRAM, V147, P171, DOI 10.1007/s10107-013-0716-2; Jin C., 2017, ABS171110456 CORR; Jin C, 2017, PR MACH LEARN RES, V70; Lacoste-Julien S., 2016, ARXIV PREPRINT ARXIV; Lei LH, 2017, ADV NEUR IN, V30; Martinez JM, 2017, J GLOBAL OPTIM, V68, P367, DOI 10.1007/s10898-016-0475-8; MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Paternain S., 2017, ARXIV170708028; Reddi SJ, 2018, PR MACH LEARN RES, V84; Reddi SJ, 2016, PR MACH LEARN RES, V48; Reddi SJ, 2016, IEEE DECIS CONTR P, P1971, DOI 10.1109/CDC.2016.7798553; Royer Clement W, 2017, ARXIV170603131; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Tseng P, 2003, SIAM J OPTIMIZ, V14, P268, DOI 10.1137/S1052623401395899; Xu Yi, 2017, ARXIV171101944; YE YY, 1992, MATH PROGRAM, V56, P285, DOI 10.1007/BF01580903	46	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303061
C	O'Kelly, M; Sinha, A; Namkoong, H; Duchi, J; Tedrake, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		O'Kelly, Matthew; Sinha, Aman; Namkoong, Hongseok; Duchi, John; Tedrake, Russ			Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CROSS-ENTROPY METHOD; STOCHASTIC-APPROXIMATION; OPTIMIZATION; SEARCH; SAFETY	While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the de facto evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by 2-20 times over naive Monte Carlo sampling methods and 10-300P times (where P is the number of processors) over real-world testing.	[O'Kelly, Matthew] Univ Penn, Philadelphia, PA 19104 USA; [Sinha, Aman; Namkoong, Hongseok; Duchi, John] Stanford Univ, Stanford, CA 94305 USA; [Tedrake, Russ] MIT, Cambridge, MA 02139 USA	University of Pennsylvania; Stanford University; Massachusetts Institute of Technology (MIT)	O'Kelly, M (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	mokelly@seas.upenn.edu; amans@stanford.edu; hnamk@stanford.edu; jduchi@stanford.edu; russt@mit.edu			National Science Foundation Graduate Research Fellowship; Stanford Graduate Fellowship; Fannie & John Hertz Foundation Fellowship; Samsung Fellowship; SAIL-Toyota Center for AI Research; National Science Foundation award [NSF-CAREER-1553086]	National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF)); Stanford Graduate Fellowship(Stanford University); Fannie & John Hertz Foundation Fellowship; Samsung Fellowship(Samsung); SAIL-Toyota Center for AI Research; National Science Foundation award(National Science Foundation (NSF))	MOK was partially supported by a National Science Foundation Graduate Research Fellowship. AS was partially supported by a Stanford Graduate Fellowship and a Fannie & John Hertz Foundation Fellowship. HN was partially supported by a Samsung Fellowship and the SAIL-Toyota Center for AI Research. JD was partially supported by the National Science Foundation award NSF-CAREER-1553086.	Abbas H., 2018, LNCS; Althoff M, 2014, IEEE T ROBOT, V30, P903, DOI 10.1109/TRO.2014.2312453; Arora S, 2014, PR MACH LEARN RES, V32; Asmussen S., 2007, STOCHASTIC SIMULATIO; Baier C, 2008, PRINCIPLES OF MODEL CHECKING, P1; Baram N, 2017, PR MACH LEARN RES, V70; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C., 2015, ARXIV PREPRINT ARXIV; Bojarski M., 2016, ARXIV160407316; Bonnefon JF, 2016, SCIENCE, V352, P1573, DOI 10.1126/science.aaf2654; Bucklew J, 2013, INTRO RARE EVENT SIM; Cheah M, 2016, LECT NOTES COMPUT SC, V9895, P262, DOI 10.1007/978-3-319-45931-8_18; Cohen N., 2016, C LEARNING THEORY, V49, P698; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Epic Games, 2022, UNR ENG; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Gal Y, 2016, PR MACH LEARN RES, V48; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Heinecke H., 2004, TECHNICAL REPORT; Henzinger TA, 1998, J COMPUT SYST SCI, V57, P94, DOI 10.1006/jcss.1998.1581; Hintjens P., 2013, ZEROMQ MESSAGING MAN; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Homem-De-Mello T, 2007, INFORMS J COMPUT, V19, P381, DOI 10.1287/ijoc.1060.0176; Hu JQ, 2014, ACM T MODEL COMPUT S, V24, DOI 10.1145/2641565; Hu JQ, 2012, IEEE T AUTOMAT CONTR, V57, P165, DOI 10.1109/TAC.2011.2158128; Hu JQ, 2011, NAV RES LOG, V58, P457, DOI 10.1002/nav.20462; Huang XW, 2017, LECT NOTES COMPUT SC, V10426, P3, DOI 10.1007/978-3-319-63387-9_1; Jiaqiao Hu, 2009, Proceedings of the 2009 Winter Simulation Conference (WSC 2009), P459, DOI 10.1109/WSC.2009.5429357; Kalra N, 2016, TRANSPORT RES A-POL, V94, P182, DOI 10.1016/j.tra.2016.09.010; Katz G., 2017, ARXIV170201135CSAI, V1, P1; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kroese DP, 2013, HANDB STAT, V31, P19, DOI 10.1016/B978-0-444-53859-8.00002-3; Kuefler A, 2017, IEEE INT VEH SYM, P204, DOI 10.1109/IVS.2017.7995721; Kwiatkowska Marta, 2011, Computer Aided Verification. Proceedings 23rd International Conference, CAV 2011, P585, DOI 10.1007/978-3-642-22110-1_47; Lygeros J., 2004, ENSIETA WORKSH; O'Kelly M., 2016, APEX AUTONOMOUS VEHI, V1; Quiter Craig, 2018, DEEPDRIVE; Roohi N., 2018, ARXIV180608810; Ross S., 2010, PROC 13 INT C ARTIF, V9, P661; Ross St<prime>ephane, 2011, AISTATS; Rubinstein R., 2004, CROSS ENTROPY METHOD; Rubinstein RY, 2001, APPL OPTIMIZAT, V54, P303; Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964; Schram R., 2013, IMPLEMENTATION AUTON; Seshia S. A., 2015, P 52 ANN DES AUT C, P148; Shah S., 2017, FIELD SERVICE ROBOTI; Shalev-Shwartz S., 2017, ARXIV170806374; Tjeng V., 2017, ARXIV171107356CSLG; Tuncali CE, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1470, DOI 10.1109/ITSC.2016.7795751; U. D. of Transportation FHWA, 2008, NGSIM NEXT GEN SIM; Vogel K, 2003, ACCIDENT ANAL PREV, V35, P427, DOI 10.1016/S0001-4575(02)00022-2; Zabinsky Z.B., 2013, STOCHASTIC ADAPTIVE, V72; Zhao D., 2016, THESIS; Zhao D, 2018, IEEE T INTELL TRANSP, V19, P733, DOI 10.1109/TITS.2017.2701846; Zhou EL, 2014, IEEE T AUTOMAT CONTR, V59, P1818, DOI 10.1109/TAC.2014.2310052	56	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004039
C	Pang, M; Gao, W; Tao, M; Zhou, ZH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pang, Ming; Gao, Wei; Tao, Min; Zhou, Zhi-Hua			Unorganized Malicious Attacks Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recommender systems have attracted much attention during the past decade. Many attack detection algorithms have been developed for better recommendations, mostly focusing on shilling attacks, where an attack organizer produces a large number of user profiles by the same strategy to promote or demote an item. This work considers another different attack style: unorganized malicious attacks, where attackers individually utilize a small number of user profiles to attack different items without organizer. This attack style occurs in many real applications, yet relevant study remains open. We formulate the unorganized malicious attacks detection as a matrix completion problem, and propose the Unorganized Malicious Attacks detection (UMA) algorithm, based on the alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of the proposed approach.	[Pang, Ming; Gao, Wei; Tao, Min; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Pang, M (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	pangm@lamda.nju.edu.cn; gaow@lamda.nju.edu.cn; taom@nju.edu.cn; zhouzh@lamda.nju.edu.cn			National Key R&D Program of China [2018YFB1004300]; NSFC [61333014, 61503179]; JiangsuSF [BK20150586]; Collaborative Innovation Center of Novel Software Technology and Industrialization; Fundamental Research Funds for the Central Universities	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); JiangsuSF; Collaborative Innovation Center of Novel Software Technology and Industrialization; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This research was supported by the National Key R&D Program of China (2018YFB1004300), NSFC (61333014, 61503179), JiangsuSF (BK20150586), and Collaborative Innovation Center of Novel Software Technology and Industrialization, and Fundamental Research Funds for the Central Universities.	Aggarwal CC, 2016, RECOMMENDER SYSTEMS, P1; Bhaumik Runa, 2011, Proceedings of the 2011 International Conference on Data Mining (DMIN 2011), P181; Bhaumik Runa, 2006, P 4 WORKSH INT TECHN; Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001; Bresler G., 2014, ADV NEURAL INFORM PR, P3347; Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704; Bryan K, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P155; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chen CH, 2016, MATH PROGRAM, V155, P57, DOI 10.1007/s10107-014-0826-5; Cong Li, 2011, 2011 International Conference of Soft Computing and Pattern Recognition, P190, DOI 10.1109/SoCPaR.2011.6089138; Famei He, 2010, Proceedings of the 2010 IEEE International Conference on Granular Computing (GrC-2010), P692, DOI 10.1109/GrC.2010.130; GOLDBERG D, 1992, COMMUN ACM, V35, P61, DOI 10.1145/138859.138867; Gunes I, 2014, ARTIF INTELL REV, V42, P767, DOI 10.1007/s10462-012-9364-9; He BS, 2015, IMA J NUMER ANAL, V35, P394, DOI 10.1093/imanum/drt060; Hurley N., 2009, P 2009 ACM C REC SYS, P149; Li B., 2009, P 26 ANN INT C MACH, P617, DOI DOI 10.1145/1553374.1553454; Ling G., 2013, P 23 INT JOINT C ART, VI, P2670; Luca M., 2016, REV REPUTATION REVEN; Mackey LW, 2011, ADV NEURAL INFORM PR, P1134, DOI DOI 10.5555/2986459.2986586; Mehta B., 2007, AAAI, P1402, DOI [10.5555/1619797.1619870, DOI 10.5555/1619797.1619870]; Mehta B, 2009, USER MODEL USER-ADAP, V19, P65, DOI 10.1007/s11257-008-9050-4; Mobasher B., 2009, INTELLIGENT SYSTEMS, V22, P56; [庞明 Pang Ming], 2018, [中国科学. 信息科学, Scientia Sinica Informationis], V48, P177; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Rao N., 2015, P ADV NEUR INF PROC, P2098; Rockafellar R.T., 2015, CONVEX ANAL; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Salakhutdinov R., 2007, P 24 INT C MACHINE L, P791, DOI 10.1145/1273496.1273596; Tao M, 2011, SIAM J OPTIMIZ, V21, P57, DOI 10.1137/100781894; Yi X., 2016, NIPS, P4152	32	5	5	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001051
C	Shah, D; Xie, QM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shah, Devavrat; Xie, Qiaomin			Q-learning with Nearest Neighbors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MARKOV DECISION-PROCESSES; CONVERGENCE; APPROXIMATION; TREES	We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available. We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a d-dimensional state space and the discounted factor gamma is an element of (0, 1), given an arbitrary sample path with "covering time" L, we establish that the algorithm is guaranteed to output an E-accurate estimate of the optimal Q-function using (O) over tilde (L/(epsilon(3)(1 - gamma)(7))) samples. For instance, for a wellbehaved MDP, the covering time of the sample path under the purely random policy scales as (O) over tilde (1/epsilon(d)), so the sample complexity scales as (O) over tilde (1/epsilon(d+3)). Indeed, we establish a lower bound that argues that the dependence of (Omega) over tilde (1/epsilon(d+2)) is necessary.	[Shah, Devavrat; Xie, Qiaomin] MIT, LIDS, Cambridge, MA 02139 USA; [Shah, Devavrat] MIT, Stat & Data Sci Ctr, Dept EECS, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Shah, D (corresponding author), MIT, LIDS, Cambridge, MA 02139 USA.; Shah, D (corresponding author), MIT, Stat & Data Sci Ctr, Dept EECS, Cambridge, MA 02139 USA.	devavrat@mit.edu; qxie@mit.edu			NSF [NeTs-1523546, TRIPODS-1740751, CMMI-1462158]	NSF(National Science Foundation (NSF))	This work was supported in parts by NSF projects NeTs-1523546, TRIPODS-1740751, and CMMI-1462158.	Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Azar M. G., 2011, NIPS; Barreto AMS, 2016, J MACH LEARN RES, V17; BENTLEY JL, 1979, IEEE T SOFTWARE ENG, V5, P333, DOI 10.1109/TSE.1979.234200; Bertsekas D. P., 2007, DYNAMIC PROGRAMMING, V2; BERTSEKAS DP, 1975, IEEE T AUTOMAT CONTR, VAC20, P415, DOI 10.1109/TAC.1975.1100984; Bhandari J., 2018, ARXIV180602450, P1691; Bhat N., 2012, NIPS; Chee-Seng Chow, 1989, Journal of Complexity, V5, P466, DOI 10.1016/0885-064X(89)90021-6; CHOW CS, 1991, IEEE T AUTOMAT CONTR, V36, P898, DOI 10.1109/9.133184; Dalal Gal, 2017, ARXIV170401161; Dasgupta S, 2008, ACM S THEORY COMPUT, P537; Duan Y, 2016, INT C MACH LEARN, P1329; Dufour F, 2012, J MATH ANAL APPL, V388, P1254, DOI 10.1016/j.jmaa.2011.11.015; Dufour F, 2015, STOCHASTICS, V87, P273, DOI 10.1080/17442508.2014.939979; Dufour F, 2013, SIAM J CONTROL OPTIM, V51, P1298, DOI 10.1137/120867925; Even-Dar E., 2004, JMLR, V5; Haskell W. B., 2016, MATH OPERATIONS RES, V41; Hasselt H. V., 2010, NIPS; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Kearns M., 1999, NIPS; Lim SH, 2005, LECT NOTES ARTIF INT, V3720, P230; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Mathy C, 2015, AAAI CONF ARTIF INTE, P2864; Melo F.S., 2008, P 25 INT C MACH LEAR, P664, DOI [DOI 10.1145/1390156.1390240, 10.1145/1390156.1390240]; Melo FS, 2007, LECT NOTES COMPUT SC, V4539, P308, DOI 10.1007/978-3-540-72927-3_23; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos R, 2008, J MACH LEARN RES, V9, P815; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829; Ormoneit D., 2002, IEEE T AUTOMATIC CON, V47; Pazis Jason, 2013, P ANN AAAI C ARTIFIC, P774; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rust J., 1997, ECONOMETRICA, V65; Saldi N., 2017, MATH OPERATIONS RES, V42; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; STONE CJ, 1982, ANN STAT, V10, P1040, DOI 10.1214/aos/1176345969; Strehl A. L., 2006, ICML; Szepesvari C., 1997, NIPS; Szepesvari C., 2004, P 21 INT C MACH LEAR, P100; TSITSIKLIS JN, 1994, MACH LEARN, V16, P185, DOI 10.1023/A:1022689125041; Tsitsiklis John N, 1997, IEEE T AUTOMATIC CON, V42; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Vershynin R., 2017, HIGH DIMENSIONAL PRO; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340	48	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303014
C	Sharma, A; Johnson, RE; Engert, F; Linderman, SW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sharma, Anuj; Johnson, Robert E.; Engert, Florian; Linderman, Scott W.			Point process latent variable models of larval zebrafish behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DYNAMICS	A fundamental goal of systems neuroscience is to understand how neural activity gives rise to natural behavior. In order to achieve this goal, we must first build comprehensive models that offer quantitative descriptions of behavior. We develop a new class of probabilistic models to tackle this challenge in the study of larval zebrafish, an important model organism for neuroscience. Larval zebrafish locomote via sequences of punctate swim bouts-brief flicks of the tail-which are naturally modeled as a marked point process. However, these sequences of swim bouts belie a set of discrete and continuous internal states, latent variables that are not captured by standard point process models. We incorporate these variables as latent marks of a point process and explore various models for their dynamics. To infer the latent variables and fit the parameters of this model, we develop an amortized variational inference algorithm that targets the collapsed posterior distribution, analytically marginalizing out the discrete latent variables. With a dataset of over 120,000 swim bouts, we show that our models reveal interpretable discrete classes of swim bouts and continuous internal states like hunger that modulate their dynamics. These models are a major step toward understanding the natural behavioral program of the larval zebrafish and, ultimately, its neural underpinnings.	[Sharma, Anuj; Linderman, Scott W.] Columbia Univ, New York, NY 10027 USA; [Johnson, Robert E.; Engert, Florian] Harvard Univ, Cambridge, MA 02138 USA	Columbia University; Harvard University	Linderman, SW (corresponding author), Columbia Univ, New York, NY 10027 USA.	scott.linderman@columbia.edu		Johnson, Robert/0000-0003-1405-6518	Simons Foundation [SCGB-418011, SCGB-542973, 325207]; National Institutes of Health's Brain Initiative [U19NS104653, R24NS086601, R43OD024879]	Simons Foundation; National Institutes of Health's Brain Initiative(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The authors thank John Cunningham and Liam Paninski for helpful advice and feedback. SWL thanks the Simons Foundation for their support (SCGB-418011). FE received funding from the National Institutes of Health's Brain Initiative U19NS104653, R24NS086601 and R43OD024879, as well as Simons Foundation grants (SCGB-542973 and 325207).	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Ahrens MB, 2012, NATURE, V485, P471, DOI 10.1038/nature11057; Berman GJ, 2018, BMC BIOL, V16, DOI 10.1186/s12915-018-0494-7; Brown Andre EX, 2018, NATURE PHYS; Cong L, 2017, ELIFE, V6, DOI 10.7554/eLife.28158; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Cunningham J. P., 2008, P 25 INT C MACH LEAR, P192, DOI DOI 10.1145/1390156.1390181; Cunningham John P, 2007, ADV NEURAL INFORM PR, P329; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Dunn TW, 2016, ELIFE, V5, DOI 10.7554/eLife.12741; Gao YJ, 2016, ADV NEUR IN, V29; Haesemeyer M, 2018, NEURON, V98, P817, DOI 10.1016/j.neuron.2018.04.013; Johnson Robert E., 2018, COMPUTATIONAL SYSTEM; Kalueff AV, 2013, ZEBRAFISH, V10, P70, DOI 10.1089/zeb.2012.0861; Kennedy A, 2014, NAT NEUROSCI, V17, P416, DOI 10.1038/nn.3650; Kim DH, 2017, NAT METHODS, V14, P1107, DOI [10.1038/NMETH.4429, 10.1038/nmeth.4429]; Kingma D., P INT C LEARN REPR I, DOI DOI 10.1145/1830483.1830503; Kingman John Frank Charles, 1992, POISSON PROCESSES, V3; Kowler E, 2011, VISION RES, V51, P1457, DOI 10.1016/j.visres.2010.12.014; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Lidster K, 2017, J FISH BIOL, V90, P1891, DOI 10.1111/jfb.13278; Linderman Scott, 2016, ADV NEURAL INFORM PR, P2002; Linderman Scott W., 2017, P 20 INT C ART INT S; Lloyd C, 2015, PR MACH LEARN RES, V37, P1814; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Marques Joao C, 2018, CURRENT BIOL; Mei H., 2017, ADV NEURAL INFORM PR, P6754; Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115; Murphy K, 2002, TECHNICAL REPORT; Orger MB, 2017, ANNU REV NEUROSCI, V40, P125, DOI 10.1146/annurev-neuro-071714-033857; Paninski L, 2018, CURR OPIN NEUROBIOL, V50, P232, DOI 10.1016/j.conb.2018.04.007; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Rao V. A., 2011, ADV NEURAL INFORM PR, P2474; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sahani M., 2016, P IEEE INT WORKSH MA, P1; Samo YLK, 2015, PR MACH LEARN RES, V37, P2227; Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI [10.1109/MASSP.1986.1165342, 10.1002/0471250953.bia03as18]; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004; Xiao S., 2017, ADV NEURAL INFORM PR, P3247; Zhao Y, 2017, NEURAL COMPUT, V29, P1293, DOI 10.1162/NECO_a_00953	46	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005050
C	Song, JM; Ren, HY; Sadigh, D; Ermon, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Song, Jiaming; Ren, Hongyu; Sadigh, Dorsa; Ermon, Stefano			Multi-Agent Generative Adversarial Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.	[Song, Jiaming; Ren, Hongyu; Sadigh, Dorsa; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Song, JM (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	tsong@cs.stanford.edu; hyren@cs.stanford.edu; dorsa@cs.stanford.edu; ermon@cs.stanford.edu			Toyota Research Institute and Future of Life Institute	Toyota Research Institute and Future of Life Institute	This work was supported by Toyota Research Institute and Future of Life Institute. The authors would like to thank Lantao Yu for discussions over implementation.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amodei D., 2016, CONCRETE PROBLEMS AI; Amodei D, 2016, FAULTY REWARD FUNCTI; [Anonymous], 2018, ARXIV180201561; [Anonymous], 2017, ARXIV170602275; Bagnell J. A., 2015, TECH REP; Barrett S, 2017, ARTIF INTELL, V242, P132, DOI 10.1016/j.artint.2016.10.005; Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156; Bogert K, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P173; Englert P, 2015, P INT S ROB RES; Filar J., 2012, COMPETITIVE MARKOV D; Finn C, 2016, PR MACH LEARN RES, V48; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta J. K., 2017, MULTIAGENT DEEP REIN; Hadfield-Menell D, 2016, ADV NEURAL INFORM PR, V29, P3909; Hadfield-Menell D., 2017, ADV NEURAL INFORM PR, P6768; Hesse C., 2017, OPENAI BASELINES; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Jang Eric, 2017, P 5 INT C LEARN REPR; Junling Hu, 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P242; Kakade S, 2002, ADV NEUR IN, V14, P1531; Le H. M., 2017, ARXIV170303121; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Li Y., 2017, ARXIV170308840; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lin X., 2014, ARXIV14036508; Littman ML, 1994, ICML 1994, P157; Maddison Chris J, 2016, ARXIV161100712; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Matignon L., 2012, AAAI; Peng P., 2017, ARXIV170310069; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Prasad H., 2015, ARXIV150700093; Reddy TS, 2012, IEEE SYS MAN CYBERN, P1930, DOI 10.1109/ICSMC.2012.6378020; Ross S., 2011, AISTATS, P6; Ross S., 2010, AISTATS, P3; Schulman John, 2015, ARXIV150602438; Song Y., 2018, INT C MACH LEARN ICM; Song Y., 2018, ABS180507894 CORR; Sosic A., 2016, STAT, V1050, P17; Stadie B.C., 2017, ICLR; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Wu Y., 2017, NIPS 2017, P5279; Ziebart B. D., 2008, AAAI, V8, P1433	47	5	5	0	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002005
C	Stern, M; Shazeer, N; Uszkoreit, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Stern, Mitchell; Shazeer, Noam; Uszkoreit, Jakob			Blockwise Parallel Decoding for Deep Autoregressive Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While common architecture classes such as recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, generation still remains an inherently sequential process. To overcome this limitation, we propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model. This allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel. We verify our approach empirically through a series of experiments using state-of-the-art self-attention models for machine translation and image super-resolution, achieving iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. In terms of wall-clock time, our fastest models exhibit real-time speedups of up to 4x over standard greedy decoding.	[Stern, Mitchell] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Stern, Mitchell; Shazeer, Noam; Uszkoreit, Jakob] Google Brain, Mountain View, CA USA	University of California System; University of California Berkeley; Google Incorporated	Stern, M (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	mitchell@berkeley.edu; noam@google.com; usz@google.com						Furlanello Tommaso, 2017, NIPS WORKSH MET; Gehring J., 2017, P ICML; Gu Jiatao, 2018, P ICLR; Hinton G., 2015, ARXIV150302531; Kaiser Lukasz, 2018, ARXIV180303382, P2390; Kalchbrenner N, 2018, PR MACH LEARN RES, V80; Kim Yoon, 2016, ARXIV160607947, DOI [10.18653/v1/D16-1139, DOI 10.18653/V1/D16-1139]; Lee Jason, 2018, ABS180206901 CORR; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Oord A.V.D., 2016, SSW; Parmar Niki, 2018, ABS180205751 CORR; van den Oord A., 2017, PARALLEL WAVENET FAS, P3918; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vaswani Ashish, 2018, ABS180307416 CORR	15	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004062
C	Subramanian, S; Rajeswar, S; Sordoni, A; Trischler, A; Courville, A; Pal, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Subramanian, Sandeep; Rajeswar, Sai; Sordoni, Alessandro; Trischler, Adam; Courville, Aaron; Pal, Christopher			Towards Text Generation with Adversarially Learned Neural Outlines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent progress in deep generative models has been fueled by two paradigms - autoregressive and adversarial models. We propose a combination of both approaches with the goal of learning generative models of text. Our method first produces a high-level sentence outline and then generates words sequentially, conditioning on both the outline and the previous outputs. We generate outlines with an adversarial model trained to approximate the distribution of sentences in a latent space induced by general-purpose sentence encoders. This provides strong, informative conditioning for the autoregressive stage. Our quantitative evaluations suggests that conditioning information from generated outlines is able to guide the autoregressive model to produce realistic samples, comparable to maximum-likelihood trained language models, even at high temperatures with multinomial sampling. Qualitative results also demonstrate that this generative procedure yields natural-looking sentences and interpolations.	[Subramanian, Sandeep; Rajeswar, Sai; Courville, Aaron; Pal, Christopher] Montreal Inst Learning Algorithms, Montreal, PQ, Canada; [Subramanian, Sandeep; Rajeswar, Sai; Courville, Aaron] Univ Montreal, Montreal, PQ, Canada; [Pal, Christopher] Ecole Polytech Montreal, Montreal, PQ, Canada; [Subramanian, Sandeep; Sordoni, Alessandro; Trischler, Adam] Microsoft Res Montreal, Montreal, PQ, Canada; [Rajeswar, Sai; Pal, Christopher] Element AI, Montreal, PQ, Canada; [Courville, Aaron] CIFAR, Toronto, ON, Canada; [Subramanian, Sandeep] Microsoft Res Montreal, Montreal, PQ, Canada	Universite de Montreal; Universite de Montreal; Universite de Montreal; Polytechnique Montreal; Canadian Institute for Advanced Research (CIFAR)	Subramanian, S (corresponding author), Montreal Inst Learning Algorithms, Montreal, PQ, Canada.; Subramanian, S (corresponding author), Univ Montreal, Montreal, PQ, Canada.; Subramanian, S (corresponding author), Microsoft Res Montreal, Montreal, PQ, Canada.	sandeep.subramanian.1@umontreal.ca; sai.rajeswar.mudumba@umontreal.ca; alsordon@microsoft.com; adam.trischler@microsoft.com; aaron.courville@umontreal.ca; christopher.pal@polymtl.ca	Subramanian, Sandeep/AAX-1580-2020	Subramanian, Sandeep/0000-0001-9840-5812	Fonds de recherche du Quebec - Nature et technologies	Fonds de recherche du Quebec - Nature et technologies	The authors thank Alex Lamb, Rithesh Kumar, Jonathan Pilault, Isabela Albuquerque, Anirudh Goyal, Kyunghyun Cho, Kelly Zhang and Varsha Embar for feedback and valuable discussions during the course of this work. We are also grateful to the PyTorch development team [38]. We thank NVIDIA for donating a DGX-1 computer used in this work and Fonds de recherche du Quebec - Nature et technologies for funding.	Adi Yossi, 2016, ARXIV160804207; An GZ, 1996, NEURAL COMPUT, V8, P643, DOI 10.1162/neco.1996.8.3.643; [Anonymous], ARXIV171105717; Arjovsky M, 2017, PR MACH LEARN RES, V70; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152; Chung J., 2014, ARXIV14123555; Conneau A, 2017, PROC 2017 C EMPIR ME, P670, DOI [10.18653/v1/d17-1070, DOI 10.18653/V1/D17-1070]; Conneau A, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P1699; Conneau A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2126; Conneau Alexis, 2018, P EMNLP 2018, P2475; Engel J, 2017, ARXIV171105772; Fedus William, 2018, INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal Anirudh, 2017, ARXIV171104755; Gulrajani I, 2017, P NIPS 2017; Heafield Kenneth, 2011, P 6 WORKSH STAT MACH, P187; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hill Felix, 2016, P 2016 C N AM CHAPT, P1367, DOI DOI 10.18653/V1/N16-1162; Joulin Armand, 2016, ARXIV161203651; Jozefowicz Rafal, 2016, ARXIV160202410; Karras T., 2017, PROGR GROWING GANS I; Kim Yoon, 2017, ARXIV170604223; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Kolesnyk Vladyslav, 2016, ARXIV160601404; Kusner Matt J, 2016, ARXIV161104051; Lample Guillaume, 2017, INT C LEARN REPR; Makhzani A., 2015, ARXIV151105644; Mathews A, 2016, AAAI CONF ARTIF INTE, P3574; Merity Stephen, 2017, ICLR; Mikolov Tomas, RECURRENT NEURAL NET; Mirza Mehdi, 2014, ARXIV14111784, P2672; Oord A.V.D., 2016, SSW; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Plappert Matthias, 2017, ARXIV170601905; Poole Ben, 2014, CORR; Press Ofir, 2017, ARXIV170601399; Rajeswar Sai, 2017, ARXIV170510929; Rifai S., 2011, PROC INT C MACH LEAR; Salimans T, 2016, ADV NEUR IN, V29; Serban I.V., 2016, BUILDING END TO END; Shen TX, 2017, ADV NEUR IN, V30; Shen Yikang, 2018, ARXIV180302710; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Subramanian Sandeep, 2018, ARXIV180400079; Theis Lucas, 2015, ARXIV151101844; Tolstikhin Ilya, 2017, ARXIV171101558; Tong Che, 2017, ARXIV170207983; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang A., 2018, P 2018 EMNLP WORKSH; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	59	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002013
C	Suggala, AS; Prasad, A; Ravikumar, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Suggala, Arun Sai; Prasad, Adarsh; Ravikumar, Pradeep			Connecting Optimization and Regularization Paths	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the implicit regularization properties of optimization techniques by explicitly connecting their optimization paths to the regularization paths of "corresponding" regularized problems. This surprising connection shows that iterates of optimization techniques such as gradient descent and mirror descent are point-wise close to solutions of appropriately regularized objectives. While such a tight connection between optimization and regularization is of independent intellectual interest, it also has important implications for machine learning: we can port results from regularized estimators to optimization, and vice versa. We investigate one key consequence, that borrows from the well-studied analysis of regularized estimators, to then obtain tight excess risk bounds of the iterates generated by optimization techniques.	[Suggala, Arun Sai; Prasad, Adarsh; Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Suggala, AS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	asuggala@cs.cmu.edu; adarshp@cs.cmu.edu; pradeepr@cs.cmu.edu			NSF [IIS-1149803, IIS-1664720, DMS-1264033]	NSF(National Science Foundation (NSF))	We acknowledge the support of NSF via IIS-1149803, IIS-1664720, DMS-1264033. The authors are grateful to Suriya Gunasekar and anonymous reviewers for helpful comments on the paper.	[Anonymous], 2015, ADV NEURAL INFORM PR; [Anonymous], 2018, ARXIV180301905; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bubeck S., 2015, FDN TRENDS MACHINE L; Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750; Friedman J., 2003, TECHNICAL REPORT; Gunasekar S., 2018, ARXIV E PRINTS; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Hardt M, 2015, ARXIV150901240; Hoofar A., 2008, J INEQUALITIES PURE, V9, P5; Hsu D., 2012, JMLR WORKSHOP C P, V23; Ji Z., 2018, ARXIV180307300; Li T, 2018, ALGORITHMS, V11, DOI 10.3390/a11110186; Neu G., 2018, ARXIV E PRINTS; Raskutti G, 2014, J MACH LEARN RES, V15, P335; Ron Guy, 2013, J INSTRUM, V8; Rosset S, 2004, J MACH LEARN RES, V5, P941; Rosset S, 2004, ADV NEUR IN, V16, P1237; Rudelson M, 2009, COMMUN PUR APPL MATH, V62, P1707, DOI 10.1002/cpa.20294; Srebro N., 2017, ARXIV171010345; Valluri SR, 2000, CAN J PHYS, V78, P823, DOI 10.1139/cjp-78-9-823; Wei Y., 2017, P ADV NEUR INF PROC, P6067; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Yu B., 2009, ADV NEURAL INFORM PR, P1348	25	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005021
C	Sung, M; Su, H; Yu, R; Guibas, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sung, Minhyuk; Su, Hao; Yu, Ronald; Guibas, Leonidas			Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Various 3D semantic attributes such as segmentation masks, geometric features, keypoints, and materials can be encoded as per-point probe functions on 3D geometries. Given a collection of related 3D shapes, we consider how to jointly analyze such probe functions over different shapes, and how to discover common latent structures using a neural network - even in the absence of any correspondence information. Our network is trained on point cloud representations of shape geometry and associated semantic functions on that point cloud. These functions express a shared semantic understanding of the shapes but are not coordinated in any way. For example, in a segmentation task, the functions can be indicator functions of arbitrary sets of shape parts, with the particular combination involved not known to the network. Our network is able to produce a small dictionary of basis functions for each shape, a dictionary whose span includes the semantic functions provided for that shape. Even though our shapes have independent discretizations and no functional correspondences are provided, the network is able to generate latent bases, in a consistent order, that reflect the shared semantic structure among the shapes. We demonstrate the effectiveness of our technique in various segmentation and keypoint selection applications.	[Sung, Minhyuk; Guibas, Leonidas] Stanford Univ, Stanford, CA 94305 USA; [Su, Hao; Yu, Ronald] Univ Calif San Diego, La Jolla, CA 92093 USA	Stanford University; University of California System; University of California San Diego	Sung, M (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	mhsung@cs.stanford.edu; haosu@eng.ucsd.edu; ronaldyu@ucsd.edu; guibas@cs.stanford.edu			DoD Vannevar Bush Faculty Fellowship; NSF [CHS-1528025, IIS-1763268]	DoD Vannevar Bush Faculty Fellowship; NSF(National Science Foundation (NSF))	We thank the anonymous reviewers for their comments and suggestions. This project was supported by a DoD Vannevar Bush Faculty Fellowship, NSF grants CHS-1528025 and IIS-1763268, and an Amazon AWS AI Research gift.	Alfaro Anali, 2016, CVPR; [Anonymous], 2014, ICLR; Armeni Iro, 2016, CVPR; Aubry M., 2011, ICCV WORKSH; Bogo F., 2014, CVPR; Bristow H., 2013, CVPR; Chang A.X., 2015, CORR; Chen S., 2001, SIAM REV; DeerWester S., 1990, J AM SOC INFORM SCI; Eynard D, 2016, INT CONF 3D VISION, P399, DOI 10.1109/3DV.2016.49; Hofmann T., 1999, P 15 C UNC ART INT; Hosang J., 2016, IEEE TPAMI; Huang Q., 2014, SIGGRAPH; HUANG Q., 2011, SIGGRAPH ASIA; Huang Q., 2018, CVPR; Huang Qi-Xing, 2013, SIGGRAPH ASIA; Kalogerakis E., 2017, CVPR; Klokov R., 2017, ICCV; Kovnatsky A., 2015, CVPR; Lee H., 2007, NIPS; Lewicki M. S., 2000, NEURAL COMPUTATION; Litany O., 2017, CVPR; Neumann U., 2018, CVPR; Nogneng Dorian, 2017, EUROGRAPHICS; Olshausen B., 1996, NATURE; Olshausen B. A., 1997, VISION RES; Olshausen Bruno A, 2002, J VISION; Olshausen Bruno A, 2004, CURRENT OPINION NEUR; Ovsjanikov M., 2012, SIGGRAPH; Pokrass Jonathan, 2013, EUROGRAPHICS; Qi C. R., 2018, CVPR; Qi C. R., 2017, CVPR; Qi Charles Ruizhongtai, 2017, NIPS; Rodola Emanuele, 2016, SGP; Sun Jian, 2009, SGP; Thome N., 2018, CVPR; Wang F., 2013, ICCV; Wang F., 2014, CVPR; Wang Yue, 2018, DYNAMIC GRAPH CM LEA; Yi L., 2017, CVPR; Yi L., 2016, SIGGRAPH ASIA; Zeiler Matthew D., 2010, CVPR	42	5	5	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300045
C	Tao, YZ; Sun, Q; Du, Q; Liu, W		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tao, Yunzhe; Sun, Qi; Du, Qiang; Liu, Wei			Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Nonlocal neural networks [25] have been proposed and shown to be effective in several computer vision tasks, where the nonlocal operations can directly capture long-range dependencies in the feature space. In this paper, we study the nature of diffusion and damping effect of nonlocal networks by doing spectrum analysis on the weight matrices of the well-trained networks, and then propose a new formulation of the nonlocal block. The new block not only learns the nonlocal interactions but also has stable dynamics, thus allowing deeper nonlocal structures. Moreover, we interpret our formulation from the general nonlocal modeling perspective, where we make connections between the proposed nonlocal network and other nonlocal models, such as nonlocal diffusion process and Markov jump process.	[Tao, Yunzhe; Du, Qiang] Columbia Univ, Sch Engn & Appl Sci, New York, NY 10027 USA; [Sun, Qi] BCSRC, Beijing, Peoples R China; [Sun, Qi] USTC, Beijing, Peoples R China; [Liu, Wei] Tencent AI Lab, Shenzhen, Peoples R China	Columbia University; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Tencent	Tao, YZ (corresponding author), Columbia Univ, Sch Engn & Appl Sci, New York, NY 10027 USA.	y.tao@columbia.edu; sunqi@csrc.ac.cn; qd2125@columbia.edu; wl2223@columbia.edu	Du, Qiang/B-1021-2008	Du, Qiang/0000-0002-1067-8937; Liu, Wei/0000-0002-3865-8145	US NSF [CCF-1740833, DMR-1534910, DMS-1719699]	US NSF(National Science Foundation (NSF))	This work is supported in part by US NSF CCF-1740833, DMR-1534910 and DMS-1719699. Y. Tao wants to thank Tingkai Liu and Xinpeng Chen for their help on the experiments.	[Anonymous], 2018, CVPR; [Anonymous], 2017, COMMUN MATH STAT; BERAN J, 1995, IEEE T COMMUN, V43, P1566, DOI 10.1109/26.380206; Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Cont R., 2005, FRACTALS ENG; Du Q, 2012, SIAM REV, V54, P667, DOI 10.1137/110833294; ELMAN JL, 1991, MACH LEARN, V7, P195, DOI 10.1007/BF00114844; Evans L.C., 1997, CURRENT DEV MATH, V1997, P65, DOI [https://doi.org/10.4310/CDM.1997.v1997.n1.a2, DOI 10.4310/CDM.1997.V1997.N1.A2]; Gilboa G, 2007, MULTISCALE MODEL SIM, V6, P595, DOI 10.1137/060669358; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Kenkre V.M., 1973, J STAT PHYS, V9, P45, DOI 10.1007/BF01016796; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Liu W., 2010, P 27 INT C MACH LEAR, P679; Liu W., 2009, CVPR; Liu W, 2012, P IEEE, V100, P2624, DOI 10.1109/JPROC.2012.2197809; Nair V., 2010, ICML, P807; Pipiras V, 2017, CA ST PR MA, DOI 10.1017/9781139600347; Silling SA, 2000, J MECH PHYS SOLIDS, V48, P175, DOI 10.1016/S0022-5096(99)00029-0; Tadmor E., 2015, SIAM NEWS, V48; Willinger W, 2003, THEORY AND APPLICATIONS OF LONG-RANGE DEPENDENCE, P373; Wu B, 2019, INT J FOOD SCI NUTR, V70, P432, DOI 10.1080/09637486.2018.1529147; Zhao YX, 2016, COMPUT MATH APPL, V71, P2497, DOI 10.1016/j.camwa.2015.09.030	29	5	5	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300046
C	Umenberger, J; Schon, TB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Umenberger, Jack; Schon, Thomas B.			Learning convex bounds for linear quadratic control policy synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RANDOMIZED ALGORITHMS; SYSTEM-IDENTIFICATION; SAFE	Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a number of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.	[Umenberger, Jack; Schon, Thomas B.] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden	Uppsala University	Umenberger, J (corresponding author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.	jack.umenberger@it.uu.se; thomas.schon@it.uu.se	; Schon, Thomas/D-4169-2009	Umenberger, Jack/0000-0001-6946-1508; Schon, Thomas/0000-0001-5183-234X	Swedish Foundation for Strategic Research (SSF) - Swedish Research Council [RIT15-0012]; project Learning flexible models for nonlinear dynamics - Swedish Research Council [2017-03807]; project NewLEADS-New Directions in Learning Dynamical Systems - Swedish Research Council [621-2016-06079]	Swedish Foundation for Strategic Research (SSF) - Swedish Research Council(Swedish Research CouncilSwedish Foundation for Strategic Research); project Learning flexible models for nonlinear dynamics - Swedish Research Council; project NewLEADS-New Directions in Learning Dynamical Systems - Swedish Research Council	This research was financially supported by the Swedish Foundation for Strategic Research (SSF) via the project ASSEMBLE (contract number: RIT15-0012) and via the projects Learning flexible models for nonlinear dynamics (contract number: 2017-03807) and NewLEADS-New Directions in Learning Dynamical Systems (contract number: 621-2016-06079), both funded by the Swedish Research Council.	Abbeel P, 2005, ICML ACM INT C PROCE, V119, P1, DOI 10.1145/1102351.1102352; Amodei D., 2016, CONCRETE PROBLEMS AI; [Anonymous], 2018, ARXIV180208334; [Anonymous], 1998, ESSENTIALS ROBUST CO; Aswani A, 2013, AUTOMATICA, V49, P1216, DOI 10.1016/j.automatica.2013.02.003; Berkenkamp F., 2017, ADV NEURAL INFORM PR; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; Bobiti R, 2016, 2016 EUROPEAN CONTROL CONFERENCE (ECC), P561, DOI 10.1109/ECC.2016.7810344; Boczar R., 2018, ARXIV180309186; Burl, 1998, LINEAR OPTIMAL CONTR; Calafiore GC, 2006, IEEE T AUTOMAT CONTR, V51, P742, DOI 10.1109/TAC.2006.875041; Calafiore GC, 2011, AUTOMATICA, V47, P1279, DOI 10.1016/j.automatica.2011.02.029; CARTER CK, 1994, BIOMETRIKA, V81, P541; CHEN J, 1993, PROCEEDINGS OF THE 32ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P68, DOI 10.1109/CDC.1993.325188; Chen J, 2000, CONTROL ORIENTED SYS, V19; Cheung SH, 2009, J ENG MECH, V135, P243, DOI 10.1061/(ASCE)0733-9399(2009)135:4(243); Ching J, 2005, P 9 INT C STRUCT SAF, P2609; Dean S., 2017, ARXIV171001688; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; DEOliveira Mauricio, 2009, MAE 280B LINEAR CONT; Depeweg S., 2017, ARXIV171007283; DOYLE J, 1994, IEEE T AUTOMAT CONTR, V39, P1575, DOI 10.1109/9.310031; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Geibel P, 2005, J ARTIF INTELL RES, V24, P81, DOI 10.1613/jair.1666; Gu S., 2017, INT C LEARN REPR; Gu SX, 2016, PR MACH LEARN RES, V48; HADDAD WM, 1991, SYST CONTROL LETT, V16, P235, DOI 10.1016/0167-6911(91)90011-3; HELMICKI AJ, 1991, IEEE T AUTOMAT CONTR, V36, P1163, DOI 10.1109/9.90229; Khalil H., 1996, NONLINEAR SYSTEMS, Vsecond; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Liu Z, 2009, SIAM J MATRIX ANAL A, V31, P1235, DOI 10.1137/090755436; Mihatsch O, 2002, MACH LEARN, V49, P267, DOI 10.1023/A:1017940631555; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ninness B, 2010, AUTOMATICA, V46, P40, DOI 10.1016/j.automatica.2009.10.015; Ostafew CJ, 2016, INT J ROBOT RES, V35, P1547, DOI 10.1177/0278364916645661; Petersen IR, 2000, IEEE T AUTOMAT CONTR, V45, P398, DOI 10.1109/9.847720; Petersen K.B., 2012, MATRIX COOKBOOK; Postlethwaite I, 2007, ANNU REV CONTROL, V31, P27, DOI 10.1016/j.arcontrol.2007.02.003; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Ross I. M., 2014, 24 INT S SPAC FLIGHT; Ross IM, 2015, J GUID CONTROL DYNAM, V38, P1251, DOI 10.2514/1.G000505; Schulman John, 2015, ARXIV150602438; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tempo R, 2012, RANDOMIZED ALGORITHM; Tu S., 2017, ARXIV170704791; Tu Stephen, 2017, ARXIV171208642; Vaida F, 2005, STAT SINICA, V15, P831; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Vidyasagar M, 1998, IEEE CONTR SYST MAG, V18, P69, DOI 10.1109/37.736014; Vidyasagar M, 2001, AUTOMATICA, V37, P1515, DOI 10.1016/S0005-1098(01)00122-4; Vidyasagar M., 2002, THEORY LEARNING GEN; Vinogradska J, 2016, PR MACH LEARN RES, V48; Wang Y.-S., 2016, ARXIV161004815; Wills A., 2012, IFAC PROC VOL, V45, P203	57	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004015
C	Upadhyay, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Upadhyay, Jalaj			The Price of Privacy for Low-rank Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PRINCIPAL COMPONENT ANALYSIS; APPROXIMATION; ALGORITHMS	In this paper, we study what price one has to pay to release differentially private low-rank factorization of a matrix. We consider various settings that are close to the real world applications of low-rank factorization: (i) the manner in which matrices are updated (row by row or in an arbitrary manner), (ii) whether matrices are distributed or not, and (iii) how the output is produced (once at the end of all updates, also known as one-shot algorithms or continually). Even though these settings are well studied without privacy, surprisingly, there are no private algorithm for these settings (except when a matrix is updated row by row). We present the first set of differentially private algorithms for all these settings. Our algorithms when private matrix is updated in an arbitrary manner promise differential privacy with respect to two stronger privacy guarantees than previously studied, use space and time comparable to the non-private algorithm, and achieve optimal accuracy. To complement our positive results, we also prove that the space required by our algorithms is optimal up to logarithmic factors. When data matrices are distributed over multiple servers, we give a non-interactive differentially private algorithm with communication cost independent of dimension. In concise, we give algorithms that incur optimal cost across all parameters of interest. We also perform experiments to verify that all our algorithms perform well in practice and outperform the best known algorithm until now for large range of parameters.	[Upadhyay, Jalaj] Johns Hopkins Univ, Baltimore, MD 21201 USA	Johns Hopkins University	Upadhyay, J (corresponding author), Johns Hopkins Univ, Baltimore, MD 21201 USA.	jalaj@jhu.edu			NSF BIGDATA [IIS-1838139, IIS-1447700, IIS-154648]	NSF BIGDATA	The author would like to thank Adam Smith for useful feedback on this paper. This research was supported in part by NSF BIGDATA grant IIS-1447700, NSF BIGDATA grant IIS-154648, and NSF BIGDATA grant IIS-1838139.	Achlioptas D, 2005, LECT NOTES COMPUT SC, V3559, P458, DOI 10.1007/11503415_31; Achlioptas D, 2001, ANN IEEE SYMP FOUND, P500; [Anonymous], WALL STREET J; Arora Raman, 2018, ADV NEURAL INFORM PR, P4141; Azar Y., 2001, P 33 ANN ACM S THEOR, P619; Bai ZJ, 2005, LECT NOTES COMPUT SC, V3756, P471; Bhatia R., 1997, MATRIX ANAL, DOI [10.1007/978-1-4612-0653-8, DOI 10.1007/978-1-4612-0653-8]; Blocki J, 2012, ANN IEEE SYMP FOUND, P410, DOI 10.1109/FOCS.2012.67; Boutsidis C, 2016, ACM S THEORY COMPUT, P236, DOI 10.1145/2897518.2897646; Bun Mark, 2017, ARXIV171104740; Chaudhuri K, 2012, ADV NEURAL INFORM PR, P989; Clarkson KL, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2061; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Deshpande A, 2006, LECT NOTES COMPUT SC, V4110, P292; Dinur I., 2003, P 22 ACM SIGMOD SIGA, P202, DOI DOI 10.1145/773153.773173; Drineas P, 2004, MACH LEARN, V56, P9, DOI 10.1023/B:MACH.0000033113.59016.96; Drineas Petros, 2002, P 34 ACM S THEORY CO, P82, DOI [10.1145/509907.509922, DOI 10.1145/509907.509922]; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2010, ACM S THEORY COMPUT, P715; Dwork Cynthia, 2014, STOC, P11, DOI DOI 10.1145/2591796.2591883; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Evfimievski A. V., 2003, P 22 ACM SIGMOD SIGA, P211, DOI DOI 10.1145/773153.773174; Garber Dan, 2017, ARXIV170208169; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hardt M, 2014, ADV NEUR IN, V27; Hardt M, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1255; Hardt M, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P331; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Jiang Wuxuan, 2015, ARXIV151105680; Kannan R, 2005, LECT NOTES COMPUT SC, V3559, P444, DOI 10.1007/11503415_30; Kapralov M., 2013, SODA, V5, P1; Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140; Le Borgne YA, 2008, SENSORS-BASEL, V8, P4821, DOI 10.3390/s8084821; Li Y, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P174, DOI 10.1145/2591796.2591812; Liang Y., 2014, ADV NEURAL INFORM PR, P3113; Magen A, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1422; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Markovsky I, 2008, AUTOMATICA, V44, P891, DOI 10.1016/j.automatica.2007.09.011; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Musco Cameron, 2017, ARXIV170403371; Muthukrishnan S., 2005, DATA STREAMS ALGORIT; Narayanan A, 2008, P IEEE S SECUR PRIV, P111, DOI 10.1109/SP.2008.33; Nguyen NH, 2009, ACM S THEORY COMPUT, P215; Papadimitriou C. H., 1998, Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1998, P159, DOI 10.1145/275487.275505; Poulson J, 2013, ACM T MATH SOFTWARE, V39, DOI 10.1145/2427023.2427030; Qu Y., 2002, WORK HIGH PERFORM DA; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35; Tisseur F, 1999, SIAM J SCI COMPUT, V20, P2223, DOI 10.1137/S1064827598336951; Upadhyay J, 2013, LECT NOTES COMPUT SC, V8269, P276, DOI 10.1007/978-3-642-42033-7_15; Upadhyay Jalaj, 2014, ARXIV14102470; Upadhyay Jalaj, 2014, ARXIV14095414; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137	59	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304021
C	Valera, I; Singla, A; Gomez-Rodriguez, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Valera, Isabel; Singla, Adish; Gomez-Rodriguez, Manuel			Enhancing the Accuracy and Fairness of Human Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Societies often rely on human experts to take a wide variety of decisions affecting their members, from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken by academics. In this context, each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However, these decisions may be imperfect due to limited experience, implicit biases, or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions? In this paper, we address the above problem from the perspective of sequential decision making and show that, for different fairness notions in the literature, it reduces to a sequence of (constrained) weighted bipartite matchings, which can be solved efficiently using algorithms with approximation guarantees. Moreover, these algorithms also benefit from posterior sampling to actively trade off exploitation-selecting expert assignments which lead to accurate and fair decisions-and exploration-selecting expert assignments to learn about the experts' preferences. We demonstrate the effectiveness of our algorithms on both synthetic and real-world data and show that they can significantly improve both the accuracy and fairness of the decisions taken by pools of experts.	[Valera, Isabel] MPI Intelligent Syst, Tubingen, Germany; [Singla, Adish; Gomez-Rodriguez, Manuel] MPI SWS, Saarbrucken, Germany; [Valera, Isabel] Max Planck Inst Intelligent Systems, Max Planck Ring 4, D-472076 Tubingen, Germany; [Singla, Adish] Max Planck Inst Software Syst MPI SWS, Campus E1 5, D-66123 Saarbrucken, Germany; [Gomez-Rodriguez, Manuel] Max Planck Inst Software Syst, Paul Ehrlich Str G26, D-67663 Kaiserslautern, Germany	Max Planck Society; Max Planck Society; Max Planck Society	Valera, I (corresponding author), MPI Intelligent Syst, Tubingen, Germany.; Valera, I (corresponding author), Max Planck Inst Intelligent Systems, Max Planck Ring 4, D-472076 Tubingen, Germany.	ivalera@tue.mpg.de; adishs@mpi-sws.org; manuelgr@mpi-sws.org	Rodriguez, Manuel Gomez/AAB-5005-2021; Singla, Adish/ABG-8960-2021	Gomez Rodriguez, Manuel/0000-0003-3930-1161; Singla, Adish/0000-0001-9922-0668	MPG Minerva Fast Track Grant	MPG Minerva Fast Track Grant	Isabel Valera acknowledges funding from a MPG Minerva Fast Track Grant.	Barocas S., 2016, CALIFORNIA LAW REV; Boyd S, 2004, CONVEX OPTIMIZATION; Corbett-Davies S., 2017, KDD; Dwork Cynthia, 2012, ITCS; Feldman M., 2015, KDD; Hardt M., 2016, NIPS; Kleinberg J, 2018, Q J ECON, V133, P237, DOI 10.1093/qje/qjx032; Mastrolilli Monaldo, 2012, Combinatorial Optimization. Second International Symposium, ISCO 2012. Revised Selected Papers, P344, DOI 10.1007/978-3-642-32147-4_31; Megan D. P. C., 2016, BIG DATA REPORT ALGO; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; West D. B., 2001, INTRO GRAPH THEORY, V2; Zafar B., 2017, AISTATS; Zafar B., 2017, WWW; Zafar B., 2017, NIPS	14	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301073
C	van der Wilk, M; Bauer, M; John, ST; Hensman, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		van der Wilk, Mark; Bauer, Matthias; John, S. T.; Hensman, James			Learning Invariances using the Marginal Likelihood	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generalising well in supervised learning tasks relies on correctly extrapolating the training data to a large region of the input space. One way to achieve this is to constrain the predictions to be invariant to transformations of the input that are known to be irrelevant (e.g. translation). Commonly, this is done through data augmentation, where the training set is enlarged by applying hand-crafted transformations to the inputs. We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by backpropagating through it to maximise the marginal likelihood.	[van der Wilk, Mark; John, S. T.; Hensman, James] PROWLER Io, Cambridge, England; [Bauer, Matthias] Univ Cambridge, MPI Intelligent Syst, Cambridge, England	Max Planck Society; University of Cambridge	van der Wilk, M (corresponding author), PROWLER Io, Cambridge, England.	mark@prowler.io; msb55@cam.ac.uk; st@prowler.io; james@prowler.io		Bauer, Matthias/0000-0001-7040-2054; John, ST/0000-0002-4540-395X	Qualcomm studentship in technology	Qualcomm studentship in technology	M.B. gratefully acknowledges partial funding through a Qualcomm studentship in technology.	Antoniou Antreas, 2017, ARXIV171104340; Bauer M, 2016, ADV NEUR IN, V29; Beymer D., 1995, P IEEE INT C COMP VI; Chapelle O., 2002, ADV NEURAL INFORM PR, V14; Cohen TS, 2016, PR MACH LEARN RES, V48; Dao T., 2018, ARXIV180306084; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Figueiras-Vidal A., 2009, ADV NEURAL INFORM PR, V22; Germain P, 2016, ADV NEUR IN, V29; Gibbs MN, 2000, IEEE T NEURAL NETWOR, V11, P1458, DOI 10.1109/72.883477; Ginsbourger D., 2012, ANN FACULTE SCI TOUL; Ginsbourger D., 2013, ARXIV13081359; Ginsbourger D, 2016, J STAT PLAN INFER, V170, P117, DOI 10.1016/j.jspi.2015.10.002; Graepel T., 2004, ADV NEURAL INFORM PR, V16; Hauberg S., 2016, P 19 INT C ART INT S; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J, 2015, ADV NEUR IN, V28; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Jaderberg M, 2015, ADV NEUR IN, V28; Kim H., 2018, P 21 INT C ARTIFICIA; Kingma D.P, P 3 INT C LEARNING R; Kondor Risi Imre, 2008, THESIS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Linderman SW, 2015, ADV NEUR IN, V28; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133; MacKay D. J. C., 2002, INFORM THEORY INFERE, P343; Matthews Alexander G, 2016, P 19 INT C ART INT S; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Raj A., 2017, P 20 INT C ART INT S; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; RASMUSSEN CE, 2001, ADV NEURAL INFORM PR, V13; Scholkopf B., 1998, ADV NEURAL INFORM PR, V10; Seeger Matthias, 2003, THESIS; Simard P., 1992, ADV NEURAL INFORM PR, V4; Simard PY, 2000, INT J IMAG SYST TECH, V11, P181, DOI 10.1002/1098-1098(2000)11:3<181::AID-IMA1003>3.0.CO;2-E; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; van der Wilk M, 2017, ADV NEUR IN, V30; Wenzel F., 2018, ARXIV180206383	45	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004049
C	Wang, Q; Xiong, JC; Han, L; Sun, P; Liu, H; Zhang, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Qing; Xiong, Jiechao; Han, Lei; Sun, Peng; Liu, Han; Zhang, Tong			Exponentially Weighted Imitation Learning for Batched Historical Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider deep policy learning with only batched historical trajectories. The main challenge of this problem is that the learner no longer has a simulator or "environment oracle" as in most reinforcement learning settings. To solve this problem, we propose a monotonic advantage reweighted imitation learning strategy that is applicable to problems with complex nonlinear function approximation and works well with hybrid (discrete and continuous) action space. The method does not rely on the knowledge of the behavior policy, thus can be used to learn from data generated by an unknown policy. Under mild conditions, our algorithm, though surprisingly simple, has a policy improvement bound and outperforms most competing methods empirically. Thorough numerical results are also provided to demonstrate the efficacy of the proposed methodology.	[Wang, Qing; Xiong, Jiechao; Han, Lei; Sun, Peng; Liu, Han; Zhang, Tong] Tencent AI Lab, Shenzhen, Peoples R China; [Liu, Han] Northwestern Univ, Evanston, IL 60208 USA	Tencent; Northwestern University	Wang, Q (corresponding author), Tencent AI Lab, Shenzhen, Peoples R China.	drwang@tencent.com; jcxiong@tencent.com; lxhan@tencent.com; pythonsun@tencent.com; hanliu@northwestern.edu; tongzhang@tongzhang-ml.org	Zhang, Tong/HGC-1090-2022					Abbeel P., 2004, P 21 INT C MACHINE L, P1; Abdolmaleki Abbas, 2018, INT C LEARN REPR; Achiam J, 2017, PR MACH LEARN RES, V70; [Anonymous], 2016, ARXIV161101224; Azar MG, 2012, J MACH LEARN RES, V13, P3207; Bain M., 1999, MACHINE INTELLIGENCE, V15, P103; Coulom Remi, 2005, BAYESIAN ELO RATING; Csiszar Imre, 2011, INFORM THEORY CODING, VSecond; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Geist M, 2014, J MACH LEARN RES, V15, P289; Guo X, 2014, ADV NEURAL INFORM PR, P3338; Hausknecht M., 2016, P INT C LEARN REPR I; Hausknecht Matthew, 2017, ROBOCUP 2D HALF FIEL; Hester T., 2018, AAAI C ART INT; Jiang Daniel, 2018, P 35 INT C MACH LEAR, V80, P2284; Jiang N, 2016, PR MACH LEARN RES, V48; Kakade S., 2002, P 19 INT C MACH LEAR; Lillicrap T. P., 2016, P 4 INT C LEARN REPR; Mnih V., 2013, PLAYING ATARI DEEP R, P1; Munos R, 2016, P 30 INT C NEUR INF; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Petrik M., 2016, ADV NEURAL INFORM PR, V29, P2298; Pirotta M., 2013, INT C MACH LEARN, P307; Precup D., 2000, INT C MACH LEARN, P759; Ross St<prime>ephane, 2011, AISTATS; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simonyan K., 2015, ICLR; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Thomas P., 2016, INT C MACH LEARN, P2139; Thomas PS, 2015, PR MACH LEARN RES, V37, P2380; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; Wu Y., 2017, NIPS 2017, P5279; Wymann B., 2014, TORCS OPEN RACING CA; Xiong J., 2018, ARXIV181006394	36	5	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000076
C	Whittington, JCR; Muller, TH; Mark, S; Barry, C; Behrens, TEJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Whittington, James C. R.; Muller, Timothy H.; Mark, Shirley; Barry, Caswell; Behrens, Timothy E. J.			Generalisation of structural knowledge in the hippocampal-entorhinal system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONCEPTUAL KNOWLEDGE; SPATIAL MAP; MEMORY; REPRESENTATIONS; CELLS; RATS	A central problem to understanding intelligence is the concept of generalisation. This allows previously learnt structure to be exploited to solve tasks in novel situations differing in their particularities. We take inspiration from neuroscience, specifically the hippocampal-entorhinal system known to be important for generalisation. We propose that to generalise structural knowledge, the representations of the structure of the world, i.e. how entities in the world relate to each other, need to be separated from representations of the entities themselves. We show, under these principles, artificial neural networks embedded with hierarchy and fast Hebbian memory, can learn the statistics of memories and generalise structural knowledge. Spatial neuronal representations mirroring those found in the brain emerge, suggesting spatial cognition is an instance of more general organising principles. We further unify many entorhinal cell types as basis functions for constructing transition graphs, and show these representations effectively utilise memories. We experimentally support model assumptions, showing a preserved relationship between entorhinal grid and hippocampal place cells across environments.	[Whittington, James C. R.; Muller, Timothy H.; Behrens, Timothy E. J.] Univ Oxford, Oxford, England; [Mark, Shirley; Barry, Caswell] UCL, London, England	University of Oxford; University of London; University College London	Whittington, JCR; Muller, TH (corresponding author), Univ Oxford, Oxford, England.	james.whittington@magd.ox.ac.uk; timothymuller127@gmail.com; s.mark@ucl.ac.uk; caswell.barry@ucl.ac.uk; behrens@fmrib.ox.ac.uk			Wellcome Trust Senior Research Fellowship [WT104765MA]; James S. McDonnell Foundation Award [JSMF220020372]; EPSRC scholarship; MRC scholarship	Wellcome Trust Senior Research Fellowship(Wellcome Trust); James S. McDonnell Foundation Award; EPSRC scholarship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); MRC scholarship(UK Research & Innovation (UKRI)Medical Research Council UK (MRC))	We acknowledge funding from a Wellcome Trust Senior Research Fellowship (WT104765MA) together with a James S. McDonnell Foundation Award (JSMF220020372) to TEJB, MRC scholarship to THM, and an EPSRC scholarship to JCRW.	Aronov D, 2017, NATURE, V543, P719, DOI 10.1038/nature21692; Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6; Barry C, 2012, P NATL ACAD SCI USA, V109, P17687, DOI 10.1073/pnas.1209918109; Bostock E, 1991, Hippocampus, V1, P193, DOI 10.1002/hipo.450010207; Buckmaster CA, 2004, J NEUROSCI, V24, P9811, DOI 10.1523/JNEUROSCI.1532-04.2004; Bush D, 2015, NEURON, V87, P507, DOI 10.1016/j.neuron.2015.07.006; Constantinescu AO, 2016, SCIENCE, V352, P1464, DOI 10.1126/science.aaf0941; Cueva Christopher J., 2018, EMERGENCE GRID LIKE, P1; Deshmukh SS, 2013, HIPPOCAMPUS, V23, P253, DOI 10.1002/hipo.22101; Dordek Y, 2016, ELIFE, V5, DOI 10.7554/eLife.10094; Dusek JA, 1997, P NATL ACAD SCI USA, V94, P7109, DOI 10.1073/pnas.94.13.7109; Fyhn M, 2007, NATURE, V446, P190, DOI 10.1038/nature05601; Gemici Mevlana, 2017, GENERATIVE TEMPORAL, P1; Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721; Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011; Hassabis D, 2007, P NATL ACAD SCI USA, V104, P1726, DOI 10.1073/pnas.0610561104; Hoydal Oyvind Arne, 2018, BIORXIV, DOI [10.1101/286286, DOI 10.1101/286286]; Jimenez Rezende Danilo, 2014, STOCHASTIC BACKPROPA, DOI [10.1051/0004-6361/201527329, DOI 10.1051/0004-6361/201527329]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kjelstrup KB, 2008, SCIENCE, V321, P140, DOI 10.1126/science.1157086; Komorowski RW, 2009, J NEUROSCI, V29, P9918, DOI 10.1523/JNEUROSCI.1378-09.2009; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krupic J, 2012, SCIENCE, V337, P853, DOI 10.1126/science.1222403; Kumaran D, 2009, NEURON, V63, P889, DOI 10.1016/j.neuron.2009.07.030; Lei Ba Jimmy, 2016, ADV NEURAL INFORM PR, P1; Leutgeb Stefan, 2005, INDEPENDENT CODES SP, P619; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Monaco JD, 2011, J NEUROSCI, V31, P9414, DOI 10.1523/JNEUROSCI.1433-11.2011; OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1; Solstad T, 2008, SCIENCE, V322, P1865, DOI 10.1126/science.1166466; Stachenfeld KL, 2017, NAT NEUROSCI, V20, P1643, DOI 10.1038/nn.4650; Stemmler M, 2015, SCI ADV, V1, DOI 10.1126/science.1500816; Stensola H, 2012, NATURE, V492, P72, DOI 10.1038/nature11649; TAUBE JS, 1990, J NEUROSCI, V10, P420; TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626; Tsao A, 2018, NATURE, V561, P57, DOI 10.1038/s41586-018-0459-6; Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949; Wood ER, 1999, NATURE, V397, P613, DOI 10.1038/17605	39	5	5	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003008
C	Wu, AQ; Pashkovski, SL; Datta, SR; Pillow, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Anqi; Pashkovski, Stan L.; Datta, Sandeep Robert; Pillow, JonathanW.			Learning a latent manifold of odor representations from neural responses in piriform cortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A major difficulty in studying the neural mechanisms underlying olfactory perception is the lack of obvious structure in the relationship between odorants and the neural activity patterns they elicit. Here we use odor-evoked responses in piriform cortex to identify a latent manifold specifying latent distance relationships between olfactory stimuli. Our approach is based on the Gaussian process latent variable model, and seeks to map odorants to points in a low-dimensional embedding space, where distances between points in the embedding space relate to the similarity of population responses they elicit. The model is specified by an explicit continuous mapping from a latent embedding space to the space of high-dimensional neural population firing rates via nonlinear tuning curves, each parametrized by a Gaussian process. Population responses are then generated by the addition of correlated, odor-dependent Gaussian noise. We fit this model to large-scale calcium fluorescence imaging measurements of population activity in layers 2 and 3 of mouse piriform cortex following the presentation of a diverse set of odorants. The model identifies a low-dimensional embedding of each odor, and a smooth tuning curve over the latent embedding space that accurately captures each neuron's response to different odorants. The model captures both signal and noise correlations across more than 500 neurons. We validate the model using a cross-validation analysis known as co-smoothing to show that the model can accurately predict the responses of a population of held-out neurons to test odorants.	[Wu, Anqi; Pillow, JonathanW.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Pashkovski, Stan L.; Datta, Sandeep Robert] Harvard Med Sch, Dept Neurobiol, Boston, MA 02115 USA	Princeton University; Harvard University; Harvard Medical School	Wu, AQ (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	anqiw@princeton.edu; pashkovs@hms.harvard.edu; srdatta@hms.harvard.edu; pillow@princeton.edu		wu, anqi/0000-0002-7866-9455; Datta, Sandeep/0000-0002-8068-3862	Simons Foundation [SCGB AWD1004351, AWD543027]; NIH [R01EY017366, R01NS104899]; U19 NIH-NINDS BRAIN Initiative Award [NS104648-01]	Simons Foundation; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); U19 NIH-NINDS BRAIN Initiative Award	This work was supported by grants from the Simons Foundation (SCGB AWD1004351 and AWD543027), the NIH (R01EY017366, R01NS104899) and a U19 NIH-NINDS BRAIN Initiative Award (NS104648-01).	Archer E., 2015, ARXIV PREPRINT ARXIV; Cox T., 2000, MULTIDIMENSIONAL SCA; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Gao YJ, 2016, ADV NEUR IN, V29; Haddad R, 2008, NAT METHODS, V5, P425, DOI 10.1038/NMETH.1197; Hardcastle K, 2017, NEURON, V94, P375, DOI 10.1016/j.neuron.2017.03.025; Koulakov AA, 2011, FRONT SYST NEUROSCI, V5, DOI 10.3389/fnsys.2011.00065; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Mamlouk AM, 2003, NEUROCOMPUTING, V52-4, P591, DOI 10.1016/S0925-2312(02)00805-6; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Meister MLR, 2013, J NEUROSCI, V33, P2254, DOI 10.1523/JNEUROSCI.2984-12.2013; Park IM, 2014, NAT NEUROSCI, V17, P1395, DOI 10.1038/nn.3800; Rakitsch B., 2013, PROC 26 INT C NEURAL, V26, P1466; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Raposo David, 2014, NATURE NEUROSCIENCE; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rigotti M, 2013, NATURE, V497, P585, DOI 10.1038/nature12160; Stegle Oliver, 2011, ADV NEURAL INFORM PR, P630; Sussillo D., 2016, ARXIV160806315; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Whiteway MR, 2017, J NEUROPHYSIOL, V117, P919, DOI 10.1152/jn.00698.2016; Wu A., 2017, ADV NEURAL INFORM PR, V30, P3499; Zhao Yuan, 2017, NEURAL COMPUTATION; Zhao Yuan, 2017, ARXIV170709049; Zhe Chen, 2018, DYNAMIC NEUROSCIENCE, P53	25	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305040
C	Wynen, D; Schmid, C; Mairal, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wynen, Daan; Schmid, Cordelia; Mairal, Julien			Unsupervised Learning of Artistic Styles with Archetypal Style Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MATRIX FACTORIZATION; MODEL	In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to neural style representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes.	[Wynen, Daan; Schmid, Cordelia; Mairal, Julien] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France; Univ Grenoble Alpes, Inst Engn, Grenoble, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria; Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA)	Wynen, D (corresponding author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.	daan.wynen@inria.fr; cordelia.schmid@inria.fr; julien.mairal@inria.fr	Mairal, Julien/AAL-5611-2021		ANR (MACARON project) [ANR-14-CE23-0003-01]; ERC [714381]; ERC advanced grant Allegro	ANR (MACARON project)(French National Research Agency (ANR)); ERC(European Research Council (ERC)European Commission); ERC advanced grant Allegro	This work was supported by a grant from ANR (MACARON project ANR-14-CE23-0003-01), by the ERC grant number 714381 (SOLARIS project) and the ERC advanced grant Allegro.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chen D., 2017, P C COMP VIS PATT RE; Chen Y., 2014, C COMP VIS PATT REC; CUTLER A, 1994, TECHNOMETRICS, V36, P338, DOI 10.2307/1269949; Dumoulin Vincent, 2017, ICLR; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Ghiasi G., 2017, P BRIT MACH VIS C BM; Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648; Huang X., 2017, P INT C COMP VIS ICC; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Li YT, 2017, ADV NEUR IN, V30; Luan F., 2017, P C COMP VIS PATT RE; Mairal J, 2012, FOUND TRENDS COMPUT, V8, DOI 10.1561/0600000058; Mairal J, 2010, J MACH LEARN RES, V11, P19; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; PAATERO P, 1994, ENVIRONMETRICS, V5, P111, DOI 10.1002/env.3170050203; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Ruder M., 2018, INT J COMPUTER VISIO; Ulyanov D, 2016, PR MACH LEARN RES, V48; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Yeh M.-C., 2018, ARXIV180400118	24	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001015
C	Xu, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Kuang			Query Complexity of Bayesian Private Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BINARY SEARCH	We study the query complexity of Bayesian Private Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary? Our main result is a query complexity lower bound that is tight up to the first order. We show that if the learner wants to estimate the target within an error of., while ensuring that no adversary estimator can achieve a constant additive error with probability greater than 1/L, then the query complexity is on the order of L log(1/epsilon) as epsilon -> 0. Our result demonstrates that increased privacy, as captured by L, comes at the expense of a multiplicative increase in query complexity. The proof builds on Fano's inequality and properties of certain proportional-sampling estimators.	[Xu, Kuang] Stanford Grad Sch Business, Stanford, CA 94305 USA	Stanford University	Xu, K (corresponding author), Stanford Grad Sch Business, Stanford, CA 94305 USA.	kuangxu@stanford.edu						Ben Or M, 2008, ANN IEEE SYMP FOUND, P221, DOI 10.1109/FOCS.2008.58; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Cover TM, 2006, ELEMENTS INFORM THEO; Cummings R, 2016, OPER RES, V64, P67, DOI 10.1287/opre.2015.1458; Duchi J.C., 2012, ADV NEURAL INFORM PR, V25, P1430; Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1; Fanti Giulia, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P271, DOI 10.1145/2745844.2745866; HORSTEIN M, 1963, IEEE T INFORM THEORY, V9, P136, DOI 10.1109/TIT.1963.1057832; Jain P., 2012, P 25 ANN C LEARNING, V23; Lindell Y., 2009, J PRIVACY CONFIDENTI, V1, P59, DOI DOI 10.29012/JPC.V1I1.566; RIVEST RL, 1980, J COMPUT SYST SCI, V20, P396, DOI 10.1016/0022-0000(80)90014-8; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Tsitsiklis JN, 2018, OPER RES, V66, P587, DOI 10.1287/opre.2017.1682; Tsitsiklis John N, 2018, C LEARN THEOR COLT; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Waeber R, 2013, SIAM J CONTROL OPTIM, V51, P2261, DOI 10.1137/120861898; Xu Z., 2017, THESIS	17	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302044
C	Xue, YX; Yuan, Y; Xu, ZT; Sabharwal, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xue, Yexiang; Yuan, Yang; Xu, Zhitian; Sabharwal, Ashish			Expanding Holographic Embeddings for Knowledge Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural models operating over structured spaces such as knowledge graphs require a continuous embedding of the discrete elements of this space (such as entities) as well as the relationships between them. Relational embeddings with high expressivity, however, have high model complexity, making them computationally difficult to train. We propose a new family of embeddings for knowledge graphs that interpolate between a method with high model complexity and one, namely Holographic embeddings (HOLE), with low dimensionality and high training efficiency. This interpolation, termed HOLEX, is achieved by concatenating several linearly perturbed copies of original HOLE. We formally characterize the number of perturbed copies needed to provably recover the full entity-entity or entity-relation interaction matrix, leveraging ideas from Haar wavelets and compressed sensing. In practice, using just a handful of Haar-based or random perturbation vectors results in a much stronger knowledge completion system. On the Freebase FB15K dataset, HOLEX outperforms originally reported HOLE by 14.7% on the HITS @10 metric, and the current path-based state-of-the-art method, PTransE, by 4% (absolute).	[Xue, Yexiang; Xu, Zhitian] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA; [Yuan, Yang] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Sabharwal, Ashish] Allen Inst Artificial Intelligence AI2, Seattle, WA USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Cornell University	Xue, YX (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.							Abadi Martin, 2015, CORR; Bizer C, 2009, J WEB SEMANT, V7, P154, DOI 10.1016/j.websem.2009.07.002; Bordes A., 2013, ADV NEURAL INFORM PR; Bordes Antoine, 2012, AISTATS; Chui CK, 1992, INTRO WAVELETS; Dettmers T., 2018, AAAI; Dong XL, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P601, DOI 10.1145/2623330.2623623; Evans C., 2008, P 2008 ACM SIGMOD IN, P1247, DOI [DOI 10.1145/1376616.1376746, 10.1145/1376616]; Guu K., 2015, EMNLP; Haar Alfred, 1910, MATH ANN; Hoffart J., 2011, WWW, P229, DOI [10.1145/1963192.1963296, DOI 10.1145/1963192.1963296]; Jenatton R., 2012, P NIPS, P3176; Kadlec Rudolf, 2017, REP4NLP WORKSH ACL; Lacroix T., 2018, ICML; Lin Y., 2015, AAAI; Lin Yankai, 2015, EMNLP; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M, 2016, AAAI CONF ARTIF INTE, P1955; Rauhut H, 2010, THEORETICAL FDN NUME, V9, P1, DOI DOI 10.1515/9783110226157.1; Salehi Farnood, 2018, NEURIPS 2018 S ADV A; Shen Yujun, 2018, CORR; Shi BX, 2017, AAAI CONF ARTIF INTE, P1236; Socher R., 2013, ADV NEURAL INFORM PR, V26; Socher Richard, 2012, EMNLP CONLL; Tandon N., 2014, WSDM; Trouillon T, 2016, PR MACH LEARN RES, V48; Wang Z., 2014, AAAI; Yang B., 2015, ICLR	29	5	5	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304050
C	Yao, SY; Hsu, TMH; Zhu, JY; Wu, JJ; Torralba, A; Freeman, WT; Tenenbaum, JB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yao, Shunyu; Hsu, Tzu-Ming Harry; Zhu, Jun-Yan; Wu, Jiajun; Torralba, Antonio; Freeman, William T.; Tenenbaum, Joshua B.			3D-Aware Scene Manipulation via Inverse Graphics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous scene representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for semantics, geometry, and appearance into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object-wise representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance supports 3D-aware scene manipulation, e.g., rotating and moving objects freely while keeping the consistent shape and texture, and changing the object appearance without affecting its shape. Experiments demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.	[Yao, Shunyu] Tsinghua Univ, IIIS, Beijing, Peoples R China; [Yao, Shunyu; Hsu, Tzu-Ming Harry; Zhu, Jun-Yan; Wu, Jiajun; Torralba, Antonio; Tenenbaum, Joshua B.] MIT, CSAIL, Cambridge, MA 02139 USA; [Freeman, William T.; Tenenbaum, Joshua B.] MIT, CSAIL, Google Res, Cambridge, MA 02139 USA	Tsinghua University; Massachusetts Institute of Technology (MIT); Google Incorporated; Massachusetts Institute of Technology (MIT)	Yao, SY (corresponding author), Tsinghua Univ, IIIS, Beijing, Peoples R China.; Yao, SY (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.		Wu, JiaJun/GQH-7885-2022; Hsu, Tzu Ming/AAD-2649-2021		NSF [1231216, 1447476, 1524817]; ONR MURI [N00014-16-1-2007]; Toyota Research Institute; Facebook	NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); Toyota Research Institute; Facebook(Facebook Inc)	This work is supported by NSF #1231216, NSF #1447476, NSF #1524817, ONR MURI N00014-16-1-2007, Toyota Research Institute, and Facebook.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Barrow H. G., 1978, COMPUTER VISION SYST; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dosovitskiy Alexey, 2016, NEURIPS; Eigen David, 2014, NEURIPS; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Geiger A., 2012, P IEEE COMP SOC C CO; Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Janner M, 2017, ADV NEUR IN, V30; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191; Kato H., 2018, IEEE C COMP 6 VIS PA; Kingma D.P, P 3 INT C LEARNING R; Kirillov A., 2018, ARXIV180100868; Kulkarni TD, 2015, ADV NEUR IN, V28; Larsen ABL, 2016, PR MACH LEARN RES, V48; Liu Ming-Yu, 2017, NIPS; Mirza M., 2014, ARXIV; Mousavian Arsalan, 2017, IEEE; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezende DJ, 2016, ADV NEUR IN, V29; Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu JJ, 2017, ADV NEUR IN, V30; Wu JJ, 2017, PROC CVPR IEEE, P7035, DOI 10.1109/CVPR.2017.744; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yang Jimei, 2015, NIPS; Yu Fisher, 2017, CVPR; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zhu Jun-Yan, 2017, ICCV	54	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301084
C	Ahuja, K; Zame, WR; van der Schaar, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ahuja, Kartik; Zame, William R.; van der Schaar, Mihaela			DPSCREEN: Dynamic Personalized Screening	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				BREAST-CANCER; COLORECTAL-CANCER; COST-EFFECTIVENESS; WOMEN; RISK; PROBABILITIES; MAMMOGRAPHY; PREVENTION; UPDATE	Screening is important for the diagnosis and treatment of a wide variety of diseases. A good screening policy should be personalized to the features of the patient and to the dynamic history of the patient (including the history of screening). The growth of electronic health records data has led to the development of many models to predict the onset and progression of different diseases. However, there has been limited work to address the personalized screening for these different diseases. In this work, we develop the first framework to construct screening policies for a large class of disease models. The disease is modeled as a finite state stochastic process with an absorbing disease state. The patient observes an external information process (for instance, self-examinations, discovering comorbidities, etc.) which can trigger the patient to arrive at the clinician earlier than scheduled screenings. The clinician carries out the tests; based on the test results and the external information it schedules the next arrival. Computing the exactly optimal screening policy that balances the delay in the detection against the frequency of screenings is computationally intractable; this paper provides a computationally tractable construction of an approximately optimal policy. As an illustration, we make use of a large breast cancer data set. The constructed policy screens patients more or less often according to their initial risk - it is personalized to the features of the patient - and according to the results of previous screens - it is personalized to the history of the patient. In comparison with existing clinical policies, the constructed policy leads to large reductions (28-68%) in the number of screens performed while achieving the same expected delays in disease detection.	[Ahuja, Kartik; van der Schaar, Mihaela] Univ Calif Los Angeles, Elect & Comp Engn Dept, Los Angeles, CA 90095 USA; [Zame, William R.] Univ Calif Los Angeles, Econ Dept, Los Angeles, CA USA; [van der Schaar, Mihaela] Univ Oxford, Engn Sci Dept, Oxford, England	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; University of Oxford	Ahuja, K (corresponding author), Univ Calif Los Angeles, Elect & Comp Engn Dept, Los Angeles, CA 90095 USA.	ahujak@ucla.edu; zame@econ.ucla.edu; mihaela.vanderschaar@oxford-man.ox.ac.uk	Jeong, Yongwook/N-7413-2016		Office of Naval Research (ONR); National Science Foundation (NSF) [1533983, 1407712]	Office of Naval Research (ONR)(Office of Naval Research); National Science Foundation (NSF)(National Science Foundation (NSF))	This work was supported by the Office of Naval Research (ONR) and the National Science Foundation (NSF) (Grant number: 1533983 and Grant number: 1407712).	Alaa A. M., 2016, P 30 INT C NEUR INF, V29, P2910; Alaa AM, 2016, IEEE T MULTIMEDIA, V18, P1942, DOI 10.1109/TMM.2016.2589160; Alagoz O, 2011, WILEY ENCY OPERATION; Armstrong K, 2007, ANN INTERN MED, V146, P516, DOI 10.7326/0003-4819-146-7-200704030-00008; Ayer T, 2012, OPER RES, V60, P1019, DOI 10.1287/opre.1110.1019; Baxter N, 2002, CAN MED ASSOC J, V166, P166; Canto MI, 2013, GUT, V62, P339, DOI 10.1136/gutjnl-2012-303108; COX DR, 1972, J R STAT SOC B, V34, P187; Crowder M., 2001, CLASSICAL COMPETING; Daskivich TJ, 2011, CANCER-AM CANCER SOC, V117, P2058, DOI 10.1002/cncr.25751; Elmore JG, 2005, JNCI-J NATL CANCER I, V97, P1035, DOI 10.1093/jnci/dji183; Elson SL, 2013, BREAST CANCER RES TR, V140, P417, DOI 10.1007/s10549-013-2612-0; Erenay FS, 2014, M&SOM-MANUF SERV OP, V16, P381, DOI 10.1287/msom.2014.0484; Frazier AL, 2000, JAMA-J AM MED ASSOC, V284, P1954, DOI 10.1001/jama.284.15.1954; GAIL MH, 1989, J NATL CANCER I, V81, P1879, DOI 10.1093/jnci/81.24.1879; Guiot C, 2003, J THEOR BIOL, V225, P147, DOI 10.1016/S0022-5193(03)00221-2; Jemal A, 2010, CA-CANCER J CLIN, V60, P277, DOI [10.3322/caac.20073, 10.3322/caac.20105]; Joelle Pineau, 2003, IJCAI, P1025; Kim D., 2011, P 22 INT JOINT C ART, P1968; Klabunde Carrie N, 2007, Semin Breast Dis, V10, P102, DOI 10.1053/j.sembd.2007.09.007; Krishnamurthy V., 2016, PARTIALLY OBSERVED M; Krishnamurthy V., 2017, ARXIV170100179; Lee M. L. T., 2003, HDB STAT, V23, P537; Lee MLT, 2006, STAT SCI, V21, P501, DOI 10.1214/088342306000000330; Liebman M. N., 2007, PERSONALIZED MED PER, P171; Loeve F, 1999, COMPUT BIOMED RES, V32, P13, DOI 10.1006/cbmr.1998.1498; Maillart LM, 2008, OPER RES, V56, P1411, DOI 10.1287/opre.1080.0614; MANDELBLATT JS, 1992, ANN INTERN MED, V116, P722, DOI 10.7326/0003-4819-116-9-722; Meira-Machado L, 2006, LIFETIME DATA ANAL, V12, P325, DOI 10.1007/s10985-006-9009-x; Miller RG, 2011, SURVIVAL ANAL, V2nd; Nelson HD, 2009, ANN INTERN MED, V151, P727, DOI 10.7326/0003-4819-151-10-200911170-00009; Oeffinger KC, 2015, JAMA-J AM MED ASSOC, V314, P1599, DOI 10.1001/jama.2015.12783; Pace LE, 2014, JAMA-J AM MED ASSOC, V311, P1327, DOI 10.1001/jama.2014.1398; Rizopoulos D, 2016, BIOSTATISTICS, V17, P149, DOI 10.1093/biostatistics/kxv031; Rizopoulos D, 2011, BIOMETRICS, V67, P819, DOI 10.1111/j.1541-0420.2010.01546.x; Rulyak SJ, 2003, GASTROINTEST ENDOSC, V57, P23, DOI 10.1067/mge.2003.28; Schulam P., 2016, ADV NEURAL INFORM PR, P4709; Sener SF, 1999, J AM COLL SURGEONS, V189, P1, DOI 10.1016/S1072-7515(99)00075-7; Si XS, 2011, EUR J OPER RES, V213, P1, DOI 10.1016/j.ejor.2010.11.018; Siu AL, 2016, ANN INTERN MED, V164, P279, DOI 10.7326/M15-2886; SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071; Thomas DB, 2002, JNCI-J NATL CANCER I, V94, P1445, DOI 10.1093/jnci/94.19.1445; Trentham-Dietz A, 2016, ANN INTERN MED, V165, P700, DOI 10.7326/M16-0476; VERDINELLI I, 1992, J AM STAT ASSOC, V87, P510, DOI 10.2307/2290284; Vilaprinyo E, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086858; Whitlock EP, 2008, ANN INTERN MED, V149, P638, DOI 10.7326/0003-4819-149-9-200811040-00245; WILSON JMG, 1968, B OFIC SANIT PANAM, V65, P281; Yu H., 2006, THESIS; Zauber AG, 2008, ANN INTERN MED, V149, P659, DOI 10.7326/0003-4819-149-9-200811040-00244	49	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401035
C	Anthony, T; Tian, Z; Barber, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Anthony, Thomas; Tian, Zheng; Barber, David			Thinking Fast and Slow with Deep Learning and Tree Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GAME; GO	Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (EXIT), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that EXIT outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MOHEX 1.0, the most recent Olympiad Champion player to be publicly released.	[Anthony, Thomas; Tian, Zheng; Barber, David] UCL, London, England; [Barber, David] Alan Turing Inst, London, England	University of London; University College London	Anthony, T (corresponding author), UCL, London, England.	thomas.anthony.14@ucl.ac.uk	Jeong, Yongwook/N-7413-2016		Alan Turing Institute under the EPSRC [EP/N510129/1]; AWS Cloud Credits for Research	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); AWS Cloud Credits for Research	This work was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1 and by AWS Cloud Credits for Research. We thank Andrew Clarke for help with efficiently parallelising the generation of datasets, Alex Botev for assistance implementing the CNN, and Ryan Hayward for providing a tool to draw Hex positions.	Arneson B, 2010, IEEE T COMP INTEL AI, V2, P251, DOI 10.1109/TCIAIG.2010.2067212; Arpit D., 2016, ARXIV160301431; Chang K., 2015, CORR; Clevert Djork-Arne, 2015, CORR; Coulom R, 2005, BAYESELO; Daume III H., 2009, MACHINE LEARNING; EVANS JSBT, 1984, BRIT J PSYCHOL, V75, P451, DOI 10.1111/j.2044-8295.1984.tb01915.x; Genesereth M, 2005, AI MAG, V26, P62; Goldberg Yoav, 2013, T ASS COMPUTATIONAL, V1, P403; Guo X, 2014, ADV NEURAL INFORM PR, P3338; Kahneman D, 2003, AM ECON REV, V93, P1449, DOI 10.1257/000282803322655392; Kingma D.P, P 3 INT C LEARNING R; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ross S., 2011, P INT C ARTIFICIAL I, P627; Ross S., 2014, ARXIV E PRINTS; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sylvain G., 2007, P 24 INT C MACH LEAR, P273; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Young K, 2016, ARXIV160407097	21	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405043
C	Chang, HS; Learned-Miller, E; McCallum, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chang, Haw-Shiuan; Learned-Miller, Erik; McCallum, Andrew			Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GRADIENT; ONLINE	Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.	[Chang, Haw-Shiuan; Learned-Miller, Erik; McCallum, Andrew] Univ Massachusetts, 140 Governors Dr, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Chang, HS (corresponding author), Univ Massachusetts, 140 Governors Dr, Amherst, MA 01003 USA.	hschang@cs.umass.edu; elm@cs.umass.edu; mccallum@cs.umass.edu	Jeong, Yongwook/N-7413-2016; Chang, Haw-Shiuan/HDN-6000-2022	Chang, Haw-Shiuan/0000-0003-4607-936X	National Science Foundation [1514053]; DARPA [FA8750-1 3-2-0020, HRO011-15-2-0036]	National Science Foundation(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This material is based on research sponsored by National Science Foundation under Grant No. 1514053 and by DARPA under agreement number FA8750-1 3-2-0020 and HRO011-15-2-0036. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.	Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420; Andrychowicz M., 2016, NIPS; [Anonymous], 2009, ICML; [Anonymous], ICML; Bengio, 2015, ARXIV151106481; Bordes A, 2005, J MACH LEARN RES, V6, P1579; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chaudhari P., 2017, ICLR; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Druck Gregory, 2011, P 20 ACM INT C INFOR, P947; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gao J, 2015, ARXIV151203880; Gopal S., 2016, ICML; GUILLORY A, 2009, AISTATS; Gulcehre C., 2017, ICLR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G, 2014, ADV NEURAL INFORM PR, P9; Hinton GE, 2007, PROG BRAIN RES, V165, P535, DOI 10.1016/S0079-6123(06)65034-6; Houlsby N, 2011, STAT; Hovy Eduard, 2006, HLT NAACL; JOHNSON R., 2013, NIPS; Kim Y., 2014, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kumar M.P., 2010, NIPS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee Guang He, 2016, ARXIV161009274; Loshchilov I., 2015, ARXIV151106343; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Mandt S., 2016, AISTATS; Mandt S., 2016, ICML; Meng Deyu, 2015, ARXIV151106049; Mu Y., 2016, IEEE T KNOWLEDGE DAT; Northcutt Curtis G., 2017, ARXIV170501936; Pi T., 2016, IJCAI; PREGIBON D, 1982, BIOMETRICS, V38, P485, DOI 10.2307/2530463; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Rennie J. D., 2005, REGULARIZED LO UNPUB; Roth D., 2002, COLING; Schein AI, 2007, MACH LEARN, V68, P235, DOI 10.1007/s10994-007-5019-5; Schohn G., 2000, ICML; Settles Burr, 2010, ACTIVE LEARNING LIT, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X; Shrivastava A., 2016, CVPR; Strubell E., 2017, EMNLP; Wang C., 2013, NIPS; Wang Y., 2016, ARXIV160603860; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang C., 2017, ICLR; Zhao P., 2014, ARXIV14122753	51	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401005
C	Combes, R; Magureanu, S; Proutiere, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Combes, Richard; Magureanu, Stefan; Proutiere, Alexandre			Minimal Exploration in Structured Stochastic Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMIZATION	This paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties. Most existing structures (e.g. linear, Lipschitz, unimodal, combinatorial, dueling, . . . ) are covered by our framework. We derive an asymptotic instance-specific regret lower bound for these problems, and develop OSSB, an algorithm whose regret matches this fundamental limit. OSSB is not based on the classical principle of "optimism in the face of uncertainty" or on Thompson sampling, and rather aims at matching the minimal exploration rates of sub-optimal arms as characterized in the derivation of the regret lower bound. We illustrate the efficiency of OSSB using numerical experiments in the case of the linear bandit problem and show that OSSB outperforms existing algorithms, including Thompson sampling.	[Combes, Richard] Cent Supelec, L2S, Gif Sur Yvette, France; [Magureanu, Stefan; Proutiere, Alexandre] KTH, EE Sch, ACL, Stockholm, Sweden	UDICE-French Research Universities; Universite Paris Saclay; Royal Institute of Technology	Combes, R (corresponding author), Cent Supelec, L2S, Gif Sur Yvette, France.	richard.combes@supelec.fr; magur@kth.se; alepro@kth.se	Jeong, Yongwook/N-7413-2016		ERC FSA [308267]; French Agence Nationale de la Recherche (ANR) [ANR-16-CE40-0002]	ERC FSA; French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR))	A. Proutiere's research is supported by the ERC FSA (308267) grant. This work is supported by the French Agence Nationale de la Recherche (ANR), under grant ANR-16-CE40-0002 (project BADASS).	Abbasi-Yadkori Y., 2011, NIPS; AGRAWAL R, 1995, SIAM J CONTROL OPTIM, V33, P1926, DOI 10.1137/S0363012992237273; Awerbuch B, 2008, J COMPUT SYST SCI, V74, P97, DOI 10.1016/j.jcss.2007.04.016; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Burnetas AN, 1996, ADV APPL MATH, V17, P122, DOI 10.1006/aama.1996.0007; Carpentier A., 2016, AISTATS; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen W., 2013, INT C MACH LEARN; Combes R., 2015, NIPS; Combes R., 2015, SIGMETRICS; Combes R., 2014, ICML; Dani V., 2008, COLT; Durand A., 2014, 28 AAAI C ART INT; Filippi S., 2010, NIPS, P586; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Garivier A., 2011, COLT; Glashoff K, 1983, LINEAR OPTIMIZATION; Gopalan A., 2014, ICML; Goyal N., 2013, ICML; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; Gyorgy A., 2007, J MACHINE LEARNING R, V8; Herkenrath U., 1983, Metrika, V30, P195, DOI 10.1007/BF02056924; Honda J., 2010, COLT; Kaufmann E., 2012, ALT; Kaufmann E, 2016, J MACH LEARN RES, V17; Komiyama Junpei, 2015, COLT; Kveton B., 2015, AISTATS; Kveton  Branislav, 2015, NIPS; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore T., 2016, AISTATS; Magureanu S., 2014, COLT; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Wen Z., 2015, ICML; Yu J. Y., 2011, ICML	36	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401077
C	Dekel, O; Flajolet, A; Haghtalab, N; Jaillet, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dekel, Ofer; Flajolet, Arthur; Haghtalab, Nika; Jaillet, Patrick			Online Learning with a Hint	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS	We study a variant of online linear optimization where the player receives a hint about the loss function at the beginning of each round. The hint is given in the form of a vector that is weakly correlated with the loss vector on that round. We show that the player can benefit from such a hint if the set of feasible actions is sufficiently round. Specifically, if the set is strongly convex, the hint can be used to guarantee a regret of O (log(T)), and if the set is q-uniformly convex for q epsilon (2; 3), the hint can be used to guarantee a regret of o (root T). In contrast, we establish Omega(root T) lower bounds on regret when the set of feasible actions is a polyhedron.	[Dekel, Ofer] Microsoft Res, Redmond, WA 98052 USA; [Flajolet, Arthur] MIT, Operat Res Ctr, Cambridge, MA 02139 USA; [Haghtalab, Nika] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Jaillet, Patrick] MIT, ORC, LIDS, EECS, Cambridge, MA 02139 USA	Microsoft; Massachusetts Institute of Technology (MIT); Carnegie Mellon University; Massachusetts Institute of Technology (MIT)	Dekel, O (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	oferd@microsoft.com; flajolet@mit.edu; nika@cmu.edu; jaillet@mit.edu	Jeong, Yongwook/N-7413-2016		IBM Ph.D. fellowship; Microsoft Ph.D. fellowship; Office of Naval Research (ONR) [N00014-15-1-2083]	IBM Ph.D. fellowship; Microsoft Ph.D. fellowship; Office of Naval Research (ONR)(Office of Naval Research)	Haghtalab was partially funded by an IBM Ph.D. fellowship and a Microsoft Ph.D. fellowship. Jaillet acknowledges the research support of the Office of Naval Research (ONR) grant N00014-15-1-2083. This work was partially done when Haghtalab was an intern at Microsoft Research, Redmond WA.	Abernethy J., 2008, PROC 19 ANN C LEARNI, P415; [Anonymous], 2012, C LEARN THEOR; [Anonymous], 2017, J MACHINE LEARNING R; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Blum A, 2007, ALGORITHMIC GAME THEORY, P79; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chiang CK, 2010, PROC APPL MATH, V135, P616; Flajolet Arthur, 2014, ARXIV14115649; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36; Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287; Hazan Elad, 2008, P 23 C LEARN THEOR C; Huang R., 2016, ADV NEURAL INFORM PR, P4970; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Pisier Gilles, 2011, COURSE IHP UNPUB FEB, P2; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Sridharan K., 2010, P 23 C LEARN THEOR, P1; Vovk V, 2007, MACH LEARN, V69, P193, DOI 10.1007/s10994-007-5021-y; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	24	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405037
C	Fan, LX		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fan, Lixin			Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance, which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular, we conjecture and empirically illustrate that, the celebrated batch normalization (BN) technique actually adapts the "normalized" bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically, neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance, the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme, on the one hand, can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand, ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed, well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.	[Fan, Lixin] Nokia Technol, Tampere, Finland	Nokia Corporation; Nokia Finland	Fan, LX (corresponding author), Nokia Technol, Tampere, Finland.	lixin.fan@nokia.com						Atanassov K., 2011, NOTES INTUITIONISTIC, V17, P1; Bedregal BC, 2009, ELECTRON NOTES THEOR, V247, P5, DOI 10.1016/j.entcs.2009.07.045; Belohlavek R., 2017, FUZZY LOGIC MATH HIS; Benitez JM, 1997, IEEE T NEURAL NETWOR, V8, P1156, DOI 10.1109/72.623216; Bulsari A., 1992, Complex Systems, V6, P443; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Glorot X., 2011, P 14 INT C ART INT S, P315; GUPTA MM, 1994, FUZZY SET SYST, V61, P1, DOI 10.1016/0165-0114(94)90279-8; Hahnloser R., 2000, DIGITAL SELECTION AN, V405; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; JANG JSR, 1993, IEEE T NEURAL NETWOR, V4, P156, DOI 10.1109/72.182710; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lin K, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301269; Liu P., 2004, SERIES MACHINE PERCE; McCulloch W., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]; Mohammad Norouzi, 2012, ADV NEURAL INFORM PR, P1061; Nair V, 2010, P 27 INT C MACHINE L, P807; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Pedrycz Witold, 2002, SOFT COMPUTING, V7; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Seung H., 2001, NIPS; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tick J, 2005, COMPUT INFORM, V24, P591; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X; Zimmermann H. - J, 2010, ADV REV; Zimmermann H.J., 2001, FUZZY SET THEORY ITS, DOI [10.1007/978-94-010-0646-0_7, DOI 10.1007/978-94-010-0646-0]	34	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401092
C	Foster, DJ; Kale, S; Mohri, M; Sridharan, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Foster, Dylan J.; Kale, Satyen; Mohri, Mehryar; Sridharan, Karthik			Parameter-Free Online Learning via Model Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INEQUALITIES; CONVERGENCE; ALGORITHMS	We introduce an efficient algorithmic framework for model selection in online learning, also known as parameter-free online learning. Departing from previous work, which has focused on highly structured function classes such as nested balls in Hilbert space, we propose a generic meta-algorithm framework that achieves online model selection oracle inequalities under minimal structural assumptions. We give the first computationally efficient parameter-free algorithms that work in arbitrary Banach spaces under mild smoothness assumptions; previous results applied only to Hilbert spaces. We further derive new oracle inequalities for matrix classes, non-nested convex sets, and R-d with generic regularizers. Finally, we generalize these results by providing oracle inequalities for arbitrary non-linear classes in the online supervised learning model. These results are all derived through a unified meta-algorithm scheme using a novel "multi-scale" algorithm for prediction with expert advice based on random playout, which may be of independent interest.	[Foster, Dylan J.; Sridharan, Karthik] Cornell Univ, Ithaca, NY 14853 USA; [Kale, Satyen; Mohri, Mehryar] Google Res, Mountain View, CA USA; [Mohri, Mehryar] NYU, New York, NY 10003 USA	Cornell University; Google Incorporated; New York University	Foster, DJ (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.		Jeong, Yongwook/N-7413-2016		NDSEG fellowship	NDSEG fellowship	We thank Francesco Orabona and David Pal for inspiring initial discussions. Part of this work was done while DF was an intern at Google Research and while DF and KS were visiting the Simons Institute for the Theory of Computing. DF is supported by the NDSEG fellowship.	Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 2002, MACH LEARN, V48, P85, DOI 10.1023/A:1013999503812; Ben-David Shai, 2009, COLT; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bubeck Sebastien, 2017, 18 ACM C EC COMP EC; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chaudhuri K., 2009, ADV NEURAL INFORM PR; Cutkosky A, 2016, ADV NEUR IN, V29; Cutkosky Ashok, 2017, 30 ANN C LEARN THEOR; de Rooij S, 2014, J MACH LEARN RES, V15, P1281; Foster D. J., 2015, ADV NEURAL INFORM PR, V28, P3375; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hazan E., 2014, C LEARNING THEORY, P197; Hazan E, 2017, SIAM J COMPUT, V46, P744, DOI 10.1137/120895731; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; Koolen Wouter M, 2015, COLT, P1155; Massart Pascal, 1896, LECT NOTES MATH; McMahan Brendan, 2013, ADV NEURAL INFORM PR, V26, P2724; McMahan H. B., 2014, P 27 C LEARN THEOR, P1020; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nie JZ, 2013, LECT NOTES ARTIF INT, V8139, P98; Orabona F., 2016, ARXIV160204128; Orabona F, 2014, ADV NEUR IN, V27; Pisier G., 2011, MARTINGALES BANACH S; Rakhlin A., 2010, ADV NEURAL INFORM PR, P1984; Rakhlin S., 2012, ADV NEURAL INFORM PR, V25, P2150; RENEGAR J, 1988, MATH PROGRAM, V40, P59, DOI 10.1007/BF01580724; Schapire, 2015, P 28 C LEARN THEOR, P1286; Srebro N., 2011, ADV NEURAL INFORM PR, V24, P2645; Streeter M., 2012, ADV NEURAL INFORM PR; van Erven T, 2015, J MACH LEARN RES, V16, P1793; Vapnik V. N., 1982, ESTIMATION DEPENDENC, V40; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	40	5	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406010
C	Goyal, A; Ke, NR; Ganguli, S; Bengio, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Goyal, Anirudh; Ke, Nan Rosemary; Ganguli, Surya; Bengio, Yoshua			Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems. The proposed training objective, which we derive via principled variational methods, encourages the transition operator to "walk back" (prefer to revert its steps) in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution.	[Goyal, Anirudh; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada; [Ke, Nan Rosemary] Ecole Polytech Montreal, MILA, Montreal, PQ, Canada; [Ganguli, Surya] Stanford Univ, Stanford, CA 94305 USA	Universite de Montreal; Universite de Montreal; Polytechnique Montreal; Stanford University	Goyal, A (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.	anirudhgoyal9119@gmail.com; rosemary.nan.ke@gmail.com; sganguli@stanford.edu; yoshua.umontreal@gmail.com	Jeong, Yongwook/N-7413-2016	Ganguli, Surya/0000-0002-9264-7551	NSERC; CIFAR; Google; Samsung; Nuance; IBM; Canada Research Chairs; Simons Foundation; McKnight Foundation; James S. McDonnell Foundation; Burroughs Wellcome Foundation; Office of Naval Research	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Google(Google Incorporated); Samsung(Samsung); Nuance; IBM(International Business Machines (IBM)); Canada Research Chairs(Canada Research ChairsCGIAR); Simons Foundation; McKnight Foundation; James S. McDonnell Foundation; Burroughs Wellcome Foundation(Burroughs Wellcome Fund); Office of Naval Research(Office of Naval Research)	The authors would like to thank Benjamin Scellier, Ben Poole, Tim Cooijmans, Philemon Brakel, Gaetan Marceau Caron, and Alex Lamb for their helpful feedback and discussions, as well as NSERC, CIFAR, Google, Samsung, Nuance, IBM and Canada Research Chairs for funding, and Compute Canada for computing resources. S.G. would like to thank the Simons, McKnight, James S. McDonnell, and Burroughs Wellcome Foundations and the Office of Naval Research for support. Y.B would also like to thank Geoff Hinton for an analogy which is used in this work, while discussing contrastive divergence (personnal communication). The authors would also like to express debt of gratitude towards those who contributed to theano over the years (as it is no longer maintained), making it such a great tool.	Al-Rfou R., 2016, ABS160502688 CORR; Alain G, 2014, J MACH LEARN RES, V15, P3563; [Anonymous], 2016, INT C LEARN REPR; Arora S., 2015, ARXIV151105653CSLG; Ba J., 2017, P 3 INT C LEARN REPR; Bengio Y., 2013, BETTER MIXING VIA DE; Bengio Y., 2014, P 31 INT C INT C MAC, V32; Bengio Y., 2013, NIPS 2013; Bengio Y., 2015, ABS150905936 CORR; Bordes F., 2017, ABS170306975 CORR; Burda Y., 2014, ABS14128566 CORR; Crooks GE, 2000, PHYS REV E, V61, P2361, DOI 10.1103/PhysRevE.61.2361; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Gingrich T. R., 2016, P NATL ACAD SCI USA; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2015, ARXIV150204623; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Kingma D. P., 2016, ABS160604934 CORR; Kingma DP, 2 INT C LEARN REPR I, P1; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lillicrap T. P., 2014, ARXIV14110247; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Markram H., 1995, SOC NEUR ABS, V21; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; Rezende D.J., 2014, PROC INT CONFER ENCE; Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE; Schmiedl T, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.108301; Sivak DA, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.190602; Sohl-Dickstein Jascha, 2015, ABS150303585 CORR; Sonderby CK, 2016, ADV NEUR IN, V29; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vincent P, 2010, J MACH LEARN RES, V11, P3371	33	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404045
C	Grave, E; Cisse, M; Joulin, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Grave, Edouard; Cisse, Moustapha; Joulin, Armand			Unbounded cache model for online language modeling with open vocabulary	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recently, continuous cache models were proposed as extensions to recurrent neural network language models, to adapt their predictions to local changes in the data distribution. These models only capture the local context, of up to a few thousands tokens. In this paper, we propose an extension of continuous cache models, which can scale to larger contexts. In particular, we use a large scale non-parametric memory component that stores all the hidden activations seen in the past. We leverage recent advances in approximate nearest neighbor search and quantization algorithms to store millions of representations while searching them efficiently. We conduct extensive experiments showing that our approach significantly improves the perplexity of pre-trained language models on new distributions, and can scale efficiently to much larger contexts than previously proposed local cache models.	[Grave, Edouard; Cisse, Moustapha; Joulin, Armand] Facebook AI Res, Menlo Pk, CA 94025 USA	Facebook Inc	Grave, E (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.	egrave@fb.com; moustaphacisse@fb.com; ajoulin@fb.com	Jeong, Yongwook/N-7413-2016					Alabdulmohsin I., ECML PKDD; Amodei D, 2016, PR MACH LEARN RES, V48; Bahl L. R., 1983, PAMI; Bellegarda J. R., 2000, P IEEE; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bengio Y, 2001, ADV NEUR IN, V13, P932; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Buck C, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3579; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Chelba Ciprian, 2013, ARXIV13123005; Chung J., 2014, ARXIV14123555; Coccaro N., 1998, ICSLP; Della Pietra S., 1992, P WORKSH SPEECH NAT; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Federico M., 2008, INTERSPEECH; Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379; Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432; Goodman JT, 2001, COMPUT SPEECH LANG, V15, P403, DOI 10.1006/csla.2001.0174; Grave E, 2017, PR MACH LEARN RES, V70; Grave Edouard, 2017, 5 INT C LEARN REPR I; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Heafield Kenneth, 2011, P 6 WORKSH STAT MACH, P187; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Iyer R. M., 1999, IEEE T SPEECH AUDIO; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jelinek F., 1991, HLT; Joulin Armand, 2016, ARXIV161203651; Jozefowicz Rafal, 2016, ARXIV160202410; KATZ SM, 1987, IEEE T ACOUST SPEECH, V35, P400, DOI 10.1109/TASSP.1987.1165125; Khudanpur S., 2000, COMPUTER SPEECH LANG; KNESER R, 1995, INT CONF ACOUST SPEE, P181, DOI 10.1109/ICASSP.1995.479394; Kneser R., 1993, ICASSP; KUHN R, 1990, IEEE T PATTERN ANAL, V12, P570, DOI 10.1109/34.56193; Kuhn R., 1988, P 12 C COMP LING, V1; Kupiec J., 1989, P WORKSH SPEECH NAT; Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431; Lahiri S., 2014, P STUD RES WORKSH 14; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lau R., 1993, ICASSP; Mikolov  T., 2012, SLT, V12, P8; Mikolov T, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P612; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Muhlbaier MD, 2009, IEEE T NEURAL NETWOR, V20, P152, DOI 10.1109/TNN.2008.2008326; Rosenfeld R., 1996, COMPUTER SPEECH LANG; Scheirer W. J., 2013, PAMI; TERRELL GR, 1992, ANN STAT, V20, P1236, DOI 10.1214/aos/1176348768; Vinyals Oriol, 2015, NIPS; Wang T., 2016, ACL; Weiss Y., 2009, NIPS; Zobel J, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132956.1132959	56	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406012
C	Greenewald, K; Park, S; Zhou, SH; Giessing, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Greenewald, Kristjan; Park, Seyoung; Zhou, Shuheng; Giessing, Alexander			Time-dependent spatially varying graphical models, with application to brain fMRI data analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONNECTIVITY NETWORKS; COVARIANCE ESTIMATION	In this work, we present an additive model for space-time data that splits the data into a temporally correlated component and a spatially correlated component. We model the spatially correlated portion using a time-varying Gaussian graphical model. Under assumptions on the smoothness of changes in covariance matrices, we derive strong single sample convergence results, confirming our ability to estimate meaningful graphical structures as they evolve over time. We apply our methodology to the discovery of time-varying spatial structures in human brain fMRI signals.	[Greenewald, Kristjan] Harvard Univ, Dept Stat, Cambridge, MA 02138 USA; [Park, Seyoung] Yale Univ, Dept Biostat, New Haven, CT 06520 USA; [Zhou, Shuheng; Giessing, Alexander] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	Harvard University; Yale University; University of Michigan System; University of Michigan	Greenewald, K (corresponding author), Harvard Univ, Dept Stat, Cambridge, MA 02138 USA.		Jeong, Yongwook/N-7413-2016; Zhou, Shuheng/FLN-6143-2022	Park, Seyoung/0000-0003-4783-411X	NSF [DMS-1316731]; Elizabeth Caroline Crosby Research Award from the Advance Program at the University of Michigan; AFOSR [FA9550-13-1-0043]	NSF(National Science Foundation (NSF)); Elizabeth Caroline Crosby Research Award from the Advance Program at the University of Michigan; AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was supported in part by NSF under Grant DMS-1316731, Elizabeth Caroline Crosby Research Award from the Advance Program at the University of Michigan, and by AFOSR grant FA9550-13-1-0043.	Arbabshirani MR, 2014, NEUROIMAGE, V102, P294, DOI 10.1016/j.neuroimage.2014.07.045; Biswal BB, 2010, P NATL ACAD SCI USA, V107, P4734, DOI 10.1073/pnas.0911855107; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Calhoun VD, 2014, NEURON, V84, P262, DOI 10.1016/j.neuron.2014.10.015; Carvalho CM, 2007, BAYESIAN ANAL, V2, P69, DOI 10.1214/07-BA204; Chang C, 2010, NEUROIMAGE, V50, P81, DOI 10.1016/j.neuroimage.2009.12.011; Chen S., 2015, ARXIV150903927; Cressie N, 2015, STAT SPATIAL DATA; Greenewald K, 2015, IEEE T SIGNAL PROCES, V63, P6368, DOI 10.1109/TSP.2015.2472364; Huang SA, 2010, NEUROIMAGE, V50, P935, DOI 10.1016/j.neuroimage.2009.12.120; Kim J, 2015, NEUROIMAGE-CLIN, V9, P625, DOI 10.1016/j.nicl.2015.10.004; Liu X, 2013, P NATL ACAD SCI USA, V110, P4392, DOI 10.1073/pnas.1216856110; Monti RP, 2014, NEUROIMAGE, V103, P427, DOI 10.1016/j.neuroimage.2014.07.033; Narayan M., 2015, ARXIV150203853; Qiu HT, 2016, J R STAT SOC B, V78, P487, DOI 10.1111/rssb.12123; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Tsiligkaridis T, 2013, IEEE T SIGNAL PROCES, V61, P5347, DOI 10.1109/TSP.2013.2279355; Varoquaux G., 2010, ADV NEURAL INFORM PR, P2334; Wehbe L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112575; Zhou SH, 2014, ANN STAT, V42, P532, DOI 10.1214/13-AOS1187; Zhou SH, 2011, J MACH LEARN RES, V12, P2975; Zhou SH, 2010, MACH LEARN, V80, P295, DOI 10.1007/s10994-010-5180-0	23	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405088
C	Hassani, H; Soltanolkotabi, M; Karbasi, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hassani, Hamed; Soltanolkotabi, Mahdi; Karbasi, Amin			Gradient Methods for Submodular Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHM	In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor 1/2 approximation to the global maxima. We also study stochastic gradient methods and show that after O(1/epsilon(2)) iterations these methods reach solutions which achieve in expectation objective values exceeding (OPT/2 - epsilon). An immediate application of our results is to maximize submodular functions that are defined stochastically, i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting, leading to a factor 1/2 approximation when the function is monotone. In particular, it allows us to approximately maximize discrete, monotone submodular optimization problems via projected gradient ascent on a continuous relaxation, directly connecting the discrete and continuous domains. Finally, experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort.	[Hassani, Hamed] Univ Penn, ESE Dept, Philadelphia, PA 19104 USA; [Soltanolkotabi, Mahdi] Univ Southern Calif, EE Dept, Los Angeles, CA USA; [Karbasi, Amin] Yale Univ, ECE Dept, New Haven, CT USA	University of Pennsylvania; University of Southern California; Yale University	Hassani, H (corresponding author), Univ Penn, ESE Dept, Philadelphia, PA 19104 USA.	hassani@seas.upenn.edu; soltanol@usc.edu; amin.karbasi@yale.edu	Jeong, Yongwook/N-7413-2016		DARPA [YFA D16AP00046]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was done while the authors were visiting the Simon's Institute for the Theory of Computing. A. K. is supported by DARPA YFA D16AP00046. The authors would like to thank Jeff Bilmes, Volkan Cevher, Chandra Chekuri, Maryam Fazel, Stefanie Jegelka, Mohammad-Reza Karimi, Andreas Krause, Mario Lucic, and Andrea Montanari for helpful discussions.	Bach F, 2015, ARXIV151100394; Bian A. A., 2016, ARXIV160605615; BRUCKER P, 1984, OPER RES LETT, V3, P163, DOI 10.1016/0167-6377(84)90010-5; Calinescu G., 2011, SIAM J COMPUTING; Chakrabarty D., 2017, STOC; Chekuri C., 2011, P 43 ACM S THEOR COM; Chekuri C, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P201, DOI 10.1145/2688073.2688086; Chekuri C, 2011, ACM S THEORY COMPUT, P783; Das A., 2011, ICML; Djolonga Josip, 2014, NIPS; Edmonds J., 1971, MATH PROGRAM, V1, P127, DOI [10.1007/BF01584082, DOI 10.1007/BF01584082, 10.1007/bf01584082]; Eghbali Reza, 2016, NEURAL INFORM PROCES, P3287; El-Arini K., 2009, KDD; Ene A., 2016, ARXIV160608362; Gomez R. M., 2010, P KDD; Gottschalk C., 2015, INT WORKSH APPR ONL; Guestrin C., 2005, ICML; Hassani Hamed, 2017, ARXIV170803949; Iyer R., 2015, ARTIFICIAL INTELLIGE; Karimi M., 2017, STOCHASTIC SUBMODULA; Kempe D., 2003, KDD; Kim B., 2016, NIPS; Leskovec J., 2007, KDD; Lin H., 2011, P ANN M ASS COMP LIN; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Mirzasoleiman B., 2013, NIPS; Mirzasoleiman B., 2016, ICML; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Oymak S., 2015, ARXIV150704793; PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748; Singla A., 2014, ICML; Soltanolkotabi M, 2017, ARXIV170206175; Soltanolkotabi Mahdi, 2017, ARXIV170504591; Soma T., 2014, ICML; Soma T., 2015, NIPS; Stan S, 2017, PR MACH LEARN RES, V70; WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435	38	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405089
C	Li, C; Jegelka, S; Sra, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Chengtao; Jegelka, Stefanie; Sra, Suvrit			Polynomial time algorithms for dual volume sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study dual volume sampling, a method for selecting k columns from an n x m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume spanned by the rows of the induced submatrix. This method was proposed by Avron and Boutsidis (2013), who showed it to be a promising method for column subset selection and its multiple applications. However, its wider adoption has been hampered by the lack of polynomial time sampling algorithms. We remove this hindrance by developing an exact (randomized) polynomial time sampling algorithm as well as its derandomization. Thereafter, we study dual volume sampling via the theory of real stable polynomials and prove that its distribution satisfies the "Strong Rayleigh" property. This result has numerous consequences, including a provably fast-mixing Markov chain sampler that makes dual volume sampling much more attractive to practitioners. This sampler is closely related to classical algorithms for popular experimental design methods that are to date lacking theoretical analysis but are known to empirically work well.	[Li, Chengtao; Jegelka, Stefanie; Sra, Suvrit] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Li, C (corresponding author), MIT, Cambridge, MA 02139 USA.	ctli@mit.edu; stefje@csail.mit.edu; suvrit@mit.edu	Jeong, Yongwook/N-7413-2016		NSF CAREER award [1553284]; NSF [IIS-1409802]; DARPA [N66001-17-1-4039]; DARPA FunLoL grant [W911NF-16-1-0551]; Siebel Scholar Fellowship	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA FunLoL grant; Siebel Scholar Fellowship	This research was supported by NSF CAREER award 1553284, NSF grant IIS-1409802, DARPA grant N66001-17-1-4039, DARPA FunLoL grant (W911NF-16-1-0551) and a Siebel Scholar Fellowship. The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.	Anari N., 2014, ARXIV14121143; Anari N., 2016, COLT, P23; Anari N., 2015, IEEE S FDN COMP SCI; Arioli M., 2015, SIAM J SCI COMPUT; Arthur D., 2007, P 18 ANN ACM SIAM S; Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287; Borcea J, 2008, DUKE MATH J, V143, P205, DOI 10.1215/00127094-2008-018; Borcea J, 2009, J AM MATH SOC, V22, P521; Borodin A., 2009, ARXIV09111153; Boutsidis C., 2011, ARXIV11102897; Boutsidis C, 2014, SIAM J COMPUT, V43, P687, DOI 10.1137/12086755X; Boutsidis C, 2013, IEEE T INFORM THEORY, V59, P6099, DOI 10.1109/TIT.2013.2255021; Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968; Chen SH, 2015, IEEE T SIGNAL PROCES, V63, P6510, DOI 10.1109/TSP.2015.2469645; Civril A, 2009, THEOR COMPUT SCI, V410, P4801, DOI 10.1016/j.tcs.2009.06.018; Derezinski Michal, 2017, ADV NEURAL INFORM PR; Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38; Elkin M., 2008, SIAM J COMPUTING; Feder T., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P26, DOI 10.1145/129712.129716; Fedorov V., 1969, PREPRINT; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; Frieze A, 2014, SIAM J COMPUT, V43, P497, DOI 10.1137/120890971; Gharan SO, 2011, ANN IEEE SYMP FOUND, P550, DOI 10.1109/FOCS.2011.80; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Joshi S, 2009, IEEE T SIGNAL PROCES, V57, P451, DOI 10.1109/TSP.2008.2007095; Kulesza A., 2012, FDN TRENDS MACHINE L, V5; Li C., 2016, ADV NEURAL INFORM PR; Li CT, 2016, PR MACH LEARN RES, V48; Lyons R, 2003, PUBL MATH-PARIS, P167; Ma P., 2015, J MACHINE LEARNING R; Macchi O., 1975, ADV APPL PROBABILITY, V7; Magen A, 2008, LECT NOTES COMPUT SC, V5171, P523; Miller A. J., 1994, J ROYAL STAT SOC; MORRIS MD, 1995, J STAT PLAN INFER, V43, P381, DOI 10.1016/0378-3758(94)00035-T; NGUYEN NK, 1992, COMPUT STAT DATA AN, V14, P489, DOI 10.1016/0167-9473(92)90064-M; Pemantle R, 2000, J MATH PHYS, V41, P1371, DOI 10.1063/1.533200; Pemantle R, 2014, COMB PROBAB COMPUT, V23, P140, DOI 10.1017/S0963548313000345; Pukelsheim F, 2006, CLASS APPL MATH, V50, P1, DOI 10.1137/1.9780898719109; Spielman D., 2004, STOC; Spielman DA, 2011, SIAM J COMPUT, V40, P1913, DOI 10.1137/080734029; Tsitsvero M, 2016, IEEE T SIGNAL PROCES, V64, P4845, DOI 10.1109/TSP.2016.2573748; Wagner DG, 2011, B AM MATH SOC, V48, P53, DOI 10.1090/S0273-0979-2010-01321-5; Wang Y., 2016, ARXIV E PRINTS; Zhao YB, 2016, IEEE DECIS CONTR P, P1859, DOI 10.1109/CDC.2016.7798535; Zhu R., 2015, OPTIMAL SUBSAMPLING	45	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405012
C	Liu, GC; Liu, QS; Yuan, XT		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Guangcan; Liu, Qingshan; Yuan, Xiao-Tong			A New Theory for Matrix Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LOW-RANK MATRIX; ALGORITHMS	Prevalent matrix completion theories reply on an assumption that the locations of the missing data are distributed uniformly and randomly (i.e., uniform sampling). Nevertheless, the reason for observations being missing often depends on the unseen observations themselves, and thus the missing data in practice usually occurs in a nonuniform and deterministic fashion rather than randomly. To break through the limits of random sampling, this paper introduces a new hypothesis called isomeric condition, which is provably weaker than the assumption of uniform sampling and arguably holds even when the missing data is placed irregularly. Equipped with this new tool, we prove a series of theorems for missing data recovery and matrix completion. In particular, we prove that the exact solutions that identify the target matrix are included as critical points by the commonly used nonconvex programs. Unlike the existing theories for nonconvex matrix completion, which are built upon the same condition as convex programs, our theory shows that nonconvex programs have the potential to work with a much weaker condition. Comparing to the existing studies on nonuniform sampling, our setup is more general.	[Liu, Guangcan; Liu, Qingshan; Yuan, Xiao-Tong] Nanjing Univ Informat Sci & Technol, B DAT, Sch Informat & Control, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China	Nanjing University of Information Science & Technology	Liu, GC (corresponding author), Nanjing Univ Informat Sci & Technol, B DAT, Sch Informat & Control, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.	gcliu@cse.ohio-state.edu; qsliu@cse.ohio-state.edu; xtyuan@nuist.edu.cn	Liu, Qing/GWC-9222-2022; liu, qingqing/HHD-0360-2022; Jeong, Yongwook/N-7413-2016		national Natural Science Foundation of China (NSFC) [61622305, 61502238]; Natural Science Foundation of Jiangsu Province of China (NSFJPC) [BK20160040]; NSFC [61402232, 61522308]; NSFJPC [BK20141003]	national Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Jiangsu Province of China (NSFJPC)(Natural Science Foundation of Jiangsu Province); NSFC(National Natural Science Foundation of China (NSFC)); NSFJPC	The work of Guangcan Liu is supported in part by national Natural Science Foundation of China (NSFC) under Grant 61622305 and Grant 61502238, in part by Natural Science Foundation of Jiangsu Province of China (NSFJPC) under Grant BK20160040.; The work of Xiao-Tong Yuan is supported in part by NSFC under Grant 61402232 and Grant 61522308, in part by NSFJPC under Grant BK20141003.	[Anonymous], 2016, ADV NEURAL INFORM PR; Bishop W. E., 2014, ADV NEURAL INF PROCE, P2762; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen YD, 2015, J MACH LEARN RES, V16, P2999; CHISTOV AL, 1984, LECT NOTES COMPUT SC, V176, P17; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kiraly FJ, 2015, J MACH LEARN RES, V16, P1391; Lee T., 2013, ADV NEURAL INFORM PR, P1781; Liu G., 2014, P ADV NEUR INF PROC, P1206; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Liu GC, 2017, IEEE T PATTERN ANAL, V39, P47, DOI 10.1109/TPAMI.2016.2539946; Liu GC, 2016, IEEE T SIGNAL PROCES, V64, P5623, DOI 10.1109/TSP.2016.2586753; Liu GC, 2016, IEEE T PATTERN ANAL, V38, P417, DOI 10.1109/TPAMI.2015.2453969; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Meka R., 2009, ADV NEURAL INFORM PR, P1258; Mohan K, 2010, IEEE INT SYMP INFO, P1573, DOI 10.1109/ISIT.2010.5513471; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Netrapalli P., 2014, P ADV NEUR INF PROC, P1107; Ni Yuzhao, 2013, INT C DAT MIN WORKSH, P1179; Pimentel- Alarcon D., 2016, P INT C MACH LEARN, P802; Recht B., 2008, TECHNICAL REPORT; Rockafellar R. T., 1970, CONVEX ANAL; Shang FH, 2016, AAAI CONF ARTIF INTE, P2016; Srebro N., 2010, PROC INT C NEURAL IN, P2056; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Weimer Markus, 2007, NEURAL INFORM PROCES; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Zhang Yin, 2006, TR0615 CAAM	37	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400075
C	Liu, Y; Chen, JS; Deng, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Yu; Chen, Jianshu; Deng, Li			Unsupervised Sequence Classification using Sequential Output Statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider learning a sequence classifier without labeled data by using sequential output statistics. The problem is highly valuable since obtaining labels in training data is often costly, while the sequential output statistics (e.g., language models) could be obtained independently of input data and thus with low or no cost. To address the problem, we propose an unsupervised learning cost function and study its properties. We show that, compared to earlier works, it is less inclined to be stuck in trivial solutions and avoids the need for a strong generative model. Although it is harder to optimize in its functional form, a stochastic primal-dual gradient method is developed to effectively solve the problem. Experiment results on real-world datasets demonstrate that the new unsupervised learning method gives drastically lower errors than other baseline methods. Specifically, it reaches test errors about twice of those obtained by fully supervised learning.	[Liu, Yu; Chen, Jianshu; Deng, Li] Microsoft Res, Redmond, WA 98052 USA; [Liu, Yu; Deng, Li] Citadel LLC, Chicago, IL USA	Microsoft	Chen, JS (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	jianshuc@microsoft.com; Li.Deng@citadel.com	Jeong, Yongwook/N-7413-2016					Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Berg-Kirkpatrick Taylor, 2013, P 51 ANN M ASS COMP, V1, P207; Beutelspacher Albrecht, 1994, CRYPTOLOGY; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Chen J., 2016, ARXIV160604646; Chintala Soumith, 2016, PATH UNSUPERVISED LE; Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090; Dai Andrew M., 2015, ADV NEURAL INFORM PR; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian, 2016, TUTORIAL AT NIPS; Graves A, 2012, ICML REPRESENTATION; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Kay A., 2007, LINUX J; Kingma D. P., 2013, AUTO ENCODING VARIAT; Knight Kevin, 2006, P COLING ACL MAIN C, P499; Le Q., 2012, INT C MACH LEARN, DOI DOI 10.1109/MSP.2011.940881; Li Deng, 2015, TUT INT C DRESD GERM; Luciano Dennis, 1987, COLL MATH J, V18, P2, DOI DOI 10.2307/2686311; Mikolov T., 2013, ARXIV; Minka, 2005, DIVERGENCE MEASURES; Parker Robert, 2009, PHILADELPHIA LINGUIS; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Stewart Russell, 2017, P AAAI; Sutskever I., 2015, ARXIV PREPRINT ARXIV; Vincent P, 2010, J MACH LEARN RES, V11, P3371	30	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403060
C	Louppe, G; Kagan, M; Cranmer, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Louppe, Gilles; Kagan, Michael; Cranmer, Kyle			Learning to Pivot with Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot - a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.	[Louppe, Gilles; Cranmer, Kyle] NYU, New York, NY 10003 USA; [Kagan, Michael] SLAC Natl Accelerator Lab, Menlo Pk, CA USA	New York University; Stanford University; United States Department of Energy (DOE); SLAC National Accelerator Laboratory	Louppe, G (corresponding author), NYU, New York, NY 10003 USA.	g.louppe@nyu.edu; makagan@slac.stanford.edu; kyle.cranmer@nyu.edu	Jeong, Yongwook/N-7413-2016		NSF [ACI-1450310]; US Department of Energy (DOE) [DE-AC02-76SF00515]; SLAC Panofsky Fellowship;  [PHY-1505463];  [PHY-1205376]	NSF(National Science Foundation (NSF)); US Department of Energy (DOE)(United States Department of Energy (DOE)); SLAC Panofsky Fellowship; ; 	We would like to thank the authors of (Baldi et al., 2016a) for sharing the data used in their studies. KC and GL are both supported through NSF ACI-1450310, additionally KC is supported through PHY-1505463 and PHY-1205376. MK is supported by the US Department of Energy (DOE) under grant DE-AC02-76SF00515 and by the SLAC Panofsky Fellowship.	Adam-Bourdarios C., 2014, NIPS 2014 WORKSH HIG, P37; [Anonymous], 2014, ARXIV14124446; ATLAS collaboration, 2015, ATLPHYSPUB2015033 CE; ATLAS Collaboration, 2014, ATLPHYSPUB2014004 CE; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; Baldi P, 2016, PHYS REV D, V93, DOI 10.1103/PhysRevD.93.094034; Baldi P, 2016, EUR PHYS J C, V76, DOI 10.1140/epjc/s10052-016-4099-4; Bishop C. M., 1994, MIXTURE DENSITY NETW; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Cranmer K., 2015, ARXIV150602169; DEGROOT MH, 1975, PROBABILITY STAT; Edwards Harrison, 2016, INT C LEARN REPR ICL, P3; Evans L, 2008, J INSTRUM, V3, DOI 10.1088/1748-0221/3/08/S08001; Ganin Yaroslav, 2014, ARXIV14097495; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Khachatryan V, 2014, J HIGH ENERGY PHYS, DOI 10.1007/JHEP12(2014)017; Louizos C., 2015, ARXIV PREPRINT ARXIV; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Sha, 2013, P INT C MACH LEARN; Shimmin C., 2017, DECORRELATED JET SUB; Zafar Muhammad Bilal, 2015, ARXIV150705259; Zemel R., 2013, P INT C MACH LEARN, P325	25	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401003
C	Medina, AM; Vassilvitskii, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Medina, Andres Munoz; Vassilvitskii, Sergei			Revenue Optimization with Approximate Bid Predictions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				AUCTIONS; REGRET	In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types, and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community. We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.	[Medina, Andres Munoz; Vassilvitskii, Sergei] Google Res, 76 9th Ave, New York, NY 10011 USA	Google Incorporated	Medina, AM (corresponding author), Google Res, 76 9th Ave, New York, NY 10011 USA.		Jeong, Yongwook/N-7413-2016					Cesa-Bianchi N, 2015, IEEE T INFORM THEORY, V61, P549, DOI 10.1109/TIT.2014.2365772; Chawla S, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P243; Cole Richard, 2015, ABS150200963 CORR; Devanur NR, 2016, ACM S THEORY COMPUT, P426, DOI 10.1145/2897518.2897553; Dhangwatnotai P, 2015, GAME ECON BEHAV, V91, P318, DOI 10.1016/j.geb.2014.03.011; Goldberg AV, 2001, SIAM PROC S, P735; Hartline JD, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P225; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; Leme RP, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1093, DOI 10.1145/2872427.2883071; Mohri M., 2018, FDN MACHINE LEARNING; Mohri M, 2014, PR MACH LEARN RES, V32; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P601; Rudolph MR, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1113, DOI 10.1145/2872427.2883051	16	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401086
C	Motiian, S; Jones, Q; Iranmanesh, SM; Doretto, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Motiian, Saeid; Jones, Quinn; Iranmanesh, Seyed Mehdi; Doretto, Gianfranco			Few-Shot Adversarial Domain Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INFORMATION; SVM	This work provides a framework for addressing the problem of supervised domain adaptation with deep models. The main idea is to exploit adversarial learning to learn an embedded subspace that simultaneously maximizes the confusion between two domains while semantically aligning their embedding. The supervised setting becomes attractive especially when there are only a few target data samples that need to be labeled. In this few-shot learning scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by carefully designing a training scheme whereby the typical binary adversarial discriminator is augmented to distinguish between four different classes, it is possible to effectively address the supervised adaptation problem. In addition, the approach has a high "speed" of adaptation, i.e. it requires an extremely low number of labeled target training samples, even one per category can be effective. We then extensively compare this approach to the state of the art in domain adaptation in two experiments: one using datasets for handwritten digit recognition, and one using datasets for visual object recognition.	[Motiian, Saeid; Jones, Quinn; Iranmanesh, Seyed Mehdi; Doretto, Gianfranco] West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA	West Virginia University	Motiian, S (corresponding author), West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.	samotiian@mix.wvu.edu; qjones1@mix.wvu.edu; seiranmanesh@mix.wvu.edu; gidoretto@mix.wvu.edu	Jeong, Yongwook/N-7413-2016					[Anonymous], 2014, ABS14123474 CORR; [Anonymous], 2017, ARXIV170401705; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; Becker Carlos J, 2013, ADV NEURAL INFORM PR, V1, P485; Bergamo A., 2010, ADV NEURAL INFORM PR, P181; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Chen L, 2014, PROC CVPR IEEE, P1418, DOI 10.1109/CVPR.2014.184; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Donahue J., 2013, 31 INT C MACH LEARN; Duan LX, 2009, PROC CVPR IEEE, P1375, DOI [10.1109/CVPR.2009.5206747, 10.1109/CVPRW.2009.5206747]; Fernando B., 2015, PATTERN RECOGITION L; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Yaroslav, 2014, ARXIV14097495; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow I., 2016, ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Gretton A., 2006, NIPS; Guo Y., 2012, P 29 INT C MACH LEAR; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Isola Phillip, 2018, IMAGE TO IMAGE TRANS; Kingma D.P., 2015, 3 INT C LEARN REPR I, P1, DOI DOI 10.1007/S11390-017-1754-7; Koniusz P., 2016, ARXIV161108195; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581; Lapin M, 2014, NEURAL NETWORKS, V53, P95, DOI 10.1016/j.neunet.2014.02.002; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu M. -Y., 2016, ADV NEURAL INFORM PR, P469; Long M., 2015, P 32 INT C MACH LEAR, V1, P97; Long MS, 2013, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2013.59; Lowell U, 2015, ICCV; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Motiian S, 2016, PROC CVPR IEEE, P1496, DOI 10.1109/CVPR.2016.166; Motiian S, 2016, LECT NOTES COMPUT SC, V9911, P630, DOI 10.1007/978-3-319-46478-7_39; Muandet Krikamol, 2013, ICML; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2016, PR MACH LEARN RES, V48; Rozantsev A., 2016, ARXIV160306432; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Salimans T., 2016, ADV NEUR IN, P2234; Sarafianos N., 2017, ARXIV170809083; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Shrivastava Ashish, 2017, IEEE C COMP VIS PATT; Simonyan K., 2014, 3 INT C LEARN REPR I; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Tommasi T, 2016, LECT NOTES COMPUT SC, V9915, P475, DOI 10.1007/978-3-319-49409-8_39; Tommasi T, 2015, LECT NOTES COMPUT SC, V9358, P504, DOI 10.1007/978-3-319-24947-6_42; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzeng E., 2017, IEEE C COMP VIS PATT; Varior Rahul Rama, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P135, DOI 10.1007/978-3-319-46478-7_9; Yang J., 2007, 7 IEEE INT C DATA MI, P69, DOI DOI 10.1109/ICDMW.2007.37; Yao T., 2015, IEEE C COMP VIS PATT; Zhang H., 2016, ARXIV161203242	64	5	5	0	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406071
C	Namkoong, H; Duchi, JC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Namkoong, Hongseok; Duchi, John C.			Variance-based Regularization with Convex Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INEQUALITIES	We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.	[Namkoong, Hongseok; Duchi, John C.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Namkoong, H (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	hnamk@stanford.edu; jduchi@stanford.edu	Jeong, Yongwook/N-7413-2016		SAIL-Toyota Center for AI Research; Samsung Fellowship; National Science Foundation [NSF-CAREER-1553086]; Sloan Foundation	SAIL-Toyota Center for AI Research; Samsung Fellowship(Samsung); National Science Foundation(National Science Foundation (NSF)); Sloan Foundation(Alfred P. Sloan Foundation)	We thank Feng Ruan for pointing out a much simpler proof of Theorem 1 than in our original paper. JCD and HN were partially supported by the SAIL-Toyota Center for AI Research and HN was partially supported Samsung Fellowship. JCD was also partially supported by the National Science Foundation award NSF-CAREER-1553086 and the Sloan Foundation.	Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; Bertsimas D., 2014, ARXIV14084445MATHOC; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Boyd S, 2004, CONVEX OPTIMIZATION; Duchi J., 2008, P 25 INT C MACH LEAR; Duchi J. C., 2016, ARXIV161003425STAT M; Hiriart-Urruty J.-B., 1993, CONVEX ANAL MINIMIZA, V306, pxviii+346, DOI 10.1007/978-3-662-06409-2; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Lichman M., 2013, UCI MACHINE LEARNING; Mammen E, 1999, ANN STAT, V27, P1808; Maurer A., 2009, P 22 ANN C COMP LEAR; Mendelson S., 2014, P 27 ANN C COMP LEAR; Namkoong H., 2016, ADV NEURAL INFORM PR, P29; Owen A.B., 2001, EMPIRICAL LIKELIHOOD; Samson PM, 2000, ANN PROBAB, V28, P416; Tsybakov AB, 2004, ANN STAT, V32, P135; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Van Der VaartJon A., 1996, WEAK CONVERGENCE EMP; Vapnik V.N, 1998, STAT LEARNING THEORY; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zubkov AM, 2013, THEOR PROBAB APPL+, V57, P539, DOI 10.1137/S0040585X97986138	28	5	5	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403004
C	Neubig, G; Goldberg, Y; Dyer, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Neubig, Graham; Goldberg, Yoav; Dyer, Chris			On-the-fly Operation Batching in Dynamic Computation Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits-both static and dynamic-require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.(2)	[Neubig, Graham] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA; [Goldberg, Yoav] Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel; [Dyer, Chris] DeepMind, London, England	Carnegie Mellon University; Bar Ilan University	Neubig, G (corresponding author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.	gneubig@cs.cmu.edu; yogo@cs.biu.ac.il; cdyer@google.com	Jeong, Yongwook/N-7413-2016		Israeli Science Foundation [1555/15]; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	Israeli Science Foundation(Israel Science Foundation); Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	The work of YG is supported by the Israeli Science Foundation (grant number 1555/15) and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).	Abadi M., TENSORFLOW LARGE SCA; [Anonymous], 2013, P C EMP METH NAT LAN; [Anonymous], 2010, P 9 PYTH SCI C JUN 2; Ballesteros Miguel, 2016, P 2016 C EMP METH NA, P2005, DOI DOI 10.18653/V1/D16-1211; Bartholomew-Biggs M, 2000, J COMPUT APPL MATH, V124, P171, DOI 10.1016/S0377-0427(00)00422-2; Battaglia Peter W., 2016, NEURAL INFORM PROCES; Bengio Samy, 2015, ABS150603099 CORR; Bowman SR, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1466; Dyer C, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P334; Dyer Chris, 2016, P 2016 C N AM CHAPT, P199, DOI DOI 10.18653/V1/N16-1024; Goldberg Yoav, 2013, T ASS COMPUTATIONAL, V1, P403; Hadjis Stefan, 2015, P 4 WORKSH DAT AN SC; Huang Z., 2015, ARXIV; Kiperwasser E., 2016, T ASS COMPUTATIONAL, V4, P313, DOI [DOI 10.1162/TACL_A_00101, 10.1162/tacl_a_00101]; Kiperwasser E., 2016, T ASS COMPUT LINGUIS, V4, P445; Ladhak Faisal, 2016, P INTERSPEECH; Li Chengtao, 2017, INT C LEARN REPR ICL; Liang Xiaodan, 2016, P ECCV; Ling Wang, 2015, P 2015 C EMP METH NA, P1520, DOI DOI 10.18653/V1/D15-1176; Looks Moshe, 2017, INT C LEARN REPR ICL; Louppe G., 2017, ARXIV170200748; Neubig Graham, 2017, ARXIV170103980; Nothman J, 2013, ARTIF INTELL, V194, P151, DOI 10.1016/j.artint.2012.03.006; Plank B, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P412; Potts CN, 2000, EUR J OPER RES, V120, P228, DOI 10.1016/S0377-2217(99)00153-8; Reed Scott, 2016, NEURAL PROGRAMMER IN; Rohou E, 2013, ACM T ARCHIT CODE OP, V9, DOI 10.1145/2400682.2400685; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shazeer Noam, 2017, INT C LEARN REPR ICL; Socher R., 2011, P 28 INT C INT C MAC, P129; Tai Kai Sheng, 2015, ANN C ASS COMP LING; Tokui S., 2015, P WORKSH MACH LEARN; Yogatama D, 2017, INT C LEARN REPR ICL	33	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404005
C	Panahi, A; Hassibi, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Panahi, Ashkan; Hassibi, Babak			A Universal Analysis of Large-Scale Regularized Least Squares Solutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RESTRICTED ISOMETRY PROPERTY; SIGNAL RECOVERY; LASSO	A problem that has been of recent interest in statistical inference, machine learning and signal processing is that of understanding the asymptotic behavior of regularized least squares solutions under random measurement matrices (or dictionaries). The Least Absolute Shrinkage and Selection Operator (LASSO or least-squares with l(1) regularization) is perhaps one of the most interesting examples. Precise expressions for the asymptotic performance of LASSO have been obtained for a number of different cases, in particular when the elements of the dictionary matrix are sampled independently from a Gaussian distribution. It has also been empirically observed that the resulting expressions remain valid when the entries of the dictionary matrix are independently sampled from certain non-Gaussian distributions. In this paper, we confirm these observations theoretically when the distribution is sub-Gaussian. We further generalize the previous expressions for a broader family of regularization functions and under milder conditions on the underlying random, possibly non-Gaussian, dictionary matrix. In particular, we establish the universality of the asymptotic statistics (e.g., the average quadratic risk) of LASSO with non-Gaussian dictionaries.	[Panahi, Ashkan] North Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27606 USA; [Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	University of North Carolina; North Carolina State University; California Institute of Technology	Panahi, A (corresponding author), North Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27606 USA.	apanahi@ncsu.edu; hassibi@caltech.edu	Jeong, Yongwook/N-7413-2016					[Anonymous], 2013, ARXIV13037291; Bai ZD, 1999, STAT SINICA, V9, P611; Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718; Barbier J., 2016, ADV NEURAL INF PROCE, P424; Barbier J, 2016, ANN ALLERTON CONF, P625, DOI 10.1109/ALLERTON.2016.7852290; Bayati M, 2012, IEEE T INFORM THEORY, V58, P1997, DOI 10.1109/TIT.2011.2174612; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Candes EJ, 2011, IEEE T INFORM THEORY, V57, P7235, DOI 10.1109/TIT.2011.2161794; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Gordon Y., 1988, MILMANS INEQUALITY R; Kabashima Y, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/09/L09003; Karoui N.E., 2013, ARXIV13112445; Lelarge M., 2016, ARXIV161103888; Lindeberg JW, 1922, MATH Z, V15, P211, DOI 10.1007/BF01494395; Liu J., 2013, P 2013 30 INT C INT, V28, P91; Montanari A., 2010, TECH REP; Oymak S., 2015, ARXIV151109433; Oymak S, 2013, ANN ALLERTON CONF, P1002, DOI 10.1109/Allerton.2013.6736635; Roman V., 2012, COMPRESSED SENSING T; Thrampoulidis C., 2015, ADV NEURAL INFORM PR; Thrampoulidis C., 2016, ARXIV160106233; Thrampoulidis C., 2015, ARXIV150204977; Thrampoulidis C, 2015, IEEE INT SYMP INFO, P2021, DOI 10.1109/ISIT.2015.7282810; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Xin B, 2014, AAAI CONF ARTIF INTE, P2163; Zerbib N., 2016, TECH REP	36	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403044
C	Sheth, R; Khardon, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Sheth, Rishit; Khardon, Roni			Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Bayesian models are established as one of the main successful paradigms for complex problems in machine learning. To handle intractable inference, research in this area has developed new approximation methods that are fast and effective. However, theoretical analysis of the performance of such approximations is not well developed. The paper furthers such analysis by providing bounds on the excess risk of variational inference algorithms and related regularized loss minimization algorithms for a large class of latent variable models with Gaussian latent variables. We strengthen previous results for variational algorithms by showing that they are competitive with any point-estimate predictor. Unlike previous work, we provide bounds on the risk of the Bayesian predictor and not just the risk of the Gibbs predictor for the same approximate posterior. The bounds are applied in complex models including sparse Gaussian processes and correlated topic models. Theoretical results are complemented by identifying novel approximations to the Bayesian objective that attempt to minimize the risk directly. An empirical evaluation compares the variational and new algorithms shedding further light on their performance.	[Sheth, Rishit; Khardon, Roni] Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA	Tufts University	Sheth, R (corresponding author), Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.	rishit.sheth@tufts.edu; roni@cs.tufts.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1714440]	NSF(National Science Foundation (NSF))	This work was partly supported by NSF under grant IIS-1714440.	Alquier P, 2016, J MACH LEARN RES, V17; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Boyd S, 2004, CONVEX OPTIMIZATION; Dalalyan A, 2008, MACH LEARN, V72, P39, DOI 10.1007/s10994-008-5051-0; Germain Pascal, 2016, ADV NEURAL INFORM PR, P1884; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Hoffman MD, 2015, JMLR WORKSH CONF PRO, V38, P361; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kakade Sham M., 2004, NIPS, P641; Lacasse A., 2006, NIPS, P769; Lafferty JohnD., 2006, ADV NEURAL INFORM PR, P147; Lichman M., 2013, UCI MACHINE LEARNING; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; Meir R., 2003, J MACHINE LEARNING R, V4, P839; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Sheth R, 2016, JMLR WORKSH CONF PRO, V51, P761; Sheth R, 2015, PR MACH LEARN RES, V37, P1302; Sheth Rishit, 2016, ARXIV13096835; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Teh Y., 2006, ADV NEURAL INFORM PR, V19, P1353; Titsias M. K., 2009, ARTIF INTELL STAT, V3; WANG SD, 1986, IEEE T AUTOMAT CONTR, V31, P654, DOI 10.1109/TAC.1986.1104370	27	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405023
C	Tang, HR; Houthooft, R; Foote, D; Stooke, A; Chen, X; Duan, Y; Schulman, J; De Turck, F; Abbeel, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tang, Haoran; Houthooft, Rein; Foote, Davis; Stooke, Adam; Chen, Xi; Duan, Yan; Schulman, John; De Turck, Filip; Abbeel, Pieter			#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.	[Tang, Haoran] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Foote, Davis; Stooke, Adam; Chen, Xi; Duan, Yan; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA USA; [Houthooft, Rein; De Turck, Filip] Univ Ghent, Imec, Dept Informat Technol, Ghent, Belgium; [Houthooft, Rein; Chen, Xi; Duan, Yan; Schulman, John; Abbeel, Pieter] OpenAI, San Francisco, CA 94110 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley; Ghent University; IMEC	Tang, HR (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Houthooft, R (corresponding author), Univ Ghent, Imec, Dept Informat Technol, Ghent, Belgium.; Houthooft, R (corresponding author), OpenAI, San Francisco, CA 94110 USA.	hrtang@math.berkeley.edu; rein.houthooft@openai.com			ONR through a PECASE award; Berkeley AI Research lab Fellowship; Huawei Fellowship; NSF [IIS-1619362]; ARC [FL110100281]; Fannie and John Hertz Foundation fellowship; Research Foundation - Flanders (FWO); ARC through ARC Centre of Excellence for Mathematical and Statistical Frontiers	ONR through a PECASE award; Berkeley AI Research lab Fellowship; Huawei Fellowship(Huawei Technologies); NSF(National Science Foundation (NSF)); ARC(Australian Research Council); Fannie and John Hertz Foundation fellowship; Research Foundation - Flanders (FWO)(FWO); ARC through ARC Centre of Excellence for Mathematical and Statistical Frontiers(Australian Research Council)	We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Adam Stooke gratefully acknowledges funding from a Fannie and John Hertz Foundation fellowship. Rein Houthooft was supported by a Ph.D. Fellowship of the Research Foundation - Flanders (FWO).	Abel D, 2016, ICML WORKSH ABSTR RE; Andoni A, 2006, ANN IEEE SYMP FOUND, P459; Bellemare M., 2016, NEURIPS; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Duan Y, 2016, INT C MACH LEARN, P1329; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Guez A., 2014, NIPS, V27, P451; He K, 2016, 2016 IEEE C COMP VIS, DOI [10.1109/cvpr.2016.90, 10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kolter J. Z., 2009, P 26 ANN INT C MACHI, P513, DOI DOI 10.1145/1553374.1553441; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Mnih V, 2016, ASYNCHRONOUS METHODS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A., 2015, ARXIV150704296; Osband I, 2016, PR MACH LEARN RES, V48; Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007; Pazis J., 2013, P 27 AAAI C ART INT; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Schulman J, 2015, P 32 INT C MACH LEAR; Simonyan K., 2015, ICLR; Stadie B. C., 2015, ARXIV150700814; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl Alexander L., 2005, P 22 INT C MACH LEAR, P856, DOI [DOI 10.1145/1102351.1102459, 10.1145/1102351.1102459]; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; van den Oord A, 2016, PR MACH LEARN RES, V48; Van Hasselt H., 2016, P 30 AAAI C ART INT; van Hasselt Hado, 2016, ARXIV160207714; Wang ZY, 2016, PR MACH LEARN RES, V48; Yi Sun, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P41, DOI 10.1007/978-3-642-22887-2_5	40	5	5	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402078
C	Wang, G; Giannakis, GB; Saad, Y; Chen, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Gang; Giannakis, Georgios B.; Saad, Yousef; Chen, Jie			Solving Most Systems of Random Quadratic Equations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PHASE RETRIEVAL; RECOVERY	This paper deals with finding an n-dimensional solution x to a system of quadratic equations y(i) = vertical bar < a(i), x >vertical bar(2), 1 <= i <= m, which in general is known to be NP-hard. We put forth a novel procedure, that starts with a weighted maximal correlation initialization obtainable with a few power iterations, followed by successive refinements based on iteratively reweighted gradient-type iterations. The novel techniques distinguish themselves from prior works by the inclusion of a fresh (re) weighting regularization. For certain random measurement models, the proposed procedure returns the true solution x with high probability in time proportional to reading the data {(a(i); y(i))}1 <= i <= m, provided that the number m of equations is some constant c > 0 times the number n of unknowns, that is, m >= cn. Empirically, the upshots of this contribution are: i) perfect signal recovery in the high-dimensional regime given only an information-theoretic limit number of equations; and, ii) (near-) optimal statistical accuracy in the presence of additive noise. Extensive numerical tests using both synthetic data and real images corroborate its improved signal recovery performance and computational efficiency relative to state-of-the-art approaches.	[Wang, Gang; Chen, Jie] Beijing Inst Technol, Key Lab Intell Contr & Decis Complex Syst, Beijing, Peoples R China; [Wang, Gang; Giannakis, Georgios B.] Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA; [Wang, Gang; Giannakis, Georgios B.] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA; [Saad, Yousef] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	Beijing Institute of Technology; University of Minnesota System; University of Minnesota Twin Cities; University of Minnesota System; University of Minnesota Twin Cities; University of Minnesota System; University of Minnesota Twin Cities	Wang, G (corresponding author), Beijing Inst Technol, Key Lab Intell Contr & Decis Complex Syst, Beijing, Peoples R China.; Wang, G (corresponding author), Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA.; Wang, G (corresponding author), Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.	gangwang@umn.edu; georgios@umn.edu; saad@umn.edu; chenjie@bit.edu.cn	Giannakis, Georgios/Z-4413-2019; Wang, Gang/I-9061-2019	Giannakis, Georgios/0000-0002-0196-0260; Wang, Gang/0000-0002-7266-2412; Saad, Yousef or Youcef/0000-0002-8614-5360	NSF [1500713, 1514056, 1505970]; National Natural Science Foundation of China [U1509215, 61621063]; Program for Changjiang Scholars and Innovative Research Team in University [IRT1208]	NSF(National Science Foundation (NSF)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Changjiang Scholars and Innovative Research Team in University(Program for Changjiang Scholars & Innovative Research Team in University (PCSIRT))	G. Wang and G. B. Giannakis were partially supported by NSF grants 1500713 and 1514056. Y. Saad was partially supported by NSF grant 1505970. J. Chen was partially supported by the National Natural Science Foundation of China grants U1509215, 61621063, and the Program for Changjiang Scholars and Innovative Research Team in University (IRT1208).	[Anonymous], 2015, ADV NEURAL INFORM PR; [Anonymous], 2017, ARXIV170406256; [Anonymous], 2001, LECT MODERN CONVEX O; Balan R, 2006, APPL COMPUT HARMON A, V20, P345, DOI 10.1016/j.acha.2005.07.001; Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Conca A, 2015, APPL COMPUT HARMON A, V38, P346, DOI 10.1016/j.acha.2014.06.005; Duchi J. C., 2017, ARXIV170502356; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; GERCHBERG RW, 1972, OPTIK, V35, P237; Goldstein T., 2016, ARXIV161007531V1; Hand P., 2016, ARXIV161103935; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Lu Y. M., 2017, ARXIV170206435; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Pardalos P. M., 1991, J GLOBAL OPTIM, V1, P15; Pereyra Gabriel, 2017, ICLRW, P5; RICE JR, 1992, NUMERICAL METHODS SO; SAAD Y., 2011, NUMERICAL METHODS LA; Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673; Soltanolkotabi M, 2017, ARXIV170206175; Sun J., 2017, FDN COMPUT MATH; Waldspurger I., 2016, AXIV160903088; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Wang G., 2016, P ADV NEUR INF PROC, P568; Wang G, 2017, IEEE T SIGNAL PROCES, V65, P1961, DOI 10.1109/TSP.2017.2652392; Wang LG, 2017, IEEE INT SYMP INFO; Yi XY, 2014, PR MACH LEARN RES, V32, P613; Zhang H., 2017, J MACH LEARN RES	30	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401087
C	Yang, F; Ramdas, A; Jamieson, K; Wainwright, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yang, Fanny; Ramdas, Aaditya; Jamieson, Kevin; Wainwright, Martin			A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications, e.g. when pharmaceutical companies test new treatment options against control pills for different diseases, or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances, which can be continuously monitored by the data scientist. When interleaving the MAB tests with an online false discovery rate (FDR) algorithm, we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using rejection thresholds of online-FDR algorithms as the confidence levels for the MAB algorithms results in both sample-optimality, high power and low FDR at any point in time. We run extensive simulations to verify our claims, and also report results on real data collected from the New Yorker Cartoon Caption contest.	[Yang, Fanny] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA; [Ramdas, Aaditya; Wainwright, Martin] Univ Calif Berkeley, Dept EECS & Stat, Berkeley, CA USA; [Jamieson, Kevin] Univ Washington, Allen Sch CSE, Seattle, WA 98195 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of Washington; University of Washington Seattle	Yang, F (corresponding author), Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.	fanny-yang@berkeley.edu; ramdas@berkeley.edu; jamieson@cs.washington.edu; wainwrig@berkeley.edu	Jeong, Yongwook/N-7413-2016		Office of Naval Research MURI [DOD-002888]; Air Force Office of Scientific Research Grant [AFOSR-FA9550-14-1-001]; National Science Foundation [CIF-31712-23800, DMS-1309356]	Office of Naval Research MURI(MURIOffice of Naval Research); Air Force Office of Scientific Research Grant(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Science Foundation(National Science Foundation (NSF))	This work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, and National Science Foundation Grants CIF-31712-23800 and DMS-1309356.	Aharoni E, 2014, J R STAT SOC B, V76, P771, DOI 10.1111/rssb.12048; Balsubramani A., 2016, P C UNCERTAINTY ARTI, P42; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Foster DP, 2008, J R STAT SOC B, V70, P429, DOI 10.1111/j.1467-9868.2007.00643.x; Jamieson K., 2014, C LEARN THEOR, P423; Javanmard A., 2017, ANN STAT; Javanmard A., 2015, ARXIV150206197; Johari Ramesh, 2015, ARXIV151204922, V2015; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Kaufmann Emilie, 2015, J MACHINE LEARNING R; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; RAMDAS A., 2017, ADV NEURAL INFORM PR; Simchowitz M., 2017, ARXIV170205186; Villar SS, 2015, STAT SCI, V30, P199, DOI 10.1214/14-STS504	14	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406004
C	Zanca, D; Gori, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zanca, Dario; Gori, Marco			Variational Laws of Visual Attention for Dynamic Scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Computational models of visual attention are at the crossroad of disciplines like cognitive science, computational neuroscience, and computer vision. This paper proposes a model of attentional scanpath that is based on the principle that there are foundational laws that drive the emergence of visual attention. We devise variational laws of the eye-movement that rely on a generalized view of the Least Action Principle in physics. The potential energy captures details as well as peripheral visual features, while the kinetic energy corresponds with the classic interpretation in analytic mechanics. In addition, the Lagrangian contains a brightness invariance term, which characterizes significantly the scanpath trajectories. We obtain differential equations of visual attention as the stationary point of the generalized action, and we propose an algorithm to estimate the model parameters. Finally, we report experimental results to validate the model in tasks of saliency detection.	[Zanca, Dario] Univ Florence, DINFO, Florence, Italy; [Zanca, Dario; Gori, Marco] Univ Siena, DIISM, Siena, Italy	University of Florence; University of Siena	Zanca, D (corresponding author), Univ Florence, DINFO, Florence, Italy.; Zanca, D (corresponding author), Univ Siena, DIISM, Siena, Italy.	dario.zanca@unifi.it; marco@diism.unisi.it	Jeong, Yongwook/N-7413-2016; Zanca, Dario/AAE-8112-2019	ZANCA, DARIO/0000-0001-5886-0597				[Anonymous], 2015, CAT2000 LARGE SCALE; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Bruce N., 2007, J VIS, V7; Bylinskii Zoya, 2016, ARXIV160403605; Connor CE, 2004, CURR BIOL, V14, pR850, DOI 10.1016/j.cub.2004.09.041; Cornia  M., 2016, PREDICTING HUMAN EYE; Garcia-Diaz A, 2012, J VISION, V12, DOI 10.1167/12.6.17; Gelfand I. M., 1993, CALCULUS VARIATION; Hadizadeh H., 2012, IEEE T IMAGE PROCESS; HAINLINE L, 1984, VISION RES, V24, P1771, DOI 10.1016/0042-6989(84)90008-7; Harel J., SALIENCY IMPLEMENTAT; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500; Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007; Jones E., 2001, SCIPY OPEN SOURCE SC; Judd T., 2012, TECHNICAL REPORT; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; KOCH C, 1985, HUM NEUROBIOL, V4, P219; Kruthiventi S. S. S, 2015, ARXIV151002927; Le Meur O, 2015, VISION RES, V116, P152, DOI 10.1016/j.visres.2014.12.026; Maggini M, 2016, LECT NOTES COMPUT SC, V10037, P321, DOI 10.1007/978-3-319-49130-1_24; McMains S, 2011, J NEUROSCI, V31, P587, DOI 10.1523/JNEUROSCI.3766-10.2011; Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Vig E., 2014, IEEE C COMP VIS PATT; Xu M., 2017, IEEE T IMAGE PROCESS; Zhang J., 2013, P IEEE INT C COMP VI	29	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403086
C	Hadfield-Menell, D; Dragan, A; Abbeel, P; Russell, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart			Cooperative Inverse Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.	[Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94709 USA	University of California System; University of California Berkeley	Hadfield-Menell, D (corresponding author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94709 USA.	dhm@cs.berkeley.edu; anca@cs.berkeley.edu; pabbeel@cs.berkeley.edu; russell@cs.berkeley.edu			DARPA Simplifying Complexity in Scientific Discovery (SIMPLEX) program; Berkeley Deep Drive Center; Center for Human Compatible AI; Future of Life Institute; Defense Sciences Office [N66001-15-2-4048]; NSF Graduate Research Fellowship	DARPA Simplifying Complexity in Scientific Discovery (SIMPLEX) program; Berkeley Deep Drive Center; Center for Human Compatible AI; Future of Life Institute; Defense Sciences Office; NSF Graduate Research Fellowship(National Science Foundation (NSF))	This work was supported by the DARPA Simplifying Complexity in Scientific Discovery (SIMPLEX) program, the Berkeley Deep Drive Center, the Center for Human Compatible AI, the Future of Life Institute, and the Defense Sciences Office contract N66001-15-2-4048. Dylan Hadfield-Menell is also supported by a NSF Graduate Research Fellowship.	Abbeel P., 2004, ICML; Balbach F, 2009, LANGUAGE AUTOMATA TH; Bernstein Daniel S., 2000, UAI; Bostrom N., 2014, SUPERINTELLIGENCE PA; Boutilier C, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P478; Cakmak M., 2012, 26 AAAI C ART INT; Dragan A., 2013, ROBOTICS SCI SYSTEMS, DOI [10.15607/RSS.2013.IX.024, DOI 10.15607/RSS.2013.IX.024]; Fern A, 2014, J ARTIF INTELL RES, V50, P71, DOI 10.1613/jair.4213; Gibbons R, 1998, TECHNICAL REPORT; GOLDMAN SA, 1993, SIAM J COMPUT, V22, P1006, DOI 10.1137/0222062; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Golland D., 2010, P 2010 C EMP METH NA, P410; JENSEN MC, 1976, J FINANC ECON, V3, P305, DOI 10.1016/0304-405X(76)90026-X; KERR S, 1975, ACAD MANAGE J, V18, P769, DOI 10.2307/255378; Kuleshov V., 2015, WEB INTERNET EC; LEVESON NG, 1993, COMPUTER, V26, P18, DOI 10.1109/MC.1993.274940; Natarajan S, 2010, INT C MACH LEARN APP; Nayyar A, 2013, IEEE T AUTOMAT CONTR, V58, P1644, DOI 10.1109/TAC.2013.2239000; Ng A. Y., 2000, ICML; Ramachandran D., 2007, IJCAI; Ratliff N. D, 2006, ICML; Russell SP, 2010, ARTIFICIAL INTELLIGE; Russell Stuart, 1998, COLT; Waugh K., 2011, ICML; WIENER N, 1960, SCIENCE, V131, P1355, DOI 10.1126/science.131.3410.1355; Ziebart B. D., 2008, AAAI	26	5	5	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703056
C	Kawaguchi, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kawaguchi, Kenji			Deep Learning without Poor Local Minima	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist "bad" saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.	[Kawaguchi, Kenji] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Kawaguchi, K (corresponding author), MIT, Cambridge, MA 02139 USA.	kawaguch@mit.edu			NSF [1420927]; ONR [N00014-14-1-0486]; ARO [W911NF1410433]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO	The author would like to thank Prof. Leslie Kaelbling, Quynh Nguyen, Li Huan and Anirbit Mukherjee for their thoughtful comments on the paper. We gratefully acknowledge support from NSF grant 1420927, from ONR grant N00014-14-1-0486, and from ARO grant W911NF1410433.	[Anonymous], 2014, INT C LEARN REPR; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; BALDI P., 1989, ADV NEURAL INFORM PR, V1, P65; Baldi P, 2012, NEURAL NETWORKS, V33, P136, DOI 10.1016/j.neunet.2012.04.011; BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3; Choromanska A., 2015, COLT, P1756; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Livni R., 2014, NIPS, V1, P855; Mhaskar Hrushikesh, 2016, 45 MIT CBMM; MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; Zhang F, 2006, SCHUR COMPLEMENT ITS, V4	15	5	5	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700076
C	Kumagai, W		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kumagai, Wataru			Learning Bound for Parameter Transfer Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability and parameter transfer learnability of parametric feature mapping, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.	[Kumagai, Wataru] Kanagawa Univ, Fac Engn, Yokohama, Kanagawa, Japan	Kanagawa University	Kumagai, W (corresponding author), Kanagawa Univ, Fac Engn, Yokohama, Kanagawa, Japan.	kumagai@kanagawa-u.ac.jp						[Anonymous], 2013, INT C MACHINE LEARNI; Arora S., 2015, ARXIV150300778, P113; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Dai Wenyuan, 2008, P 25 INT C MACH LEAR, P2, DOI DOI 10.1145/1390156.1390182; Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343; Lee H, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1113; Mairal J., 2009, ADV NEURAL INFORM PR, P1033; Maurer A., 2012, ARXIV12090738; Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592; Sridharan K., 2009, ADV NEURAL INFORM PR, P1545; Trindade L, 2013, INT CONF MACH LEARN, P277, DOI 10.1109/ICMLC.2013.6890481; Vainsencher D, 2011, J MACH LEARN RES, V12, P3259; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zhu XF, 2013, PATTERN RECOGN, V46, P215, DOI 10.1016/j.patcog.2012.07.018	18	5	5	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700100
C	Lu, J; Liang, GN; Sun, JW; Bi, JB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lu, Jin; Liang, Guannan; Sun, Jiangwen; Bi, Jinbo			A Sparse Interactive Model for Matrix Completion with Side Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				LOW-RANK MATRIX; THRESHOLDING ALGORITHM	Matrix completion methods can benefit from side information besides the partially observed matrix. The use of side features that describe the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix. We propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries. Unlike early methods, this model does not require the low rank condition on the model parameter matrix. We prove that when the side features span the latent feature space of the matrix to be recovered, the number of observed entries needed for an exact recovery is O(log N) where N is the size of the matrix. If the side features are corrupted latent features of the matrix with a small perturbation, our method can achieve an epsilon-recovery with O(log N) sample complexity. If side information is useless, our method maintains a O(N-3/2) sampling rate similar to classic methods. An efficient linearized Lagrangian algorithm is developed with a convergence guarantee. Empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets.	[Lu, Jin; Liang, Guannan; Sun, Jiangwen; Bi, Jinbo] Univ Connecticut, Storrs, CT 06269 USA	University of Connecticut	Lu, J (corresponding author), Univ Connecticut, Storrs, CT 06269 USA.	jin.lu@uconn.edu; guannan.liang@uconn.edu; jiangwen.sun@uconn.edu; jinbo.bi@uconn.edu			NSF [IIS-1320586, DBI-1356655, CCF-1514357]; NIH [R01DA037349]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Jinbo Bi and her students Jin Lu, Guannan Liang and Jiangwen Sun were supported by NSF grants IIS-1320586, DBI-1356655, and CCF-1514357 and NIH R01DA037349.	Abernethy J, 2009, J MACH LEARN RES, V10, P803; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen P, 2004, IEEE T PATTERN ANAL, V26, P1051, DOI 10.1109/TPAMI.2004.52; Chen TQ, 2012, J MACH LEARN RES, V13, P3619; Chiang K.-Y., 2015, NIPS, V28, P3429; Chiang KY, 2016, PR MACH LEARN RES, V48; Daemen A, 2013, GENOME BIOL, V14, DOI 10.1186/gb-2013-14-10-r110; Fang EX, 2015, MATH PROGRAM COMPUT, V7, P149, DOI 10.1007/s12532-015-0078-2; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Jain P., 2013, ABS13060626 CORR; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Lin Z., 2010, MATH PROGRAMMING; Liu GC, 2016, IEEE T SIGNAL PROCES, V64, P5623, DOI 10.1109/TSP.2016.2586753; Menon A. K., 2011, P 17 ACM SIGKDD INT, P141, DOI DOI 10.1145/2020408.2020436; Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269; Ning X., 2012, 6 ACM C RECOMMENDER, P155, DOI DOI 10.1145/2365952.2365983; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Shamir O, 2014, J MACH LEARN RES, V15, P3401; Shi W, 2015, IEEE T SIGNAL PROCES, V63, P6013, DOI 10.1109/TSP.2015.2461520; Sindhwani Vikas, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1055, DOI 10.1109/ICDM.2010.164; Srebro N., 2005, RANK TRACE NORM MAX, P545; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Weng ZY, 2012, INT CONF ACOUST SPEE, P2697, DOI 10.1109/ICASSP.2012.6288473; Xu Miao, 2013, ADV NEURAL INFORM PR, P2301, DOI DOI 10.5555/2999792.2999869; Yang J., 2013, MATH COMPUT, V82	29	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702012
C	Moon, T; Min, S; Lee, B; Yoon, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Moon, Taesup; Min, Seonwoo; Lee, Byunghan; Yoon, Sungroh			Neural Universal Discrete Denoiser	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ERRORS	We present a new framework of applying deep neural networks (DNN) to devise a universal discrete denoiser. Unlike other approaches that utilize supervised learning for denoising, we do not require any additional training data. In such setting, while the ground-truth label, i.e., the clean data, is not available, we devise "pseudolabels" and a novel objective function such that DNN can be trained in a same way as supervised learning to become a discrete denoiser. We experimentally show that our resulting algorithm, dubbed as Neural DUDE, significantly outperforms the previous state-of-the-art in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice.	[Moon, Taesup] DGIST, Daegu 42988, South Korea; [Min, Seonwoo; Lee, Byunghan; Yoon, Sungroh] Seoul Natl Univ, Seoul 08826, South Korea	Daegu Gyeongbuk Institute of Science & Technology (DGIST); Seoul National University (SNU)	Moon, T (corresponding author), DGIST, Daegu 42988, South Korea.	tsmoon@dgist.ac.kr; mswzeus@snu.ac.kr; styxkr@snu.ac.kr; sryoon@snu.ac.kr			DGIST Faculty Start-up Fund [2016010060]; Basic Science Research Program through the National Research Foundation of Korea [2016R1C1B2012170]; Ministry of Science, ICT and Future Planning; Brain Korea 21 Plus Project (SNU ECE)	DGIST Faculty Start-up Fund; Basic Science Research Program through the National Research Foundation of Korea(National Research Foundation of Korea); Ministry of Science, ICT and Future Planning(Ministry of Science, ICT & Future Planning, Republic of Korea); Brain Korea 21 Plus Project (SNU ECE)	T. Moon was supported by DGIST Faculty Start-up Fund (2016010060) and Basic Science Research Program through the National Research Foundation of Korea (2016R1C1B2012170), both funded by Ministry of Science, ICT and Future Planning. S. Min, B. Lee, and S. Yoon were supported in part by Brain Korea 21 Plus Project (SNU ECE) in 2016.	Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y, 2001, ADV NEUR IN, V13, P932; Benitez-Paez A., 2015, BIORXIV021758; BURGER HC, 2012, PROC CVPR IEEE, P2392, DOI DOI 10.1109/CVPR.2012.6247952; Goodwin S, 2015, GENOME RES, V25, P1750, DOI 10.1101/gr.191395.115; Jain M, 2015, NAT METHODS, V12, P351, DOI [10.1038/NMETH.3290, 10.1038/nmeth.3290]; Jain V, 2008, P ADV NEUR INF PROC, P769; Kingma D.P, P 3 INT C LEARNING R; Laehnemann D, 2016, BRIEF BIOINFORM, V17, P154, DOI 10.1093/bib/bbv029; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee B., 2016, ARXIV151104836; Moon T., 2009, IEEE T INFORM THEORY; Motta G, 2011, IEEE T IMAGE PROCESS, V20, P1, DOI 10.1109/TIP.2010.2053939; Ordentlich E., 2003, IEEE ICIP; Ordentlich E, 2008, IEEE T INFORM THEORY, V54, P2243, DOI 10.1109/TIT.2008.920187; Salmela L, 2011, BIOINFORMATICS, V27, P1455, DOI 10.1093/bioinformatics/btr170; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Tieleman, 2012, 65 U TOR; Weissman T, 2005, IEEE T INFORM THEORY, V51, P5, DOI 10.1109/TIT.2004.839518; Weissman T, 2007, IEEE T INFORM THEORY, V53, P1253, DOI 10.1109/TIT.2007.892782; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605	22	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704022
C	Picheny, V; Gramacy, RB; Wild, S; Le Digabel, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Picheny, Victor; Gramacy, Robert B.; Wild, Stefan; Le Digabel, Sebastien			Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					An augmented Lagrangian (AL) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems, which are then usually solved with local solvers. Recently, surrogate-based Bayesian optimization (BO) sub-solvers have been successfully deployed in the AL framework for a more global search in the presence of inequality constraints; however, a drawback was that expected improvement (EI) evaluations relied on Monte Carlo. Here we introduce an alternative slack variable AL, and show that in this formulation the EI may be evaluated with library routines. The slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof. We show our new slack "ALBO" compares favorably to the original. Its superiority over conventional alternatives is reinforced on several mixed constraint examples.	[Picheny, Victor] Univ Toulouse, MIAT, INRA, Castanet Tolosan, France; [Gramacy, Robert B.] Virginia Tech, Blacksburg, VA USA; [Wild, Stefan] Argonne Natl Lab, 9700 S Cass Ave, Argonne, IL 60439 USA; [Le Digabel, Sebastien] Ecole Polytech Montreal, Montreal, PQ, Canada	INRAE; Virginia Polytechnic Institute & State University; United States Department of Energy (DOE); Argonne National Laboratory; Universite de Montreal; Polytechnique Montreal	Picheny, V (corresponding author), Univ Toulouse, MIAT, INRA, Castanet Tolosan, France.	victor.picheny@toulouse.inra.fr; rbg@vt.edu; sebastien.le-digabel@polymtl.ca	Wild, Stefan/P-4907-2016; Le Digabel, Sébastien/A-7740-2010	Wild, Stefan/0000-0002-6099-2772; Le Digabel, Sébastien/0000-0003-3148-5090	National Science Foundation [DMS-1521702]; U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research [DE-AC02-06CH11357]; Natural Sciences and Engineering Research Council of Canada [418250]	National Science Foundation(National Science Foundation (NSF)); U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research(United States Department of Energy (DOE)); Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR)	We are grateful to Mickael Binois for comments on early drafts. RBG is grateful for partial support from National Science Foundation grant DMS-1521702. The work of SMW is supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research under Contract No. DE-AC02-06CH11357. The work of SLD is supported by the Natural Sciences and Engineering Research Council of Canada grant 418250.	[Anonymous], 2010, ARXIV1012; AUDET C, 2000, AIAA USAF NASA ISSMO; Bertsekas D.P., 2019, REINFORCEMENT LEARNI; Box G. E. P., 1987, EMPIRICAL MODEL BUIL; Boyle P., 2007, THESIS; Gardner J.R, 2014, P 31 INT C MACH LEAR, V32; Gelbart M. A., 2014, UNCERTAINTY ARTIFICI; Gramacy RB, 2016, J STAT SOFTW, V72, P1, DOI 10.18637/jss.v072.i01; Gramacy RB, 2016, TECHNOMETRICS, V58, P1, DOI 10.1080/00401706.2015.1014065; Hernandez-Lobato J.M., 2015, P 32 INT C MACH LEAR, V37; Johnson SG, 2014, NLOPT NONLINEAR OPTI; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Le Digabel S, 2011, ACM T MATH SOFTWARE, V37, DOI 10.1145/1916461.1916468; Mockus J., 1989, BAYESIAN APPROACH GL; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Parr JM, 2012, ENG OPTIMIZ, V44, P1147, DOI 10.1080/0305215X.2011.637556; Picheny V., 2016, DICEOPTIM KRIGING BA; Picheny V, 2014, JMLR WORKSH CONF PRO, V33, P787; Picheny V, 2016, TECHNOMETRICS, V58, P17, DOI 10.1080/00401706.2015.1079246; R Development Core Team, 2004, R LANG ENV STAT COMP; Sasena MJ, 2002, THESIS; Schonlau M., 1998, NEW DEV APPL EXPT DE, V34, P11, DOI [DOI 10.1214/LNMS/1215456182, 10.1214/lnms/1215456182]; Snoek J., 2012, ADV NEURAL INF PROCE	23	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703075
C	Chaturapruek, S; Duchi, JC; Re, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chaturapruek, Sorathan; Duchi, John C.; Re, Chris			Asynchronous stochastic convex optimization: the noise is in the noise and SGD don't care	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				APPROXIMATION	We show that asymptotically, completely asynchronous stochastic gradient procedures achieve optimal (even to constant factors) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures. Roughly, the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony. We also give empirical evidence demonstrating the strong performance of asynchronous, parallel stochastic optimization schemes, demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods. In short, we show that for many stochastic approximation problems, as FreddieMercury sings in Queen's Bohemian Rhapsody, "Nothing really matters."	[Chaturapruek, Sorathan; Re, Chris] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Duchi, John C.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Duchi, John C.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University	Chaturapruek, S (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	sorathan@stanford.edu; jduchi@stanford.edu; chrismre@stanford.edu		Duchi, John/0000-0003-0045-7185				Agarwal A., 2011, ADV NEURAL INFORM PR, V24; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Casella G., 1998, THEORY POINT ESTIMAT; Duchi J. C., 2013, ADV NEURAL INFORM PR, V26; Duchi J. C., 2015, ASYNCHRONOUS STOCHAS; Duchi J. C., 2015, ARXIV150800882MATHOC; Ermoliev Y. M., 1969, KIBERNETIKA, V2, P72; Juditsky A., 2011, STOCHASTIC SYST, V1, P17, DOI 10.1287/10-SSY011; Le Cam L., 2000, ASYMPTOTICS STAT SOM; Lichman M., 2013, UCI MACHINE LEARNING; Liu J., 2014, P 31 INT C MACH LEAR; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Niu F., 2011, ADV NEURAL INFORM PR, V24; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Recht B., 2012, P 25 ANN C COMP LEAR; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Robbins H., 1971, OPTIMIZING METHODS S, P233; Vaart A.W.v.d., 1998, ASYMPTOTIC STAT	21	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103069
C	Henao, R; Gan, Z; Lu, J; Carin, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Henao, Ricardo; Gan, Zhe; Lu, James; Carin, Lawrence			Deep Poisson Factor Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a new deep architecture for topic modeling, based on Poisson Factor Analysis (PFA) modules. The model is composed of a Poisson distribution to model observed vectors of counts, as well as a deep hierarchy of hidden binary units. Rather than using logistic functions to characterize the probability that a latent binary unit is on, we employ a Bernoulli-Poisson link, which allows PFA modules to be used repeatedly in the deep architecture. We also describe an approach to build discriminative topic models, by adapting PFA modules. We derive efficient inference via MCMC and stochastic variational methods, that scale with the number of non-zeros in the data and binary units, yielding significant efficiency, relative to models based on logistic links. Experiments on several corpora demonstrate the advantages of our model when compared to related deep models.	[Henao, Ricardo; Gan, Zhe; Lu, James; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Duke University	Henao, R (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	r.henao@duke.edu; zhe.gan@duke.edu; james.lu@duke.edu; lcarin@duke.edu		Henao, Ricardo/0000-0003-4980-845X; Carin, Lawrence/0000-0001-6277-7948	ARO; DARPA; ONR; NGA; DOE	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); NGA; DOE(United States Department of Energy (DOE))	This research was supported in part by ARO, DARPA, DOE, NGA and ONR.	Blei D., 2004, NIPS; Blei D. M., 2007, AOAS; Blei D. M., 2003, JMLR; Chen T., 2014, ICML; Ding N., 2014, NIPS; Gan Z., 2015, NIPS; Gan Z., 2015, AISTATS; Gan Z., 2015, ICML; Guhaniyogi R, 2014, ARXIV14013632; Hinton G, 2002, NEURAL COMPUTATION; Hinton G. E., 2013, UAI; Hinton G.E., 2006, NEURAL COMPUTATION; Hinton G. E., 2009, NIPS; Ho Q., 2013, NIPS; Hoffman M. D., 2010, NEURAL; Hoffman Matthew D, 2013, JMLR; Lacoste- Julien S., 2009, NIPS; Lauly S., 2012, NIPS; LeCun Y., 1998, P IEEE; Li M., 2014, NIPS; Maaloe L, 2015, ARXIV150104325; Mcauliffe J. D., 2008, NIPS; Neal Radford M., 1992, ARTIFICIAL INTELLIGE; Paisley J., 2015, PAMI; Ranganath R., 2014, AISTATS; Salakhutdinov R., 2009, AISTATS; Teh Y. W., 2006, JASA; Welling M., 2011, ICML; Williamson S., 2010, ICML; Zhou M., 2015, AISTATS; Zhou M., 2012, AISTATS; Zhou M., 2015, PAMI; Zhu J., 2012, JMLR	33	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101046
C	Hensman, J; Matthews, AGD; Filippone, M; Ghahramani, Z		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hensman, James; Matthews, Alexander G. de G.; Filippone, Maurizio; Ghahramani, Zoubin			MCMC for Variationally Sparse Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CLASSIFICATION	Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper is available at github.com/sparseMCMC.	[Hensman, James] Univ Lancaster, CHICAS, Lancaster, England; [Matthews, Alexander G. de G.; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England; [Filippone, Maurizio] EURECOM, Biot, France	Lancaster University; University of Cambridge; IMT - Institut Mines-Telecom; EURECOM	Hensman, J (corresponding author), Univ Lancaster, CHICAS, Lancaster, England.	james.hensman@lancaster.ac.uk; am554@cam.ac.uk; maurizio.filippone@eurecom.fr; zoubin@cam.ac.uk			MRC fellowship; EPSRC [EP/I036575/1]; Google Focussed Research award	MRC fellowship(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google Focussed Research award(Google Incorporated)	JH was funded by an MRC fellowship, AM and ZG by EPSRC grant EP/I036575/1 and a Google Focussed Research award.	Chai KMA, 2012, J MACH LEARN RES, V13, P1745; Christensen OF, 2005, J R STAT SOC B, V67, P253, DOI 10.1111/j.1467-9868.2005.00500.x; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Filippone M, 2013, MACH LEARN, V93, P93, DOI 10.1007/s10994-013-5388-x; Filippone M., 2015, ICML 2015; Gal Y., 2014, NIPS; Gibbs MN, 2000, IEEE T NEURAL NETWOR, V11, P1458, DOI 10.1109/72.883477; Girolami M., 2006, NEURAL COMPUT, V18, P2005; Hensman J., 2014, AISTATS, P351; Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR, P280; Khan Emtiyaz, 2012, ADV NEURAL INFORM PR, P3140; Kim HC, 2006, IEEE T PATTERN ANAL, V28, P1948, DOI 10.1109/TPAMI.2006.238; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Lloyd C., 2015, ICML 2015; Matthews A. G. D., 2015, 150407027 ARXIV; Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115; Murray I., 2010, AISTATS, V9; Murray I., 2010, ADV NEURAL INFORM PR, V23, P1732, DOI DOI 10.5555/2997046.2997089; Nguyen Trung V., 2014, NIPS; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; Sarkka S., 2013, BAYESIAN FILTERING S; Smith SP, 1995, J COMPUT GRAPH STAT, V4, P134, DOI DOI 10.2307/1390762; Snelson E., 2005, ADV NEURAL INFORM PR, P1257; Solin A., 2014, 14015508 ARXIV; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Titsias Michalis K, 2011, BAYESIAN TIME SERIES; Vanhatalo J, 2007, J MACHINE LEARNING R, V1, P73; Wang Z., 2013, ICML, P1462; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Wilson A., 2014, P ADV NEUR INF PROC, V4, P3626	34	5	5	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102024
C	Hosseini, R; Sra, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hosseini, Reshad; Sra, Suvrit			Matrix Manifold Optimization for Gaussian Mixtures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MAXIMUM-LIKELIHOOD; EM ALGORITHM	We take a new look at parameter estimation for Gaussian Mixture Model (GMMs). Specifically, we advance Riemannian manifold optimization (on the manifold of positive definite matrices) as a potential replacement for Expectation Maximization (EM), which has been the de facto standard for decades. An out-of-the-box invocation of Riemannian optimization, however, fails spectacularly: it obtains the same solution as EM, but vastly slower. Building on intuition from geometric convexity, we propose a simple reformulation that has remarkable consequences: it makes Riemannian optimization not only match EM (a nontrivial result on its own, given the poor record nonlinear programming has had against EM), but also outperform it in many settings. To bring our ideas to fruition, we develop a welltuned Riemannian LBFGS method that proves superior to known competing methods (e.g., Riemannian conjugate gradient). We hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics.	[Hosseini, Reshad] Univ Tehran, Coll Engn, Sch ECE, Tehran, Iran; [Sra, Suvrit] MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA	University of Tehran; Massachusetts Institute of Technology (MIT)	Hosseini, R (corresponding author), Univ Tehran, Coll Engn, Sch ECE, Tehran, Iran.	reshad.hosseini@ut.ac.ir; suvrit@mit.edu	Hosseini, Reshad/AAD-8561-2021					Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bishop C. M., 2006, J ELECT IMAG, V16, P140; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Burer S., 1999, TR9917 RIC U; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duda R.O., 2000, PATTERN CLASSIFICATI; Ge R., 2015, ARXIV150300424; Ghahramani Z., 2003, P INT C MACH LEARN, P672; Hosseini R., 2015, ARXIV150706065; Hosseini R., 2015, ARXIV150607677; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Keener RW, 2010, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-93839-4; Lee J., 2012, GRADUATE TEXTS MATH, DOI 10.1007/978-1-4419-9982-5; Ma JW, 2000, NEURAL COMPUT, V12, P2881, DOI 10.1162/089976600300014764; Mclachlan G., 2000, WILEY SER PROB STAT; Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15; Naim I., 2012, P 29 INT COFERENCE I, P1427; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X; Sra S., 2013, ADV NEURAL INFORM PR, P2562; Sra S, 2015, SIAM J OPTIMIZ, V25, P713, DOI 10.1137/140978168; Udriste C., 1994, CONVEX FUNCTIONS OPT; Vanderbei R. J., 2000, TECHNICAL REPORT; Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768; Verbeek JJ, 2003, NEURAL COMPUT, V15, P469, DOI 10.1162/089976603762553004; Wiesel A, 2012, IEEE T SIGNAL PROCES, V60, P6182, DOI 10.1109/TSP.2012.2218241; Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129; Zoran D., 2012, ADV NEURAL INF PROCE, P1736	35	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101072
C	Hughes, MC; Stephenson, W; Sudderth, EB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hughes, Michael C.; Stephenson, William; Sudderth, Erik B.			Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DISCOVERY	Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.	[Hughes, Michael C.; Stephenson, William; Sudderth, Erik B.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA	Brown University	Hughes, MC (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.	mhughes@cs.brown.edu; wtstephe@gmail.com; sudderth@cs.brown.edu	Hughes, Michael C./AAO-7155-2021	Hughes, Michael C./0000-0003-4859-7400; Sudderth, Erik/0000-0002-0595-9726	NSF CAREER Award [IIS-1349774]; NSF Graduate Research Fellowship [DGE0228243]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF Graduate Research Fellowship(National Science Foundation (NSF))	This research supported in part by NSF CAREER Award No. IIS-1349774. M. Hughes supported in part by an NSF Graduate Research Fellowship under Grant No. DGE0228243.	Beal M.J, 2003, THESIS; Beal MJ, 2001, ADV NEURAL INFORM PR, P577; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Bryant M., 2012, ADV NEURAL INFORM PR, p[2708, 2716]; Chang J, 2014, ADV NEUR IN, V27; Ernst J, 2010, NAT BIOTECHNOL, V28, P817, DOI 10.1038/nbt.1662; Foti N., 2014, ADV NEURAL INFORM PR, P3599; Fox EB, 2014, ANN APPL STAT, V8, P1281, DOI 10.1214/14-AOAS742; Fox EB, 2011, ANN APPL STAT, V5, P1020, DOI 10.1214/10-AOAS395; Ghahramani Z, 2001, INT J PATTERN RECOGN, V15, P9, DOI 10.1142/S0218001401000836; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hoffman MM, 2012, NAT METHODS, V9, P473, DOI [10.1038/NMETH.1937, 10.1038/nmeth.1937]; Hughes MC, 2015, JMLR WORKSH CONF PRO, V38, P370; Johnson MJ, 2014, PR MACH LEARN RES, V32, P1854; Liang P., 2007, P 2007 JOINT C EMP M, P688; NIST, 2007, RICH TRANSCR DAT; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Stolcke A., 1993, P ADV NEUR INF PROC, P11; Sudderth E., 2013, ADV NEURAL INFORM PR, V26, P1133; Teh Y. W., 2008, ADV NEURAL INFORM PR, P1481; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Van Gael J., 2008, PROC 25 INT C MACHIN, V25, P1088; Wang C., 2012, ADV NEURAL INFORM PR; Wang C., 2012, ARXIV12011657	24	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100029
C	Kawale, J; Bui, H; Kveton, B; Thanh, LT; Chawla, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kawale, Jaya; Bui, Hung; Kveton, Branislav; Thanh, Long Tran; Chawla, Sanjay			Efficient Thompson Sampling for Online Matrix-Factorization Recommendation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DISTRIBUTIONS	Matrix factorization (MF) collaborative filtering is an effective and widely used method in recommendation systems. However, the problem of finding an optimal trade-off between exploration and exploitation (otherwise known as the bandit problem), a crucial problem in collaborative filtering from cold-start, has not been previously addressed. In this paper, we present a novel algorithm for online MF recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items. Our approach, called Particle Thompson sampling for MF (PTS), is based on the general Thompson sampling framework, but augmented with a novel efficient online Bayesian probabilistic matrix factorization method based on the Rao-Blackwellized particle filter. Extensive experiments in collaborative filtering using several real-world datasets demonstrate that PTS significantly outperforms the current state-of-the-arts.	[Kawale, Jaya; Bui, Hung; Kveton, Branislav] Adobe Res, San Jose, CA 95120 USA; [Thanh, Long Tran] Univ Southampton, Southampton, Hants, England; [Chawla, Sanjay] Qatar Comp Res Inst, Doha, Qatar; [Chawla, Sanjay] Univ Sydney, Sydney, NSW, Australia	Adobe Systems Inc.; University of Southampton; Qatar Foundation (QF); Hamad Bin Khalifa University-Qatar; Qatar Computing Research Institute; University of Sydney	Kawale, J (corresponding author), Adobe Res, San Jose, CA 95120 USA.	kawale@adobe.com; hubui@adobe.com; kveton@adobe.com; ltt08r@ecs.soton.ac.uk; sanjay.chawla@sydney.edu.au						Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; [Anonymous], 2008, P 25 INT C MACH LEAR; Arnold BC, 2000, STAT PROBABIL LETT, V49, P355, DOI 10.1016/S0167-7152(00)00068-7; Arnold BC, 2001, STAT SCI, V16, P249; Chopin N, 2002, BIOMETRIKA, V89, P539, DOI 10.1093/biomet/89.3.539; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; GELMAN A, 1991, AM STAT, V45, P125, DOI 10.2307/2684374; Gentile C., 2014, ARXIV14018257; Gopalan A, 2014, PR MACH LEARN RES, V32; Kocak T, 2014, AAAI CONF ARTIF INTE, P1911; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Maillard OA, 2014, PR MACH LEARN RES, V32; SALAKHUTDINOV C, 2007, NIPS, V1, P2; Valko M., 2014, 31 INT C MACH LEARN; Zhao XX, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1411, DOI 10.1145/2505515.2505690	17	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103023
C	Lienart, T; Teh, YW; Doucet, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lienart, Thibaut; Teh, Yee Whye; Doucet, Arnaud			Expectation Particle Belief Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.	[Lienart, Thibaut; Teh, Yee Whye; Doucet, Arnaud] Univ Oxford, Dept Stat, Oxford, England	University of Oxford	Lienart, T (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	lienart@stats.ox.ac.uk; teh@stats.ox.ac.uk; doucet@stats.ox.ac.uk		Doucet, Arnaud/0000-0002-7662-419X	EPSRC [EP/K000276/1, EP/K009850/1, 1379622, EP/K009362/1]; Scatcherd European scholarship scheme; ERC under the EU's FP7 Programme [617411]; AFOSR/AOARD [AOARD-144042]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Scatcherd European scholarship scheme; ERC under the EU's FP7 Programme; AFOSR/AOARD(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	We thank Alexander Ihler and Drew Frank for sharing their implementation of Particle Belief Propagation. TL gratefully acknowledges funding from EPSRC (grant 1379622) and the Scatcherd European scholarship scheme. YWT's research leading to these results has received funding from EPSRC (grant EP/K009362/1) and ERC under the EU's FP7 Programme (grant agreement no. 617411). AD's research was supported by the EPSRC (grant EP/K000276/1, EP/K009850/1) and by AFOSR/AOARD (grant AOARD-144042).	Briers M, 2005, 2005 7TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), VOLS 1 AND 2, P705; Briers M, 2010, ANN I STAT MATH, V62, P61, DOI 10.1007/s10463-009-0236-2; Crick C, 2003, UNCERTAINTY ARTIFICI, P159; Felzenszwalb Pedro F., 2004, INT J COMP VIS, V59; Ihler A., 2009, P AISTATS, V5, P256; Ihler AT, 2005, IEEE J SEL AREA COMM, V23, P809, DOI 10.1109/JSAC.2005.843548; Klaus A, 2006, INT C PATT RECOG, P15; MINKA T, 2004, MSRTR2004149; Minka T.P., 2001, P 17 C UNC ART INT, P362; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Naesseth C.A., 2014, ADV NEURAL INFORM PR, P1862; Nikolova M, 2000, IEEE T SIGNAL PROCES, V48, P3437, DOI 10.1109/78.887035; Noorshams N, 2013, J MACH LEARN RES, V14, P2799; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schiff J, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1369, DOI 10.1109/IROS.2009.5354772; Sudderth EB, 2003, PROC CVPR IEEE, P605; Sudderth EB, 2010, COMMUN ACM, V53, P95, DOI 10.1145/1831407.1831431; Sudderth Erik B., 2004, P IEEE COMP VIS PATT; Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; YEDIDIA J, 2002, CONSTRUCTING FREE EN	22	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100042
C	Makhzani, A; Frey, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Makhzani, Alireza; Frey, Brendan			Winner-Take-All Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we propose a winner-take-allmethod for learning hierarchical sparse representations in an unsupervised fashion. We first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units. We then propose the convolutional winner-take-all autoencoderwhich combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations. We describe a way to train convolutional autoencoders layer by layer, where in addition to lifetime sparsity, a spatial sparsity within each feature map is achieved using winner-take-all activation functions. We will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the MNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets, and achieve competitive classification performance.	[Makhzani, Alireza; Frey, Brendan] Univ Toronto, Toronto, ON, Canada	University of Toronto	Makhzani, A (corresponding author), Univ Toronto, Toronto, ON, Canada.	makhzani@psi.toronto.edu; frey@psi.toronto.edu			NVIDIA	NVIDIA	We would like to thank Ruslan Salakhutdinov and Andrew Delong for the valuable comments. We also acknowledge the support of NVIDIA with the donation of the GPUs used for this research.	Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Coates A., 2011, AISTATS; Coates A., 2011, NIPS; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, P766; Hinton GE, 2012, IMPROVING NEURAL NET; Kavukcuoglu K., 2010, NIPS, P5; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4; Krizhevsky A., 2010, CONVOLUTIONAL DEEP B, P1; Lee H., 2009, P ANN INT C MACH LEA, P609; Lin TH, 2014, PR MACH LEARN RES, V32, P1323; Mairal J, 2014, ADV NEURAL INFORM PR, V27, P2627; Makhzani A., 2014, INT C LEARN REPR ICL; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; Ng A., 2011, CS294A LECT NOTES, V72; Ranzato M.A., 2007, IEEE C COMP VIS PATT, P1; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Warde-Farley D., 2013, ICML; Zeiler M. D., 2012, ARXIV12070151; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	22	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101043
C	Mueller, J; Jaakkola, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mueller, Jonas; Jaakkola, Tommi			Principal Differences Analysis: Interpretable Characterization of Differences between Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SHRINKAGE; SELECTION	We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.	[Mueller, Jonas; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mueller, J (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	jonasmueller@csail.mit.edu; tommi@csail.mit.edu	Mueller, Jonas/AAY-6891-2020		NIH [T32HG004947]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research was supported by NIH Grant T32HG004947.	Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas D.P., 1998, NETWORK OPTIMIZATION; Bertsekas DP, 2012, OPTIMIZATION FOR MACHINE LEARNING, P85; BERTSEKAS DP, 1988, MATH PROGRAM, V42, P203, DOI 10.1007/BF01589405; Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82; Clemmensen L, 2011, TECHNOMETRICS, V53, P406, DOI 10.1198/TECH.2011.08118; Cramer H., 1936, J LONDON MATH SOC, Vs1-11, P290, DOI 10.1112/jlms/s1-11.4.290; Cuesta-Albertos JA, 2007, J THEOR PROBAB, V20, P201, DOI 10.1007/s10959-007-0060-7; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Geiler-Samerotte KA, 2013, CURR OPIN BIOTECH, V24, P752, DOI 10.1016/j.copbio.2013.03.010; Gibbs AL, 2002, INT STAT REV, V70, P419, DOI 10.2307/1403865; Good P.I., 1994, PERMUTATION TESTS PR; Gretton A, 2012, J MACH LEARN RES, V13, P723; Guyon I., 2006, FEATURE EXTRACTION F; Jirak M, 2011, J MULTIVARIATE ANAL, V102, P1032, DOI 10.1016/j.jmva.2011.02.003; Lopes M., 2011, ADV NEURAL INFORM PR, P1206; Rosenbaum PR, 2005, J ROY STAT SOC B, V67, P515, DOI 10.1111/j.1467-9868.2005.00513.x; Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18; Szekely G.J., 2004, INTERSTAT, V5; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Van Der VaartJon A., 1996, WEAK CONVERGENCE EMP; Wang Zhaoran, 2014, Adv Neural Inf Process Syst, V2014, P3383; Wei S, 2015, J COMPUTATIONAL GRAP; Wright SJ, 2010, OPTIMIZATION ALGORIT; Zeisel A, 2015, SCIENCE, V347, P1138, DOI 10.1126/science.aaa1934; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	29	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102043
C	Pan, XH; Papailiopoulos, D; Oymak, S; Recht, B; Ramchandran, K; Jordan, MI		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Pan, Xinghao; Papailiopoulos, Dimitris; Oymak, Samet; Recht, Benjamin; Ramchandran, Kannan; Jordan, Michael I.			Parallel Correlation Clustering on Big Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Given a similarity graph between items, correlation clustering (CC) groups similar items together and dissimilar ones apart. One of the most popular CC algorithms is KwikCluster: an algorithm that serially clusters neighborhoods of vertices, and obtains a 3-approximation ratio. Unfortunately, in practice KwikCluster requires a large number of clustering rounds, a potential bottleneck for large graphs. We present C4 and ClusterWild!, two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds, and provably achieve nearly linear speedups. C4 uses concurrency control to enforce serializability of a parallel clustering process, and guarantees a 3-approximation ratio. ClusterWild! is a coordination free algorithm that abandons consistency for the benefit of better scaling; this leads to a provably small loss in the 3 approximation ratio. We demonstrate experimentally that both algorithms outperform the state of the art, both in terms of clustering accuracy and running time. We show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores, while achieving a 15 x speedup.	[Pan, Xinghao; Papailiopoulos, Dimitris; Oymak, Samet; Recht, Benjamin; Jordan, Michael I.] Univ Calif Berkeley, AMP Lab, Berkeley, CA 94720 USA; [Pan, Xinghao; Papailiopoulos, Dimitris; Oymak, Samet; Recht, Benjamin; Ramchandran, Kannan; Jordan, Michael I.] Univ Calif Berkeley, EECS, Berkeley, CA USA; [Recht, Benjamin; Jordan, Michael I.] Univ Calif Berkeley, Stat, Berkeley, CA USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of California System; University of California Berkeley	Pan, XH (corresponding author), Univ Calif Berkeley, AMP Lab, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513; Alon N, 2006, INVENT MATH, V163, P499, DOI 10.1007/s00222-005-0465-9; Arasu A, 2009, PROC INT CONF DATA, P952, DOI 10.1109/ICDE.2009.43; Bansal N, 2002, ANN IEEE SYMP FOUND, P238, DOI 10.1109/SFCS.2002.1181947; Ben-Dor A, 1999, J COMPUT BIOL, V6, P281, DOI 10.1089/106652799318274; Blelloch G. E., 2012, P S PARALLEL ALGORIT; Boldi P, 2004, SOFTWARE PRACT EXPER, V34, P711, DOI 10.1002/spe.587; Boldi P., 2004, WWW; Bonchi F., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P51, DOI 10.1109/ICDM.2011.114; Bonchi F., 2012, P 18 ACM SIGKDD INT, P1321, DOI DOI 10.1145/2339530.2339735; Bonchi F., 2014, KDD, P1972; Cesa-Bianchi N., 2012, ANN C LEARN THEOR MI, P34; Charikar M, 2004, ANN IEEE SYMP FOUND, P54, DOI 10.1109/FOCS.2004.39; Charikar M, 2003, ANN IEEE SYMP FOUND, P524, DOI 10.1109/SFCS.2003.1238225; Chawla S, 2015, ACM S THEORY COMPUT, P219, DOI 10.1145/2746539.2746604; Chierichetti F, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P641, DOI 10.1145/2623330.2623743; Demaine ED, 2006, THEOR COMPUT SCI, V361, P172, DOI 10.1016/j.tcs.2006.05.008; Elmagarmid AK, 2007, IEEE T KNOWL DATA EN, V19, P1, DOI 10.1109/TKDE.2007.250581; Elsner Micha, 2009, P WORKSH INT LIN PRO, P19; Giotis I, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1167, DOI 10.1145/1109557.1109686; Hussain Bilal, 2013, TECHNICAL REPORT; Krivelevich Michael, 2014, ARXIV14045731; Pan X., 2013, ADV NEURAL INFORM PR, P1403; Puleo Gregory J, 2014, ARXIV14110547; Santini M., 2011, WWW; Swamy Chaitanya, 2004, P S DISCRETE ALGORIT, P526; Yang B, 2007, IEEE T KNOWL DATA EN, V19, P1333, DOI [10.1109/TKDE.2007.1061, 10.1109/TKDE.2007.1061.]	27	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101111
C	Schiratti, JB; Allassonniere, S; Colliot, O; Durrleman, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Schiratti, Jean-Baptiste; Allassonniere, Stephanie; Colliot, Olivier; Durrleman, Stanley			Learning spatiotemporal trajectories from manifold-valued longitudinal data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MAXIMUM-LIKELIHOOD; ALZHEIMERS-DISEASE; PROGRESSION; MODELS	We propose a Bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data, namely repeated measurements of the same objects or individuals at several points in time. The model allows to estimate a group-average trajectory in the space of measurements. Random variations of this trajectory result from spatiotemporal transformations, which allow changes in the direction of the trajectory and in the pace at which trajectories are followed. The use of the tools of Riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints, which lie therefore on a Riemannian manifold. Stochastic approximations of the Expectation-Maximization algorithm is used to estimate the model parameters in this highly non-linear setting. The method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of Alzheimer's disease. Experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease, thus validating the fact that it effectively estimated a normative scenario of disease progression. Random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals.	[Schiratti, Jean-Baptiste; Colliot, Olivier; Durrleman, Stanley] UPMC Univ Paris 06, Sorbonne Univ,Inserm, INRIA Paris,Inst Cerveau & Moelle Epiniere,ICM, U1127,CNRS,UMR 7225,UMR S 1127,ARAMIS Lab, F-75013 Paris, France; [Schiratti, Jean-Baptiste; Allassonniere, Stephanie] Ecole Polytech, CMAP, Palaiseau, France	Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Biology (INSB); Institut National de la Sante et de la Recherche Medicale (Inserm); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Institut Polytechnique de Paris	Schiratti, JB (corresponding author), UPMC Univ Paris 06, Sorbonne Univ,Inserm, INRIA Paris,Inst Cerveau & Moelle Epiniere,ICM, U1127,CNRS,UMR 7225,UMR S 1127,ARAMIS Lab, F-75013 Paris, France.; Schiratti, JB (corresponding author), Ecole Polytech, CMAP, Palaiseau, France.	jean-baptiste.schiratti@cmap.polytechnique.fr; stephanie.allassonniere@polytechnique.edu; olivier.colliot@upmc.fr; stanley.durrleman@inria.fr	Jeong, Yongwook/N-7413-2016					Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229; BRAAK H, 1995, NEUROBIOL AGING, V16, P271, DOI 10.1016/0197-4580(95)00021-6; Delyon B, 1999, ANN STAT, V27, P94; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Diggle P., 2002, J R STAT SOC, DOI DOI 10.2307/2983303; Donohue MC, 2014, ALZHEIMERS DEMENT, V10, pS400, DOI 10.1016/j.jalz.2013.10.003; Durrleman S, 2013, INT J COMPUT VISION, V103, P22, DOI 10.1007/s11263-012-0592-x; Fonteijn HM, 2012, NEUROIMAGE, V60, P1880, DOI 10.1016/j.neuroimage.2012.01.062; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Hirsch M., 2012, DIFFERENTIAL TOPOLOG; Hyvarinen A, 2004, INDEPENDENT COMPONEN, V46; Jack CR, 2010, LANCET NEUROL, V9, P119, DOI 10.1016/S1474-4422(09)70299-6; Kuhn E, 2005, COMPUT STAT DATA AN, V49, P1020, DOI 10.1016/j.csda.2004.07.002; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Singer J. D., 2003, APPL LONGITUDINAL DA; Singh Nikhil, 2013, Inf Process Med Imaging, V23, P560, DOI 10.1007/978-3-642-38868-2_47; Su JY, 2014, ANN APPL STAT, V8, P530, DOI 10.1214/13-AOAS701	17	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100031
C	Wei, K; Iyer, R; Wang, SJ; Bai, WR; Bilmes, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wei, Kai; Iyer, Rishabh; Wang, Shengjie; Bai, Wenruo; Bilmes, Jeff			Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call Submodular Partitioning. These problems generalize purely robust instances of the problem, namely max-min submodular fair allocation (SFA) [12] and min-max submodular load balancing (SLB) [25], and also average-case instances, that is the submodular welfare problem (SWP) [26] and submodular multiway partition (SMP) [5]. While the robust versions have been studied in the theory community [11, 12, 16, 25, 26], existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art. We moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives. We show that these problems have many applications in machine learning (ML), including data partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation.	[Wei, Kai; Iyer, Rishabh; Bai, Wenruo; Bilmes, Jeff] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA; [Wang, Shengjie] Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Wei, K (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.	kaiwei@u.washington.edu; rkiyer@u.washington.edu; wangsj@u.washington.edu; wrbai@u.washington.edu; bilmes@u.washington.edu			National Science Foundation [IIS-1162606]; National Institutes of Health [R01GM103544]; Google; Microsoft; Intel; Microsoft Research Ph.D Fellowship; TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program - MARCO; TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program - DARPA	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Google(Google Incorporated); Microsoft(Microsoft); Intel(Intel Corporation); Microsoft Research Ph.D Fellowship(Microsoft); TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program - MARCO; TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program - DARPA	This material is based upon work supported by the National Science Foundation under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft, and an Intel research award. R. Iyer acknowledges support from a Microsoft Research Ph.D Fellowship. This work was supported in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.	Garcia-Escudero LA, 2010, ADV DATA ANAL CLASSI, V4, P89, DOI 10.1007/s11634-010-0064-5; [Anonymous], 2014, ARXIV14107455; [Anonymous], 2011, FDN TRENDS MACHINE L; Arthur D, 2007, SODA; Asadpour A., 2010, SICOMP; Buchbinder N., 2012, FOCS; Chekuri C., 2011, FOCS; Chekuri C, 2011, LECT NOTES COMPUT SC, V6755, P354, DOI 10.1007/978-3-642-22006-7_30; Ene A., 2013, SODA; Fisher M., 1978, POLYHEDRAL COMBINATO; Goemans M., 2009, SODA; Golovin D, 2005, CMUCS05144; Iyer R., 2013, ICML; Iyer R., MONOTONE CLOSURE REL; Jegelka S., 2011, CVPR; Khot S., 2007, APPROX; Kolmogorov V., 2004, TPAMI; Krause A., 2008, JMLR; Lenstra J. K., 1990, MATH PROGRAMMING; Li M., 2015, ARXIV PREPRINT ARXIV; Minoux M., 1978, OPTIMIZATION TECHNIQ; Narasimhan M., 2005, NIPS; Orlin James B., 2009, MATH PROGRAMMING; Svitkina Z., 2008, FOCS; Vondrak Jan, 2008, STOC; Wei K., MIXED ROBUST AVERAGE; Wei K., 2015, ICML; Zhao L, 2004, DISCRETE APPL MATH, V143, P130, DOI 10.1016/j.dam.2003.10.007	29	5	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100074
C	Yi, XY; Caramanis, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yi, Xinyang; Caramanis, Constantine			Regularized EM Algorithms: A Unified Framework and Statistical Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MATRIX RECOVERY; RANK	Latent models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in [1] has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M-step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M-step using the state-of-the-art highdimensional prescriptions (e.g., a la [19]) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.	[Yi, Xinyang; Caramanis, Constantine] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Yi, XY (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	yixy@utexas.edu; constantine@utexas.edu		Caramanis, Constantine/0000-0001-9939-8378	NSF [1056028, 1302435, 1116955]; U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center	NSF(National Science Foundation (NSF)); U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center	The authors would like to acknowledge NSF grants 1056028, 1302435 and 1116955. This research was also partially supported by the U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center.	Balakrishnan Sivaraman, 2014, ARXIV14082156; Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chaganty A. T., 2013, ARXIV13063729; Chen YD, 2014, IEEE T INFORM THEORY, V60, P6440, DOI 10.1109/TIT.2014.2346205; Chen Yudong, 2014, C LEARN THEOR; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ma JW, 2005, NEUROCOMPUTING, V68, P105, DOI 10.1016/j.neucom.2004.12.009; McLachlan G., 2007, EM ALGORITHM EXTENSI, V382; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Po-Ling Loh, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2601, DOI 10.1109/ISIT.2012.6283989; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Stadler N, 2010, TEST-SPAIN, V19, P209, DOI 10.1007/s11749-010-0197-z; Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073; Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643; Wang Zhaoran, 2014, ARXIV14128729; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Yi X., 2013, ARXIV13103745; Yu B., 2009, ADV NEURAL INFORM PR, P1348	22	5	5	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100088
C	Arpit, D; Nwogu, I; Govindaraju, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Arpit, Devansh; Nwogu, Ifeoma; Govindaraju, Venu			Dimensionality Reduction with Subspace Structure Preservation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FACE RECOGNITION; SEGMENTATION; MODELS	Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that 2K projection vectors are sufficient for the independence preservation of any K class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving state-of-the-art results compared to popular dimensionality reduction techniques.	[Arpit, Devansh; Nwogu, Ifeoma; Govindaraju, Venu] SUNY Buffalo, Dept Comp Sci, Buffalo, NY 14260 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Arpit, D (corresponding author), SUNY Buffalo, Dept Comp Sci, Buffalo, NY 14260 USA.	devansha@buffalo.edu; inwogu@buffalo.edu; govind@buffalo.edu	Nwogu, Ifeoma/GYV-0014-2022	Nwogu, Ifeoma/0000-0003-1414-6433				Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Cai D, 2007, IEEE DATA MINING, P427, DOI 10.1109/ICDM.2007.88; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; He X., 2004, P NIPS ADV NEUR INF, P103; He XF, 2005, IEEE I CONF COMP VIS, P1208; Ho J, 2003, PROC CVPR IEEE, P11, DOI 10.1109/cvpr.2003.1211332; Hong W, 2006, IEEE T IMAGE PROCESS, V15, P3655, DOI 10.1109/TIP.2006.882016; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Ma Yi, 2007, IEEE T PATTERN ANAL, V3; Martinez A., 1998, AR FACE DATABASE; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130; Vidal R, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P167, DOI 10.1109/cdc.2003.1272554; Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005	16	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100089
C	Ding, N; Fang, YH; Babbush, R; Chen, CY; Skeel, RD; Neven, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ding, Nan; Fang, Youhan; Babbush, Ryan; Chen, Changyou; Skeel, Robert D.; Neven, Hartmut			Bayesian Sampling Using Stochastic Gradient Thermostats	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.	[Ding, Nan; Babbush, Ryan; Neven, Hartmut] Google Inc, Mountain View, CA 94043 USA; [Fang, Youhan; Skeel, Robert D.] Purdue Univ, W Lafayette, IN 47907 USA; [Chen, Changyou] Duke Univ, Durham, NC 27706 USA	Google Incorporated; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Duke University	Ding, N (corresponding author), Google Inc, Mountain View, CA 94043 USA.	dingnan@google.com; yfang@cs.purdue.edu; babbush@google.com; cchangyou@gmail.com; skeel@cs.purdue.edu; neven@google.com	Skeel, Robert/AAH-8137-2020					Ahn S., 2012, P 29 INT C MACH LEAR; [Anonymous], 2014, P 31 INT C MACH LEAR; [Anonymous], 2008, P 25 INT C MACH LEAR; Balan A. K., 2014, P 31 INT C MACH LEAR; Bardenet R, 2014, PR MACH LEARN RES, V32; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Fang YH, 2014, J CHEM PHYS, V140, DOI 10.1063/1.4874000; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; HOROWITZ AM, 1991, PHYS LETT B, V268, P247, DOI 10.1016/0370-2693(91)90812-5; Leimkuhler B., 2014, IMA J NUM ANAL; Leimkuhler B., 2012, ARXIV12035428; Leimkuhler B, 2009, ESAIM-MATH MODEL NUM, V43, P743, DOI 10.1051/m2an/2009023; Maclaurin D., 2014, ARXIV14035693; Mattingly J. C., 2014, SIAM J NUMER ANAL, V48, P552; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Neal R. M., 1901, ARXIV12061901; Patterson S., 2013, P 26 INT C NEUR INF, P3102; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Talay D., 1990, Stochastics and Stochastics Reports, V29, P13, DOI 10.1080/17442509008833606; Tuckerman M., 2010, STAT MECH THEORY MOL; Welling Max, 2011, P 28 TH INT C MACH L, P681	24	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103043
C	Foti, NJ; Xu, J; Laird, D; Fox, EB		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Foti, Nicholas J.; Xu, Jason; Laird, Dillon; Fox, Emily B.			Stochastic Variational Inference for Hidden Markov Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ELEMENTS	Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.	[Foti, Nicholas J.; Xu, Jason; Laird, Dillon; Fox, Emily B.] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Foti, NJ (corresponding author), Univ Washington, Seattle, WA 98195 USA.	nfoti@stat.washington.edu; jasonxu@stat.washington.edu; dillon12@cs.washington.edu; ebfox@stat.washington.edu	Xu, Jason/GRR-9638-2022; XU, Jason/GPW-9039-2022	Fox, Emily/0000-0003-3188-9685	TerraSwarm Research Center - MARCO; DARPA [FA9550-12-1-0406]; NSF CAREER Award [IIS-1350133]; NDSEG fellowship	TerraSwarm Research Center - MARCO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NDSEG fellowship	This work was supported in part by the TerraSwarm Research Center sponsored by MARCO and DARPA, DARPA Grant FA9550-12-1-0406 negotiated by AFOSR, and NSF CAREER Award IIS-1350133. JX was supported by an NDSEG fellowship. We also appreciate the data, discussions, and guidance on the ENCODE project provided by Max Libbrecht and William Noble.	Beale M. J., 2003, THESIS; Bishop C. M., 2006, INFOR MATION SCI STA, V2nd, DOI DOI 10.1117/1.2819119; Bottou L., 1998, ONLINE ALGORITHMS ST; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Bryant M., 2012, ADV NEURAL INFORM PR, p[2708, 2716]; Day N, 2007, BIOINFORMATICS, V23, P1424, DOI 10.1093/bioinformatics/btm096; Dunham I, 2012, NATURE, V489, P57, DOI 10.1038/nature11247; Fruhwirth-Schnatter S, 2006, FINITE MIXTURE MARKO; Gonzalez J., 2009, INT C ART INT STAT F, P177; Gopalan Prem K, 2012, ADV NEURAL INFORM PR, P2249; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hoffman MM, 2013, NUCLEIC ACIDS RES, V41, P827, DOI 10.1093/nar/gks1284; Hoffman MM, 2012, NAT METHODS, V9, P473, DOI [10.1038/NMETH.1937, 10.1038/nmeth.1937]; Johnson MJ, 2014, PR MACH LEARN RES, V32, P1854; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Maclaurin D, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P543; Neiswanger W., 2014, CORR; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Polyak B. T., 1973, AUTOMATICS TELEMECHA, V3, P45; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Russell SJ, 1995, ARTIF INTELL, V4th; Scott SL, 2002, J AM STAT ASSOC, V97, P337, DOI 10.1198/016214502753479464; Wang X., 2014, CORR; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	27	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103011
C	Frigola, R; Chen, YT; Rasmussen, CE		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Frigola, Roger; Chen, Yutian; Rasmussen, Carl E.			Variational Gaussian Process State-Space Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.	[Frigola, Roger; Chen, Yutian; Rasmussen, Carl E.] Univ Cambridge, Dept Engn, Cambridge, England	University of Cambridge	Frigola, R (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	rf342@cam.ac.uk; yc373@cam.ac.uk; cer54@cam.ac.uk						Bishop C.M, 2006, PATTERN RECOGN; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Brown EN, 1998, J NEUROSCI, V18, P7411; Candela JQ, 2003, INT CONF ACOUST SPEE, P701; Damianou A., 2011, ADV NEURAL INFORM PR, P2510; Daunizeau J, 2009, PHYSICA D, V238, P2089, DOI 10.1016/j.physd.2009.08.002; Deisenroth Marc P., 2012, ADV NEURAL INFORM PR, P2618; Deisenroth MP, 2012, IEEE T AUTOMAT CONTR, V57, P1865, DOI 10.1109/TAC.2011.2179426; Frigola R., 2013, ADV NEURAL INFORM PR, V26; Ghahramani Z., 1999, ADV NEURAL INFORM PR, V11; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840; Lawrence Neil D., 2007, P 24 INT C MACH LEAR; Lindsten F, 2013, FOUND TRENDS MACH LE, V6, P1, DOI 10.1561/2200000045; Opper M., 1998, ON LINE LEARNING NEU; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sarkka S., 2013, BAYESIAN FILTERING S; Shumway RH, 2011, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-1-4419-7865-3; Titsias M.K., 2009, ARTIF INTELL; Turner R., 2010, W CP, P868; Valpola H, 2002, NEURAL COMPUT, V14, P2647, DOI 10.1162/089976602760408017; Van Overschee P., 1996, SUBSPACE IDENTIFICAT; Wang J., 2006, ADV NEURAL INFORM PR, P1441	24	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101046
C	Guo, XX; Singh, S; Lee, H; Lewis, R; Wang, XS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Guo, Xiaoxiao; Singh, Satinder; Lee, Honglak; Lewis, Richard; Wang, Xiaoshi			Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.	[Guo, Xiaoxiao; Singh, Satinder; Lee, Honglak; Wang, Xiaoshi] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA; [Lewis, Richard] Univ Michigan, Dept Psychol, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan; University of Michigan System; University of Michigan	Guo, XX (corresponding author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.	guoxiao@umich.edu; baveja@umich.edu; honglak@umich.edu; rickl@umich.edu; xiaoshiw@umich.edu			NSF [IIS-1148668]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF grant IIS-1148668. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsors.	Bellemare M., 2012, ADV NEURAL INF PROCE, V25, P2222; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Erhan Dumitru, 2009, TECHNICAL REPORT; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hausknecht M, 2012, PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL CONFERENCE ON GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P217, DOI 10.1145/2330163.2330195; Karpathy Andrej, 2014, IEEE C COMP VIS PATT; Kearns M, 2002, MACH LEARN, V49, P193, DOI 10.1023/A:1017932429737; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee H., 2009, P ANN INT C MACH LEA, P609; Mnih V., 2013, DEEP LEARN NEUR INF; Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382; Ross S., 2011, AISTATS; Schmidhuber Jurgen, 2014, NEURAL NETWORKS; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343	20	5	5	5	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101092
C	Koyejo, O; Khanna, R; Ghosh, J; Poldrack, RA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Koyejo, Oluwasanmi; Khanna, Rajiv; Ghosh, Joydeep; Poldrack, Russell A.			On Prior Distributions and Approximate Inference for Structured Variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SELECTION; STATES	We present a general framework for constructing prior distributions with structured variables. The prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest. In cases where this projection is intractable, we propose a family of parameterized approximations indexed by subsets of the domain. We further analyze the special case of sparse structure. While the optimal prior is intractable in general, we show that approximate inference using convex subsets is tractable, and is equivalent to maximizing a submodular function subject to cardinality constraints. As a result, inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value. Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data. For this task, we employ the Gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support. Experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.	[Koyejo, Oluwasanmi; Poldrack, Russell A.] Dept Psychol, Stanford, CA 94305 USA; [Khanna, Rajiv; Ghosh, Joydeep] UT Austin, ECE Dept, Austin, TX USA	University of Texas System; University of Texas Austin	Koyejo, O (corresponding author), Dept Psychol, Stanford, CA 94305 USA.	sanmi@stanford.edu; rajivak@utexas.edu; ghosh@ece.utexas.edu; poldrack@stanford.edu	Khanna, Rajiv/GPK-2566-2022	Khanna, Rajiv/0000-0003-1314-3126; Poldrack, Russell/0000-0001-6755-0259	Consortium for Neuropsychiatric Phenomics (NIH Roadmap for Medical Research) [UL1-DE019580, RL1MH083269, RL1DA024853, PL1MH083271]	Consortium for Neuropsychiatric Phenomics (NIH Roadmap for Medical Research)	fMRI data was provided by the Consortium for Neuropsychiatric Phenomics (NIH Roadmap for Medical Research grants UL1-DE019580, RL1MH083269, RL1DA024853, PL1MH083271).	Carvalho CM, 2010, BIOMETRIKA, V97, P465, DOI 10.1093/biomet/asq017; Chang JT, 1997, STAT NEERL, V51, P287, DOI 10.1111/1467-9574.00056; Damien P., 2001, J COMPUTATIONAL GRAP, V10; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Ishwaran H, 2005, ANN STAT, V33, P730, DOI 10.1214/009053604000001147; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Kolmogorov A.N., 1933, FDN THEORY PROBABILI; Koyejo O., 2013, ICML WDDL WORKSH; Koyejo O., 2013, UAI; Kullback S, 1959, INFORM THEORY STAT; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Mitchell TM, 2004, MACH LEARN, V57, P145, DOI 10.1023/B:MACH.0000035475.85309.1b; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Park M., 2013, ADV NEURAL INFORM PR, P2688; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Poldrack RA, 2011, NEURON, V72, P692, DOI 10.1016/j.neuron.2011.11.001; Robert C. P., 1999, MONTE CARLO STAT MET, V58; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Titsias M. K., 2011, PROC 24 INT C NEURAL; White Corey N, 2014, J COGNITIVE NEUROSCI; WILLIAMS PM, 1980, BRIT J PHILOS SCI, V31, P131, DOI 10.1093/bjps/31.2.131; Wipf DP, 2007, ADV NEURAL INFORM PR, P1625	24	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103072
C	Liu, AQ; Ziebart, BD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Anqi; Ziebart, Brian D.			Robust Classification Under Sample Selection Bias	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				COVARIATE SHIFT; MAXIMUM-ENTROPY; INFERENCE	In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample reweighted empirical loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for learning a robust bias-aware (RBA) probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on binary classification tasks.	[Liu, Anqi; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Liu, AQ (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	aliu33@uic.edu; bziebart@uic.edu			National Science Foundation [1227495]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. #1227495, Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals.	Altun Y, 2006, LECT NOTES ARTIF INT, V4005, P139, DOI 10.1007/11776420_13; Bickel S, 2009, J MACH LEARN RES, V10, P2137; Boyd S, 2004, CONVEX OPTIMIZATION; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Daume H, 2007, P 45 ANN M ASS COMP, V45, P256; Dudik M, 2006, LECT NOTES ARTIF INT, V4005, P123, DOI 10.1007/11776420_12; Dudik Miroslav, 2005, ADV NEURAL INF PROCE, V18, P323; Fan W, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P605; Globerson A, 2009, NEURAL INF PROCESS S, P179; Gong Boqing, 2013, P ADV NEUR INF PROC, P1286; Grant M., 2014, CVX MATLAB SOFTWARE; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; Huang J., 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0080; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Lichman M, 2013, UCI MACHINE LEARNING; Little RJ., 1986, STAT ANAL MISSING DA; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Sugiyama M., 2008, NIPS, P1433; TOPSOE F, 1979, KYBERNETIKA, V15, P8; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wen JF, 2014, PR MACH LEARN RES, V32, P631; Yu Yao-Liang, 2012, ARXIV12064650, P607; Zadrozny  B., 2004, INT C MACH LEARN ICM, DOI 10.1145/1015330.1015425	26	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102019
C	Netrapalli, P; Niranjan, UN; Sanghavi, S; Anandkumar, A; Jain, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Netrapalli, Praneeth; Niranjan, U. N.; Sanghavi, Sujay; Anandkumar, Animashree; Jain, Prateek			Provable Non-convex Robust PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA			Robust PCA; matrix decomposition; non-convex methods; alternating projections		We propose a new method for robust PCA - the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is non-convex but easy to compute. In spite of this non-convexity, we establish exact recovery of the low-rank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For an m x n input matrix (m <= n), our method has a running time of O (r(2)mn) per iteration, and needs O (log(1/is an element of)) iterations to reach an accuracy of is an element of. This is close to the running times of simple PCA via the power method, which requires O (rmn) per iteration, and O (log(1/is an element of)) iterations. In contrast, the existing methods for robust PCA, which are based on convex optimization, have O (m(2)n) complexity per iteration, and take O (1/is an element of)iterations, i.e., exponentially more iterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations.	[Netrapalli, Praneeth] Microsoft Res, Cambridge, MA 02142 USA; [Niranjan, U. N.; Anandkumar, Animashree] Univ Calif Irvine, Irvine, CA 92717 USA; [Sanghavi, Sujay] Univ Texas Austin, Austin, TX 78712 USA; [Netrapalli, Praneeth; Niranjan, U. N.; Jain, Prateek] Microsoft Res, Bengaluru, Karnataka, India	Microsoft; University of California System; University of California Irvine; University of Texas System; University of Texas Austin	Netrapalli, P (corresponding author), Microsoft Res, Cambridge, MA 02142 USA.				NSF [CCF-1219234, 1302435, 0954059, 1017525]; ONR [N00014-14-1-0665]; Microsoft; DTRA grant [HDTRA1-13-1-0024]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Microsoft(Microsoft); DTRA grant	AA and UN would like to acknowledge NSF grant CCF-1219234, ONR N00014-14-1-0665, and Microsoft faculty fellowship. SS would like to acknowledge NSF grants 1302435, 0954059, 1017525 and DTRA grant HDTRA1-13-1-0024. PJ would like to acknowledge Nikhil Srivastava and Deeparnab Chakrabarty for several insightful discussions during the course of the project.	Agarwal A., 2013, LEARNING SPARSELY US; Anandkumar A., 2012, TENSOR METHODS LEARN; Bhatia Rajendra, 1997, MATRIX ANAL, DOI 10.1007/978-1-4612-0653-8; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chen Y., 2012, ADV NEURAL INFORM PR, P2204; Chen Y., 2013, ARXIV E PRINTS; Erdos Laszlo, 2013, ANN PROBABILITY; Hardt M., 2013, ARXIV13120925; Hsu Daniel, 2011, ITIT; Jain Prateek, 2013, STOC; Keshavan R. H., 2012, THESIS; Kyrillidis Anastasios, 2012, SSP WORKSH; Li Liyuan, 2004, ITIP; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Luo Z.-Q, 2013, ARXIV13085294; Mobahi Hossein, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P593, DOI 10.1109/ICCVW.2011.6130297; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Sedghi H., 2014, PREPRINT; Shi L., 2013, P ADV NEUR INF PROC, P172; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156	22	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101101
C	Yun, H; Raman, P; Vishwanathan, SVN		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yun, Hyokun; Raman, Parameswaran; Vishwanathan, S. V. N.			Ranking via Robust Binary Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose RoBiRank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. It shows competitive performance on standard benchmark datasets against a number of other representative algorithms in the literature. We also discuss extensions of RoBiRank to large scale problems where explicit feature vectors and scores are not given. We show that RoBiRank can be efficiently parallelized across a large number of machines; for a task that requires 386, 133 x 49, 824, 519 pairwise interactions between items to be ranked, RoBi-Rank finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation.	[Yun, Hyokun] Amazon, Seattle, WA 98109 USA; [Raman, Parameswaran; Vishwanathan, S. V. N.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA	Amazon.com; University of California System; University of California Santa Cruz	Yun, H (corresponding author), Amazon, Seattle, WA 98109 USA.	yunhyoku@amazon.com; params@ucsc.edu; vishy@ucsc.edu			National Science Foundation [IIS-1117705]	National Science Foundation(National Science Foundation (NSF))	We thank anonymous reviewers for their constructive comments, and Texas Advanced Computing Center for infrastructure and support for experiments. This material is partially based upon work supported by the National Science Foundation under grant No. IIS-1117705.	Agarwal Shivani, 2011, P 2011 SIAM INT C DA; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bertin-Mahieux T., 2011, ISMIR; Bottou L, 2012, OPTIMIZATION FOR MACHINE LEARNING, P351; Bouchard G, 2007, WORKSH APPR BAYES IN; Boyd S, 2004, CONVEX OPTIMIZATION; Buffoni D., 2011, P 28 INT C MACH LEAR, P825; Chapelle O, 2008, NIPS, P281; Chapelle O., 2011, P LEARN RANK CHALL, P1; Ding Nan, 2013, THESIS; Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094; Huber P., 1981, ROBUST STAT; Le Q. V., 2007, 07043359 ARXIV; Lee C.-P., 2013, NEURAL COMPUTATION; Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Qin T, 2010, INFORM RETRIEVAL, V13, P346, DOI 10.1007/s10791-009-9123-y; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rudin C, 2009, J MACH LEARN RES, V10, P2233; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Usunier N., 2009, P INT C MACH LEARN; Weston J., 2012, ARXIV12064603	24	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101034
C	Achan, K; Roweis, ST; Frey, BJ		Thrun, S; Saul, K; Scholkopf, B		Achan, K; Roweis, ST; Frey, BJ			Probabilistic inference of speech signals from phaseless spectrograms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Many techniques for complex speech processing such as denoising and deconvolution, time/frequency warping, multiple speaker separation, and multiple microphone analysis operate on sequences of short-time power spectra (spectrograrns), a representation which is often well-suited to these tasks. However, a significant problem with algorithms that manipulate spectrograms is that the output spectrogram does not include a phase component, which is needed to create a time-domain signal that has good perceptual quality. Here we describe a generative model of time-domain speech signals and their spectrograms, and show how an efficient optimizer can be used to find the maximum a posteriori speech signal, given the spectrogram. In contrast to techniques that alternate between estimating the phase and a spectrally-consistent signal, our technique directly infers the speech signal, thus jointly optimizing the phase and a spectrally-consistent signal. We compare our technique with a standard method using signal-to-noise ratios, but we also provide audio files on the web for the purpose of demonstrating the improvement in perceptual quality that our technique offers.	Univ Toronto, Machine Learning Grp, Toronto, ON, Canada	University of Toronto	Achan, K (corresponding author), Univ Toronto, Machine Learning Grp, Toronto, ON, Canada.							BESAG J, 1986, J R STAT SOC B, V48, P259; Fletcher R, 1987, PRACTICAL METHODS OP, V1; GRIFFIN DW, 1984, IEEE T ACOUSTIC SPEE, V32; JORDAN M, 1998, LEARNING GRAPHICAL M; KSCHISCHANG F, 2001, IEEE T INFORMATION T, V47; Neal R.M., 1993, PROBABILISTIC INFERE; Rabiner L., 1993, FUNDAMENTALS SPEECH; ROUCOS S, 1985, P IEEE INT C AC SPEE, P493; SAUL LK, 2003, ADV NEURAL INFORMATI, V15; WAN EA, 1998, P INT C AC SPEECH PR	10	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1393	1400						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500173
C	Chigirev, D; Bialek, W		Thrun, S; Saul, K; Scholkopf, B		Chigirev, D; Bialek, W			Optimal manifold representation of data: An information theoretic approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				DIMENSIONALITY REDUCTION; STATISTICAL VARIABLES; COMPONENT ANALYSIS; PRINCIPAL; COMPLEX	We introduce an information theoretic method for nonparametric, nonlinear dimensionality reduction, based on the infinite cluster limit of rate distortion theory. By constraining the information available to manifold coordinates, a natural probabilistic map emerges that assigns original data to corresponding points on a lower dimensional manifold. With only the information-distortion trade off as a parameter, our method determines the shape of the manifold, its dimensionality, the probabilistic map and the prior that provide optimal description of the data.	Princeton Univ, Dept Phys, Princeton, NJ 08544 USA	Princeton University	Chigirev, D (corresponding author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.	chigirev@princeton.edu; wbialek@princeton.edu						Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; BLAHUT RE, 1972, IEEE T INFORM THEORY, V18, P460, DOI 10.1109/TIT.1972.1054855; BRAND M, 2003, ADV NEURAL INFORMATI, V15; BREGLER C, 1995, ADV NEURAL INFORMATI, V7; GRASSBERGER P, 1983, PHYS REV LETT, V50, P346, DOI 10.1103/PhysRevLett.50.346; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319	12	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						161	168						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500021
C	Rudary, MR; Singh, S		Thrun, S; Saul, K; Scholkopf, B		Rudary, MR; Singh, S			A Nonlinear predictive state representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Predictive state representations (PSRs) use predictions of a set of tests to represent the state of controlled dynamical systems. One reason why this representation is exciting as an alternative to partially observable Markov decision processes (POMDPs) is that PSR models of dynamical systems may be much more compact than POMDP models. Empirical work on PSRs to date has focused on linear PSRs, which have not allowed for compression relative to POMDPs. We introduce a new notion of tests which allows us to define a new type of PSR that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems. These new tests, called e-tests, are related to the tests used by Rivest and Schapire [1] in their work with the diversity representation, but our PSR avoids some of the pitfalls of their representation-in particular, its potential to be exponentially larger than the equivalent POMDP.	Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Rudary, MR (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.							Jaeger H, 2000, NEURAL COMPUT, V12, P1371, DOI 10.1162/089976600300015411; Littman M, 1996, THESIS BROWN U, Patent No. AAI9709069; LITTMAN ML, 2001, ADV NEURAL INFORMATI, V14; Lovejoy WS, 1991, ANN OPER RES, V28, P47, DOI 10.1007/BF02055574; RIVEST RL, 1994, J ACM, V41, P555, DOI 10.1145/176584.176589; SINGH S, 2003, IN PRESS 20 INT C MA	6	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						855	862						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500107
C	Chapados, N; Bengio, Y; Vincent, P; Ghosn, J; Dugas, C; Takeuchi, I; Meng, LY		Dietterich, TG; Becker, S; Ghahramani, Z		Chapados, N; Bengio, Y; Vincent, P; Ghosn, J; Dugas, C; Takeuchi, I; Meng, LY			Estimating car insurance premia: A case study in high-dimensional data inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers.	Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Chapados, N (corresponding author), Univ Montreal, Dept IRO, CP 6128,Succ Ctr Ville, Montreal, PQ H3C 3J7, Canada.							Bailey R.A, 1960, ASTIN BULL, V1, P192; Biggs D., 1991, J APPL STAT, V18, P49, DOI [DOI 10.1080/02664769100000005, 10.1080/02664769100000005]; Dugas C, 2001, ADV NEUR IN, V13, P472; Hampel FR., 2011, WILEY SERIES PROBABI; HUBER PJ, 1982, ROBUST STAT; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Kass, 1980, APPL STAT, V29, P119, DOI [10.2307/2986296, DOI 10.2307/2986296]; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; VAPNIK V, 1998, STAT LEARNING THEORY, V454	10	5	6	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1369	1376						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100170
C	Donchin, O; Shadmehr, R		Dietterich, TG; Becker, S; Ghahramani, Z		Donchin, O; Shadmehr, R			Linking motor learning to function approximation: Learning in an unlearnable force field	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				DYNAMICS; MODEL	Reaching movements require the brain to generate motor commands that rely on an internal model of the task's dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reflect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we find a surprisingly good fit accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force field, and in a random sequence of force fields where learning is not possible. Remarkably, we find that the basis remains invariant.	Johns Hopkins Univ, Dept Biomed Engn, Baltimore, MD 21205 USA	Johns Hopkins University	Donchin, O (corresponding author), Johns Hopkins Univ, Dept Biomed Engn, Baltimore, MD 21205 USA.		Donchin, Opher/A-4389-2008	Donchin, Opher/0000-0003-0963-4467				ATKESON CG, 1989, ANNU REV NEUROSCI, V12, P157, DOI 10.1146/annurev.neuro.12.1.157; Sanner RM, 1999, BIOL CYBERN, V80, P369, DOI 10.1007/s004220050532; Scheidt RA, 2001, J NEUROPHYSIOL, V86, P971, DOI 10.1152/jn.2001.86.2.971; SHADMEHR R, 1994, J NEUROSCI, V14, P3208; Shadmehr R, 1997, SCIENCE, V277, P821, DOI 10.1126/science.277.5327.821; Thoroughman KA, 2000, NATURE, V407, P742, DOI 10.1038/35037588; UNO Y, 1989, BIOL CYBERN, V61, P89, DOI 10.1007/BF00204593	7	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						197	203						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100025
C	Frey, BJ; Koetter, R; Petrovic, N		Dietterich, TG; Becker, S; Ghahramani, Z		Frey, BJ; Koetter, R; Petrovic, N			Very loopy belief propagation for unwrapping phase images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				CODES; GRAPHS	Since the discovery that the best error-correcting decoding algorithm can be viewed as belief propagation in a cycle-bound graph, researchers have been trying to determine under what circumstances "loopy belief propagation" is effective for probabilistic inference. Despite several theoretical advances in our understanding of loopy belief propagation, to our knowledge, the only problem that has been solved using loopy belief propagation is error-correcting decoding on Gaussian channels. We propose a new representation for the two-dimensional phase unwrapping problem, and we show that loopy belief propagation produces results that are superior to existing techniques. This is an important result, since many imaging techniques, including magnetic resonance imaging and interferometric synthetic aperture radar, produce phase-wrapped images. Interestingly, the graph that we use has a very large number of very short cycles, supporting evidence that a large minimum cycle length is not needed for excellent results using belief propagation.	Univ Toronto, Probabilist & Stat Inference Grp, Toronto, ON, Canada	University of Toronto	Frey, BJ (corresponding author), Univ Toronto, Probabilist & Stat Inference Grp, Toronto, ON, Canada.							ACHAN K, 2001, UNCERTAINTY ARTIFICI; Chen CW, 2000, J OPT SOC AM A, V17, P401, DOI 10.1364/JOSAA.17.000401; Freeman W. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1182, DOI 10.1109/ICCV.1999.790414; Frey B.J., 1996, P 34 ALL C COMM CONT; FREY BJ, 2000, P IEEE C COMP VIS PA; Ghiglia D.C., 1998, 2 DIMENSIONAL PHASE; KOETTER R, 2001, P INT C AC SPEECH SI; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; MacKay DJC, 1995, LECT NOTES COMPUT SC, V1025, P100; MCELIECE RJ, 1998, IEEE J SEL AREA COMM, P16; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; WIBERG N, 1995, EUR T TELECOMMUN, V6, P513, DOI 10.1002/ett.4460060507; YEDIDA J, 2001, ADV NEURAL INFORMATI, V13	14	5	5	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						737	743						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100092
C	Isbell, CL; Shelton, CR; Kearns, M; Singh, S; Stone, P		Dietterich, TG; Becker, S; Ghahramani, Z		Isbell, CL; Shelton, CR; Kearns, M; Singh, S; Stone, P			Cobot: A social reinforcement learning agent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modifing his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment.					Shelton, Christian/GQJ-1146-2022	Shelton, Christian/0000-0001-6698-7838				Foner L., 1997, P 1 INT C AUT AG; ISBELL CL, 2000, IN PRESS P AAAI 2000; Mauldin Michael Loaren, 1994, P 12 NAT C ART INT; SHELTON CR, 2000, UNPUB NEURAL INFORMA; SINGH S, 2000, IN PRESS P AAAI 2000; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; SUTTON RS, 1999, NEURAL INFORMATION P	7	5	5	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1393	1400						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100173
C	Tegner, J; Kepecs, A		Dietterich, TG; Becker, S; Ghahramani, Z		Tegner, J; Kepecs, A			Why neuronal dynamics should control synaptic learning rules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				PLASTICITY	Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the additive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes.	Royal Inst Technol, Stockholm Bioinformat Ctr, Dept Numer Anal & Comp Sci, S-10044 Stockholm, Sweden	Royal Institute of Technology	Tegner, J (corresponding author), Royal Inst Technol, Stockholm Bioinformat Ctr, Dept Numer Anal & Comp Sci, S-10044 Stockholm, Sweden.		tegner, jesper N/R-5095-2017	tegner, jesper N/0000-0002-9568-5588; Kepecs, Adam/0000-0003-0049-8120				Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453; BELL A, 1992, ADV NEURAL INFORMATI; Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0; Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; Kempter R, 2001, NEURAL COMPUT, V13, P2709, DOI 10.1162/089976601317098501; LEMASSON G, 1993, SCIENCE, V259, P1915, DOI 10.1126/science.8456317; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; MILLER KD, 1994, NEURAL COMPUT, V6, P100, DOI 10.1162/neco.1994.6.1.100; Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364; SEJNOWSKI T, 1997, J THEOR BIOL, V69, P385; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; Turrigiano GG, 1998, NATURE, V391, P892, DOI 10.1038/36103; Turrigiano GG, 2000, CURR OPIN NEUROBIOL, V10, P358, DOI 10.1016/S0959-4388(00)00091-X; van Rossum MCW, 2000, J NEUROSCI, V20, P8812	14	5	5	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						285	292						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100036
C	Thrun, S; Langford, J; Verma, V		Dietterich, TG; Becker, S; Ghahramani, Z		Thrun, S; Langford, J; Verma, V			Risk sensitive particle filters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We propose a new particle filter that incorporates a model of costs when generating particles. The approach is motivated by the observation that the costs of accidentally not tracking hypotheses might be significant in some areas of state space, and next to irrelevant in others. By incorporating a cost model into particle filtering, states that are more critical to the system performance are more likely to be tracked. Automatic calculation of the cost model is implemented using an MDP value function calculation that estimates the value of tracking a particular state. Experiments in two mobile robot domains illustrate the appropriateness of the approach.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Thrun, S (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.							BOYEN X, P UAI 98; DELLAERT F, P ICRA 99; Doucet A., 2001, SEQUENTIAL MONTE CAR; Engelson S.P., 1994, THESIS YALE U; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179; POUPART P, P UAI 2001; ROY N, P NIPS 99; Rubin D.B., 1988, BAYESIAN STAT, V3; Tanner M.A., 1996, TOOLS STAT INFERENCE	12	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						961	968						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100120
C	Yang, MH		Dietterich, TG; Becker, S; Ghahramani, Z		Yang, MH			Face recognition using kernel methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				COMPONENT ANALYSIS; EIGENFACES	Principal Component Analysis and Fisher Linear Discriminant methods have demonstrated their success in face detection, recognition, and tracking. The representation in these subspace methods is based on second order statistics of the image set, and does not address higher order statistical dependencies such as the relationships among three or more pixels. Recently Higher Order Statistics and Independent Component Analysis (ICA) have been used as informative low dimensional representations for visual recognition. In this paper, we investigate the use of Kernel Principal Component Analysis and Kernel Fisher Linear Discriminant for learning low dimensional representations for face recognition, which we call Kernel Eigenface and Kernel Fisherface methods. While Eigenface and Fisherface methods aim to find projection directions based on the second order correlation of samples, Kernel Eigenface and Kernel Fisherface methods provide generalizations which take higher order correlations into account. We compare the performance of kernel methods with Eigenface, Fisherface and ICA-based methods for face recognition with variation in pose, scale, lighting and expression. Experimental results show that kernel methods provide better representations and achieve lower error rates for face recognition.	Honda Fundamental Res Labs, Mountain View, CA 94041 USA	Honda Motor Company	Yang, MH (corresponding author), Honda Fundamental Res Labs, Mountain View, CA 94041 USA.	myang@hra.com						Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229; Bartlett M., 1998, THESIS U CALIFORNIA; Bartlett M. S., 1997, NEURAL INFORMATION P, P817; Bartlett MS, 1998, P SOC PHOTO-OPT INS, V3299, P528, DOI 10.1117/12.320144; Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bishop, 1995, NEURAL NETWORKS PATT; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71; Mika S, 2000, ADV NEUR IN, V12, P526; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Moghaddam B., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1131, DOI 10.1109/ICCV.1999.790407; PHILLIPS PJ, 1998, NIPS, V11, P803; Rajagopalan AN, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P640, DOI 10.1109/ICCV.1998.710785; RAJAGOPALAN AN, 1999, P 7 IEEE INT C COMP, V2, P1204; Roth V, 2000, ADV NEUR IN, V12, P568; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Teh YW, 2001, ADV NEUR IN, V13, P908; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71	20	5	5	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1457	1464						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100181
C	Frey, BJ; Patrascu, R; Jaakkola, TS; Moran, J		Leen, TK; Dietterich, TG; Tresp, V		Frey, BJ; Patrascu, R; Jaakkola, TS; Moran, J			Sequentially fitting "inclusive" trees for inference in noisy-OR networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				BELIEF NETWORKS	An important class of problems can be cast as inference in noisy-OR Bayesian networks, where the binary state of each variable is a logical OR of noisy versions of the states of the variable's parents. For example, in medical diagnosis, the presence of a symptom can be expressed as a noisy-OR of the diseases that may cause the symptom - on some occasions, a disease may fail to activate the symptom. Inference in richly-connected noisy-OR networks is intractable, but approximate methods (e.g., variational techniques) are showing increasing promise as practical solutions. One problem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior, ignoring other plausible configurations of the hidden variables. We introduce a new sequential variational method for bipartite noisy-OR networks, that favors including all modes of the true posterior and models the posterior distribution as a tree. We compare this method with other approximations using an ensemble of networks with network statistics that are comparable to the QMR-DT medical diagnostic network.	Univ Toronto, Intelligent Algorithms Lab, Toronto, ON, Canada	University of Toronto	Frey, BJ (corresponding author), Univ Toronto, Intelligent Algorithms Lab, Toronto, ON, Canada.							Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458; Bock F.C., 1971, DEV OPERATIONS RES, P29; FREEMAN WT, 2001, IN PRESS IEEE T INFO; Frey BJ, 1999, NEURAL COMPUT, V11, P193, DOI 10.1162/089976699300016872; FREY BJ, 2000, P IEEE C COMP VIS PA; Gallager RG, 1963, LOW DENSITY PARITY C; Heckerman D., 1989, TRACTABLE INFERENCE; Jaakkola TS, 1999, J ARTIF INTELL RES, V10, P291, DOI 10.1613/jair.583; Jordan M. I., 1999, LEARNING GRAPHICAL M; MacKay DJC, 1999, IEEE T INFORM THEORY, V45, P399, DOI 10.1109/18.748992; MACKAY DJC, 1999, LEARNING GRAPHICAL M; MCELIECE RJ, 1996, P 33 ALL C COMM CONT; Murphy K. P., 1999, P 15 C UNC ART INT; Neal R. M., 1993, PROBABILISTIC INFERE; Neal RM, 1996, STAT COMPUT, V6, P353, DOI 10.1007/BF00143556; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; SAUL LK, 1996, ADV NEURAL INFORMATI, V8; SHWE MA, 1991, METHOD INFORM MED, V30, P241, DOI 10.1055/s-0038-1634846; [No title captured]	20	5	5	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						493	499						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800070
C	Jebara, T; Pentland, A		Leen, TK; Dietterich, TG; Tresp, V		Jebara, T; Pentland, A			On reversing Jensen's inequality	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then permits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown.	MIT, Media Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Jebara, T (corresponding author), MIT, Media Lab, Cambridge, MA 02139 USA.							BUNTINE W, 1994, 2 JAIR; DEMPSTER AP, 1977, J ROYAL STAT SOC B, V39; GHARAMANI Z, 1999, 12 NIPS; GOPALAKRISHNAN PS, 1991, IEEE T INFORM THEORY, V37, P107, DOI 10.1109/18.61108; JAAKKOLA T, 1999, 12 NIPS; JEBARA T, 2000, FEATURE SELECTION DU; JEBARA T, 1998, 11 NIPS; JORDAN MI, 1997, LEARNING GRAPHICAL M; Peajcariaac J.E., 1992, CONVEX FUNCTIONS PAR	9	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						231	237						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800033
C	Koltchinskii, V; Panchenko, D; Lozano, F		Leen, TK; Dietterich, TG; Tresp, V		Koltchinskii, V; Panchenko, D; Lozano, F			Some new bounds on the generalization error of combined classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MARGIN	In this paper we develop the method of bounding the generalization error of a classifier in terms of its margin distribution which was introduced in the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee. The theory of Gaussian and empirical processes allow us to prove the margin type inequalities for the most general functional classes, the complexity of the class being measured via the so called Gaussian complexity functions. As a simple application of our results, we obtain the bounds of Schapire, Freund, Bartlett and Lee for the generalization error of boosting. We also substantially improve the results of Bartlett on bounding the generalization error of neural networks in terms of l(1) - norms of the weights of neurons. Furthermore, under additional assumptions on the complexity of the class of hypotheses we provide some tighter bounds, which in the case of boosting improve the results of Schapire, Freund, Bartlett and Lee.	Univ New Mexico, Dept Math & Stat, Albuquerque, NM 87131 USA	University of New Mexico	Koltchinskii, V (corresponding author), Univ New Mexico, Dept Math & Stat, Albuquerque, NM 87131 USA.	vlad@math.unm.edu; panchenk@math.unm.edu; flozano@eece.unm.edu						Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y, 1999, MACHINE LEARNING, PROCEEDINGS, P124; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; KOTCHINSKII V, 2000, EMPIRICAL MARGIN DIS; MASON L, 1999, IN PRESS ADV LARGE M; Schapire RE, 1998, ANN STAT, V26, P1651; Shawe-Taylor J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P278, DOI 10.1145/307400.307470; Shawe-Taylor J, 1999, LECT NOTES ARTIF INT, V1572, P263; Van Der VaartJon A., 1996, WEAK CONVERGENCE EMP	11	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						245	251						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800035
C	Kurino, H; Nakagawa, M; Lee, KW; Nakamura, T; Yamada, Y; Park, KT; Koyanagi, M		Leen, TK; Dietterich, TG; Tresp, V		Kurino, H; Nakagawa, M; Lee, KW; Nakamura, T; Yamada, Y; Park, KT; Koyanagi, M			Smart vision chip fabricated using three dimensional integration technology	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The smart vision chip has a large potential for application in general purpose high speed image processing systems. In order to fabricate smart vision chips including photo detector compactly, we have proposed the application of three dimensional LSI technology for smart vision chips. Three dimensional technology has great potential to realize new neuromorphic systems inspired by not only the biological function but also the biological structure. In this paper, we describe our three dimensional LSI technology for neuromorphic circuits and the design of smart vision chips.	Tohoku Univ, Dept Machine Intelligence & Syst Engn, Aoba Ku, Sendai, Miyagi 9808579, Japan	Tohoku University	Kurino, H (corresponding author), Tohoku Univ, Dept Machine Intelligence & Syst Engn, Aoba Ku, 01 Aza-Aramaki, Sendai, Miyagi 9808579, Japan.							Koyanagi N, 1998, IEEE MICRO, V18, P17, DOI 10.1109/40.710867; Kurino H., 1999, International Electron Devices Meeting 1999. Technical Digest (Cat. No.99CH36318), P879, DOI 10.1109/IEDM.1999.824289; KURINO H, 1999, P ISFILE MARCH, P175; LEE KW, 1999, INT C SSDM, P588; Matsumoto T, 1998, JPN J APPL PHYS 1, V37, P1217, DOI 10.1143/JJAP.37.1217; Mead C., ANALOG VLSI NEURAL S	6	5	5	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						720	726						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800102
C	Legenstein, RA; Maass, W		Leen, TK; Dietterich, TG; Tresp, V		Legenstein, RA; Maass, W			Foundations for a circuit complexity theory of sensory processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.	Graz Tech Univ, Inst Theoret Comp Sci, Graz, Austria	Graz University of Technology	Legenstein, RA (corresponding author), Graz Tech Univ, Inst Theoret Comp Sci, Graz, Austria.							ABELES M, 1998, CORTICONICS NEURAL C; [Anonymous], [No title captured]; Braitenberg VB, 1998, CORTEX STAT GEOMETRY; Cajal SRY., 1995, HISTOLOGY NERVOUS SY, V1; Chklovskii DB, 2000, ADV NEUR IN, V12, P103; Gordon S.., 1998, SYNAPTIC ORG BRAIN, V5; Lazzaro J., 1988, ADV NEURAL INFORMATI, V1, P703; MEAD C, 1989, ANAL VLSI NEURAL SYS; MEAD CA, 1979, IEEE J SOLID-ST CIRC, V14, P455, DOI 10.1109/JSSC.1979.1051197; Ramon y Cajal S., 1995, HISTOLOGY NERVOUS SY, V1	11	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						259	265						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800037
C	Mannor, S; Meir, R		Leen, TK; Dietterich, TG; Tresp, V		Mannor, S; Meir, R			Weak learners and improved rates of convergence in boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The problem of constructing weak classifiers for boosting algorithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Mannor, S (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.	shie@technion.ac.il; rmeir@technion.ac.il		Mannor, Shie/0000-0003-4439-7647				Anthony M., 1999, NEURAL NETWORK LEARN, V9; Bartlett P, 1999, STAT PROBABIL LETT, V44, P55, DOI 10.1016/S0167-7152(98)00291-0; BARTLETT P, 1999, P 4 EUR C COMP LEARN; HASTIE T, 2000, IN PRESS ANN STAT; Johnson D. S., 1978, Theoretical Computer Science, V6, P93, DOI 10.1016/0304-3975(78)90006-3; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; MANNOR S, UNPUB MACHINE LEARNI; MASON L, 2000, IN PRESS MACHINE LEA; Mason Llew, 2000, ADV LARGE MARGIN CLA; Schatten G, 1998, J LAW MED ETHICS, V26, P29, DOI 10.1111/j.1748-720X.1998.tb01903.x; Vapnik V., 1982, ESTIMATION DEPENDENC	11	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						280	286						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800040
C	Mayraz, G; Hinton, GE		Leen, TK; Dietterich, TG; Tresp, V		Mayraz, G; Hinton, GE			Recognizing hand-written digits using hierarchical products of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the n(th) level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)(th) level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Mayraz, G (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, Mortimer St,17 Queen Sq, London WC1N 3AR, England.		Mayraz, Guy/N-9511-2015	Mayraz, Guy/0000-0002-4419-2900				Burges CJC, 1997, ADV NEUR IN, V9, P375; FREUND Y, 1992, ADV NEUR IN, V4, P912; Hinton G. E., 2000, 2000004 GCNU TR U CO; LeCun Y., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P53; McClelland JL, 1986, PARALLEL DISTRIBUTED, V1, P1; Smolensky P, 1986, PARALLEL DISTRIBUTED, V1	6	5	5	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						953	959						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800134
C	Still, S; Scholkopf, B; Hepp, K; Douglas, RJ		Leen, TK; Dietterich, TG; Tresp, V		Still, S; Scholkopf, B; Hepp, K; Douglas, RJ			Four-legged walking gait control using a neuromorphic chip interfaced to a support vector learning algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				LOCOMOTION; ANIMALS	To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Still, S (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				ALEXANDER RM, 1984, INT J ROBOT RES, V3, P49, DOI 10.1177/027836498400300205; DELCOMYN F, 1980, SCIENCE, V210, P492, DOI 10.1126/science.7423199; GRILLNER S, 1998, NEURONAL MECH GENERA; Grillner S., 1981, HDB PHYSL NERVOUS SY, P1179, DOI DOI 10.1002/CPHY.CP010226; HASSLACHER B, 1995, ROBOTICS AUTONOMOUS; HOYT DF, 1981, NATURE, V292, P239, DOI 10.1038/292239a0; SCHOLKOPF B, 1999, IN PRESS NEURAL COMP; STILL S, 2000, THESIS ETH ZURICH; VANDERBEI RJ, 1997, SOR9708 PRINC U STAT; Weste NH., 1985, PRINCIPLES CMOS VLSI; YANAGIHARA D, 1993, NEUROSCI RES, V18, P241, DOI 10.1016/0168-0102(93)90060-4; [No title captured]	13	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						741	747						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800105
C	Wainwright, MJ; Sudderth, EB; Willsky, AS		Leen, TK; Dietterich, TG; Tresp, V		Wainwright, MJ; Sudderth, EB; Willsky, AS			Tree-based modeling and estimation of Gaussian processes on graphs with cycles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				STATISTICAL-ANALYSIS; SYSTEMS	We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded spanning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other methods, the embedded trees algorithm also computes exact error covariances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an embedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power relative to trees, with only a minor increase in estimation complexity.	MIT, Dept Elect Engn & Comp Sci, Informat & Decis Syst Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Wainwright, MJ (corresponding author), MIT, Dept Elect Engn & Comp Sci, Informat & Decis Syst Lab, Cambridge, MA 02139 USA.			Sudderth, Erik/0000-0002-0595-9726; Wainwright, Martin J./0000-0002-8760-2236				AXELSSON O, 1992, SIAM J MATRIX ANAL A, V13, P847, DOI 10.1137/0613052; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; CHOU KC, 1994, IEEE T AUTOMAT CONTR, V39, P479, DOI 10.1109/9.280747; Demmel JW, 1997, APPL NUMERICAL LINEA, V56; FIEGUTH PW, 1995, IEEE T GEOSCI REMOTE, V33, P280, DOI 10.1109/36.377928; FRAKT A, IN PRESS MULT SYS SI; GALLICCHIO VS, 1994, GROWTH REGULAT, V4, P41; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rusmevichientong P, 2000, ADV NEUR IN, V12, P575; SUDDERTH E, 2000, UNPUB EMBEDDED TREES; Weiss Y, 2000, ADV NEUR IN, V12, P673	11	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						661	667						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800094
C	Bakker, R; Schouten, JC; Coppens, MO; Takens, F; Giles, CL; van den Bleek, CM		Solla, SA; Leen, TK; Muller, KR		Bakker, R; Schouten, JC; Coppens, MO; Takens, F; Giles, CL; van den Bleek, CM			Robust learning of chaotic attractors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					A fundamental problem with the modeling of chaotic time series data is that minimizing short-term prediction errors does not guarantee a match between the reconstructed attractors of model and experiments. We introduce a modeling paradigm that simultaneously learns to short-term predict and to locate the outlines of the attractor by a new way of nonlinear principal component analysis. Closed-loop predictions are constrained to stay within these outlines, to prevent divergence from the attractor. Learning is exceptionally fast: parameter estimation for the 1000 sample laser data from the 1991 Santa Fe time series competition took less than a minute on a 166 MHz Pentium PC.	DelftChemTech, Chem Reactor Engn Lab, NL-2628 BL Delft, Netherlands		Bakker, R (corresponding author), DelftChemTech, Chem Reactor Engn Lab, Julianalaan 136, NL-2628 BL Delft, Netherlands.	r.bakker@stm.tudelft.nl; J.C.Schouten@tue.nl; coppens@stm.tudelft.nl; F.Takens@math.rug.nl; giles@research.nj.nec.com; vdbleek@stm.tudelft.nl						BAKKER R, UNPUB LEARNING CHAOT; Diks C, 1996, PHYS REV E, V53, P2169, DOI 10.1103/PhysRevE.53.2169; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Kambhatla N, 1997, NEURAL COMPUT, V9, P1493, DOI 10.1162/neco.1997.9.7.1493; KUO JM, 1994, P IEEE INT C NEUR NE, V5, P3131; LAPEDES A, LAUR872662; Malthouse EC, 1998, IEEE T NEURAL NETWOR, V9, P165, DOI 10.1109/72.655038; Principe JC, 1998, P IEEE, V86, P2240, DOI 10.1109/5.726789; Principe JC, 1992, INT J BIFURCAT CHAOS, V2, P989, DOI 10.1142/S0218127492000598; Takens F, 1981, LECT NOTES MATH, V898, P365	10	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						879	885						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700124
C	Brown, TX		Solla, SA; Leen, TK; Muller, KR		Brown, TX			Low power wireless communication via reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					This paper examines the application of reinforcement learning to a wireless communication problem. The problem requires that channel utility be maximized while simultaneously minimizing battery usage. We present a solution to this multi-criteria problem that is able to significantly reduce power consumption. The solution uses a variable discount factor to capture the effects of battery usage.	Univ Colorado, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Brown, TX (corresponding author), Univ Colorado, Boulder, CO 80309 USA.	timxb@colorado.edu						Boyan J. A., 1994, P INT C ADV NEURAL I, P671; Brown TX, 1999, ADV NEUR IN, V11, P982; Goldsmith AJ, 1996, IEEE T INFORM THEORY, V42, P868, DOI 10.1109/18.490551; Govil K., 1995, P 1 ACM INT C MOB CO; Jaakkola T., 1995, Advances in Neural Information Processing Systems 7, P345; KRAVITS R, 1999, APPL DRIVEN POWER MA; MARBACH P, 1998, ADV NIPS, V10; Rappaport T. S., 1996, WIRELESS COMMUNICATI; Singh S, 1997, ADV NEUR IN, V9, P974	9	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						893	899						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700126
C	Coolen, ACC; Mace, CWH		Solla, SA; Leen, TK; Muller, KR		Coolen, ACC; Mace, CWH			Dynamics of supervised learning with restricted training sets and noisy teachers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				PERCEPTRON	We generalize a recent formalism to describe the dynamics of supervised learning in layered neural networks, in the regime where data recycling is inevitable, to the case of noisy teachers. Our theory generates reliable predictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes in large neural networks to those situations where overfitting can occur.	Univ London Kings Coll, Dept Math, London WC2R 2LS, England	University of London; King's College London	Coolen, ACC (corresponding author), Univ London Kings Coll, Dept Math, London WC2R 2LS, England.							BIEHL M, 1995, PHYS REV E, V52, pR4624, DOI 10.1103/PhysRevE.52.R4624; COOLEN ACC, 1999, ADV NEURAL INFORMATI, V11; COOLEN ACC, 1998, ON LINE LEARNING NEU; COOLEN ACC, 1999, KCLMTH9933; COOLEN ACC, 1999, KCLMTH9932; HORNER H, 1992, Z PHYS B CON MAT, V87, P371, DOI 10.1007/BF01309290; HORNER H, 1992, Z PHYS B CON MAT, V86, P291, DOI 10.1007/BF01313839; INOUE JI, 1999, COMMUNICATION; Mace CWH, 1998, STAT COMPUT, V8, P55, DOI 10.1023/A:1008896910704; Rae HC, 1999, J PHYS A-MATH GEN, V32, P3321, DOI 10.1088/0305-4470/32/18/308; RAE HC, 1999, ADV NEURAL INFORMATI, V11; SAAD D, 1998, ON LINE LEARNING NEU; WONG KYM, 1999, CONDMAT9909004	14	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						237	243						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700034
C	Eurich, CW; Wilke, SD; Schwegler, H		Solla, SA; Leen, TK; Muller, KR		Eurich, CW; Wilke, SD; Schwegler, H			Neural representation of multi-dimensional stimuli	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				RESOLUTION	The encoding accuracy of a population of stochastically spiking neurons is studied for different distributions of their tuning widths. The situation of identical radially symmetric receptive fields for all neurons, which is usually considered in the literature, turns out to be disadvantageous from an information-theoretic point of view. Both a variability of tuning widths and a fragmentation of the neural population into specialized subpopulations improve the encoding accuracy.	Univ Bremen, Inst Theoret Phys, D-2800 Bremen 33, Germany	University of Bremen	Eurich, CW (corresponding author), Univ Bremen, Inst Theoret Phys, D-2800 Bremen 33, Germany.							BALDI P, 1988, BIOL CYBERN, V59, P313, DOI 10.1007/BF00332921; DECO G, 1997, INFORMATION THEORETI; Eurich CW, 1997, BIOL CYBERN, V76, P357, DOI 10.1007/s004220050349; EURICH CW, 2000, IN PRESS NEURAL COMP; Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P77; KNUDSEN EI, 1978, SCIENCE, V200, P795, DOI 10.1126/science.644324; KUFFLER SW, 1953, J NEUROPHYSIOL, V16, P37, DOI 10.1152/jn.1953.16.1.37; LETTVIN JY, 1959, P IRE, V47, P1940, DOI 10.1109/JRPROC.1959.287207; SNIPPE HP, 1992, BIOL CYBERN, V66, P543, DOI 10.1007/BF00204120; WIGGERS W, 1995, J COMP PHYSIOL A, V176, P365; Worgotter F, 1998, NATURE, V396, P165, DOI 10.1038/24157; Zhang KC, 1999, NEURAL COMPUT, V11, P75, DOI 10.1162/089976699300016809	12	5	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						115	121						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700017
C	Munro, P; Hernandez, G		Solla, SA; Leen, TK; Muller, KR		Munro, P; Hernandez, G			LTD facilitates learning in a noisy environment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SYNAPTIC EFFICACY; NEURAL NETWORKS; POTENTIATION; STIMULATION; HIPPOCAMPUS; NEURONS	Long-term potentiation (LTP) has long been held as a biological substrate for associative learning. Recently. evidence has emerged that long-term depression (LTD) results when the presynaptic cell fires after the postsynaptic cell. The computational utility of LTD is explored here. Synaptic modification kernels for both LTP and LTD have been proposed by other laboratories based studies of one postsynaptic unit. Here: the interaction between time-dependent LTP and LTD is studied in small networks.	Univ Pittsburgh, Sch Informat Sci, Pittsburgh, PA 15260 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Munro, P (corresponding author), Univ Pittsburgh, Sch Informat Sci, Pittsburgh, PA 15260 USA.	pwm+@pitt.edu; gehst5+@pitt.edu						Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406; ABBOTT LF, 1999, ADV NEURAL INFORMATI, V11; ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; BARRIONUEVO G, 1980, LIFE SCI, V27, P2385, DOI 10.1016/0024-3205(80)90509-3; BLISS TVP, 1973, J PHYSIOL-LONDON, V232, P331, DOI 10.1113/jphysiol.1973.sp010273; Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0; GOLDMAN MS, 1998, IN PRESS NEUROCOMPUT; Hebb D., 1949, ORG BEHAV; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Kempter R, 1996, ADV NEUR IN, V8, P124; KEMPTER R, 1999, ADV NEURAL INFORMATI, V11; Malenka R.C., 1995, NEUROSCIENTIST, V1, P35, DOI [10.1177/107385849500100106, DOI 10.1177/107385849500100106]; Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; SEJNOWSKI TJ, 1977, J MATH BIOL, V4, P303, DOI 10.1007/BF00275079; SOMPOLINSKY H, 1986, PHYS REV LETT, V57, P2861, DOI 10.1103/PhysRevLett.57.2861; STENT GS, 1973, P NATL ACAD SCI USA, V70, P997, DOI 10.1073/pnas.70.4.997; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Thiels E, 1996, HIPPOCAMPUS, V6, P43, DOI 10.1002/(SICI)1098-1063(1996)6:1<43::AID-HIPO8>3.0.CO;2-8; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	20	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						150	156						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700022
C	Piche, S; Keeler, J; Martin, G; Boe, G; Johnson, D; Gerules, M		Solla, SA; Leen, TK; Muller, KR		Piche, S; Keeler, J; Martin, G; Boe, G; Johnson, D; Gerules, M			Neural network based Model Predictive Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Model Predictive Control (MPC), a control algorithm which uses an optimizer to solve for the optimal control moves over a future time horizon based upon a model of the process, has become a standard control technique in the process industries over the past two decades. In most industrial applications, a linear dynamic model developed using empirical data is used even though the process itself is often nonlinear. Linear models have been used because of the difficulty in developing a generic nonlinear model from empirical data and the computational expense often involved in using nonlinear models. In this paper, we present a generic neural network based technique for developing nonlinear dynamic models from empirical data and show that these models can be efficiently used in a model predictive control framework. This nonlinear MPC based approach has been successfully implemented in a number of industrial applications in the refining, petrochemical, paper and food industries. Performance of the controller on a nonlinear industrial process, a polyethylene reactor, is presented.	Pavil Technol, Austin, TX 78758 USA		Piche, S (corresponding author), Pavil Technol, Austin, TX 78758 USA.							David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; GOFF S, 1998, P CHEM ENG EXPO; HARTMAN E, 2000, IN PRESS NEURAL COMP; Ljung L., 1987, SYSTEM IDENTIFICATIO; Meadows E.S., 1997, NONLINEAR PROCESS CO, P233; Nash S.G., 1996, LINEAR NONLINEAR PRO; QIN SJ, 1998, P IFAC WORKSH NONL M; SEBORG DE, 1999, IN PRESS P EUR CONTR; [No title captured]	9	5	5	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1029	1035						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700145
C	Rusmevichientong, P; Van Roy, B		Solla, SA; Leen, TK; Muller, KR		Rusmevichientong, P; Van Roy, B			An analysis of turbo decoding with Gaussian densities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				PROPAGATION; CODES	We provide an analysis of the turbo decoding algorithm (TDA) in a setting involving Gaussian densities. In this context, we are able to show that the algorithm converges and that - somewhat surprisingly - though the density generated by the TDA may differ significantly from the desired posterior density, the means of these two densities coincide.	Stanford Univ, Stanford, CA 94305 USA	Stanford University	Rusmevichientong, P (corresponding author), Stanford Univ, Stanford, CA 94305 USA.							BALLAGER RG, 1963, LOW DENSITY PARITY C; Benedetto S, 1996, IEEE T INFORM THEORY, V42, P409, DOI 10.1109/18.485713; BERROU G, 1993, P 1993 INT C COMM GE, P1064; FREY B, IN PRESS ADV NEURAL, V12; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; RICHARDSON T, UNPUB IEEE T INFORMA; RICHARDSON TJ, IN PRESS IEEE T INFO; WEISS Y, IN PRESS ADV NEURAL, V12; Weiss Y., 1997, BELIEF PROPAGATION R	11	5	5	1	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						575	581						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700082
C	Smola, AJ; Shawe-Taylor, J; Scholkopf, B; Williamson, RC		Solla, SA; Leen, TK; Muller, KR		Smola, AJ; Shawe-Taylor, J; Scholkopf, B; Williamson, RC			The entropy regularization information criterion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Effective methods of capacity control via uniform convergence bounds for function expansions have been largely limited to Support Vector machines, where good bounds are obtainable by the entropy number approach. We extend these methods to systems with expansions in terms of arbitrary (parametrized) basis functions and a wide range of regularization methods covering the whole range of general linear additive models. This is achieved by a data dependent analysis of the eigenvalues of the corresponding design matrix.	Australian Natl Univ, Dept Engn & RSISE, Canberra, ACT 0200, Australia	Australian National University	Smola, AJ (corresponding author), Australian Natl Univ, Dept Engn & RSISE, GPO Box 4, Canberra, ACT 0200, Australia.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925; Shawe-Taylor, John/0000-0002-2030-0073				Carl B., 1990, ENTROPY COMPACTNESS; CHEN S, 1995, 479 STANF U DEP STAT; HORN RA, 1992, MATRIX ANAL; SCHOLKOPF B, 1999, NCTR99035 NEUROCOLT2; SHAWETAYLOR J, 1999, P EUROCOLT 99; WILLIAMSON RC, 1998, NCTR98019 NEUROCOLT; WILLIAMSON RC, 1999, MAXIMUM MARGIN MISCE	9	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						342	348						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700049
C	Spence, CD; Parra, L		Solla, SA; Leen, TK; Muller, KR		Spence, CD; Parra, L			Hierarchical image probability (HIP) models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				TEXTURE	We formulate a model for probability distributions on image spaces. We show that any distribution of images can be factored exactly into conditional distributions of feature vectors at one resolution (pyramid level) conditioned on the image information at lower resolutions. We would like to factor this over positions in the pyramid levels to make it tractable, but such factoring may miss long-range dependencies. To fix this, we introduce hidden class labels at each pixel in the pyramid. The result is a hierarchical mixture of conditional probabilities, similar to a hidden Markov model on a tree. The model parameters can be found with maximum likelihood estimation using the EM algorithm. We have obtained encouraging preliminary results on the problems of detecting various objects in SAR images and target recognition in optical aerial images.	Sarnoff Corp, Princeton, NJ 08543 USA	Sarnoff Corporation	Spence, CD (corresponding author), Sarnoff Corp, CN5300, Princeton, NJ 08543 USA.							BUCCIGROSSI RW, 1998, 414 1 PENN GRASP LAB; CHELLAPPA R, 1985, IEEE T ACOUST SPEECH, V33, P959, DOI 10.1109/TASSP.1985.1164641; DEBONET JS, 1998, P SPIE, V3370; GEMAN S, 1984, IEEE T PAMI, V6, P194; JEREMY S, 1998, C COMP VIS PATT REC; JORDAN MI, 1998, NATO SCI SERIES D, V89; LUETTGEN MR, 1995, IEEE T IMAGE PROCESS, V4, P194, DOI 10.1109/83.342185; SPENCE CD, 1998, NIPS, V11, P981; Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627	9	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						848	854						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700120
C	Wu, S; Nakahara, H; Murata, N; Amari, S		Solla, SA; Leen, TK; Muller, KR		Wu, S; Nakahara, H; Murata, N; Amari, S			Population decoding based on an unfaithful model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				INFORMATION; RECEIVERS; CODES	We study a population decoding paradigm in which the maximum likelihood inference is based on an unfaithful decoding model (UMLI), This is usually the case for neural population decoding because the encoding process of the brain is not exactly known, or because a simplified decoding model is preferred for saving computational cost. We consider an unfaithful decoding model which neglects the pair-wise correlation between neuronal activities, and prove that UMLI is asymptotically efficient when the neuronal correlation is uniform or of limited-range. The performance of UMLI is compared with that of the maximum likelihood inference based on a faithful model and that of the center of mass decoding method. It turns out that UMLI has advantages of decreasing the computational complexity remarkablely and maintaining a high-level decoding accuracy at the same time. The effect of correlation on the decoding accuracy is also discussed.	RIKEN, Brain Sci Inst, Wako, Saitama 35101, Japan	RIKEN	Wu, S (corresponding author), RIKEN, Brain Sci Inst, Hirosawa 2-1, Wako, Saitama 35101, Japan.	phwusi@brain.riken.go.jp; hiro@brain.riken.go.jp; mura@brain.riken.go.jp; amari@brain.riken.go.jp	Murata, Noboru/J-3345-2012	Murata, Noboru/0000-0002-4258-6877				Abbott LF, 1999, NEURAL COMPUT, V11, P91, DOI 10.1162/089976699300016827; ANDERSON CH, 1994, INT J MOD PHYS C, V5, P135; BALDI P, 1988, BIOL CYBERN, V59, P313, DOI 10.1007/BF00332921; GEORGOPOULOS AP, 1982, J NEUROSCI, V2, P1527; JOHNSON KO, 1980, J NEUROPHYSIOL, V43, P1793, DOI 10.1152/jn.1980.43.6.1793; Pouget A, 1998, NEURAL COMPUT, V10, P373, DOI 10.1162/089976698300017809; Salinas E, 1994, J Comput Neurosci, V1, P89, DOI 10.1007/BF00962720; SNIPPE HP, 1992, BIOL CYBERN, V67, P183, DOI 10.1007/BF00201025; Snippe HP, 1996, NEURAL COMPUT, V8, P511, DOI 10.1162/neco.1996.8.3.511; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818; [No title captured]	12	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						192	198						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700028
C	Brown, TX; Tong, H; Singh, S		Kearns, MS; Solla, SA; Cohn, DA		Brown, TX; Tong, H; Singh, S			Optimizing admission control while ensuring quality of service in multimedia networks via reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					This paper examines the application of reinforcement learning to a telecommunications networking problem. The problem requires that revenue be maximized while simultaneously meeting a quality of service constraint that forbids entry into certain slates. We present a general solution to this multi-criteria problem that is able to earn significantly higher revenues than alternatives.	Univ Colorado, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Brown, TX (corresponding author), Univ Colorado, Boulder, CO 80309 USA.	timxb@colorado.edu; tongh@colorado.edu; baveja@colorado.edu						Boyan J. A., 1994, P INT C ADV NEURAL I, P671; Brown TX, 1997, ADV NEUR IN, V9, P932; BROWN TX, 1997, 4 IFIP WORKSH PERF M; GABOR Z, 1998, INT C MACH LEARN MAD; Hiramatsu A, 1990, IEEE Trans Neural Netw, V1, P122, DOI 10.1109/72.80211; MARBACH P, 1998, ADV NIPS, P10; NIE J, IN PRESS IEEE T VEHI; Singh S, 1997, ADV NEUR IN, V9, P974	8	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						982	988						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700138
C	Dailey, MN; Cottrell, GW; Busey, TA		Kearns, MS; Solla, SA; Cohn, DA		Dailey, MN; Cottrell, GW; Busey, TA			Facial memory is kernel density estimation (almost)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We compare the ability of three exemplar-based memory models, each using three different face stimulus representations, to account for the probability a human subject responded "old" in an old/new facial memory experiment. The models are 1) the Generalized Context Model, 2) SimSample, a probabilistic sampling model, and 3) MMOM, a novel model related to kernel density estimation that explicitly encodes stimulus distinctiveness. The representations are 1) positions of stimuli in MDS "face space," 2) projections of test faces onto the "eigenfaces" of the study set, and 3) a representation based on response to a grid of Gabor filter jets. Of the 9 model/representation combinations, only the distinctiveness model in MDS space predicts the observed "morph familiarity inversion" effect, in which the subjects' false alarm rate for morphs between similar faces is higher than their hit rate for many of the studied faces. This evidence is consistent with the hypothesis that human memory for faces is a kernel density estimation task, with the caveat that distinctive faces require larger kernels than do typical faces.	Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Dailey, MN (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.	mdailey@cs.ucsd.edu; gary@cs.ucsd.edu; busey@indiana.edu		Cottrell, Garrison/0000-0001-7538-1715				Bishop, 1995, NEURAL NETWORKS PATT; BUHMANN J, 1990, P IJCNN INT JOINT C, V2, P411; BUSEY TA, UNPUB J EXPT PSYCHOL; BUSEY TA, 1999, IN PRESS PSYCHOL SCI; Dailey MN, 1998, PROCEEDINGS OF THE TWENTIETH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P273; GILLUND G, 1984, PSYCHOL REV, V93, P411; NOSOFSKY RM, 1986, J EXP PSYCHOL GEN, V115, P39, DOI 10.1037/0096-3445.115.1.39; REINITZ MT, 1992, MEM COGNITION, V20, P1, DOI 10.3758/BF03208247; SOLSO RL, 1981, BRIT J PSYCHOL, V72, P499, DOI 10.1111/j.2044-8295.1981.tb01779.x; TANAKA J, UNPUB MAPPING ATTRAC; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; VALENTINE T, 1992, Q J EXP PSYCHOL-A, V44, P671, DOI 10.1080/14640749208401305	12	5	5	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						24	30						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700004
C	Itti, L; Braun, J; Lee, DK; Koch, C		Kearns, MS; Solla, SA; Cohn, DA		Itti, L; Braun, J; Lee, DK; Koch, C			Attentional modulation of human pattern discrimination psychophysics reproduced by a quantitative model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				VISUAL-ATTENTION; AREAS V1; CORTEX; V2; V4	We previously proposed a quantitative model of early visual processing in primates, based on non-linearly interacting visual filters and statistically efficient decision. We now use this model to interpret the observed modulation of a range of human psychophysical thresholds with and without focal Visual attention. Our model calibrated by an automatic fitting procedure - simultaneously reproduces thresholds for four classical pattern discrimination tasks, performed while attention was engaged by another concurrent task. Our model then predicts that the seemingly complex improvements of certain thresholds, which we observed when attention was fully available for the discrimination tasks, can best be explained by a strengthening of competition among early visual filters.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Itti, L (corresponding author), CALTECH, MSC 139-74, Pasadena, CA 91125 USA.	itti@klab.caltech.edu; achim@klab.caltech.edu; jjwen@klab.caltech.edu; koch@klab.caltech.edu		Koch, Christof/0000-0001-6482-8067				BONNEL AM, 1992, Q J EXP PSYCHOL-A, V44, P601, DOI 10.1080/14640749208401302; GANDHI SP, 1998, INV OPHT VIS SCI ARV, V39, P5194; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; ITTI L, IN PRESS P NIPS; ITTI L, 1998, INV OPHT VIS SCI P A, V39, P2934; KOCH C, 1985, HUM NEUROBIOL, V4, P219; Lee DK, 1997, VISION RES, V37, P2409, DOI 10.1016/S0042-6989(97)00055-2; LEE DK, 1998, INV OPHT VIS SCI P A, V39, P2938; Luck SJ, 1997, J NEUROPHYSIOL, V77, P24, DOI 10.1152/jn.1997.77.1.24; MAUNSELL JHR, 1995, SCIENCE, V270, P764, DOI 10.1126/science.270.5237.764; MOTTER BC, 1993, J NEUROPHYSIOL, V70, P909, DOI 10.1152/jn.1993.70.3.909; NAKAYAMA K, 1989, VISION RES, V29, P1631, DOI 10.1016/0042-6989(89)90144-2; Pouget A, 1998, NEURAL COMPUT, V10, P373, DOI 10.1162/089976698300017809; SOMERS DC, 1998, INV OPHT VIS SCI P A, V39, P5192; SPERLING G, 1978, SCIENCE, V202, P315, DOI 10.1126/science.694536; SPITZER H, 1988, SCIENCE, V240, P338, DOI 10.1126/science.3353728; Treue S, 1996, NATURE, V382, P539, DOI 10.1038/382539a0	17	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						789	795						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700111
C	Jebara, T; Pentland, A		Kearns, MS; Solla, SA; Cohn, DA		Jebara, T; Pentland, A			Maximum conditional likelihood via bound maximization and the CEM algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We present the CEM (Conditional Expectation Maximization) algorithm as an extension of the EM (Expectation Maximization) algorithm to conditional density estimation under missing data. A bounding and maximization process is given to specifically optimize conditional likelihood instead of the usual joint likelihood. We apply the method to conditioned mixture models and use bounding techniques to derive the model's update rules. Monotonic convergence, computational efficiency and regression results superior to EM are demonstrated.	MIT, Media Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Jebara, T (corresponding author), MIT, Media Lab, Cambridge, MA 02139 USA.							AMARI S, 1995, NEURAL NETWORKS, V8; Bishop C.M., 1996, NEURAL NETWORKS PATT, P1; DEMPSTER AP, 1977, J ROYAL STAT SOC B, V39; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; MENG X, 1993, BIOMETRIKA, V80; POPAT A, 1997, 461 MIT MED LAB; XU L, 1995, NEURAL INFORMATION P, V7	7	5	6	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						494	500						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700070
C	Moghaddam, B; Jebara, T; Pentland, A		Kearns, MS; Solla, SA; Cohn, DA		Moghaddam, B; Jebara, T; Pentland, A			Bayesian modeling of facial similarity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				RECOGNITION	In previous work [6, 9, 10], we advanced a new technique for direct visual matching of images for the purposes of face recognition and image retrieval, using a probabilistic measure of similarity based primarily on a Bayesian (MAP) analysis of image differences. leading to a "dual" basis similar to eigenfaces [13]. The performance advantage of this probabilistic matching technique over standard Euclidean nearest-neighbor eigenface matching was recently demonstrated using results from DARPA's 1996 "FERET" face recognition competition, in which this probabilistic matching algorithm was found to be the top performer. We have further developed a simple method of replacing the costly compution of nonlinear (online) Bayesian similarity measures by the relatively inexpensive computation of linear (offline) subspace projections and simple (online) Euclidean norms, thus resulting in a significant computational speed-up for implementation with very large image databases as typically encountered in real-world applications.	Mitsubishi Elect Res Lab, Cambridge, MA 02139 USA		Moghaddam, B (corresponding author), Mitsubishi Elect Res Lab, 201 Broadway, Cambridge, MA 02139 USA.	baback@merl.com; jebara@media.mit.edu; sandy@media.mit.edu						Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BRUNELLI R, 1993, IEEE T PATTERN ANAL, P15; Etemad K, 1996, INT CONF ACOUST SPEE, P2148, DOI 10.1109/ICASSP.1996.545741; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; JONES MJ, 1996, 1583 MIT ART INT LAB; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Moghaddam B, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P30, DOI 10.1109/AFGR.1998.670921; MOGHADDAM B, 1995, IEEE P 5 INT C COMP; MOGHADDAM B, 1996, P IEEE C COMP VIS PA; NASTAR C, 1996, P 4 EUR C COMP VIS E; Phillips PJ, 1997, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.1997.609311; Swets DL, 1996, IEEE T PATTERN ANAL, V18, P831, DOI 10.1109/34.531802; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71	13	5	5	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						910	916						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700128
C	Williamson, M; Murray-Smith, R; Hansen, V		Kearns, MS; Solla, SA; Cohn, DA		Williamson, M; Murray-Smith, R; Hansen, V			Robot docking using Mixtures of Gaussians	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					This paper applies the Mixture of Gaussians probabilistic model, combined with Expectation Maximization optimization to the task of summarizing three dimensional range data for a mobile robot. This provides a flexible way of dealing with uncertainties in sensor information, and allows the introduction of prior knowledge into low-level perception modules. Problems with the basic approach were solved in several ways: the mixture of Gaussians was reparameterized to reflect the types of objects expected in the scene, and priors on model parameters were included in the optimization process. Both approaches force the optimization to find 'interesting' objects, given the sensor and object characteristics. A higher level classifier was used to interpret the results provided by the model, and to reject spurious solutions.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Williamson, M (corresponding author), MIT, AI Lab, Cambridge, MA 02139 USA.		Murray-Smith, Roderick/AAF-9987-2020	Murray-Smith, Roderick/0000-0003-4228-7962				ARMAN F, 1993, COMPUT SURV, V25, P5, DOI 10.1145/151254.151255; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duda R.O., 1973, J ROYAL STAT SOC SER; McMichael D. W., 1995, Fourth International Conference on `Artificial Neural Networks' (Conf. Publ. No.409), P364, DOI 10.1049/cp:19950583; *PIAP, 1995, 02486 PIAP TRC; RICHARDSON S, 1997, J ROYAL STAT SOC B, V50, P700; TITTERINGTON DM, 1985, STAT ANAL FINITE MIX	7	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						945	951						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700133
C	Zhou, P; Austin, J; Kennedy, J		Kearns, MS; Solla, SA; Cohn, DA		Zhou, P; Austin, J; Kennedy, J			A high performance k-NN classifier using a binary correlation matrix memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NEIGHBOR	This paper presents a novel and fast R-NN classifier that is based on a binary CMM (Correlation Matrix Memory) neural network. A robust encoding method is developed to meet CMM input requirements. A hardware implementation of the CMM is described, which gives over 200 times the speed of a current mid-range workstation, and is scaleable to very large problems. When tested on several benchmarks and compared with a simple k-NN method, the CMM classifier gave less than 1% lower accuracy and over 4 and 12 times speed-up in software and hardware respectively.	Univ York, Adv Comp Architecture Grp, Dept Comp Sci, York YO10 5DD, N Yorkshire, England	University of York - UK	Zhou, P (corresponding author), Univ York, Adv Comp Architecture Grp, Dept Comp Sci, York YO10 5DD, N Yorkshire, England.							AUSTIN J, 1987, IMAGE VISION COMPUT, V5, P251, DOI 10.1016/0262-8856(87)90001-1; AUSTIN J, 1996, AURA DISTRIBUTED ASS; DASARATHY BV, 1994, IEEE T SYST MAN CYB, V24, P511, DOI 10.1109/21.278999; Grother PJ, 1997, PATTERN RECOGN, V30, P459, DOI 10.1016/S0031-3203(96)00098-2; MICHIE D, 1994, MACHINE LERNING NEUR, pCH9; Turner M, 1997, NEURAL NETWORKS, V10, P1637, DOI 10.1016/S0893-6080(97)00059-2; WILLSHAW DJ, 1969, NATURE, V222, P960, DOI 10.1038/222960a0; Zhou P, 1998, NEURAL COMPUT APPL, V7, P334, DOI 10.1007/BF01428124	8	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						713	719						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700101
C	Barber, D; Schottky, B		Jordan, MI; Kearns, MJ; Solla, SA		Barber, D; Schottky, B			Radial Basis Functions: a Bayesian treatment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Bayesian methods have been successfully applied to regression and classification problems in multi-layer perceptrons. We present a novel application of Bayesian techniques to Radial Basis Function networks by developing a Gaussian approximation to the posterior distribution which, for fixed basis function widths, is analytic in the parameters. The setting of regularization constants by cross-validation is wasteful as only a single optimal parameter estimate is retained. We treat this issue by assigning prior distributions to these constants, which are then adapted in light of the data under a simple re-estimation formula.	Univ Nijmegen, SNN, Nijmegen, Netherlands	Radboud University Nijmegen	Barber, D (corresponding author), Univ Nijmegen, SNN, Geert Grooteplein 21, Nijmegen, Netherlands.								0	5	5	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						402	408						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700057
C	Seung, HS; Richardson, TJ; Lagarias, JC; Hopfield, JJ		Jordan, MI; Kearns, MJ; Solla, SA		Seung, HS; Richardson, TJ; Lagarias, JC; Hopfield, JJ			Minimax and Hamiltonian dynamics of excitatory-inhibitory networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A Lyapunov function for excitatory-inhibitory networks is constructed. The construction assumes symmetric interactions within excitatory and inhibitory populations of neurons, and antisymmetric interactions between populations. The Lyapunov function yields sufficient conditions for the global asymptotic stability of fixed points. If these conditions are violated, limit cycles may be stable. The relations of the Lyapunov function to optimization theory and classical mechanics are revealed by minimax and dissipative Hamiltonian forms of the network dynamics.	AT&T Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T; Nokia Corporation; Nokia Bell Labs	Seung, HS (corresponding author), AT&T Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.								0	5	5	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						329	335						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700047
C	Shawe-Taylor, J; Cristianini, N		Jordan, MI; Kearns, MJ; Solla, SA		Shawe-Taylor, J; Cristianini, N			Data-dependent structural risk minimisation for Perceptron Decision Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Perceptron Decision Trees (also known as Linear Machine DTs, etc.) are analysed in order that data-dependent Structural Risk Minimization can be applied. Data-dependent analysis is performed which indicates that choosing the maximal margin hyperplanes at the decision nodes will improve the generalization. The analysis uses a novel technique to bound the generalization error in terms of the margins at individual nodes. Experiments performed on real data sets confirm the validity of the approach.	Univ London Royal Holloway & Bedford New Coll, Dept Comp Sci, Egham TW20 0EX, Surrey, England	University of London; Royal Holloway University London	Shawe-Taylor, J (corresponding author), Univ London Royal Holloway & Bedford New Coll, Dept Comp Sci, Egham TW20 0EX, Surrey, England.			Shawe-Taylor, John/0000-0002-2030-0073					0	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						336	342						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700048
C	Yang, HH; Amari, S		Jordan, MI; Kearns, MJ; Solla, SA		Yang, HH; Amari, S			The efficiency and the robustness of natural gradient descent learning rule	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The inverse of the Fisher information matrix is used in the natural gradient descent algorithm to train single-layer and multi-layer perceptrons. We have discovered a new scheme to represent the Fisher information matrix of a stochastic multi-layer perceptron. Based on this scheme, we have designed an algorithm to compute the natural gradient. When the input dimension it is much larger than the number of hidden neurons, the complexity of this algorithm is of order O(n). It is confirmed by simulations that the natural gradient descent learning rule is not only efficient but also robust.	Oregon Grad Inst, Dept Comp Sci, Portland, OR 97291 USA		Yang, HH (corresponding author), Oregon Grad Inst, Dept Comp Sci, POB 91000, Portland, OR 97291 USA.	hyang@cse.ogi.edu; amari@zoo.brain.riken.go.jp							0	5	5	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						385	391						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700055
C	Coetzee, FM; Stonick, VL		Mozer, MC; Jordan, MI; Petsche, T		Coetzee, FM; Stonick, VL			488 solutions to the XOR problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A globally convergent homotopy method is defined that is capable of sequentially producing large numbers of stationary points of the multi-layer perceptron mean-squared error surface. Using this algorithm large subsets of the stationary points of two test problems are found. It is shown empirically that the MLP neural network appears to have an extreme ratio of saddle points compared to local minima, and that even small neural network problems have extremely large numbers of solutions.			Coetzee, FM (corresponding author), CARNEGIE MELLON UNIV,DEPT ELECT ENGN,PITTSBURGH,PA 15213, USA.								0	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						410	416						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00058
C	Dong, DW		Mozer, MC; Jordan, MI; Petsche, T		Dong, DW			Spatiotemporal coupling and scaling of natural images nd human visual sensitivities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We study the spatiotemporal correlation in natural time-varying images and explore the hypothesis that the visual system is concerned with the optimal coding of visual representation through spatiotemporal decorrelation of the input signal. Based on the measured spatiotemporal power spectrum, the transform needed to decorrelate input signal is derived analytically and then compared with the actual processing observed in psychophysical experiments.			Dong, DW (corresponding author), CALTECH,MAIL CODE 139-74,PASADENA,CA 91125, USA.								0	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						859	865						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00121
C	Duff, MO; Barto, AG		Mozer, MC; Jordan, MI; Petsche, T		Duff, MO; Barto, AG			Local bandit approximation for optimal learning problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In general, procedures for determining Bayes-optimal adaptive controls for Markov decision processes (MDP's) require a prohibitive amount of computation--the optimal learning problem is intractable. This paper proposes an approximate approach in which bandit processes are used to model, in a certain ''local'' sense, a given MDP. Bandit processes constitute an important subclass of MDP's, and have optimal learning strategies (defined in terms of Gittins indices) that can be computed relatively efficiently. Thus, one scheme for achieving approximately-optimal learning for general MDP's proceeds by taking actions suggested by strategies that are optimal with respect to local bandit models.			Duff, MO (corresponding author), UNIV MASSACHUSETTS,DEPT COMP SCI,AMHERST,MA 01003, USA.								0	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1019	1025						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00143
C	Dunmur, AP; Titterington, DM		Mozer, MC; Jordan, MI; Petsche, T		Dunmur, AP; Titterington, DM			On a modification to the mean field EM algorithm in factorial learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A modification is described to the use of mean field approximations in the E step of EM algorithms for analysing data from latent structure models, as described by Ghahramani (1995), among others. The modification involves second-order Taylor approximations to expectations computed in the E step. The potential benefits of the method are illustrated using very simple latent profile models.			Dunmur, AP (corresponding author), UNIV GLASGOW,DEPT STAT,MATH BLDG,GLASGOW G12 8QQ,LANARK,SCOTLAND.								0	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						431	437						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00061
C	Kang, KJ; Oh, JH		Mozer, MC; Jordan, MI; Petsche, T		Kang, KJ; Oh, JH			Statistical mechanics of the mixture of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We study generalization capability of the mixture of experts learning from examples generated by another network with the same architecture. When the number of examples is smaller than a critical value, the network shows a symmetric phase where the role of the experts is not specialized. Upon crossing the critical point, the system undergoes a continuous phase transition to a symmetry breaking phase where the gating network partitions the input space effectively and each expert is assigned to an appropriate sub-space. We also find that the mixture of experts with multiple level of hierarchy shows multiple phase transitions.			Kang, KJ (corresponding author), POHANG UNIV SCI & TECHNOL,DEPT PHYS,HYOJA SAN 31,POHANG 790784,KYONGBUK,SOUTH KOREA.								0	5	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						183	189						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00026
C	Singer, Y; Warmuth, MK		Mozer, MC; Jordan, MI; Petsche, T		Singer, Y; Warmuth, MK			Training algorithms for hidden Markov models using entropy based distance functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present new algorithms for parameter estimation of HMMs. By adapting a framework used for supervised learning, we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay ''close'' to the current estimated parameters. We use a bound on the relative entropy between the two HMMs as a distance measure between them. The result is new iterative training algorithms which are similar to the EM (Baum-Welch) algorithm for training HMMs. The proposed algorithms are composed of a step similar to the expectation step of Baum-Welch and a new update of the parameters which replaces the maximization (re-estimation) step. The algorithm takes only negligibly more time per iteration and an approximated version uses the same expectation step as Baum-Welch. We evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data. For sparse models, i.e. models with relatively small number of non-zero parameters, the proposed algorithms require significantly fewer iterations.			Singer, Y (corresponding author), AT&T BELL LABS,600 MT AVE,MURRAY HILL,NJ 07974, USA.								0	5	5	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						641	647						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00091
C	Bishop, CM; Svensen, M; Williams, CKI		Touretzky, DS; Mozer, MC; Hasselmo, ME		Bishop, CM; Svensen, M; Williams, CKI			EM optimization of latent-variable density models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						ASTON UNIV,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND	Aston University									0	5	5	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						465	471						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00066
C	Goodhill, GJ; Finch, S; Sejnowski, TJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Goodhill, GJ; Finch, S; Sejnowski, TJ			Optimizing cortical mappings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SALK INST BIOL STUDIES,LA JOLLA,CA 92037	Salk Institute			Sejnowski, Terrence/AAV-5558-2021; Goodhill, Geoffrey J/D-9984-2013						0	5	5	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						330	336						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00047
C	Indiveri, G; Kramer, J; Koch, C		Touretzky, DS; Mozer, MC; Hasselmo, ME		Indiveri, G; Kramer, J; Koch, C			Parallel analog VLSI architectures for computation of heading direction and time-to-contact	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CALTECH,DIV BIOL,PASADENA,CA 91125	California Institute of Technology			Indiveri, Giacomo/A-4282-2010; Indiveri, Giacomo/AAX-3310-2020	Indiveri, Giacomo/0000-0002-7109-1689; Indiveri, Giacomo/0000-0002-7109-1689					0	5	5	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						720	726						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00102
C	Lawrence, S; Tsoi, AC; Back, AD		Touretzky, DS; Mozer, MC; Hasselmo, ME		Lawrence, S; Tsoi, AC; Back, AD			The Gamma MLP for speech phoneme recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV QUEENSLAND,DEPT ELECT & COMP ENGN,ST LUCIA,QLD 4072,AUSTRALIA	University of Queensland				Tsoi, Ah Chung/0000-0003-2904-7008; Back, Andrew/0000-0001-5474-1910					0	5	5	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						785	791						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00111
C	Petersen, RS; Taylor, JG		Touretzky, DS; Mozer, MC; Hasselmo, ME		Petersen, RS; Taylor, JG			Reorganisation of somatosensory cortex after tactile training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV LONDON KINGS COLL,CTR NEURAL NETWORKS,LONDON WC2R 2LS,ENGLAND	University of London; King's College London									0	5	5	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						82	88						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00012
C	Sabes, PN; Jordan, MI		Touretzky, DS; Mozer, MC; Hasselmo, ME		Sabes, PN; Jordan, MI			Reinforcement learning by probability matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,DEPT BRAIN & COGNIT SCI,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)			Jordan, Michael I/C-5253-2013						0	5	5	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1080	1086						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00152
C	ANDERSON, DZ; BENKERT, C; HEBLER, V; JANG, JS; MONTGOMERY, D; SAFFMAN, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		ANDERSON, DZ; BENKERT, C; HEBLER, V; JANG, JS; MONTGOMERY, D; SAFFMAN, M			OPTICAL IMPLEMENTATION OF A SELF-ORGANIZING FEATURE EXTRACTOR	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	5	5	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						821	828						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00101
C	BERNASCONI, J; GUSTAFSON, K		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BERNASCONI, J; GUSTAFSON, K			HUMAN AND MACHINE QUICK MODELING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	5	5	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1151	1158						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00142
C	PEARLMUTTER, B		MOODY, JE; HANSON, SJ; LIPPMANN, RP		PEARLMUTTER, B			GRADIENT DESCENT - 2ND-ORDER MOMENTUM AND SATURATING ERROR	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Pearlmutter, Barak A./AAL-8999-2020; Pearlmutter, Barak A/M-8791-2014	Pearlmutter, Barak A/0000-0003-0521-4553					0	5	5	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						887	894						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00109
C	PINKAS, G		MOODY, JE; HANSON, SJ; LIPPMANN, RP		PINKAS, G			CONSTRUCTING PROOFS IN SYMMETRICAL NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	5	5	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						217	224						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00027
C	RAMACHANDRAN, S; PRATT, LY		MOODY, JE; HANSON, SJ; LIPPMANN, RP		RAMACHANDRAN, S; PRATT, LY			INFORMATION MEASURE BASED SKELETONIZATION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	5	5	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1080	1087						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00133
C	SANGER, TD; SUTTON, RS; MATHEUS, CJ		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SANGER, TD; SUTTON, RS; MATHEUS, CJ			ITERATIVE CONSTRUCTION OF SPARSE POLYNOMIAL APPROXIMATIONS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	5	5	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1064	1071						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00131
C	SUTTON, JP; MAMELAK, AN; HOBSON, JA		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SUTTON, JP; MAMELAK, AN; HOBSON, JA			NETWORK MODEL OF STATE-DEPENDENT SEQUENCING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	5	5	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						283	290						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00035
C	Awasthi, P; Jain, H; Rawat, AS; Vijayaraghavan, A		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Awasthi, Pranjal; Jain, Himanshu; Rawat, Ankit Singh; Vijayaraghavan, Aravindan			Adversarial robustness via robust low rank representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Adversarial robustness measures the susceptibility of a classifier to imperceptible perturbations made to the inputs at test time. In this work we highlight the benefits of natural low rank representations that often exist for real data such as images, for training neural networks with certified robustness guarantees. Our first contribution is for certified robustness to perturbations measured in l(2) norm. We exploit low rank data representations to provide improved guarantees over state-of-the-art randomized smoothing-based approaches on standard benchmark datasets such as CIFAR-10 and CIFAR-100. Our second contribution is for the more challenging setting of certified robust- ness to perturbations measured in l(infinity) norm. We demonstrate empirically that natural low rank representations have inherent robustness properties, that can be leveraged to provide significantly better guarantees for certified robustness to l(infinity) perturbations in those representations. Our certificate of l(infinity) robustness relies on a natural quantity involving the infinity -> 2 matrix operator norm associated with the representation, to translate robustness guarantees from l(2) to l(infinity) perturbations. A key technical ingredient for our certification guarantees is a fast algorithm with provable guarantees based on the multiplicative weights update method to provide upper bounds on the above matrix norm. Our algorithmic guarantees improve upon the state of the art for this problem, and may be of independent interest.	[Awasthi, Pranjal; Jain, Himanshu; Rawat, Ankit Singh] Google Res, New York, NY 10011 USA; [Awasthi, Pranjal] Rutgers State Univ, New Brunswick, NJ 08854 USA; [Vijayaraghavan, Aravindan] Northwestern Univ, Evanston, IL 60208 USA	Google Incorporated; Rutgers State University New Brunswick; Northwestern University	Awasthi, P (corresponding author), Google Res, New York, NY 10011 USA.; Awasthi, P (corresponding author), Rutgers State Univ, New Brunswick, NJ 08854 USA.	pranjal.awasthi@rutgers.edu; himj@google.com; ankitsrawat@google.com; aravindv@northwestern.edu	Vijayaraghavan, Aravindan/I-2257-2015		NSF HDR TRIPODS award [CCF-1934924]; National Science Foundation (NSF) [CCF-1652491, CCF-1637585, CCF-1934931]	NSF HDR TRIPODS award; National Science Foundation (NSF)(National Science Foundation (NSF))	The first author acknowledges support from the NSF HDR TRIPODS award CCF-1934924. The last author was supported by the National Science Foundation (NSF) under Grant No. CCF-1652491, CCF-1637585 and CCF-1934931.	Allen-Zhu Zeyuan, 2016, P 2016 ANN ACMSIAM S, P1824; Alon N, 2006, INVENT MATH, V163, P499, DOI 10.1007/s00222-005-0465-9; Alon N, 2004, P 30 SIXTHANNUAL ACM, P72; Arora S, 2005, ANN IEEE SYMP FOUND, P339, DOI 10.1109/SFCS.2005.35; Arora S, 2007, ACM S THEORY COMPUT, P227, DOI 10.1145/1250790.1250823; Awasthi Pranjal, 2019, ADV NEURAL INFORM PR, P13737; Awasthi Pranjal, 2019, ARXIV191113268; Bafna Mitali, 2018, ADV NEURAL INFORM PR, V31, P10075; Bhattiprolu Vijay, 2018, ARXIV180207425; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Blum Avrim, 2020, ARXIV200203517; Cao XY, 2017, ANN COMPUT SECURITY, P278, DOI 10.1145/3134600.3134606; Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009; Charikar M, 2004, ANN IEEE SYMP FOUND, P54, DOI 10.1109/FOCS.2004.39; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cohen Jeremy M, 2019, ARXIV190202918; Croce F., 2018, ARXIV181007481; Diamond S, 2016, J MACH LEARN RES, V17; Dvijotham K, 2018, ARXIV180510265; Ebrahimi Javid, 2017, ARXIV171206751; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Goodfellow I. J., 2014, ARXIV14126572; Gowal Sven, 2018, EFFECTIVENESS INTERV; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang ZY, 2018, ACM S THEORY COMPUT, P17, DOI 10.1145/3188745.3188858; Iyengar G, 2011, SIAM J OPTIMIZ, V21, P231, DOI 10.1137/090762671; Jain R, 2011, ANN IEEE SYMP FOUND, P463, DOI 10.1109/FOCS.2011.25; Jambulapati A, 2020, ACM S THEORY COMPUT, P789, DOI 10.1145/3357713.3384338; Kale Satyen, 2007, EFFICIENT ALGORITHMS; Klein P., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P338, DOI 10.1145/237814.237980; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kumar Aounon, 2020, ARXIV200203239; Lecuyer M, 2019, P IEEE S SECUR PRIV, P656, DOI 10.1109/SP.2019.00044; Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68; Lee Yin Tat, 2019, ABS190301859 CORR; Li B, 2019, ADV NEUR IN, V32; Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23; Madry A., 2018, ARXIV PREPRINT ARXIV; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Mirman M, 2018, PR MACH LEARN RES, V80; MOSEK ApS, 2019, MOSEK FUS API PYTH M; Nesterov Y, 1998, OPTIM METHOD SOFTW, V9, P141, DOI 10.1080/10556789808805690; Pedregosa F., 2011, J MACH LEARN RES, V12, P2825; PLOTKIN SA, 1991, PROCEEDINGS - 32ND ANNUAL SYMPOSIUM ON FOUNDATIONS OF COMPUTER SCIENCE, P495, DOI 10.1109/SFCS.1991.185411; Raghunathan A, 2018, ADV NEUR IN, V31; Razenshteyn Ilya, 2019, ADV NEURAL INFORM PR; Salman Hadi, 2020, ARXIV200301908; Sanyal Amartya, 2018, ABS180407090 CORR; Shafahi A, 2019, ADV NEUR IN, V32; Singh G, 2018, ADV NEUR IN, V31; Wang SQ, 2018, ADV NEUR IN, V31; Wang Shiqi, 2018, ABS181102625 CORR; Wong E, 2018, PR MACH LEARN RES, V80; Wong Eric, 2020, ARXIV200103994; Yang Greg, 2020, ARXIV200208118; Yang Yao-Yuan, 2020, ARXIV200302460; Yang Yuzhe, 2019, ARXIV190511971; Zhang H., 2019, ARXIV190108573; Zhang Huan, 2018, ARXIV180409699	60	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000065
C	Faghri, F; Tabrizian, I; Markov, I; Alistarh, D; Roy, DM; Ramezani-Kebrya, A		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Faghri, Fartash; Tabrizian, Iman; Markov, Ilia; Alistarh, Dan; Roy, Daniel M.; Ramezani-Kebrya, Ali			Adaptive Gradient Quantization for Data-Parallel SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Many communication-efficient variants of SGD use gradient quantization schemes. These schemes are often heuristic and fixed over the course of training. We empirically observe that the statistics of gradients of deep models change during the training. Motivated by this observation, we introduce two adaptive quantization schemes, ALQ and AMQ. In both schemes, processors update their compression schemes in parallel by efficiently computing sufficient statistics of a parametric distribution. We improve the validation accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost communication setups. Our adaptive methods are also significantly more robust to the choice of hyperparameters.	[Faghri, Fartash; Tabrizian, Iman; Roy, Daniel M.] Univ Toronto, Toronto, ON, Canada; [Faghri, Fartash; Tabrizian, Iman; Roy, Daniel M.; Ramezani-Kebrya, Ali] Vector Inst, Toronto, ON, Canada; [Markov, Ilia; Alistarh, Dan] IST Austria, Klosterneuburg, Austria; [Alistarh, Dan] NeuralMagic, Cambridge, MA USA	University of Toronto; Institute of Science & Technology - Austria	Faghri, F (corresponding author), Univ Toronto, Toronto, ON, Canada.; Faghri, F (corresponding author), Vector Inst, Toronto, ON, Canada.	faghri@cs.toronto.edu; iman.tabrizian@mail.utoronto.ca; alir@vectorinstitute.ai		Alistarh, Dan/0000-0003-3650-940X; Ramezani-Kebrya, Ali/0000-0002-8767-5603	OGS Scholarship; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [805223 ScaleML]; NSERC; Province of Ontario; Government of Canada through CIFAR; Vector Institute	OGS Scholarship; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Province of Ontario; Government of Canada through CIFAR; Vector Institute	The authors would like to thank Blair Bilodeau, David Fleet, Mufan Li, and Jeffrey Negrea for helpful discussions. FF was supported by OGS Scholarship. DA and IM were supported the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 805223 ScaleML). DMR was supported by an NSERC Discovery Grant. ARK was supported by NSERC Postdoctoral Fellowship. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.	Abadi M, 2015, P 12 USENIX S OPERAT; Alistarh D., 2020, ARXIV200209268V2; Alistarh D, 2017, ADV NEUR IN, V30; Bernstein J, 2018, PR MACH LEARN RES, V80; Bubeck S., 2015, FDN TRENDS MACHINE L; Chaturapruek S, 2015, ADV NEUR IN, V28; CHILIMBI TM, 2014, P OSDI, V14, P571; Choi D, 2019, ARXIV191005446; Coates A., 2013, P INT C MACH LEARN I; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; CUMMISKEY P, 1973, BELL SYST TECH J, V52, P1105, DOI 10.1002/j.1538-7305.1973.tb02007.x; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fu F., 2020, P INT C MACH LEARN I; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gupta S., 2015, P INT C MACH LEARN I; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Horvath S., 2019, ARXIV190510988V1; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; PROTTER MH, 1985, INTERMEDIATE CALCULU; Ramezani-Kebrya A., 2019, ARXIV190806077V1; Ramezani-Kebrya A., NUQSGD PROVABLY COMM; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Seide F, 2014, INT CONF ACOUST SPEE; Wen W, 2017, ADV NEUR IN, V30; Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014; Yang T., 2016, ARXIV160403257V2; Zhang D., 2018, P EUR C COMP VIS ECC; Zhang H., 2017, P INT C MACH LEARN I; Zhang SX, 2015, ADV NEUR IN, V28; Zhou S., 2016, ARXIV160606160; Zinkevich Martin, 2010, PROC ADV NEURAL INF	36	4	4	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000036
C	Liu, MR; Zhang, W; Mroueh, Y; Cui, XD; Ross, J; Yang, TB; Das, P		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Liu, Mingrui; Zhang, Wei; Mroueh, Youssef; Cui, Xiaodong; Ross, Jerret; Yang, Tianbao; Das, Payel			A Decentralized Parallel Algorithm for Training Generative Adversarial Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				VARIATIONAL-INEQUALITIES; MONOTONE-OPERATORS; CONVERGENCE	Generative Adversarial Networks (GANs) are a powerful class of generative models in the deep learning community. Current practice on large-scale GAN training utilizes large models and distributed large-batch training strategies, and is implemented on deep learning frameworks (e.g., TensorFlow, PyTorch, etc.) designed in a centralized manner. In the centralized network topology, every worker needs to either directly communicate with the central node or indirectly communicate with all other workers in every iteration. However, when the network bandwidth is low or network latency is high, the performance would be significantly degraded. Despite recent progress on decentralized algorithms for training deep neural networks, it remains unclear whether it is possible to train GANs in a decentralized manner. The main difficulty lies at handling the nonconvex-nonconcave min-max optimization and the decentralized communication simultaneously. In this paper, we address this difficulty by designing the first gradient-based decentralized parallel algorithm which allows workers to have multiple rounds of communications in one iteration and to update the discriminator and generator simultaneously, and this design makes it amenable for the convergence analysis of the proposed decentralized algorithm. Theoretically, our proposed decentralized algorithm is able to solve a class of non-convex non-concave min-max problems with provable non-asymptotic convergence to first-order stationary point. Experimental results on GANs demonstrate the effectiveness of the proposed algorithm.	[Liu, Mingrui; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Zhang, Wei; Mroueh, Youssef; Cui, Xiaodong; Ross, Jerret; Das, Payel] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA	University of Iowa; International Business Machines (IBM)	Liu, MR (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	mingruiliu.ml@gmail.com		Liu, Mingrui/0000-0002-5181-3429	NSF [1933212]; NSF CAREER Award [1844403]	NSF(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank anonymous reviewers for their constructive comments. This work was partially supported by NSF #1933212, NSF CAREER Award #1844403.	Ahmed A., 2014, P 11 USENIX C OP SYS, P583; [Anonymous], 2012, C LEARN THEOR; [Anonymous], 2018, P ADV NEUR INF PROC; [Anonymous], 1984, PROBLEMS DECENTRALIZ; Antipov G, 2017, IEEE IMAGE PROC, P2089; Arjovsky M., 2017, ARXIV170107875; Assran M, 2018, STOCHASTIC GRADIENT; Aybat NS, 2018, IEEE T AUTOMAT CONTR, V63, P5, DOI 10.1109/TAC.2017.2713046; Azizian Waiss, 2019, ARXIV190605945; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877; Chavdarova Tatjana, 2019, ARXIV190408598; Chen YR, 2018, FEAST'18: PROCEEDINGS OF THE 2018 WORKSHOP ON FORMING AN ECOSYSTEM AROUND SOFTWARE TRANSFORMATION, P1, DOI 10.1145/3273045.3273048; Dang CD, 2015, COMPUT OPTIM APPL, V60, P277, DOI 10.1007/s10589-014-9673-9; Dash Ayushman, 2017, ARXIV170306412; Daskalakis Constantinos, 2017, ARXIV171100141; Daskalakis Constantinos, 2018, ADV NEURAL INFORM PR, P9236; Dean Jeffrey, 2012, ADV NEURAL INFORM PR, P1223; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Gidel G., 2018, ARXIV180210551; Gidel Gauthier, 2019, P MACHINE LEARNING R, V89, P1802; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grnarova P., 2017, ARXIV170603269; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Hendrikx Hadrien, 2018, ARXIV181002660; Hensel M, 2017, ADV NEUR IN, V30; Iutzeler F, 2016, IEEE T AUTOMAT CONTR, V61, P892, DOI 10.1109/TAC.2015.2448011; Jakovetic D, 2014, IEEE T AUTOMAT CONTR, V59, P1131, DOI 10.1109/TAC.2014.2298712; Jiang Z, 2017, ADV NEUR IN, V30; Juditsky A., 2011, STOCHASTIC SYST, V1, P17, DOI 10.1287/10-SSY011; Kempe D, 2003, ANN IEEE SYMP FOUND, P482, DOI 10.1109/SFCS.2003.1238221; Kleinberg R, 2018, PR MACH LEARN RES, V80; Koepelevich, 1976, EKONOMIKA MATEMATICH, V12, P747; Koloskova A., 2019, ARXIV190200340; Koloskova Anastasia, 2019, ARXIV190709356; Koppel A, 2015, IEEE T SIGNAL PROCES, V63, P5149, DOI 10.1109/TSP.2015.2449255; Kurach K., 2018, ARXIV180704720; Li J, 2017, P 2017 C EMP METH NA, P2157; Li X., 2019, ARXIV191009126; Lian X., 2018, ICML; Lian XR, 2017, ADV NEUR IN, V30; Liang Tengyuan, 2019, AISTATS; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lin Tianyi, 2019, ARXIV190600331; Liu Mingrui, 2020, INT C LEARN REPR; Liu Weijie, 2019, ARXIV191014380; Lu ST, 2019, INT CONF ACOUST SPEE, P4754, DOI 10.1109/ICASSP.2019.8683795; Mateos-Nunez D, 2015, IEEE DECIS CONTR P, P5462, DOI 10.1109/CDC.2015.7403075; Mazumdar Eric V, 2019, ARXIV190100838; Mokhtari A, 2016, J MACH LEARN RES, V17; Nagarajan Vaishnavh, 2017, ADV NEURAL INFORM PR, P5585; Namkoong Hongseok, 2017, NEURIPS, P2971; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; Patarasuk P, 2009, J PARALLEL DISTR COM, V69, P117, DOI 10.1016/j.jpdc.2008.09.002; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Piliouras, 2018, ARXIV180702629; Rabbat M, 2015, 2015 IEEE 6TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP), P517, DOI 10.1109/CAMSAP.2015.7383850; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Ram SS, 2009, IEEE DECIS CONTR P, P3581, DOI 10.1109/CDC.2009.5399485; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Sanjabi M., 2018, ARXIV181202878; Scaman K, 2017, PR MACH LEARN RES, V70; Schafer Florian, 2019, ADV NEURAL INFORM PR, P7623; Shamir O, 2014, ANN ALLERTON CONF, P850, DOI 10.1109/ALLERTON.2014.7028543; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Singh N., 2019, ARXIV191014280; Sinha Aman, 2017, ARXIV PREPRINT ARXIV; Sirb B, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P76, DOI 10.1109/BigData.2016.7840591; Srivastava Kunal, 2011, 2011 17 INT C DIG SI, P1; Tang Hanlin, 2018, ARXIV180307068; Thekumparampil K.K., 2019, ADV NEURAL PROCESSIN, P12659; Tsianos KI, 2012, IEEE DECIS CONTR P, P5453, DOI 10.1109/cdc.2012.6426375; Wai HT, 2018, ADV NEUR IN, V31; Wang J., 2019, ARXIV190509435; Wang Jianyu, 2018, ARXIV180807576; Wei E, 2012, IEEE DECIS CONTR P, P5445, DOI 10.1109/CDC.2012.6425904; Wu TY, 2018, IEEE T SIGNAL INF PR, V4, P293, DOI 10.1109/TSIPN.2017.2695121; Xiao L, 2004, SYST CONTROL LETT, V53, P65, DOI 10.1016/j.sysconle.2004.02.022; Xin Ran, 2019, ARXIV191004057; Yadav A, 2017, ARXIV PREPRINT ARXIV; Yan F, 2013, IEEE T KNOWL DATA EN, V25, P2483, DOI 10.1109/TKDE.2012.191; Yu Hao, 2019, ICML; Yuan K, 2019, IEEE T SIGNAL PROCES, V67, P351, DOI 10.1109/TSP.2018.2872003; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Zhang Han, 2018, ARXIV180508318; Zhang RL, 2014, PR MACH LEARN RES, V32, P1701; Zhang W., 2017, P 25 ACM INT C INF K; Zhang Wei, 2020, ICASSP 2020; Zhou Yi, 2019, ARXIV190100451; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	98	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													15	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000048
C	Liu, R; Wu, TY; Mozafari, B		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Liu, Rui; Wu, Tianyi; Mozafari, Barzan			Adam with Bandit Sampling for Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				ONLINE	Adam is a widely used optimization method for training deep learning models. It computes individual adaptive learning rates for different parameters. In this paper, we propose a generalization of Adam, called ADAMBS, that allows us to also adapt to different training examples based on their importance in the model's convergence. To achieve this, we maintain a distribution over all examples, selecting a mini-batch in each iteration by sampling according to this distribution, which we update using a multi-armed bandit algorithm. This ensures that examples that are more beneficial to the model training are sampled with higher probabilities. We theoretically show that ADAMBS improves the convergence rate of Adam-O(root log n/T) instead of O(root n/T) in some cases. Experiments on various models and datasets demonstrate ADAMBS's fast convergence in practice.	[Liu, Rui; Wu, Tianyi; Mozafari, Barzan] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Liu, R (corresponding author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.	ruixliu@umich.edu; tianyiwu@umich.edu; mozafari@umich.edu			National Science Foundation [1629397]; Michigan Institute for Data Science (MIDAS) PODS	National Science Foundation(National Science Foundation (NSF)); Michigan Institute for Data Science (MIDAS) PODS	This material is based upon work supported by the National Science Foundation under Grant No. 1629397 and the Michigan Institute for Data Science (MIDAS) PODS. The authors would like to thank Junghwan Kim and Morgan Lovay for their detailed feedback on the manuscript, and anynomous reviewers for their insightful comments.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2015, INT C LEARN REPR ICL; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bengio, 2015, ARXIV151106481; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bordes A, 2005, J MACH LEARN RES, V6, P1579; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Canevet O, 2016, PR MACH LEARN RES, V48; Chollet F, 2018, DEEP LEARNING PYTHON; Connamacher H, 2020, MACH LEARN, V109, P51, DOI 10.1007/s10994-019-05826-x; Csiba D., 2015, J MACH LEARN RES, V37, P674; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hacohen G., 2019, ARXIV190403626; Katharopoulos A., 2018, ARXIV180300942; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lai SW, 2015, AAAI CONF ARTIF INTE, P2267; Le Q. V., 2020, INT C LEARN REPR; Liu R, 2019, AAAI CONF ARTIF INTE, P4376; Liu R, 2017, IEEE DATA MINING, P287, DOI 10.1109/ICDM.2017.38; Lu XF, 2019, ADV MATER, V31, DOI 10.1002/adma.201902339; Namkoong H, 2017, PR MACH LEARN RES, V70; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Reddi Sashank J., 2018, INT C LEARN REPR ICL; Richtarik P, 2016, OPTIM LETT, V10, P1233, DOI 10.1007/s11590-015-0916-1; Rudin C, 2004, J MACH LEARN RES, V5, P1557; Salehi Farnood, 2017, ARXIV171203010; Salehi Farnood, 2018, ADV NEURAL INFORM PR, P9267; Schapire R. E., 2013, EMPIRICAL INFERENCE, P37, DOI DOI 10.1007/978-3-642-41136-6_5; Schapire RE, 2003, LECT NOTES STAT, V171, P149, DOI 10.1007/978-0-387-21579-2_9; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Xiao H., 2017, ARXIV 170807747; Zeiler M.D, 2012, CORR ABS12125701; Zhang Jingzhao, 2019, ARXIV190511881; Zhao PL, 2015, PR MACH LEARN RES, V37, P1; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	38	4	4	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000030
C	Sinclair, SR; Wang, TY; Jain, G; Banerjee, S; Yu, CL		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Sinclair, Sean R.; Wang, Tianyu; Jain, Gauri; Banerjee, Siddhartha; Yu, Christina Lee			Adaptive Discretization for Model-Based Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				GO	We introduce the technique of adaptive discretization to design an efficient model-based episodic reinforcement learning algorithm in large (potentially continuous) state-action spaces. Our algorithm is based on optimistic one-step value iteration extended to maintain an adaptive discretization of the space. From a theoretical perspective we provide worst-case regret bounds for our algorithm which are competitive compared to the state-of-the-art model-based algorithms. Moreover, our bounds are obtained via a modular proof technique which can potentially extend to incorporate additional structure on the problem. From an implementation standpoint, our algorithm has much lower storage and computational requirements due to maintaining a more efficient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs significantly better than fixed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while fixed discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization.	[Sinclair, Sean R.; Jain, Gauri; Banerjee, Siddhartha; Yu, Christina Lee] Cornell Univ, Ithaca, NY 14853 USA; [Wang, Tianyu] Duke Univ, Durham, NC 27706 USA	Cornell University; Duke University	Sinclair, SR (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	srs429@cornell.edu; tianyu@cs.duke.edu; gauri.g.jain@gmail.com; sbanerjee@cornell.edu; cleeyu@cornell.edu		Sinclair, Sean/0000-0002-7011-8253	NSF [ECCS-1847393, DMS-1839346, CCF-1948256, CNS-1955997]; ARL [W911NF-17-1-0094]; Cornell Engaged Grant: Applied Mathematics in Action	NSF(National Science Foundation (NSF)); ARL(United States Department of DefenseUS Army Research Laboratory (ARL)); Cornell Engaged Grant: Applied Mathematics in Action	We are grateful to the referees for their constructive input. Part of this work was done while Sean Sinclair and Christina Yu were visiting the Simons Institute for the Theory of Computing for the semester on the Theory of Reinforcement Learning. We also gratefully acknowledge funding from the NSF under grants ECCS-1847393, DMS-1839346, CCF-1948256, and CNS-1955997, the ARL under grant W911NF-17-1-0094, and the Cornell Engaged Grant: Applied Mathematics in Action.	Alizadeh M, 2013, ACM SIGCOMM COMP COM, V43, P435, DOI 10.1145/2534169.2486031; Alizadeh M, 2010, ACM SIGCOMM COMP COM, V40, P63, DOI 10.1145/1851275.1851192; Ayoub Alex, 2020, ARXIV200601107; Azar MG, 2017, PR MACH LEARN RES, V70; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cao Tongyi, 2020, PROVABLY ADAPTIVE RE; Chinchali S, 2018, AAAI CONF ARTIF INTE, P766; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Domingues Omar Darwiche, 2020, ARXIV200405599; Du SS, 2019, ADV NEUR IN, V32; Efroni Yonathan, 2019, ADV NEURAL INFORM PR, P12203; Henaff M, 2019, ADV NEUR IN, V32; Ipek E, 2008, CONF PROC INT SYMP C, P39, DOI 10.1109/ISCA.2008.21; Jin C., 2019, ARXIV190705388; Kakade Sham, 2003, P 20 INT C MACHINE L, P306; Keller PW, 2006, P 23 INT C MACH LEAR, P449; Kleinberg R, 2019, J ACM, V66, DOI 10.1145/3299873; Kumar P, 2017, 2017 INTERNATIONAL CONFERENCE ON INFOCOM TECHNOLOGIES AND UNMANNED SYSTEMS (TRENDS AND FUTURE DIRECTIONS) (ICTUS), P361; Lakshmanan K, 2015, PR MACH LEARN RES, V37, P524; Lolos K, 2017, PROC INT CONF DATA, P191, DOI 10.1109/ICDE.2017.72; Lykouris T, 2018, ARXIV180205399; Mason W, 2012, P NATL ACAD SCI USA, V109, P764, DOI 10.1073/pnas.1110069108; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2016, PR MACH LEARN RES, V48; Nishtala Rajesh, 2013, 10 USENIX S NETWORKE, P385; Osband I., 2014, ADV NEURAL INFORM PR, V27, P1466; Osband I., 2020, INT C LEARN REPR; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Pyeatt L.D., 2001, P 3 INT S ADAPTIVE S, V2, P70; Russo Daniel, 2013, ADV NEURAL INFORM PR, P2256; Shah D., 2018, ADV NEURAL INFORM PR, P3111; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simchowitz M., 2019, ADV NEURAL INFORM PR, P1153; Sinclair S.R., 2019, PROC ACM MEASUR ANAL, V3, P1; Slivkins A, 2019, FOUND TRENDS MACH LE, V12, P1, DOI 10.1561/2200000068; Song Z., 2019, ARXIV PREPRINT ARXIV; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Wang R., 2020, ADV NEURAL INF PROCE, V33, P6123; Wang Yining, 2019, ARXIV191204136; Whiteson S, 2006, J MACH LEARN RES, V7, P877; Yang Lin F, 2019, ARXIV190501576; Zanette A., 2019, ADV NEURAL INFORM PR, V32, P5616; Zanette Andrea, 2019, ARXIV190100210	50	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													14	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000034
C	Abbasi, E; Salehi, F; Hassibi, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Abbasi, Ehsan; Salehi, Fariborz; Hassibi, Babak			Universality in Learning from Linear Measurements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STABLE SIGNAL RECOVERY; PHASE-TRANSITIONS; CONVEX; GEOMETRY	We study the problem of recovering a structured signal from independently and identically drawn linear measurements. A convex penalty function f(center dot) is considered which penalizes deviations from the desired structure, and signal recovery is performed by minimizing f(center dot) subject to the linear measurement constraints. The main question of interest is to determine the minimum number of measurements that is necessary and sufficient for the perfect recovery of the unknown signal with high probability. Our main result states that, under some mild conditions on f(center dot) and on the distribution from which the linear measurements are drawn, the minimum number of measurements required for perfect recovery depends only on the first and second order statistics of the measurement vectors. As a result, the required of number of measurements can be determining by studying measurement vectors that are Gaussian (and have the same mean vector and covariance matrix) for which a rich literature and comprehensive theory exists. As an application, we show that the minimum number of random quadratic measurements (also known as rank-one projections) required to recover a low rank positive semi-definite matrix is 3nr, where n is the dimension of the matrix and r is its rank. As a consequence, we settle the long standing open question of determining the minimum number of measurements required for perfect signal recovery in phase retrieval using the celebrated PhaseLift algorithm, and show it to be 3n.	[Abbasi, Ehsan; Salehi, Fariborz; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Abbasi, E (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.	eabbasi@caltech.edu; fsalehi@caltech.edu; hassibi@caltech.edu			National Science Foundation [CNS-0932428, CCF1018927, CCF-1423663, CCF-1409204]; Qualcomm Inc.; Futurewei Inc.; NASA's Jet Propulsion Laboratory through the President and Director's Fund; King Abdullah University of Science and Technology	National Science Foundation(National Science Foundation (NSF)); Qualcomm Inc.; Futurewei Inc.; NASA's Jet Propulsion Laboratory through the President and Director's Fund; King Abdullah University of Science and Technology(King Abdullah University of Science & Technology)	This work was supported in part by the National Science Foundation under grants CNS-0932428, CCF1018927, CCF-1423663 and CCF-1409204, by a grant from Qualcomm Inc., by a grant from Futurewei Inc., by NASA's Jet Propulsion Laboratory through the President and Director's Fund, and by King Abdullah University of Science and Technology.	Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005; Ariananda DD, 2012, IEEE T SIGNAL PROCES, V60, P4775, DOI 10.1109/TSP.2012.2201153; Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chandrasekaran V., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1610, DOI 10.1109/ALLERTON.2010.5707106; Donoho D, 2009, PHILOS T R SOC A, V367, P4273, DOI 10.1098/rsta.2009.0152; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; El Karoui N, 2008, ANN STAT, V36, P2717, DOI 10.1214/07-AOS559; GORDON Y, 1988, LECT NOTES MATH, V1317, P84; GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Oymak S, 2018, INF INFERENCE, V7, P337, DOI 10.1093/imaiai/iax011; Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574; Oymak Samet, 2010, ARXIV10116326; Panahi Ashkan, 2017, ADV NEURAL INFORM PR, P3381; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rudelson M, 2006, 2006 40TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-4, P207, DOI 10.1109/CISS.2006.286463; Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687; STOJNIC M, 2009, VARIOUS THRESHOLDS 1; Stojnic Mihailo, 2013, ARXIV13037289; Thrampoulidis C., 2015, PROC C LEARN THEORY, P1683; Thrampoulidis C, 2018, IEEE T INFORM THEORY, V64, P5592, DOI 10.1109/TIT.2018.2840720; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp J.A., 2009, ARXIV09020026; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; White C. D., 2015, ARXIV150607868; Yuxin Chen, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7669, DOI 10.1109/ICASSP.2014.6855092	35	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904008
C	Alaya, MZ; Berar, M; Gasso, G; Rakotomamonjy, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alaya, Mokhtar Z.; Berar, Maxime; Gasso, Gilles; Rakotomamonjy, Alain			Screening Sinkhorn Algorithm for Regularized Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPLEXITY; DISTANCE; MATRICES	We introduce in this paper a novel strategy for efficiently approximating the Sinkhorn distance between two discrete measures. After identifying neglectable components of the dual solution of the regularized Sinkhorn problem, we propose to screen those components by directly setting them at that value before entering the Sinkhorn problem. This allows us to solve a smaller Sinkhorn problem while ensuring approximation with provable guarantees. More formally, the approach is based on a new formulation of dual of Sinkhorn divergence problem and on the KKT optimality conditions of this problem, which enable identification of dual components to be screened. This new analysis leads to the SCREENKHORN algorithm. We illustrate the efficiency of SCREENKHORN on complex tasks such as dimensionality reduction and domain adaptation involving regularized optimal transport.	[Alaya, Mokhtar Z.; Berar, Maxime; Gasso, Gilles; Rakotomamonjy, Alain] Univ Rouen Normandy, LITIS EA4108, Mont St Aignan, France; [Rakotomamonjy, Alain] Criteo Paris, Criteo AI Lab, Paris, France		Alaya, MZ (corresponding author), Univ Rouen Normandy, LITIS EA4108, Mont St Aignan, France.	mokhtarzahdi.alaya@gmail.com; maxime.berar@univ-rouen.fr; gilles.gasso@insa-rouen.fr; alain.rakoto@insa-rouen.fr	Alaya, Mokhtar Z./AAM-9242-2021; GASSO, Gilles/HCI-8911-2022	Alaya, Mokhtar Z./0000-0002-1103-6944; 	Normandie Projet GRR-DAISI; European funding FEDER DAISI; OATMIL Project of the French National Research Agency (ANR) [ANR-17-CE23-0012]	Normandie Projet GRR-DAISI; European funding FEDER DAISI; OATMIL Project of the French National Research Agency (ANR)(French National Research Agency (ANR))	This work was supported by grants from the Normandie Projet GRR-DAISI, European funding FEDER DAISI and OATMIL ANR-17-CE23-0012 Project of the French National Research Agency (ANR).	ABID BK, 2018, MACHINE LEARNING RES, V1505, P1512; Altschuler J., 2018, MASSIVELY SCALABLE S; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Arjovsky M, 2017, PR MACH LEARN RES, V70; Benamou J. D., 2015, SIAM J SCI COMPUTING, V37; Bigot J, 2017, ANN I H POINCARE-PR, V53, P1, DOI 10.1214/15-AIHP706; Blondel M, 2018, PR MACH LEARN RES, V84; Bonneel N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024192; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600; Dvurechensky P, 2018, PR MACH LEARN RES, V80; Ebert J., 2017, CONSTRUCTION NONASYM; El Ghaoui L., 2010, CORR; Flamary R<prime>emi, 2017, POT PYTHON OPTIMAL T; Flamary R, 2018, MACH LEARN, V107, P1923, DOI 10.1007/s10994-018-5717-1; Frogner Charlie, 2015, ADV NEURAL INF PROCE, V2, P2053; Ganin Y., 2016, JMLR, V17, P2096; Kalantari B, 2008, MATH PROGRAM, V112, P371, DOI 10.1007/s10107-006-0021-4; Kalantari B, 1996, LINEAR ALGEBRA APPL, V240, P87, DOI 10.1016/0024-3795(94)00188-X; Kantorovich L., 1942, DOKL AKAD NAUK+, V37, P227; Knight PA, 2008, SIAM J MATRIX ANAL A, V30, P261, DOI 10.1137/060659624; Kolouri S, 2017, IEEE SIGNAL PROC MAG, V34, P43, DOI 10.1109/MSP.2017.2695801; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Lee YT, 2014, ANN IEEE SYMP FOUND, P424, DOI 10.1109/FOCS.2014.52; Lin T., 2019, CORR; Ho N, 2017, PR MACH LEARN RES, V70; NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.1090/S0025-5718-1980-0572855-7; Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; SINKHORN R, 1967, AM MATH MON, V74, P402, DOI 10.2307/2314570; Solomon J, 2014, PR MACH LEARN RES, V32; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; WERMAN M, 1985, COMPUT VISION GRAPH, V32, P328, DOI 10.1016/0734-189X(85)90055-6; Xie Yujia, 2018, FAST PROXIMAL POINT	39	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903077
C	Aljundi, R; Caccia, L; Belilovsky, E; Caccia, M; Lin, M; Charlin, L; Tuytelaars, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aljundi, Rahaf; Caccia, Lucas; Belilovsky, Eugene; Caccia, Massimo; Lin, Min; Charlin, Laurent; Tuytelaars, Tinne			Online Continual Learning with Maximally Interfered Retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or "single-pass through the data" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https ://github.com/optimass/Maximally-Interfered_Retrieval.	[Aljundi, Rahaf; Tuytelaars, Tinne] Katholieke Univ Leuven, Leuven, Belgium; [Caccia, Lucas; Belilovsky, Eugene; Caccia, Massimo; Lin, Min; Charlin, Laurent] Mila, Montreal, PQ, Canada	KU Leuven	Aljundi, R (corresponding author), Katholieke Univ Leuven, Leuven, Belgium.	rahaf.aljundi@gmail.com; lucas.page-caccia@mail.mcgill.ca; eugene.belilovsky@umontreal.ca; massimo.p.caccia@gmail.com; mavenlin@gmail.com; lcharlin@gmail.com; tinne.tuytelaars@esat.kuleuven.be	Tuytelaars, Tinne/B-4319-2015	Tuytelaars, Tinne/0000-0003-3307-9723	FWO	FWO(FWO)	We would like to thank Kyle Kastner and Puneet Dokania for helpful discussion. Eugene Belilvosky is funded by IVADO and Rahaf Aijundi is funded by FWO.	Aijundi Rahaf, CVPR 2019; Aijundi Rahaf, 2016, IEEE C COMP VIS PATT; Ajundi Rahaf, ECCV 2018; Aljundi R., 2019, ARXIV190308671; Aljundi Rahaf, ONLINE CONTINUAL LEA; [Anonymous], 1994, NETWORK; Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33; Chaudhry Arslan, 2019, CONTINUAL LEARNING T; Chaudhry Arslan, 2019, P INT C LEARN REPR I, P2; Farquhar Sebastian, 2018, ARXIV180509733; French R. M., 1992, Connection Science, V4, P365, DOI 10.1080/09540099208946624; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Grossberg Stephen, 1982, BOSTON STUDIES PHILO, V70; Honey CJ, 2017, NETW NEUROSCI, V1, P339, DOI [10.1162/NETN_a_00024, 10.1162/netn_a_00024]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kruschke J. K., 1993, Connection Science, V5, P3, DOI 10.1080/09540099308915683; KRUSCHKE JK, 1992, PSYCHOL REV, V99, P22, DOI 10.1037/0033-295X.99.1.22; Kuhl BA, 2018, 3265727 SSRN; Lavda Frantzeska, 2018, CONTINUAL CLASSIFICA; Lesort Timothee, 2018, ARXIV PREPRINT ARXIV; Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37; Lopez-Paz D, 2017, ADV NEUR IN, V30; McClelland JL, 1998, ANN NY ACAD SCI, V843, P153, DOI 10.1111/j.1749-6632.1998.tb08212.x; Nguyen C. V., 2017, ARXIV171010628; Ramapuram Jason, 2017, ARXIV170509847; Rebuffi Sylvestre-Alvise, 2017, PROC CVPR IEEE, P8, DOI DOI 10.1109/CVPR.2017.587; Riemer M, 2019, AAAI CONF ARTIF INTE, P1352; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; Rolnick D., 2018, ABS181111682 CORR; Rolnick David, 2018, CORR; Rusu A. A., 2016, ARXIV160604671; Shin H, 2017, ADV NEUR IN, V30; Sloman Steven A, 1992, ESSAYS HONOR WK ESTE, V1, P227; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Xu Ju, 2018, ARXIV180512369; Zenke F., 2017, P INT C MACH LEARN I; Zenke F, 2017, PR MACH LEARN RES, V70	40	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903048
C	Amani, S; Alizadeh, M; Thrampoulidis, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Amani, Sanae; Alizadeh, Mahnoosh; Thrampoulidis, Christos			Linear Stochastic Bandits Under Safety Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bandit algorithms have various application in safety-critical systems, where it is important to respect the system constraints that rely on the bandit's unknown parameters at every round. In this paper, we formulate a linear stochastic multiarmed bandit problem with safety constraints that depend (linearly) on an unknown parameter vector. As such, the learner is unable to identify all safe actions and must act conservatively in ensuring that her actions satisfy the safety constraint at all rounds (at least with high probability). For these bandits, we propose a new UCB-based algorithm called Safe-LUCB, which includes necessary modifications to respect safety constraints. The algorithm has two phases. During the pure exploration phase the learner chooses her actions at random from a restricted set of safe actions with the goal of learning a good approximation of the entire unknown safe set. Once this goal is achieved, the algorithm begins a safe exploration-exploitation phase where the learner gradually expands their estimate of the set of safe actions while controlling the growth of regret. We provide a general regret bound for the algorithm, as well as a problem dependent bound that is connected to the location of the optimal action within the safe set. We then propose a modified heuristic that exploits our problem dependent analysis to improve the regret.	[Amani, Sanae; Alizadeh, Mahnoosh; Thrampoulidis, Christos] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA	University of California System; University of California Santa Barbara	Amani, S (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.	samanigeshnigani@ucsb.edu; alizadeh@ucsb.edu; cthrampo@ucsb.edu	Alizadeh, Mahnoosh/AAZ-7159-2020	Alizadeh, Mahnoosh/0000-0003-3369-3846	UCOP grant [LFR-18-548175]; NSF [1847096]	UCOP grant; NSF(National Science Foundation (NSF))	This research is supported by UCOP grant LFR-18-548175 and NSF grant 1847096.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Achiam J, 2017, PR MACH LEARN RES, V70; Agrawal S., 2016, ADV NEURAL INFORM PR, V29, P3450; Akametalu AK, 2014, IEEE DECIS CONTR P, P1424, DOI 10.1109/CDC.2014.7039601; Aswani A, 2013, AUTOMATICA, V49, P1216, DOI 10.1016/j.automatica.2013.02.003; Audibert JY, 2009, THEOR COMPUT SCI, V410, P1876, DOI 10.1016/j.tcs.2009.01.016; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Badanidiyuru A, 2014, P C LEARN THEOR, V35, P1109; Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30; Berkenkamp F., 2016, ARXIV PREPRINT ARXIV; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Dani V., 2008, STOCHASTIC LINEAR OP; Filippi S., 2010, NIPS, P586; Gillula JH, 2011, IEEE INT C INT ROBOT, P2979, DOI 10.1109/IROS.2011.6048864; Kazerouni A., 2017, ADV NEURAL INFORM PR; Li LH, 2017, PR MACH LEARN RES, V70; Moldovan T. M., 2012, ARXIV12054810; Ostafew CJ, 2016, INT J ROBOT RES, V35, P1547, DOI 10.1177/0278364916645661; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Schreiter J, 2015, LECT NOTES ARTIF INT, V9286, P133, DOI 10.1007/978-3-319-23461-8_9; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sui YA, 2018, PR MACH LEARN RES, V80; Sui YA, 2015, PR MACH LEARN RES, V37, P997; Usmanova I., 2019, 22 INT C ARTIFICIAL, P2106; Wu HS, 2015, ADV NEUR IN, V28; Wu YF, 2016, PR MACH LEARN RES, V48	27	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900080
C	Backurs, A; Indyk, P; Wagner, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Backurs, Arturs; Indyk, Piotr; Wagner, Tal			Space and Time Efficient Kernel Density Estimation in High Dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently, Charikar and Siminelakis (2017) presented a framework for kernel density estimation in provably sublinear query time, for kernels that possess a certain hashing-based property. However, their data structure requires a significantly increased super-linear storage space, as well as super-linear preprocessing time. These limitations inhibit the practical applicability of their approach on large datasets. In this work, we present an improvement to their framework that retains the same query time, while requiring only linear space and linear preprocessing time. We instantiate our framework with the Laplacian and Exponential kernels, two popular kernels which possess the aforementioned property. Our experiments on various datasets verify that our approach attains accuracy and query time similar to Charikar and Siminelakis (2017), with significantly improved space and preprocessing time.	[Backurs, Arturs] TTIC, Chicago, IL 60637 USA; [Indyk, Piotr; Wagner, Tal] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Backurs, A (corresponding author), TTIC, Chicago, IL 60637 USA.	backurs@ttic.edu; indyk@mit.edu; talw@mit.edu			NSF TRIPODS award [1740751]; Simons Investigator Award	NSF TRIPODS award; Simons Investigator Award	We thank the anonymous reviewers for useful suggestions. Piotr Indyk was supported by NSF TRIPODS award #1740751 and Simons Investigator Award.	Andoni A, 2006, ANN IEEE SYMP FOUND, P459; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Charikar M., 2017, HASHING BASED ESTIMA; Chen Beidi, LSH SAMPLING BREAKS; Chen Yutian, 2012, SUPER SAMPLES KERNEL; Chierichetti F, 2015, J ACM, V62, DOI 10.1145/2816813; Datar M., 2004, P ACM 20 ANN S COMP, P253; Gan E, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P945, DOI 10.1145/3035918.3064035; Gray AG, 2001, ADV NEUR IN, V13, P521; GREENGARD L, 1991, SIAM J SCI STAT COMP, V12, P79, DOI 10.1137/0912004; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jaakkola T, 1999, Proc Int Conf Intell Syst Mol Biol, P149; LeCun Yann, 2018, PREPRINT; Luo C, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1439; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Phillips JM, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1622; Phillips Jeff M, 2018, ARXIV180201751; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Siminelakis Paris, 2019, INT C MACH LEARN; Wu Xian, 2018, ARXIV180907471; Zheng Y., 2013, P 2013 ACM SIGMOD IN, P433, DOI DOI 10.1145/2463676.2465319	23	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907045
C	Baharav, TZ; Tse, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Baharav, Tavor Z.; Tse, David			Ultra Fast Medoid Identification via Correlated Sequential Halving	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The medoid of a set of n points is the point in the set that minimizes the sum of distances to other points. It can be determined exactly in O(n(2)) time by computing the distances between all pairs of points. Previous works show that one can significantly reduce the number of distance computations needed by adaptively querying distances [1]. The resulting randomized algorithm is obtained by a direct conversion of the computation problem to a multi-armed bandit statistical inference problem. In this work, we show that we can better exploit the structure of the underlying computation problem by modifying the traditional bandit sampling strategy and using it in conjunction with a suitably chosen multi-armed bandit algorithm. Four to five orders of magnitude gains over exact computation are obtained on real data, in terms of both number of distance computations needed and wall clock time. Theoretical results are obtained to quantify such gains in terms of data parameters. Our code is publicly available online at https://github.com/TavorB/Correlated-Sequential-Halving.	[Baharav, Tavor Z.; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Stanford University	Baharav, TZ (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	tavorb@stanford.edu; dntse@stanford.edu	Baharav, Tavor/AAU-7553-2020	Baharav, Tavor/0000-0001-8924-0243	NSF GRFP; Alcatel-Lucent Stanford Graduate Fellowship; NSF [CCF-1563098]; Center for Science of Information (CSoI), an NSF Science and Technology Center [CCF-0939370]	NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); Alcatel-Lucent Stanford Graduate Fellowship; NSF(National Science Foundation (NSF)); Center for Science of Information (CSoI), an NSF Science and Technology Center	The authors gratefully acknowledge funding from the NSF GRFP, Alcatel-Lucent Stanford Graduate Fellowship, NSF grant under CCF-1563098, and the Center for Science of Information (CSoI), an NSF Science and Technology Center under grant agreement CCF-0939370.	Bagaria V., 2018, ARXIV180508321; Bagaria V, 2018, PR MACH LEARN RES, V84; Bennett James, 2007, P KDD CUP WORKSH, V2007, P35; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Kaufmann E, 2016, J MACH LEARN RES, V17; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; LeJeune D, 2019, PR MACH LEARN RES, V89; Leskovec J, 2014, MINING OF MASSIVE DATASETS, 2ND EDITION, P1; Li L, 2016, ARXIV160306560; Newling J., 2016, ARXIV160506950; Ntranos V, 2016, GENOME BIOL, V17, DOI 10.1186/s13059-016-0970-8; Okamoto K, 2008, LECT NOTES COMPUT SC, V5059, P186; Rousseeuw L. K. P. J., 1987, CLUSTERING MEANS MED; Wang D. E. J., 2006, GRAPH ALGORITHMS APP, V5, P39; Yann L., 1998, MNIST DATABASE HANDW, P1; Zhang Martin J, 2019, ARXIV190200197	17	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303062
C	Bernstein, G; Sheldon, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bernstein, Garrett; Sheldon, Daniel			Differentially Private Bayesian Linear Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Linear regression is an important tool across many fields that work with sensitive human-sourced data. Significant prior work has focused on producing differentially private point estimates, which provide a privacy guarantee to individuals while still allowing modelers to draw insights from data by estimating regression coefficients. We investigate the problem of Bayesian linear regression, with the goal of computing posterior distributions that correctly quantify uncertainty given privately released statistics. We show that a naive approach that ignores the noise injected by the privacy mechanism does a poor job in realistic data settings. We then develop noise-aware methods that perform inference over the privacy mechanism and produce correct posteriors across a wide range of scenarios.	[Bernstein, Garrett; Sheldon, Daniel] Univ Massachusetts, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Bernstein, G (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.	gbernstein@cs.umass.edu; sheldon@cs.umass.edu						Agresti A., 2009, STAT METHODS SOCIAL; Awan Jordan, 2018, ARXIV180109236; Barry AE, 2021, J AM COLL HEALTH, V69, P335, DOI 10.1080/07448481.2019.1676249; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Bernstein G, 2017, PR MACH LEARN RES, V70; Bernstein Garrett, 2018, ADV NEURAL INFORM PR, P2919; Bickel P.J., 2015, MATH STAT BASIC IDEA, VI; Bickel PJ, 2015, MATH STAT BASIC IDEA, VI (Vol 117); Casella George, 2002, STAT INFERENCE; Cook SR, 2006, J COMPUT GRAPH STAT, V15, P675, DOI 10.1198/106186006X136976; Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21; Dwork C., 2014, FDN TRENDS THEORETIC; Dwork C, 2010, J PRIVACY CONFIDENT, V1; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Foulds J., 2016, P 32 C UNC ART INT U, P192; Geumlek Joseph, 2015, ADV NEURAL INFORM PR, P5295; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Honkela A, 2018, BIOL DIRECT, V13, DOI 10.1186/s13062-017-0203-4; Jalko J, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Karwa V, 2016, ANN STAT, V44, P87, DOI 10.1214/15-AOS1358; Karwa V, 2014, LECT NOTES COMPUT SC, V8744, P143, DOI 10.1007/978-3-319-11257-2_12; Kifer D., 2011, P 2011 ACM SIGMOD IN, P193, DOI DOI 10.1145/1989323.1989345; Kifer D., 2012, J MACHINE LEARNING R, V1, P3; MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095; McSherry F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P627; Minami K., 2016, ADV NEURAL INFORM PR, P956; OHAGAN A, 1994, KENDALLS ADV THEORY, V2; Park James, 2016, ARXIV161100340; Park T, 2008, J AM STAT ASSOC, V103, P681, DOI 10.1198/016214508000000337; Rencher A.C., 2003, METHODS MULTIVARIATE, V492; Salvatier J, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.55; Schein Aaron, 2017, NIPS 2017 WORKSH ADV; Sheffet Or, 2017, P 34 INT C MACH LEAR; Smith Adam, 2008, ARXIV08094794; Vu D, 2009, INT CONF DAT MIN WOR, P138, DOI 10.1109/ICDMW.2009.52; Wang L, 2015, I NAVIG SAT DIV INT, P2493; Wang Yu-Xiang, 2018, C UNC ART INT UAI; Williams O., 2010, P 23 INT C NEURAL IN, P2451; Zhang J, 2012, PROC VLDB ENDOW, V5, P1364, DOI 10.14778/2350229.2350253; Zhang Z., 2016, 30 AAAI C ART INT	41	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300048
C	Bhat, SP; Prashanth, LA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bhat, Sanjay P.; Prashanth, L. A.			Concentration of risk measures: A Wasserstein distance approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PROSPECT-THEORY; REPRESENTATION; BOUNDS	Known finite-sample concentration bounds for the Wasserstein distance between the empirical and true distribution of a random variable are used to derive a two-sided concentration bound for the error between the true conditional value-at-risk (CVaR) of a (possibly unbounded) random variable and a standard estimate of its CVaR computed from an i.i.d. sample. The bound applies under fairly general assumptions on the random variable, and improves upon previous bounds which were either one sided, or applied only to bounded random variables. Specializations of the bound to sub-Gaussian and sub-exponential random variables are also derived. Using a different proof technique, the results are extended to the class of spectral risk measures having a bounded risk spectrum. A similar procedure is followed to derive concentration bounds for the error between the true and estimated Cumulative Prospect Theory (CPT) value of a random variable, in cases where the random variable is bounded or sub-Gaussian. These bounds are shown to match a known bound in the bounded case, and improve upon the known bound in the sub-Gaussian case. The usefulness of the bounds is illustrated through an algorithm, and corresponding regret bound for a stochastic bandit problem, where the underlying risk measure to be optimized is CVaR.	[Bhat, Sanjay P.] Tata Consultancy Serv Ltd, Hyderabad, Telangana, India; [Prashanth, L. A.] Indian Inst Technol Madras, Dept Comp Sci & Engn, Madras, Tamil Nadu, India	Tata Sons; Tata Consultancy Services Limited (TCS); Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Bhat, SP (corresponding author), Tata Consultancy Serv Ltd, Hyderabad, Telangana, India.	sanjay.bhat@tcs.com; prashla@cse.iitm.ac.in			DST grant under the ECRA program	DST grant under the ECRA program	Supported in part by a DST grant under the ECRA program.	Acerbi C, 2002, J BANK FINANC, V26, P1487, DOI 10.1016/S0378-4266(02)00283-2; Allais M, 1953, ECONOMETRICA, V21, P503, DOI 10.2307/1907921; Apostol T. M., 1974, MATH ANAL; Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barberis NC, 2013, J ECON PERSPECT, V27, P173, DOI 10.1257/jep.27.1.173; Brown DB, 2007, OPER RES LETT, V35, P722, DOI 10.1016/j.orl.2007.01.001; Dowd K, 2006, J RISK INSUR, V73, P193, DOI 10.1111/j.1539-6975.2006.00171.x; ELLSBERG D, 1961, Q J ECON, V75, P643, DOI 10.2307/1884324; Galichet N., 2013, AS C MACH LEARN, P245; GIVENS CR, 1984, MICH MATH J, V31, P231; KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185; Kolla R.K., 2019, ARXIV PREPRINT ARXIV; Kolla RK, 2019, OPER RES LETT, V47, P16, DOI 10.1016/j.orl.2018.11.005; Prashanth LA, 2016, PR MACH LEARN RES, V48; Prashanth L. A., 2019, CONCENTRATION BOUNDS; Rockafellar R., 2000, J RISK, V2, P21, DOI 10.21314/JOR.2000.038; TVERSKY A, 1992, J RISK UNCERTAINTY, V5, P297, DOI 10.1007/BF00122574; Vallender SS, 1974, THEOR PROBAB APPL, V18, P784, DOI DOI 10.1137/1118101; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wainwright M.J, 2019, CAMBRIDGE SER STATIS, DOI [10.1017/9781108627771, DOI 10.1017/9781108627771]; Wang Y, 2010, OPER RES LETT, V38, P236, DOI 10.1016/j.orl.2009.11.008; Wasserman L. A, 2015, ALL NONPARAMETRIC ST	27	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903038
C	Biswas, N; Jacob, PE; Vanetti, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Biswas, Niloy; Jacob, Pierre E.; Vanetti, Paul			Estimating Convergence of Markov chains with L-Lag Couplings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEQUENTIAL MONTE-CARLO; RATES	Markov chain Monte Carlo (MCMC) methods generate samples that are asymptotically distributed from a target distribution of interest as the number of iterations goes to infinity. Various theoretical results provide upper bounds on the distance between the target and marginal distribution after a fixed number of iterations. These upper bounds are on a case by case basis and typically involve intractable quantities, which limits their use for practitioners. We introduce L-lag couplings to generate computable, non-asymptotic upper bound estimates for the total variation or the Wasserstein distance of general Markov chains. We apply L-lag couplings to the tasks of (i) determining MCMC burn-in, (ii) comparing different MCMC algorithms with the same target, and (iii) comparing exact and approximate MCMC. Lastly, we (iv) assess the bias of sequential Monte Carlo and self-normalized importance samplers.	[Biswas, Niloy; Jacob, Pierre E.] Harvard Univ, Cambridge, MA 02138 USA; [Vanetti, Paul] Univ Oxford, Oxford, England	Harvard University; University of Oxford	Biswas, N (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	niloy_biswas@g.harvard.edu; pjacob@fas.harvard.edu; paul.vanetti@spc.ox.ac.uk			National Science Foundation [DMS-1712872, DMS-1844695]	National Science Foundation(National Science Foundation (NSF))	The authors are grateful to Espen Bernton, Nicolas Chopin, Andrew Gelman, Lester Mackey, John O'Leary, Christian Robert, Jeffrey Rosenthal, James Scott, Aki Vehtari and reviewers for helpful comments on an earlier version of the manuscript. The second author gratefully acknowledges support by the National Science Foundation through awards DMS-1712872 and DMS-1844695. The figures were created with packages [58, 57] in R Core Team [45].	Agapiou S, 2017, STAT SCI, V32, P405, DOI 10.1214/17-STS611; Andrieu C, 2018, BERNOULLI, V24, P842, DOI 10.3150/15-BEJ785; Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Berard J., 2014, ELECT J PROBABILITY, V19; Bou-Rabee N., 2018, ARXIV180500452; Brooks SP, 1998, STAT COMPUT, V8, P319, DOI 10.1023/A:1008820505350; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Chatterjee S, 2018, ANN APPL PROBAB, V28, P1099, DOI 10.1214/17-AAP1326; Chopin N, 2015, BERNOULLI, V21, P1855, DOI 10.3150/14-BEJ629; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Collevecchio A, 2018, J STAT PHYS, V170, P22, DOI 10.1007/s10955-017-1912-x; Cowles MK, 1998, STAT COMPUT, V8, P115, DOI 10.1023/A:1008982016666; Dalalyan A., 2019, STOCHASTIC PROCESSES; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Del Moral P., 2004, PROB APPL S; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Durmus A, 2016, ANN I H POINCARE-PR, V52, P1799, DOI 10.1214/15-AIHP699; Dwivedi R., 2018, C LEARNING THEORY PM, P793; Gelman A, 1992, STAT SCI; Gelman Andrew, 1998, J COMPUTATIONAL GRAP; Geweke John, 1998, BAYESIAN STAT; Geyer Charles, 1991, TECHNICAL REPORT; Glynn PW, 2014, J APPL PROBAB, V51, P377, DOI 10.1017/S0021900200021392; Gorham J, 2015, ADV NEUR IN, V28; Gorham Jackson, 2018, ARXIV161106972V6; Heng J, 2019, BIOMETRIKA, V106, P287, DOI 10.1093/biomet/asy074; Huggins JH, 2019, BERNOULLI, V25, P584, DOI 10.3150/17-BEJ999; Jacob P.E., 2019, J AM STAT ASS; Jacob Pierre E., 2019, J ROYAL STAT SOC B; Johndrow James E., 2018, ARXIV170500841V3; Johnson VE, 1998, J AM STAT ASSOC, V93, P238, DOI 10.2307/2669620; Johnson VE, 1996, J AM STAT ASSOC, V91, P154, DOI 10.2307/2291391; Lee A, 2018, BIOMETRIKA, V105, P609, DOI 10.1093/biomet/asy028; Liu Q, 2016, PR MACH LEARN RES, V48; Mangoubi O., 2017, ARXIV170807114; Middleton Lawrence, 2018, ARXIV180708691; Middleton Lawrence, 2019, P 22 INT C ART INT S, V89, P2378; Mossel E, 2013, ANN PROBAB, V41, P294, DOI 10.1214/11-AOP737; Neal R.M., 1993, ADV NEURAL INFORM PR, P475; Owen A. B, 2019, MONTE CARLO THEORY M; Peyre Gabriel, 2019, ARXIV180300567V3; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Propp JG, 1996, RANDOM STRUCT ALGOR, V9, P223, DOI 10.1002/(SICI)1098-2418(199608/09)9:1/2<223::AID-RSA14>3.0.CO;2-O; R Development Core Team, 2018, R LANG ENV STAT COMP; Robert C., 2013, MONTE CARLO STAT MET; Roberts GO, 2001, STAT SCI, V16, P351, DOI 10.1214/ss/1015346320; Rosenthal JS, 1996, STAT COMPUT, V6, P269, DOI 10.1007/BF00140871; Rosenthal JS., 2004, PROBAB SURV, V1, P20, DOI [10.1214/154957804100000024, DOI 10.1214/154957804100000024]; Rudolf D, 2018, BERNOULLI, V24, P2610, DOI 10.3150/17-BEJ938; Salvatier J, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.55; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Syed S., 2019, ARXIV190502939; THORISSON H, 1986, ANN PROBAB, V14, P873, DOI 10.1214/aop/1176992443; Titsias MK, 2017, J AM STAT ASSOC, V112, P1598, DOI 10.1080/01621459.2016.1222288; Vats D, 2019, BIOMETRIKA, V106, P321, DOI 10.1093/biomet/asz002; Wickham H, 2009, USE R, P1, DOI 10.1007/978-0-387-98141-3_1; Wilke Claus O., 2017, GGRIDGES RIDGELINE P, P1; Woodard DB, 2009, ANN APPL PROBAB, V19, P617, DOI 10.1214/08-AAP555; Zanella G, 2020, J AM STAT ASSOC, V115, P852, DOI 10.1080/01621459.2019.1585255	59	4	4	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307041
C	Brando, A; Rodriguez-Serrano, JA; Vitria, J; Rubio, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Brando, Axel; Rodriguez-Serrano, Jose A.; Vitria, Jordi; Rubio, Alberto			Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In regression tasks, aleatoric uncertainty is commonly addressed by considering a parametric distribution of the output variable, which is based on strong assumptions such as symmetry, unimodality or by supposing a restricted shape. These assumptions are too limited in scenarios where complex shapes, strong skews or multiple modes are present. In this paper, we propose a generic deep learning framework that learns an Uncountable Mixture of Asymmetric Laplacians (UMAL), which will allow us to estimate heterogeneous distributions of the output variable and we show its connections to quantile regression. Despite having a fixed number of parameters, the model can be interpreted as an infinite mixture of components, which yields a flexible approximation for heterogeneous distributions. Apart from synthetic cases, we apply this model to room price forecasting and to predict financial operations in personal bank accounts. We demonstrate that UMAL produces proper distributions, which allows us to extract richer insights and to sharpen decision-making.	[Brando, Axel; Rodriguez-Serrano, Jose A.; Rubio, Alberto] BBVA Data & Analyt, Barcelona, Spain; [Brando, Axel; Vitria, Jordi] Univ Barcelona, Barcelona, Spain	University of Barcelona	Brando, A (corresponding author), BBVA Data & Analyt, Barcelona, Spain.; Brando, A (corresponding author), Univ Barcelona, Barcelona, Spain.	axel.brando@bbvadata.com; joseantonio.rodriguez.serrano@bbvadata.com; jordi.vitria@ub.edu		Brando, Axel/0000-0001-8103-391X	Government of Catalonia's Industrial Doctorates Plan [RTI2018-095232-B-C21, SGR 1219]; BBVA Data and Analytics	Government of Catalonia's Industrial Doctorates Plan; BBVA Data and Analytics	We gratefully acknowledge the Government of Catalonia's Industrial Doctorates Plan for funding part of this research. The UB acknowledges that part of the research described in this chapter was partially funded by RTI2018-095232-B-C21 and SGR 1219. We would also like to thank BBVA Data and Analytics for sponsoring the industrial PhD.	Abadi M, 2015, P 12 USENIX S OPERAT; Belagiannis V, 2015, IEEE I CONF COMP VIS, P2830, DOI 10.1109/ICCV.2015.324; Bishop C.M., 1994, NCRG4288; Bishop CM, 2006, PATTERN RECOGNITION; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chamberlain G, 1994, ADV EC 6 WORLD C, V2, P171, DOI DOI 10.1017/CCOL0521444594.005; Chollet Francois, 2015, KERAS 2015; Cinar YG, 2018, NEUROCOMPUTING, V312, P177, DOI 10.1016/j.neucom.2018.05.090; Cox N, 2020, INSIDE AIRBNB ADDING; Dabney W, 2018, AAAI CONF ARTIF INTE, P2892; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Fred Ana, 2016, 2 INT C PERS TECHN, V10163; Gal Y, 2016, PR MACH LEARN RES, V48; GUTENBRUNNER C, 1992, ANN STAT, V20, P305, DOI 10.1214/aos/1176348524; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Khan Mohammad E, 2010, ADV NEURAL INFORM PR, P1108; Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020; Koenker R., 2018, HDB QUANTILE REGRESS, DOI [10.1201/9781315120256, DOI 10.1201/9781315120256]; Kostantinos N, 2000, ADV SIGNAL PROCESSIN, P3; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Nielsen F, 2016, IEEE SIGNAL PROC LET, V23, P1543, DOI 10.1109/LSP.2016.2606661; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; Rasmussen CE, 1996, ADV NEUR IN, V8, P598; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tagasovska N., 2018, BAYES DEEP LEARN WOR; Teye M., 2018, P MACHINE LEARNING R, P4914; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01; Wen R., 2017, TIME SERIES WORKSHOP; Yu KM, 2001, STAT PROBABIL LETT, V54, P437, DOI 10.1016/S0167-7152(01)00124-9; Zheng HY, 2015, 2015 IEEE TRUSTCOM/BIGDATASE/ISPA, VOL 1, P1, DOI 10.1109/Trustcom.2015.350	37	4	4	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900043
C	Cardona, JL; Howland, MF; Dabiri, JO		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cardona, Jennifer L.; Howland, Michael F.; Dabiri, John O.			Seeing the Wind: Visual Wind Speed Prediction with a Coupled Convolutional and Recurrent Neural Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ENERGY	Wind energy resource quantification, air pollution monitoring, and weather forecasting all rely on rapid, accurate measurement of local wind conditions. Visual observations of the effects of wind-the swaying of trees and flapping of flags, for example-encode information regarding local wind conditions that can potentially be leveraged for visual anemometry that is inexpensive and ubiquitous. Here, we demonstrate a coupled convolutional neural network and recurrent neural network architecture that extracts the wind speed encoded in visually recorded flow-structure interactions of a flag and tree in naturally occurring wind. Predictions for wind speeds ranging from 0.75-11 m/s showed agreement with measurements from a cup anemometer on site, with a root-mean-squared error approaching the natural wind speed variability due to atmospheric turbulence. Generalizability of the network was demonstrated by successful prediction of wind speed based on recordings of other flags in the field and in a controlled wind tunnel test. Furthermore, physicsbased scaling of the flapping dynamics accurately predicts the dependence of the network performance on the video frame rate and duration.	[Cardona, Jennifer L.; Howland, Michael F.] Stanford Univ, Dept Mech Engn, Stanford, CA 94305 USA; [Dabiri, John O.] CALTECH, Grad Aerosp Labs GALCIT, Pasadena, CA 91125 USA; [Dabiri, John O.] CALTECH, Mech Engn, Pasadena, CA 91125 USA	Stanford University; California Institute of Technology; California Institute of Technology	Cardona, JL (corresponding author), Stanford Univ, Dept Mech Engn, Stanford, CA 94305 USA.	94305jcard27@stanford.edu; mhowland@stanford.edu; jodabiri@caltech.edu		Howland, Michael/0000-0002-2878-3874	Brit and Alex d'Arbeloff Stanford Graduate Fellowship; National Science Foundation [DGE-1656518]; Stanford Graduate Fellowship	Brit and Alex d'Arbeloff Stanford Graduate Fellowship; National Science Foundation(National Science Foundation (NSF)); Stanford Graduate Fellowship(Stanford University)	The authors acknowledge Kelyn Wood, who assisted in setup for the wind tunnel test set. J.L.C. is funded through the Brit and Alex d'Arbeloff Stanford Graduate Fellowship, and M.F.H. is funded through a National Science Foundation Graduate Research Fellowship under Grant DGE-1656518 and a Stanford Graduate Fellowship.	Alben Silas, 2008, PHYS REV LETT, V100; Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4; Barbounis TG, 2006, IEEE T ENERGY CONVER, V21, P273, DOI 10.1109/TEC.2005.847954; Bhaskar K, 2012, IEEE T SUSTAIN ENERG, V3, P306, DOI 10.1109/TSTE.2011.2182215; Bhat K. S., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P37; Bouman KL, 2013, IEEE I CONF COMP VIS, P1984, DOI 10.1109/ICCV.2013.455; Cadenas E, 2010, RENEW ENERG, V35, P2732, DOI 10.1016/j.renene.2010.04.022; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Castellani F, 2014, ENRGY PROCED, V45, P188, DOI 10.1016/j.egypro.2014.01.021; Dai A, 1999, J GEOPHYS RES-ATMOS, V104, P31109, DOI 10.1029/1999JD900927; Davis A, 2015, PROC CVPR IEEE, P5335, DOI 10.1109/CVPR.2015.7299171; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Guo ZH, 2012, RENEW ENERG, V37, P241, DOI 10.1016/j.renene.2011.06.023; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kammen DM, 2016, SCIENCE, V352, P922, DOI 10.1126/science.aad9302; Karpathy A., 2015, ARXIV150602078; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Li G, 2010, APPL ENERG, V87, P2313, DOI 10.1016/j.apenergy.2009.12.013; Liu H, 2018, ENERG CONVERS MANAGE, V159, P54, DOI 10.1016/j.enconman.2018.01.010; Meka A, 2018, PROC CVPR IEEE, P6315, DOI 10.1109/CVPR.2018.00661; Mohandes MA, 1998, RENEW ENERG, V13, P345, DOI 10.1016/S0960-1481(98)00001-9; Mottaghi R, 2016, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2016.383; Rumelhart D. E., 1985, ICS8506 CAL U SAN DI; Runia Tom F. H., 2019, ARXIV191007861V1; Sakamoto H, 2008, CEREBELLUM, V7, P18, DOI 10.1007/s12311-008-0007-2; Shi XS, 2015, 2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P802, DOI 10.1109/IAEAC.2015.7428667; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Spencer L, 2004, IEEE IMAGE PROC, P2705; Stolaroff J.K., 2018, NAT COMMUN, V409, P9; The MathWorks Inc, 2019, DEEP LEARNING TOOLBO; The MathWorks Inc, 2019, RESNET18; WADE JE, 1979, J APPL METEOROL, V18, P1182, DOI 10.1175/1520-0450(1979)018<1182:TAALCW>2.0.CO;2; Wu J., 2015, ADV NEURAL INF PROCE, V28, P1, DOI DOI 10.1007/978-3-319-26532-2_15; Wu Jiajun, 2016, BMVC, V2; Yang S, 2017, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2017.470; Yu HL, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17010013	41	4	4	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900034
C	Chakrabarti, A; Moseley, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chakrabarti, Ayan; Moseley, Benjamin			Backprop with Approximate Activations for Memory-efficient Network Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Training convolutional neural network models is memory intensive since back-propagation requires storing activations of all intermediate layers. This presents a practical concern when seeking to deploy very deep architectures in production, especially when models need to be frequently re-trained on updated datasets. In this paper, we propose a new implementation for back-propagation that significantly reduces memory usage, by enabling the use of approximations with negligible computational cost and minimal effect on training performance. The algorithm reuses common buffers to temporarily store full activations and compute the forward pass exactly. It also stores approximate per-layer copies of activations, at significant memory savings, that are used in the backward pass. Compared to simply approximating activations within standard back-propagation, our method limits accumulation of errors across layers. This allows the use of much lower-precision approximations without affecting training accuracy. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that our method yields performance close to exact training, while storing activations compactly with as low as 4-bit precision.	[Chakrabarti, Ayan] Washington Univ, 1 Brookings Dr, St Louis, MO 63130 USA; [Moseley, Benjamin] Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15213 USA	Washington University (WUSTL); Carnegie Mellon University	Chakrabarti, A (corresponding author), Washington Univ, 1 Brookings Dr, St Louis, MO 63130 USA.	ayan@wustl.edu; moseleyb@andrew.cmu.edu			NSF [IIS-1820693, CCF-1830711, CCF-1824303, CCF-1733873]; Google Research Award	NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated)	A. Chakrabarti acknowledges support from NSF grant IIS-1820693. B. Moseley was supported in part by a Google Research Award and NSF grants CCF-1830711, CCF-1824303, and CCF-1733873.	[Anonymous], 2016, P EUR C COMP VIS ECC; Banner R., 2018, ADV NEURAL INFORM PR; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Dean J., 2012, NIPS; Gruslys A, 2016, ADV NEUR IN, V29; Gupta S., 2015, P 32 INT C MACH LEAR, V37, P1737; Han S., 2015, 4 INT C LEARN REPR; Huang GL, 2017, IEEE ICC; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27; Micikevicius Paulius, 2017, ARXIV171003740; Orozco-Messana J, 2017, CONGRESO NACIONAL DE INNOVACION EDUCATIVA Y DOCENCIA EN RED (IN RED 2017), P1121, DOI 10.4995/INRED2017.2017.6745; Recht Benjamin, 2011, ADV NEURAL INFORM PR; Robbins Herbert, 1985, H ROBBINS SELECTED P, P102, DOI DOI 10.1007/978-1-4612-5110-1_9; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Seide F, 2014, INT CONF ACOUST SPEE; tlitbara I., 2017, J MACHINE LEARNING R; Wen W., 2017, ADV NEURAL INFORM PR	20	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302043
C	Chen, JC; Wang, LJ; Li, X; Fang, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Jianchun; Wang, Lingjing; Li, Xiang; Fang, Yi			Arbicon-Net: Arbitrary Continuous Geometric Transformation Networks for Image Registration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODEL	This paper concerns the undetermined problem of estimating geometric transformation between image pairs. Recent methods introduce deep neural networks to predict the controlling parameters of hand-crafted geometric transformation models (e.g. thin-plate spline) for image registration and matching. However the low-dimension parametric models are incapable of estimating a highly complex geometric transform with limited flexibility to model the actual geometric deformation from image pairs. To address this issue, we present an end-to-end trainable deep neural networks, named Arbitrary Continuous Geometric Transformation Networks (Arbicon-Net), to directly predict the dense displacement field for pair-wise image alignment. Arbicon-Net is generalized from training data to predict the desired arbitrary continuous geometric transformation in a data-driven manner for unseen new pair of images. Particularly, without imposing penalization terms, the predicted displacement vector function is proven to be spatially continuous and smooth. To verify the performance of Arbicon-Net, we conducted semantic alignment tests over both synthetic and real image dataset with various experimental settings. The results demonstrate that Arbicon-Net outperforms the previous image alignment techniques in identifying the image correspondences.	[Chen, Jianchun; Wang, Lingjing; Li, Xiang] NYU, Multimedia & Visual Comp Lab, Brooklyn, NY 11201 USA; [Fang, Yi] New York Univ Abu Dhabi, NYU Multimedia & Visual Comp Lab, Abu Dhabi, U Arab Emirates	New York University	Fang, Y (corresponding author), New York Univ Abu Dhabi, NYU Multimedia & Visual Comp Lab, Abu Dhabi, U Arab Emirates.	jc7009@nyu.edu; lw1474@nyu.edu; xl845@nyu.edu; yfang@nyu.edu	Li, Xiang/AAC-4866-2019	Li, Xiang/0000-0002-9946-7000	ADEK Grant [AARE-18150]	ADEK Grant	We would like to thank the reviewers for their thoughtful comments and efforts towards improving our manuscript. This work is partially supported by ADEK Grant (No. AARE-18150).	Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Berg AC, 2005, PROC CVPR IEEE, P26; Bergner S, 2006, IEEE T VIS COMPUT GR, V12, P1353, DOI 10.1109/TVCG.2006.113; Chen Z, 2002, NEURAL COMPUT, V14, P2791, DOI 10.1162/089976602760805296; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Duchon J., 1977, CONSTRUCTIVE THEORY, P85; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Ham B, 2018, IEEE T PATTERN ANAL, V40, P1711, DOI 10.1109/TPAMI.2017.2724510; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Han K, 2017, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2017.203; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Jeon Sangryul, 2018, EUR C COMP VIS ECCV; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kanazawa A, 2016, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2016.354; Kim S., 2017, P IEEE C COMP VIS PA, P6560; Kim S, 2017, IEEE I CONF COMP VIS, P4539, DOI 10.1109/ICCV.2017.485; Kim Seungryong, 2018, ADV NEURAL INFORM PR, P6129; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Nair V., 2010, ICML, P807; Narayanan R, 2005, LECT NOTES COMPUT SC, V3565, P174; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Rocco I, 2018, ADV NEUR IN, V31; Rocco I, 2018, PROC CVPR IEEE, P6917, DOI 10.1109/CVPR.2018.00723; Seo Paul Hongsuck, 2018, P EUR C COMP VIS ECC, P349; Sivic J, 2017, CVPR, P6148; Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460; Ufer N., 2017, P IEEE C COMP VIS PA, P6914; Yang Fan, 2017, P IEEE C COMP VIS PA, P2777; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; YUILLE AL, 1989, INT J COMPUT VISION, V3, P155, DOI 10.1007/BF00126430; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zheng EL, 2015, IEEE I CONF COMP VIS, P2075, DOI 10.1109/ICCV.2015.240; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	39	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303041
C	Chierchia, G; Perret, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chierchia, Giovanni; Perret, Benjamin			Ultrametric Fitting by Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TREE	We study the problem of fitting an ultrametric distance to a dissimilarity graph in the context of hierarchical cluster analysis. Standard hierarchical clustering methods are specified procedurally, rather than in terms of the cost function to be optimized. We aim to overcome this limitation by presenting a general optimization framework for ultrametric fitting. Our approach consists of modeling the latter as a constrained optimization problem over the continuous space of ultrametrics. So doing, we can leverage the simple, yet effective, idea of replacing the ultrametric constraint with a min-max operation injected directly into the cost function. The proposed reformulation leads to an unconstrained optimization problem that can be efficiently solved by gradient descent methods. The flexibility of our framework allows us to investigate several cost functions, following the classic paradigm of combining a data fidelity term with a regularization. While we provide no theoretical guarantee to find the global optimum, the numerical results obtained over a number of synthetic and real datasets demonstrate the good performance of our approach with respect to state-of-the-art agglomerative algorithms. This makes us believe that the proposed framework sheds new light on the way to design a new generation of hierarchical clustering methods. Our code is made publicly available at https://github.com/PerretB/ultrametric-fitting.	[Chierchia, Giovanni; Perret, Benjamin] Univ Paris Est, CNRS, ENPC, LIGM,UMR 8049,ESIEE Paris,UPEM, F-93162 Noisy Le Grand, France	Universite Gustave-Eiffel; ESIEE Paris; Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech	Chierchia, G (corresponding author), Univ Paris Est, CNRS, ENPC, LIGM,UMR 8049,ESIEE Paris,UPEM, F-93162 Noisy Le Grand, France.	giovanni.chierchia@esiee.fr; benjamin.perret@esiee.fr			INS2I JCJC project [2019OSCI]	INS2I JCJC project	This work was partly supported by the INS2I JCJC project under grant 2019OSCI. We are deeply grateful to Julian Yarkony and Charless Fowlkes for sharing their code, and to Fred Hamprecht for many insightful discussions.	Ackerman Margareta, 2016, J MACHINE LEARNING R, V17, P1; Ailon N, 2011, SIAM J COMPUT, V40, P1275, DOI 10.1137/100806886; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Baviera R, 2015, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2015/12/P12007; Bender MA, 2000, LECT NOTES COMPUT SC, V1776, P88, DOI 10.1007/10719839_9; Bonald T., 2018, KDD WORKSH; Carleo G., 2019, ARXIV190310563; Carlsson G, 2010, J MACH LEARN RES, V11, P1425; Charikar M, 2019, P 30 ANN ACM SIAM S, P2291, DOI DOI 10.1137/1.9781611975482.139; Charikar M, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P841; Chatziafratis V, 2018, P ICML STOCKH SWED, P774; Chehreghani M.H., 2021, ARXIV190102063, P1; Cohen-Addad V., 2017, ADV NEURAL INFORM PR, P6201; Cohen-Addad V, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P378; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; de Amorim RC, 2015, J CLASSIF, V32, P46, DOI 10.1007/s00357-015-9167-1; DESOETE G, 1984, PATTERN RECOGN LETT, V2, P133, DOI 10.1016/0167-8655(84)90036-9; Di Summa M, 2015, DISCRETE APPL MATH, V180, P70, DOI 10.1016/j.dam.2014.07.023; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Felsenstein J., 2004, INFERRING PHYLOGENIE; Foare M., 2018, HAL01782346; Funke J., 2018, IEEE PAMI, V99, P1; GOWER JC, 1969, ROY STAT SOC C-APP, V18, P54; GROSSMAN B, 1989, J PHYS A-MATH GEN, V22, pL33, DOI 10.1088/0305-4470/22/1/006; HARTIGAN JA, 1985, J CLASSIF, V2, P63, DOI 10.1007/BF01908064; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Jagannath A, 2017, COMMUN PUR APPL MATH, V70, P611, DOI 10.1002/cpa.21685; JARDINE N, 1968, COMPUT J, V11, P177, DOI 10.1093/comjnl/11.2.177; Katzgraber HG, 2012, PHYS REV B, V86, DOI 10.1103/PhysRevB.86.184405; Katzgraber HG, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.037207; Kobren A, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P255, DOI 10.1145/3097983.3098079; KRIVANEK M, 1988, INFORM PROCESS LETT, V27, P265, DOI 10.1016/0020-0190(88)90090-7; Leuzzi L, 1999, J PHYS A-MATH GEN, V32, P1417, DOI 10.1088/0305-4470/32/8/010; Mollenhoff T, 2016, PROC CVPR IEEE, P3948, DOI 10.1109/CVPR.2016.428; Monath N., 2017, NIPS 2017 WORKSH DIS; Moseley B., 2017, ADV NEURAL INFORM PR, P3094; Murtagh F, 2012, WIRES DATA MIN KNOWL, V2, P86, DOI 10.1002/widm.53; Najman Laurent, 2013, Mathematical Morphology and Its Applications to Signal and Image Processing. 11th International Symposium, ISMM 2013. Proceedings, P135, DOI 10.1007/978-3-642-38294-9_12; Najman L, 1996, IEEE T PATTERN ANAL, V18, P1163, DOI 10.1109/34.546254; Neal RM, 2003, BAYESIAN STATISTICS 7, P619; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Perret B, 2019, SOFTWAREX, V10, DOI 10.1016/j.softx.2019.100335; Pock T, 2008, LECT NOTES COMPUT SC, V5304, P792, DOI 10.1007/978-3-540-88690-7_59; Pock T, 2009, IEEE I CONF COMP VIS, P1133, DOI 10.1109/ICCV.2009.5459348; Pock T, 2009, PROC CVPR IEEE, P810, DOI 10.1109/CVPRW.2009.5206604; Roy A., 2017, JMLR, V18, P1; Roy A., 2016, ADV NEURAL INFORM PR, P2316; SNEATH PHA, 1962, NATURE, V193, P855, DOI 10.1038/193855a0; Turaga S., 2009, ADV NEURAL INFORM PR, P1865; Vikram S, 2016, PR MACH LEARN RES, V48; WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967; Yarkony J. E., 2015, P NEURIPS, P64	53	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303020
C	Choi, J; Gao, C; Messou, JCE; Huang, JB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Choi, Jinwoo; Gao, Chen; Messou, Joseph C. E.; Huang, Jia-Bin			Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.	[Choi, Jinwoo; Gao, Chen; Messou, Joseph C. E.; Huang, Jia-Bin] Virginia Tech, Blacksburg, VA 24061 USA	Virginia Polytechnic Institute & State University	Choi, J (corresponding author), Virginia Tech, Blacksburg, VA 24061 USA.	jinchoi@vt.edu; chengao@vt.edu; mejc2014@vt.edu; jbhuang@vt.edu			NSF [1755785]; Google Faculty Research Award; NVIDIA Corporation	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); NVIDIA Corporation	This work was supported in part by NSF under Grant No. 1755785 and a Google Faculty Research Award. We thank the support of NVIDIA Corporation with the GPU donation.	Adeli Ehsan, 2019, ARXIV191003676; Ai TT, 2018, SIGNAL TRANSDUCT TAR, V3, DOI 10.1038/s41392-017-0001-6; [Anonymous], 2017, IEEE INT C COMP VIS; [Anonymous], 2016, CVPR; Bazzani Loris, 2016, SELF; Behl H. S., 2018, BMVC; Bolukbasi Tolga, 2016, NEURIPS; Bousmalis Konstantinos, 2017, CVPR; CARBONETTO P, 2004, ECCV; Carreira Joao, 2017, CVP; Chao Y.-W., 2018, CVPR; Choi M. J., 2010, CVPR; Dave Achal, 2017, CVPR; Divvala S. K., 2009, CVPR; Du Tran Heng Wang, 2017, CVPR; Feichtenhofer C., 2016, CVPR; Ganin Y., 2016, JMLR, V17, P2096; Ganin Y., 2015, ICML; Geirhos R., 2019, ICLR; Ghiasi Golnaz, 2018, NEURIPS; Gkioxari G., 2015, CVPR; Hara K., 2018, CVPR; He Jiawei, 2018, GENERIC TUBELET PROP; He K., 2017, ICCV; He Yun, 2016, ECCV WORKSH; Hendricks Lisa Anne, 2018, ECCV; Hoffman Judy, 2018, NEURIPS; Hou Rui, 2017, ICCV, V1, P2; Jahan S, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON NETWORKING, SYSTEMS AND SECURITY (NSYSS), P39, DOI 10.1109/NSysS.2017.7885799; Jhuang Hueihan, 2013, ICCV; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia WW, 2017, ADV BIO SCI RES, V4, P1; Jiang Y, 2014, TRENDS FOOD SCI TECH, V36, P15, DOI 10.1016/j.tifs.2013.12.005; Kalogeiton V., 2017, ICCV; Khosla A., 2014, CVPR; Kim Byungju, 2019, CVPR; Kuehne H., 2011, ICCV; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; Li Dong, 2019, TPAMI; Li Y., 2018, ECCV; Lucchi A, 2011, IEEE I CONF COMP VIS, P9, DOI 10.1109/ICCV.2011.6126219; Mottaghi R., 2014, CVPR; Park T., 2017, ICML; Peng Xiaojiang, 2016, ECCV; Saha S., 2016, BMVC; Saha S., 2017, ARXIV170707213; Shou Z., 2017, CVPR; Shuicheng Y., 2019, CVPR; Simonyan Karen, 2015, INT C LEARN REPR; Singh Gurkirt, 2017, ICCV; Singh K. K., 2017, ICCV; Soomro K., 2012, COMPUT SCI; Tran Du, 2015, ICCV; Tzeng E., 2017, CVPR; Vu T.-H., 2014, ECCV; Wang L., 2016, ECCV; Wang Limin, 2018, TPAMI; Wang Shiguang, 2018, FASTER PYTORCH IMPLE; Wang Tiancai, 2019, ICCV; Wang Y., 2018, CVPR; Weinzaepfel P., 2015, ICCV; Wu CP, 2019, ARAB WORLD ENGL J, P3, DOI 10.24093/awej/call5.1; Xie Saining, 2018, ECCV; Yeung S., 2016, CVPR; Zeiler MD, 2014, ECCV; Zhang Brian Hu, 2018, P AAAI ACM C ETH SOC; Zhao H, 2017, P IEEE C COMPUTER VI; Zhao Y., 2018, CVPR; Zhao Y., 2017, ICCV; Zhou Bolei, 2017, TPAMI; Zhou H, 2016, 2016 INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2016), P9, DOI 10.1109/ICIVC.2016.7571265; Zisserman Andrew, 2014, NEURIPS; Zolfaghari M., 2017, ICCV	74	4	4	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300077
C	Defazio, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Defazio, Aaron			On the Curved Geometry of Accelerated Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GRADIENT	In this work we propose a differential geometric motivation for Nesterov's accelerated gradient method (AGM) for strongly-convex problems. By considering the optimization procedure as occurring on a Riemannian manifold with a natural structure, The AGM method can be seen as the proximal point method applied in this curved space. This viewpoint can also be extended to the continuous time case, where the accelerated gradient method arises from the natural block-implicit Euler discretization of an ODE on the manifold. We provide an analysis of the convergence rate of this ODE for quadratic objectives.	[Defazio, Aaron] Facebook AI Res, New York, NY 10003 USA	Facebook Inc	Defazio, A (corresponding author), Facebook AI Res, New York, NY 10003 USA.							Allen-Zhu Z., 2017, 8 INN THEOR COMP SCI; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], 2013, ICML 3; Auslender A, 2006, SIAM J OPTIMIZ, V16, P697, DOI 10.1137/S1052623403427823; Beck A., 2009, SIAM J IMAGING SCI; Beck Amir, 2003, OPERATIONS RES LETT; Boyd S., 1994, SIAM; Bubeck S., 2015, GEOMETRIC ALTERNATIV; Lan GH, 2018, MATH PROGRAM, V171, P167, DOI 10.1007/s10107-017-1173-0; Lee John M, 2018, INTRO RIEMANNIAN MAN; Nesterov Y., 2007, TECHNICAL REPORT; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y., 2008, MATH PROGRAMMING; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Ryu EK, 2016, APPL COMPUT MATH-BAK, V15, P3; Scieur D., 2017, ADV NEURAL INFORM PR, V30; Shima H., 2007, GEOMETRY HESSIAN STR, DOI [10.1142/9789812707536, DOI 10.1142/9789812707536]; Su Weijie, 2014, ADV NEURAL INFORM PR; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Wilson A. C., 2018, ARXIV161102635; ZHANG YC, 2017, J MACHINE LEARNING R, V0018, P02939	23	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301072
C	Deng, W; Zhang, X; Liang, FM; Lin, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Deng, Wei; Zhang, Xiao; Liang, Faming; Lin, Guang			An Adaptive Empirical Bayesian Method for Sparse Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIABLE SELECTION	We propose a novel adaptive empirical Bayesian (AEB) method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors. The proposed method works by alternatively sampling from an adaptive hierarchical posterior distribution using stochastic gradient Markov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic approximation (SA). We further prove the convergence of the proposed method to the asymptotically correct distribution under mild conditions. Empirical applications of the proposed method lead to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks (CNN) and the state-of-the-art compression performance on CIFAR10 with Residual Networks. The proposed method also improves resistance to adversarial attacks.	[Deng, Wei; Lin, Guang] Purdue Univ, Dept Math, W Lafayette, IN 47907 USA; [Zhang, Xiao] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA; [Liang, Faming; Lin, Guang] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA; [Lin, Guang] Purdue Univ, Sch Mech Engn, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Deng, W (corresponding author), Purdue Univ, Dept Math, W Lafayette, IN 47907 USA.	deng106@purdue.edu; zhang923@purdue.edu; fmliang@purdue.edu; guanglin@purdue.edu	Lin, Guang/ABB-2145-2021	Lin, Guang/0000-0002-0976-1987	National Science Foundation [DMS-1555072, DMS-1736364, DMS-1821233, DMS-1818674]; GPU grant program from NVIDIA	National Science Foundation(National Science Foundation (NSF)); GPU grant program from NVIDIA	We would like to thank Prof. Vinayak Rao, Dr. Yunfan Li and the reviewers for their insightful comments. We acknowledge the support from the National Science Foundation (DMS-1555072, DMS-1736364, DMS-1821233 and DMS-1818674) and the GPU grant program from NVIDIA.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andrieu C, 2005, SIAM J CONTROL OPTIM, V44, P283, DOI 10.1137/S0363012902417267; Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI [10.1109/ICCV.2015.104, 10.1109/ICCV.2015.312]; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Faghri Fartash, 2016, ARXIV PREPRINT ARXIV; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Ghosh S, 2018, PR MACH LEARN RES, V80; Gomez A., 2018, ADV NEURAL INFORM PR, V208; Goodfellow I., 2013, P INT C MACH LEARN I; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han Song, 2016, IEEE C COMP VIS PATT; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lang FM, 2010, ANN STAT, V38, P2823, DOI 10.1214/10-AOS807; Li Yingzhen, 2017, P INT C MACH LEARN I; Liang FM, 2018, J AM STAT ASSOC, V113, P955, DOI 10.1080/01621459.2017.1409122; Lin J, 2017, ADV NEUR IN, V30; Louizos Christos, 2017, P C ADV NEUR INF PRO; Ma YA, 2015, ADV NEUR IN, V28; Mangoubi O., 2018, C LEARN THEOR COLT; Molchanov P., 2017, P INT C LEARN REPR I, P1; Paszke Adam, 2017, NIPS AUT WORKSH, DOI DOI 10.1017/CBO9781107707221.009; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Rockova V, 2018, J AM STAT ASSOC, V113, P431, DOI 10.1080/01621459.2016.1260469; Rockova V, 2014, J AM STAT ASSOC, V109, P828, DOI 10.1080/01621459.2013.869223; Saatci Y., 2017, ADV NEURAL INFORM PR, P3622; Shimkin Nahum, 2011, INTRO STOCHASTIC APP; Teh Y. W., 2016, J MACH LEARN RES, V17, P193; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xiao Han, 2017, ARXIV E PRINTS; Xu Pan, 2018, P C ADV NEUR INF PRO; Ye M, 2018, PR MACH LEARN RES, V80; Zhang YG, 2017, 2017 IEEE 2ND ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P1980, DOI 10.1109/IAEAC.2017.8054361; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	39	4	4	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305054
C	Diakonikolas, I; Karmalkar, S; Kane, D; Price, E; Stewart, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Diakonikolas, Ilias; Karmalkar, Sushrut; Kane, Daniel; Price, Eric; Stewart, Alistair			Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study high-dimensional sparse estimation tasks in a robust setting where a constant fraction of the dataset is adversarially corrupted. Specifically, we focus on the fundamental problems of robust sparse mean estimation and robust sparse PCA. We give the first practically viable robust estimators for these problems. In more detail, our algorithms are sample and computationally efficient and achieve near-optimal robustness guarantees. In contrast to prior provable algorithms which relied on the ellipsoid method, our algorithms use spectral techniques to iteratively remove outliers from the dataset. Our experimental evaluation on synthetic data shows that our algorithms are scalable and significantly outperform a range of previous approaches, nearly matching the best error rate without corruptions.	[Diakonikolas, Ilias] Univ Wisconsin, Madison, WI 53706 USA; [Karmalkar, Sushrut; Price, Eric] UT Austin, Austin, TX USA; [Kane, Daniel] Univ Calif San Diego, La Jolla, CA 92093 USA; [Stewart, Alistair] Web3 Fdn, La Jolla, CA USA	University of Wisconsin System; University of Wisconsin Madison; University of Texas System; University of Texas Austin; University of California System; University of California San Diego	Diakonikolas, I (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	ilias.diakonikolas@gmail.com; s.sushrut@gmail.com; dakane@ucsd.edu; ecprice@cs.utexas.edu; stewart.al@gmail.com			NSF Award [CCF-1652862]; Sloan Research Fellowship; NSF [CCF-1751040, CNS-1414023., CCF-1553288]	NSF Award(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation); NSF(National Science Foundation (NSF))	Ilias Diakonikolas was supported by the NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. Sushrut Karmalkar was supported by NSF Award CNS-1414023. Daniel Kane was supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research Fellowship. Eric Price was supported in part by NSF Award CCF-1751040 (CAREER). A part of this work was performed when Alistair Stewart was a postdoctoral researcher at USC.	Balakrishnan S., 2017, C LEARN THEOR PMLR, P169; Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Biggio B., 2012, P 29 INT C INT C MAC, P1467; Cheng Y., 2018, P 33 ANN C NEUR INF; Cheng Y., 2019, C LEARN THEOR, P727; Cheng Yu, 2019, P 30 ANN ACM SIAM S, P2755; Diakonikolas I., 2019, P 30 ANN S DISCR ALG; Diakonikolas I, 2018, ARXIV180302815; Diakonikolas I, 2019, SIAM J COMPUT, V48, P742, DOI 10.1137/17M1126680; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1061, DOI 10.1145/3188745.3188754; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1047, DOI 10.1145/3188745.3188758; Diakonikolas I, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2683; Diakonikolas I, 2017, PR MACH LEARN RES, V70; Diakonikolas I, 2017, ANN IEEE SYMP FOUND, P73, DOI 10.1109/FOCS.2017.16; Steinhardt J., 2017, P 31 INT C NEUR INF, P3520; Steinhardt Jacob, 2018, P 9 C INN THEOR COMP; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Valiant L. G., 1985, IJCAI, P283	20	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902033
C	Durfee, D; Rogers, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Durfee, David; Rogers, Ryan			Practical Differentially Private Top-k Selection with Pay-what-you-get Composition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the problem of top-k selection over a large domain universe subject to user-level differential privacy. Typically, the exponential mechanism or report noisy max are the algorithms used to solve this problem. However, these algorithms require querying the database for the count of each domain element. We focus on the setting where the data domain is unknown, which is different than the setting of frequent itemsets where an apriori type algorithm can help prune the space of domain elements to query. We design algorithms that ensures (approximate) (epsilon; delta > 0)-differential privacy and only needs access to the true top-(k) over bar elements from the data for any chosen (k) over bar >= k. We consider both the setting where a user's data can modify an arbitrary number of counts by at most 1, i.e. unrestricted sensitivity, and the setting where a user's data can modify at most some small, fixed number of counts by at most 1, i.e. restricted sensitivity. Additionally, we provide a pay-what-you-get privacy composition bound for our algorithms. That is, our algorithms might return fewer than k elements when the top-k elements are queried, but the overall privacy budget only decreases by the size of the outcome.	[Durfee, David; Rogers, Ryan] LinkedIn, Data Sci Appl Res, Sunnyvale, CA 94085 USA		Durfee, D (corresponding author), LinkedIn, Data Sci Appl Res, Sunnyvale, CA 94085 USA.							Bafna M., 2017, P MACHINE LEARNING R, V65, P151; Bassily R, 2015, ACM S THEORY COMPUT, P127, DOI 10.1145/2746539.2746632; Bassily Raef, 2017, ADV NEURAL INFORM PR, P2288; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Chaudhuri K., 2014, ADV NEURAL INFORM PR, V27, P1287; Choi JP, 2014, I C INF COMM TECH CO, P930, DOI 10.1109/ICTC.2014.6983336; Differential Privacy Team Apple, 2017, LEARN PRIV SCAL; Ding B., 2017, COLLECTING TELEMETRY; Dwork C., 2016, ARXIV160301887CSDS; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Dwork Cynthia, 2015, ARXIV151103803; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Fanti Giulia, 2016, POPETS, V3, P2016; Ilyas IF, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391730; Johnson N, 2018, PROC VLDB ENDOW, V11, P526, DOI 10.1145/3177732.3177733; Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Kantarcioglu M., 2004, PROC 900 SIGKDD INT, P599; Karwa Vishesh, 2017, MODERNIZATION STAT D; Kenthapadi K, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2183, DOI 10.1145/3269206.3272031; Laxman S., 2010, KDD, P503, DOI DOI 10.1145/1835804.1835869; Li N, 2012, PROC VLDB ENDOW, V5, P1340, DOI 10.14778/2350229.2350251; Machanavajjhala A., 2007, ACM T KNOWL DISCOV D, V1; McSherry F., 2007, 48 ANN S FDN COMP SC; Murtagh J, 2016, LECT NOTES COMPUT SC, V9562, P157, DOI 10.1007/978-3-662-49096-9_7; Rogers Ryan M, 2016, ADV NEURAL INFORM PR, P1921; Zeng C, 2012, PROC VLDB ENDOW, V6, P25, DOI 10.14778/2428536.2428539; Zhu W., 2019, ABS190208534 CORR	29	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303051
C	Fortunato, M; Tan, M; Faulkner, R; Hansen, S; Badia, AP; Buttimore, G; Deck, C; Leibo, JZ; Blundell, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fortunato, Meire; Tan, Melissa; Faulkner, Ryan; Hansen, Steven; Badia, Adria Puigdomenech; Buttimore, Gavin; Deck, Charlie; Leibo, Joel Z.; Blundell, Charles			Generalization of Reinforcement Learners with Working and Episodic Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REPRESENTATIONS	Memory is an important aspect of intelligence and plays a role in many deep reinforcement learning models. However, little progress has been made in understanding when specific memory systems help more than others and how well they generalize. The field also has yet to see a prevalent consistent and rigorous approach for evaluating agent performance on holdout data. In this paper, we aim to develop a comprehensive methodology to test different kinds of memory in an agent and assess how well the agent can apply what it learns in training to a holdout set that differs from the training set along dimensions that we suggest are relevant for evaluating memory-specific generalization. To that end, we first construct a diverse set of memory tasks(1) that allow us to evaluate test-time generalization across multiple dimensions. Second, we develop and perform multiple ablations on an agent architecture that combines multiple memory systems, observe its baseline models, and investigate its performance against the task suite.	[Fortunato, Meire; Tan, Melissa; Faulkner, Ryan; Hansen, Steven; Badia, Adria Puigdomenech; Buttimore, Gavin; Deck, Charlie; Leibo, Joel Z.; Blundell, Charles] DeepMind, London, England		Fortunato, M (corresponding author), DeepMind, London, England.	meirefortunato@google.com; melissatan@google.com; rfaulk@google.com; stevenhansen@google.com; adriap@google.com; buttimore@google.com; cdeck@google.com; jzl@google.com; cblundell@google.com		Leibo, Joel/0000-0002-3153-916X				Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6; Beattie C., 2016, ARXIV161203801; Blundell C., 2016, ARXIV160604460 ARXIV; Cobbe K., 2018, ARXIV181202341, P1; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Espeholt L, 2018, PR MACH LEARN RES, V80; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Guo Z., 2018, CORR; HANSEN S, 2018, ADV NEURAL INFORM PR, P10567; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg M., 2016, ARXIV161105397 CORR; Ke N. R., 2018, CORR; Leibo J. Z., 2018, CORR; MIYAKE A, 1999, MODELS WORKING MEMOR, P442, DOI DOI 10.1017/CB09781139174909; Mnih V, 2016, PR MACH LEARN RES, V48; Murray D, 1985, CAN PSYCHOL, V26, P235, DOI [10.1037/h0084438, DOI 10.1037/h0084438]; Pineau J., 2018, ADV NEURAL INFORM PR, V2018; Pritzel A, 2017, PR MACH LEARN RES, V70; Ritter S, 2018, PR MACH LEARN RES, V80; Santoro A., 2018, CORR; Smith C, 2005, J NEUROSCI, V25, P10138, DOI 10.1523/JNEUROSCI.2731-05.2005; Sukhbaatar S, 2015, ADV NEUR IN, V28; Tulving E, 2002, ANNU REV PSYCHOL, V53, P1, DOI 10.1146/annurev.psych.53.100901.135114; van den Oord Aaron, 2018, ARXIV180703748; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wayne G., 2018, CORR; Zambaldi V.F., 2018, CORR	30	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904015
C	Fu, C; Chen, HL; Liu, HL; Chen, XY; Tian, YD; Koushanfar, F; Zhao, JS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fu, Cheng; Chen, Huili; Liu, Haolan; Chen, Xinyun; Tian, Yuandong; Koushanfar, Farinaz; Zhao, Jishen			Coda: An End-to-End Neural Program Decompiler	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda(1), the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into of two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70% program accuracy. Our work reveals the vulnerability of binary executables and imposes a new threat to the protection of Intellectual Property (IP) for software development.	[Fu, Cheng; Chen, Huili; Liu, Haolan; Koushanfar, Farinaz; Zhao, Jishen] Univ Calif San Diego, San Diego, CA 92103 USA; [Chen, Xinyun] Univ Calif Berkeley, Berkeley, CA USA; [Tian, Yuandong] Facebook, Menlo Pk, CA USA	University of California System; University of California San Diego; University of California System; University of California Berkeley; Facebook Inc	Fu, C (corresponding author), Univ Calif San Diego, San Diego, CA 92103 USA.	cfu@ucsd.edu; huc044@ucsd.edu; hal022@ucsd.edu; xinyun.chen@berkeley.edu; yuandong@fb.com; farinaz@ucsd.edu; jzhao@ucsd.edu	Chen, Xinyun/ABZ-9877-2022	Koushanfar, Farinaz/0000-0003-0798-3794				Nguyen AT, 2015, IEEE INT CONF AUTOM, P585, DOI 10.1109/ASE.2015.74; [Anonymous], 2017, ARXIV170407535; [Anonymous], 2009, 18 USENIX SEC S USEN; Bao T, 2014, PROCEEDINGS OF THE 23RD USENIX SECURITY SYMPOSIUM, P845; Brumley David, 2013, 22 USENIX SEC S USEN, P353; Chen X., 2018, ADV NEURAL INFORM PR, P2547; Chen X, 2019, ENERGY, V172, P1, DOI 10.1016/j.energy.2019.01.112; Cifuentes C., 1994, REVERSE COMPILATION; Dolan-Gavitt B, 2011, P IEEE S SECUR PRIV, P297, DOI 10.1109/SP.2011.11; Dong L, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P33; Gupta R, 2017, AAAI CONF ARTIF INTE, P1345; Hyyro H., 2001, TECH REP; Intel Corporation, 2017, INT 64 IA 32 ARCH SO; Jaffe A, 2018, INT C PROGRAM COMPRE, P20, DOI 10.1145/3196321.3196330; Kai Sheng Tai, 2015, ARXIV150300075; KANE G, 1988, MIPS RISC ARCHITECTU; Katz DS, 2018, 2018 25TH IEEE INTERNATIONAL CONFERENCE ON SOFTWARE ANALYSIS, EVOLUTION AND REENGINEERING (SANER 2018), P346; Katz O., 2019, ARXIV190508325; Kroustek J., 2014, THESIS; LEE J, 2011, TIE PRINCIPLED REVER; Lin ZC, 2011, THESIS; Ling W, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P599; McCann Bryan, 2017, P ADV NEURAL INFORM, P6294; Nguyen Anh Tuan, 2013, P 2013 9 JOINT M FDN, P651; Pattis R. E., KAREL ROBOT; Piech C., 2015, ARXIV150505969; Rahimian A, 2015, DIGIT INVEST, V14, pS146, DOI 10.1016/j.diin.2015.05.015; Rosenblum N. E., 2008, LEARNING ANAL BINARY; Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150; Shin ECR, 2015, PROCEEDINGS OF THE 24TH USENIX SECURITY SYMPOSIUM, P611; Van Emmerik M, 2004, 11TH WORKING CONFERENCE ON REVERSE ENGINEERING, PROCEEDINGS, P27; Wang Ke, 2017, ICLR; Warren H.S., 2013, HACKERS DELIGHT; Yakdan K, 2016, P IEEE S SECUR PRIV, P158, DOI 10.1109/SP.2016.18; Yin PC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P440, DOI 10.18653/v1/P17-1041	36	4	4	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303067
C	Ge, R; Kakade, SM; Kidambi, R; Netrapalli, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ge, Rong; Kakade, Sham M.; Kidambi, Rahul; Netrapalli, Praneeth			The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Minimax optimal convergence rates for numerous classes of stochastic convex optimization problems are well characterized, where the majority of results utilize iterate averaged stochastic gradient descent (SGD) with polynomially decaying step sizes. In contrast, the behavior of SGD's final iterate has received much less attention despite the widespread use in practice. Motivated by this observation, this work provides a detailed study of the following question: what rate is achievable using the final iterate of SGD for the streaming least squares regression problem with and without strong convexity? First, this work shows that even if the time horizon T (i.e. the number of iterations that SGD is run for) is known in advance, the behavior of SGD's final iterate with any polynomially decaying learning rate scheme is highly sub-optimal compared to the statistical minimax rate (by a condition number factor in the strongly convex case and a factor of T in the non-strongly convex case). In contrast, this paper shows that Step Decay schedules, which cut the learning rate by a constant factor every constant number of epochs (i.e., the learning rate decays geometrically) offer significant improvements over any polynomially decaying step size schedule. In particular, the behavior of the final iterate with step decay schedules is off from the statistical minimax rate by only log factors (in the condition number for the strongly convex case, and in T in the non-strongly convex case). Finally, in stark contrast to the known horizon case, this paper shows that the anytime (i.e. the limiting) behavior of SGD's final iterate is poor (in that it queries iterates with highly sub-optimal function value infinitely often, i.e. in a limsup sense) irrespective of the stepsize scheme employed. These results demonstrate the subtlety in establishing optimal learning rate schedules (for the final iterate) for stochastic gradient procedures in fixed time horizon settings.	[Ge, Rong] Duke Univ, Durham, NC 27706 USA; [Kakade, Sham M.] Univ Washington, Seattle, WA 98195 USA; [Kidambi, Rahul] Cornell Univ, Ithaca, NY 14853 USA; [Netrapalli, Praneeth] Microsoft Res, Bengaluru, India	Duke University; University of Washington; University of Washington Seattle; Cornell University	Ge, R (corresponding author), Duke Univ, Durham, NC 27706 USA.	rongge@cs.duke.edu; sham@cs.washington.edu; rkidambi@cornell.edu; praneeth@microsoft.com			NSF [CCF-1704656, CCF-1845171, 174055 1, 1740822]; Sloan Fellowship; Google Faculty Research Award; Washington Research Foundation for Innovation in Data-intensive Discovery; ONR [N00014-18-1-2247]	NSF(National Science Foundation (NSF)); Sloan Fellowship(Alfred P. Sloan Foundation); Google Faculty Research Award(Google Incorporated); Washington Research Foundation for Innovation in Data-intensive Discovery; ONR(Office of Naval Research)	Rong Ge acknowledges funding from NSF CCF-1704656, NSF CCF-1845171 (CAREER), Sloan Fellowship and Google Faculty Research Award. Sham Kakade acknowledges funding from the Washington Research Foundation for Innovation in Data-intensive Discovery, NSF Award 174055 1, and ONR award N00014-18-1-2247. Rahul Kidambi acknowledges funding from NSF Award 1740822.	Agarwal Alekh, 2012, IEEE T INFORM THEORY; Allen-Zhu Z, 2018, CORR; Anbar Dan, 1971, OPTIMAL ESTIMATION M; [Anonymous], 2012, ICML; Aybat Necdet Serhat, 2019, CORR; Bach Francis R., 2014, J MACHINE LEARNING R, V15; Bach Francis R., 2013, NIPS 26; Bach Francis R., 2011, NIPS 24; Benveniste Albert, 1990, SPRINGER TEXTS STOCH; Bharath B., 1999, SADHANA; Bottou L6on, 2007, NIPS 20; Bubeck S6bastien, 2014, CORR; Davis Damek, 2019, CORR; Dekel Ofer, 2012, J MACHINE LEARNING R, V13; Dffossez Alexandre, 2015, ARTIFICAL INTELLIGEN; Dieuleveut Aymeric, 2015, ANN STAT; Dieuleveut Aymeric, 2016, CORR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Frostig Roy, 2015, COLT; Ghadimi S., 2013, SIAM J OPTIMIZATION, V23; Ghadimi Saeed, 2012, SIAM J OPTIMIZATION; Ghadimi Saeed, 2013, SIAM J OPTIMIZATION; GOFFIN JL, 1977, MATH PROGRAM, V13, P329, DOI 10.1007/BF01584346; Harvey Nicholas J. A., 2018, CORR; Hazan Elad, 2014, J MACHINE LEARNING R, V15; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Jain P., 2016, ARXIV161003774; Jain P., 2017, ARXIV170408227; Jain Prateek, 2017, CORR; Jain Prateek, 2019, CORR; Johnson Rie, 2013, NIPS 26; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kulunchakov Andrei, 2019, CORR; Kushner HJ., 2003, STOCHASTIC APPROXIMA; Kushner HJ, 1978, STOCHASTIC APPROXIMA; Lacoste-Julien Simon, 2012, CORR; Lai Tze Leung, 2003, STOCHASTIC APPROXIMA; Lehmann E.L., 1998, THEORY POINT ESTIMAT, V2nd ed; LJUNG L, 1992, STOCHASTIC APPROXIMA; Nagumo Jin-Ichi, 1967, IEEE T AUTOMATIC CON; Needell Deanna, 2016, MATH PROGRAMMING; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Yurii E., 1983, DOKLADY AN SSSR, V269; Neu Gergely, 2018, CORR; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Proakis John G., 1974, IEEE T AUTOMATIC CON; Raginsky Maxim, 2011, IEEE T INFORM THEORY; Robbins H., 1951, ANN MATH STAT, V22; Roy Sumit, 1990, IEEE T ACOUSTICS SPE; Ruppert D., 1988, EFFICIENT ESTIMATION; Shamir Ohad, 2012, COLT; Shamir Ohad, 2012, CORR; Sharma Rajesh, 1998, IEEE T SIGNAL PROCES; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Widrow B., 1985, ADAPTIVE SIGNAL PROC; Widrow B., 1960, IRE WESCOM CONV REC, V4, P96, DOI DOI 10.21236/AD0241531; Xu Y., 2016, CORR; Yang Tianbao, 2018, CORR	60	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906061
C	Gourdeau, P; Kanade, V; Kwiatkowska, M; Worrell, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gourdeau, Pascale; Kanade, Varun; Kwiatkowska, Marta; Worrell, James			On the Hardness of Robust Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. In this paper we study the feasibility of robust learning from the perspective of computational learning theory, considering both sample and computational complexity. In particular, our definition of robust learnability requires polynomial sample complexity. We start with two negative results. We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit. We show moreover that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb omega(log n) input bits. However if the adversary is restricted to perturbing O(log n) bits, then the class of monotone conjunctions can be robustly learned with respect to a general class of distributions (that includes the uniform distribution). Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on another computational model (e.g. the statistical query model) nor on any hardness assumption other than the existence of a hard learning problem in the PAC framework.	[Gourdeau, Pascale; Kanade, Varun; Kwiatkowska, Marta; Worrell, James] Univ Oxford, Oxford, England	University of Oxford	Gourdeau, P (corresponding author), Univ Oxford, Oxford, England.	pascale.gourdeau@cs.ox.ac.uk; varunk@cs.ox.ac.uk; marta.kwiatkowska@cs.ox.ac.uk; james.worrell@cs.ox.ac.uk			Alan Turing Institute under the EPSRC [EP/N510129/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Varun Kanade was supported in part by the Alan Turing Institute under the EPSRC grant EP/N510129/1.	Applebaum Benny, 2008, P 49 ANN IEEE S FDN; Awasthi Pranjal, 2013, COLT, V30, P1; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Biggio B., 2017, ARXIV171203141; Bose R., 1960, ELSEVIER INFORM CONT, V3, P68, DOI DOI 10.1016/S0019-9958(60)90287-4; Bubeck S., 2018, ARXIV180510204; Bubeck S., 2018, ARXIV181106418; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Degwekar Akshay, 2019, ARXIV190201086; Diochnos Dimitrios, 2018, ADV NEURAL INFORM PR; Dreossi T., 2019, ARXIV190310033; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Fawzi Alhussein, 2018, ARXIV180208686; Feldman Dan, 2012, P 23 ANN ACM SIAM S, P1343; Gilmer J., 2018, INT C LEARN REPR WOR; Hocquenghem A., 1959, CHIFFRES, V2, P147; Koltun V, 2007, THEOR COMPUT SCI, V371, P148, DOI 10.1016/j.tcs.2006.11.003; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Lowd D., 2005, CEAS, V2005; Mahloujifar Saeed, 2018, ARXIV181001407; Mahloujifar Saeed, 2019, AAAI C ART INT; Mohri M., 2018, FDN MACHINE LEARNING; Shafahi Ali, 2018, ARXIV180902104; Szegedy C., 2013, INT C LEARN REPR; Turner A., 2019, INT C LEARN REPR; Valiant L. G., 1984, Communications of the ACM, V27, P1134, DOI 10.1145/1968.1972	27	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307046
C	Ha, H; Rana, S; Gupta, S; Nguyen, T; Tran-The, H; Venkatesh, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ha, Huong; Rana, Santu; Gupta, Sunil; Nguyen, Thanh; Tran-The, Hung; Venkatesh, Svetha			Bayesian Optimization with Unknown Search Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EFFICIENT GLOBAL OPTIMIZATION	Applying Bayesian optimization in problems wherein the search space is unknown is challenging. To address this problem, we propose a systematic volume expansion strategy for the Bayesian optimization. We devise a strategy to guarantee that in iterative expansions of the search space, our method can find a point whose function value within epsilon of the objective function maximum. Without the need to specify any parameters, our algorithm automatically triggers a minimal expansion required iteratively. We derive analytic expressions for when to trigger the expansion and by how much to expand. We also provide theoretical analysis to show that our method achieves c-accuracy after a finite number of iterations. We demonstrate our method on both benchmark test functions and machine learning hyper-parameter tuning tasks and demonstrate that our method outperforms baselines.	[Ha, Huong; Rana, Santu; Gupta, Sunil; Nguyen, Thanh; Tran-The, Hung; Venkatesh, Svetha] Deakin Univ, Appl Artificial Intelligence Inst A2I2, Geelong, Vic, Australia	Deakin University	Ha, H (corresponding author), Deakin Univ, Appl Artificial Intelligence Inst A2I2, Geelong, Vic, Australia.	huong.ha@deakin.edu.au; santu.rana@deakin.edu.au; sunil.gupta@deakin.edu.au; thanhnt@deakin.edu.au; hung.tranthe@deakin.edu.au; svetha.venkatesh@deakin.edu.au		Rana, Santu/0000-0003-2247-850X; gupta, sunil/0000-0002-4669-9940; venkatesh, svetha/0000-0001-8675-6631; Nguyen-Tang, Thanh/0000-0002-1917-2190; Ha, Huong/0000-0003-2463-7770	Australian Government through the Australian Research Council (ARC); ARC Australian Laureate Fellowship [FL170100006]	Australian Government through the Australian Research Council (ARC)(Australian Research Council); ARC Australian Laureate Fellowship(Australian Research Council)	This research was partially funded by the Australian Government through the Australian Research Council (ARC). Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).	Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bogunovic I., 2016, ADV NEURAL INFORM PR, V29, P1507; Bull AD, 2011, J MACH LEARN RES, V12, P2879; Dani V., 2008, 21 ANN C LEARN THEOR; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Huang QZ, 2017, INTERSPEECH, P3627, DOI 10.21437/Interspeech.2017-109; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kushner HaroldJ., 1964, J BASIC ENG-T ASME, V86, P97, DOI [10.1115/1.3653121, DOI 10.1115/1.3653121]; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Scarlett J, 2018, PR MACH LEARN RES, V80; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Shahriari B, 2016, JMLR WORKSH CONF PRO, V51, P1168; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Nguyen V, 2017, IEEE DATA MINING, P347, DOI 10.1109/ICDM.2017.44	20	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903041
C	Hanneke, S; Kpotufe, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hanneke, Steve; Kpotufe, Samory			On the Value of Target Data in Transfer Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We aim to understand the value of additional labeled or unlabeled target data in transfer learning, for any given amount of source data; this is motivated by practical questions around minimizing sampling costs, whereby, target data is usually harder or costlier to acquire than source data, but can yield better accuracy. To this aim, we establish the first minimax-rates in terms of both source and target sample sizes, and show that performance limits are captured by new notions of discrepancy between source and target, which we refer to as transfer exponents. Interestingly, we find that attaining minimax performance is akin to ignoring one of the source or target samples, provided distributional parameters were known a priori. Moreover, we show that practical decisions - w.r.t. minimizing sampling costs - can be made in a minimax-optimal way without knowledge or estimation of distributional parameters nor of the discrepancy between source and target.	[Hanneke, Steve] Toyota Technol Inst, Chicago, IL 60637 USA; [Kpotufe, Samory] Columbia Univ, Stat, New York, NY 10027 USA	Toyota Technological Institute - Chicago; Columbia University	Hanneke, S (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.	steve.hanneke@gmail.com; skk2175@columbia.edu						Bartlett PL, 2006, PROBAB THEORY REL, V135, P311, DOI 10.1007/s00440-005-0462-3; Ben-David Shai, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P139, DOI 10.1007/978-3-642-34106-9_14; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Berlind C, 2015, PR MACH LEARN RES, V37, P1870; Blanchard Gilles, 2017, ARXIV171107910; Cai T Tony, 2019, ARXIV190602903; Chattopadhyay R., 2013, PROC 30 INT C MACH L, P253; Chen M., 2011, ADV NEURAL INF PROCE, P2456; Cortes C, 2008, LECT NOTES ARTIF INT, V5254, P38, DOI 10.1007/978-3-540-87987-9_8; Cortes Corinna, MACHINE LEARNING RES; Du Simon S., 2017, ADV NEURAL INFORM PR, P2; Germain P., 2013, INT C MACH LEARN, P738; Gretton A., 2009, DATASET SHIFT MACHIN, V3, P5; Hanneke S., 2012, ARXIV12073772; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Kpotufe Samory, 2018, ARXIV180301833; Kuzborskij Ilja, 2013, P 30 INT C MACH LEAR, P942; Mammen E, 1999, ANN STAT, V27, P1808; Mansour Y., 2009, P 25 C UNC ART INT, P367; Mansour Yishay, 2009, P COLT; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; Mohri Mehryar, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P124, DOI 10.1007/978-3-642-34106-9_13; Pal D., 2010, INT C ART INT STAT, P129; Saha A, 2011, LECT NOTES ARTIF INT, V6913, P97, DOI 10.1007/978-3-642-23808-6_7; Scott C., 2019, P 30 INT C ALG LEARN, V98, P1; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Sugiyama M., 2008, NIPS, P1433; Tsybakov AB, 2004, ANN STAT, V32, P135; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Vaart A. W., 1998, ASYMPTOTIC STAT; Vapnik V., 1974, THEORY PATTERN RECOG; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Yang L, 2013, MACH LEARN, V90, P161, DOI 10.1007/s10994-012-5310-y	35	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901049
C	Holland, MJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Holland, Matthew J.			PAC-Bayes under potentially heavy tails	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We derive PAC-Bayesian learning guarantees for heavy-tailed losses, and obtain a novel optimal Gibbs posterior which enjoys finite-sample excess risk bounds at logarithmic confidence. Our core technique itself makes use of PAC-Bayesian inequalities in order to derive a robust risk estimator, which by design is easy to compute. In particular, only assuming that the first three moments of the loss distribution are bounded, the learning algorithm derived from this estimator achieves nearly sub-Gaussian statistical error, up to the quality of the prior.	[Holland, Matthew J.] Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan	Osaka University	Holland, MJ (corresponding author), Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan.	matthew-h@ar.sanken.osaka-u.ac.jp			JSPS KAKENHI [18H06477]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was partially supported by the JSPS KAKENHI Grant Number 18H06477.	Alquier P., 2016, JMLR, V17, P8374; Alquier P, 2018, MACH LEARN, V107, P887, DOI 10.1007/s10994-017-5690-0; [Anonymous], 2004, ARXIVCS0411099; Begin L, 2016, JMLR WORKSH CONF PRO, V51, P435; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Catoni O., 2017, ARXIV171202747; Catoni O., 2004, LECT NOTES MATH, V1851; Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454; Devroye L, 2016, ANN STAT, V44, P2695, DOI 10.1214/16-AOS1440; Germain P., 2016, ADV NEURAL INFORM PR, P1884; Holland MJ, 2019, PR MACH LEARN RES, V89, P703; McAllester D., 2013, ARXIV PREPRINT ARXIV; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Seeger M., 2002, J MACHINE LEARNING R, P233; Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334; Shawe-Taylor J., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P68, DOI 10.1145/238061.238070; Tolstikhin Ilya O., 2013, ADV NEURAL INFORM PR, P109; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972	19	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302068
C	Hou, L; Zhu, JH; Kwok, JT; Gao, F; Qin, T; Liu, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hou, Lu; Zhu, Jinhua; Kwok, James T.; Gao, Fei; Qin, Tao; Liu, Tie-yan			Normalization Helps Training of Quantized LSTM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The long-short-term memory (LSTM), though powerful, is memory and computation expensive. To alleviate this problem, one approach is to compress its weights by quantization. However, existing quantization methods usually have inferior performance when used on LSTMs. In this paper, we first show theoretically that training a quantized LSTM is difficult because quantization makes the exploding gradient problem more severe, particularly when the LSTM weight matrices are large. We then show that the popularly used weight/layer/batch normalization schemes can help stabilize the gradient magnitude in training quantized LSTMs. Empirical results show that the normalized quantized LSTMs achieve significantly better results than their unnormalized counterparts. Their performance is also comparable with the full-precision LSTM, while being much smaller in size.	[Hou, Lu; Kwok, James T.] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China; [Zhu, Jinhua] Univ Sci & Technol China, Hefei, Peoples R China; [Gao, Fei; Qin, Tao; Liu, Tie-yan] Microsoft Res, Beijing, Peoples R China	Hong Kong University of Science & Technology; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft	Hou, L (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.	lhouab@cse.ust.hk; teslazhu@mail.ustc.edu.cn; jamesk@cse.ust.hk; feiga@microsoft.com; taoqin@microsoft.com; tyliu@microsoft.com		Qin, Tao/0000-0002-9095-0776	Microsoft Research Asia	Microsoft Research Asia(Microsoft)	This research project is partially funded by Microsoft Research Asia.	[Anonymous], PENN TREEBANK OVERVI, DOI DOI 10.1007/978-94-010-0201-1_1; Ardakani A., 2019, INT C LEARN REPR; Ba L.J, 2016, P C WORKSH NEUR INF; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Cooijmans Tim, 2016, INT C LEARN REPR; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Graves A, 2012, STUD COMPUT INTELL, V385, P5; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He Q., 2016, P CVPR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hou L., 2017, INT C LEARN REPR; Hou Lu, 2018, INT C LEARN REPR; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Karpathy A., 2016, ARXIV CSLG; Kingma D.P., 2015, INT C LEARN REPR, P1; Le Q.V., 2015, CORR, Vabs/1504.00941; Li Fengfu, 2016, ARXIV; Li Z., 2018, INT C MACH LEARN; Lin Z., 2016, INT C LEARN REPR; Mikolov T., 2010, ANN C INT SPEECH COM; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Polino Antonio, 2018, P REPR INT C LEARN; Rastegari M., 2016, EUR C COMP VIS; Santurkar S., 2018, NEURAL INFORM PROCES; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Sutskever I., 2014, ARXIV; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Xu C., 2018, INT C LEARN REPR	30	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307037
C	Hsieh, YG; Iutzeler, F; Malick, J; Mertikopoulos, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hsieh, Yu-Guan; Iutzeler, Franck; Malick, Jerome; Mertikopoulos, Panayotis			On the Convergence of Single-Call Stochastic Extra-Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIATIONAL-INEQUALITIES	Variational inequalities have recently attracted considerable interest in machine learning as a flexible paradigm for models that go beyond ordinary loss function minimization (such as generative adversarial networks and related deep learning systems). In this setting, the optimal O(1/t) convergence rate for solving smooth monotone variational inequalities is achieved by the Extra-Gradient (EG) algorithm and its variants. Aiming to alleviate the cost of an extra gradient step per iteration (which can become quite substantial in deep learning applications), several algorithms have been proposed as surrogates to Extra-Gradient with a single oracle call per iteration. In this paper, we develop a synthetic view of such algorithms, and we complement the existing literature by showing that they retain a O(1/t) ergodic convergence rate in smooth, deterministic problems. Subsequently, beyond the monotone deterministic case, we also show that the last iterate of single-call, stochastic extra-gradient methods still enjoys a O(1/t) local convergence rate to solutions of non-monotone variational inequalities that satisfy a second-order sufficient condition.	[Hsieh, Yu-Guan; Iutzeler, Franck] Univ Grenoble Alpes, LJK, F-38000 Grenoble, France; [Hsieh, Yu-Guan] ENS Paris, F-38000 Grenoble, France; [Malick, Jerome] CNRS, LJK, F-38000 Grenoble, France; [Mertikopoulos, Panayotis] Univ Grenoble Alpes, Grenoble INP, CNRS, INRIA,LIG, F-38000 Grenoble, France	Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS)	Hsieh, YG (corresponding author), Univ Grenoble Alpes, LJK, F-38000 Grenoble, France.; Hsieh, YG (corresponding author), ENS Paris, F-38000 Grenoble, France.	yu-guan.hsieh@ens.fr; franck.iutzeler@univ-grenoble-alpes.fr; jerome.malick@univ-grenoble-alpes.fr; panayotis.mertikopoulos@imag.fr			MIAI Grenoble Alpes (Multidisciplinary Institute in Artificial Intelligence); French National Research Agency (ANR) grant ORACLESS [ANR-16-CE33-0004-01]; EU COST Action "European Network for Game Theory" (GAMENET) [CA16228]	MIAI Grenoble Alpes (Multidisciplinary Institute in Artificial Intelligence); French National Research Agency (ANR) grant ORACLESS(French National Research Agency (ANR)); EU COST Action "European Network for Game Theory" (GAMENET)	This work benefited from financial support by MIAI Grenoble Alpes (Multidisciplinary Institute in Artificial Intelligence). P. Mertikopoulos was partially supported by the French National Research Agency (ANR) grant ORACLESS (ANR-16-CE33-0004-01) and the EU COST Action CA16228 "European Network for Game Theory" (GAMENET).	Adolphs Leonard, 2019, AISTATS 19 P 22 INT; Bach Francis, 2019, COLT 19 P 32 ANN C L; Balduzzi David, 2018, ICML 18 P 35 INT C M; Bauschke HH, 2017, CONVEX ANAL MONOTONE, DOI 10.1007/978-3-319-48311-5; Bertsekas D. P., 1997, J OPER RES SOC, V48, P334, DOI DOI 10.1057/PALGRAVE.JORS.2600425; Bot Radu Ioan, 2019, FORWARD BACKWARD FOR; Bubeck S., 2015, FDN TRENDS MACHINE L; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chavdarova Tatjana, 2019, REDUCING NOISE GAN T; Chiang Chao-Kai, 2012, COLT 12 P 25 ANN C L; Combettes P.L., 2001, INHERENTLY PARALLEL, P115, DOI DOI 10.1016/S1570-579X(01)80010-0; Combettes PL, 2015, SIAM J OPTIMIZ, V25, P1221, DOI 10.1137/140971233; Cui Shisheng, 2016, CDC 16 P 57 IEEE ANN; Daskalakis Constantinos, 2018, ICLR 18 P 2018 INT C; Daskalakis Constantinos, 2018, NIPS 18 P 31 INT C N; Facchinei F., 2003, SPRINGER SERIES OPER; Facchinei F, 2007, 4OR-Q J OPER RES, V5, P173, DOI 10.1007/s10288-007-0054-4; Gidel Gauthier, 2019, ICLR 19 P 2019 INT C; Gidel Gauthier, 2019, AISTATS 19 P 22 INT; Goodfellow Ian J., 2014, NIPS 14 P 27 INT C N; Juditsky A., 2011, STOCHASTIC SYST, V1, P17, DOI 10.1287/10-SSY011; Koepelevich, 1976, EKONOMIKA MATEMATICH, V12, P747; Liang Tengyuan, 2019, AISTATS 19 P 22 INT; Malitsky Y., 2019, MATH PROGRAM, P1; Malitsky Y, 2015, SIAM J OPTIMIZ, V25, P502, DOI 10.1137/14097238X; Mazumdar E. V., 2019, FINDING LOCAL NASH E; Mertikopoulos P, 2019, MATH PROGRAM, V173, P465, DOI 10.1007/s10107-018-1254-8; Mertikopoulos Panayotis, 2019, ICLR 19 P 2019 INT C; Mokhtari Aryan, 2019, CONVERGENCE RATE 1 K; Mokhtari Aryan, 2019, UNIFIED ANAL EXTRAGR; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; Peng Wei, 2019, TRAINING GANS CENTRI; POPOV LD, 1980, MATH NOTES+, V28, P845, DOI 10.1007/BF01141092; Rakhlin Alexander, 2013, COLT 13 P 26 ANN C L; Rakhlin Alexander, 2013, NIPS 13 P 26 INT C N; Ratliff LJ, 2013, ANN ALLERTON CONF, P917, DOI 10.1109/Allerton.2013.6736623; ROSEN JB, 1965, ECONOMETRICA, V33, P520, DOI 10.2307/1911749; Tseng P, 2000, SIAM J CONTROL OPTIM, V38, P431, DOI 10.1137/S0363012998338806; TSENG P, 1995, J COMPUT APPL MATH, V60, P237, DOI 10.1016/0377-0427(94)00094-H; Yadav Abhay, 2018, ICLR 18 P 2018 INT C	44	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306089
C	Hu, J; Ji, RR; Zhang, SC; Sun, XS; Ye, QX; Lin, CW; Tian, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Jie; Ji, Rongrong; Zhang, ShengChuan; Sun, Xiaoshuai; Ye, Qixiang; Lin, Chia-Wen; Tian, Qi			Information Competing Process for Learning Diversified Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning representations with diversified information remains as an open problem. Towards learning diversified representations, a new approach, termed Information Competing Process (ICP), is proposed in this paper. Aiming to enrich the information carried by feature representations, ICP separates a representation into two parts with different mutual information constraints. The separated parts are forced to accomplish the downstream task independently in a competitive environment which prevents the two parts from learning what each other learned for the downstream task. Such competing parts are then combined synergistically to complete the task. By fusing representation parts learned competitively under different conditions, ICP facilitates obtaining diversified representations which contain rich information. Experiments on image classification and image reconstruction tasks demonstrate the great potential of ICP to learn discriminative and disentangled representations in both supervised and self-supervised learning settings. (1)	[Hu, Jie; Ji, Rongrong; Zhang, ShengChuan; Sun, Xiaoshuai] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen, Peoples R China; [Hu, Jie; Ji, Rongrong] Xiamen Univ, Natl Inst Data Sci Hlth & Med, Xiamen, Peoples R China; [Ji, Rongrong] Peng Cheng Lab, Shenzhen, Peoples R China; [Ye, Qixiang] Univ Chinese Acad Sci, Beijing, Peoples R China; [Lin, Chia-Wen] Natl Tsing Hua Univ, Hsinchu, Taiwan; [Tian, Qi] Huawei, Noahs Ark Lab, Shenzhen, Peoples R China	Xiamen University; Xiamen University; Peng Cheng Laboratory; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; National Tsing Hua University; Huawei Technologies	Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen, Peoples R China.; Ji, RR (corresponding author), Xiamen Univ, Natl Inst Data Sci Hlth & Med, Xiamen, Peoples R China.; Ji, RR (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.		Lin, Chia-Wen/ABH-6075-2020; Lin, Chia-Wen/M-4571-2013	Lin, Chia-Wen/0000-0002-9097-2318	National Key RD Program [2017YFC0113000, 2016YFB1001503]; Nature Science Foundation of China [61772443, 61802324, 61572410, 61702136]; Nature Science Foundation of Fujian Province, China [2017J01125, 2018J01106]	National Key RD Program; Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); Nature Science Foundation of Fujian Province, China(Natural Science Foundation of Fujian Province)	This work is supported by the National Key R&D Program (No.2017YFC0113000, and No.2016YFB1001503), Nature Science Foundation of China (No.U1705262, No.61772443, No. 61802324, No.61572410 and No.61702136), and Nature Science Foundation of Fujian Province, China (No. 2017J01125 and No. 2018J01106).	Alemi Alex, 2017, ICLR; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Belghazi MI, 2018, PR MACH LEARN RES, V80; Bell Anthony J, 1995, NEURAL COMPUTATION; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Burgess CP, 2018, ARXIV180403599; Chen Fuhai, 2019, ADV NEURAL INFORM PR; Chen T.Q., 2018, NEURIPS, P2610; Dapello J, 2018, INFORM BOTTLENECK TH; Greff K., 2016, ADV NEURAL INFORM PR; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Higgins I., 2017, P INT C LEARN REPR T; Hjelm R Devon, 2019, INT C LEARN REPR; Hu Jie, 2019, P IEEE C COMP VIS PA; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kolesnikov Alexander, 2019, P IEEE C COMP VIS PA; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Linsker Ralph, 1988, COMPUTER; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Nowozin S, 2016, ADV NEUR IN, V29; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranjan Anurag, 2019, P IEEE C COMP VIS PA; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tishby Naftali, 2000, PHYSICS0004057 ARXIV; van den Oord Aaron, 2018, ARXIV180703748; van Steenkiste Sjoerd, 2017, ADV NEURAL INFORM PR, P6691; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zhang XS, 2019, ADV NEUR IN, V32	40	4	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302020
C	Husain, H; Nock, R; Williamson, RC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Husain, Hisham; Nock, Richard; Williamson, Robert C.			A Primal-Dual link between GANs and Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Since the introduction of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAE), the literature on generative modelling has witnessed an overwhelming resurgence. The impressive, yet elusive empirical performance of GANs has lead to the rise of many GAN-VAE hybrids, with the hopes of GAN level performance and additional benefits of VAE, such as an encoder for feature reduction, which is not offered by GANs. Recently, the Wasserstein Autoencoder (WAE) was proposed, achieving performance similar to that of GANs, yet it is still unclear whether the two are fundamentally different or can be further improved into a unified model. In this work, we study the f-GAN and WAE models and make two main discoveries. First, we find that the f-GAN and WAE objectives partake in a primal-dual relationship and are equivalent under some assumptions, which then allows us to explicate the success of WAE. Second, the equivalence result allows us to, for the first time, prove generalization bounds for Autoencoder models, which is a pertinent problem when it comes to theoretical analyses of generative models. Furthermore, we show that the WAE objective is related to other statistical quantities such as the f-divergence and in particular, upper bounded by the Wasserstein distance, which then allows us to tap into existing efficient (regularized) optimal transport solvers. Our findings thus present the first primal-dual relationship between GANs and Autoencoder models, comment on generalization abilities and make a step towards unifying these models.	[Husain, Hisham; Nock, Richard; Williamson, Robert C.] Australian Natl Univ, Canberra, ACT, Australia; [Husain, Hisham; Nock, Richard; Williamson, Robert C.] Data61, Sydney, NSW, Australia; [Nock, Richard] Univ Sydney, Sydney, NSW, Australia	Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO); University of Sydney	Husain, H (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.; Husain, H (corresponding author), Data61, Sydney, NSW, Australia.	hisham.husain@data61.csiro.au; richard.nock@data61.csiro.au; robert.williamson@data61.csiro.au	Husain, Hisham/AAO-7898-2021					Alanov Aibek, 2018, ARXIV181004920; Alemi AA, 2018, PR MACH LEARN RES, V80; Amari SI, 2016, APPL MATH SCI, V194, P1, DOI 10.1007/978-4-431-55978-8; [Anonymous], 2016, P WORKSH ADV APPR BA; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Borland L, 1998, PHYS LETT A, V245, P67, DOI 10.1016/S0375-9601(98)00467-8; Dumoulin Vincent, 2016, ARXIV E PRINTS; Falconer K.J., 2014, FRACTAL GEOMETRY MAT, V3rd ed.; Farnia F., 2018, ADV NEURAL INFORM PR, P5254; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Higgins I, 2016, BETA VAE LEARNING BA; Hu Z., 2017, CORR, Vabs/1706.00550; Kingma D. P., 2013, AUTO ENCODING VARIAT; Li Ke, 2018, ARXIV181112402; Liu S., 2018, ARXIV180904542; Liu S, 2017, I C COMM SOFTW NET, P454; Makhzani A., 2015, ARXIV151105644; Mescheder L, 2017, PR MACH LEARN RES, V70; Mohamed Shakir, 2016, ARXIV161003483; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Nock Richard, 2017, NIPS, P456; Nowozin S, 2016, ADV NEUR IN, V29; Patrini G., 2018, ARXIV181001118; Sriperumbudur B. K., 2009, ARXIV PREPRINT ARXIV; Tolstikhin Ilya, 2017, ARXIV171101558; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Weed J, 2017, ARXIV170700087; Zhang P., 2017, ARXIV171102771; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490; Zhou Zhiming, 2018, UNDERSTANDING EFFECT	32	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300038
C	Indyk, P; Vakilian, A; Yuan, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Indyk, Piotr; Vakilian, Ali; Yuan, Yang			Learning-Based Low-Rank Approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	We introduce a "learning-based" algorithm for the low-rank decomposition problem: given an n x d matrix A, and a parameter k, compute a rank-k matrix A' that minimizes the approximation loss parallel to A - A'parallel to(F). The algorithm uses a training set of input matrices in order to optimize its performance. Specifically, some of the most efficient approximate algorithms for computing low-rank approximations proceed by computing a projection SA, where S is a sparse random m x n "sketching matrix", and then performing the singular value decomposition of SA. We show how to replace the random matrix S with a "learned" matrix of the same sparsity to reduce the error. Our experiments show that, for multiple types of data sets, a learned sketch matrix can substantially reduce the approximation loss compared to a random matrix S, sometimes by one order of magnitude. We also study mixed matrices where only some of the rows are trained and the remaining ones are random, and show that matrices still offer improved performance while retaining worst-case guarantees. Finally, to understand the theoretical aspects of our approach, we study the special case of m = 1. In particular, we give an approximation algorithm for minimizing the empirical loss, with approximation factor depending on the stable rank of matrices in the training set. We also show generalization bounds for the sketch matrix learning problem.	[Indyk, Piotr; Vakilian, Ali; Yuan, Yang] MIT, CSAIL, Cambridge, MA 02139 USA; [Vakilian, Ali] Univ Wisconsin, Madison, WI USA; [Yuan, Yang] Tsinghua Univ, Beijing, Peoples R China	Massachusetts Institute of Technology (MIT); University of Wisconsin System; University of Wisconsin Madison; Tsinghua University	Indyk, P (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	indyk@mit.edu; vakilian@wisc.edu; yuanyang@tsinghua.edu.cn			NSF TRIPODS award [1740751]; Simons Investigator Award	NSF TRIPODS award; Simons Investigator Award	This research was supported by NSF TRIPODS award #1740751 and Simons Investigator Award. The authors would like to thank the anonymous reviewers for their insightful comments and suggestions.	Allen-Zhu Z., 2016, ADV NEURAL INFORM PR, P974; Balcan MF, 2018, PR MACH LEARN RES, V80; Baldassarre L, 2016, IEEE J-STSP, V10, P809, DOI 10.1109/JSTSP.2016.2548442; Bora A, 2017, PR MACH LEARN RES, V70; Boutsidis C, 2013, SIAM J MATRIX ANAL A, V34, P1301, DOI 10.1137/120874540; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Clarkson KL, 2017, J ACM, V63, DOI 10.1145/3019134; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Davidov D., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P250, DOI 10.1145/1008992.1009036; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Ghashami M., 2014, P 25 ANN ACM SIAM S, P707, DOI [DOI 10.1137/1.9781611973402.53, 10.1137/1.9781611973402.53]; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Gollapudi S, 2019, PR MACH LEARN RES, V97; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hand Paul, 2018, P 31 C LEARN THEOR, P970; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Hsu C.-Y., 2019, ICLR, P1; Imamoglu NR, 2018, INT WORK QUAL MULTIM, P165; Khalil Elias, 2017, ADV NEURAL INFORM PR, P1; Khani M., 2019, ABS190604610 CORR; Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Lykouris T, 2018, PR MACH LEARN RES, V80; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Metzler CA, 2017, ADV NEUR IN, V30; Mitzenmacher M, 2018, ADV NEUR IN, V31; Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Purohit Manish, 2018, ADV NEURAL INFORM PR, P9684; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Woolfe F, 2008, APPL COMPUT HARMON A, V25, P335, DOI 10.1016/j.acha.2007.12.002	35	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307042
C	Jonas, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jonas, Eric			Deep imitation learning for molecular inverse problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CHEMICAL-SHIFTS; PREDICTION	Many measurement modalities arise from well-understood physical processes and result in information-rich but difficult-to-interpret data. Much of this data still requires laborious human interpretation. This is the case in nuclear magnetic resonance (NMR) spectroscopy, where the observed spectrum of a molecule provides a distinguishing fingerprint of its bond structure. Here we solve the resulting inverse problem: given a molecular formula and a spectrum, can we infer the chemical structure? We show for a wide variety of molecules we can quickly compute the correct molecular structure, and can detect with reasonable certainty when our method fails. We treat this as a problem of graph-structured prediction where, armed with per-vertex information on a subset of the vertices, we infer the edges and edge types. We frame the problem as a Markov decision process (MDP) and incrementally construct molecules one bond at a time, training a deep neural network via imitation learning, where we learn to imitate a subisomorphic oracle which knows which remaining bonds are correct. Our method is fast, accurate, and is the first among recent chemical-graph generation approaches to exploit per-vertex information and generate graphs with vertex constraints. Our method points the way towards automation of molecular structure identification and active learning for spectroscopy.	[Jonas, Eric] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA	University of Chicago	Jonas, E (corresponding author), Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.	ericj@uchicago.edu						Adams David C, 2018, ENC; Battaglia Peter W, 2018, ARXIV180601261; Cao N. D., 2018, MOLGAN IMPLICIT GENE; Chang Kai-Wei, 2014, P 32 INT C MACH LEAR, V37, P3; Chang Kai-Wei, 2014, CREDIT ASSIGNMENT CO; Chen CZ, 2012, COPD, V9, P197, DOI 10.3109/15412555.2011.654143; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Gregor Karol, 2010, ICML, V152, P318; Hoffmann F, 2017, J PHYS CHEM A, V121, P3071, DOI 10.1021/acs.jpca.7b01954; Jin W., 2018, JUNCTION TREE VARIAT; Jonas E, 2019, J CHEMINFORMATICS, V11, DOI 10.1186/s13321-019-0374-3; Juttner A, 2018, DISCRETE APPL MATH, V242, P69, DOI 10.1016/j.dam.2018.02.018; Kabanikhin SI, 2008, J INVERSE ILL-POSE P, V16, P317, DOI 10.1515/JIIP.2008.019; Kipf T. N., 2016, SEMI SUPERVISED CLAS, P1; Kitamura S., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), DOI 10.1109/IJCNN.1989.118365; Krivdin LB, 2017, PROG NUCL MAG RES SP, V102, P98, DOI 10.1016/j.pnmrs.2017.08.001; Kuhn S, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-400; Landrum G., 2006, RDKIT OPEN SOURCE CH; Lederberg J., 1987, P ACM C HIST MED INF, V1945, P5; Liu HY, 2015, OPT EXPRESS, V23, P14461, DOI 10.1364/OE.23.014461; Lodewyk MW, 2012, CHEM REV, V112, P1839, DOI 10.1021/cr200106v; McCarty M, 2017, INTERNATIONAL CONFERENCE ON SUSTAINABLE INFRASTRUCTURE 2017: TECHNOLOGY, P1; Ross St<prime>ephane, 2011, AISTATS; Simonovsky Martin, 2017, GRAPHVAE GENERATION; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a; Yang Y, 2016, ARCH ENVIRON OCCUP H, V10, P1, DOI DOI 10.1016/J.J0T.2016.10.001; Zhou CY, 2011, IEEE T IMAGE PROCESS, V20, P3322, DOI 10.1109/TIP.2011.2171700; Zhou T, 2018, ESTIMATION AND CONTROL OF LARGE-SCALE NETWORKED SYSTEMS, P1, DOI 10.1016/B978-0-12-805311-9.00001-4; Zhu PIsolaJun-Yan, 2018, ARXIV	31	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305004
C	Kagrecha, A; Nair, J; Jagannathan, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kagrecha, Anmol; Nair, Jayakrishnan; Jagannathan, Krishna			Distribution Oblivious, Risk-Aware Algorithms for Multi-Armed Bandits with Unbounded Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Classical multi-armed bandit problems use the expected value of an arm as a metric to evaluate its goodness. However, the expected value is a risk-neutral metric. In many applications like finance, one is interested in balancing the expected return of an arm (or portfolio) with the risk associated with that return. In this paper, we consider the problem of selecting the arm that optimizes a linear combination of the expected reward and the associated Conditional Value at Risk (CVaR) in a fixed budget best-arm identification framework. We allow the reward distributions to be unbounded or even heavy-tailed. For this problem, our goal is to devise algorithms that are entirely distributionoblivious, i.e., the algorithm is not aware of any information on the reward distributions, including bounds on the moments/tails, or the suboptimality gaps across arms. In this paper, we provide a class of such algorithms with provable upper bounds on the probability of incorrect identification. In the process, we develop a novel estimator for the CVaR of unbounded (including heavy-tailed) distributions and prove a concentration inequality for the same, which could be of independent interest. We also compare the error bounds for our distribution oblivious algorithms with those corresponding to standard non-oblivious algorithms. Finally, numerical experiments reveal that our algorithms perform competitively when compared with non-oblivious algorithms, suggesting that distribution obliviousness can be realised in practice without incurring a significant loss of performance.	[Kagrecha, Anmol; Nair, Jayakrishnan] Indian Inst Technol, Mumbai, Maharashtra, India; [Jagannathan, Krishna] IIT Madras, Chennai, Tamil Nadu, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Bombay; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Kagrecha, A (corresponding author), Indian Inst Technol, Mumbai, Maharashtra, India.	akagrecha@gmail.com; jayakrishnan.nair@ee.iitb.ac.in; krishnaj@ee.iitm.ac.in			Google	Google(Google Incorporated)	The first author is grateful to Google for a generous travel award to enable him to present this work at NeurIPS 2019.	Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068; Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; BICKEL PJ, 1965, ANN MATH STAT, V36, P847, DOI 10.1214/aoms/1177700058; Bradley BO, 2003, HANDBOOKS FINANCE, P35, DOI 10.1016/B978-044450896-6.50004-2; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Carpentier A, 2014, ADV NEUR IN, V27; David Y., 2016, JOINT EUR C MACH LEA, P556; David Y., 2018, ISAIM; Galichet N., 2013, AS C MACH LEARN, P245; Kolla RK, 2019, OPER RES LETT, V47, P16, DOI 10.1016/j.orl.2018.11.005; Lattimore Tor, 2018, PREPRINT; Prashanth LA, 2019, ARXIV190100997; Sani A., 2012, ADV NEURAL INFORM PR, V25, P3275; Vakili S, 2013, IEEE J-STSP, V7, P759, DOI 10.1109/JSTSP.2013.2263494; Wang Y, 2010, OPER RES LETT, V38, P236, DOI 10.1016/j.orl.2009.11.008; Yu XT, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P937	17	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902085
C	Kallus, N; Zhou, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kallus, Nathan; Zhou, Angela			The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the xAUC Metric	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Where machine-learned predictive risk scores inform high-stakes decisions, such as bail and sentencing in criminal justice, fairness has been a serious concern. Recent work has characterized the disparate impact that such risk scores can have when used for a binary classification task. This may not account, however, for the more diverse downstream uses of risk scores and their non-binary nature. To better account for this, in this paper, we investigate the fairness of predictive risk scores from the point of view of a bipartite ranking task, where one seeks to rank positive examples higher than negative ones. We introduce the xAUC disparity as a metric to assess the disparate impact of risk scores and define it as the difference in the probabilities of ranking a random positive example from one protected group above a negative one from another group and vice versa. We provide a decomposition of bipartite ranking loss into components that involve the discrepancy and components that involve pure predictive ability within each group. We use xAUC analysis to audit predictive risk scores for recidivism prediction, income prediction, and cardiac arrest prediction, where it describes disparities that are not evident from simply comparing within-group predictive performance.	[Kallus, Nathan; Zhou, Angela] Cornell Univ, New York, NY 10021 USA	Cornell University	Kallus, N (corresponding author), Cornell Univ, New York, NY 10021 USA.	kallus@cornell.edu; az434@cornell.edu			National Science Foundation [1846210]; JPMorgan Chase Co.	National Science Foundation(National Science Foundation (NSF)); JPMorgan Chase Co.	This material is based upon work supported by the National Science Foundation under Grant No. 1846210. This research was funded in part by JPMorgan Chase & Co. Any views or opinions expressed herein are solely those of the authors listed, and may differ from the views and opinions expressed by JPMorgan Chase & Co. or its affiliates. This material is not a product of the Research Department of J.P. Morgan Securities LLC. This material should not be construed as an individual recommendation for any particular client and is not intended as a recommendation of particular securities, financial instruments or strategies for a particular client. This material does not constitute a solicitation or offer in any jurisdiction.	Agarwal S., 2005, P 18 ANN C LEARN THE; Angwin J., 2016, MACHINE BIAS; Barabas C., 2017, P MACHINE LEARNING R; Barocas S., 2014, CALIFORNIA LAW REV; Barocas Solon, 2018, FAIRNESS MACHINE LEA; Bonta J., 2007, REHABILITATION; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Celis L.E., 2018, LIPICS, V107; Chan C., 2018, MSOM; Chen JH, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P339, DOI 10.1145/3287560.3287594; Chojnacki A., 2017, P KDD 2017; Chouldechova A., 2018, FAT; Chouldechova A., 2016, P FATML; Corbett-Davies Sam, 2018, ARXIV180800023; Cortes C., 2003, P 16 INT C NEUR INF; DeLong E., 1988, BIOMETRICS; Dieterich W., 2016, TECHNICAL REPORT; Freund Y., 2003, J MACHINE LEARNING R, V4, P2003; Friedler SA, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P329, DOI 10.1145/3287560.3287589; Fuster A, 2018, PREDICTABLY UNEQUAL; Hand D. J., 2009, MACHINE LEARNING; Hanley J. A., 1982, RADIOLOGY; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hebert-Johnson U, 2018, PR MACH LEARN RES, V80; Holstein K., 2019, 2019 ACM CHI C HUM F; Jones J., 2011, AM J PREVENTIVE MED; Kallus N, 2018, P 35 INT C MACH LEAR, P2444; Kallus Nathan, 2019, ADV NEURAL INFORM PR; Kleinberg J.M., 2017, 8 INNOVATIONS THEORE, DOI [10.4230/LIPIcs.ITCS.2017.43, DOI 10.4230/LIPICS.ITCS.2017.43]; Kontokosta C. E., 2018, BLOOMB DAT GOOD EXCH; Liu L., 2018, GROUP CALIBRATION IS; Menon A., 2016, J MACHINE LEARNING R; Michael Feldman J. M. C. S. V., 2015, P KDD 2015; Mohri M., 2018, FDN MACHINE LEARNING; Narasimhan H., 2013, P NIPS 2013; Rajkomar A., 2018, ANN INTERNAL MED; Reilly B., 2006, ANN INTERNAL MED; Rudin C., 2010, MACHINE LEARNING; WILSON PWF, 1987, AM J CARDIOL, V59, pG91, DOI 10.1016/0002-9149(87)90165-2; Yadav AS, 2018, 2018 5TH IEEE UTTAR PRADESH SECTION INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS AND COMPUTER ENGINEERING (UPCON), P1201; Yang K., 2017, P SSDBM 17; Zafar M. B., 2017, P WWW 2017	43	4	4	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303043
C	Kishimoto, A; Buesser, B; Chen, B; Botea, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kishimoto, Akihiro; Buesser, Beat; Chen, Bei; Botea, Adi			Depth-First Proof-Number Search with Heuristic Edge Cost and Application to Chemical Synthesis Planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DEEP NEURAL-NETWORKS; TREE; GAME; GO	Search techniques, such as Monte Carlo Tree Search (MCTS) and Proof-Number Search (PNS), are effective in playing and solving games. However, the understanding of their performance in industrial applications is still limited. We investigate MCTS and Depth-First Proof-Number (DFPN) Search, a PNS variant, in the domain of Retrosynthetic Analysis (RA). We find that DFPN's strengths, that justify its success in games, have limited value in RA, and that an enhanced MCTS variant by Segler et al. significantly outperforms DFPN. We address this disadvantage of DFPN in RA with a novel approach to combine DFPN with Heuristic Edge Initialization. Our new search algorithm DFPN-E outperforms the enhanced MCTS in search time by a factor of 3 on average, with comparable success rates.	[Kishimoto, Akihiro; Buesser, Beat; Chen, Bei] IBM Res, Dublin, Ireland; [Botea, Adi] Eaton, Dublin, Ireland		Kishimoto, A (corresponding author), IBM Res, Dublin, Ireland.							ALLIS LV, 1994, ARTIF INTELL, V66, P91, DOI 10.1016/0004-3702(94)90004-3; [Anonymous], 1980, PRINCIPLES ARTIFICIA; Arneson B, 2011, LECT NOTES COMPUT SC, V6515, P1, DOI 10.1007/978-3-642-17928-0_1; Bonet B, 2001, ARTIF INTELL, V129, P5, DOI 10.1016/S0004-3702(01)00108-4; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Campbell M, 2002, ARTIF INTELL, V134, P57, DOI 10.1016/S0004-3702(01)00129-1; COREY E. J., 1967, PURE APPL CHEM, V14, P19, DOI 10.1351/pac196714010019; COREY EJ, 1969, SCIENCE, V166, P178, DOI 10.1126/science.166.3902.178; Gao C, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3668; Heifets A., 2012, AAAI, P1564; Helmert M, 2006, J ARTIF INTELL RES, V26, P191, DOI 10.1613/jair.1705; Kishimoto A, 2018, AAAI CONF ARTIF INTE, P7978; Kishimoto A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P400; Kishimoto A, 2010, AAAI CONF ARTIF INTE, P108; Kishimoto A, 2012, ICGA J, V35, P131, DOI 10.3233/ICG-2012-35302; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Law J, 2009, J CHEM INF MODEL, V49, P593, DOI 10.1021/ci800228y; Lowe D. M., 2012, THESIS U CAMBRIDGE, DOI [10.17863/CAM.16293, DOI 10.17863/CAM.16293]; Muller M., 2005, AAAI, P1374; Nagai A., 1999, GAM PROGR WORKSH JAP, P16; Nagai A, 2002, THESIS; Pawlewicz J, 2007, LECT NOTES COMPUT SC, V4630, P160; Rogers D, 2010, J CHEM INF MODEL, V50, P742, DOI 10.1021/ci100050t; Rosin C, 2011, ANN MATH ARTIF INTEL, V61, P203, DOI 10.1007/s10472-011-9258-6; Schaeffer J, 2007, SCIENCE, V317, P1518, DOI 10.1126/science.1144079; Schreck JS, 2019, ACS CENTRAL SCI, V5, P970, DOI 10.1021/acscentsci.9b00055; Segler MHS, 2018, NATURE, V555, P604, DOI 10.1038/nature25978; Segler Marwin H. S., 2017, CHEM-EUR J, V1521, P3765; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sylvain G., 2007, P 24 INT C MACH LEAR, P273; Szymkuc S, 2016, ANGEW CHEM INT EDIT, V55, P5904, DOI 10.1002/anie.201506101; Winands MHM, 2011, LECT NOTES COMPUT SC, V6515, P23, DOI 10.1007/978-3-642-17928-0_3; Winands MHM, 2008, LECT NOTES COMPUT SC, V5131, P25, DOI 10.1007/978-3-540-87608-3_3	35	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307026
C	Kumar, R; Purohit, M; Svitkina, Z; Vee, E; Wang, JR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumar, Ravi; Purohit, Manish; Svitkina, Zoya; Vee, Erik; Wang, Joshua R.			Efficient Rematerialization for Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM	When training complex neural networks, memory usage can be an important bottleneck. The question of when to rematerialize, i.e., to recompute intermediate values rather than retaining them in memory, becomes critical to achieving the best time and space efficiency. In this work we consider the rematerialization problem and devise efficient algorithms that use structural characterizations of computation graphs-treewidth and pathwidth-to obtain provably efficient rematerialization schedules. Our experiments demonstrate the performance of these algorithms on many common deep learning models.	[Kumar, Ravi; Purohit, Manish; Svitkina, Zoya; Vee, Erik; Wang, Joshua R.] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Kumar, R (corresponding author), Google Res, Mountain View, CA 94043 USA.	ravi.k53@gmail.com; mpurohit@google.com; zoya@google.com; erikvee@google.com; oshuawang@google.com	Purohit, Manish/ABD-3458-2021					Beaumont Olivier, 2019, RR9273 INR; Bodlaender HL, 2016, SIAM J COMPUT, V45, P317, DOI 10.1137/130947374; Bodlaender HL, 2010, INFORM COMPUT, V208, P259, DOI 10.1016/j.ic.2009.03.008; Chen T, 2015, 151201274 ARXIV; Chen Tianqi, 2016, 160406174 ARXIV; COURCELLE B, 1990, INFORM COMPUT, V85, P12, DOI 10.1016/0890-5401(90)90043-H; Feng Jianwei, 2018, 180800079 ARXIV; Gilbert John R., 1979, STOC, P237; Griewank A, 2000, ACM T MATH SOFTWARE, V26, P19, DOI 10.1145/347837.347846; Gruslys A, 2016, PROC 30 INT C NEURAL, P4132, DOI [10.5555/3157382.3157559, DOI 10.5555/3157382.3157559]; Halevy AY, 2001, VLDB J, V10, P270, DOI 10.1007/s007780100054; Jordan C., 1869, J F R REINE ANGEW MA, V70, P185, DOI DOI 10.1515/CRLL.1869.70.185; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kusumoto Mitsuru, 2019, NEURIPS; Meng Chen, 2017, P ML SYST WORKSH NEU; Rhu Minsoo, 2016, 49 ANN IEEE ACM INT, P18; Sethi R., 1975, SIAM Journal on Computing, V4, P226, DOI 10.1137/0204020; Shirahata Koichi, 2016, WORKSH MACH LEARN SI, P1; Thorup M, 1998, INFORM COMPUT, V142, P159, DOI 10.1006/inco.1997.2697; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; [No title captured]	21	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906079
C	Li, XC; Wu, D; Mackey, L; Erdogdu, MA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Xuechen; Wu, Denny; Mackey, Lester; Erdogdu, Murat A.			Stochastic Runge-Kutta Accelerates Langevin Monte Carlo and Beyond	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATIONS; STATISTICS	Sampling with Markov chain Monte Carlo methods often amounts to discretizing some continuous-time dynamics with numerical integration. In this paper, we establish the convergence rate of sampling algorithms obtained by discretizing smooth Ito diffusions exhibiting fast Wasserstein-2 contraction, based on local deviation properties of the integration scheme. In particular, we study a sampling algorithm constructed by discretizing the overdamped Langevin diffusion with the method of stochastic Runge-Kutta. For strongly convex potentials that are smooth up to a certain order, its iterates converge to the target distribution in 2-Wasserstein distance in (O) over tilde (d epsilon(-2/3)) iterations. This improves upon the best-known rate for strongly log-concave sampling based on the overdamped Langevin equation using only the gradient oracle without adjustment. In addition, we extend our analysis of stochastic Runge-Kutta methods to uniformly dissipative diffusions with possibly non-convex potentials and show they achieve better rates compared to the Euler-Maruyama scheme in terms of the dependence on tolerance epsilon. Numerical studies show that these algorithms lead to better stability and lower asymptotic errors.	[Li, Xuechen; Wu, Denny; Erdogdu, Murat A.] Univ Toronto, Toronto, ON, Canada; [Li, Xuechen; Wu, Denny; Erdogdu, Murat A.] Vector Inst, Hyderabad, India; [Mackey, Lester] Microsoft Res, Cambridge, MA USA	University of Toronto; Microsoft	Li, XC (corresponding author), Univ Toronto, Toronto, ON, Canada.; Li, XC (corresponding author), Vector Inst, Hyderabad, India.	lxuechen@cs.toronto.edu; dennywu@cs.toronto.edu; lmackey@microsoft.com; erdogdu@cs.toronto.edu			NSERC [2019-06167]; CIFAR AI Chairs program at the Vector Institute	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR AI Chairs program at the Vector Institute	MAE is partially funded by NSERC [2019-06167] and CIFAR AI Chairs program at the Vector Institute.	Anderson David F, 2009, ARXIV09063475; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Burrage K, 2004, P ROY SOC A-MATH PHY, V460, P373, DOI 10.1098/rspa.2003.1247; Calogero S, 2012, COMMUN PART DIFF EQ, V37, P1357, DOI 10.1080/03605302.2011.648039; CHEN CY, 2015, ADV NEURAL INFORM PR, P2722, DOI DOI 10.1109/ICCV.2015.312; Chen Y., 2019, ARXIV190512247; Cheng X., 2018, ARXIV180501648; Cheng X., 2017, ARXIV170509048; Cheng X., 2017, ARXIV170703663; Chwialkowski Kacper, 2016, JMLR WORKSHOP C P; Dalalyan A., 2019, STOCHASTIC PROCESSES; Dalalyan AS, 2012, J COMPUT SYST SCI, V78, P1423, DOI 10.1016/j.jcss.2011.12.023; Dalalyan A. S., 2018, ARXIV180709382; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; DOBRIC V, 1995, J THEOR PROBAB, V8, P97, DOI 10.1007/BF02213456; DUDLEY RM, 1969, ANN MATH STAT, V40, P40, DOI 10.1214/aoms/1177697802; Durmus A., 2016, ARXIV160501559; DURMUS A., 2016, ADV NEURAL INFORM PR, P2047; Durmus A, 2018, SIAM J IMAGING SCI, V11, P473, DOI 10.1137/16M1108340; Dwivedi R., 2018, ARXIV180102309; Eberle A., 2017, ARXIV170301617; Eberle A, 2016, PROBAB THEORY REL, V166, P851, DOI 10.1007/s00440-015-0673-1; Erdogdu M.A., 2018, ADV NEURAL INFORM PR, P9694; Flamary R<prime>emi, 2017, POT PYTHON OPTIMAL T; GELFAND SB, 1991, SIAM J CONTROL OPTIM, V29, P999, DOI 10.1137/0329055; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Gentil I, 2015, ARXIV151008230; Gorham J., 2016, ARXIV161106972; Gorham J, 2017, PR MACH LEARN RES, V70; Gretton A, 2012, J MACH LEARN RES, V13, P723; KLOEDEN PE, 1992, STOCH ANAL APPL, V10, P431, DOI 10.1080/07362999208809281; Kloeden Peter E., 2013, NUMERICAL SOLUTION S, V23; Kuznetsov Dmitriy F, 2018, ARXIV180204844; Laurent Adrien, 2017, ARXIV170702877; Liu Q, 2016, PR MACH LEARN RES, V48; Ma Y, 2019, ARXIV190708990; Ma Y.-A., 2018, ARXIV181108413; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Mao X., 2007, STOCHASTIC DIFFERENT; Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Meyn S. P., 2012, MARKOV CHAINS STOCHA; Milstein G.N., 2013, STOCHASTIC NUMERICS; Milstein GN, 2003, IMA J NUMER ANAL, V23, P593, DOI 10.1093/imanum/23.4.593; Mou Wenlong, 2019, ARXIV190810859; Oksendal Bernt, 2003, STOCHASTIC DIFFERENT, P2; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Raginsky Maxim, 2017, ARXIV170203849; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Rossler A, 2010, SIAM J NUMER ANAL, V48, P922, DOI 10.1137/09076636X; Sabanis S, 2019, J COMPLEXITY, V50, P84, DOI 10.1016/j.jco.2018.09.004; Sabanis Sotirios, 2018, ARXIV180800728; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Shen Ruoqi, 2019, ARXIV190905503; Simon M.K., 2007, PROBABILITY DISTRIBU; Szekely G. J., 2003, BOWLING GREEN STATE, V3, P1; Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018; Varadarajan V., 1958, SANKHYA, V19, P23; Vempala S.S., 2019, ADV NEURAL INFORM PR, P8092; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vilmart G, 2015, SIAM J SCI COMPUT, V37, pA201, DOI 10.1137/140974328; Weed J, 2017, ARXIV170700087; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wiktorsson M, 2001, ANN APPL PROBAB, V11, P470; Xu LH, 2018, PROCEEDINGS OF THE SEVENTH NORTHWAST ASIA INTERNATIONAL SYMPOSIUM ON LANGUAGE, LITERATURE AND TRANSLATION, P390; Xu P., 2018, ADV NEURAL INFORM PR, P3122; Zou Difan, 2019, P MACHINE LEARNING R, P2936	69	4	4	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307073
C	Loaiza-Ganem, G; Cunningham, JP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Loaiza-Ganem, Gabriel; Cunningham, John P.			The continuous Bernoulli: fixing a pervasive error in variational autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Variational autoencoders (VAE) have quickly become a central tool in machine learning, applicable to a broad range of data types and latent variable models. By far the most common first step, taken by seminal papers and by core software libraries alike, is to model MNIST data using a deep network parameterizing a Bernoulli likelihood. This practice contains what appears to be and what is often set aside as a minor inconvenience: the pixel data is [0, 1] valued, not {0, 1} as supported by the Bernoulli likelihood. Here we show that, far from being a triviality or nuisance that is convenient to ignore, this error has profound importance to VAE, both qualitative and quantitative. We introduce and fully characterize a new [0, 1]-supported, single parameter distribution: the continuous Bernoulli, which patches this pervasive bug in VAE. This distribution is not nitpicking; it produces meaningful performance improvements across a range of metrics and datasets, including sharper image samples, and suggests a broader class of performant VAE.(1)	[Loaiza-Ganem, Gabriel; Cunningham, John P.] Columbia Univ, Dept Stat, New York, NY 10027 USA	Columbia University	Loaiza-Ganem, G (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.	g12480@columbia.edu; jpc2181@columbia.edu			Simons Foundation; Sloan Foundation; McKnight Endowment Fund; NIH NINDS [5R0 INS 100066]; NSF [1707398]; Gatsby Charitable Foundation	Simons Foundation; Sloan Foundation(Alfred P. Sloan Foundation); McKnight Endowment Fund; NIH NINDS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); NSF(National Science Foundation (NSF)); Gatsby Charitable Foundation	We thank Yixin Wang, Aaron Schein, Andy Miller, and Keyon Vafa for helpful conversations, and the Simons Foundation, Sloan Foundation, McKnight Endowment Fund, NIH NINDS 5R0 INS 100066, NSF 1707398, and the Gatsby Charitable Foundation for support.	[Anonymous], 2016, ARXIV; Burda Yuri, 2015, ICLR; Dai B, 2019, INT C LEARN REPR; Damoulas T, 2019, ARXIV190402063; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Doersch Carl, 2016, ARXIV160605908V2; Gao YJ, 2016, ADV NEUR IN, V29; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Gulrajani I., 2017, INT C LEARN REPR, P2; Higgins M, 2017, PALGR COMMUN, V3, DOI 10.1057/s41599-017-0005-4; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hu ZT, 2017, PR MACH LEARN RES, V70; Jang E., 2017, ICLR; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D., 2014, ADAM METHOD STOCHAST; Kingma D.P., 2015, INT C LEARN REPR ICL; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Koller D., 2009, PROBABILISTIC GRAPHI; Larochelle H., 2011, INT C ART INT STAT; Larsen ABL, 2016, PR MACH LEARN RES, V48; Loaiza-Ganem G., 2017, INT C LEARN REPR; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Maddison Chris J, 2017, ICLR; Malouf, 2002, P 6 C NAT LANG LEARN, V20, P1, DOI DOI 10.3115/1118853.1118871; Miller A., 2017, ADV NEURAL INFORM PR, P3708; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pearl J, 1982, REVEREND BAYES INFER; Rezende D., 2015, ICML, P1530; Rumelhart D., 1986, P 1986 PARALLEL DIST, P194; Salimans T., 2016, ADV NEUR IN, P2234; Sonderby CK, 2016, ADV NEUR IN, V29; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	36	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904087
C	Loog, M; Viering, T; Mey, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Loog, Marco; Viering, Tom; Mey, Alexander			Minimizers of the Empirical Risk and Risk Monotonicity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LEARNING-CURVES; BOUNDS; REGRESSION	Plotting a learner's average performance against the number of training samples results in a learning curve. Studying such curves on one or more data sets is a way to get to a better understanding of the generalization properties of this learner. The behavior of learning curves is, however, not very well understood and can display (for most researchers) quite unexpected behavior. Our work introduces the formal notion of risk monotonicity, which asks the risk to not deteriorate with increasing training set sizes in expectation over the training samples. We then present the surprising result that various standard learners, specifically those that minimize the empirical risk, can act nonmonotonically irrespective of the training sample size. We provide a theoretical underpinning for specific instantiations from classification, regression, and density estimation. Altogether, the proposed monotonicity notion opens up a whole new direction of research.	[Loog, Marco; Viering, Tom; Mey, Alexander] Delft Univ Technol, Delft, Netherlands; [Loog, Marco] Univ Copenhagen, Copenhagen, Denmark	Delft University of Technology; University of Copenhagen	Loog, M (corresponding author), Delft Univ Technol, Delft, Netherlands.; Loog, M (corresponding author), Univ Copenhagen, Copenhagen, Denmark.			Mey, Alexander/0000-0003-0528-3081; Viering, Tom/0000-0002-7337-8624	Netherlands Organisation for Scientific Research (NWO) [612.001.402]	Netherlands Organisation for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	This work was funded in part by the Netherlands Organisation for Scientific Research (NWO) and carried out under TOP grant project number 612.001.402.	AMARI S, 1992, NEURAL COMPUT, V4, P605, DOI 10.1162/neco.1992.4.4.605; AMARI S, 1993, NEURAL COMPUT, V5, P140, DOI 10.1162/neco.1993.5.1.140; [Anonymous], 2018, ADV NEURAL INFORM PR, P1790; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Belkin M., 2018, ARXIV181211118; Ben-David S., 2012, INT C MACH LEARN INT C MACH LEARN, P83; Ben-David Shai, 2011, PHIL MACH LEARN WORK; Cortes C., 1994, ADV NEURAL INFORM PR, V6, P327; Duin, 1995, P 9 SCAND C IM AN, V2, P957; Duin RPW, 2000, INT C PATT RECOG, P1, DOI 10.1109/ICPR.2000.906006; Grunwald Peter D., 2011, P 24 ANN C LEARN THE, P813; Haussler D, 1996, MACH LEARN, V25, P195, DOI 10.1007/BF00114010; HAUSSLER D, 1992, ADV NEUR IN, V4, P855; Hestness J, 2017, ARXIV171200409; Kolachina P., 2012, P 50 ANN M ASS COMP, V1, P22; Kramer Nicole, 2009, ARXIV09044416; Krijthe JH, 2017, MACH LEARN, V106, P993, DOI 10.1007/s10994-017-5626-8; LEVIN E, 1990, P IEEE, V78, P1568, DOI 10.1109/5.58339; Loog M, 2016, IEEE T PATTERN ANAL, V38, P462, DOI 10.1109/TPAMI.2015.2452921; Loog M, 2012, LECT NOTES COMPUT SC, V7626, P310, DOI 10.1007/978-3-642-34166-3_34; Micchelli Charles A., 1979, 565 WISC U DEP STAT; Opper M, 1999, ADV NEUR IN, V11, P302; Opper M, 1998, THEORETICAL ASPECTS OF NEURAL COMPUTATION, P17; OPPER M, 1991, PROCEEDINGS OF THE FOURTH ANNUAL WORKSHOP ON COMPUTATIONAL LEARNING THEORY, P75; Opper M., 1996, MODELS NEURAL NETWOR, P151; Opper M, 2001, FRONTIERS LIFE, V3, P763; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Smola A. J., 2000, ADV LARGE MARGIN CLA; Sollich P, 2002, NEURAL COMPUT, V14, P1393, DOI 10.1162/089976602753712990; SOMPOLINSKY H, 1990, PHYS REV LETT, V65, P1683, DOI 10.1103/PhysRevLett.65.1683; Spigler S., 2018, ARXIV181009665; Tishby N., 1989, P INT 1989 JOINT C N, P403; Vapnik V.N, 1998, STAT LEARNING THEORY; Viering T., 2019, P 32 C LEARN THEOR, P3198; Williams CKI, 2000, MACH LEARN, V40, P77, DOI 10.1023/A:1007601601278	38	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307049
C	Mandal, D; Procaccia, AD; Shah, N; Woodruff, DP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mandal, Debmalya; Procaccia, Ariel D.; Shah, Nisarg; Woodruff, David P.			Efficient and Thrifty Voting by Any Means Necessary	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SOCIAL CHOICE FUNCTIONS; COMPLEXITY	We take an unorthodox view of voting by expanding the design space to include both the elicitation rule, whereby voters map their (cardinal) preferences to votes, and the aggregation rule, which transforms the reported votes into collective decisions. Intuitively, there is a tradeoff between the communication requirements of the elicitation rule (i.e., the number of bits of information that voters need to provide about their preferences) and the efficiency of the outcome of the aggregation rule, which we measure through distortion (i.e., how well the utilitarian social welfare of the outcome approximates the maximum social welfare in the worst case). Our results chart the Pareto frontier of the communication-distortion tradeoff.	[Mandal, Debmalya] Columbia Univ, New York, NY 10027 USA; [Procaccia, Ariel D.; Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Shah, Nisarg] Univ Toronto, Toronto, ON, Canada	Columbia University; Carnegie Mellon University; University of Toronto	Mandal, D (corresponding author), Columbia Univ, New York, NY 10027 USA.	dm3557@columbia.edu; arielpro@cs.cmu.edu; nisarg@cs.toronto.edu; dwoodruf@cs.cmu.edu			Post-Doctoral fellowship from the Columbia Data Science Institute; National Science Foundation [IIS-1350598, IIS1714140, CCF-1525932, CCF-1733556, CCF-1815840]; Office of Naval Research [0001416-1-3075, N00014-17-1-2428]; J.P. Morgan AI Research Award; Guggenheim Fellowship; Natural Sciences and Engineering Research Council	Post-Doctoral fellowship from the Columbia Data Science Institute; National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); J.P. Morgan AI Research Award; Guggenheim Fellowship; Natural Sciences and Engineering Research Council(Natural Sciences and Engineering Research Council of Canada (NSERC))	Mandal was partially supported by the Post-Doctoral fellowship from the Columbia Data Science Institute, and part of this work was done while he was a graduate student at Harvard University. Procaccia was partially supported by the National Science Foundation under grants IIS-1350598, IIS1714140, CCF-1525932, and CCF-1733556; by the Office of Naval Research under grants N0001416-1-3075 and N00014-17-1-2428; by a J.P. Morgan AI Research Award; and by a Guggenheim Fellowship. Shah was partially supported by the Natural Sciences and Engineering Research Council under a Discovery grant. Woodruff was partially supported by the National Science Foundation under grant CCF-1815840, and part of this work was done while he was visiting the Simons Institute for the Theory of Computing.	[Anonymous], 2014, P 15 ACM C EC COMP; Anshelevich E, 2018, ARTIF INTELL, V264, P27, DOI 10.1016/j.artint.2018.07.006; Anshelevich E, 2017, J ARTIF INTELL RES, V58, P797, DOI 10.1613/jair.5340; Arrow Kenneth J., 1951, SOCIAL CHOICE INDIVI; Awad E, 2018, NATURE, V563, P59, DOI 10.1038/s41586-018-0637-6; Badanidiyuru A., 2012, SODA; Balcan MF, 2018, SIAM J COMPUT, V47, P703, DOI 10.1137/120888909; Balkanski E, 2017, ACM S THEORY COMPUT, P1016, DOI 10.1145/3055399.3055406; Bar-Yossef Z, 2004, J COMPUT SYST SCI, V68, P702, DOI 10.1016/j.jcss.2003.11.006; Benade G., 2019, P 33 AAAI C ART INT; Benade G, 2017, AAAI CONF ARTIF INTE, P376; Bhaskar U., 2018, P 38 IARCS ANN C FDN; Bhaskar U, 2018, AAAI CONF ARTIF INTE, P924; Borovoy A, 2019, ANTHEM COMPANION SOC, P33; Boutilier C, 2015, ARTIF INTELL, V227, P190, DOI 10.1016/j.artint.2015.06.003; Brandt F, 2016, HDB COMPUTATIONAL SO, P57; Caragiannis I, 2017, J ARTIF INTELL RES, V58, P123, DOI 10.1613/jair.5282; Caragiannis I, 2011, ARTIF INTELL, V175, P1655, DOI 10.1016/j.artint.2011.03.005; Chakrabarti A, 2003, ANN IEEE CONF COMPUT, P107, DOI 10.1109/CCC.2003.1214414; Cheng Y, 2018, AAAI CONF ARTIF INTE, P973; CONITZER V, 2005, P 6 ACM C EL COMM, P78; Conitzer V, 2017, AAAI CONF ARTIF INTE, P4831; deCondorcet Marquis, 1785, ESS APPL AN PROB DEC; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman M, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P269, DOI 10.1145/2940716.2940725; Ghodsi Mohammad, 2019, P 33 AAAI C ART INT; Goel A, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P287, DOI 10.1145/3033274.3085138; Greene J, 2016, AAAI CONF ARTIF INTE, P4147; GRONEMEIER ANDRE, 2009, P 26 INT S THEOR ASP, V3, P505, DOI [10.4230/LIPIcs.STACS.2009.1846, DOI 10.4230/LIPICS.STACS.2009.1846]; Gross S, 2017, AAAI CONF ARTIF INTE, P544; Jayram TS, 2009, LECT NOTES COMPUT SC, V5687, P562, DOI 10.1007/978-3-642-03685-9_42; Kushilevitz E., 1996, COMMUNICATION COMPLE; Mirrokni V, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P70; Noothigattu R, 2018, AAAI CONF ARTIF INTE, P1587; Procaccia AD, 2006, LECT NOTES COMPUT SC, V4149, P317; Skowron P, 2019, ARXIV190110848; Suzumura K., 2010, HDB SOCIAL CHOICE WE, V2; Wang Quan, 1986, Tectonics, V5, P1073, DOI 10.1029/TC005i007p01073	40	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307022
C	Matsushima, S; Brbic, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Matsushima, Shin; Brbic, Maria			Selective Sampling-based Scalable Sparse Subspace Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FACE RECOGNITION; MODELS	Sparse subspace clustering (SSC) represents each data point as a sparse linear combination of other data points in the dataset. In the representation learning step SSC finds a lower dimensional representation of data points, while in the spectral clustering step data points are clustered according to the underlying subspaces. However, both steps suffer from high computational and memory complexity, preventing the application of SSC to large-scale datasets. To overcome this limitation, we introduce Selective Sampling-based Scalable Sparse Subspace Clustering ((SC)-C-5) algorithm which selects subsamples based on the approximated subgradients and linearly scales with the number of data points in terms of time and memory requirements. Along with the computational advantages, we derive theoretical guarantees for the correctness of (SC)-C-5. Our theoretical result presents novel contribution for SSC in the case of limited number of subsamples. Extensive experimental results demonstrate effectiveness of our approach.	[Matsushima, Shin] Univ Tokyo, Tokyo, Japan; [Brbic, Maria] Stanford Univ, Stanford, CA 94305 USA	University of Tokyo; Stanford University	Matsushima, S (corresponding author), Univ Tokyo, Tokyo, Japan.	smatsus@graco.c.u-tokyo.ac.jp; mbrbic@cs.stanford.edu			KAKENHI [19K20336]	KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by KAKENHI 19K20336.	Acharya J, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P1, DOI 10.1109/ICCNC.2015.7069284; [Anonymous], 2016, J MACH LEARN RES; [Anonymous], 2018, ARXIV180406291; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Brbic M, 2020, IEEE T CYBERNETICS, V50, P1711, DOI 10.1109/TCYB.2018.2883566; Brbic M, 2018, PATTERN RECOGN, V73, P247, DOI 10.1016/j.patcog.2017.08.024; Cai D, 2015, IEEE T CYBERNETICS, V45, P1669, DOI 10.1109/TCYB.2014.2358564; Chitta R., 2011, PROC 17 ACM SIGKDD I, P895, DOI DOI 10.1145/2020408.2020558; Chung F R. K., 1997, SPECTRALGRAPH THEORY; Dyer EL, 2013, J MACH LEARN RES, V14, P2487; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Elhamifar E, 2009, PROC CVPR IEEE, P2782; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; FREY PW, 1991, MACH LEARN, V6, P161, DOI 10.1023/A:1022606404104; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993; Heckel R, 2015, IEEE T INFORM THEORY, V61, P6320, DOI 10.1109/TIT.2015.2472520; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Lin F., 2010, P 27 INT C MACH LEAR, V10, P655; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Nene S.A., 1996, TECH REP CUCS 006 96; Ng AY, 2002, ADV NEUR IN, V14, P849; Park D., 2014, ADV NEURAL INFORM PR, P2753; Peng X., 2018, IEEE T NEURAL NETWOR, V27, P2499; Peng X, 2013, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2013.62; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Tang KW, 2014, IEEE T NEUR NET LEAR, V25, P2167, DOI 10.1109/TNNLS.2014.2306063; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tron R., BENCHMARK COMP 3D MO; Tsakiris M. C., 2018, P 35 INT C MACH LEAR, P4006; Vidal R., 2011, IEEE SIGNAL PROCESSI, V28, P52; Vidal R, 2014, PATTERN RECOGN LETT, V43, P47, DOI 10.1016/j.patrec.2013.08.006; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wu TT, 2008, ANN APPL STAT, V2, P224, DOI 10.1214/07-AOAS147; You C, 2018, LECT NOTES COMPUT SC, V11213, P68, DOI 10.1007/978-3-030-01240-3_5; You C, 2016, PROC CVPR IEEE, P3918, DOI 10.1109/CVPR.2016.425; You C, 2016, PROC CVPR IEEE, P3928, DOI 10.1109/CVPR.2016.426	43	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904012
C	Moulos, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Moulos, Vrettos			Optimal Best Markovian Arm Identification with Fixed Confidence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EFFICIENT ALLOCATION RULES; MULTIARMED BANDIT PROBLEM; MULTIPLE PLAYS	We give a complete characterization of the sampling complexity of best Markovian arm identification in one-parameter Markovian bandit models. We derive instance specific nonasymptotic and asymptotic lower bounds which generalize those of the IID setting. We analyze the Track-and-Stop strategy, initially proposed for the IID setting, and we prove that asymptotically it is at most a factor of four apart from the lower bound. Our one-parameter Markovian bandit model is based on the notion of an exponential family of stochastic matrices for which we establish many useful properties. For the analysis of the Track-and-Stop strategy we derive a novel concentration inequality for Markov chains that may be of interest in its own right.	[Moulos, Vrettos] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Moulos, V (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	vrettos@berkeley.edu			NSF [CCF-1816861]	NSF(National Science Foundation (NSF))	We would like to thank Venkat Anantharam, Jim Pitman and Satish Rao for many helpful discussions. This research was supported in part by the NSF grant CCF-1816861.	ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P977, DOI 10.1109/TAC.1987.1104485; ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; [Anonymous], [No title captured]; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chung K.-M., 2012, STACS; Cover TM, 2006, ELEMENTS INFORM THEO; CSISZAR I, 1987, IEEE T INFORM THEORY, V33, P788, DOI 10.1109/TIT.1987.1057385; DAVISSON LD, 1981, IEEE T INFORM THEORY, V27, P431, DOI 10.1109/TIT.1981.1056377; Dembo A., 1998, APPL MATH NEW YORK, V38; Dinwoodie IH, 1995, ANN APPL PROBAB, V5, P37, DOI 10.1214/aoap/1177004826; DONSKER MD, 1975, COMMUN PUR APPL MATH, V28, P1, DOI 10.1002/cpa.3160280102; Durrett R., 2010, CAMBRIDGE SERIES STA, V31, DOI 10.1017/CBO9780511779398; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Garivier A., 2016, JMLR WORKSHOP C PROC, V49, P1; Gillman D., 1993, Proceedings. 34th Annual Symposium on Foundations of Computer Science (Cat. No.93CH3368-8), P680, DOI 10.1109/SFCS.1993.366819; Hayashi M, 2016, ANN STAT, V44, P1495, DOI 10.1214/15-AOS1420; Horn R.A., 2013, MATRIX ANAL, Vsecond; Jamieson K., 2014, C LEARN THEOR, P423; Kaufmann E., 2018, MIXTURE MARTINGALES; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Leon CA, 2004, ANN APPL PROBAB, V14, P958, DOI 10.1214/105051604000000170; Lezaud P, 1998, ANN APPL PROBAB, V8, P849; Mannor S, 2004, J MACH LEARN RES, V5, P623; MILLER HD, 1961, ANN MATH STAT, V32, P1260, DOI 10.1214/aoms/1177704865; Nagaoka H., 2005, P 28 S INF THEOR ITS, P1091; NAKAGAWA K, 1993, IEEE T INFORM THEORY, V39, P629, DOI 10.1109/18.212294; Ortega JM., 1990, CLASSICS APPL MATH, V3; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	30	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305058
C	Ni, C; Charoenphakdee, N; Honda, J; Sugiyama, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ni, Chenri; Charoenphakdee, Nontawat; Honda, Junya; Sugiyama, Masashi			On the Calibration of Multiclass Classification with Rejection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONSISTENCY	We investigate the problem of multiclass classification with rejection, where a classifier can choose not to make a prediction to avoid critical misclassification. First, we consider an approach based on simultaneous training of a classifier and a rejector, which achieves the state-of-the-art performance in the binary case. We analyze this approach for the multiclass case and derive a general condition for calibration to the Bayes-optimal solution, which suggests that calibration is hard to achieve by general loss functions unlike the binary case. Next, we consider another traditional approach based on confidence scores, in which the existing work focuses on a specific class of losses. We propose rejection criteria for more general losses for this approach and guarantee calibration to the Bayes-optimal solution. Finally, we conduct experiments to validate the relevance of our theoretical findings.	[Ni, Chenri; Charoenphakdee, Nontawat; Honda, Junya; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan; [Charoenphakdee, Nontawat; Honda, Junya; Sugiyama, Masashi] RIKEN, Ctr Adv Intelligence Project, Wako, Saitama, Japan	University of Tokyo; RIKEN	Ni, C (corresponding author), Univ Tokyo, Tokyo, Japan.	nichenri@ms.k.u-tokyo.ac.jp; nontawat@ms.k.u-tokyo.ac.jp; jhonda@k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	MEXT scholarship; JST AIP; KAKENHI [18K17998]; International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study	MEXT scholarship; JST AIP; KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study	We thank Han Bao for fruitful discussions. We also thank anonymous reviewers for providing insightful comments. NC was supported by MEXT scholarship and JST AIP challenge. JH was supported by KAKENHI 18K17998. MS was supported by the International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study.	Bartlett P. L., 2002, ANN STAT, V33, P1487; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Ben-David S, 2003, J COMPUT SYST SCI, V66, P496, DOI 10.1016/S0022-0000(03)00038-2; Bengio Y., 2009, ADV NEURAL INFORM PR, V21; Bishop C.M, 2006, PATTERN RECOGN; CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; Cortes C., 2018, P 35 INT C MACH LEAR, P1059; Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5; DUBUISSON B, 1993, PATTERN RECOGN, V26, P155, DOI 10.1016/0031-3203(93)90097-G; Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094; Garcia A., 2018, INT C MACH LEARN PML, P1695; Hamid K, 2017, INT CONF FRONT INFO, P356, DOI 10.1109/FIT.2017.00070; Herbei R, 2006, CAN J STAT, V34, P709, DOI 10.1002/cjs.5550340410; Mohri M., 2018, FDN MACHINE LEARNING; Pires Bernardo Avila, 2016, ARXIV PREPRINT ARXIV; Ramaswamy HG, 2018, ELECTRON J STAT, V12, P530, DOI 10.1214/17-EJS1388; Reddi S., 2018, ICLR; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Tax DMJ, 2008, PATTERN RECOGN LETT, V29, P1565, DOI 10.1016/j.patrec.2008.03.010; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Vernet Elodie, 2011, NIPS 24, P1224; Wegkamp M, 2011, BERNOULLI, V17, P1368, DOI 10.3150/10-BEJ320; Weston J., 1998, TECHNICAL REPORT; Wu Q, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 1, PROCEEDINGS, P34; Yuan M, 2010, J MACH LEARN RES, V11, P111; Zhang T, 2004, ANN STAT, V32, P56; Zhang T, 2004, J MACH LEARN RES, V5, P1225	30	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302057
C	Nicolicioiu, A; Duta, L; Leordeanu, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nicolicioiu, Andrei; Duta, Lulia; Leordeanu, Marius			Recurrent Space-time Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning in the space-time domain remains a very challenging problem in machine learning and computer vision. Current computational models for understanding spatio-temporal visual data are heavily rooted in the classical single-image based paradigm. It is not yet well understood how to integrate information in space and time into a single, general model. We propose a neural graph model, recurrent in space and time, suitable for capturing both the local appearance and the complex higher-level interactions of different entities and objects within the changing world scene. Nodes and edges in our graph have dedicated neural networks for processing information. Nodes operate over features extracted from local parts in space and time and over previous memory states. Edges process messages between connected nodes at different locations and spatial scales or between past and present time. Messages are passed iteratively in order to transmit information globally and establish long range interactions. Our model is general and could learn to recognize a variety of high level spatio-temporal concepts and be applied to different learning tasks. We demonstrate, through extensive experiments and ablation studies, that our model outperforms strong baselines and top published methods on recognizing complex activities in video. Moreover, we obtain state-of-the-art performance on the challenging Something-Something human-object interaction dataset.	[Nicolicioiu, Andrei; Duta, Lulia; Leordeanu, Marius] Bitdefender, Bucharest, Romania; [Leordeanu, Marius] Univ Politehn Bucuresti, Romanian Acad, Inst Math, Bucharest, Romania	Bitdefender; Institute of Mathematics of the Romanian Academy; Polytechnic University of Bucharest; Romanian Academy of Sciences; University of Bucharest	Nicolicioiu, A (corresponding author), Bitdefender, Bucharest, Romania.	anicolicioiu@bitdefender.com; iduta@bitdefender.com; marius.leordeanu@imar.ro			UEFISCDI [EEA-RO-2018-0496, TE-2016-2182]	UEFISCDI(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI))	This work has been supported in part by Bitdefender and UEFISCDI, through projects EEA-RO-2018-0496 and TE-2016-2182.	Abadi M, 2015, P 12 USENIX S OPERAT; Baradel F, 2018, LECT NOTES COMPUT SC, V11217, P106, DOI 10.1007/978-3-030-01261-8_7; Battaglia Peter W, 2018, ARXIV180601261; Battaglia Peter W, 2016, ARXIV161200222; BESAG J, 1986, J R STAT SOC B, V48, P259; Bruna J, 2013, PROC INT C LEARN REP; Cao ZC, 2018, IEEE INT CON AUTO SC, P803, DOI 10.1109/COASE.2018.8560578; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen YP, 2018, ADV NEUR IN, V31; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Dehghani Mostafa, 2019, ICLR; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Duvenaud David K, 2015, P NIPS; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Geman S., 1984, IEEE T PATTERN ANAL, V72, P1; Geman S, 1987, P INT C MATH, V1, P1496; Ghosh Pallabi, 2018, ARXIVABS181110575; Gilmer J, 2017, PR MACH LEARN RES, V70; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Henaff M, 2015, ARXIV150605163; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; JAIN A, 2016, PROC CVPR IEEE, P5308, DOI DOI 10.1109/CVPR.2016.573; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee MG, 2018, LECT NOTES COMPUT SC, V11214, P392, DOI 10.1007/978-3-030-01249-6_24; Leordeanu M, 2012, INT J COMPUT VISION, V96, P28, DOI 10.1007/s11263-011-0442-2; Li Yujia, 2016, P INT C LEARN REPR I, P2; Ng AY, 2002, ADV NEUR IN, V14, P849; Pearl Judea, 2014, PROBABILISTIC REASON; Ravikumar Pradeep, 2006, P 23 INT C MACH LEAR, P737, DOI DOI 10.1145/1143844.1143937; Santoro A, 2018, ADV NEUR IN, V31; Santoro A, 2017, ADV NEUR IN, V30; Schaeffer SE, 2007, COMPUT SCI REV, V1, P27, DOI 10.1016/j.cosrev.2007.05.001; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Soomro K., 2012, ARXIV; Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Velickovic P., 2018, P INT C LEARN REPR; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang YB, 2017, ADV NEUR IN, V30; Wang Yunbo, 2019, ICLR; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xu K, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337923; Yan Sijie, 2018, AAAI; Zhao Y, 2018, ADV NEUR IN, V31; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43	60	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904047
C	Paquette, P; Lu, YC; Bocco, S; Smith, MO; Ortiz-Gagne, S; Kummerfeld, JK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Paquette, Philip; Lu, Yuchen; Bocco, Steven; Smith, Max O.; Ortiz-Gagne, Satya; Kummerfeld, Jonathan K.			No Press Diplomacy: Modeling Multi-Agent Gameplay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GO	Diplomacy is a seven-player non-stochastic, non-cooperative game, where agents acquire resources through a mix of teamwork and betrayal. Reliance on trust and coordination makes Diplomacy the first non-cooperative multi-agent benchmark for complex sequential social dilemmas in a rich environment. In this work, we focus on training an agent that learns to play the No Press version of Diplomacy where there is no dedicated communication channel between players. We present DipNet, a neural-network-based policy model for No Press Diplomacy. The model was trained on a new dataset of more than 150,000 human games. Our model is trained by supervised learning (SL) from expert trajectories, which is then used to initialize a reinforcement learning (RL) agent trained through self-play. Both the SL and RL agents demonstrate state-of-the-art No Press performance by beating popular rule-based bots.	[Paquette, Philip; Lu, Yuchen; Bocco, Steven; Ortiz-Gagne, Satya] Univ Montreal, Mila, Montreal, PQ, Canada; [Smith, Max O.] McGill Univ, Mila, Montreal, PQ, Canada; [Kummerfeld, Jonathan K.] Univ Michigan, Ann Arbor, MI 48109 USA	Universite de Montreal; McGill University; University of Michigan System; University of Michigan	Paquette, P (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.	pcpaquette@mail.com; luyuchen.paul@gmail.com; stevenbocco@gmail.com; max.olan.smith@gmail.com; s.ortizgagne@gmail.com; jkummerf@umich.edu						AXELROD R, 1981, SCIENCE, V211, P1390, DOI 10.1126/science.7466396; Brown N, 2018, SCIENCE, V359, P418, DOI 10.1126/science.aao1733; Camerer CF, 2004, ADVANCES IN UNDERSTANDING STRATEGIC BEHAVIOUR: GAME THEORY, EXPERIMENTS AND BOUNDED RATIONALITY, P120; Cao K., 2018, EMERGENT COMMUNICATI; CRAWFORD VP, 1982, ECONOMETRICA, V50, P1431, DOI 10.2307/1913390; Dumoulin V., 2018, DISTILL, V3, pE 11; Fabregues A., 2010, P 9 INT C AUT AG MUL, V1, P1619; Ferreira Andre, 2015, ICAART 2015; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Foerster J. N., 2019, ARXIV190200506; Foerster J, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P122; Herbrich R., 2007, ADV NEURAL INFORM PR, V20, P569; Hughes E., 2018, ADV NEURAL INFORM PR; Jaderberg, 2018, ARXIV180701281; Jaques Natasha, 2018, ARXIV181008647; Jonge Dave, 2015, NEGOTIATIONS LARGE A; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kipf TN, 2016, P INT C LEARN REPR; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; Nichols Tony, 1998, PLAYER RATING SYSTEM; Norman David, 2013, DAIDE DIPLOMACY ARTI; Norman David, 2013, DAIDE CLIENTS; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Peysakhovich Alexander, 2017, ARXIV171006975; Shapiro Ari, 2002, INT C COMP GAM, P42; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Szykman Simon, 1995, DP W1995A COMMUNICAT; van Hal Jason, 2013, DIPLOMACY AI ALBERT; Vinyals O., 2019, ALPHASTAR MASTERING	33	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304047
C	Paul, S; van Baar, J; Roy-Chowdhury, AK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Paul, Sujoy; van Baar, Jeroen; Roy-Chowdhury, Amit K.			Learning from Trajectories via Subgoal Discovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning to solve complex goal-oriented tasks with sparse terminal-only rewards often requires an enormous number of samples. In such cases, using a set of expert trajectories could help to learn faster. However, Imitation Learning (IL) via supervised pre-training with these trajectories may not perform as well and generally requires additional finetuning with expert-in-the-loop. In this paper, we propose an approach which uses the expert trajectories and learns to decompose the complex main task into smaller sub-goals. We learn a function which partitions the state-space into sub-goals, which can then be used to design an extrinsic reward function. We follow a strategy where the agent first learns from the trajectories using IL and then switches to Reinforcement Learning (RL) using the identified sub-goals, to alleviate the errors in the IL step. To deal with states which are under-represented by the trajectory set, we also learn a function to modulate the sub-goal predictions. We show that our method is able to solve complex goal-oriented tasks, which other RL, IL or their combinations in literature are not able to solve.	[Paul, Sujoy; Roy-Chowdhury, Amit K.] Univ Calif Riverside, Riverside, CA 92521 USA; [van Baar, Jeroen] MERL, Cambridge, MA USA	University of California System; University of California Riverside	Paul, S (corresponding author), Univ Calif Riverside, Riverside, CA 92521 USA.	supaul@ece.ucr.edu; jeroen@merl.com; amitrc@ece.ucr.edu			US NSF [1724341]; Mitsubishi Electric Research Labs	US NSF(National Science Foundation (NSF)); Mitsubishi Electric Research Labs	This work was partially supported by US NSF grant 1724341 and Mitsubishi Electric Research Labs.	Andreas J, 2017, PR MACH LEARN RES, V70; [Anonymous], 2005, IDENTIFYING USEFUL S, DOI [DOI 10.1145/1102351.1102454, 10.1145/1102351.1102454]; [Anonymous], 2004, ACM INT C P SERIES, DOI DOI 10.1145/1015330.1015353; [Anonymous], 2017, P INT C MACH LEARN; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Bojarski M., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/IVS.2017.7995975; Chang Kai-Wei, 2015, ICML; Cheng Ching-An, 2018, UAI; Chernova S, 2009, J ARTIF INTELL RES, V34, P1, DOI 10.1613/jair.2584; Duan Y, 2016, INT C MACH LEARN, P1329; Dubey R., 2018, ARXIV180210217; Florensa C., 2017, INT C LEARN REPR ICL; Held David, 2017, ICML; Klein U., 2004, P 21 INT C MACH LEAR, P71, DOI DOI 10.1145/1015330.1015355; Krishnan S, 2019, INT J ROBOT RES, V38, P126, DOI 10.1177/0278364918784350; Levine S, 2016, J MACH LEARN RES, V17; Levine Sergey, 2013, ICML; Lukas R., 2018, P INT C MACH LEARN, P4390; McGovern A., 2001, ICML; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Murali A, 2016, IEEE INT CONF ROBOT, P4150, DOI 10.1109/ICRA.2016.7487607; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Paul S., 2018, P ECCV, P563; Paul Sujoy, 2018, ARXIV181111441; Precup D., 2000, TEMPORAL ABSTRACTION; Rajeswaran Aravind, 2017, RSS; Ranchod P, 2015, IEEE INT C INT ROBOT, P471, DOI 10.1109/IROS.2015.7353414; Riemer Matthew, 2018, NIPS; Ross St<prime>ephane, 2011, AISTATS; Ross Stephane, 2014, ARXIV14065979; Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3; Schulman John, 2015, ICLR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Silver David, 2008, RSS; Silver David, 2012, ARXIV12066473; Sun W., 2018, ARXIV180511240; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Todorov E, 2014, IEEE INT CONF ROBOT, P6054, DOI 10.1109/ICRA.2014.6907751; Torabi Faraz, 2018, IJCAI; van Baar Jeroen, 2018, ICRA	46	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900005
C	Pike-Burke, C; Grunewalder, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pike-Burke, Ciara; Grunewalder, Steffen			Recovering Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BOUNDS	We study the recovering bandits problem, a variant of the stochastic multi-armed bandit problem where the expected reward of each arm varies according to some unknown function of the time since the arm was last played. While being a natural extension of the classical bandit problem that arises in many real-world settings, this variation is accompanied by significant difficulties. In particular, methods need to plan ahead and estimate many more quantities than in the classical bandit setting. In this work, we explore the use of Gaussian processes to tackle the estimation and planing problem. We also discuss different regret definitions that let us quantify the performance of the methods. To improve computational efficiency of the methods, we provide an optimistic planning approximation. We complement these discussions with regret bounds and empirical studies.	[Pike-Burke, Ciara] Univ Pompeu Fabra, Barcelona, Spain; [Grunewalder, Steffen] Univ Lancaster, Lancaster, England	Pompeu Fabra University; Lancaster University	Pike-Burke, C (corresponding author), Univ Pompeu Fabra, Barcelona, Spain.	c.pikeburke@gmail.com; s.grunewalder@clancaster.ac.uk		Pike-Burke, Ciara/0000-0002-5847-1193	EPSRC STOR-i centre for doctoral training and Sparx [EP/L015692/1]	EPSRC STOR-i centre for doctoral training and Sparx(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	CPB would like to thank the EPSRC funded EP/L015692/1 STOR-i centre for doctoral training and Sparx. We would also like to thank the reviewers for their helpful comments.	Abeille M., 2017, INT C QART INT STAT; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Azar MG, 2017, PR MACH LEARN RES, V70; Besbes O., 2014, ADV NEURAL INFORM PR, P199; Bogunovic I, 2016, JMLR WORKSH CONF PRO, V51, P314; Bouneffouf D, 2016, NEUROCOMPUTING, V205, P16, DOI 10.1016/j.neucom.2016.02.052; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Cortes C., 2017, ARXIV171010657; Devroye L., 2001, SPRINGER SERIES STAT; Garivier A, 2011, LECT NOTES ARTIF INT, V6925, P174, DOI 10.1007/978-3-642-24412-4_16; GPy, 2012, GAUSS PROC FRAM PYTH; Heidari H., 2016, P 25 INT JOINT C ART, P1562; Hren JF, 2008, LECT NOTES ARTIF INT, V5323, P151, DOI 10.1007/978-3-540-89722-4_12; Immorlica N., 2018, IEEE 59 ANN S FDN CO; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Levine N., 2017, ADV NEURAL INFORM PR, P3077; Mintz Y., 2017, ARXIV170708423; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Murray I., 2016, GAUSSIAN PROCESSES K; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Puterman M. L., 1994, MARKOV DECISION PROC; Raj V., 2017, ARXIV170709727; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Seznec J., 2019, 22 INT C ART INT STA, P2564; Slivkins Aleksandrs, 2008, COLT, P343; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Whittle P., 1988, J APPL PROBAB, V25, P287, DOI DOI 10.2307/3214163; Yi Jinfeng, 2017, ADV NEURAL INFORM PR, P2409	31	4	4	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905076
C	Qian, J; Fruit, R; Pirotta, M; Lazaric, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qian, Jian; Fruit, Ronan; Pirotta, Matteo; Lazaric, Alessandro			Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The exploration bonus is an effective approach to manage the exploration-exploitation trade-off in Markov Decision Processes (MDPs). While it has been analyzed in infinite-horizon discounted and finite-horizon problems, we focus on designing and analysing the exploration bonus in the more challenging infinite-horizon undiscounted setting. We first introduce SCAL(+), a variant of SCAL [1], that uses a suitable exploration bonus to solve any discrete unknown weakly-communicating MDP for which an upper bound c on the span of the optimal bias function is known. We prove that SCAL enjoys the same regret guarantees as SCAL, which relies on the less efficient extended value iteration approach. Furthermore, we leverage the flexibility provided by the exploration bonus scheme to generalize SCAL(+) to smooth MDPs with continuous state space and discrete actions. We show that the resulting algorithm (SCCAL) achieves the same regret bound as UCCRL [2] while being the first implementable algorithm for this setting.	[Qian, Jian; Fruit, Ronan] Inria Lille, Sequel Team, Lille, France; [Pirotta, Matteo; Lazaric, Alessandro] Facebook AI Res, New York, NY USA	Facebook Inc	Qian, J (corresponding author), Inria Lille, Sequel Team, Lille, France.	jian.qian@ens.fr; ronan.fruit@inria.fr; pirotta@fb.com; lazaric@fb.com						AGRAWAL S, 2017, ADV NEURAL INFORM PR, P1184; [Anonymous], 2004, LEARNING APPROXIMATE; ARCHIBALD TW, 1995, J OPER RES SOC, V46, P354, DOI 10.2307/2584329; Azar MG, 2017, PR MACH LEARN RES, V70; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Bellemare M., 2016, NEURIPS; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V2; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi Nicolo, 2005, NIPS, P195; Chow YS, 1988, PROBABILITY THEORY I, V2nd; Even-Dar E., 2001, P NIPS, P1499; Fruit Ronan, 2018, ICML; Fruit Ronan, 2017, NIPS, P3169; Fruit Ronan, 2018, NIPS; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jin Chi, 2018, ABS180703765 CORR; Kakade Sham, 2018, ABS180209184 CORR; Klenke A., 2013, GRADUATE TEXTS MATH; Lakshmanan K, 2015, PR MACH LEARN RES, V37, P524; Lattimore Tor, 2018, BANDIT ALGORITHMS PR; Martin Jarryd, 2017, ABS170608090 CORR; Moore A.W., 1990, TECHNICAL REPORT; Ortner R., 2018, ABS180801813 CORR; Ortner R, 2008, MIND MACH, V18, P521, DOI 10.1007/s11023-008-9115-5; Ortner Ronald, 2012, ADV NEURAL INFORM PR, P1772; Ostrovski G, 2017, PR MACH LEARN RES, V70; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Talebi Mohammad Sadegh, 2018, P MACHINE LEARNING R, V83, P770; Tang H., 2017, NEURIPS	32	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304085
C	Qian, SR; Singer, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qian, Sharon; Singer, Yaron			Fast Parallel Algorithms for Statistical Subset Selection Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we propose a new framework for designing fast parallel algorithms for fundamental statistical subset selection tasks that include feature selection and experimental design. Such tasks are known to be weakly submodular and are amenable to optimization via the standard greedy algorithm. Despite its desirable approximation guarantees, the greedy algorithm is inherently sequential and in the worst case, its parallel runtime is linear in the size of the data. Recently, there has been a surge of interest in a parallel optimization technique called adaptive sampling which produces solutions with desirable approximation guarantees for submodular maximization in exponentially faster parallel runtime. Unfortunately, we show that for general weakly submodular functions such accelerations are impossible. The major contribution in this paper is a novel relaxation of submodularity which we call differential submodularity. We first prove that differential submodularity characterizes objectives like feature selection and experimental design. We then design an adaptive sampling algorithm for differentially submodular functions whose parallel runtime is logarithmic in the size of the data and achieves strong approximation guarantees. Through experiments, we show the algorithm's performance is competitive with state-of-the-art methods and obtains dramatic speedups for feature selection and experimental design problems.	[Qian, Sharon; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Qian, SR (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	sharonqian@g.harvard.edu; yaron@seas.harvard.edu			Smith Family Graduate Science and Engineering Fellowship; NSF [CCF 1452961, CCF 1301976]; BSF [2014389]; NSF USICCS [1540428]; Google Research award; Facebook research award	Smith Family Graduate Science and Engineering Fellowship; NSF(National Science Foundation (NSF)); BSF(US-Israel Binational Science Foundation); NSF USICCS; Google Research award(Google Incorporated); Facebook research award(Facebook Inc)	The authors would like to thank Eric Balkanski for helpful discussions. This research was supported by a Smith Family Graduate Science and Engineering Fellowship, NSF grant CAREER CCF 1452961, NSF CCF 1301976, BSF grant 2014389, NSF USICCS proposal 1540428, a Google Research award, and a Facebook research award.	Balkanski Eric, 2018, P MACH LEARN RES STO, V80; Balkanski Eric, 2019, P ACM SIAM S DISCR A; Balkanski Eric, 2018, STOC; Balkanski Eric, 2018, ADV NEURAL INFORM PR, P2353; Balkanski Eric, 2019, STOC; Bian AA, 2017, PR MACH LEARN RES, V70; Chekuri C., 2019, P 30 ANN ACM SIAM S; Chekuri Chandra, 2019, STOC; Chen Lin, 2019, STOC; Das Abhimanyu, 2012, ADV NEURAL INFORM PR; Das A, 2011, IEEE IND ELEC, P1058, DOI 10.1109/IECON.2011.6119454; Elenberg Ethan, 2017, ADV NEURAL INFORM PR; Elenberg ER, 2018, ANN STAT, V46, P3539, DOI 10.1214/17-AOS1679; Ene Alina, 2019, STOC; Ene Alina, 2019, SODA; Fahrbach Matthew, 2019, ICML; Fahrbach Matthew, 2019, SODA; Gupta Gaurav, 2018, CORR; Horel T., 2016, ADV NEURAL INFORM PR; JOHNSON RA, 2004, ENCY STAT SCI, V8; Krause A, 2008, J MACH LEARN RES, V9, P235; Krause Andreas, 2010, ICML, P567; Li Yuanzhi, 2018, P 35 INT C MACH LEAR; Mairal J, 2012, P 29 INT C MACH LEAR; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177	25	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305011
C	Qu, Q; Li, X; Zhu, ZH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qu, Qing; Li, Xiao; Zhu, Zhihui			A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RECONSTRUCTION; CONVERGENCE; FMRI	We study the multi-channel sparse blind deconvolution (MCS-BD) problem, whose task is to simultaneously recover a kernel a and multiple sparse inputs {x(i)}(i=1)(p) from their circulant convolution y(i) = a circle star x(i) (i = 1, . . . , p). We formulate the task as a nonconvex optimization problem over the sphere. Under mild statistical assumptions of the data, we prove that the vanilla Riemannian gradient descent (RGD) method, with random initializations, provably recovers both the kernel a and the signals {x(i)}(i=1)(p) up to a signed shift ambiguity. In comparison with state-of-the-art results, our work shows significant improvements in terms of sample complexity and computational efficiency. Our theoretical results are corroborated by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods on both synthetic and real datasets.	[Qu, Qing] NYU, New York, NY 10003 USA; [Li, Xiao] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Zhu, Zhihui] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Zhu, Zhihui] Univ Denver, Dept Elect & Comp Engn, Denver, CO 80208 USA	New York University; Chinese University of Hong Kong; Johns Hopkins University; University of Denver	Qu, Q (corresponding author), NYU, New York, NY 10003 USA.	qq213@nyu.edu; xli@ee.cuhk.edu.hk; zzhu29@jhu.edu	Qu, Qing/AAA-8226-2019; Zhu, Zhihui/AAR-5029-2020; Li, Xiao/ADO-7061-2022	Qu, Qing/0000-0001-9136-558X; Zhu, Zhihui/0000-0002-3856-0375; 	Microsoft PhD fellowship; MooreSloan foundation fellowship; Hong Kong Research Grants Council [CUHK14210617]; NSF [1704458]	Microsoft PhD fellowship(Microsoft); MooreSloan foundation fellowship; Hong Kong Research Grants Council(Hong Kong Research Grants Council); NSF(National Science Foundation (NSF))	Due to space limitation, we refer readers to Section 5 of our full paper [31] for a comprehensive discussion. QQ also would like to acknowledge the support of Microsoft PhD fellowship, and MooreSloan foundation fellowship. XL would like to acknowledge the support by Grant CUHK14210617 from the Hong Kong Research Grants Council. ZZ was partly supported by NSF Grant 1704458.	Amari S, 1997, FIRST IEEE SIGNAL PROCESSING WORKSHOP ON SIGNAL PROCESSING ADVANCES IN WIRELESS COMMUNICATIONS, P101, DOI 10.1109/SPAWC.1997.630083; [Anonymous], 2011, P ADV NEURAL PROCESS; Betzig E, 2006, SCIENCE, V313, P1642, DOI 10.1126/science.1127344; Boyd S, 2003, STANFORD U LECT NOTE; Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57; BURKE JV, 1993, SIAM J CONTROL OPTIM, V31, P1340, DOI 10.1137/0331063; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chen Y, 2018, IEEE SIGNAL PROC MAG, V35, P64, DOI 10.1109/MSP.2018.2842097; Chi Y., 2018, ARXIV180909573; COLEMAN TF, 1986, SIAM J ALGEBRA DISCR, V7, P527, DOI 10.1137/0607059; Cosse A, 2017, PROC SPIE, V10394, DOI 10.1117/12.2272836; Friedrich J, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005423; Garcia-Cardona C, 2018, IEEE T COMPUT IMAG, V4, P366, DOI 10.1109/TCI.2018.2840334; Gilboa D., 2018, ARXIV180910313; Gitelman DR, 2003, NEUROIMAGE, V19, P200, DOI 10.1016/S1053-8119(03)00058-2; GOFFIN JL, 1977, MATH PROGRAM, V13, P329, DOI 10.1007/BF01584346; Grant M., 2013, CVX MATLAB SOFTWARE; Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Hess ST, 2006, BIOPHYS J, V91, P4258, DOI 10.1529/biophysj.106.091116; Huber P.J., 1992, BREAKTHROUGHS STAT, P492, DOI DOI 10.1007/978-1-4612-4380-9_35; Jiang Q., 2018, ARXIV181010702; Kaaresen KF, 1998, GEOPHYSICS, V63, P2093, DOI 10.1190/1.1444503; Lau Yenson, 2019, PREPRINT; Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148; Li X, 2018, ARXIV180909237; Li Xiao, 2019, ARXIVORG190711687; Li YJ, 2017, IEEE T INFORM THEORY, V63, P822, DOI 10.1109/TIT.2016.2637933; Li Yanjun, 2018, ARXIV180510437; Ling SY, 2018, SIAM J IMAGING SCI, V11, P252, DOI 10.1137/16M1103634; Ma C., 2017, ARXIV171110467; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Neyshabur B., 2017, ARXIV170503071; Nose K, 2016, GEOPHYSICS, V81, pV7, DOI 10.1190/GEO2015-0069.1; Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037; QU Q., 2014, P ADV NEUR INF PROC, V27, P3401; Qu Qing, UNPUB; Repetti A, 2015, IEEE SIGNAL PROC LET, V22, P539, DOI 10.1109/LSP.2014.2362861; Rust MJ, 2006, NAT METHODS, V3, P793, DOI 10.1038/nmeth929; Sarder P, 2006, IEEE SIGNAL PROC MAG, V23, P32, DOI 10.1109/MSP.2006.1628876; She HJ, 2015, MAGN RESON IMAGING, V33, P1106, DOI 10.1016/j.mri.2015.06.008; Sroubek F, 2012, IEEE T IMAGE PROCESS, V21, P1687, DOI 10.1109/TIP.2011.2175740; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Tian N, 2017, J ACOUST SOC AM, V141, P3337, DOI 10.1121/1.4983311; Wang LM, 2016, IEEE SIGNAL PROC LET, V23, P1384, DOI 10.1109/LSP.2016.2599104; Wu GR, 2013, MED IMAGE ANAL, V17, P365, DOI 10.1016/j.media.2013.01.003; Zhai Y., 2019, ARXIV190602435; Zhang HC, 2013, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2013.140; ZHANG YQ, 2018, ADV NEURAL INFORM PR, V31, P2328	52	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304006
C	Rahmani, M; Li, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rahmani, Mostafa; Li, Ping			Outlier Detection and Robust PCA Using a Convex Measure of Innovation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PURSUIT	This paper presents a provable and strong algorithm, termed Innovation Search (iSearch), to robust Principal Component Analysis (PCA) and outlier detection. An outlier by definition is a data point which does not participate in forming a low dimensional structure with a large number of data points in the data. In other words, an outlier carries some innovation with respect to most of the other data points. iSearch ranks the data points based on their values of innovation. A convex optimization problem is proposed whose optimal value is used as our measure of innovation. We derive analytical performance guarantees for the proposed robust PCA method under different models for the distribution of the outliers including randomly distributed outliers, clustered outliers, and linearly dependent outliers. Moreover, it is shown that iSearch provably recovers the span of the inliers when the inliers lie in a union of subspaces. In the challenging scenarios in which the outliers are close to each other or they are close to the span of the inliers, iSearch is shown to outperform most of the existing methods.	[Rahmani, Mostafa; Li, Ping] Baidu Res, Cognit Comp Lab, 10900 NE 8th St, Bellevue, WA 98004 USA	Baidu	Rahmani, M (corresponding author), Baidu Res, Cognit Comp Lab, 10900 NE 8th St, Bellevue, WA 98004 USA.	mostafarahmani@baidu.com; lipingll@baidu.com						Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Charles A, 2013, INT CONF ACOUST SPEE, P6571, DOI 10.1109/ICASSP.2013.6638932; Choulakian V, 2006, COMPUT STAT DATA AN, V50, P1441, DOI 10.1016/j.csda.2005.01.009; Ding C., 2006, PROC INT C MACH LEAR, P281, DOI DOI 10.1145/1143844.1143880; Feng Jiashi, 2012, P 29 INT C MACH LEAR; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gitlin Andrew, 2018, IEEE J SELECTED TOPI; Harada Yoshiyuki, 2017, ARXIV170103249; Hardt M., 2013, C LEARN THEOR, P354; Hauskrecht M, 2013, J BIOMED INFORM, V46, P47, DOI 10.1016/j.jbi.2012.08.004; Heckel R, 2015, IEEE T INFORM THEORY, V61, P6320, DOI 10.1109/TIT.2015.2472520; Karrila S, 2011, CANCER INFORM, V10, P109, DOI 10.4137/CIN.S6868; Ke QF, 2005, PROC CVPR IEEE, P739; Kruegel C., 2003, SER CCS 03, P251, DOI 10.1145/948109.948144; Lerman G., 2017, INFORM INF A J IMA, V7, P277; Lerman G, 2018, P IEEE, V106, P1380, DOI 10.1109/JPROC.2018.2853141; Lerman G, 2015, FOUND COMPUT MATH, V15, P363, DOI 10.1007/s10208-014-9221-0; LI GY, 1985, J AM STAT ASSOC, V80, P759, DOI 10.2307/2288497; Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169; Li XG, 2015, IEEE T SIGNAL PROCES, V63, P1792, DOI 10.1109/TSP.2015.2401536; Liu G., 2014, P ADV NEUR INF PROC, P1206; Liu GC, 2016, IEEE T SIGNAL PROCES, V64, P5623, DOI 10.1109/TSP.2016.2586753; Markopoulos PP, 2014, IEEE T SIGNAL PROCES, V62, P5046, DOI 10.1109/TSP.2014.2338077; McCoy M, 2011, ELECTRON J STAT, V5, P1123, DOI 10.1214/11-EJS636; Rahmani M, 2017, IEEE SIGNAL PROC LET, V24, P1793, DOI 10.1109/LSP.2017.2757901; Rahmani M, 2017, IEEE T SIGNAL PROCES, V65, P6260, DOI 10.1109/TSP.2017.2749215; Rahmani M, 2017, IEEE T SIGNAL PROCES, V65, P6276, DOI 10.1109/TSP.2017.2749206; Rahmani Mostafa, 2019, ARXIV191212988; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974; Tsakiris M., 2015, P IEEE INT C COMP VI, P10; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; You C, 2017, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR.2017.460; Zhang T, 2016, INF INFERENCE, V5, P1, DOI 10.1093/imaiai/iav012; Zhang T, 2014, J MACH LEARN RES, V15, P749	39	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905083
C	Rohekar, RY; Gurwicz, Y; Nisimov, S; Novik, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rohekar, Raanan Y.; Gurwicz, Yaniv; Nisimov, Shami; Novik, Gal			Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modeling uncertainty in deep neural networks, despite recent important advances, is still an open problem. Bayesian neural networks are a powerful solution, where the prior over network weights is a design choice, often a normal distribution or other distribution encouraging sparsity. However, this prior is agnostic to the generative process of the input data, which might lead to unwarranted generalization for out-of-distribution tested data. We suggest the presence of a confounder for the relation between the input data and the discriminative function given the target label. We propose an approach for modeling this confounder by sharing neural connectivity patterns between the generative and discriminative networks. This approach leads to a new deep architecture, where networks are sampled from the posterior of local causal structures, and coupled into a compact hierarchy. We demonstrate that sampling networks from this hierarchy, proportionally to their posterior, is efficient and enables estimating various types of uncertainties. Empirical evaluations of our method demonstrate significant improvement compared to state-of-the-art calibration and out-of-distribution detection methods.	[Rohekar, Raanan Y.; Gurwicz, Yaniv; Nisimov, Shami; Novik, Gal] Intel AI Lab, Hillsboro, OR 97124 USA		Rohekar, RY (corresponding author), Intel AI Lab, Hillsboro, OR 97124 USA.	raanan.yehezkel@intel.com; yaniv.gurwicz@intel.com; shami.nisimov@intel.com; gal.novik@intel.com						Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; DAWID AP, 1982, J AM STAT ASSOC, V77, P605, DOI 10.2307/2287720; DeVries Terrance, 2018, ARXIV180204865; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Gal Y, 2016, PR MACH LEARN RES, V48; Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Izmailov P, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P876; Kingma D.P, P 3 INT C LEARNING R; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Maddox WJ, 2019, ADV NEUR IN, V32; Malinin Andrey, 2018, NEURAL INFORM PROCES, V3, P6; Murphy K, 2001, COMPUTER SCI STAT, V33, P1024; Naeini MP, 2015, AAAI CONF ARTIF INTE, P2901; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Pearl J., 2009, CAUSALITY MODELS REA; Platt JC, 2000, ADV NEUR IN, P61; Ritter Hippolyt, 2018, ICLR; Rohekar RY, 2018, ADV NEUR IN, V31; Shiyu L., 2018, P INT C LEARN REPR I; Smith L, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P560; Yehezkel R, 2009, J MACH LEARN RES, V10, P1527; Zhu LH, 2017, 2017 IEEE SECOND INTERNATIONAL CONFERENCE ON DATA SCIENCE IN CYBERSPACE (DSC), P213, DOI 10.1109/DSC.2017.89	27	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304027
C	Sahin, MF; Eftekhari, A; Alacaoglu, A; Latorre, F; Cevher, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sahin, Mehmet Fatih; Eftekhari, Armin; Alacaoglu, Ahmet; Latorre, Fabian; Cevher, Volkan			An Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEMIDEFINITE PROGRAMS; EVALUATION COMPLEXITY; RANK; EIGENVECTORS; CONVERGENCE; OPTIMALITY; ALGORITHM; BOUNDS	We propose a practical inexact augmented Lagrangian method (iALM) for nonconvex problems with nonlinear constraints. We characterize the total computational complexity of our method subject to a verifiable geometric condition, which is closely related to the Polyak-Lojasiewicz and Mangasarian-Fromowitz conditions. In particular, when a first-order solver is used for the inner iterates, we prove that iALM finds a first-order stationary point with (O) over tilde (1/epsilon(3)) calls to the first-order oracle. If, in addition, the problem is smooth and a second-order solver is used for the inner iterates, iALM finds a second-order stationary point with (O) over tilde (1/epsilon(5)) calls to the second-order oracle. These complexity results match the known theoretical results in the literature. We also provide strong numerical evidence on large-scale machine learning problems, including the Burer-Monteiro factorization of semidefinite programs, and a novel nonconvex relaxation of the standard basis pursuit template. For these examples, we also show how to verify our geometric condition.	[Sahin, Mehmet Fatih; Eftekhari, Armin; Alacaoglu, Ahmet; Latorre, Fabian; Cevher, Volkan] Ecole Polytech Fed Lausanne, LIONS, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Sahin, MF (corresponding author), Ecole Polytech Fed Lausanne, LIONS, Lausanne, Switzerland.	mehmet.sahin@epfl.ch; armin.eftekhari@epfl.ch; ahmet.alacaoglu@epfl.ch; fabian.latorre@epfl.ch; volkan.cevher@epfl.ch			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [725594 time-data]; Swiss National Science Foundation (SNSF) [200021_178865/1]; Department of the Navy, Office of Naval Research (ONR) [N62909-17-1-2111]; Hasler Foundation Program: Cyber Human Systems [16066]; Swiss Data Science Center (SDSC) [P18-07]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF)); Department of the Navy, Office of Naval Research (ONR)(Office of Naval Research); Hasler Foundation Program: Cyber Human Systems; Swiss Data Science Center (SDSC)	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 725594 time-data) and was supported by the Swiss National Science Foundation (SNSF) under grant number 200021_178865/1. This project was also sponsored by the Department of the Navy, Office of Naval Research (ONR) under a grant number N62909-17-1-2111 and was supported by Hasler Foundation Program: Cyber Human Systems (project number 16066). This research was supported by the PhD fellowship program of the Swiss Data Science Center (SDSC) under grant 1D number P18-07.	[Anonymous], 2017, ARXIV171209196; [Anonymous], 2016, ARXIV160601316; [Anonymous], 2018, INT C LEARN REPR; BARVINOK AI, 1995, DISCRETE COMPUT GEOM, V13, P189, DOI 10.1007/BF02574037; Bertsekas D. P., 1982, COMPUTER SCI APPL MA; Bertsekas D. P, 2014, CONSTRAINEDOPTIMIZAT; BERTSEKAS DP, 1976, SIAM J CONTROL, V14, P216, DOI 10.1137/0314017; Bhojanapalli S., 2018, ARXIV180300186; Bhojanapalli Srinadh, 2016, P C LEARN THEOR, P530; Birgin EG, 2014, FUND ALGORITHMS, P1, DOI 10.1137/1.9781611973365; Birgin EG, 2016, SIAM J OPTIMIZ, V26, P951, DOI 10.1137/15M1031631; Bolte J., 2018, MATH OPERATIONS RES; Bolte J, 2017, MATH PROGRAM, V165, P471, DOI 10.1007/s10107-016-1091-6; BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Boumal N., 2016, ARXIV160508101; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Cartis C, 2011, SIAM J OPTIMIZ, V21, P1721, DOI 10.1137/11082381X; Cartis Coralia, 2018, J COMPLEXITY; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Clason C., 2018, ARXIV180203347; Fernandez D, 2012, SIAM J OPTIMIZ, V22, P384, DOI 10.1137/10081085X; Fletcher R., 2013, PRACTICAL METHODS OP; Flores-Bazan F, 2012, J GLOBAL OPTIM, V53, P185, DOI 10.1007/s10898-011-9673-6; Ge R, 2016, PR MACH LEARN RES, V48; Goodfellow I.J., 2014, GENERATIVE ADVERSARI; Hestenes M. R., 1969, Journal of Optimization Theory and Applications, V4, P303, DOI 10.1007/BF00927673; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Khot S., 2011, ARXIV11082464; Kingma D.P, P 3 INT C LEARNING R; Kulis B., 2007, INT C ART INT STAT, P235; Lan GH, 2016, MATH PROGRAM, V155, P511, DOI 10.1007/s10107-015-0861-x; LOVASZ L, 2003, RECENT ADV ALGORITHM, P137; Mascarenhas WF, 2004, MATH PROGRAM, V99, P49, DOI 10.1007/s10107-003-0421-7; Mixon Dustin G, 2016, ARXIV160206612; Mossel E, 2015, ACM S THEORY COMPUT, P69, DOI 10.1145/2746539.2746603; Nedelcu V, 2014, SIAM J CONTROL OPTIM, V52, P3109, DOI 10.1137/120897547; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Nouiehed M., 2018, ARXIV181002024; Obozinski Guillaume, 2011, ARXIV11100413; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339; Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983; Powell M., 1969, OPTIMIZATION; Tran-Dinh Q, 2018, SIAM J OPTIMIZ, V28, P96, DOI 10.1137/16M1093094; Radford A., 2015, ARXIV E PRINTS; Raghavendra P, 2008, ACM S THEORY COMPUT, P245; Rockafellar R. T., 1970, CONVEX ANAL, V28; ROCKAFELLAR RT, 1993, SIAM REV, V35, P183, DOI 10.1137/1035044; Singer A, 2011, SIAM J IMAGING SCI, V4, P543, DOI 10.1137/090767777; Singer A, 2011, APPL COMPUT HARMON A, V30, P20, DOI 10.1016/j.acha.2010.02.001; Song L., 2007, ICML, P815; Tepper M, 2018, J MACH LEARN RES, V19; Tran-Dinh Q., 2018, ARXIV180804648; Waters A., 2018, ARXIV181203046; Xiao H., 2017, ARXIV 170807747; Xu Y., 2017, ARXIV7110582V2; Xu YY, 2017, J SCI COMPUT, V72, P700, DOI 10.1007/s10915-017-0376-0; Yang LQ, 2015, MATH PROGRAM COMPUT, V7, P331, DOI 10.1007/s12532-015-0082-6; Yu-Hong Dai, 2002, SIAM Journal on Optimization, V13, P693, DOI 10.1137/S1052623401383455; Yurtsever A., 2018, ARXIV180408544; Yurtsever Alp, 2015, ADV NEURAL INFORM PR, P3150; Zhao Q, 1998, J COMB OPTIM, V2, P71, DOI 10.1023/A:1009795911987	70	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905060
C	Sanchez, E; Tzimiropoulos, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sanchez, Enrique; Tzimiropoulos, Georgios			Object landmark discovery through unsupervised adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper proposes a method to ease the unsupervised learning of object landmark detectors. Similarly to previous methods, our approach is fully unsupervised in a sense that it does not require or make any use of annotated landmarks for the target object category. Contrary to previous works, we do however assume that a landmark detector, which has already learned a structured representation for a given object category in a fully supervised manner, is available. Under this setting, our main idea boils down to adapting the given pre-trained network to the target object categories in a fully unsupervised manner. To this end, our method uses the pre-trained network as a core which remains frozen and does not get updated during training, and learns, in an unsupervised manner, only a projection matrix to perform the adaptation to the target categories. By building upon an existing structured representation learned in a supervised manner, the optimization problem solved by our method is much more constrained with significantly less parameters to learn which seems to be important for the case of unsupervised learning. We show that our method surpasses fully unsupervised techniques trained from scratch as well as a strong baseline based on fine-tuning, and produces state-of-the-art results on several datasets. Code can be found at tiny. cc/GitHub-Unsupervised.	[Sanchez, Enrique] Samsung AI Ctr, Cambridge, England; [Tzimiropoulos, Georgios] Univ Nottingham, Comp Vis Lab, Nottingham, England	University of Nottingham	Sanchez, E (corresponding author), Samsung AI Ctr, Cambridge, England.	e.lozano@samsung.com; georgios.t@samsung.com		Sanchez-Lozano, Enrique/0000-0003-0196-922X				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Charles J., 2013, BRIT MACH VIS C; Choy Christopher, 2016, ADV NEURAL INFORM PR, V6; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu YB, 2018, PROC CVPR IEEE, P8398, DOI 10.1109/CVPR.2018.00876; Huh Minyoung, 2016, ARXIV160808614; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jakab T, 2018, ADV NEUR IN, V31; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kanazawa A, 2016, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2016.354; Kingma D.P, P 3 INT C LEARNING R; Lenc Karel, 2016, EUR C COMP VIS WORKS; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lorenz D, 2019, PROC CVPR IEEE, P10947, DOI 10.1109/CVPR.2019.01121; Ma LQ, 2017, ADV NEUR IN, V30; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Paszke Adam, 2017, AUT WORKSH NEURIPS; Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50; Rebuffi SA, 2018, PROC CVPR IEEE, P8119, DOI 10.1109/CVPR.2018.00847; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Rosenfeld A, 2020, IEEE T PATTERN ANAL, V42, P651, DOI 10.1109/TPAMI.2018.2884462; Roth Peter M., 2011, P 1 IEEE INT WORKSH; Rusu A. A., 2016, ARXIV160604671; Sahasrabudhe Mihir, 2019, ARXIV190411960; Shu ZX, 2018, LECT NOTES COMPUT SC, V11214, P664, DOI 10.1007/978-3-030-01249-6_40; Suwajanakorn S, 2018, ADV NEUR IN, V31; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thewlis J, 2017, IEEE I CONF COMP VIS, P3229, DOI 10.1109/ICCV.2017.348; Thewlis James, 2017, ADV NEURAL INFORM PR, V3, P8; Wang X., 2019, ARXIV190307593; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Xue Tianfan, 2016, ADV NEURAL INFORM SY; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Yu A, 2017, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2017.594; Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32; Zhang W, 2018, IEEE CONF COMPUT; Zhang Weiwei, 2008, EUR C COMP VIS; Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhu Jun-Yan, 2017, ICCV; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	47	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905020
C	Sandkuhler, R; Andermatt, S; Bauman, G; Nyilas, S; Jud, C; Cattin, PC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sandkuhler, Robin; Andermatt, Simon; Bauman, Grzegorz; Nyilas, Sylvia; Jud, Christoph; Cattin, Philippe C.			Recurrent Registration Neural Networks for Deformable Image Registration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Parametric spatial transformation models have been successfully applied to image registration tasks. In such models, the transformation of interest is parameterized by a fixed set of basis functions as for example B-splines. Each basis function is located on a fixed regular grid position among the image domain because the transformation of interest is not known in advance. As a consequence, not all basis functions will necessarily contribute to the final transformation which results in a non-compact representation of the transformation. We reformulate the pairwise registration problem as a recursive sequence of successive alignments. For each element in the sequence, a local deformation defined by its position, shape, and weight is computed by our recurrent registration neural network. The sum of all local deformations yield the final spatial alignment of both images. Formulating the registration problem in this way allows the network to detect non-aligned regions in the images and to learn how to locally refine the registration properly. In contrast to current non-sequence-based registration methods, our approach iteratively applies local spatial deformations to the images until the desired registration accuracy is achieved. We trained our network on 2D magnetic resonance images of the lung and compared our method to a standard parametric B-spline registration. The experiments show, that our method performs on par for the accuracy but yields a more compact representation of the transformation. Furthermore, we achieve a speedup of around 15 compared to the B-spline registration.	[Sandkuhler, Robin; Andermatt, Simon; Jud, Christoph; Cattin, Philippe C.] Univ Basel, Dept Biomed Engn, Basel, Switzerland; [Bauman, Grzegorz] Univ Basel Hosp, Dept Radiol, Div Radiol Phys, Basel, Switzerland; [Nyilas, Sylvia] Univ Bern, Univ Hosp Bern, Inselspital, Pediat Resp Med,Dept Pediat, Bern, Switzerland	University of Basel; University of Basel; University of Bern; University Hospital of Bern	Sandkuhler, R (corresponding author), Univ Basel, Dept Biomed Engn, Basel, Switzerland.	robin.sandkuehler@unibas.ch; simon.andermatt@unibas.ch; grzegorz.bauman@usb.ch; sylvia.nyilas@insel.ch; christoph.jud@unibas.ch; philippe.cattin@unibas.ch		Cattin, Philippe C./0000-0001-8785-2713	Swiss National Science Foundation [SNF 320030_149576]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	We would like to thank Oliver Bieri, Orso Pusterla (Division of Radiological Physics, Department of Radiology, University Hospital Basel, Switzerland), and Philipp Latzin (Pediatric Respiratory Medicine, Department of Pediatrics, Inselspital, Bern University Hospital, University of Bern, Switzerland) for there support during the development of this work. Furthermore, we thank the Swiss National Science Foundation for funding this project (SNF 320030_149576).	Andermatt Simon, 2017, INT WORKSH BRAINL GL; Bauman G, 2016, MAGN RESON MED, V75, P1647, DOI 10.1002/mrm.25697; Bengio Y., 2014, ARXIV14061078; Dalca AV, 2018, LECT NOTES COMPUT SC, V11070, P729, DOI 10.1007/978-3-030-00928-1_82; de Vos BD, 2017, LECT NOTES COMPUT SC, V10553, P204, DOI 10.1007/978-3-319-67558-9_24; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Haskins G, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01060-x; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hu YP, 2018, MED IMAGE ANAL, V49, P1, DOI 10.1016/j.media.2018.07.002; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jud C, 2016, IEEE COMPUT SOC CONF, P449, DOI 10.1109/CVPRW.2016.63; Jud C, 2016, LECT NOTES COMPUT SC, V10019, P10, DOI 10.1007/978-3-319-47157-0_2; Kingma D.P, P 3 INT C LEARNING R; Krebs J., 2017, LNCS, P344, DOI DOI 10.1007/978-3-319-66182-7_40; Liao R, 2017, AAAI CONF ARTIF INTE, P4168; Liu R, 2018, ADV NEUR IN, V31; Miao S, 2018, AAAI CONF ARTIF INTE, P4694; Reddi Sashank J, 2019, ARXIV190409237, DOI DOI 10.48550/ARXIV.1904.09237; Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284; Sandkuhler R., 2018, AIRLAB AUTOGRAD IMAG, p1806.09907; Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stergios C, 2018, LECT NOTES COMPUT SC, V11040, P13, DOI 10.1007/978-3-030-00946-5_2; Vishnevskiy V, 2017, IEEE T MED IMAGING, V36, P385, DOI 10.1109/TMI.2016.2610583; Wan L., 2013, P INT C MACHINE LEAR, P1058	28	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900036
C	Satorras, VG; Akata, Z; Welling, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Satorras, Victor Garcia; Akata, Zeynep; Welling, Max			Combining Generative and Discriminative Models for Hybrid Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A graphical model is a structured representation of the data generating process. The traditional method to reason over random variables is to perform inference in this graphical model. However, in many cases the generating process is only a poor approximation of the much more complex true data generating process, leading to suboptimal estimations. The subtleties of the generative process are however captured in the data itself and we can "learn to infer", that is, learn a direct mapping from observations to explanatory latent variables. In this work we propose a hybrid model that combines graphical inference with a learned inverse model, which we structure as in a graph neural network, while the iterative algorithm as a whole is formulated as a recurrent neural network. By using cross-validation we can automatically balance the amount of work performed by graphical inference versus learned inference. We apply our ideas to the Kalman filter, a Gaussian hidden Markov model for time sequences, and show, among other things, that our model can estimate the trajectory of a noisy chaotic Lorenz Attractor much more accurately than either the learned or graphical inference run in isolation.	[Satorras, Victor Garcia; Welling, Max] Univ Amsterdam, UvA Bosch Delta Lab, Amsterdam, Netherlands; [Akata, Zeynep] Univ Tubingen, Cluster Excellence ML, Tubingen, Germany; [Akata, Zeynep] Univ Amsterdam, Amsterdam, Netherlands	University of Amsterdam; Eberhard Karls University of Tubingen; University of Amsterdam	Satorras, VG (corresponding author), Univ Amsterdam, UvA Bosch Delta Lab, Amsterdam, Netherlands.	v.garciasatorras@uva.nl; zeynep.akata@uni-tuebingen.de; m.welling@uva.nl						Abbeel P., 2005, ROBOTICS SCI SYSTEMS, V1, P289, DOI 10.15607/rss.2005.i.038; Andrychowicz M, 2016, ADV NEUR IN, V29; Belanger D, 2017, PR MACH LEARN RES, V70; Bishop CM, 2006, PATTERN RECOGNITION; Bruna J, 2013, PROC INT C LEARN REP; Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638; Chen L.-C., 2014, ARXIV PREPRINT ARXIV; Chung J., 2014, ARXIV14123555; Coskun H., 2017, ARXIV170801885; Crick C., 2002, P 19 C UNC ART INT, P159, DOI DOI 10.1371/JOURNAL.PONE.0041827; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516; Falkovich G., 2011, FLUID MECH SHORT COU; Haarnoja T, 2016, ADV NEUR IN, V29; Henaff M, 2015, ARXIV150605163; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kipf T, 2018, PR MACH LEARN RES, V80; Kipf TN, 2016, P INT C LEARN REPR; Koller D., 2009, PROBABILISTIC GRAPHI; Labbe R., 2014, KALMAN BAYESIAN FILT; Li Yujia, 2015, ARXIV151105493; LJUNG L, 1979, IEEE T AUTOMAT CONTR, V24, P36, DOI 10.1109/TAC.1979.1101943; Marino J, 2018, PR MACH LEARN RES, V80; Mirowski P, 2009, LECT NOTES ARTIF INT, V5782, P128, DOI 10.1007/978-3-642-04174-7_9; Murphy K.P., 2012, PROBABILISTIC PERSPE; Nauata N., 2018, ARXIV180206459; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2; Putzky P., 2017, ARXIV PREPRINT ARXIV; RAUCH HE, 1965, AIAA J, V3, P1445, DOI 10.2514/3.3166; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Yoon K., 2018, ARXIV180307710; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	37	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905047
C	Singh, R; Sahani, M; Gretton, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Singh, Rahul; Sahani, Maneesh; Gretton, Arthur			Kernel Instrumental Variable Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPERATOR-EQUATIONS; IDENTIFICATION; MODELS; RATES; REGULARIZATION; INFERENCE; SPACES	Instrumental variable (IV) regression is a strategy for learning causal relationships in observational data. If measurements of input X and output Y are confounded, the causal relationship can nonetheless be identified if an instrumental variable Z is available that influences X directly, but is conditionally independent of Y given X and the unmeasured confounder. The classic two-stage least squares algorithm (2SLS) simplifies the estimation problem by modeling all relationships as linear functions. We propose kernel instrumental variable regression (KIV), a nonparametric generalization of 2SLS, modeling relations among X, Y, and Z as nonlinear functions in reproducing kernel Hilbert spaces (RKHSs). We prove the consistency of KIV under mild assumptions, and derive conditions under which convergence occurs at the minimax optimal rate for unconfounded, single-stage RKHS regression. In doing so, we obtain an efficient ratio between training sample sizes used in the algorithm's first and second stages. In experiments, KIV outperforms state of the art alternatives for nonparametric IV regression.	[Singh, Rahul] MIT Economics, Cambridge, MA 02142 USA; [Sahani, Maneesh; Gretton, Arthur] UCL, Gatsby Unit, London, England	University of London; University College London	Singh, R (corresponding author), MIT Economics, Cambridge, MA 02142 USA.	rahul.singh@mit.edu; maneesh@gatsby.ucl.ac.uk; arthur.gretton@gmail.com		Gretton, Arthur/0000-0003-3169-7624				ANDERSON TW, 1981, J AM STAT ASSOC, V76, P598, DOI 10.2307/2287517; ANGRIST JD, 1990, AM ECON REV, V80, P313; Angrist JD, 1996, J AM STAT ASSOC, V91, P444, DOI 10.2307/2291629; ANGRIST JD, 1995, J BUS ECON STAT, V13, P225, DOI 10.2307/1392377; Arbel Michael, 2018, INT C ART INT STAT, P1337; ARELLANO M, 1991, REV ECON STUD, V58, P277, DOI 10.2307/2297968; Bell J, 2016, TECHNICAL REPORT; Berlinet A., 2011, REPRODUCING KERNEL H; Bishop C.M, 2006, PATTERN RECOGN; Blundell R, 2007, ECONOMETRICA, V75, P1613, DOI 10.1111/j.1468-0262.2007.00808.x; Blundell R, 2012, QUANT ECON, V3, P29, DOI 10.3982/QE91; BOOTS B, 2013, P 29 INT C UNC ART I, P92; BOUND J, 1995, J AM STAT ASSOC, V90, P443, DOI 10.2307/2291055; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Carmeli C, 2006, ANAL APPL, V4, P377, DOI 10.1142/S0219530506000838; Carrasco M, 2007, HBK ECON, V2, P5633, DOI 10.1016/S1573-4412(07)06077-1; Chen XH, 2018, QUANT ECON, V9, P39, DOI 10.3982/QE722; Chen Xiaohong, 1997, TECHNICAL REPORT; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Cortes CE, 2005, TRANSPORT RES REC, P153, DOI 10.3141/1923-17; Cucker F, 2002, B AM MATH SOC, V39, P1; Darolles S, 2004, J ECONOMETRICS, V119, P323, DOI 10.1016/S0304-4076(03)00199-4; Darolles S, 2011, ECONOMETRICA, V79, P1541, DOI 10.3982/ECTA6539; De Vito Ernesto, 2005, TECHNICAL REPORT; Downey C, 2017, ADV NEUR IN, V30; Dutordoir V., 2018, ADV NEURAL INFORM PR, P2385; Engl Heinz W, 1996, REGULARIZATIONOF INV, V375; FLORENS JP, 2003, ADV EC ECONOMETRICS, V0002, P00046; Fukumizu K, 2013, J MACH LEARN RES, V14, P3753; Gretton Arthur, 2018, TECHNICAL REPORT; Grtinewalder Steffen, 2012, INT C MACH LEARN, V5; Hartford J, 2017, PR MACH LEARN RES, V70; Hefny A., 2015, ADV NEURAL INFORM PR; Hernan Miguel A, 2019, CAUSAL INFERENCE WHA; Hsu D, 2012, ELECTRON COMMUN PROB, V17, P1, DOI 10.1214/ECP.v17-1869; Imbens GW, 2009, ECONOMETRICA, V77, P1481, DOI 10.3982/ECTA7108; Kress Rainer, 1989, LINEAR INTEGRAL EQUA, V3; Lever Guy, 2012, ICML, P1603; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Murray MP, 2006, J ECON PERSPECT, V20, P111, DOI 10.1257/jep.20.4.111; NASHED MZ, 1974, SIAM J MATH ANAL, V5, P974, DOI 10.1137/0505095; NASHED MZ, 1974, B AM MATH SOC, V80, P1213, DOI 10.1090/S0002-9904-1974-13684-0; NASHED MZ, 1974, MATH COMPUT, V28, P69, DOI 10.1090/S0025-5718-1974-0461895-1; Newey WK, 2003, ECONOMETRICA, V71, P1565, DOI 10.1111/1468-0262.00459; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Saunders C., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P515; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Singh Rahul, 2019, TECHNICAL REPORT; Smale S, 2005, APPL COMPUT HARMON A, V19, P285, DOI 10.1016/j.acha.2005.03.001; Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y; Song Le, 2010, AISTATS JMLR W CP, P765; Song Le, 2009, P 26 INT C MACHINE L, P961, DOI 10.1145/1553374.1553497; Sriperumbudur B. K., 2010, P 13 INT C ART INT S, V9, P773; Staiger D, 1997, ECONOMETRICA, V65, P557, DOI 10.2307/2171753; Steinwart I., 2008, SUPPORT VECTOR MACHI; Stock JH, 2002, J BUS ECON STAT, V20, P518, DOI 10.1198/073500102288618658; Stock JH, 2003, J ECON PERSPECT, V17, P177, DOI 10.1257/089533003769204416; Sugiyama M, 2010, IEICE T INF SYST, VE93D, P583, DOI 10.1587/transinf.E93.D.583; Sutherland Dougal, 2017, TECHNICAL REPORT; Szabo Z, 2015, JMLR WORKSH CONF PRO, V38, P948; Szabo Z, 2016, J MACH LEARN RES, V17; Wahba G., 1990, SPLINE MODELS OBSERV, V59; Wright PG., 1928, TARIFF ANIMAL VEGETA; [No title captured]	68	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304058
C	Sotoudeh, M; Thakur, AV		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sotoudeh, Matthew; Thakur, Aditya V.			Computing Linear Restrictions of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A linear restriction of a function is the same function with its domain restricted to points on a given line. This paper addresses the problem of computing a succinct representation for a linear restriction of a piecewise-linear neural network. This primitive, which we call EXACTLINE, allows us to exactly characterize the result of applying the network to all of the infinitely many points on a line. In particular, EXACTLINE computes a partitioning of the given input line segment such that the network is affine on each partition. We present an efficient algorithm for computing EXACTLINE for networks that use ReLU, MaxPool, batch normalization, fully-connected, convolutional, and other layers, along with several applications. First, we show how to exactly determine decision boundaries of an ACAS Xu neural network, providing significantly improved confidence in the results compared to prior work that sampled finitely many points in the input space. Next, we demonstrate how to exactly compute integrated gradients, which are commonly used for neural network attributions, allowing us to show that the prior heuristic-based methods had relative errors of 25-45% and show that a better sampling method can achieve higher accuracy with less computation. Finally, we use EXACTLINE to empirically falsify the core assumption behind a well-known hypothesis about adversarial examples, and in the process identify interesting properties of adversarially-trained networks.	[Sotoudeh, Matthew; Thakur, Aditya V.] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA	University of California System; University of California Davis	Sotoudeh, M (corresponding author), Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.	masotoudeh@ucdavis.edu; avthakur@ucdavis.edu		Sotoudeh, Matthew/0000-0003-2060-1009	AWS Cloud Credits for Research	AWS Cloud Credits for Research	We thank Nina Amenta, Yong Jae Lee, Cindy Rubio-Gonzalez, and Mukund Sundararajan for their feedback and suggestions on this work. This material is based upon work supported by AWS Cloud Credits for Research.	Adusumalli M, 2018, ROUT INT HANDB, P121; Anderson Greg, 2019, CORR; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2018, BRIEF BIOINFORM, DOI DOI 10.1093/bib/bbx044; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Bastani Osbert, 2016, ADV NEURAL INFORM PR; Bau David, 2017, 2017 IEEE C COMP VIS; Beyer D, 2019, INT J SOFTW TOOLS TE, V21, P1, DOI 10.1007/s10009-017-0469-y; Bunel R., 2018, ADV NEURAL INFORM PR, P4795; Carlini M, 2018, UPDATES SURG SER, P1, DOI 10.1007/978-88-470-3955-1; Ching T, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2017.0387; Croce F, 2019, PR MACH LEARN RES, V89; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171, DOI [10.18653/v1/N19-1423, DOI 10.18653/V1/N19-1423]; Ehlers Ruediger, 2017, INT S AUT TECHN VER; Gehr T., 2018, 2018 IEEE S SEC PRIV; Goodfellow I., 2016, DEEP LEARNING; Goodfellow Ian J., 2015, INT C LEARNING REPRE; Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009; Hanin B, 2019, PR MACH LEARN RES, V97; Hosny Ahmed, 2018, NAT REV CANCER, P1; Huang Xiaowei, 2017, INT C COMP AID VER C; Jordan M., 2019, ARXIV190308778; Julian KD, 2019, J GUID CONTROL DYNAM, V42, P598, DOI 10.2514/1.G003724; Katz G., 2017, INT C COMP AID VER; Kim Been, 2018, INT C MACH LEARN ICM; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Mendelson EB, 2019, AM J ROENTGENOL, V212, P293, DOI 10.2214/AJR.18.20532; Mirman M., 2018, INT C MACH LEARN ICM; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Preuer Kristina, 2019, CORR; Shrikumar Avanti, 2016, CORR; Singh G., 2019, PACMPL, V3; Sundararajan Mukund, 2017, INT C MACH LEARN ICM; Szegedy C, 2016, P IEEE C COMP VIS PA; Szegedy Christian, 2014, INT C LEARN REOR ICL; Tanay T., 2016, ARXIV PREPRINT ARXIV; Wang SQ, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1599; Weng T., 2018, INT C MACH LEARN ICM; Xiang W., 2017, ARXIV171208163; Xiang Weiming, 2018, 2018 ANN AM CONTR C; Yan Q., 2018, ARXIV180512233; Yosinski J., 2015, ICML DEEP LEARN WORK; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	43	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905077
C	Su, LL; Yang, PK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Su, Lili; Yang, Pengkun			On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider training over-parameterized two-layer neural networks with Rectified Linear Unit (ReLU) using gradient descent (GD) method. Inspired by a recent line of work, we study the evolutions of network prediction errors across GD iterations, which can be neatly described in a matrix form. When the network is sufficiently over-parameterized, these matrices individually approximate an integral operator which is determined by the feature vector distribution rho only. Consequently, GD method can be viewed as approximately applying the powers of this integral operator on the underlying function f* that generates the responses. We show that if f* admits a low-rank approximation with respect to the eigenspaces of this integral operator, then the empirical risk decreases to this low-rank approximation error at a linear rate which is determined by f* and rho only, i.e., the rate is independent of the sample size n. Furthermore, if f* has zero low-rank approximation error, then, as long as the width of the neural network is Omega(n log n), the empirical risk decreases to Theta(1/root n). To the best of our knowledge, this is the first result showing the sufficiency of nearly-linear network over-parameterization. We provide an application of our general results to the setting where rho is the uniform distribution on the spheres and f* is a polynomial. Throughout this paper, we consider the scenario where the input dimension d is fixed.	[Su, Lili] MIT, CSAIL, Cambridge, MA 02139 USA; [Yang, Pengkun] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA	Massachusetts Institute of Technology (MIT); Princeton University	Su, LL (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	lilisu@mit.edu; pengkuny@princeton.edu						Allen-Zhu Zeyuan, 2018, ARXIV181104918; [Anonymous], ARXIV181103962; Arora Sanjeev, 2019, ARXIV190108584; Athalye VR, 2018, SCIENCE, V359, P1024, DOI 10.1126/science.aao6058; Brutzkus A, 2017, PR MACH LEARN RES, V70; Cao Yuan, 2019, ARXIV190513210; Chizat L., 2018, NOTE LAZY TRAINING S; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Dunford N., 1963, LINEAR OPERATORS 2; Feng Dai, 2013, APPROXIMATION THEORY; Ghorbani B., 2019, ARXIV190412191; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Cantero MJ, 2012, SIAM J NUMER ANAL, V50, P307, DOI 10.1137/110829568; Klusowski J.M., 2018, APPROXIMATION COMBIN; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li M, PMLR; Li Y., 2018, ADV NEURAL INFORM PR; Marder E, 2006, NAT REV NEUROSCI, V7, P563, DOI 10.1038/nrn1949; Nesterov Yurii, 2018, SPRINGER OPTIMIZATIO, V137; Oymak S., 2019, ARXIV190204674; Rivest R.L., 1989, NIPS, P494; Rosasco L, 2010, J MACH LEARN RES, V11, P905; Saad D, 1996, ADV NEUR IN, V8, P302; Song M, 2018, ARXIV180406561; Szego G, 1975, ORTHOGONAL POLYNOMIA; Tian Y., 2016, SYMMETRY BREAKING CO; Vempala S., 2018, ARXIV180502677; Wang Yi, LECTURENOTES; Woodworth B., 2019, ARXIV190605827; Xie B, 2017, PR MACH LEARN RES, V54, P1216; Yehudai Gilad, 2019, ARXIV190400687; Zhang Chiyuan, 2016, ARXIV161103530; Zhong K, 2017, PR MACH LEARN RES, V70; Zhou LL, 2017, IEEE INT CONF ELECTR, P599, DOI 10.1109/ICEIEC.2017.8076638; Zou D, 2018, ARXIV181108888	38	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302062
C	Tantipongpipat, U; Samadi, S; Singh, M; Morgenstern, J; Vempala, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tantipongpipat, Uthaipon (Tao); Samadi, Samira; Singh, Mohit; Morgenstern, Jamie; Vempala, Santosh			Multi-Criteria Dimensionality Reduction with Applications to Fairness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEMIDEFINITE PROGRAMS; RANK; SYSTEMS; FIT	Dimensionality reduction is a classical technique widely used for data analysis. One foundational instantiation is Principal Component Analysis (PCA), which minimizes the average reconstruction error. In this paper, we introduce the multi-criteria dimensionality reductionproblem where we are given multiple objectives that need to be optimized simultaneously. As an application, our model captures several fairness criteria for dimensionality reduction such as the Fair-PCA problem introduced by Samadi et al. [2018] and the Nash Social Welfare (NSW) problem. In the Fair-PCA problem, the input data is divided into k groups, and the goal is to find a single d-dimensional representation for all groups for which the maximum reconstruction error of any one group is minimized. In NSW the goal is to maximize the product of the individual variances of the groups achieved by the common low-dimensional space. Our main result is an exact polynomial-time algorithm for the two-criteria dimensionality reduction problem when the two criteria are increasing concave functions. As an application of this result, we obtain a polynomial time algorithm for Fair-PCA for k = 2 groups, resolving an open problem of Samadi et al. [2018], and a polynomial time algorithm for NSW objective for k 2 groups. We also give approximation algorithms for k > 2. Our technical contribution in the above results is to prove new low-rank properties of extreme point solutions to semi-definite programs. We conclude with experiments indicating the effectiveness of algorithms based on extreme point solutions of semi-definite programs on several real-world datasets.	[Tantipongpipat, Uthaipon (Tao); Samadi, Samira; Singh, Mohit; Morgenstern, Jamie; Vempala, Santosh] Georgia Inst Technol, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Tantipongpipat, U (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	tao@gatech.edu; ssamadi6@gatech.edu; mohit.singh@isye.gatech.edu; jamiemmt.cs@gatech.edu; vempala@cc.gatech.edu			NSF [AF:1910423, AF:1717947, CCF-1563838, CCF-1717349]	NSF(National Science Foundation (NSF))	Supported by NSF-AF:1910423 and NSF-AF:1717947.; Supported in part by NSF awards CCF-1563838 and CCF-1717349.	[Anonymous], 2001, LECT MODERN CONVEX O; Bandeira A.S., 2016, C LEARNING THEORY, P361; BARVINOK AI, 1995, DISCRETE COMPUT GEOM, V13, P189, DOI 10.1007/BF02574037; BARVINOK AI, 1993, DISCRETE COMPUT GEOM, V10, P1, DOI 10.1007/BF02573959; Bhangale A, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1407; Bhangale A, 2015, LECT NOTES COMPUT SC, V9134, P193, DOI 10.1007/978-3-662-47672-7_16; BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Celis L Elisa, 2018, ARXIV180204023; CHIERICHETTI F, 2017, NIPS, P5029; Crawford Kate, 2017, K CRAWF NIPS 2017 LO; Deb Kalyanmoy, 2014, SEARCH METHODOLOGIES, V4, P5, DOI DOI 10.1007/978-1-4614-6940-7_15; Fang Zuyuan, 2012, IEEE INFOCOM, V2, P1284; Grandoni F, 2014, MATH PROGRAM, V146, P525, DOI 10.1007/s10107-013-0703-7; GRIGOREV DY, 1988, J SYMB COMPUT, V5, P37, DOI 10.1016/S0747-7171(88)80005-1; Grigoriev D, 2005, COMPUT COMPLEX, V14, P20, DOI 10.1007/s00037-005-0189-7; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; IEZZONI AF, 1991, HORTSCIENCE, V26, P334, DOI 10.21273/HORTSCI.26.4.334; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; KALAI E, 1977, ECONOMETRICA, V45, P1623, DOI 10.2307/1913954; KALAI E, 1975, ECONOMETRICA, V43, P513, DOI 10.2307/1914280; KANEKO M, 1979, ECONOMETRICA, V47, P423, DOI 10.2307/1914191; Kleindessner Matthaus, 2019, ARXIV190108668; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Lau L. C., 2011, ITERATIVE METHODS CO, V46; Nash JF, 1950, ECONOMETRICA, V18, P155, DOI 10.2307/1907266; Noble S.U., 2018, ALGORITHMS OPPRESSIO; Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Ravi R., 1996, Algorithm Theory - SWAT '96. 5th Scandinavian Workshop on Algorithm Theory. Proceedings, P66; Raychaudhuri S, 2000, Pac Symp Biocomput, P455; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Samadi Samira, 2018, ADV NEURAL INFORM PR, P10976; So AMC, 2008, MATH OPER RES, V33, P910, DOI 10.1287/moor.1080.0326; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wei GY, 2010, J SUPERCOMPUT, V54, P252, DOI 10.1007/s11227-009-0318-1; Yeh IC, 2009, EXPERT SYST APPL, V36, P2473, DOI 10.1016/j.eswa.2007.12.020	40	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906078
C	Tschiatschek, S; Ghosh, A; Haug, L; Devidze, R; Singla, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tschiatschek, Sebastian; Ghosh, Ahana; Haug, Luis; Devidze, Rati; Singla, Adish			Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MAXIMUM CAUSAL ENTROPY	Inverse reinforcement learning (IRL) enables an agent to learn complex behavior by observing demonstrations from a (near-)optimal policy. The typical assumption is that the learner's goal is to match the teacher's demonstrated behavior. In this paper, we consider the setting where the learner has its own preferences that it additionally takes into consideration. These preferences can for example capture behavioral biases, mismatched worldviews, or physical constraints. We study two teaching approaches: learner-agnosticteaching, where the teacher provides demonstrations from an optimal policy ignoring the learner's preferences, and learner-awareteaching, where the teacher accounts for the learner's preferences. We design learner-aware teaching algorithms and show that significant performance improvements can be achieved over learner-agnostic teaching.	[Tschiatschek, Sebastian] Microsoft Res, Redmond, WA 98052 USA; [Ghosh, Ahana; Devidze, Rati; Singla, Adish] MPI SWS, New York, NY USA; [Haug, Luis] Swiss Fed Inst Technol, Zurich, Switzerland	Microsoft; Swiss Federal Institutes of Technology Domain; ETH Zurich	Tschiatschek, S (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	setschia@microsoft.com; gahana@mpi-sws.org; lhaug@inf.ethz.ch; rdevidze@mpi-sws.org; adishs@mpi-sws.org	Singla, Adish/ABG-8960-2021	Tschiatschek, Sebastian/0000-0002-2592-0108	Microsoft Research through its PhD Scholarship Programme	Microsoft Research through its PhD Scholarship Programme(Microsoft)	This work was supported by Microsoft Research through its PhD Scholarship Programme.	Abbeel P., 2004, ICML; Altman E, 1999, CONSTRAINED MARKOV D, V7; Amin Kareem, 2017, NEURAL INFORM PROCES, P1813; [Anonymous], 2018, ARXIV180105927; Boularias A., 2011, INT C ART INT STAT, P182; Boyd S, 2004, CONVEX OPTIMIZATION; Brown, 2019, AAAI; Brown D. S., 2018, PROC C ROBOT LEARN, P362; Brown DS, 2018, AAAI CONF ARTIF INTE, P2754; Cakmak M., 2012, 26 AAAI C ART INT; Chen Yuxin, 2018, ADV NEURAL INFORM PR, P1476; COHN R, 2011, P 10 INT C AUT AG MU, V3, P1287; Cui YC, 2018, IEEE INT CONF ROBOT, P6907; De G. G., 1960, PROBLEMES DECISIONS, V2, P161; Dimitrakakis C., 2017, ADV NEURAL INFORM PR, P5443; Ghosh A, 2019, WORKSH SAF ROB DEC M; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Hadfield- Menell Dylan, 2016, NIPS; Haug L, 2018, ADV NEURAL INFORM PR, P8473; Hunziker A, 2019, ADV NEURAL INFORM PR; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Kamalaruban P, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2692; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Levine S, 2010, NIPS, P1342; Liu J, 2016, J MACH LEARN RES, V17; Liu W, 2018, ICML; Mac Aodha O, 2018, PROC CVPR IEEE, P3820, DOI 10.1109/CVPR.2018.00402; Melo Francisco S, 2018, P INT JOINT C ART IN, P2567; Mendez J. A. M., 2018, ADV NEURAL INFORM PR, P4507; Osa Takayuki, 2018, ARXIV181106711; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Singla A., 2013, NIPS WORKSH DAT DRIV; Singla A., 2014, ICML; Sinha A, 2018, IEEE T EVOLUT COMPUT, V22, P276, DOI 10.1109/TEVC.2017.2712906; YEO T, 2019, AAAI, P5684; Zhou ZY, 2018, IEEE T AUTOMAT CONTR, V63, P2787, DOI 10.1109/TAC.2017.2775960; Zhu X., 2013, ADV NEURAL INFORM PR, P1905; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083; Ziebart B.D., 2010, THESIS CARNEGIE MELL; Ziebart B. D., 2008, AAAI; Ziebart BD, 2013, IEEE T INFORM THEORY, V59, P1966, DOI 10.1109/TIT.2012.2234824	41	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304018
C	van der Hoeven, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		van der Hoeven, Dirk			User-Specified Local Differential Privacy in Unconstrained Adaptive Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Local differential privacy is a strong notion of privacy in which the provider of the data guarantees privacy by perturbing the data with random noise. In the standard application of local differential privacy the distribution of the noise is constant and known by the learner. In this paper we generalize this approach by allowing the provider of the data to choose the distribution of the noise without disclosing any parameters of the distribution to the learner, under the constraint that the distribution is symmetrical. We consider this problem in the unconstrained Online Convex Optimization setting with noisy feedback. In this setting the learner receives the subgradient of a loss function, perturbed by noise, and aims to achieve sublinear regret with respect to some competitor, without constraints on the norm of the competitor. We derive the first algorithms that have adaptive regret bounds in this setting, i.e. our algorithms adapt to the unknown competitor norm, unknown noise, and unknown sum of the norms of the subgradients, matching state of the art bounds in all cases.	[van der Hoeven, Dirk] Leiden Univ, Math Inst, NL-2333 CA Leiden, Netherlands	Leiden University; Leiden University - Excl LUMC	van der Hoeven, D (corresponding author), Leiden Univ, Math Inst, NL-2333 CA Leiden, Netherlands.	dirkvderhoeven@gmail.com			Netherlands Organization for Scientific Research (NWO) [TOP2EW.15.211]	Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	The author would like to thank Tim van Erven for his comments on an earlier version of this paper. The author was supported by the Netherlands Organization for Scientific Research (NWO grant TOP2EW.15.211).	[Anonymous], 2017, P MACHINE LEARNING R; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cutkosky A., 2018, C LEARN THEOR, P1493; Cutkosky A., 2017, C LEARN THEOR COLT, P643; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Duchi JC, 2014, J ACM, V61, DOI 10.1145/2666468; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Foster D. J., 2018, C LEARN THEOR COLT, P3028; Foster D. J., 2015, ADV NEURAL INFORM PR, V28, P3375; Foster Dylan J, 2017, ADV NEURAL INFORM PR, P6020; Hiriart-Urruty JB, 2006, ADV MECH MATH, V12, P35; Jain P, 2012, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2012/05/020; Jain P, 2014, PR MACH LEARN RES, V32; Jun K. -S., 2019, P 32 ANN C LEARN THE; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Koolen Wouter M, 2015, COLT, P1155; Lee C., 2017, ARXIV171110019; McMahan H. B., 2014, P 27 C LEARN THEOR, P1020; Melhem SB, 2017, 2017 IEEE 5TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD 2017), P32, DOI 10.1109/FiCloud.2017.37; Orabona F., 2017, ADV NEURAL INFORM PR, P2160; Orabona F., 2016, ADV NEURAL INFORM PR, V29, P577; Orabona F, 2018, THEOR COMPUT SCI, V716, P50, DOI 10.1016/j.tcs.2017.11.021; Song S, 2015, JMLR WORKSH CONF PRO, V38, P894; Streeter M., 2012, ADV NEURAL INFORM PR; Thakurta A., 2013, ADV NEURAL INFORM PR, P2733; Troshina K, 2010, IEEE INT WORK C SO, P179, DOI 10.1109/SCAM.2010.24; Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651	28	4	4	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905072
C	Virtanen, S; Girolami, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Virtanen, Seppo; Girolami, Mark			Precision-Recall Balanced Topic Modelling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Topic models are becoming increasingly relevant probabilistic models for dimensionality reduction of text data, inferring topics that capture meaningful themes of frequently co-occurring terms. We formulate topic modelling as an information retrieval task, where the goal is, based on the latent topic representation, to capture relevant term co-occurrence patterns. We evaluate performance for this task rigorously with regard to two types of errors, false negatives and positives, based on the well-known precision-recall trade-off and provide a statistical model that allows the user to balance between the contributions of the different error types. When the user focuses solely on the contribution of false negatives ignoring false positives altogether our proposed model reduces to a standard topic model. Extensive experiments demonstrate the proposed approach is effective and infers more coherent topics than existing related approaches.	[Virtanen, Seppo; Girolami, Mark] Univ Cambridge, Cambridge, England; [Girolami, Mark] Alan Turing Inst, London, England	University of Cambridge	Virtanen, S (corresponding author), Univ Cambridge, Cambridge, England.	sjv35@cam.ac.uk; mag92@cam.ac.uk		Girolami, Mark/0000-0003-3008-253X	EPSRC, Inference COmputation and Numerics for Insights into Cities (ICONIC) [EP/P020720/1]	EPSRC, Inference COmputation and Numerics for Insights into Cities (ICONIC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors were supported by the EPSRC grant EP/P020720/1, Inference COmputation and Numerics for Insights into Cities (ICONIC), https://iconicmath.org/.	AlSumait Loulwah, 2009, ECML; [Anonymous], 2010, BAYESIAN NONPARAMETR; Arora Sanjeev, 2012, 2012 IEEE 53 ANN S F; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Casella G, 2001, Biostatistics, V2, P485, DOI 10.1093/biostatistics/2.4.485; Chang J., 2009, NIPS; Chemudugunta Chaitanya, 2006, NIPS; Flaxman S., 2018, ARXIV180102858; Griffiths Thomas L, 2004, NIPS; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hinton G., 2002, NIPS; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Mimno D., 2011, EMNLP; Minka Thomas, 2002, UAI; Peltonen J, 2011, AISTATS; Wallach H., 2009, ICML; Wang C, 2009, NIPS	17	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306072
C	Wang, B; Yuan, BJ; Shi, ZQ; Osher, SJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Bao; Yuan, Binjie; Shi, Zuoqiang; Osher, Stanley J.			ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We unify the theory of optimal control of transport equations with the practice of training and testing of ResNets. Based on this unified viewpoint, we propose a simple yet effective ResNets ensemble algorithm to boost the accuracy of the robustly trained model on both clean and adversarial images. The proposed algorithm consists of two components: First, we modify the base ResNets by injecting a variance specified Gaussian noise to the output of each residual mapping, and it results in a special type of neural stochastic ordinary differential equation. Second, we average over the production of multiple jointly trained modified ResNets to get the final prediction. These two steps give an approximation to the Feynman-Kac formula for representing the solution of a convection-diffusion equation. For the CIFAR10 benchmark, this simple algorithm leads to a robust model with a natural accuracy of 85.62% on clean images and a robust accuracy of 57:94% under the 20 iterations of the IFGSM attack, which outperforms the current state-of-the-art in defending against IFGSM attack on the CIFAR10. The code is available at https://github.com/BaoWangMath/EnResNet.	[Wang, Bao; Osher, Stanley J.] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA; [Yuan, Binjie] Tsinghua Univ, Sch Aerosp, Beijing, Peoples R China; [Shi, Zuoqiang] Tsinghua Univ, Dept Math, Beijing, Peoples R China	University of California System; University of California Los Angeles; Tsinghua University; Tsinghua University	Wang, B (corresponding author), Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.	wangbaonj@gmail.com; ybj14@mail.tsinghua.edu.cn; zqshi@mail.tsinghua.edu.cn; sjo@math.ucla.edu						Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385; Athalye A, 2018, PR MACH LEARN RES, V80; Cao H, 2017, 2017 INTERNATIONAL CONFERENCE ON MATERIALS SCIENCE AND BIOLOGICAL ENGINEERING (ICMSBE 2017), P33; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen T.Q., 2018, NEURIPS, P2610; Chen Xinyun, 2017, ARXIV171205526; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cohen J., 2019, ARXIV190202918V1; Goodfellow I. J., 2014, ARXIV14126275; Grosse K., 2016, ADVERSARIAL PERTURBA; Guisti A., 2016, IEEE ROBOTICS AUTOMA, P661; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Ilyas A, 2018, PR MACH LEARN RES, V80; KAC M, 1949, T AM MATH SOC, V65, P1, DOI 10.2307/1990512; Kingma D.P, P 3 INT C LEARNING R; Kloeden P., 1992, NUMERICAL SOLUTION S; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; LadyZenskaja O.A., 1968, LINEAR QUASILINEAR E; Lecuyer M., 2019, 2019 IEEE S SEC PRIV; Li B., 2018, ARXIV180903113, P1; Li Z, 2017, ARXIV170805115; Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23; Lu Y., 2018, INT C MACH LEARN; Madry Aleksander, 2017, ARXIV; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Papernot Nicolas, 2016, ARXIV161103814; Raghunathan A, 2018, ADV NEUR IN, V31; Raghunathan Aditi, 2018, ARXIV180109344; Rakin A. Siraj, 2018, ARXIV181109310; Ross A.S., 2017, ARXIV171109404; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Salman H, 2019, ADV NEUR IN, V32; Strauss T., 2017, ARXIV17090342; Wang B., 2018, ARXIV180908516; Wang B., 2019, ARXIV190706800; Wang B, 2018, ADV NEUR IN, V31; Wang Y., 2019, INT C MACH LEARN; Wong E, 2018, ADV NEUR IN, V31; Wong E, 2018, PR MACH LEARN RES, V80; Yin D., 2018, ARXIV181011914; Zhang HY, 2019, PR MACH LEARN RES, V97; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485; Zhu M, 2018, ARXIV180208831	46	4	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301062
C	Wang, BY; Mendez, JA; Cai, MB; Eaton, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Boyu; Mendez, Jorge A.; Cai, Ming Bo; Eaton, Eric			Transfer Learning via Minimizing the Performance Gap Between Domains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new principle for transfer learning, based on a straightforward intuition: if two domains are similar to each other, the model trained on one domain should also perform well on the other domain, and vice versa. To formalize this intuition, we define the performance gap as a measure of the discrepancy between the source and target domains. We derive generalization bounds for the instance weighting approach to transfer learning, showing that the performance gap can be viewed as an algorithm-dependent regularizer, which controls the model complexity. Our theoretical analysis provides new insight into transfer learning and motivates a set of general, principled rules for designing new instance weighting schemes for transfer learning. These rules lead to gapBoost, a novel and principled boosting approach for transfer learning. Our experimental evaluation on benchmark data sets shows that gapBoost significantly outperforms previous boosting-based transfer learning algorithms.	[Wang, Boyu] Univ Western Ontario, Dept Comp Sci, London, ON, Canada; [Mendez, Jorge A.; Eaton, Eric] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA; [Cai, Ming Bo] Princeton Univ, Princeton Neurosci Insititute, Princeton, NJ 08544 USA	Western University (University of Western Ontario); University of Pennsylvania; Princeton University	Wang, BY (corresponding author), Univ Western Ontario, Dept Comp Sci, London, ON, Canada.	bwang@csd.uwo.ca; mendezme@seas.upenn.edu; mcai@princeton.edu; eeaton@seas.upenn.edu	Mendez, Jorge/AAS-5542-2021	Wang, Boyu/0000-0002-7413-4162; Cai, Mingbo/0000-0003-0768-7215	Faculty of Science at the University of Western Ontario; Lifelong Learning Machines program from DARPA/MTO [FA8750-18-2-0117]	Faculty of Science at the University of Western Ontario; Lifelong Learning Machines program from DARPA/MTO	The research presented in this paper was supported by the Faculty of Science at the University of Western Ontario and the Lifelong Learning Machines program from DARPA/MTO under grant #FA8750-18-2-0117. We would like to thank the anonymous reviewers for their helpful feedback.	[Anonymous], [No title captured]; [Anonymous], 2014, ADV NEURAL INFORM PR; Arjovsky M., 2017, ARXIV170107875; Azizzadenesheli K., 2019, INT C LEARN REPR; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Ben-David Shai, 2007, NEURIPS, P7; Blitzer J., 2007, ADV NEURAL INFORM PR, V20, P129; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Cortes C, 2008, LECT NOTES ARTIF INT, V5254, P38, DOI 10.1007/978-3-540-87987-9_8; Cortes C, 2019, J MACH LEARN RES, V20; Crammer K, 2008, J MACH LEARN RES, V9, P1757; Dai W., 2007, PROC INT C MACH LEAR, P193, DOI [10.1145/1273496.1273521, DOI 10.1145/1273496.1273521]; Du Simon S., 2017, ADV NEURAL INFORM PR, P2; Eaton E., 2011, PROC 25 AAAI C ARTIF, P337; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Germain P., 2013, INT C MACH LEARN, P738; Gretton A, 2012, J MACH LEARN RES, V13, P723; Kuzborskij I, 2017, MACH LEARN, V106, P171, DOI 10.1007/s10994-016-5594-4; Kuzborskij Ilja, 2013, P 30 INT C MACH LEAR, P942; Long M, 2015, AUST HUMANIT REV, P93; Long MS, 2017, PR MACH LEARN RES, V70; Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111; MEAGHER M, 2007, ADV NEURAL INFORM PR, P601, DOI DOI 10.7551/MITPRESS/7503.003.0080; Mohri Mehryar, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P124, DOI 10.1007/978-3-642-34106-9_13; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pardoe D., 2010, ICML, P863; Pineau, 2016, P INT JOINT C ART IN, P2097; Qi Y, 2017, IN C IND ENG ENG MAN, P2215; Ruvolo P., 2013, P 30 INT C MACHINE L, P507; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shui C., 2019, ARXIV PREPRINT ARXIV; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sugiyama M., 2008, NIPS, P1433; Sugiyama M, 2013, NEURAL COMPUT, V25, P2734, DOI 10.1162/NECO_a_00492; SUN Q, 2011, ADV NEURAL INFORM PR, P509, DOI DOI 10.1109/TRUSTCOM.2011.66; Wang Boyu, 2019, 22 INT C ART INT STA, V89, P3362; Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6; Yang Q., 2017, IJCAI, P2365; Yao Y, 2010, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2010.5539857	42	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902029
C	Weiss, G; Goldberg, Y; Yahav, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Weiss, Gail; Goldberg, Yoav; Yahav, Eran			Learning Deterministic Weighted Automata with Queries and Counterexamples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PROBABILISTIC-AUTOMATA; PAC-LEARNABILITY	We present an algorithm for extraction of a probabilistic deterministic finite automaton (PDFA) from a given black-box language model, such as a recurrent neural network (RNN). The algorithm is a variant of the exact-learning algorithm L*, adapted to a probabilistic setting with noise. The key insight is the use of conditional probabilities for observations, and the introduction of a local tolerance when comparing them. When applied to RNNs, our algorithm often achieves better word error rate (WER) and normalised distributed cumulative gain (NDCG) than that achieved by spectral extraction of weighted finite automata (WFA) from the same networks. PDFAs are substantially more expressive than n-grams, and are guaranteed to be stochastic and deterministic - unlike spectrally extracted WFAs.	[Weiss, Gail; Yahav, Eran] Technion, Haifa, Israel; [Goldberg, Yoav] Bar Ilan Univ, Allen Inst AI, Ramat Gan, Israel	Bar Ilan University	Weiss, G (corresponding author), Technion, Haifa, Israel.	sgailw@cs.technion.ac.il; yogo@cs.biu.ac.il; yahave@cs.technion.ac.il			Israeli Science Foundation [1319/16]; European Research Council (ERC) [802774]	Israeli Science Foundation(Israel Science Foundation); European Research Council (ERC)(European Research Council (ERC)European Commission)	The authors wish to thank Remi Eyraud for his helpful discussions and comments, and Chris Hammerschmidt for his assistance in obtaining the results with FLEXFRINGE. The research leading to the results presented in this paper is supported by the Israeli Science Foundation (grant No.1319/16), and the European Research Council (ERC) under the European Union's Seventh Framework Programme (FP7-2007-2013), under grant agreement no. 802774 (iEXTRACT).	Ayache S., 2018, ARXIV E PRINTS; Bailly Raphael, 2011, J MACHINE LEARNING R, P147; Bailly Raphael, 2009, P 26 ANN INT C MACH, P33; Balle B, 2015, LECT NOTES COMPUT SC, V9270, P1, DOI 10.1007/978-3-319-23021-4_1; Balle B, 2014, MACH LEARN, V96, P33, DOI 10.1007/s10994-013-5416-x; Balle B, 2013, THEOR COMPUT SCI, V473, P46, DOI 10.1016/j.tcs.2012.10.009; Balle Borja, 2016, P 13 INT C GRAMM INF; Carrasco R. C., 1994, Grammatical Inference and Applications. Second International Colloquium, ICGI-94 Proceedings, P139; Carrasco RC, 1999, RAIRO-INF THEOR APPL, V33, P1, DOI 10.1051/ita:1999102; Castro J, 2008, LECT NOTES ARTIF INT, V5278, P163; Cechin AL, 2003, SCCC 2003: XXIII INTERNATIONAL CONFERENCE OF THE CHILEAN COMPUTER SCIENCE SOCIETY, PROCEEDINGS, P73; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung J., 2014, ARXIV14123555; Clark A, 2004, J MACH LEARN RES, V5, P473; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Goodman JT, 2001, COMPUT SPEECH LANG, V15, P403, DOI 10.1006/csla.2001.0174; Hammerschmidt Christian Albert, 2016, ARXIV161107100; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HSU D, 2008, CORR; Mayr F, 2018, LECT NOTES COMPUT SC, V11015, P350, DOI 10.1007/978-3-319-99740-7_25; Okudono Takamasa, 2019, WEIGHTED AUTOMATA EX; Omlin CW, 1996, NEURAL NETWORKS, V9, P41, DOI 10.1016/0893-6080(95)00086-0; Palmer N, 2007, THEOR COMPUT SCI, V387, P18, DOI 10.1016/j.tcs.2007.07.023; Quattoni Ariadna, 2017, CORR; Rabusseau G., 2019, P MACHINE LEARNING R, P1630; Ron D, 1998, J COMPUT SYST SCI, V56, P133, DOI 10.1006/jcss.1997.1555; Rosenfeld R, 2000, P IEEE, V88, P1270, DOI 10.1109/5.880083; Rulot H., 1988, Syntactic and Structural Pattern Recognition. Proceedings of the NATO Advanced Research Workshop, P173; Sharan Vatsal, 2016, CORR; Thollard F., 2000, P 17 INT C MACH LEAR, P975; Tomita M., 1982, P 4 ANN C COGN SCI S, P105; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Verwer S, 2017, PROC IEEE INT CONF S, P638, DOI 10.1109/ICSME.2017.58; Verwer S, 2014, MACH LEARN, V96, P129, DOI 10.1007/s10994-013-5409-9; Wang Q., 2017, CORR; Weiss G, 2018, PR MACH LEARN RES, V80	39	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900018
C	Wilson, AC; Mackey, L; Wibisono, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wilson, Ashia C.; Mackey, Lester; Wibisono, Andre			Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CUBIC REGULARIZATION; MINIMIZATION	We present a family of algorithms, called descent algorithms, for optimizing convex and non-convex functions. We also introduce a new first-order algorithm, called rescaled gradientdescent (RGD), and show that RGD achieves a faster convergence rate than gradient descent over the class of strongly smooth functions - a natural generalization of the standard smoothness assumption on the objective function. When the objective function is convex, we present two frameworks for accelerating descent algorithms, one in the style of Nesterov and the other in the style of Monteiro and Svaiter, using a single Lyapunov function. Rescaled gradient descent can be accelerated under the same strong smoothness assumption using both frameworks. We provide several examples of strongly smooth loss functions in machine learning and numerical experiments that verify our theoretical findings. We also present several extensions of our novel Lyapunov framework including deriving optimal universal higher-order tensor methods and extending our framework to the coordinate descent setting.	[Wilson, Ashia C.; Mackey, Lester] Microsoft Res, Redmond, WA 98052 USA; [Wibisono, Andre] Georgia Tech, Atlanta, GA USA	Microsoft; University System of Georgia; Georgia Institute of Technology	Wilson, AC (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	ashia.wilson@microsoft.com; lmackey@microsoft.com; wibisono@gatech.edu						Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Baes M., 2009, ESTIMATE SEQUENCE ME; Betancourt M., 2018, ARXIV180203653; Bubeck Sdbastien, P MACHINE LEARNING R, V99, P492; Carmon Yair, 2017, ARXIV17110084; Chen G, 1993, SIAM J OPTIMIZ, V3, P538, DOI 10.1137/0803026; Diakonikolas Jelena, 2018, 9 INN THEOR COMP SCI; Gasnikov A., 2019, P 32 C LEARN THEORY, P1374; Grapiglia G.N, 2019, ARXIVL90412559; Hazan E., 2015, ADV NEURAL INFORM PR, P1594; Jiang B., 2018, ARXIV181206557; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Lin Hongzhou, 2017, J MACHINE LEARNING R, V18; LOJASIEWICZ S., 1963, EQUATIONS DERIVEES P, P87; Maddison Chris J, 2018, ARXIV180905042; Monteiro RDC, 2013, SIAM J OPTIMIZ, V23, P1092, DOI 10.1137/110833786; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 2018, CORE DISCUSSION PAPA; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2008, MATH PROGRAM, V112, P159, DOI 10.1007/s10107-006-0089-x; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Schropp J, 2000, NUMER FUNC ANAL OPT, V21, P537, DOI 10.1080/01630560008816971; Shi Bin, 2018, ARXIV181008907; Su WJ, 2014, ADV NEUR IN, V27; Sundaramoorthi Ganesh, 2018, ADV NEURAL INFORM PR, V2018, P3797; Wibisono A., 2015, ARXIV150903616; Wibisono A., 2018, PROC C LEARNING THEO, Vvol 31, P2093; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Wilson Ashia, 2016, ARXIVL6L102635; Zhang Jingzhao, 2018, P 32 INT C NEUR INF, P3904; Zhu Zeyuan Allen, 2017, 8 INN THEOR COMP SCI	36	4	4	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905023
C	Xu, YX; Wang, YH; Chen, HT; Han, K; Xu, CJ; Tao, DC; Xu, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Yixing; Wang, Yunhe; Chen, Hanting; Han, Kai; Xu, Chunjing; Tao, Dacheng; Xu, Chang			Positive-Unlabeled Compression on the Cloud	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many attempts have been done to extend the great success of convolutional neural networks (CNNs) achieved on high-end GPU servers to portable devices such as smart phones. Providing compression and acceleration service of deep learning models on the cloud is therefore of significance and is attractive for end users. However, existing network compression and acceleration approaches usually fine-tuning the svelte model by requesting the entire original training data (e.g. ImageNet), which could be more cumbersome than the network itself and cannot be easily uploaded to the cloud. In this paper, we present a novel positive-unlabeled (PU) setting for addressing this problem. In practice, only a small portion of the original training set is required as positive examples and more useful training examples can be obtained from the massive unlabeled data on the cloud through a PU classifier with an attention based multi-scale feature extractor. We further introduce a robust knowledge distillation (RKD) scheme to deal with the class imbalance problem of these newly augmented training examples. The superiority of the proposed method is verified through experiments conducted on the benchmark models and datasets. We can use only 8% of uniformly selected data from the ImageNet to obtain an efficient model with comparable performance to the baseline ResNet-34.	[Xu, Yixing; Wang, Yunhe; Han, Kai; Xu, Chunjing] Peking Univ, Sch EECS, Huawei Noahs Ark Lab, Beijing, Peoples R China; [Chen, Hanting] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, CMIC, Beijing, Peoples R China; [Tao, Dacheng; Xu, Chang] Univ Sydney, Darlington, NSW 2008, Australia	Huawei Technologies; Peking University; Peking University; University of Sydney	Xu, YX (corresponding author), Peking Univ, Sch EECS, Huawei Noahs Ark Lab, Beijing, Peoples R China.	yixing.xu@huawei.com; yunhe.wang@huawei.com; htchen@pku.edu.cn; kai.han@huawei.com; xuchunjing@huawei.com; dacheng.tao@sydney.edu.au; c.xu@sydney.edu.au	Xu, Chang/AAG-9337-2019	Xu, Chang/0000-0002-4756-0609	Australian Research Council [DE180101438]	Australian Research Council(Australian Research Council)	We thank anonymous area chair and reviewers for their helpful comments. Chang Xu was supported by the Australian Research Council under Project DE180101438.	Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338; Chen H., 2019, ARXIV190401186; Dentinel Zarembaw, 2014, NEURIPS, P1269; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gong Yunchao, 2014, ARXIV14126115; Han K., 2019, ARXIV190802023; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Kiryo R, 2017, ADV NEUR IN, V30; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li T., 2018, ARXIV181201839; Lopes Raphael Gontijo, 2017, DATA FREE KNOWLEDGE, P2; Lyu HL, 2015, CHIN CONT DECIS CONF, P2885; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Oguntola I, 2018, IEEE HIGH PERF EXTR; Ramaswamy HG, 2016, PR MACH LEARN RES, V48; Romero Adriana, 2014, ARXIV14126550; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shen XB, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3178119; Shen XB, 2018, IEEE T NEUR NET LEAR, V29, P4324, DOI 10.1109/TNNLS.2017.2763967; Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441; Srinivas S., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.5244/C.29.31; Wang R., 2012, ARXIV12050406; Wang YH, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2476, DOI 10.1145/3219819.3219970; Xu Miao, 2019, ARXIV190110155; Xu YX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3182; Zhou P, 2018, PROC CVPR IEEE, P1596, DOI 10.1109/CVPR.2018.00172	31	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302055
C	Yang, FN; Wang, ZW; Heinze-Deml, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Fanny; Wang, Zuowen; Heinze-Deml, Christina			Invariance-inducing regularization using worst-case transformations suffices to boost accuracy and spatial robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NETWORKS	This work provides theoretical and empirical evidence that invariance-inducing regularizers can increase predictive accuracy for worst-case spatial transformations (spatial robustness). Evaluated on these adversarially transformed examples, we demonstrate that adding regularization on top of standard augmented or adversarial training reduces the relative robust error on CIFAR-10 by 20% with minimal computational overhead. Similar relative gains hold for SVHN and CIFAR-100. Regularized augmentation-based methods in fact even outperform handcrafted networks that were explicitly designed to be spatial-equivariant. Furthermore, we observe for SVHN, known to have inherent variance in orientation, that robust training also improves standard accuracy on the test set. We prove that this no-trade-off phenomenon holds for adversarial examples from transformation groups in the infinite data limit.	[Yang, Fanny] Stanford Univ, Stanford, CA 94305 USA; [Yang, Fanny; Wang, Zuowen; Heinze-Deml, Christina] Swiss Fed Inst Technol, Zurich, Switzerland	Stanford University; Swiss Federal Institutes of Technology Domain; ETH Zurich	Yang, FN (corresponding author), Stanford Univ, Stanford, CA 94305 USA.; Yang, FN (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	fan.yang@stat.math.ethz.ch; wangzu@ethz.ch; heinzedeml@stat.math.ethz.ch		Wang, Zuowen/0000-0002-8051-9886	Institute for Theoretical Studies ETH Zurich; Dr. Max Rossler and Walter Haefner Foundation; ETH Foundations of Data Science; Office of Naval Research Young Investigator Award [N00014-19-1-2288]	Institute for Theoretical Studies ETH Zurich; Dr. Max Rossler and Walter Haefner Foundation; ETH Foundations of Data Science; Office of Naval Research Young Investigator Award(Office of Naval Research)	We thank Luzius Brogli for initial experiments, Nicolai Meinshausen and Armeen Taeb for valuable feedback on the manuscript and Ludwig Schmidt for helpful discussions. FY was supported by the Institute for Theoretical Studies ETH Zurich, the Dr. Max Rossler and Walter Haefner Foundation, ETH Foundations of Data Science and the Office of Naval Research Young Investigator Award N00014-19-1-2288. ZW was supported by the ETH Foundations of Data Science.	Abadi M, 2015, P 12 USENIX S OPERAT; Alcorn M. A., 2018, ARXIV181111553; Baird HS., 1992, STRUCTURED DOCUMENT, P546, DOI [10.1007/978-3-642-77281-8_26, DOI 10.1007/978-3-642-77281-8_26]; Ben-Tal A, 2009, PRINC SER APPL MATH, P27; Bengio Y., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cohen T. S., 2018, P INT C LEARN REPR; Cohen TS, 2016, PR MACH LEARN RES, V48; Dumont B., 2018, ARXIV180206627; Engstrom Logan, 2019, P INT C MACH LEARN; Esteves Carlos, 2018, P INT C LEARN REPR; Fawzi A., 2015, BRIT MACH VIS C BMVC; Geirhos R, 2018, ADV NEUR IN, V31; Hanin B., 2017, ARXIV170802691; Heinze-Deml C., 2017, ARXIV171011469; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Jana Suman, 2017, ARXIV PREPRINT ARXIV; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kanbak C, 2018, PROC CVPR IEEE, P4441, DOI 10.1109/CVPR.2018.00467; Kannan Harini, 2018, ARXIV180306373; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Laptev D, 2016, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.2016.38; Madry A., 2018, P INT C LEARN REPR; Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540; Mirman M, 2018, PR MACH LEARN RES, V80; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Motiian S., 2017, IEEE INT C COMPUTER, V2, P3; Na T., 2018, P INT C LEARN REPR P INT C LEARN REPR; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; Raghunathan A., 2019, ARXIV190606032; Raghunathan A., 2018, P INT C LEARN REPR; Samangouei P, 2018, P INT C LEARN REPR; Sinha A., 2018, P INT C LEARN REPR; Szegedy C., 2014, ICLR 2014; Tai K.S., 2019, P INT C MACH LEARN; Tsipras D, 2019, P INT C LEARN REPR; Weiler Maurice, 2018, P IEEE C COMP VIS PA; Wong E, 2018, PR MACH LEARN RES, V80; Worrall D. E, 2017, P IEEE C COMP VIS PA, P5028; Xie Qizhe, 2019, ARXIV190412848, P2; Yaeger L, 1997, ADV NEUR IN, V9, P807; Zhang Chiyuan, 2015, P INT C LEARN REPR; Zhang Hongyang, 2019, P INT C MACH LEARN; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485; Zhou B, 2017, PROCEEDINGS OF THE 23RD ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM '17), P519, DOI 10.1145/3117811.3119864	50	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906044
C	Yu, T; De Sa, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Tao; De Sa, Christopher			Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Hyperbolic embeddings achieve excellent performance when embedding hierarchical data structures like synonym or type hierarchies, but they can be limited by numerical error when ordinary floating-point numbers are used to represent points in hyperbolic space. Standard models such as the Poincare disk and the Lorentz model have unbounded numerical error as points get far from the origin. To address this, we propose a new model which uses an integer-based tiling to represent any point in hyperbolic space with provably bounded numerical error. This allows us to learn high-precision embeddings without using BigFloats, and enables us to store the resulting embeddings with fewer bits. We evaluate our tiling-based model empirically, and show that it can both compress hyperbolic embeddings (down to 2% of a Poincare embedding on WordNet Nouns) and learn more accurate embeddings on real-world datasets.	[Yu, Tao; De Sa, Christopher] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Yu, T (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	tyu@cs.cornell.edu; cdesa@cs.cornell.edu						Beardon Alan F, 2012, GEOMETRY DISCRETE GR, V91; Bordes A., 2013, ADV NEURAL INFORM PR; Bowditch Brian H, 2006, MSJ MEMOIRS, V16; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Cannon J.W., 1997, FLAVORS GEOMETRY, P59; Chamberlain B., 2017, P 13 INT WORKSH MIN; Clauset A, 2008, NATURE, V453, P98, DOI 10.1038/nature06830; Conway J.H., 2008, AK PETERS SERIES; Coxeter HSM, 1956, P INT C MATHEMATICIA, V3, P155; Cvetkovski Andrej, 2016, APPL MATH INFORM SCI, V10, P125, DOI DOI 10.18576/AMIS/100112; Datta Basudeb, 2018, ARXIV180611393; Dhingra Bhuwan, 2018, ARXIV PREPRINT ARXIV, P59, DOI DOI 10.18653/V1/W18-1708; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; Ganea Octavian, 2018, ADV NEURAL INFORM PR, P5345; Ganea OE, 2018, PR MACH LEARN RES, V80; GANS D, 1966, AM MATH MON, V73, P291, DOI 10.2307/2315350; Gromov Mikhael, 1987, MATH SCI RES I PUBL, V8, P75, DOI 10.1007/978-1-4613-9586-7_3; Gu A., 2019, ICLR; Gulcehre Caglar, 2018, INT C LEARN REPR; Katok S., 1992, FUCHSIAN GROUPS; Kolbe Benedikt, 2019, ARXIV190403788; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Miyagawa Shigeru, 2013, Front Psychol, V4, P71, DOI 10.3389/fpsyg.2013.00071; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2017, ADV NEUR IN, V30; Nickel M, 2016, AAAI CONF ARTIF INTE, P1955; Potter Melissa, 2005, AM J UNDERGRADUATE R, V3, DOI [10.33697/ajur.2005.005, DOI 10.33697/AJUR.2005.005]; Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292; Sala F, 2018, PR MACH LEARN RES, V80; Salomon D., 2007, VARIABLE LENGTH CODE; Sarkar R., 2011, INT S GRAPH DRAW; Sugimoto T, 2015, GRAPH COMBINATOR, V31, P281, DOI 10.1007/s00373-013-1385-x; Tay Y, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P583, DOI 10.1145/3159652.3159664; Tifrea Alexandru, 2019, ICLR; Voight John, 2014, PREPRINT; Yuncken R, 2003, MOSC MATH J, V3, P249, DOI 10.17323/1609-4514-2003-3-1-249-252; [No title captured]; [No title captured]	38	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302006
C	Yuan, ZN; Yan, Y; Jin, R; Yang, TB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yuan, Zhuoning; Yan, Yan; Jin, Rong; Yang, Tianbao			Stagewise Training Accelerates Convergence of Testing Error Over SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Stagewise training strategy is widely used for learning neural networks, which runs a stochastic algorithm (e.g., SGD) starting with a relatively large step size (aka learning rate) and geometrically decreasing the step size after a number of iterations. It has been observed that the stagewise SGD has much faster convergence than the vanilla SGD with a polynomially decaying step size in terms of both training error and testing error. But how to explain this phenomenon has been largely ignored by existing studies. This paper provides some theoretical evidence for explaining this faster convergence. In particular, we consider a stagewise training strategy for minimizing empirical risk that satisfies the Polyak-Lojasiewicz (PL) condition, which has been observed/proved for neural networks and also holds for a broad family of convex functions. For convex loss functions and two classes of "nice-behaved" non-convex objectives that are close to a convex function, we establish faster convergence of stagewise training than the vanilla SGD under the PL condition on both training error and testing error. Experiments on stagewise learning of deep neural networks exhibits that it satisfies one type of non-convexity assumption and therefore can be explained by our theory.	[Yuan, Zhuoning; Yan, Yan; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Jin, Rong] Alibaba Grp, Machine Intelligence Technol, Bellevue, WA 98004 USA	University of Iowa; Alibaba Group	Yuan, ZN (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	zhuoning-yuan@uiowa.edu; yan-yan-2@uiowa.edu; jinrong.jr@alibaba-inc.com; tianbao-yang@uiowa.edu	Yan, Yan/L-1864-2018	Yan, Yan/0000-0001-9108-6767				Allen-Zhu Z., 2018, ABS181103962 CORR; [Anonymous], 2011, JMLR WORKSHOP C P; Bassily R., 2018, ABS181102564 CORR; Bolte Trong Phong, 2015, ABS151008234 CORR; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Charles Z, 2018, PR MACH LEARN RES, V80; Chaudhari Pratik, 2016, ABS161101838 CORR; Chen Z., 2019, INT C LEARN REPR; Clevert D.A., 2015, ABS151107289 CORR; Davis D., 2018, ABS180202988 CORR; Du S.S., 2019, ARXIV191003016; Ge R., 2019, INT C LEARN RE UNPUB; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hardt M., 2016, ABS161104231 CORR; Hardt M, 2016, PR MACH LEARN RES, V48; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Kleinberg R., 2018, P 35 INT C MACH LEAR, V80, P2698; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuzborskij I., 2018, P MACHINE LEARNING R, P2820; Lan G., 2018, ABS180505411 CORR; Lei LH, 2017, ADV NEUR IN, V30; Li Y., 2017, ADV NEURAL INFORM PR, P597; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Qu C, 2016, PR MACH LEARN RES, V48; Reddi SJ, 2016, PR MACH LEARN RES, V48; Xie B., 2016, ABS161103131 CORR; Xu Y, 2017, BIOMED RES INT, V2017, DOI 10.1155/2017/4316821; Zhou Y., 2017, ABS171006910 CORR	31	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302059
C	Zhang, MR; Chen, L; Hassani, H; Karbasi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Mingrui; Chen, Lin; Hassani, Hamed; Karbasi, Amin			Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION; ALGORITHMS	In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from T-1/2 [18] and T-3/2 [17] to 1, and achieves a (1 - 1/e)-regret bound of O(T-4/5). The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a (1 - 1/e)-regret bound of O(T-8/9). Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a (1 - 1/e)-regret bound of O(T-8/9) in the responsive bandit setting.	[Zhang, Mingrui] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA; [Chen, Lin; Karbasi, Amin] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA; [Hassani, Hamed] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA; [Karbasi, Amin] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA	Yale University; Yale University; University of Pennsylvania; Yale University	Zhang, MR (corresponding author), Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA.	mingrui.zhang@yale.edu; lin.chen@yale.edu; hassani@seas.upenn.edu; amin.karbasi@yale.edu	Chen, Lin/CAH-1961-2022	Chen, Lin/0000-0003-0349-6577	Google PhD Fellowship; NSF [IIS-1845032]; ONR [N00014-19-1-2406]; AFOSR [FA9550-18-1-0160]	Google PhD Fellowship(Google Incorporated); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work is partially supported by the Google PhD Fellowship, NSF (IIS-1845032), ONR (N00014-19-1-2406) and AFOSR (FA9550-18-1-0160). We would like to thank Marko Mitrovic for his valuable comments and Zheng Wei for help preparing some of the illustrations.	Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2; Awerbuch B, 2008, J COMPUT SYST SCI, V74, P97, DOI 10.1016/j.jcss.2007.04.016; Bach F, 2015, ARXIV151100394; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Balkanski Eric, 2015, P 16 ACM C EC COMP E, P529, DOI DOI 10.1145/2764468.2764505; Bian A., 2018, ARXIV180507482; Bian A. A., 2017, AISTATS; Biswas D, 2010, ANN CLIN MICROB ANTI, V9, DOI 10.1186/1476-0711-9-28; Bubeck S., 2016, C LEARN THEOR, P583; Bubeck S, 2017, ACM S THEORY COMPUT, P72, DOI 10.1145/3055399.3055403; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck Sebastien, 2015, C LEARN THEOR, P266; Bubeck Sebastien, 2012, COLT, V23; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Chekuri C, 2011, ACM S THEORY COMPUT, P783; Chen L, 2018, PROC INTERNAT C ARTI, P1896; Chen Lin, 2019, ARXIV190109515; Chen Lin, 2018, ICML; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; Dekel O., 2015, ADV NEURAL INFORM PR, P2926; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gabillon V., 2013, ADV NEURAL INFORM PR, P2697; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Golovin Daniel, 2014, ONLINE SUBMODULAR MA; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hazan E., 2014, ADV NEURAL INFORM PR, V27, P784; Hazan E., 2012, ICML, P1843; Hazan E., 2016, ARXIV PREPRINT ARXIV; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704; Kimura S, 2019, ADVANCES IN ENGINEERING MATERIALS, STRUCTURES AND SYSTEMS: INNOVATIONS, MECHANICS AND APPLICATIONS, P2047; Kleinberg R, 2005, P 17 INT C NEUR INF, V17, P697; Mokhtari A., 2018, C ART INT STAT, V84, P1886; Mokhtari A., 2018, ARXIV180409554; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Saha A, 2011, P 14 INT C ART INT S, P636; Shalev-Shwartz Shai, 2007, THESIS; Shamir O., 2013, P C LEARN THEOR, P3; Streeter M., 2009, P ADV NEUR INF PROC, P1577; Tschiatschek Sebastian, 2014, ADV NEURAL INFORM PR, P1413; Wei K, 2015, PR MACH LEARN RES, V37, P1954; Yu BS, 2016, AAAI CONF ARTIF INTE, P1380; Yue Y., 2011, ADV NEURAL INFORM PR, P2483; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	50	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900076
C	Zhang, TJ; Yao, ZW; Gholami, A; Keutzer, K; Gonzalez, J; Biros, G; Mahoney, MW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Tianjun; Yao, Zhewei; Gholami, Amir; Keutzer, Kurt; Gonzalez, Joseph; Biros, George; Mahoney, Michael W.			ANODEV2: A Coupled Neural ODE Framework	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					It has been observed that residual networks can be viewed as the explicit Euler discretization of an Ordinary Differential Equation (ODE). This observation motivated the introduction of so-called Neural ODEs, which allow more general discretization schemes with adaptive time stepping. Here, we propose ANODEV2, which is an extension of this approach that allows evolution of the neural network parameters, in a coupled ODE-based formulation. The Neural ODE method introduced earlier is in fact a special case of this new framework. We present the formulation of ANODEV2, derive optimality conditions, and implement the coupled framework in PyTorch. We present empirical results using several different configurations of ANODEV2, testing them on multiple models on CIFAR-10. We report results showing that this coupled ODE-based framework is indeed trainable, and that it achieves higher accuracy, as compared to the baseline models as well as the recently-proposed Neural ODE approach.	[Zhang, Tianjun; Yao, Zhewei; Gholami, Amir; Keutzer, Kurt; Gonzalez, Joseph; Mahoney, Michael W.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Biros, George] Univ Texas Austin, Austin, TX 78712 USA; [Mahoney, Michael W.] ICSI, Cambridge, MA USA	University of California System; University of California Berkeley; University of Texas System; University of Texas Austin	Zhang, TJ (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	tianjunz@berkeley.edu; zheweiy@berkeley.edu; amirgh@berkeley.edu; keutzer@berkeley.edu; jegonzal@berkeley.edu; biros@ices.utexas.edu; mahoneymw@berkeley.edu	Yao, Zhewei/ABE-1531-2021	Yao, Zhewei/0000-0001-7678-4321	Intel corporation; Berkeley Deep Drive (BDD); Berkeley AI Research (BAIR); NVIDIA Corporation; ARO; DARPA; NSF; ONR	Intel corporation(Intel Corporation); Berkeley Deep Drive (BDD); Berkeley AI Research (BAIR); NVIDIA Corporation; ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This work was supported by a gracious fund from Intel corporation, Berkeley Deep Drive (BDD), and Berkeley AI Research (BAIR) sponsors. We would like to thank the Intel VLAB team for providing us with access to their computing cluster. We also gratefully acknowledge the support of NVIDIA Corporation for their donation of two Titan Xp GPU used for this research. We would also like to acknowledge ARO, DARPA, NSF, and ONR for providing partial support of this work.	[Anonymous], 2017, COMMUN MATH STAT; [Anonymous], 2019, ANONYMIZED REV; BELEW RK, 1993, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P629; Bentley P, 1999, GECCO-99: PROCEEDINGS OF THE GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P35; Chen R. T., 2018, ADV NEURAL INFORM PR, P6571; Ciccone Marco, 2018, ARXIV180407209; Dellaert F., 1996, From Animals to Animats 4. Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, P393; Eggenberger P, 1997, FROM ANIM ANIMAT, P205; Fernando C, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P109, DOI 10.1145/2908812.2908890; Gholami A., 2019, ARXIV190210298; Ha David, 2016, ARXIV160909106; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hornby GS, 2002, ARTIF LIFE, V8, P223, DOI 10.1162/106454602320991837; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Koutnik J, 2010, P 12 ANN C GEN EV CO, P619, DOI DOI 10.1145/1830483.1830596; LINDENMAYER A, 1968, J THEOR BIOL, V18, P280, DOI 10.1016/0022-5193(68)90079-9; Lu Y., 2017, FINITE LAYER NEURAL; Ruthotto L., 2018, ARXIV180404272; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Schmidhuber J., 1993, INT C ART NEUR NETW, P446; Stanley K. O., 2006, P AAAI FALL S DEV SY, P37; Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8; Stanley KO, 2009, ARTIF LIFE, V15, P185, DOI 10.1162/artl.2009.15.2.15202; TURING AM, 1990, B MATH BIOL, V52, P153, DOI 10.1007/BF02459572; Younes L., SHAPES DIFFEOMORPHIS	26	4	4	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305018
C	Zhang, ZH; Ji, XY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Zihan; Ji, Xiangyang			Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present an algorithm based on the Optimism in the Face of Uncertainty (OFU) principle which is able to learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space efficiently. By evaluating the state -pair difference of the optimal bias function h*, the proposed algorithm achieves a regret bound of 0( VSAHTt for MDP with S states and A actions, in the case that an upper bound H on the span of h*, i.e., sp(h*) is known. This result outperforms the best previous regret bounds a(SVAHT)[Fruit et al., 2019] by a factor of -VS. Furthermore, this regret bound matches the lower bound of S2( VSAHT)[Jaksch et al., 2010] up to a logarithmic factor. As a consequence, we show that there is a near optimal regret bound of 0( VSADT) for MDPs with a finite diameter D compared to the lower bound of C2( VSADT)IJaksch et al. 20101.	[Zhang, Zihan; Ji, Xiangyang] Tsinghua Univ, Beijing, Peoples R China	Tsinghua University	Zhang, ZH (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	zihan-zh17@mails.tsinghua.edu.cn; xyji@tsinghua.edu.cn						Abbasi-Yadkori Yasin, 2015, C UNC ART INT; AGRAWAL S, 2017, ADV NEURAL INFORM PR, P1184; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; [Anonymous], [No title captured]; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bartlett RA, 2009, 2009 ICSE WORKSHOP ON SOFTWARE ENGINEERING FOR COMPUTATIONAL SCIENCE AND ENGINEERING, P35, DOI 10.1109/SECSE.2009.5069160; Burnetas A. N., 1997, OPTIMAL ADAPTIVE POL; Fruit R., 2019, IMPROVED ANAL UCRL2B; Fruit Ronan, 2018, ARXIV180204020; Fruit Ronan, 2018, P 32 INT C NEUR INF, P2998; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jiang N., 2018, C LEARN THEOR, P3395; Kakade Sham, 2018, ARXIV180209184; Ortner R, 2008, LECT NOTES ARTIF INT, V5254, P123, DOI 10.1007/978-3-540-87987-9_14; Osband I., 2016, ARXIV160700215; OUYANG Y, 2017, ADV NEURAL INFORM PR, V30, P1333; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Stoltz G, 2011, P 24 ANN C LEARN THE, P497; Talebi MS, 2018, ALGORITHMIC LEARNING, P770; Theocharous G., 2017, ARXIV171107979; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Zanette Andrea, 2019, ARXIV190100210	24	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302078
C	Acerbi, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Acerbi, Luigi			Variational Bayesian Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as MCMC, which would require access to the gradient or a large number of likelihood evaluations. We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective. Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection. We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to D = 10), VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.	[Acerbi, Luigi] Univ Geneva, Dept Basic Neurosci, Geneva, Switzerland	University of Geneva	Acerbi, L (corresponding author), Univ Geneva, Dept Basic Neurosci, Geneva, Switzerland.	luigi.acerbi@unige.ch						Acerbi L, 2017, ADV NEURAL INF PROCE, V30, P1834; [Anonymous], 2017, P 34 INT C MACH LEAR; Bishop C.M, 2006, PATTERN RECOGN; Briol FX, 2015, ADV NEUR IN, V28; BROCHU E, 2010, ARXIV 1012 2599; Carpenter B., 2017, J STAT SOFTWARE, V76; Gershman S., 2012, P 29 INT COF MACH LE; Geyer C.J, 1994, 568 U MINN SCH STAT; GILKS WR, 1994, STATISTICIAN, V43, P179, DOI 10.2307/2348942; Goris RLT, 2015, NEURON, V88, P819, DOI 10.1016/j.neuron.2015.10.009; Gramacy RB, 2012, STAT COMPUT, V22, P713, DOI 10.1007/s11222-010-9224-x; Gunter T., 2014, ADV NEURAL INFORM PR, P2789; Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kandasamy K, 2015, 24 INT JOINT C ART I; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Kingma D. P., 2014, ADAM METHOD STOCHAST; Kingma DP, 2013, P 2 INT C LEARN REPR; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Osborne M., 2012, ADV NEURAL INF PROCE, V25, P1; Rasmussen C. E., 2002, ADV NEURAL INFORM PR, P505; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Wang HQ, 2018, NEURAL COMPUT, V30, P3072, DOI 10.1162/neco_a_01127	27	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002073
C	Bailey, B; Telgarsky, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bailey, Bolton; Telgarsky, Matus			Size-Noise Tradeoffs in Generative Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BOUNDS	This paper investigates the ability of generative networks to convert their input noise distributions into other distributions. Firstly, we demonstrate a construction that allows ReLU networks to increase the dimensionality of their noise distribution by implementing a "space-filling" function based on iterated tent maps. We show this construction is optimal by analyzing the number of affine pieces in functions computed by multivariate ReLU networks. Secondly, we provide efficient ways (using polylog(1/epsilon) nodes) for networks to pass between univariate uniform and normal distributions, using a Taylor series approximation and a binary search gadget for computing function inverses. Lastly, we indicate how high dimensional distributions can be efficiently transformed into low dimensional distributions.	[Bailey, Bolton; Telgarsky, Matus] Univ Illinois, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Bailey, B (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	boltonb2@illinois.edu; mjt@illinois.edu			NSF [IIS-1750051]; NVIDIA	NSF(National Science Foundation (NSF)); NVIDIA	The authors are grateful for support from the NSF under grant IIS-1750051, and for a GPU grant from NVIDIA.	Arjovsky M, 2017, PR MACH LEARN RES, V70; BOX GEP, 1958, ANN MATH STAT, V29, P610, DOI 10.1214/aoms/1177706645; Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Donahue C, 2018, ARXIV PREPRINT ARXIV; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Kolmogorov A. N., 1975, INTRO REAL ANAL; Lee Holden, 2017, ABILITY NEURAL NETS, V65, P1271; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Osokin A, 2017, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2017.245; Pinelis Iosif, 2013, NONUNIFORM BERRY ESS; Safran Itay, 2016, CORR; Telgarsky M.., 2016, C LEARNING THEORY, P1517; Tong Y, 2017, ARXIV PREPRINT ARXIV; Villani C., 2003, TOPICS OPTIMAL TRANS, V58; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002; Zhang LW, 2018, PR MACH LEARN RES, V80	21	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001006
C	Blum, A; Gunasekar, S; Lykouris, T; Srebro, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Blum, Avrim; Gunasekar, Suriya; Lykouris, Thodoris; Srebro, Nathan			On preserving non-discrimination when combining expert advice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions. We consider the most basic extension of classical online learning: Given a class of predictors that are individually non-discriminatory with respect to a particular metric, how can we combine them to perform as well as the best predictor, while preserving non-discrimination? Surprisingly we show that this task is unachievable for the prevalent notion of equalized odds that requires equal false negative rates and equal false positive rates across groups. On the positive side, for another notion of non-discrimination, equalized error rates, we show that running separate instances of the classical multiplicative weights algorithm for each group achieves this guarantee. Interestingly, even for this notion, we show that algorithms with stronger performance guarantees than multiplicative weights cannot preserve non-discrimination.	[Blum, Avrim; Gunasekar, Suriya; Srebro, Nathan] TTI Chicago, Chicago, IL 60637 USA; [Lykouris, Thodoris] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Blum, A (corresponding author), TTI Chicago, Chicago, IL 60637 USA.	avrim@ttic.edu; suriya@ttic.edu; teddlyk@cs.cornell.edu; nati@ttic.edu			NSF [CCF-1800317, CCF-1563714]; Google Ph.D. Fellowship	NSF(National Science Foundation (NSF)); Google Ph.D. Fellowship(Google Incorporated)	The authors would like to thank Manish Raghavan for useful discussions that improved the presentation of the paper. This work was supported by the NSF grants CCF-1800317 and CCF-1563714, as well as a Google Ph.D. Fellowship.	Angwin J., 2016, PROPUBLICA, P254; Angwin Julia, 2016, PROPUBLICA BLOG, V28; Balcan M.-F., 2018, ENVY FREE CLASSIFICA; Barocas S., 2016, CALIFORNIA LAW REV; Bird Sarah, 2016, WORKSH FAIRN ACC TRA; Blum Avrim, 2005, P 18 ANN C LEARN THE; Buolamwini J., 2018, C FAIRN ACC TRANSP P; Calders Toon, 2009, IEEE INT C DAT MIN I; Celis L. Elisa, 2017, WORKSH FAIRN ACC TRA; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Corbett-Davies Sam, 2018, ARXIV180800023; DATTA A., 2015, P PRIV ENH TECHN; Dwork C., 2012, INNOVATIONS THEORETI; Even-Dar Eyal, 2008, J MACHINE LEARNING J; Feldman M., 2015, P 21 ACM SIGKDD INT; Feller Avi, 2016, WASHINGTON POST; Freund Yoav, 1997, J COMPUT SYST SCI; Gillen S., 2018, ADV NEURAL INFORM PR; GOH G, 2016, ADV NEURAL INFORM PR; Hardt M., 2016, ADV NEURAL INFORM PR; Herbster Mark, 2001, J MACHINE LEARNING R; Joseph Matthew, ADV NEURAL INFORM PR; Kannan Sampath, 2018, ADV NEURAL INFORM PR; Kannan Sampath, 2017, P 2017 ACM C EC COMP; Kay M., 2015, P 33 ANN ACM C HUM F; Kearns M., 2018, 35 INT C MACH LEARN, V6, P4008; Kleinberg J. M., 2017, P 8 INN THEOR COMP S, V67, P43; Kusner Matt J, 2017, ADV NEURAL INFORM PR; Liu Katherine A, 2016, PHARM PRACTICE; Liu Lydia T., 2018, 35 INT C MACH LEARN; Liu Yang, 2017, WORKSH FAIRN ACC TRA; Luo Haipeng, 2015, P 28 C LEARN THEOR C; Lykouris Thodoris, 2016, P 27 ANN ACM SIAM S; Parascandolo G., 2017, ADV NEURAL INFORM PR; Pedreshi D., 2008, KDD; Raghavan Manish, 2018, P 31 C LEARN THEOR C; Sweeney L, 2013, COMMUN ACM, V56, P44, DOI 10.1145/2447976.2447990; Swersky K., 2013, INT C MACH LEARN ICM; Woodworth B., 2017, C LEARN THEOR COLT; Zafar Muhammad Bilal, 2017, P 30 NEUR INF PROC S	41	4	4	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002088
C	Carmon, Y; Duchi, JC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Carmon, Yair; Duchi, John C.			Analysis of Krylov Subspace Solutions of Regularized Nonconvex Quadratic Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We provide convergence rates for Krylov subspace solutions to the trust-region and cubic-regularized (nonconvex) quadratic problems. Such solutions may be efficiently computed by the Lanczos method and have long been used in practice. We prove error bounds of the form 1/t(2) and e(-4t/root kappa), where kappa is a condition number for the problem, and t is the Krylov subspace order (number of Lanczos iterations). We also provide lower bounds showing that our analysis is sharp.	[Carmon, Yair] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Duchi, John C.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Duchi, John C.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University	Carmon, Y (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	yairc@stanford.edu; jduchi@stanford.edu		Duchi, John/0000-0003-0045-7185	NSF-CAREER Award [1553086]; Sloan Foundation; Stanford Graduate Fellowship	NSF-CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Foundation(Alfred P. Sloan Foundation); Stanford Graduate Fellowship(Stanford University)	We thank the anonymous reviewers for several helpful questions and suggestions. Both authors were supported by NSF-CAREER Award 1553086 and the Sloan Foundation. YC was partially supported by the Stanford Graduate Fellowship.	Agarwal N., 2017, P 49 ANN ACM S THEOR; Allen-Zhu Z., 2017, 8 INN THEOR COMP SCI; Bau III D, 1997, NUMERICAL LINEAR ALG; Blanchet J., 2016, ARXIV160907428MATHOC; Carmon Y., 2017, ARXIV171100841MATHOC; Carmon Y., 2017, P 34 INT C MACH LEAR; Carmon Y., 2016, ARXIV161200547MATHOC; Carmon Y, 2018, SIAM J OPTIMIZ, V28, P1751, DOI 10.1137/17M1114296; Cartis C, 2011, MATH PROGRAM, V127, P245, DOI 10.1007/s10107-009-0286-5; Coakley ES, 2013, APPL COMPUT HARMON A, V34, P379, DOI 10.1016/j.acha.2012.06.003; Conn A.R., 2000, TRUST REGION METHODS, DOI [10.1137/1.9780898719857, DOI 10.1137/1.9780898719857]; Cullum J., 1974, 1974 IEEE C DEC CONT, P505; Druskin V., 1991, U S S R COMPUTATIONA, V31, P970; Golub G, 1977, MATH SOFTWARE, P361, DOI DOI 10.1016/B978-0-12-587260-7.50018-2; Golub G. H., 1996, MATRIX COMPUTATIONS; Gould NIM, 1999, SIAM J OPTIMIZ, V9, P504, DOI 10.1137/S1052623497322735; Gould NIM, 2003, ACM T MATH SOFTWARE, V29, P373, DOI 10.1145/962437.962439; Griewank A., 1981, NA12; Hazan E, 2016, MATH PROGRAM, V158, P363, DOI 10.1007/s10107-015-0933-y; Hestenes M. R., 1952, J RES NBS, V49; Ho-Nguyen N., 2016, ARXIV160303366MATHOC; Jin C., 2017, ARXIV171110456CSLG; Kohler J. M., 2017, P 34 INT C MACH LEAR; KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066; Lenders F, 2018, OPTIM METHOD SOFTW, V33, P420, DOI 10.1080/10556788.2018.1449842; Musco Christopher, 2017, ARXIV170807788CSDS; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Regier J, 2017, ADV NEUR IN, V30; Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683; Simchowitz M., 2018, ARXIV180709386CSLG; TRIPURANENI N., 2017, ARXIV171102838CSLG; Yao Z., 2018, ARXIV180206925MATHOC; Zhang LH, 2017, SIAM J OPTIMIZ, V27, P2110, DOI 10.1137/16M1095056	39	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005030
C	Chen, JC; Azer, ES; Zhang, Q		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Jiecao; Azer, Erfan Sadeqi; Zhang, Qin			A Practical Algorithm for Distributed Clustering and Outlier Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				K-MEANS	We study the classic k - means/median clustering, which are fundamental problems in unsupervised learning, in the setting where data are partitioned across multiple sites, and where we are allowed to discard a small portion of the data by labeling them as outliers. We propose a simple approach based on constructing small summary for the original dataset. The proposed method is time and communication efficient, has good approximation guarantees, and can identify the global outliers effectively. To the best of our knowledge, this is the first practical algorithm with theoretical guarantees for distributed clustering with outliers. Our experiments on both real and synthetic data have demonstrated the clear superiority of our algorithm against all the baseline algorithms in almost all metrics.	[Chen, Jiecao; Azer, Erfan Sadeqi; Zhang, Qin] Indiana Univ, Bloomington, IN 47405 USA	Indiana University System; Indiana University Bloomington	Chen, JC (corresponding author), Indiana Univ, Bloomington, IN 47405 USA.	jiecchen@indiana.edu; esadeqia@indiana.edu; qzhangcs@indiana.edu			NSF [CCF-1525024, CCF-1844234, IIS-1633215]	NSF(National Science Foundation (NSF))	Jiecao Chen, Erfan Sadeqi Azer and Qin Zhang are supported in part by NSF CCF-1525024, NSF CCF-1844234 and IIS-1633215.	Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915; Balcan M.F., 2013, P 26 INT C NEURAL IN, P1995; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Charikar M, 2001, SIAM PROC S, P642; Chawla S., 2013, P 2013 SIAM INT C DA; Chen J., 2016, NIPS; Chen K, 2009, SIAM J COMPUT, V39, P923, DOI 10.1137/070699007; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Diakonikolas I., 2017, ADV NEURAL INFORM PR, P6394; Ene A., 2011, SIGKDD, DOI DOI 10.1145/2020408.2020515; Feldman Dan, 2012, P 23 ANN ACM SIAM S, P1343; Guha S, 2003, IEEE T KNOWL DATA EN, V15, P515, DOI 10.1109/TKDE.2003.1198387; GUHA S, 2017, SPAA, P143, DOI DOI 10.1145/3087556.3087568; Gupta S, 2017, PROC VLDB ENDOW, V10, P757, DOI 10.14778/3067421.3067425; Li S., 2018, ARXIV181007852; Liang Y., 2014, ADV NEURAL INFORM PR, P3113; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; METTU RR, 2002, P 18 C UNC ART INT, P344; Sanderson C., 2010, ARMADILLO OPEN SOURC	20	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302027
C	Cummings, R; Krehbiel, S; Mei, YJ; Tuo, R; Zhang, WR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cummings, Rachel; Krehbiel, Sara; Mei, Yajun; Tuo, Rui; Zhang, Wanrong			Differentially Private Change-Point Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The change-point detection problem seeks to identify distributional changes at an unknown change-point k. in a stream of data. This problem appears in many important practical settings involving personal data, including biosurveillance, fault detection, finance, signal detection, and security systems. The field of differential privacy offers data analysis tools that provide powerful worst-case privacy guarantees. We study the statistical problem of change-point detection through the lens of differential privacy. We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and provide empirical validation of our results.	[Cummings, Rachel; Mei, Yajun; Zhang, Wanrong] Georgia Inst Technol, Atlanta, GA 30332 USA; [Krehbiel, Sara] Univ Richmond, Richmond, VA 23173 USA; [Tuo, Rui] Texas A&M Univ, College Stn, TX 77843 USA	University System of Georgia; Georgia Institute of Technology; University of Richmond; Texas A&M University System; Texas A&M University College Station	Zhang, WR (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	rachelc@gatech.edu; krehbiel@richmond.edu; ymei@gatech.edu; ruituo@tamu.edu; wanrongz@gatech.edu			Mozilla Research Grant; NSF [DMS-156443, CMMI-1362876]	Mozilla Research Grant; NSF(National Science Foundation (NSF))	R.C. and S.K. were supported in part by a Mozilla Research Grant. Y.M. and W.Z. were supported in part by NSF grant CMMI-1362876. R.T. was supported in part by NSF grant DMS-156443. R.T.'s contribution was completed while the author was visiting the Georgia Institute of Technology.	Bai J, 2003, J APPL ECONOMET, V18, P1, DOI 10.1002/jae.659; Chan HP, 2017, ANN STAT, V45, P2736, DOI 10.1214/17-AOS1546; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2009, ACM S THEORY COMPUT, P381; HINKLEY DV, 1970, BIOMETRIKA, V57, P1; Kulldorff M, 2001, J ROY STAT SOC A STA, V164, P61, DOI 10.1111/1467-985X.00186; LAI TL, 1995, J R STAT SOC B, V57, P613, DOI 10.1111/j.2517-6161.1995.tb02052.x; Lai TL, 2001, STAT SINICA, V11, P303; LORDEN G, 1971, ANN MATH STAT, V42, P1897, DOI 10.1214/aoms/1177693055; Lund R, 2002, J CLIMATE, V15, P2547, DOI 10.1175/1520-0442(2002)015<2547:DOUCAR>2.0.CO;2; Mei Y, 2010, BIOMETRIKA, V97, P419, DOI 10.1093/biomet/asq010; Mei Y., 2008, SEQUENTIAL ANAL, V27, P354, DOI [DOI 10.1080/07474940802445790, 10.1080/07474940802445790]; Mei YJ, 2006, ANN STAT, V34, P92, DOI 10.1214/009053605000000859; MOUSTAKIDES GV, 1986, ANN STAT, V14, P1379, DOI 10.1214/aos/1176350164; PAGE ES, 1954, BIOMETRIKA, V41, P100, DOI 10.2307/2333009; POLLAK M, 1985, ANN STAT, V13, P206, DOI 10.1214/aos/1176346587; POLLAK M, 1987, ANN STAT, V15, P749, DOI 10.1214/aos/1176350373; ROBERTS SW, 1966, TECHNOMETRICS, V8, P411, DOI 10.2307/1266688; Shewhart W.A., 1980, EC CONTROL QUALITY M; Shiryaev A.N., 1963, THEORY PROBAB ITS AP, V8, P22, DOI DOI 10.1137/1108002; Van Der Vaart AW, 1996, WEAK CONVERGENCE; Zhang NR, 2012, STAT SINICA, V22, P1507, DOI 10.5705/ss.2010.257	24	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005041
C	Derezinski, M; Warmuth, MK; Hsu, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Derezinski, Michal; Warmuth, Manfred K.; Hsu, Daniel			Leveraged volume sampling for linear regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				APPROXIMATION; MATRICES	Suppose an n x d design matrix in a linear regression problem is given, but the response for each point is hidden unless explicitly requested. The goal is to sample only a small number k << n of the responses, and then produce a weight vector whose sum of squares loss over all points is at most 1 + epsilon times the minimum. When k is very small (e.g., k = d), jointly sampling diverse subsets of points is crucial. One such method called volume sampling has a unique and desirable property that the weight vector it produces is an unbiased estimate of the optimum. It is therefore natural to ask if this method offers the optimal unbiased estimate in terms of the number of responses k needed to achieve a 1 + epsilon loss approximation. Surprisingly we show that volume sampling can have poor behavior when we require a very accurate approximation - indeed worse than some i.i.d. sampling techniques whose estimates are biased, such as leverage score sampling. We then develop a new rescaled variant of volume sampling that produces an unbiased estimate which avoids this bad behavior and has at least as good a tail bound as leverage score sampling: sample size k = O(d log d + d/epsilon) suffices to guarantee total loss at most 1 + epsilon. times the minimum with high probability. Thus we improve on the best previously known sample size for an unbiased estimator, k = O(d(2)/epsilon). Our rescaling procedure leads to a new efficient algorithm for volume sampling which is based on a determinantal rejection sampling technique with potentially broader applications to determinantal point processes. Other contributions include introducing the combinatorics needed for rescaled volume sampling and developing tail bounds for sums of dependent random matrices which arise in the process.	[Derezinski, Michal; Warmuth, Manfred K.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA; [Hsu, Daniel] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	University of California System; University of California Santa Cruz; Columbia University	Derezinski, M (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.	mderezin@berkeley.edu; manfred@ucsc.edu; djhsu@cs.columbia.edu		Hsu, Daniel/0000-0002-3495-7113	NSF [CCF-1740833, IIS-1619271]	NSF(National Science Foundation (NSF))	Michal Derezinski and Manfred K. Warmuth were supported by NSF grant IIS-1619271. Daniel Hsu was supported by NSF grant CCF-1740833.	Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Allen-Zhu Z, 2017, PR MACH LEARN RES, V70; Ando T., 1987, LINEAR MULTILINEAR A, V21, P345; Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287; Batson J, 2012, SIAM J COMPUT, V41, P1704, DOI 10.1137/090772873; Celis L. E., 2016, ARXIV PREPRINT ARXIV; Celis LE, 2018, PR MACH LEARN RES, V80; CesaBianchi N, 1996, IEEE T NEURAL NETWOR, V7, P604, DOI 10.1109/72.501719; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen  Xue, 2017, ABS171110051 CORR; Derezifiski Michal, 2017, ADV NEURAL INFORM PR, V30, P3087; Derezinski M, 2018, J MACH LEARN RES, V19, P1; Derezinski  Michal, 2018, P 21 INT C ART INT S; Deshpande A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1117, DOI 10.1145/1109557.1109681; Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; Gartrell M, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P349, DOI 10.1145/2959100.2959178; Gross D., 2010, ARXIV10012738; Harvey Nicholas J. A., 2014, P 25 ANN ACM SIAM S, P926, DOI DOI 10.1137/1.9781611973402.69; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jegelka S., 2017, ADV NEURAL INFORM PR, P5045; Kulesza Alex, 2011, ICML; Kulesza Alex, 2012, DETERMINANTAL POINT; Lee YT, 2015, ANN IEEE SYMP FOUND, P250, DOI 10.1109/FOCS.2015.24; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mariet Z, 2017, ADV NEUR IN, V30; Nikolov  Aleksandar, 2018, ARXIV180208318; Pemantle R, 2014, COMB PROBAB COMPUT, V23, P140, DOI 10.1017/S0963548313000345; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	33	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302051
C	Du, YL; Liu, ZJ; Basevi, H; Leonardis, A; Freeman, WT; Tenenbaum, JB; Wu, JJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Du, Yilun; Liu, Zhijian; Basevi, Hector; Leonardis, Ales; Freeman, William T.; Tenenbaum, Joshua B.; Wu, Jiajun			Learning to Exploit Stability for 3D Scene Parsing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Human scene understanding uses a variety of visual and non-visual cues to perform inference on object types, poses, and relations. Physics is a rich and universal cue that we exploit to enhance scene understanding. In this paper, we integrate the physical cue of stability into the learning process by looping in a physics engine into bottom-up recognition models, and apply it to the problem of 3D scene parsing. We first show that applying physics supervision to an existing scene understanding model increases performance, produces more stable predictions, and allows training to an equivalent performance level with fewer annotated training examples. We then present a novel architecture for 3D scene parsing named Prim R-CNN, learning to predict bounding boxes as well as their 3D size, translation, and rotation. With physics supervision, Prim R-CNN outperforms existing scene understanding approaches on this problem. Finally, we show that finetuning with physics supervision on unlabeled real images improves real domain transfer of models training on synthetic data.	[Du, Yilun; Liu, Zhijian; Freeman, William T.; Tenenbaum, Joshua B.; Wu, Jiajun] MIT, CSAIL, Cambridge, MA 02139 USA; [Basevi, Hector; Leonardis, Ales] Univ Birmingham, Birmingham, W Midlands, England	Massachusetts Institute of Technology (MIT); University of Birmingham	Wu, JJ (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	jiajunwu@mit.edu	Liu, Zhijian/ABF-4061-2020; Wu, JiaJun/GQH-7885-2022	Liu, Zhijian/0000-0003-3632-9986; 	ONR MURI [N00014-16-1-2007]; Center for Brain, Minds, and Machines (CBMM); NSF [1447476]; Toyota Research Institute; Facebook; MoD/Dstl; EPSRC [EP/N019415/1]	ONR MURI(MURIOffice of Naval Research); Center for Brain, Minds, and Machines (CBMM); NSF(National Science Foundation (NSF)); Toyota Research Institute; Facebook(Facebook Inc); MoD/Dstl; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work is in part supported by ONR MURI N00014-16-1-2007, the Center for Brain, Minds, and Machines (CBMM), Toyota Research Institute, NSF #1447476, and Facebook. We acknowledge MoD/Dstl and EPSRC for providing the grant to support the UK academics' involvement in a Department of Defense funded MURI project through EPSRC grant EP/N019415/1.	Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Battaglia Peter W, 2016, ARXIV161200222; Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366; Chang MB., 2017, 5 INT C LEARN REPR I; Coumans E., 2010, OPEN SOURCE SOFTWARE; Dai JH, 2017, IEEE T CYBERNETICS, V47, P2460, DOI 10.1109/TCYB.2016.2636339; Ehrhardt S., 2017, ARXIV170300247; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eslami SM, 2016, NEURIPS, V1; Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586; Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154; Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929; Fragkiadaki K., 2016, ICLR; Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Huang Jonathan, 2015, ICLR WORKSH; Jia ZY, 2013, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2013.8; Kingma D.P, P 3 INT C LEARNING R; Lerer A, 2016, PR MACH LEARN RES, V48; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; McCormac J, 2017, IEEE I CONF COMP VIS, P2697, DOI 10.1109/ICCV.2017.292; Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Roberts Lawrence G, 1963, THESIS, P2; Shao TJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661288; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Stewart R., 2017, AAAI, P2576; Tulsiani Shubham, 2018, CVPR, DOI DOI 10.1109/CVPR.2018.00306; Wenbin Li, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2606, DOI 10.1109/ICRA.2017.7989304; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu J., 2016, THESIS; Wu J., 2016, PROC BRIT MACH VIS C, V2, P7; Wu JJ, 2017, PROC CVPR IEEE, P7035, DOI 10.1109/CVPR.2017.744; Wu JX, 2015, IEEE INT ULTRA SYM, DOI 10.1109/ULTSYM.2015.0328; Wu JH, 2018, INT J RADIAT BIOL, V94, P782, DOI 10.1080/09553002.2017.1364801; Zhang Renqiao, 2016, COGSCI; Zhang Yuting, 2017, CVPR; Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4; Zheng B, 2013, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2013.402; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	47	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301069
C	Eriksson, D; Dong, K; Lee, EH; Bindel, D; Wilson, AG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Eriksson, David; Dong, Kun; Lee, Eric Hans; Bindel, David; Wilson, Andrew Gordon			Scaling Gaussian Process Regression with Derivatives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVOLUTION	Gaussian processes (GPs) with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at n points in d dimensions requires linear solves and log determinants with an n(d + 1) x n(d + 1) positive definite matrix - leading to prohibitive O(n(3)d(3)) computations for standard direct methods. We propose iterative solvers using fast O(nd) matrix-vector multiplications (MVMs), together with pivoted Cholesky preconditioning that cuts the iterations to convergence by several orders of magnitude, allowing for fast kernel learning and prediction. Our approaches, together with dimensionality reduction, enables Bayesian optimization with derivatives to scale to high-dimensional problems and large evaluation budgets.	[Eriksson, David; Dong, Kun] Cornell Univ, Ctr Appl Math, Ithaca, NY 14853 USA; [Lee, Eric Hans; Bindel, David] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Wilson, Andrew Gordon] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14853 USA	Cornell University; Cornell University; Cornell University	Eriksson, D (corresponding author), Cornell Univ, Ctr Appl Math, Ithaca, NY 14853 USA.	dme65@cornell.edu; kd383@cornell.edu; ehl59@cornell.edu; bindel@cornell.edu; andrew@cornell.edu			NSF [DMS-1620038, HS-1563887]; Facebook Research	NSF(National Science Foundation (NSF)); Facebook Research(Facebook Inc)	We thank NSF DMS-1620038, NSF HS-1563887, and Facebook Research for support.	[Anonymous], 2018, VIRTUAL LIB SIMULATI; [Anonymous], 2017, P 32 INT C NEUR INF; Bekas C, 2007, APPL NUMER MATH, V57, P1214, DOI 10.1016/j.apnum.2007.01.003; Ben-Ari Einat Neumann, 2007, Quality Engineering, V19, P327, DOI 10.1080/08982110701580930; Constantine P, 2015, ACTIVE SUBSPACES EME; Cutajar K, 2016, PR MACH LEARN RES, V48; Forrester A., 2008, ENG DESIGN VIA SURRO, P168; Gardner Jacob R, 2018, ARTIFICIAL INTELLIGE; Gingras David, 2010, Proceedings of the 2010 Seventh Canadian Conference on Computer and Robot Vision (CRV 2010), P191, DOI 10.1109/CRV.2010.32; Hadsell R, 2010, INT J ROBOT RES, V29, P981, DOI 10.1177/0278364910369996; Han I, 2015, PR MACH LEARN RES, V37, P908; Harbrecht H, 2012, APPL NUMER MATH, V62, P428, DOI 10.1016/j.apnum.2011.10.001; Hensman James, 2013, P C UNC ART INT UAI; Konolige K, 2010, SPRINGER TRAC ADV RO, V66, P201; Le Quoc V., 2013, INT C MACH LEARN, P244; Macedo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Meijering EHW, 1999, IEEE T IMAGE PROCESS, V8, P192, DOI 10.1109/83.743854; Puget Sound LiDAR Consortium, 2002, MOUNT SAINT HEL LIDA; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rasmussen CE, 2001, ADV NEUR IN, V13, P294; Snelson E., 2005, ADV NEURAL INFORM PR, P1257; Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974; Wang Z., 2013, INT JOINT C ART INT; Wilson A.G., 2015, ADV NEURAL INFORM PR, V28, P2854; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Wu J., 2017, P 31 INT C NEUR INF, P5273	29	4	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001041
C	Ghoshdastidar, D; von Luxburg, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ghoshdastidar, Debarghya; von Luxburg, Ulrike			Practical Methods for Graph Two-Sample Testing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMMUNITY DETECTION	Hypothesis testing for graphs has been an important tool in applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question. In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods.	[Ghoshdastidar, Debarghya] Univ Tubingen, Dept Comp Sci, Tubingen, Germany; [von Luxburg, Ulrike] Univ Tubingen, Dept Comp Sci, Max Planck Inst Intelligent Syst, Tubingen, Germany	Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Max Planck Society	Ghoshdastidar, D (corresponding author), Univ Tubingen, Dept Comp Sci, Tubingen, Germany.	ghoshdas@informatik.uni-tuebingen.de; luxburg@informatik.uni-tuebingen.de			German Research Foundation (Research Unit 1735); Institutional Strategy of the University of Tubingen (DFG) [ZUK 63]	German Research Foundation (Research Unit 1735)(German Research Foundation (DFG)); Institutional Strategy of the University of Tubingen (DFG)	This work is supported by the German Research Foundation (Research Unit 1735) and the Institutional Strategy of the University of Tubingen (DFG, ZUK 63).	Anderson T. W, 1984, INTRO MULTIVARIATE S; Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907; Bassett DS, 2008, J NEUROSCI, V28, P9239, DOI 10.1523/JNEUROSCI.1929-08.2008; Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Clarke R, 2008, NAT REV CANCER, V8, P37, DOI 10.1038/nrc2294; Dua D., 2017, UCI MACHINE LEARNING; Erdos L, 2012, ADV MATH, V229, P1435, DOI 10.1016/j.aim.2011.12.010; Ghoshdastidar D., 2017, C LEARN THEOR COLT; Ghoshdastidar Debarghya, 2017, ARXIV170700833; Ginestet CE, 2017, ANN APPL STAT, V11, P725, DOI 10.1214/16-AOAS1015; Ginestet CE, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00051; Goldreich O, 1998, J ACM, V45, P653, DOI 10.1145/285055.285060; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hyduke DR, 2013, MOL BIOSYST, V9, P167, DOI 10.1039/c2mb25453k; Kondor R., 2016, ADV NEURAL INFORM PR; Landman BA, 2011, NEUROIMAGE, V54, P2854, DOI 10.1016/j.neuroimage.2010.11.047; Lee JO, 2014, DUKE MATH J, V163, P117, DOI 10.1215/00127094-2414767; Leskovec J., 2005, KDD; Leskovec J, 2014, SNAP DATASETS STANFO; Lovasz L., 2012, LARGE NETWORKS GRAPH; Mukherjee S. S., 2017, ADV NEURAL INFORM PR; Ng A.Y., 2002, ADV NEURAL INFORM PR; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Tang M., 2016, J COMPUTATIONAL GRAP, V26, P344; Tang M, 2017, BERNOULLI, V23, P1599, DOI 10.3150/15-BEJ789; Tracy CA, 1996, COMMUN MATH PHYS, V177, P727, DOI 10.1007/BF02099545; Yang J., 2013, P 6 ACM INT C WEB SE, P587, DOI [DOI 10.1145/2433396.2433471, 10.1145/2433396.2433471]; Zhang B, 2009, BIOINFORMATICS, V25, P526, DOI 10.1093/bioinformatics/btn660	33	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303005
C	Hao, Y; Orlitsky, A; Pichapati, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hao, Yi; Orlitsky, Alon; Pichapati, Venkatadheeraj			On Learning Markov Chains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The problem of estimating an unknown discrete distribution from its samples is a fundamental tenet of statistical learning. Over the past decade, it attracted significant research effort and has been solved for a variety of divergence measures. Surprisingly, an equally important problem, estimating an unknown Markov chain from its samples, is still far from understood. We consider two problems related to the min-max risk (expected loss) of estimating an unknown k-state Markov chain from its n sequential samples: predicting the conditional distribution of the next sample with respect to the KL-divergence, and estimating the transition matrix with respect to a natural loss induced by KL or a more general f -divergence measure. For the first measure, we determine the min-max prediction risk to within a linear factor in the alphabet size, showing it is Omega(k log log n/n) and O(k(2) log log n/n). For the second, if the transition probabilities can be arbitrarily small, then only trivial uniform risk upper bounds can be derived. We therefore consider transition probabilities that are bounded away from zero, and resolve the problem for essentially all sufficiently smooth f -divergences, including KL-, L-2-, Chi-squared, Hellinger, and Alpha-divergences.	[Hao, Yi; Orlitsky, Alon; Pichapati, Venkatadheeraj] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Hao, Y (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.	yih179@ucsd.edu; alon@ucsd.edu; dheerajpv7@ucsd.edu						Bishop C.M, 2006, PATTERN RECOGN; Braess D, 2004, J APPROX THEORY, V128, P187, DOI 10.1016/j.jat.2004.04.010; Braess D, 2002, LECT NOTES ARTIF INT, V2533, P380; Chung F., 2006, CBMS REGIONAL C SERI; Chung KL., 2012, ELEMENTARY PROBABILI; COVER TM, 1972, IEEE T INFORM THEORY, V18, P216, DOI 10.1109/TIT.1972.1054738; Crooks Gavin E., 2017, MEASURES ENTROPY INF, V4; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; Falahatgar M., 2016, NIPS, P4860; Falahatgar M, 2016, IEEE INT SYMP INFO, P2689, DOI 10.1109/ISIT.2016.7541787; GILBERT EN, 1971, IEEE T INFORM THEORY, V17, P304, DOI 10.1109/TIT.1971.1054638; KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; LEVIN D. A., 2017, AM MATH SOC, DOI [10.1090/mbk/107, DOI 10.1090/MBK/107]; Nielsen F, 2014, IEEE SIGNAL PROC LET, V21, P10, DOI 10.1109/LSP.2013.2288355; Nikulin M.S., 2001, ENCY MATH, V151; Norris J, 2017, COMB PROBAB COMPUT, V26, P603, DOI 10.1017/S0963548317000074; Orlitsky A., 2015, ADV NEURAL INFORM PR, P2143; Orlitsky Alon, 2015, C LEARN THEOR, P1066; Paninski L, 2004, P ADV NEUR INF PROC; Valiant G, 2016, ACM S THEORY COMPUT, P142, DOI 10.1145/2897518.2897641; Wolfer Geoffrey, 2018, ARXIV180905014	22	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300060
C	Huang, ZY; Liu, JY; Wang, XN		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huang, Zhiyi; Liu, Jinyan; Wang, Xiangning			Learning Optimal Reserve Price against Non-myopic Bidders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of learning optimal reserve price in repeated auctions against non-myopic bidders, who may bid strategically in order to gain in future rounds even if the single-round auctions are truthful. Previous algorithms, e.g., empirical pricing, do not provide non-trivial regret rounds in this setting in general. We introduce algorithms that obtain a small regret against non-myopic bidders either when the market is large, i.e., no single bidder appears in more than a small constant fraction of the rounds, or when the bidders are impatient, i.e., they discount future utility by some factor mildly bounded away from one. Our approach carefully controls what information is revealed to each bidder, and builds on techniques from differentially private online learning as well as the recent line of works on jointly differentially private algorithms.	[Huang, Zhiyi; Liu, Jinyan; Wang, Xiangning] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China	University of Hong Kong	Huang, ZY (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.	zhiyi@cs.hku.hk; jyliu@cs.hku.hk; xnwang@cs.hku.hk	Huang, Zhiyi/I-7976-2013	Huang, Zhiyi/0000-0003-2963-9556	RGC [HKU17257516E]	RGC(Hong Kong Research Grants Council)	Supported in part by an RGC grant HKU17257516E.	Abernethy J. D., 2014, COLT, P807; Agarwal N, 2017, PR MACH LEARN RES, V70; Amin K., 2014, ADV NEURAL INFORM PR, P622; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Blum A, 2004, THEOR COMPUT SCI, V324, P137, DOI 10.1016/j.tcs.2004.05.012; Blum A, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1156; Bubeck S, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P497, DOI 10.1145/3033274.3085145; Cai Yang, 2017, 58 ANN IEEE S FDN CO; Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Devanur NR, 2016, ACM S THEORY COMPUT, P426, DOI 10.1145/2897518.2897553; Devanur Nikhil R., 2015, P TWENTYSIXTH ANN AC, P983; Dughmi S, 2014, LECT NOTES COMPUT SC, V8877, P277, DOI 10.1007/978-3-319-13129-0_22; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2010, ACM S THEORY COMPUT, P715; Epasto A, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1369, DOI 10.1145/3178876.3186042; Gonczarowski YA, 2017, ACM S THEORY COMPUT, P856, DOI 10.1145/3055399.3055427; Hartline JD, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P225; Hsu J, 2016, SIAM J COMPUT, V45, P1953, DOI 10.1137/15100271X; Hsu Justin, 2016, P 27 ANN ACM SIAM S, P580; Huang Zhiyi, 2015, 16 ACM C EC COMP EC, P45; Huang Zhiyi, 2018, P 29 ANN ACM SIAM S; Immorlica N, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P167, DOI 10.1145/3033274.3085130; Jain P., 2012, P 25 ANN C LEARNING, V23; Kearns M, 2014, P 5 C INN THEOR COMP, P403; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mirrokni V, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P169, DOI 10.1145/3219166.3219224; Mohri Mehryar, 2014, ADV NEURAL INFORM PR, P1871; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Nekipelov Denis, 2015, P 16 ACM C EC COMPUT, P1, DOI [10.1145/2764468.2764522, DOI 10.1145/2764468.2764522]; Nissim Kobbi, 2012, 3 INN THEOR COMP SCI, P203; Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P1, DOI 10.1145/2940716.2940723; Tossou ACY, 2017, AAAI CONF ARTIF INTE, P2653	37	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302008
C	Jaiswal, A; Wu, Y; AbdAlmageed, W; Natarajan, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jaiswal, Ayush; Wu, Yue; AbdAlmageed, Wael; Natarajan, Premkumar			Unsupervised Adversarial Invariance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality.	[Jaiswal, Ayush; Wu, Yue; AbdAlmageed, Wael; Natarajan, Premkumar] USC Informat Sci Inst, Marina Del Rey, CA 90292 USA	University of Southern California	Jaiswal, A (corresponding author), USC Informat Sci Inst, Marina Del Rey, CA 90292 USA.	ajaiswal@isi.edu; yue_wu@isi.edu; wamageed@isi.edu; pnataraj@isi.edu		Wu, Yue/0000-0003-0126-3614	Defense Advanced Research Projects Agency [FA8750-16-2-0204]	Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work is based on research sponsored by the Defense Advanced Research Projects Agency under agreement number FA8750-16-2-0204. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government.	Antoniou Antreas, 2017, ARXIV171104340; Aubry Mathieu, 2014, P IEEE COMP SOC C CO; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Chen M., 2012, ARXIV12064683; Ganin Y., 2016, JMLR, V17, P2096; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ko T, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3586; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Yujia, 2014, ARXIV14125244; Louizos C., 2016, 4 INT C LEARN REPR; Masi I., 2018, IEEE T PATTERN ANAL; Miao JY, 2016, PROCEDIA COMPUT SCI, V91, P919, DOI 10.1016/j.procs.2016.07.111; Ruder S., 2017, ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Thompson P., 2018, ARXIV180501049; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Xie Q., 2017, PROC NEURAL INF PROC, P585	19	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305013
C	Jiao, JT; Gao, WH; Han, YJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiao, Jiantao; Gao, Weihao; Han, Yanjun			The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MUTUAL INFORMATION; INTEGRAL FUNCTIONALS; FEATURE-SELECTION; DENSITIES; ENTROPY	We analyze the Kozachenko-Leonenko (KL) fixed k-nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance for any fixed k over Holder balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a recent minimax lower bound over the Holder ball, we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the Holder ball for s is an element of (0, 2] and arbitrary dimension d, rendering it the first estimator that provably satisfies this property.	[Jiao, Jiantao] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Gao, Weihao] Univ Illinois, Dept ECE, Coordinated Sci Lab, Urbana, IL USA; [Han, Yanjun] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	University of California System; University of California Berkeley; University of Illinois System; University of Illinois Urbana-Champaign; Stanford University	Jiao, JT (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	jiantao@berkeley.edu; wgao9@illinois.edu; yjhan@stanford.edu						[Anonymous], 2017, ARXIV171102141; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Beirlant J., 1997, INT J MATH STAT SCI, V6, P17; BERRETT TB, 2016, ARXIV160600304; Biau G., 2015, LECT NEAREST NEIGHBO; BICKEL PJ, 1988, SANKHYA SER A, V50, P381; Bu YH, 2016, IEEE INT SYMP INFO, P1118, DOI 10.1109/ISIT.2016.7541473; Cai TT, 2005, ANN STAT, V33, P2930, DOI 10.1214/009053605000000147; Cai TT, 2003, ANN STAT, V31, P1140; Chan C, 2015, P IEEE, V103, P1883, DOI 10.1109/JPROC.2015.2458316; Delattre S, 2017, J STAT PLAN INFER, V185, P69, DOI 10.1016/j.jspi.2017.01.004; Donoho D. L., 1990, Journal of Complexity, V6, P290, DOI 10.1016/0885-064X(90)90025-9; EFRON B, 1981, ANN STAT, V9, P586, DOI 10.1214/aos/1176345462; Evans L. C., 2015, MEASURE THEORY FINE; FAN JQ, 1991, ANN STAT, V19, P1273, DOI 10.1214/aos/1176348249; Gao W., 2016, P 33 INT C MACH LEAR, P2780; Gao WH, 2017, IEEE INT SYMP INFO, P1267, DOI 10.1109/ISIT.2017.8006732; Gao Weihao, 2017, NIPS, P5988; Gao Weihao, 2016, P ADV NEUR INF PROC, P2460; Ginc E, 2008, BERNOULLI, V14, P47, DOI 10.3150/07-BEJ110; Haje H. F. E., 2009, J MATH SCI, V163, P290, DOI DOI 10.1007/s10958-009-9674-x; HALL P, 1993, ANN I STAT MATH, V45, P69, DOI 10.1007/BF00773669; HALL P, 1987, STAT PROBABIL LETT, V6, P109, DOI 10.1016/0167-7152(87)90083-6; HALL P, 1984, MATH PROC CAMBRIDGE, V96, P517, DOI 10.1017/S0305004100062459; Han Yanjun, 2016, ARXIV160509124; Han Yanjun, 2017, ARXIV171003863; JOE H, 1989, ANN I STAT MATH, V41, P683, DOI 10.1007/BF00057735; Kerkyacharian G, 1996, ANN STAT, V24, P485; Kozachenko L.F., 1987, PROBL PEREDACHI INF, V23, P9; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Krishnamurthy A, 2014, PR MACH LEARN RES, V32, P919; Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689; Laurent B, 1996, ANN STAT, V24, P659; Lepski O, 1999, PROBAB THEORY REL, V113, P221, DOI 10.1007/s004409970006; Lepskii O.V., 1992, TOPICS NONPARAMETRIC, V12, P87; Li Pan, 2017, ARXIV170901249; MACK YP, 1979, J MULTIVARIATE ANAL, V9, P1, DOI 10.1016/0047-259X(79)90065-4; Mukherjee R., 2017, SEMIPARAMETRIC EFFIC; Mukherjee R., 2016, ARXIV160801364; Mukherjee Rajarshi, 2015, ARXIV150800249; Muller A. C., 2012, INFORM THEORETIC CLU; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Robins J., 2016, ANN STAT; Robins J. M., 2008, PROBABILITY STAT ESS, V2, P335, DOI [10.1214/193940307000000527, DOI 10.1214/193940307000000527]; Sricharan K, 2012, IEEE T INFORM THEORY, V58, P4135, DOI 10.1109/TIT.2012.2195549; Steeg G. Ver, 2014, STAT, V1050, P27; Tchetgen E, 2008, STAT PROBABIL LETT, V78, P3307, DOI 10.1016/j.spl.2008.07.001; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Tsybakov AB, 1996, SCAND J STAT, V23, P75; VANES B, 1992, SCAND J STAT, V19, P61; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060	59	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303018
C	Kalra, A; Rashwan, A; Hsu, W; Poupart, P; Doshi, P; Trimponias, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kalra, Agastya; Rashwan, Abdullah; Hsu, Wilson; Poupart, Pascal; Doshi, Prashant; Trimponias, George			Online Structure Learning for Feed-Forward and Recurrent Sum-Product Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which marginal inference is always tractable. These properties follow from the conditions of completeness and decomposability, which must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice. This paper describes a new online structure learning technique for feed-forward and recurrent SPNs. The algorithm is demonstrated on real-world datasets with continuous features and sequence datasets of varying length for which the best network architecture is not obvious.	[Kalra, Agastya; Rashwan, Abdullah; Hsu, Wilson; Poupart, Pascal] Univ Waterloo, Waterloo AI Inst, Cheriton Sch Comp Sci, Waterloo, ON, Canada; [Kalra, Agastya; Rashwan, Abdullah; Hsu, Wilson; Poupart, Pascal] Vector Inst, Toronto, ON, Canada; [Doshi, Prashant] Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA; [Trimponias, George] Huawei Noahs Ark Lab, Hong Kong, Peoples R China	University of Waterloo; University System of Georgia; University of Georgia; Huawei Technologies	Kalra, A (corresponding author), Univ Waterloo, Waterloo AI Inst, Cheriton Sch Comp Sci, Waterloo, ON, Canada.; Kalra, A (corresponding author), Vector Inst, Toronto, ON, Canada.	agastya.kalra@gmail.com; arashwan@uwaterloo.ca; wwhsu@uwaterloo.ca; ppoupart@uwaterloo.ca; pdoshi@cs.uga.edu; g.trimponias@huawei.com			Huawei Technologies; NSERC; NSF [IIS-1815598]	Huawei Technologies(Huawei Technologies); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); NSF(National Science Foundation (NSF))	This research was funded by Huawei Technologies and NSERC. Prashant Doshi acknowledges support from NSF grant #IIS-1815598.	Adel T, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P32; [Anonymous], 2015, THESIS GRAZ U TECHNO; [Anonymous], P 32 C UNC ART INT U; [Anonymous], 2002, P EIGHTS INT C PRINC; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Dennis A., 2012, ADV NEURAL INFORM PR, P2033; Dinh L, 2017, 5 INT C LEARN REPR I; Gens R., 2012, 26 ADV NEURAL INFORM, P3239; Gens R., 2013, 30 INT C MACHINE LEA, P873; Jaini P., 2016, C PROB GRAPH MOD, P228; Melibari M., 2016, C PROB GRAPH MOD, V52, P345; Peharz Robert, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P612, DOI 10.1007/978-3-642-40991-2_39; Poon H., 2011, P 27 C UNC ART INT, P337, DOI DOI 10.1109/ICCVW.2011; Rashwan A, 2016, JMLR WORKSH CONF PRO, V51, P1469; Rooshenas A, 2014, PR MACH LEARN RES, V32; Roth D, 1996, ARTIF INTELL, V82, P273, DOI 10.1016/0004-3702(94)00092-1; Sang-Woo Lee, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8227, P220, DOI 10.1007/978-3-642-42042-9_28; VERGARI A, 2015, ECML PKDD, V9285, P343, DOI DOI 10.1007/978-3-319-23525-7_21; Zhao H., 2016, NIPS 16, P433; Zhao H, 2015, PR MACH LEARN RES, V37, P116; Zhao Han, 2016, ICML	21	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001048
C	Kazemi, H; Soleymani, S; Taherkhani, F; Iranmanesh, SM; Nasrabadi, NM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kazemi, Hadi; Soleymani, Sobhan; Taherkhani, Fariborz; Iranmanesh, Seyed Mehdi; Nasrabadi, Nasser M.			Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches usually fail to model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.	[Kazemi, Hadi; Soleymani, Sobhan; Taherkhani, Fariborz; Iranmanesh, Seyed Mehdi; Nasrabadi, Nasser M.] West Virginia Univ, Morgantown, WV 26505 USA	West Virginia University	Kazemi, H (corresponding author), West Virginia Univ, Morgantown, WV 26505 USA.	hakazemi@mix.wvu.edu; ssoleyma@mix.wvu.edu; fariborztaherkhani@gmail.com; seiranmanesh@mix.wvu.edu; nasser.nasrabadi@mail.wvu.edu		Taherkhani, Fariborz/0000-0001-7966-734X				Almahairi A, 2018, PR MACH LEARN RES, V80; Bansal A., 2017, PROC INT C LEARN REP; Bousmalis K., 2017, UNSUPERVISED PIXEL L; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; IIZUKA S, 2016, ACM T GRAPHIC, V35, DOI DOI 10.1145/2897824.2925974; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A., 2014, CORR; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Liu Ming-Yu, 2017, NIPS; Peng CL, 2017, IEEE T CIRC SYST VID, V27, P288, DOI 10.1109/TCSVT.2015.2502861; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Royer A., 2017, ARXIV PREPRINT ARXIV; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zhu Jun-Yan, 2017, ICCV; Zohrizadeh F, 2018, IEEE WINT CONF APPL, P1470, DOI 10.1109/WACV.2018.00165	39	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004086
C	Keskin, C; Izadi, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Keskin, Cem; Izadi, Shahram			SplineNets: Continuous Neural Decision Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e., conditioned on the computational path). SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. In particular, we embed infinitely many function weights (e.g. filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter. Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight. We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks. Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet.				cemkeskin@google.com; shahrami@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; Aila T., 2016, ARXIV PREPRINT ARXIV; Baek  Seungryul, 2017, ABS170602003 CORR; Bicici Ufuk Can, 2018, PATT REC ICPR 2018 2; Bulo SR, 2014, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2014.18; De Brabandere B, 2016, ADV NEUR IN, V29; Denoyer  Ludovic, 2014, ABS14100510 CORR; Ha David, 2016, ARXIV160909106; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663; Iandola F.N., 2016, ARXIV; Ioannou  Yani, 2016, ABS160301250 CORR; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kontschieder P., 2016, IJCAI, P4190; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Murdock C, 2017, PROC CVPR IEEE, P673, DOI 10.1109/CVPR.2017.79; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sabour Sara, 2017, PROC 31 INT C NEURAL; Shotton J., 2013, ADV NEURAL INFORM PR, P234; Wu  Jiaxiang, 2015, ABS151206473 CORR; Xiong C, 2015, IEEE I CONF COMP VIS, P3667, DOI 10.1109/ICCV.2015.418; Yang  Tien-Ju, 2016, ABS161105128 CORR; Zhao TT, 2017, IEEE PHOTONICS J, V9, P2, DOI 10.1109/JPHOT.2016.2644864; Zhou A, 2017, INCREMENTAL NETWORK; Zhou S., 2016, ARXIV160606160	31	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302004
C	Kondor, R; Lin, Z; Trivedi, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kondor, Risi; Lin, Zhen; Trivedi, Shubhendu			Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent work by Cohen et al. [1] has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch-Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.	[Kondor, Risi; Lin, Zhen] Univ Chicago, Chicago, IL 60637 USA; [Trivedi, Shubhendu] Toyota Technol Inst, Nagoya, Aichi, Japan	University of Chicago; Toyota Technological Institute	Kondor, R (corresponding author), Univ Chicago, Chicago, IL 60637 USA.	risi@uchicago.edu; zlin7@uchicago.edu; shubhendu@ttic.edu						[Anonymous], 2018, CORR; Blum L. C., 2009, J AM CHEM SOC; Boomsma W., 2017, ADV NEURAL INFORM PR, V30, P3433; Cohen T., 2017, ICLR; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen TS, 2018, P 6 INT C LEARNING R; Cruz-Mota J, 2012, INT J COMPUT VISION, V98, P217, DOI 10.1007/s11263-011-0505-4; Diaconis P., 1988, IMS LECT SERIES, V11; Esteves C., 2017, LEARNING SO 3 EQUIVA; Esteves C., 2017, POLAR TRANSFORMER NE; Gens R., 2014, NIPS 2014, P1; GUTMAN B, 2008, 2 MICCAI WORKSH MATH, P56; Hanrahan P., 2015, SHAPENET INFORM RICH; Healy DM, 1996, INT CONF ACOUST SPEE, P1323, DOI 10.1109/ICASSP.1996.543670; Khasanova R., 2017, GRAPH BASED CLASSIFI; Kondor Risi, 2018, GEN EQUIVARIANCE CON; Kostelec PJ, 2008, J FOURIER ANAL APPL, V14, P145, DOI 10.1007/s00041-008-9013-5; Lai W-S, SEMANTIC DRIVEN GENE, P1; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Masci J., 2015, GEODESIC CONVOLUTION; Montavon G., 2012, NIPS; Monti F., 2016, GEOMETRIC DEEP LEARN; Raj A., 2016, LOCAL GROUP INVARIAN; Ravanbakhsh S., 2017, P INT C MACH LEARN; Rupp M., 2012, PHYS REV LETT; Savva M., 2017, EUR WORKSH 3D OBJ RE; Serre J-P., 1977, GRADUATE TEXTS MATHA, V42; Su Y. -C., 2017, MAKING 360 VIDEO WAT; Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10; Terras A., 1999, LONDON MATH SOC STUD, V43; Thomas N., 2018, ARXIV180208219CS; Worrall D. E., 2016, HARMONIC NETWORKS DE; Zelnik-Manor L, 2005, IEEE I CONF COMP VIS, P1292	33	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004065
C	Korba, A; Garcia, A; d'Alche-Buc, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Korba, Anna; Garcia, Alexandre; d'Alche-Buc, Florence			A Structured Prediction Approach for Label Ranking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose to solve a label ranking problem as a structured output regression task. In this view, we adopt a least square surrogate loss approach that solves a supervised learning problem in two steps: a regression step in a well-chosen feature space and a pre-image (or decoding) step. We use specific feature maps/embeddings for ranking data, which convert any ranking/permutation into a vector representation. These embeddings are all well-tailored for our approach, either by resulting in consistent estimators, or by solving trivially the pre-image problem which is often the bottleneck in structured prediction. Their extension to the case of incomplete or partial rankings is also discussed. Finally, we provide empirical results on synthetic and real-world datasets showing the relevance of our method.	[Korba, Anna; Garcia, Alexandre; d'Alche-Buc, Florence] Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Korba, A (corresponding author), Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.	anna.korba@telecom-paristech.fr; alexandre.garcia@telecom-paristech.fr; florence.dalche-buc@telecom-paristech.fr						Aiguzhinov A, 2010, LECT NOTES ARTIF INT, V6332, P16, DOI 10.1007/978-3-642-16184-1_2; Ailon N, 2010, ALGORITHMICA, V57, P284, DOI 10.1007/s00453-008-9211-1; Aledo JA, 2017, INFORM FUSION, V35, P38, DOI 10.1016/j.inffus.2016.09.002; Brazdil PB, 2003, MACH LEARN, V50, P251, DOI 10.1023/A:1021713901879; Brouard C, 2016, J MACH LEARN RES, V17; Calauzenes C., 2012, ADV NEURAL INFORM PR, V25, P197; Cao Z., 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Cheng W., 2009, P 26 ANN INT C MACH, P161, DOI DOI 10.1145/1553374.1553395; Cheng W., 2013, PROC MULTIDISCIPLINA, P1; Cheng W., 2010, ICML, P215; Chiang T.H., 2012, ASIAN C MACHINE LEAR, P81; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Clemencon S, 2017, ARXIV171100070; Cortes C., 2005, PROC 22 INT C MACH L, P153, DOI DOI 10.1145/1102351.1102371; de Sa CR, 2018, INFORM FUSION, V40, P112, DOI 10.1016/j.inffus.2017.07.001; Dekel O, 2004, ADV NEUR IN, V16, P497; Devroye L, 2013, PROBABILISTIC THEORY, V31; Deza M. M., 2009, ENCY DISTANCES; Djuric N, 2014, AAAI CONF ARTIF INTE, P1788; Fagin R., 2004, P ACM S PRINC DAT SY, P47, DOI DOI 10.1145/1055558.1055568; Fathony R, 2018, INT C MACH LEARN, P1456; Furnkranz J, 2003, LECT NOTES ARTIF INT, V2837, P145; Geng X, 2014, PROC CVPR IEEE, P3742, DOI 10.1109/CVPR.2014.478; Gurrieri M, 2012, CCIS, P613, DOI [10.1007/978-3-642-31709-5_62, DOI 10.1007/978-3-642-31709-5-62]; Jiao Y., 2016, P INT C MACH LEARN I, V48, P2971; Kadri Hachem, 2013, INT C MACH LEARN ICM, P471; Kamishima T, 2010, PREFERENCE LEARNING, P181, DOI 10.1007/978-3-642-14125-6_9; Kenkre S., 2011, P 2011 SIAM INT C DA; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Li P, 2017, ARXIV170109083; Mares M, 2007, LECT NOTES COMPUT SC, V4698, P187; Merlin VR, 1997, J ECON THEORY, V72, P148, DOI 10.1006/jeth.1996.2205; Micchelli CA, 2005, J MACH LEARN RES, V6, P1099; Myrvold W, 2001, INFORM PROCESS LETT, V79, P281, DOI 10.1016/S0020-0190(01)00141-7; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Osokin A., 2017, ADV NEURAL INFORM PR, P302; Ramaswamy H. G., 2013, ADV NEURAL INFORM PR, P1475; Sa C. R, 2017, LABEL RANKING FOREST; Steinwart I., 2008, SUPPORT VECTOR MACHI; Vembu S, 2010, PREFERENCE LEARNING, P45, DOI 10.1007/978-3-642-14125-6_3; Wang D, 2015, IEEE T INFORM THEORY, V61, P6417, DOI 10.1109/TIT.2015.2485270; Wang QS, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P164, DOI 10.1109/ACPR.2011.6166699; Yu PLH, 2010, PREFERENCE LEARNING, P83, DOI 10.1007/978-3-642-14125-6_5; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhou Y, 2016, ARXIV160807710; Zhou YM, 2014, J COMPUT, V9, P557, DOI 10.4304/jcp.9.3.557-565	47	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003054
C	Lee, W; Yu, H; Yang, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Wonyeol; Yu, Hangyeol; Yang, Hongseok			Reparameterization Gradient for Non-differentiable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a new algorithm for stochastic variational inference that targets at models with non-differentiable densities. One of the key challenges in stochastic variational inference is to come up with a low-variance estimator of the gradient of a variational objective. We tackle the challenge by generalizing the reparameterization trick, one of the most effective techniques for addressing the variance issue for differentiable models, so that the trick works for non-differentiable models as well. Our algorithm splits the space of latent variables into regions where the density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the algorithm applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling and computes the direction for variational parameters that, if followed, would increase the boundary's contribution to the variational objective. The sum of all the estimates becomes the gradient estimate of our algorithm. Our estimator enjoys the reduced variance of the reparameterization gradient while remaining unbiased even for non-differentiable models. The experiments with our preliminary implementation confirm the benefit of reduced variance and unbiasedness.	[Lee, Wonyeol; Yu, Hangyeol; Yang, Hongseok] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Lee, W (corresponding author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.	wonyeol@kaist.ac.kr; yhk1344@kaist.ac.kr; hongseok.yang@kaist.ac.kr	Yang, Hongseok/AAC-4471-2020		Engineering Research Center Program through the National Research Foundation of Korea (NRF) - Korean Government MSIT [NRF-2018R1A5A1059921]; National Research Foundation of Korea (NRF) - Ministry of Science, ICT [2017M3C4A7068177]	Engineering Research Center Program through the National Research Foundation of Korea (NRF) - Korean Government MSIT; National Research Foundation of Korea (NRF) - Ministry of Science, ICT	We thank Hyunjik Kim, George Tucker, Frank Wood and anonymous reviewers for their helpful comments, and Shin Yoo and Seongmin Lee for allowing and helping us to use their cluster machines. This research was supported by the Engineering Research Center Program through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-2018R1A5A1059921), and also by Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT (2017M3C4A7068177).	Diaconis P., 2013, ADV MODERN STAT THEO, P102; FLANDERS H, 1973, AM MATH MON, V80, P615, DOI 10.2307/2319163; Goodman N. D., 2008, P 24 C UNC ART INT U; Gordon A. D., 2014, INT C SOFTW ENG ICS; Grathwohl W., 2018, P 6 INT C LEARN REPR; Gu S., 2016, P 4 INT C LEARN REPR; Gu S., 2017, P 5 INT C LEARN REPR; Gumbel E.J., 1954, STAT THEORY EXTREME, V33; Jang E., 2017, P 5 INT C LEARN REPR; Kingma D. P., 2016, P 30 INT C NEUR INF, P4743; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Kingma Diederik P., 2015, 3 INT C LEARN REPRES, V3; Knowles David A., 2015, STOCHASTIC GRADIENT; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Le T. A., 2018, P 6 INT C LEARN REPR; Li Y., 2016, P 30 INT C NEUR INF; Maclaurin D., 2016, THESIS HARVARD U; Maddison C. J., 2014, P 28 INT C NEUR INF; Maddison C.J., 2017, P 5 INT C LEARN REPR; Maddison C. J., 2017, P 31 INT C NEUR INF; Mansinghka V. K., 2014, VENTURE HIGHER ORDER; Miller Andrew C, 2017, ARXIV170507880; Naesseth C. A., 2017, P 20 INT C ART INT; Naesseth C. A., 2018, P 21 INT C ART INT S; Paisley J., 2012, P 29 INT C MACH LEAR; Ranganath Rajesh, 2014, P 17 INT C ART INT S; Rezende, 2016, P 33 INT C INT C MAC; Rezende D. J., 2014, P 31 INT C MACH LEAR; Rezende D. J., 2015, P 32 INT C INT C MAC; Ruiz F. J. R., 2016, P 30 INT C NEUR INF; Soudjani S. E. Z., 2017, P 14 INT C QUANT EV; Stoffer D. S, 2005, SPRINGER TEXTS STAT; Tucker G., 2017, P 31 INT C NEUR INF; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wingate D., 2013, CORR; Wood F, 2014, P 17 INT C ART INT S	37	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000009
C	Lindgren, EM; Kocaoglu, M; Dimakis, AG; Vishwanath, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lindgren, Erik M.; Kocaoglu, Murat; Dimakis, Alexandros G.; Vishwanath, Sriram			Experimental Design for Cost-Aware Learning of Causal Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORKS	We consider the minimum cost intervention design problem: Given the essential graph of a causal graph and a cost to intervene on a variable, identify the set of interventions with minimum total cost that can learn any causal graph with the given essential graph. We first show that this problem is NP-hard. We then prove that we can achieve a constant factor approximation to this problem with a greedy algorithm. We then constrain the sparsity of each intervention. We develop an algorithm that returns an intervention design that is nearly optimal in terms of size for sparse graphs with sparse interventions and we discuss how to use it when there are costs on the vertices.	[Lindgren, Erik M.; Dimakis, Alexandros G.; Vishwanath, Sriram] Univ Texas Austin, Austin, TX 78712 USA; [Kocaoglu, Murat] MIT IBM Watson AI Lab, Cambridge, MA USA	University of Texas System; University of Texas Austin	Lindgren, EM (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	erikml@utexas.edu; murat@ibm.com; dimakis@austin.utexas.edu; sriram@ece.utexas.edu	Dimakis, Alexandros G/P-6034-2019; Dimakis, Alexandros G/A-5496-2011	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033	National Science Foundation Graduate Research Fellowship [DGE-1110007]; NSF [CCF 1422549, 1618689, DMS 1723052, CCF 1763702]; ARO YIP [W911NF-14-1-0258]	National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); ARO YIP	This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1110007. This research has been supported by NSF Grants CCF 1422549, 1618689, DMS 1723052, CCF 1763702, ARO YIP W911NF-14-1-0258 and research gifts by Google, Western Digital, and NVIDIA.	CAI MC, 1984, DISCRETE MATH, V49, P15; Delle Donne D, 2016, DISCRETE OPTIM, V21, P1, DOI 10.1016/j.disopt.2016.05.001; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Eberhardt Frederich, 2005, UNCERTAINTY ARTIFICI; Frank Andras, 1975, BRIT COMB C; Garey M.R., 1979, COMPUTERS INTRACTABI; GEIGER D, 1994, UNCERTAINTY ARTIFICI; Ghassami A., 2018, INT C MACH LEARN; Gurobi Optimization LLC, 2018, GUROBI OPTIMIZER REF; Hauser A, 2012, J MACH LEARN RES, V13, P2409; Hauser Alain, 2012, EUR WORKSH PROB GRAP; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Hu Huining, 2014, NEURAL INFORM PROCES; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; IBARRA OH, 1975, J ACM, V22, P463, DOI 10.1145/321906.321909; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Jansen Klaus, 1997, IT C ALG COMPL; Katona G., 1966, J COMB THEORY, V1, P174; King RD, 2004, NATURE, V427, P247, DOI 10.1038/nature02236; Kocaoglu Murat, 2017, INT C MACH LEARN; Krause Andreas, 2014, SUBMODULAR FUNCTION, P5; Kroon Leo G, 1996, INT WORKSH GRAPH THE; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Ramsey Joseph, 2017, Int J Data Sci Anal, V3, P121, DOI 10.1007/s41060-016-0032-z; Rotmensch M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05778-z; Rubin DB, 2006, STAT SCI, V21, P206, DOI 10.1214/088342306000000259; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Sachs Karen, 2017, INT C RES COMP MOL B; Shanmugam Karthikeyan, 2015, NEURAL INFORM PROCES; Sverchkov Y, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005466; Williamson D. P., 2011, DESIGN APPROXIMATION, DOI DOI 10.1017/CBO9780511921735; WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435	35	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305031
C	Liu, Q; Wang, DL		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Qiang; Wang, Dilin			Stein Variational Gradient Descent as Moment Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set, whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing properties of SVGD with different kernels, shedding insight into optimal kernel choice. In particular, we show that SVGD with linear kernels yields exact estimation of means and variances on Gaussian distributions, while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein's identity or solving the Stein equation, which may motivate more efficient algorithms.	[Liu, Qiang; Wang, Dilin] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Liu, Q (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	lqiang@cs.utexas.edu; dilin@cs.utexas.edu						Anderes Ethan, 2002, ARXIV12055314; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Chen C., 2018, ARXIV180511659; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Feng YH, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Gorham J, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2017, PR MACH LEARN RES, V70; Kim T., 2018, ADV NEURAL INFORM PR; Koller D., 2009, PROBABILISTIC GRAPHI; Liu Q., 2016, ADV NEURAL INFORM PR, V29, P2378; Liu Q, 2016, PR MACH LEARN RES, V48; Liu Yang, 2017, C UNC ART INT UAI; Lu J., 2018, ARXIV180504035; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Ollivier Yann, 2014, OPTIMAL TRANSPORT TH, V413; Pu YC, 2017, ADV NEUR IN, V30; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607; Srebro N., 2010, ADV NEURAL INFORM PR, P2199; Stein C., 1972, PROC 6 BERKELEY S MA, VII, p583?602; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang D., 2016, ARXIV161101722	24	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003041
C	Mariet, Z; Sra, S; Jegelka, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mariet, Zelda; Sra, Suvrit; Jegelka, Stefanie			Exponentiated Strongly Rayleigh Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DETERMINANTS; CONVERGENCE; POLYNOMIALS	Strongly Rayleigh (SR) measures are discrete probability distributions over the subsets of a ground set. They enjoy strong negative dependence properties, as a result of which they assign higher probability to subsets of diverse elements. We introduce in this paper Exponentiated Strongly Rayleigh (ESR) measures, which sharpen (or smoothen) the negative dependence property of SR measures via a single parameter (the exponent) that can be intuitively understood as an inverse temperature. We develop efficient MCMC procedures for approximate sampling from ESRs, and obtain explicit mixing time bounds for two concrete instances: exponentiated versions of Determinantal Point Processes and Dual Volume Sampling. We illustrate some of the potential of ESRs, by applying them to a few machine learning problems; empirical results confirm that beyond their theoretical appeal, ESR-based models hold significant promise for these tasks.	[Mariet, Zelda; Sra, Suvrit; Jegelka, Stefanie] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mariet, Z (corresponding author), MIT, Cambridge, MA 02139 USA.	zelda@csail.mit.edu; suvrit@mit.edu; stefje@csail.mit.edu			NSF CAREER award [1553284]; NSF-BIGDATA award [1741341]; Defense Advanced Research Projects Agency [YFA17 N66001-17-1-4039]	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF-BIGDATA award; Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work is in part supported by NSF CAREER award 1553284, NSF-BIGDATA award 1741341, and by The Defense Advanced Research Projects Agency (grant number YFA17 N66001-17-1-4039). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.	Affandi R. H., 2013, P INT C ART INT STAT; Affandi RH, 2014, PR MACH LEARN RES, V32, P1224; Amer M., 2012, PROC 3 RAPIDMINER CO, P1; Anari N., 2016, JMLR WORKSHOP C P, V49, P103; Anari N, 2017, ACM S THEORY COMPUT, P384, DOI 10.1145/3055399.3055469; Andrieu C., 2003, INTRO MCMC MACHINE L; Angiulli F., 2002, Principles of Data Mining and Knowledge Discovery. 6th European Conference, PKDD 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2431), P15; Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287; Borcea J, 2008, DUKE MATH J, V143, P205, DOI 10.1215/00127094-2008-018; Borcea J, 2009, ANN MATH, V170, P465, DOI 10.4007/annals.2009.170.465; Borcea J, 2009, J AM MATH SOC, V22, P521; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675; Cai HY, 2000, STOCH ANAL APPL, V18, P63, DOI 10.1080/07362990008809654; Chao WL, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P191; Derezinski M, 2017, ADV NEUR IN, V30; Diaconis P., 1993, ANN APPL PROBAB, V3, P696, DOI [10.1214/aoap/1177005359, DOI 10.1214/AOAP/1177005359]; Diaconis Persi, 1991, ANN APPL PROBAB, V1, P36, DOI DOI 10.1214/aoap/1177005980; Djolonga J., 2016, NEURAL INFORM PROCES; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; Gartrell M, 2017, AAAI CONF ARTIF INTE, P1912; Gillenwater J., 2014, THESIS; Goldstein M., 2012, KI 2012 POSTER DEMO, DOI DOI 10.1007/978-3-642-21329-8_4; Goldstein M, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0152173; Gotovos A., 2015, NEURAL INFORM PROCES; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Krause A, 2008, J MACH LEARN RES, V9, P235; Kriegel H. P., 2009, P 18 ACM C INFORM KN, DOI DOI 10.1145/1645953.1646195; Kulesza A., 2012, DETERMINANTAL POINT, V5; Li C., 2016, NEURAL INFORM PROCES; Li C., 2016, P 33 INT C INT C MAC, V48, P2061; Li CT, 2016, PR MACH LEARN RES, V48; Li C, 2017, ADV NEUR IN, V30; Lin H., 2012, ARXIV PREPRINT ARXIV; Marcus AW, 2015, ANN MATH, V182, P327, DOI 10.4007/annals.2015.182.1.8; Mariet Z, 2017, ADV NEUR IN, V30; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; MicenkovuY B., 2014, P ACM SIGKDD 2014 WO, P51; Nystrom EJ, 1930, ACTA MATH-DJURSHOLM, V54, P185, DOI 10.1007/BF02547521; Pemantle R, 2000, J MATH PHYS, V41, P1371, DOI 10.1063/1.533200; Pemantle R, 2014, COMB PROBAB COMPUT, V23, P140, DOI 10.1017/S0963548313000345; Rebeschini Patrick, 2015, P MACHINE LEARNING R, V40, P1480; Shirai T, 2003, J FUNCT ANAL, V205, P414, DOI 10.1016/S0022-1236(03)00171-X; Specht W., 1960, MATH Z, V74, DOI [10.1007/BF01180475, DOI 10.1007/BF01180475]; Sra, 2016, P INT C LEARN REPR I; Street N., 1993, NUCL FEATURE EXTRACT; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zhou T, 2010, P NATL ACAD SCI USA, V107, P4511, DOI 10.1073/pnas.1000488107; Zou J. Y., 2012, NIPS	49	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304047
C	McCurdy, SR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		McCurdy, Shannon R.			Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SELECTION	Ridge leverage scores provide a balance between low-rank approximation and regularization, and are ubiquitous in randomized linear algebra and machine learning. Deterministic algorithms are also of interest in the moderately big data regime, because deterministic algorithms provide interpretability to the practitioner by having no failure probability and always returning the same results. We provide provable guarantees for deterministic column sampling using ridge leverage scores. The matrix sketch returned by our algorithm is a column subset of the original matrix, yielding additional interpretability. Like the randomized counterparts, the deterministic algorithm provides (1 + epsilon) error column subset selection, (1 + epsilon) error projection-cost preservation, and an additive-multiplicative spectral bound. We also show that under the assumption of power-law decay of ridge leverage scores, this deterministic algorithm is provably as accurate as randomized algorithms. Lastly, ridge regression is frequently used to regularize ill-posed linear least-squares problems. While ridge regression provides shrinkage for the regression coefficients, many of the coefficients remain small but non-zero. Performing ridge regression with the matrix sketch returned by our algorithm and a particular regularization parameter forces coefficients to zero and has a provable (1 + epsilon) bound on the statistical risk. As such, it is an interesting alternative to elastic net regularization.	[McCurdy, Shannon R.] Univ Calif Berkeley, Calif Inst Quantitat Biosci, Berkeley, CA 94702 USA	University of California System; University of California Berkeley	McCurdy, SR (corresponding author), Univ Calif Berkeley, Calif Inst Quantitat Biosci, Berkeley, CA 94702 USA.	smccurdy@berkeley.edu			National Human Genome Research Institute of the National Institutes of Health [F32HG008713]	National Human Genome Research Institute of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Human Genome Research Institute (NHGRI))	Research reported in this publication was supported by the National Human Genome Research Institute of the National Institutes of Health under Award Number F32HG008713. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. SRM thanks Michael Mahoney, Ahmed El Alaoui, Elaine Angelino, and Kai Rothauge for thoughtful comments and the Barcellos and Pachter Labs.	Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Brat DJ, 2015, NEW ENGL J MED, V372, P2481, DOI 10.1056/NEJMoa1402121; Breiman L, 1996, ANN STAT, V24, P2350; Broad Institute of MIT and Harvard, 2016, BROAD I TCGA GEN DAT, DOI DOI 10.7908/C11G0KM9DATASET; Chatterjee S., 1986, STAT SCI, V1, P379, DOI [10.1214/ss/1177013622, DOI 10.1214/SS/1177013622]; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Drineas, 2009, ADV NEURAL INFORM PR, P153, DOI DOI 10.1016/J.FUTURE.2017.11.043; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Horn R.A., 2013, MATRIX ANAL, Vsecond; McCurdy  Shannon, 2017, BIORXIV, DOI [10.1101/159079, DOI 10.1101/159079]; Mezzadri  Francesco, 2006, NOTICES AM MATH SOC, V54; Papailiopoulos D, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P997, DOI 10.1145/2623330.2623698; Rudi  Alessandro, 2015, ARXIV150704717CSSTAT; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; VELLEMAN PF, 1981, AM STAT, V35, P234, DOI 10.2307/2683296; Wan YW, 2016, BIOINFORMATICS, V32, P952, DOI 10.1093/bioinformatics/btv677; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zou H, 2009, ANN STAT, V37, P1733, DOI 10.1214/08-AOS625	21	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302047
C	Mehri, S; Sigal, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mehri, Shikib; Sigal, Leonid			Middle-Out Decoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Despite being virtually ubiquitous, sequence-to-sequence models are challenged by their lack of diversity and inability to be externally controlled. In this paper, we speculate that a fundamental shortcoming of sequence generation models is that the decoding is done strictly from left-to-right, meaning that outputs values generated earlier have a profound effect on those generated later. To address this issue, we propose a novel middle-out decoder architecture that begins from an initial middle-word and simultaneously expands the sequence in both directions. To facilitate information flow and maintain consistent decoding, we introduce a dual self-attention mechanism that allows us to model complex dependencies between the outputs. We illustrate the performance of our model on the task of video captioning, as well as a synthetic sequence de-noising task. Our middle-out decoder achieves significant improvements on de-noising and competitive performance in the task of video captioning, while quantifiably improving the caption diversity. Furthermore, we perform a qualitative analysis that demonstrates our ability to effectively control the generation process of our decoder.	[Mehri, Shikib; Sigal, Leonid] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada	University of British Columbia	Mehri, S (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.	amehri@cs.cmu.edu; lsigal@cs.ubc.ca						Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen David, 2011, P 49 ANN M ASS COMP, P190; Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI [10.1109/CIS.2016.133, 10.1109/CIS.2016.0134]; Chen X, 2015, CORR, V1504, P325; Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352; Daniluk M., 2017, P ICLR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Devlin J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P100; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Heuer H., 2016, WORKSH STOR IM VID; Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094; Hokamp C, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1535, DOI 10.18653/v1/P17-1141; Huang Liang, 2017, P 2017 C EMP METH NA, P2134, DOI DOI 10.18653/V1/D17-1227; Kingma D.P, P 3 INT C LEARNING R; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Liu Y., 2017, ARXIV170509207; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9; Mao J, 2015, P ICLR; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117; Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117; Polosukhin I., 2018, ARXIV180204335; Post Matt, 2018, P NAACL HLT, P1314; Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Venugopalan S., 2015, P C N AM CHAPT ASS C, P1494, DOI 10.3115/v1/N15-1173; Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang Z., 2016, P 25 INT JOINT C ART, P2957; Werlen L. M., 2018, ANN C N AM CHAPT ASS; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512; Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496; Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347; Zhu YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P1097, DOI 10.1145/3209978.3210080	46	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000006
C	Mukherjee, SS; Sarkar, P; Wang, YXR; Yan, BW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mukherjee, Soumendu Sunder; Sarkar, Purnamrita; Wang, Y. X. Rachel; Yan, Bowei			Mean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Variational approximation has been widely used in large-scale Bayesian inference recently, the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field, theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper, we focus on the problem of community detection for a simple twoclass Stochastic Blockmodel (SBM). Using batch co-ordinate ascent (BCAVI) for updates, we show different convergence behavior with respect to different initializations. When the parameters are known, we show that a random initialization can converge to the ground truth, whereas in the case when the parameters themselves need to be estimated, a random initialization will converge to an uninformative local optimum.	[Mukherjee, Soumendu Sunder] Indian Stat Inst Kolkata, ISRU, Kolkata 700108, India; [Sarkar, Purnamrita; Yan, Bowei] Univ Texas Austin, Dept Stat & Data Sci, Austin, TX 78712 USA; [Wang, Y. X. Rachel] Univ Sydney, Sch Math & Stat, Sydney, NSW 2006, Australia	Indian Statistical Institute; Indian Statistical Institute Kolkata; University of Texas System; University of Texas Austin; University of Sydney	Mukherjee, SS (corresponding author), Indian Stat Inst Kolkata, ISRU, Kolkata 700108, India.	soumendu041@gmail.com; purna.sarkar@austin.utexas.edu; rachel.wang@sydney.edu.au; boweiy@utexas.edu		Wang, Y. X. Rachel/0000-0003-0594-680X	NSF [DMS1713082]; ARC DECRA Fellowship	NSF(National Science Foundation (NSF)); ARC DECRA Fellowship(Australian Research Council)	SSM thanks Professor Peter J. Bickel for helpful discussions. PS is partially funded by NSF grant DMS1713082. YXRW is supported by the ARC DECRA Fellowship.	[Anonymous], 2018, APPENDIX MEAN FIELD; Awasthi Pranjal, 2015, ADV NEURAL INFORM PR, P2098; Bickel P, 2013, ANN STAT, V41, P1922, DOI 10.1214/13-AOS1124; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ghorbani Behrooz, 2018, ARXIV180200568; Giordano R., 2017, ARXIV170902536; Hofman JM, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.258701; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; JAAKKOLA TS, 1999, LEARNING GRAPHICAL M, P163; Jin Chi, 2016, ADV NEURAL INFORM PR, P4116; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Mei S., 2016, LANDSCAPE EMPIRICAL; Ng AY, 2002, ADV NEUR IN, V14, P849; Pati Debdeep, 2017, ARXIV171208983; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang B., 2005, AISTATS; Wang B., 2004, P 20 C UNC ART INT B, p577 584; Wang B, 2006, BAYESIAN ANAL, V1, P625, DOI 10.1214/06-BA121; Wang Y., 2017, ARXIV170503439; Westling Ted, 2015, ARXIV151008151; Xu Ji, 2016, ADV NEURAL INFORM PR, P2676; Zhang Anderson Y., 2017, ARXIV171011268; [No title captured]	26	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005029
C	Ng, YC; Colombo, N; Silva, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ng, Yin Cheng; Colombo, Nicolo; Silva, Ricardo			Bayesian Semi-supervised Learning with Graph Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGULARIZATION	We propose a data-efficient Gaussian process-based Bayesian approach to the semi-supervised learning problem on graphs. The proposed model shows extremely competitive performance when compared to the state-of-the-art graph neural networks on semi-supervised learning benchmark experiments, and outperforms the neural networks in active learning experiments where labels are scarce. Furthermore, the model does not require a validation data set for early stopping to control over-fitting. Our model can be viewed as an instance of empirical distribution regression weighted locally by network connectivity. We further motivate the intuitive construction of the model with a Bayesian linear model interpretation where the node features are filtered by an operator related to the graph Laplacian. The method can be easily implemented by adapting off-the-shelf scalable variational inference algorithms for Gaussian processes.	[Ng, Yin Cheng; Colombo, Nicolo; Silva, Ricardo] UCL, Stat Sci, London, England; [Silva, Ricardo] Alan Turing Inst, London, England	University of London; University College London	Ng, YC (corresponding author), UCL, Stat Sci, London, England.	y.ng.12@ucl.ac.uk; nicolo.colombo@ucl.ac.uk; ricardo.silva@ucl.ac.uk	Ng, Wing Yin/HHN-1743-2022		Alan Turing Institute under the EPSRC [EP/N510129/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.	Atwood J, 2016, C WORKSH NEUR INF PR, P1993; Balcan MF, 2010, MACH LEARN, V80, P111, DOI 10.1007/s10994-010-5174-y; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Chu Wei, 2007, P ADV NEUR INF PROC, P289; Chu Wei, 2008, P ADV NEUR INF PROC, P1345; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Dasarathy G., 2015, P C LEARN THEOR, P503; de Garis Matthews A. G., 2016, THESIS; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Duvenaud D., 2014, AUTOMATIC MODEL CONS; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; Fan R. K. C., 1997, SPECTRAL GRAPH THEOR; Girolami M, 2006, NEURAL COMPUT, V18, P1790, DOI 10.1162/neco.2006.18.8.1790; Goldenberg A, 2010, FOUND TRENDS MACH LE, V2, P129, DOI 10.1561/2200000005; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR, P280; Jun KS, 2016, IEEE GLOB CONF SIG, P1325, DOI 10.1109/GlobalSIP.2016.7906056; Kim HC, 2006, IEEE T PATTERN ANAL, V28, P1948, DOI 10.1109/TPAMI.2006.238; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Lu Q., 2003, P 20 INT C MACH LEAR, P496, DOI DOI 10.5555/3041838.3041901; Ma Y., 2013, ADV NEURAL INFORM PR, P2751; Mac Aodha O., 2014, CVPR; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Salimbeni H, 2017, ADV NEUR IN, V30; Sandryhaila A, 2014, IEEE SIGNAL PROC MAG, V31, P80, DOI 10.1109/MSP.2014.2329213; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Szabo Z, 2016, J MACH LEARN RES, V17; van der Wilk M, 2017, ADV NEUR IN, V30; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Williams Christopher KI, 2006, GAUSSIAN PROCESSES M, V2, P4, DOI DOI 10.1142/50129065704001899; Yang Z, 2016, PR MACH LEARN RES, V48; Yu Byron M, 2009, ADV NEURAL INFORM PR, P1881, DOI DOI 10.1152/JN.90941; Zhu X., 2003, TECHNICAL REPORT; Zhu X.J., 2005, COMPUT SCI; Zhu Xiaojin., 2003, P ICLR, P912	49	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301065
C	Nimishakavi, M; Jawanpuria, P; Mishra, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nimishakavi, Madhav; Jawanpuria, Pratik; Mishra, Bamdev			A dual framework for low-rank tensor completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION; ALGORITHMS; NETWORKS	One of the popular approaches for low-rank tensor completion is to use the latent trace norm regularization. However, most existing works in this direction learn a sparse combination of tensors. In this work, we fill this gap by proposing a variant of the latent trace norm that helps in learning a non-sparse combination of tensors. We develop a dual framework for solving the low-rank tensor completion problem. We first show a novel characterization of the dual solution space with an interesting factorization of the optimal solution. Overall, the optimal solution is shown to lie on a Cartesian product of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian optimization framework for proposing computationally efficient trust region algorithm. The experiments illustrate the efficacy of the proposed algorithm on several real-world datasets across applications.	[Nimishakavi, Madhav] Indian Inst Sci, Bengaluru, Karnataka, India; [Jawanpuria, Pratik; Mishra, Bamdev] Microsoft, Hyderabad, Telangana, India	Indian Institute of Science (IISC) - Bangalore	Nimishakavi, M (corresponding author), Indian Inst Sci, Bengaluru, Karnataka, India.	madhav@iisc.ac.in; pratik.jawanpuria@microsoft.com; bamdevm@microsoft.com		Mishra, Bamdev/0000-0001-7430-2843				Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Acar E, 2011, CHEMOMETR INTELL LAB, V106, P41, DOI 10.1016/j.chemolab.2010.08.004; [Anonymous], 2017, ARXIV170205594; Argyriou A., 2006, NIPS; Balzano L., 2010, 48 ANN ALL C COMM CO; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bordes A., 2013, ADV NEURAL INF PROCE, V26, P1; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Caiafa CF, 2015, IEEE T SIGNAL PROCES, V63, P780, DOI 10.1109/TSP.2014.2385040; Cheng H., 2016, AISTATS; Cichocki A., 2017, FDN TRENDS MACHINE L, V9, P431; Cichocki A, 2016, FOUND TRENDS MACH LE, V9, P431, DOI [10.1561/2200000067, 10.1561/2200000059]; Cortes C., 2009, UAI; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Ermis B., 2015, KDD; Filipovic M., 2015, MULTIDIMENSIONAL SYS; Foster DH, 2004, VISUAL NEUROSCI, V21, P331, DOI 10.1017/S0952523804213335; Guo X., 2017, AAAI; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Jawanpuria P, 2015, NIPS; Jawanpuria P., 2011, SDM; Jawanpuria P., 2014, ICML; Jawanpuria P., 2018, ICML; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Kasai  H., 2016, ICML; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kressner D, 2014, BIT, V54, P447, DOI 10.1007/s10543-013-0455-z; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Liu X.-Y., 2016, SPIE C DEF SEC; Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027; Meghawanshi M., 2018, NEURIPS WORKSH MACH; Mishra B., 2019, MACHINE LEARNING; Nimishakavi M., 2017, TECHNICAL REPORT; Pong TK, 2010, SIAM J OPTIMIZ, V20, P3465, DOI 10.1137/090763184; Romera-Paredes B., 2013, ICML; Sato H, 2015, OPTIMIZATION, V64, P1011, DOI 10.1080/02331934.2013.836650; Signoretto M, 2014, MACH LEARN, V94, P303, DOI 10.1007/s10994-013-5366-3; Suzuki T., 2011, NIPS; Symeonidis P., 2008, RECSYS; Tang L, 2009, ICDM; Tomioka R., 2013, NIPS; Tomioka R., 2010, TECHNICAL REPORT; Tomioka Ryota, 2011, NIPS; Toutanova K., 2015, EMNLP; Wimalawarne K., 2014, NIPS; Xin Y., 2012, AISTATS; ZHANG H., 2016, P ADV NEURAL INFORM, P4599; Zhang Z. M., 2014, CVPR; Zhao QB, 2015, IEEE T PATTERN ANAL, V37, P1751, DOI 10.1109/TPAMI.2015.2392756	49	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000003
C	Orseau, L; Lelis, LHS; Lattimore, T; Weber, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Orseau, Laurent; Lelis, Levi H. S.; Lattimore, Tor; Weber, Theophane			Single-Agent Policy Tree Search With Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce two novel tree search algorithms that use a policy to guide search. The first algorithm is a best-first enumeration that uses a cost function that allows us to prove an upper bound on the number of nodes to be expanded before reaching a goal state. We show that this best-first algorithm is particularly well suited for "needle-in-a-haystack" problems. The second algorithm is based on sampling and we prove an upper bound on the expected number of nodes it expands before reaching a set of goal states. We show that this algorithm is better suited for problems where many paths lead to a goal. We validate these tree search algorithms on 1,000 computer-generated levels of Sokoban, where the policy used to guide the search comes from a neural network trained using A3C. Our results show that the policy tree search algorithms we introduce are competitive with a state-of-the-art domain-independent planner that uses heuristic search.	[Orseau, Laurent; Lattimore, Tor; Weber, Theophane] DeepMind, London, England; [Lelis, Levi H. S.] Univ Fed Vicosa, Vicosa, MG, Brazil; [Lelis, Levi H. S.] Univ Alberta, Edmonton, AB, Canada	Universidade Federal de Vicosa; University of Alberta	Orseau, L (corresponding author), DeepMind, London, England.	lorseau@google.com; levi.lelis@ufv.br; lattimore@google.com; theophane@google.com			Natural Sciences and Engineering Research Council of Canada (NSERC)	Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	The authors wish to thank Peter Sunehag, Andras Gyorgy, Remi Munos, Joel Veness, Arthur Guez, Marc Lanctot, Andre Grahl Pereira, and Michael Bowling for helpful discussions pertaining this research. Financial support for this research was in part provided by the Natural Sciences and Engineering Research Council of Canada (NSERC).	Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Culberson J. C., 1999, P INFORM, P65; DORAN JE, 1966, PROC R SOC LON SER-A, V294, P235, DOI 10.1098/rspa.1966.0205; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Helmert M, 2006, J ARTIF INTELL RES, V26, P191, DOI 10.1613/jair.1705; Hoffmann J, 2001, J ARTIF INTELL RES, V14, P253, DOI 10.1613/jair.855; KORF RE, 1985, ARTIF INTELL, V27, P97, DOI 10.1016/0004-3702(85)90084-0; Levin L. A., 1973, Problems of Information Transmission, V9, P265; LUBY M, 1993, INFORM PROCESS LETT, V47, P173, DOI 10.1016/0020-0190(93)90029-9; Mnih V, 2016, PR MACH LEARN RES, V48; Nakhost H., 2013, THESIS; Orseau L., 2017, INT C ALGORITHMIC LE, P372; REINEFELD A, 1994, IEEE T PATTERN ANAL, V16, P701, DOI 10.1109/34.297950; Richter S, 2010, J ARTIF INTELL RES, V39, P127, DOI 10.1613/jair.2972; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Silver David, 2017, CORR; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; TRAKHTENBROT BA, 1984, ANN HIST COMPUT, V6, P384; Xie F., 2012, P 22 INT C AUT PLANN, P315; [No title captured]	22	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303022
C	Qi, L; Liu, S; Shi, JP; Jia, JY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Qi, Lu; Liu, Shu; Shi, Jianping; Jia, Jiaya			Sequential Context Encoding for Duplicate Removal	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Duplicate removal is a critical step to accomplish a reasonable amount of predictions in prevalent proposal-based object detection frameworks. Albeit simple and effective, most previous algorithms utilize a greedy process without making sufficient use of properties of input data. In this work, we design a new two-stage framework to effectively select the appropriate proposal candidate for each object. The first stage suppresses most of easy negative object proposals, while the second stage selects true positives in the reduced proposal set. These two stages share the same network structure, i.e., an encoder and a decoder formed as recurrent neural networks (RNN) with global attention and context gate. The encoder scans proposal candidates in a sequential manner to capture the global context information, which is then fed to the decoder to extract optimal proposals. In our extensive experiments, the proposed method outperforms other alternatives by a large margin.	[Qi, Lu; Liu, Shu; Jia, Jiaya] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Shi, Jianping] SenseTime Res, Hong Kong, Peoples R China; [Liu, Shu; Jia, Jiaya] Tencent, YouTu Lab, Shenzhen, Peoples R China	Chinese University of Hong Kong; Tencent	Qi, L (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	luqi@cse.cuhk.edu.hk; sliu@cse.cuhk.edu.hk; shijianping@sensetime.com; leojia@cse.cuhk.edu.hk	Jia, Jiaya/I-3251-2012					Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365; Bodla N., 2017, CVPR; Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440; Chorowski J., 2014, P NIPS WORKSH DEEP L; Chung J., 2016, ARXIV14123555; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Desai C, 2011, INT J COMPUT VISION, V95, P1, DOI 10.1007/s11263-011-0439-x; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gidaris S., 2015, CVPR; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Goel K., 2014, ANN; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; He K., 2014, ECCV; Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685; Hu H., 2017, CVPR; Huang S, 2016, IEEE IPCCC; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu SF, 2015, PROC CVPR IEEE, P3451, DOI 10.1109/CVPR.2015.7298967; Luong M., 2015, ARXIV150804025; Mnih V, 2014, ADV NEUR IN, V27; Pedro F. F., 2010, PAMI, V32, P1627, DOI [10.1109/TPAMI.2009.167, DOI 10.1109/TPAMI.2009.167]; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Tu Z., 2016, ARXIV160806043; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2	34	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302009
C	Rivasplata, O; Parrado-Hernandez, E; Shawe-Taylor, J; Sun, SL; Szepesvari, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rivasplata, Omar; Parrado-Hernandez, Emilio; Shawe-Taylor, John; Sun, Shiliang; Szepesvari, Csaba			PAC-Bayes bounds for stable algorithms with instance-dependent priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					PAC-Bayes bounds have been proposed to get risk estimates based on a training sample. In this paper the PAC-Bayes approach is combined with stability of the hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting is used with a Gaussian prior centered at the expected output. Thus a novelty of our paper is using priors defined in terms of the data-generating distribution. Our main result estimates the risk of the randomized algorithm in terms of the hypothesis stability coefficients. We also provide a new bound for the SVM classifier, which is compared to other known bounds experimentally. Ours appears to be the first uniform hypothesis stability-based bound that evaluates to non-trivial values.	[Rivasplata, Omar; Shawe-Taylor, John] UCL, London, England; [Parrado-Hernandez, Emilio] Univ Carlos III Madrid, Madrid, Spain; [Sun, Shiliang] East China Normal Univ, Shanghai, Peoples R China; [Szepesvari, Csaba] DeepMind, London, England	University of London; University College London; Universidad Carlos III de Madrid; East China Normal University	Rivasplata, O (corresponding author), UCL, London, England.		PARRADO-HERNANDEZ, EMILIO/ABH-2027-2020	PARRADO-HERNANDEZ, EMILIO/0000-0003-2146-2135; Shawe-Taylor, John/0000-0002-2030-0073	DeepMind; Alberta Innovates - Technology Futures; Natural Sciences and Engineering Research Council of Canada; NSFC [61673179]; Shanghai Knowledge Service Platform Project [ZF1213]; UK Defence Science and Technology Laboratory (Dstl); Engineering and Physical Sciences Research Council (EPSRC) [EP/R018693/1]	DeepMind; Alberta Innovates - Technology Futures; Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); NSFC(National Natural Science Foundation of China (NSFC)); Shanghai Knowledge Service Platform Project; UK Defence Science and Technology Laboratory (Dstl); Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Omar Rivasplata is sponsored by DeepMind via an Overseas Impact Studentship to undertake grad studies at UCL Department of Computer Science. Csaba Szepesvari gratefully acknowledges the Alberta machine intelligence institute (Amii), with funding from Alberta Innovates - Technology Futures and from the Natural Sciences and Engineering Research Council of Canada. Shiliang Sun is supported by the NSFC Project 61673179, and Shanghai Knowledge Service Platform Project ZF1213. John Shawe-Taylor acknowledges support of the UK Defence Science and Technology Laboratory (Dstl) and Engineering and Physical Sciences Research Council (EPSRC) under grant EP/R018693/1. This is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative (MURI). This work was done in part while John Shawe-Taylor was visiting the Simons Institute for the Theory of Computing at UC Berkeley.	Abou-Moustafa  Karim, 2017, PRIORI EXPONENTIAL T; Ben London, 2017, ADV NEURAL INFORM PR, P2931; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Catoni  Olivier, 2007, TECHNICAL REPORT; Celisse  Alain, 2016, ARXIV160806412; Chapelle O., 2005, P AISTATS, P1; Dziugaite G. K., 2017, UAI; Dziugaite Gintare Karolina, 2018, ARXIV180209583; Freund Y., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P247, DOI 10.1145/279943.279993; Germain P, 2015, J MACH LEARN RES, V16, P787; Germain Pascal, 2009, INT C MACH LEARN; Kontorovich A., 2014, P MACHINE LEARNING R, V32, P28; Langford J, 2005, J MACH LEARN RES, V6, P273; Langford J, 2003, MACH LEARN, V51, P165, DOI 10.1023/A:1022806918936; Lever G, 2010, LECT NOTES ARTIF INT, V6331, P119, DOI 10.1007/978-3-642-16108-7_13; Liu  Tongliang, 2017, P INT C MACH LEARN, P2159; London B., 2013, INT C MACH LEARN, P828; McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Parrado-Hernandez E, 2012, J MACH LEARN RES, V13, P3507; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386; Shawe-Taylor J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P2, DOI 10.1145/267460.267466; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Steinwart I., 2008, SUPPORT VECTOR MACHI	27	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003074
C	Sinz, FH; Ecker, AS; Fahey, PG; Walker, EY; Cobos, E; Froudarakis, E; Yatsenko, D; Pitkow, X; Reimer, J; Tolias, AS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sinz, Fabian H.; Ecker, Alexander S.; Fahey, Paul G.; Walker, Edgar Y.; Cobos, Erick; Froudarakis, Emmanouil; Yatsenko, Dimitri; Pitkow, Xaq; Reimer, Jacob; Tolias, Andreas S.			Stimulus domain transfer in recurrent models for large scale cortical population prediction on video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURONAL RESPONSES; STATE; NETWORK; CORTEX	To better understand the representations in visual cortex, we need to generate better predictions of neural activity in awake animals presented with their ecological input: natural video. Despite recent advances in models for static images, models for predicting responses to natural video are scarce and standard linear-nonlinear models perform poorly. We developed a new deep recurrent network architecture that predicts inferred spiking activity of thousands of mouse V1 neurons simultaneously recorded with two-photon microscopy, while accounting for confounding factors such as the animal's gaze position and brain state changes related to running state and pupil dilation. Powerful system identification models provide an opportunity to gain insight into cortical functions through in silico experiments that can subsequently be tested in the brain. However, in many cases this approach requires that the model is able to generalize to stimulus statistics that it was not trained on, such as band-limited noise and other parameterized stimuli. We investigated these domain transfer properties in our model and find that our model trained on natural images is able to correctly predict the orientation tuning of neurons in responses to artificial noise stimuli. Finally, we show that we can fully generalize from movies to noise and maintain high predictive performance on both stimulus domains by fine-tuning only the final layer's weights on a network otherwise trained on natural movies. The converse, however, is not true.	[Sinz, Fabian H.; Fahey, Paul G.; Walker, Edgar Y.; Cobos, Erick; Froudarakis, Emmanouil; Yatsenko, Dimitri; Pitkow, Xaq; Reimer, Jacob; Tolias, Andreas S.] Baylor Coll Med, Dept Neurosci, Houston, TX 77030 USA; [Sinz, Fabian H.; Ecker, Alexander S.; Fahey, Paul G.; Walker, Edgar Y.; Cobos, Erick; Froudarakis, Emmanouil; Yatsenko, Dimitri; Pitkow, Xaq; Reimer, Jacob; Tolias, Andreas S.] Baylor Coll Med, Ctr Neurosci & Artificial Intelligence, Houston, TX 77030 USA; [Pitkow, Xaq; Tolias, Andreas S.] Rice Univ, Dept Elect & Comp Engn, POB 1892, Houston, TX 77251 USA; [Ecker, Alexander S.] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany; [Sinz, Fabian H.; Ecker, Alexander S.; Tolias, Andreas S.] Univ Tubingen, Bernstein Ctr Computat Neurosci, Tubingen, Germany; [Ecker, Alexander S.] Univ Tubingen, Inst Theoret Phys, Tubingen, Germany; [Sinz, Fabian H.] Univ Tubingen, Inst Comp Sci, Tubingen, Germany	Baylor College of Medicine; Baylor College of Medicine; Rice University; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen	Sinz, FH (corresponding author), Baylor Coll Med, Dept Neurosci, Houston, TX 77030 USA.; Sinz, FH (corresponding author), Baylor Coll Med, Ctr Neurosci & Artificial Intelligence, Houston, TX 77030 USA.; Sinz, FH (corresponding author), Univ Tubingen, Bernstein Ctr Computat Neurosci, Tubingen, Germany.; Sinz, FH (corresponding author), Univ Tubingen, Inst Comp Sci, Tubingen, Germany.	sinz@bcm.edu	Ecker, Alexander S/A-5184-2010; Froudarakis, Emmanouil/AAK-9093-2021; Sinz, Fabian/E-6708-2010	Ecker, Alexander S/0000-0003-2392-5105; Froudarakis, Emmanouil/0000-0002-3249-3845; Sinz, Fabian/0000-0002-1348-9736; Acenko, Dmitrii/0000-0001-6702-569X; Yatsenko, Dimitri/0000-0002-1844-641X; Walker, Edgar/0000-0003-0057-957X	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]; Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft) [ZUK 63]; Carl-Zeiss-Stiftung; NSF NeuroNex grant [1707400]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC); Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft); Carl-Zeiss-Stiftung; NSF NeuroNex grant	The authors would like to thank David Klindt and Zhe Li for comments on the manuscript. Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. Fabian Sinz is supported by the Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, ZUK 63) and the Carl-Zeiss-Stiftung. This work was supported in part by NSF NeuroNex grant 1707400. The authors thank Vathes LLC (https://vathes. com/) for hosting the database to reproduce results of this work.	Antolik J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004927; Ba J., 2017, P 3 INT C LEARN REPR; Batty E., 2016, MULTILAYER NETWORK M; Cadena Santiago A., 2017, BIORXIV, DOI [10.1101/201764, DOI 10.1101/201764]; Cho K, 2014, TECHNICAL REPORT; David SV, 2005, NETWORK-COMP NEURAL, V16, P239, DOI 10.1080/09548980500464030; David SV, 2004, J NEUROSCI, V24, P6991, DOI 10.1523/JNEUROSCI.1422-04.2004; Djork-Arn, ICLR 2016; Ecker AS, 2014, NEURON, V82, P235, DOI 10.1016/j.neuron.2014.02.006; Fournier J, 2011, NAT NEUROSCI, V14, P1053, DOI 10.1038/nn.2861; Fu Y, 2014, CELL, V156, P1139, DOI 10.1016/j.cell.2014.01.050; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55; Ioffe S., 2015, TECHNICAL REPORT; Karpathy A., 2014, CVPR; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kindel William F., 2017, ARXIV170606208CSQBIO; Klindt DA, 2017, ADV NEUR IN, V30; Kluyver T, 2016, POSITIONING AND POWER IN ACADEMIC PUBLISHING: PLAYERS, AGENTS AND AGENDAS, P87, DOI 10.3233/978-1-61499-649-1-87; Lau B, 2002, P NATL ACAD SCI USA, V99, P8974, DOI 10.1073/pnas.122173799; LEHKY SR, 1992, J NEUROSCI, V12, P3568; McGinley MJ, 2015, NEURON, V87, P1143, DOI 10.1016/j.neuron.2015.09.012; Merkel D, 2014, LINUX J; Niell CM, 2010, NEURON, V65, P472, DOI 10.1016/j.neuron.2010.01.033; Nishimoto S, 2011, J NEUROSCI, V31, P14551, DOI 10.1523/JNEUROSCI.6801-10.2011; Pachitariu M., 2012, ADV NEURAL INFORM PR, P1322; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037; Poulet JFA, 2008, NATURE, V454, P881, DOI 10.1038/nature07150; Prenger R, 2004, NEURAL NETWORKS, V17, P663, DOI 10.1016/j.neunet.2004.03.008; Reimer J, 2014, NEURON, V84, P355, DOI 10.1016/j.neuron.2014.09.033; Sofroniew NJ, 2016, ELIFE, V5, DOI 10.7554/eLife.14472; Stahl J. S, COMPARISON VIDEO MAG, V99, P101, DOI [10.1016/S0165-0270(00)00218-1, DOI 10.1016/S0165-0270(00)00218-1]; Stringer C., 2018, TECHNICAL REPORT; Sussillo D., 2016, TECHNICAL REPORT; van Alphen Bart, 3 DIMENSIONAL OPTOKI, V51, P623, DOI [10.1167/iovs.09-4072, DOI 10.1167/IOVS.09-4072]; van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37; Vintch B, 2015, J NEUROSCI, V35, P14829, DOI 10.1523/JNEUROSCI.2815-13.2015; Waskom M., 2017, MWASK SEAB V0 8 1, DOI [10.5281/zenodo.883859, DOI 10.5281/ZENODO.883859]; Yatsenko D., 2015, TECHNICAL REPORT; Zhang Yimeng, 2018, BIORXIV; ZIPSER D, 1988, NATURE, V331, P679, DOI 10.1038/331679a0; Zoccolan D, 2010, FRONT NEUROSCI-SWITZ, V4, DOI 10.3389/fnins.2010.00193	43	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001072
C	Tutunov, R; Kim, D; Bou-Ammar, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tutunov, Rasul; Kim, Dongho; Bou-Ammar, Haitham			Distributed Multitask Reinforcement Learning with Quadratic Convergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multitask reinforcement learning (MTRL) suffers from scalability issues when the number of tasks or trajectories grows large. The main reason behind this drawback is the reliance on centeralised solutions. Recent methods exploited the connection between MTRL and general consensus to propose scalable solutions. These methods, however, suffer from two drawbacks. First, they rely on predefined objectives, and, second, exhibit linear convergence guarantees. In this paper, we improve over state-of-the-art by deriving multitask reinforcement learning from a variational inference perspective. We then propose a novel distributed solver for MTRL with quadratic convergence guarantees.	[Tutunov, Rasul; Kim, Dongho; Bou-Ammar, Haitham] PROWLER Io, Cambridge, England		Tutunov, R (corresponding author), PROWLER Io, Cambridge, England.	rasul@prowler.io; dongho@prowler.io; haitham@prowler.io						Abdolmaleki Abbas, 2018, ICLR, P1; [Anonymous], INT C MACH LEARN ICM; [Anonymous], 2020, REINFORCEMENT LEARNI; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Bou-Ammar Haitham, 2014, P 31 INT C MACH LEAR; Boyd S, 2004, CONVEX OPTIMIZATION; Chavarnakul Thira, 2009, NEUROCOMPUT, V72; Deisenroth MP, 2014, IEEE INT CONF ROBOT, P3876, DOI 10.1109/ICRA.2014.6907421; ElBsat Bou-Ammar Haitham, 2017, AAAI; Forero PA, 2010, J MACH LEARN RES, V11, P1663; Goffin J. L., 1977, MATH PROGRAMMING, V13; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Kober J, 2011, MACHINE LEARNING, V84; Kober J., 2009, PROC NEURAL INF PROC, P849; Lazaric A., 2011, REINFORCEMENT LEARNI; Lazaric A., 2013, ADV NEURAL INFORM PR, V26; Levine Sergey, 2013, P 26 INT C NEUR INF, P207; Li H, 2009, J MACH LEARN RES, V10, P1131; Mokhtari A., 150406017 ARXIV; Mokhtari A., 150406020 ARXIV; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Neumann G, 2011, P 28 INT C MACH LEAR, P817; Olshevsky A., 2014, ARXIV E PRINTS; Ruvolo P., 2013, P ICML; Schulman J., 2015, ICML; Snel M, 2014, AUTON AGENT MULTI-AG, V28, P637, DOI 10.1007/s10458-013-9235-z; Spielman Daniel A., 2006, CORR; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Thrun Sebastian, 1996, SEMINAR DIGEST; Toussaint M., 2009, INT C MACH LEARN ICM, P1049; Tutunov R., 2015, ARXIV E PRINTS; Wei E, 2012, IEEE DECIS CONTR P, P5445, DOI 10.1109/CDC.2012.6425904; Zargham M., 2014, IEEE T AUTOMATIC CON	34	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003046
C	Wang, DL; Liu, H; Liu, Q		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Dilin; Liu, Hao; Liu, Qiang			Variational Inference with Tail-adaptive f-Divergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFORMATION	Variational inference with alpha-divergences has been widely used in modern probabilistic machine learning. Compared to Kullback-Leibler (KL) divergence, a major advantage of using alpha-divergences (with positive alpha values) is their mass-covering property. However, estimating and optimizing alpha-divergences require to use importance sampling, which may have large or infinite variance due to heavy tails of importance weights. In this paper, we propose a new class of tail-adaptive f-divergences that adaptively change the convex function f with the tail distribution of the importance weights, in a way that theoretically guarantees finite moments, while simultaneously achieving mass-covering properties. We test our method on Bayesian neural networks, and apply it to improve a recent soft actor-critic (SAC) algorithm (Haarnoja et al., 2018) in deep reinforcement learning. Our results show that our approach yields significant advantages compared with existing methods based on classical KL and alpha-divergences.	[Wang, Dilin; Liu, Hao; Liu, Qiang] UT Austin, Austin, TX 78712 USA; [Liu, Hao] UESTC, Chengdu, Sichuan, Peoples R China	University of Texas System; University of Texas Austin; University of Electronic Science & Technology of China	Wang, DL (corresponding author), UT Austin, Austin, TX 78712 USA.	dilin@cs.utexas.edu; uestcliuhao@gmail.com; lqiang@cs.utexas.edu			NSF CRII [1830161]; Google Cloud	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); Google Cloud(Google Incorporated)	This work is supported in part by NSF CRII 1830161. We would like to acknowledge Google Cloud for their support.	Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Burda Yuri, 2016, INT C LEARN REPR; Cappe O, 2008, STAT COMPUT, V18, P447, DOI 10.1007/s11222-008-9059-x; Christopher M. B., 2016, PATTERN RECOGN; Cotter Colin, 2015, ARXIV150801132; De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2732; FELDMAN D, 1989, STUD SCI MATH HUNG, V24, P191; Gal Y, 2016, PR MACH LEARN RES, V48; Haarnoja T., 2018, INT C MACH LEARN ICM INT C MACH LEARN ICM; Hernandez-Lobato J.M., 2016, INT C MACH LEARN ICM; HILL BM, 1975, ANN STAT, V3, P1163, DOI 10.1214/aos/1176343247; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jang Eric, 2017, CATEGORICAL REPARAME; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma DP, 2015, INT C LEARN REPR ICL; Levine Sergey, 2018, ARXIV180500909; Li Y., 2016, ADV NEURAL INFORM PR, P1073; Liese F, 2006, IEEE T INFORM THEORY, V52, P4394, DOI 10.1109/TIT.2006.881731; Maddison Chris J., 2017, 5 INT C LEARN REPR; Minka T.P., 2001, P 17 C UNC ART INT, P362; Minka Tom, 2005, TECHNICAL REPORT; Opper M, 2005, J MACH LEARN RES, V6, P2177; OSTERREICHER F, 2003, F DIVERGENCES REPRES; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Reid MD, 2011, J MACH LEARN RES, V12, P731; Resnick SI, 2007, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-45024-7; Ryu Ernest K, 2014, ARXIV14124845; Sason I, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20050383; Vehtari A., 2015, PARETO SMOOTHED IMPO; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	33	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000026
C	Wang, JK; Abernethy, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Jun-Kun; Abernethy, Jacob			Acceleration through Optimistic No-Regret Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of minimizing a smooth convex function by reducing the optimization to computing the Nash equilibrium of a particular zero-sum convex-concave game. Zero-sum games can be solved using online learning dynamics, where a classical technique involves simulating two no-regret algorithms that play against each other and, after T rounds, the average iterate is guaranteed to solve the original optimization problem with error decaying as O(log T/T). In this paper we show that the technique can be enhanced to a rate of O(1/T-2) by extending recent work [22, 25] that leverages optimistic learning to speed up equilibrium computation. The resulting optimization algorithm derived from this analysis coincides exactly with the well-known NESTEROVACCELERATION [16] method, and indeed the same story allows us to recover several variants of the Nesterov's algorithm via small tweaks. We are also able to establish the accelerated linear rate for a function which is both strongly-convex and smooth. This methodology unifies a number of different iterative optimization methods: we show that the HEAVYBALL algorithm is precisely the non-optimistic variant of NESTEROVACCELERATION, and recent prior work already established a similar perspective on FRANKWOLFE [2, 1].	[Wang, Jun-Kun; Abernethy, Jacob] Georgia Inst Technol, Coll Comp, Atlanta, GA 30313 USA	University System of Georgia; Georgia Institute of Technology	Wang, JK (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30313 USA.	jimwang@gatech.edu; prof@gatech.edu			Division of Computer Science and Engineering at the University of Michigan; College of Computing at the Georgia Institute of Technology; NSF TRIPODS award [1740776]; NSF CAREER award [1453304]	Division of Computer Science and Engineering at the University of Michigan; College of Computing at the Georgia Institute of Technology; NSF TRIPODS award; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We would like to thank Kevin Lai and Kfir Levy for helpful discussions leading up to the results in this paper. This work was supported by funding from the Division of Computer Science and Engineering at the University of Michigan, from the College of Computing at the Georgia Institute of Technology, NSF TRIPODS award 1740776, and NSF CAREER award 1453304.	Abernethy J, 2008, PROC 21 ANN C LEARN, P437; Abernethy J. D., 2017, NIPS; Abernethy Jacob, 2018, COLT; Allen-Zhu Zeyuan, 2017, ITCS; Balduzzi David, 2018, ARXIV180205642; Beck A., 2009, SIAM J IMAGING SCI; Bubeck S., 2015, GEOMETRIC ALTERNATIV; Chiang Chao-Kai, 2012, ONLINE OPTIMIZATION; Daskalakis Constantinos, 2017, ARXIV171100141; Flammarion N., 2015, COLT; Gidel G., 2018, ARXIV180704740; Lan G., 2017, MATH PROGRAMMING; Lessard Laurent, 2016, SIAM J OPTIMIZATION; McMahan Brendan, 2013, ADV NEURAL INFORM PR, V26, P2724; Nesterov Y., 1988, MAT METODY RESHENIYA, V24, P509; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Yuri, 2005, MATH PROGRAMMING; Nesterov Yuri, 1983, DOKLADY AN USSR; Parikh N., 2014, FDN TRENDS OPTIMIZAT; Rakhlin Alexander, 2013, NIPS; Rakhlin Alexander, 2013, COLT; Rockafellar Tyrrell, 1996, CONVEX ANAL; Su Weijie, 2014, NIPS; Syrgkanis Vasilis, 2015, NIPS; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Zinkevich M, 2003, ICML	28	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303079
C	Xiao, C; Zhong, PL; Zheng, CX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xiao, Chang; Zhong, Peilin; Zheng, Changxi			BourGAN: Generative Networks with Metric Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into the l(2) space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features.	[Xiao, Chang; Zhong, Peilin; Zheng, Changxi] Columbia Univ, New York, NY 10027 USA	Columbia University	Xiao, C (corresponding author), Columbia Univ, New York, NY 10027 USA.	chang@cs.columbia.edu; peilin@cs.columbia.edu; cxz@cs.columbia.edu			National Science Foundation [CAREER-1453101, 1717178, 1816041, CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955, CCF-1740833]; Simons Foundation [491119]; Google Research Award	National Science Foundation(National Science Foundation (NSF)); Simons Foundation; Google Research Award(Google Incorporated)	We thank Daniel Hsu, Carl Vondrick and Henrique Maia for the helpful feedback. Chang Xiao and Changxi Zheng are supported in part by the National Science Foundation (CAREER-1453101, 1717178 and 1816041) and generous donations from SoftBank and Adobe. Peilin Zhong is supported in part by National Science Foundation (CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955 and CCF-1740833), Simons Foundation (#491119 to Alexandr Andoni) and Google Research Award.	Arjovsky M., 2017, ARXIV170107875; Bojanowski Piotr, 2017, ARXIV170705776; Bora A, 2017, PR MACH LEARN RES, V70; Bora Ashish, 2018, INT C LEARN REPR; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Che Tong, 2016, ARXIV161202136; Courty  Nicolas, 2017, ARXIV171007457; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Indyk P., 2004, HDB DISCRETE COMPUTA, P177, DOI DOI 10.1201/9781420035315.CH8; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Leighton T., 1988, 29th Annual Symposium on Foundations of Computer Science (IEEE Cat. No.88CH2652-6), P422, DOI 10.1109/SFCS.1988.21958; Lin Z., 2017, ARXIV171204086; LINIAL N, 1995, COMBINATORICA, V15, P215, DOI 10.1007/BF01200757; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Matousek  Jiri, 2002, LECT DISCRETE GEOMET, P355; Metz Luke, 2016, ARXIV161102163; Mirza M., 2014, ARXIV; Nagarajan V, 2017, ADV NEUR IN, V30; Nowozin S, 2016, ADV NEUR IN, V29; PUKELSHEIM F, 1994, AM STAT, V48, P88, DOI 10.2307/2684253; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Saatci Y., 2017, ADV NEURAL INFORM PR, P3622; Salimans T, 2016, ADV NEUR IN, V29; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Theis Lucas, 2015, ARXIV151101844; Tolstikhin I. O., 2017, ADV NEURAL INFORM PR, P5424, DOI DOI 10.5555/3295222.3295294; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Xiao H., 2017, FASHION MNIST NOVEL; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zhu Jun-Yan, 2017, ICCV	51	4	4	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302029
C	Xie, TY; Liu, B; Xu, YY; Ghavamzadeh, M; Chow, Y; Lyu, D; Yoon, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xie, Tengyang; Liu, Bo; Xu, Yangyang; Ghavamzadeh, Mohammad; Chow, Yinlam; Lyu, Daoming; Yoon, Daesub			A Block Coordinate Ascent Algorithm for Mean-Variance Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DESCENT METHOD; MIRROR DESCENT; CONVERGENCE; CONVEX; NONSMOOTH; RISK	Risk management in dynamic decision problems is a primary concern in many fields, including financial investment, autonomous driving, and healthcare. The mean-variance function is one of the most widely used objective functions in risk management due to its simplicity and interpretability. Existing algorithms for mean-variance optimization are based on multi-time-scale stochastic approximation, whose learning rate schedules are often hard to tune, and have only asymptotic convergence proof. In this paper, we develop a model-free policy search framework for mean-variance optimization with finite-sample error bound analysis (to local optima). Our starting point is a reformulation of the original mean-variance function with its Legendre-Fenchel dual, from which we propose a stochastic block coordinate ascent policy search algorithm. Both the asymptotic convergence guarantee of the last iteration's solution and the convergence rate of the randomly picked solution are provided, and their applicability is demonstrated on several benchmark domains.	[Xie, Tengyang] UMass Amherst, Amherst, MA 01003 USA; [Liu, Bo; Lyu, Daoming] Auburn Univ, Auburn, AL 36849 USA; [Xu, Yangyang] Rensselaer Polytech Inst, Troy, NY 12181 USA; [Ghavamzadeh, Mohammad] Facebook AI Res, Menlo Pk, CA 94025 USA; [Chow, Yinlam] Google DeepMind, London, England; [Yoon, Daesub] ETRI, Kwangju, South Korea	University of Massachusetts System; University of Massachusetts Amherst; Auburn University System; Auburn University; Rensselaer Polytechnic Institute; Facebook Inc; Google Incorporated; Electronics & Telecommunications Research Institute - Korea (ETRI)	Liu, B (corresponding author), Auburn Univ, Auburn, AL 36849 USA.	txie@cs.umass.edu; boliu@auburn.edu; xuy21@rpi.edu; mgh@fb.com; yinlamchow@google.com; daoming.lyu@auburn.edu; eyetracker@etri.re.kr	Lyu, Daoming/AAL-8557-2020; Xu, Yangyang/R-6164-2019	Lyu, Daoming/0000-0003-2625-9865; 	Transportation and Logistics R&D Program - Ministry of Land, Infrastructure and Transport of Korean government [18TLRP-B131486-02]; NSF [DMS-1719549]	Transportation and Logistics R&D Program - Ministry of Land, Infrastructure and Transport of Korean government; NSF(National Science Foundation (NSF))	Bo Liu, Daoming Lyu, and Daesub Yoon were partially supported by a grant (18TLRP-B131486-02) from Transportation and Logistics R&D Program funded by Ministry of Land, Infrastructure and Transport of Korean government. Yangyang Xu was partially supported by the NSF grant DMS-1719549.	Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bellemare M. G., 2016, INT C MACH LEARN; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Borkar VS, 2002, MATH OPER RES, V27, P294, DOI 10.1287/moor.27.2.294.324; Boyd S, 2004, CONVEX OPTIMIZATION; Chow Y., 2014, P ADV NEURAL INFORM, P3509; Chow Yinlam, 2018, J MACHINE LEARNING R; Dai B., 2017, 20 INT C ART INT STA; Du S., 2017, ARXIV170207944; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Lai TL, 2011, ANN APPL STAT, V5, P798, DOI 10.1214/10-AOAS422; Li D, 2000, MATH FINANC, V10, P387, DOI 10.1111/1467-9965.00100; Liu B., 2015, C UNC ART INT; Liu B., 2018, J ARTIFICIAL INTELLI; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Mairal J., 2013, ADV NEURAL INFORM PR, V2, P2283; Mannor S., 2011, P 28 INT C MACH LEAR; Markowitz HM, 2000, MEAN VARIANCE ANAL P, V66; Maurer M., 2016, AUTONOMOUS DRIVING, DOI 10.1007/978-3-662-48847-8_18; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Parker D, 2009, J NURS MANAGE, V17, P218, DOI 10.1111/j.1365-2834.2009.00993.x; Prashanth LA, 2016, MACH LEARN, V105, P367, DOI 10.1007/s10994-016-5569-5; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2014, ICML ICML 14, P387; SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tamar A., 2012, 29 INT C MACH LEARN, P935; Tamar A., 2015, AAAI C ART INT; Tamar A., 2015, ADV NEURAL INFORM PR, P1468; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Wang MD, 2017, MATH PROGRAM, V161, P419, DOI 10.1007/s10107-016-1017-3; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Xu YY, 2015, SIAM J OPTIMIZ, V25, P1686, DOI 10.1137/140983938; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795	45	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301009
C	Yang, ZL; Zhao, J; Dhingra, B; He, KM; Cohen, WW; Salakhutdinov, R; LeCun, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yang, Zhilin; Zhao, Jake (Junbo); Dhingra, Bhuwan; He, Kaiming; Cohen, William W.; Salakhutdinov, Ruslan; LeCun, Yann			GLoMo: Unsupervised Learning of Transferable Relational Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels.	[Yang, Zhilin; Dhingra, Bhuwan; Cohen, William W.; Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zhao, Jake (Junbo); LeCun, Yann] NYU, New York, NY 10003 USA; [Zhao, Jake (Junbo); LeCun, Yann] Facebook AI Res, New York, NY USA	Carnegie Mellon University; New York University; Facebook Inc	Yang, ZL (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zhiliny@cs.cmu.edu; jakezhao@cs.nyu.com; bdhingra@cs.cmu.edu; kaiminghe@fb.com; wcohen@cs.cmu.edu; rsalakhu@cs.cmu.edu; yann@cs.nyu.com			Office of Naval Research; DARPA [D17AP00001]; Google focused award; Nvidia NVAIL award; Nvidia PhD Fellowship; Apple	Office of Naval Research(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Google focused award(Google Incorporated); Nvidia NVAIL award; Nvidia PhD Fellowship; Apple	This work was supported in part by the Office of Naval Research, DARPA award D17AP00001, Apple, the Google focused award, and the Nvidia NVAIL award. ZY is supported by the Nvidia PhD Fellowship. The authors would also like to thank Sam Bowman for useful discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Battaglia Peter W, 2018, ARXIV180601261; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152; Choi J, 2018, AAAI CONF ARTIF INTE, P5094; Chomsky  Noam, 2014, ASPECTS THEORY SYNTA, V11; Chung J., 2014, ARXIV14123555; Clark Christopher, 2017, ARXIV171010723; Conneau A, 2017, PROC 2017 C EMPIR ME, P670, DOI [10.18653/v1/d17-1070, DOI 10.18653/V1/D17-1070]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dhingra  Bhuwan, 2018, NEURAL MODELS REASON; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ji Yangfeng, 2017, EMNLP; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kipf T, 2018, INT C MACH LEARN PML, P2688; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Liu Peter J., 2018, GENERATING WIKIPEDIA, P2; Liu Xiaodong, 2018, ARXIV180407888; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Maillard Jean, 2017, ARXIV170509189; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Miyato Takeru, 2016, ARXIV160507725; Negrinho R., 2017, ARXIV170408792; Parmar Niki, 2018, ARXIV180205751, P2; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; RAJPURKAR P, 2016, P 2016 C EMP METH NA, V2016, P2383, DOI DOI 10.18653/V1/D16-1264; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans Tim, 2017, ARXIV170105517; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shen Tao, 2017, ARXIV170904696; Socher R., 2011, P 28 INT C INT C MAC, P129; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Strubell E., 2018, EMNLP; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; van den Oord A, 2016, PR MACH LEARN RES, V48; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Verga P, 2017, ARXIV17100831; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Williams A, 2018, T ASSOC COMPUT LING, V6, P253, DOI DOI 10.1162/TACL_A_00019; Williams Adina, 2017, ARXIV170405426; Yang Zhilin, 2017, ARXIV171103953; Yogatama  Dani, 2016, ICLR; Yu Adams Wei, 2018, ARXIV180409541; Zhang Han, 2018, ARXIV180508318; Zoph B., 2016, ARXIV161101578	51	4	5	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003050
C	Zhang, XY; Li, YT; Shen, DH; Carin, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Xinyuan; Li, Yitong; Shen, Dinghan; Carin, Lawrence			Diffusion Maps for Textual Network Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Textual network embedding leverages rich text information associated with the network to learn low-dimensional vectorial representations of vertices. Rather than using typical natural language processing (NLP) approaches, recent research exploits the relationship of texts on the same edge to graphically embed text. However, these models neglect to measure the complete level of connectivity between any two texts in the graph. We present diffusion maps for textual network embedding (DMTE), integrating global structural information of the graph to capture the semantic relatedness between texts, with a diffusion-convolution operation applied on the text inputs. In addition, a new objective function is designed to efficiently preserve the high-order proximity using the graph diffusion. Experimental results show that the proposed approach outperforms state-of-the-art methods on the vertex-classification and link-prediction tasks.	[Zhang, Xinyuan; Li, Yitong; Shen, Dinghan; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27707 USA	Duke University	Zhang, XY (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27707 USA.	xy.zhang@duke.edu; yitong.li@duke.edu; dinghan.shen@duke.edu; lcarin@duke.edu		Carin, Lawrence/0000-0001-6277-7948	DARPA; DOE; NIH; ONR; NSF	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	The authors would like to thank the anonymous reviewers for their insightful comments. This research was supported in part by DARPA, DOE, NIH, ONR and NSF.	Abrahamson E, 1997, ORGAN SCI, V8, P289, DOI 10.1287/orsc.8.3.289; [Anonymous], 2018, ARXIV180105062; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Belkin M, 2002, ADV NEUR IN, V14, P585; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cao S., 2015, P 24 ACM INT C INF K, P891, DOI DOI 10.1145/2806416.2806512; Gan Zhe, 2017, P 2017 C EMP METH NA, P2390; Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Iyyer M, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1681; Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988; Mikolov T., 2013, ARXIV; Mitchell J, 2010, COGNITIVE SCI, V34, P1388, DOI 10.1111/j.1551-6709.2010.01106.x; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shen DH, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P440; Shen Dinghan, 2018, ARXIV180809633, P1829, DOI DOI 10.18653/V1/D18-1209; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sun X., 2016, ARXIV161002906; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Tang Jie, 2008, P 14 ACM SIGKDD INT, P990, DOI DOI 10.1145/1401890.1402008; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tu CC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1722, DOI 10.18653/v1/P17-1158; Tu CC, 2014, COMM COM INF SC, V489, P1; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Wang Y., 2016, NETWORK, V11, P12; Yang C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2111	33	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002016
C	Zhou, P; Yuan, XT; Feng, JS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Pan; Yuan, Xiao-Tong; Feng, Jiashi			New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					As an incremental-gradient algorithm, the hybrid stochastic gradient descent (HS-GD) enjoys merits of both stochastic and full gradient methods for finite-sum problem optimization. However, the existing rate-of-convergence analysis for HSGD is made under with-replacement sampling (WRS) and is restricted to convex problems. It is not clear whether HSGD still carries these advantages under the common practice of without-replacement sampling (WoRS) for non-convex problems. In this paper, we affirmatively answer this open question by showing that under WoRS and for both convex and non-convex problems, it is still possible for HSGD (with constant step-size) to match full gradient descent in rate of convergence, while maintaining comparable sample-size-independent incremental first-order oracle complexity to stochastic gradient descent. For a special class of finite-sum problems with linear prediction models, our convergence results can be further improved in some cases. Extensive numerical results confirm our theoretical affirmation and demonstrate the favorable efficiency of WoRS-based HSGD.	[Zhou, Pan; Feng, Jiashi] Natl Univ Singapore, Learning & Vis Lab, Singapore, Singapore; [Yuan, Xiao-Tong] Nanjing Univ Informat Sci & Technol, B DAT Lab, Nanjing, Jiangsu, Peoples R China	National University of Singapore; Nanjing University of Information Science & Technology	Zhou, P (corresponding author), Natl Univ Singapore, Learning & Vis Lab, Singapore, Singapore.	pzhou@u.nus.edu; xtyuan@nuist.edu.cn; elefjia@nus.edu.sg	Feng, Jiashi/AGX-6209-2022		NUS startup [R-263-000-C08-133]; MOE Tier-I [R-263-000-C21-112]; NUS IDS [R-263-000-C67-646]; ECRA [R-263-000-C87-133]; MOE Tier-II [R-263-000-D17-112]; Natural Science Foundation of China (NSFC) [61522308, 61876090]; Tencent AI Lab Rhino-Bird Joint Research Program [JR201801]	NUS startup; MOE Tier-I; NUS IDS; ECRA; MOE Tier-II; Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Tencent AI Lab Rhino-Bird Joint Research Program	Jiashi Feng was partially supported by NUS startup R-263-000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112. Xiao-Tong Yuan was supported in part by Natural Science Foundation of China (NSFC) under Grant 61522308 and Grant 61876090, and in part by Tencent AI Lab Rhino-Bird Joint Research Program No. JR201801.	Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; [Anonymous], [No title captured]; Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022; Bottou L, 2009, P S LEARN DAT SCI PA P S LEARN DAT SCI PA; Cauchy A. L., 1847, COMP REND SCI PARIS, V25, P536; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio AJ, 2014, PR MACH LEARN RES, V32, P1125; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Gurbuzbalaban M., 2015, ARXIV151008560; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kohler J. M., 2017, P INT C MACH LEARN; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Reddi SJ, 2016, PR MACH LEARN RES, V48; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Shamir O, 2016, ADV NEUR IN, V29; Ying B., 2017, ARXIV170801383; Zhang YC, 2015, PR MACH LEARN RES, V37, P353; Zhou P., 2018, P C NEUTR INF PROC S	23	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301024
C	Zoltowski, DM; Pillow, JW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zoltowski, David M.; Pillow, Jonathan W.			Scaling the Poisson GLM to massive neural datasets through polynomial approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent advances in recording technologies have allowed neuroscientists to record simultaneous spiking activity from hundreds to thousands of neurons in multiple brain regions. Such large-scale recordings pose a major challenge to existing statistical methods for neural data analysis. Here we develop highly scalable approximate inference methods for Poisson generalized linear models (GLMs) that require only a single pass over the data. Our approach relies on a recently proposed method for obtaining approximate sufficient statistics for GLMs using polynomial approximations [7], which we adapt to the Poisson GLM setting. We focus on inference using quadratic approximations to nonlinear terms in the Poisson GLM log-likelihood with Gaussian priors, for which we derive closed-form solutions to the approximate maximum likelihood and MAP estimates, posterior distribution, and marginal likelihood. We introduce an adaptive procedure to select the polynomial approximation interval and show that the resulting method allows for efficient and accurate inference and regularization of high-dimensional parameters. We use the quadratic estimator to fit a fully-coupled Poisson GLM to spike train data recorded from 831 neurons across five regions of the mouse brain for a duration of 41 minutes, binned at 1 ms resolution. Across all neurons, this model is fit to over 2 billion spike count bins and identifies fine-timescale statistical dependencies between neurons within and across cortical and subcortical areas.	[Zoltowski, David M.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst & Psychol, Princeton, NJ 08544 USA	Princeton University; Princeton University	Zoltowski, DM (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	zoltowski@princeton.edu; pillow@princeton.edu			NIH [T32MH065214, R01EY017366, R01NS104899]; Simons Foundation [SCGB AWD1004351, AWD543027]; U19 NIH-NINDS BRAIN Initiative Award [NS104648-01]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Simons Foundation; U19 NIH-NINDS BRAIN Initiative Award	DMZ was supported by NIH grant T32MH065214 and JWP was supported by grants from the Simons Foundation (SCGB AWD1004351 and AWD543027), the NIH (R01EY017366, R01NS104899) and a U19 NIH-NINDS BRAIN Initiative Award (NS104648-01). The authors thank Nick Steinmetz for sharing the Neuropixels data and thank Rob Kass, Stephen Keeley, Michael Morais, Neil Spencer, Nick Steinmetz, Nicholas Roy, and the anonymous reviewers for providing helpful comments.	Aoi Mikio, 2017, BIORXIV; Bishop CM, 1999, ADV NEUR IN, V11, P382; Calabrese F, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0020814; Chung Jason E, 2018, BIORXIV; David SV, 2007, NETWORK-COMP NEURAL, V18, P191, DOI 10.1080/09548980701609235; Gerwinn S, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00012; Huggins J., 2017, ADV NEURAL INFORM PR, P3614; Jun JJ, 2017, NATURE, V551, P232, DOI 10.1038/nature24636; Linderman Scott, 2016, ADV NEURAL INFORM PR, P2002; MacKay DJC, 1994, ASHRAE T, V100, P1053; Mason J.C., 2002, CHEBYSHEV POLYNOMIAL; Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002; Park I. M., 2011, ADV NEURAL INF PROCE, V24, P1692; Park IM, 2014, NAT NEUROSCI, V17, P1395, DOI 10.1038/nn.3800; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; RAHNAMA RAD K., 2010, NETWORK, V21, P142; Ramirez AD, 2014, J COMPUT NEUROSCI, V36, P215, DOI 10.1007/s10827-013-0466-4; Sahani M, 2002, ADV NEURAL INFORM PR, V15, P317; Simoncelli Eero P, 2004, COGNITIVE NEUROSCIEN, V3, P1; Stevenson IH, 2009, IEEE T NEUR SYS REH, V17, P203, DOI 10.1109/TNSRE.2008.2010471; Stringer C, 2018, BIORXIV, DOI [10.1101/306019, DOI 10.1101/306019]; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004; Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003; Wipf D. P., 2008, ADV NEURAL INFORM PR, V20, P1625	25	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF	31244513				2022-12-19	WOS:000461823303051
C	Balkanski, E; Singer, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Balkanski, Eric; Singer, Yaron			Minimizing a Submodular Function from Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper we consider the problem of minimizing a submodular function from training data. Submodular functions can be efficiently minimized and are consequently heavily applied in machine learning. There are many cases, however, in which we do not know the function we aim to optimize, but rather have access to training data that is used to learn it. In this paper we consider the question of whether submodular functions can be minimized when given access to its training data. We show that even learnable submodular functions cannot be minimized within any non-trivial approximation when given access to polynomially-many samples. Specifically, we show that there is a class of submodular functions with range in [0, 1] such that, despite being PAC-learnable and minimizable in polynomial-time, no algorithm can obtain an approximation strictly better than 1/2 - o(1) using polynomially-many samples drawn from any distribution. Furthermore, we show that this bound is tight via a trivial algorithm that obtains an approximation of 1/2.	[Balkanski, Eric; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Balkanski, E (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	ericbalkanski@g.harvard.edu; yaron@seas.harvard.edu	Jeong, Yongwook/N-7413-2016					Balcan M.F., 2016, P 29 C LEARN THEOR C, P310; Balcan MF, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P3; Balcan MF, 2011, ACM S THEORY COMPUT, P793; Balkanski E., 2016, ADV NEURAL INFORM PR, P4017; Balkanski Eric., 2017, P C LEARNING THEORY; Balkanski Eric, 2017, P 49 ANN ACM S THEOR; Chakrabarty Deeparnab, 2016, ARXIV161009800; Djolonga J., 2016, NEURAL INFORM PROCES; Ene A, 2015, PR MACH LEARN RES, V37, P787; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman V., 2014, C LEARNING THEORY, P679; Feldman V., 2013, P COLT; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096; Iyer Rishabh K., 2013, ADV NEURAL INFORM PR, P2742; Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589; Jegelka S., 2011, ADV NEURAL INFORM PR, V24, P460; Narasimhan M., 2012, ARXIV12071404; Queyranne M, 1998, MATH PROGRAM, V82, P3, DOI 10.1007/BF01585863; Shai S.-S., 2014, UNDERSTANDING MACHIN; Stobbe P., 2010, ADV NEURAL INFO PROC, P2208	21	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400078
C	Bian, A; Levy, KY; Krause, A; Buhmann, JM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bian, An; Levy, Kfir Y.; Krause, Andreas; Buhmann, Joachim M.			Continuous DR-submodular Maximization: Structure and Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					DR-submodular continuous functions are important objectives with wide real-world applications spanning MAP inference in determinantal point processes (DPPs), and mean-field inference for probabilistic submodular models, amongst others. DR-submodularity captures a subclass of non-convex functions that enables both exact minimization and approximate maximization in polynomial time. In this work we study the problem of maximizing non-monotone continuous DR-submodular functions under general down-closed convex constraints. We start by investigating geometric properties that underlie such objectives, e.g., a strong relation between (approximately) stationary points and global optimum is proved. These properties are then used to devise two optimization algorithms with provable guarantees. Concretely, we first devise a "two-phase" algorithm with 1/4 approximation guarantee. This algorithm allows the use of existing methods for finding (approximately) stationary points as a subroutine, thus, harnessing recent progress in non-convex optimization. Then we present a non-monotone FRANK-WOLFE variant with 1/e approximation guarantee and sublinear convergence rate. Finally, we extend our approach to a broader class of generalized DR-submodular continuous functions, which captures a wider spectrum of applications. Our theoretical findings are validated on synthetic and real-world problem instances.	[Bian, An; Levy, Kfir Y.; Krause, Andreas; Buhmann, Joachim M.] Swiss Fed Inst Technol, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Bian, A (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	ybian@inf.ethz.ch; yehuda.levy@inf.ethz.ch; krausea@ethz.ch; jbuhmann@inf.ethz.ch	Buhmann, Joachim/AAU-4760-2020	Levy, Kfir Yehuda/0000-0003-1236-2626; Bian, Yatao/0000-0002-2368-4084	ERC [StG 307036]; Max Planck ETH Center for Learning Systems; ETH Zurich Postdoctoral Fellowship program	ERC(European Research Council (ERC)European Commission); Max Planck ETH Center for Learning Systems; ETH Zurich Postdoctoral Fellowship program	This research was partially supported by ERC StG 307036, by the Max Planck ETH Center for Learning Systems, and by the ETH Zurich Postdoctoral Fellowship program.	Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Antoniadis A, 2011, ANN I STAT MATH, V63, P585, DOI 10.1007/s10463-009-0242-4; Bach F, 2015, ARXIV151100394; Bian AA, 2017, PR MACH LEARN RES, V54, P111; Boyd S, 2004, CONVEX OPTIMIZATION; Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182; Chekuri C, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P201, DOI 10.1145/2688073.2688086; Chekuri C, 2014, SIAM J COMPUT, V43, P1831, DOI 10.1137/110839655; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; Eghbali Reza, 2016, ADV NEURAL INFORM PR, P3279; Ene A., 2016, ARXIV160608362; Ene A, 2016, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2016.34; Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46; Friedland S, 2013, LINEAR ALGEBRA APPL, V438, P3872, DOI 10.1016/j.laa.2011.11.021; Fuchssteiner Benno, 2011, CONVEX CONES, V56; Garg V.K., 2015, INTRO LATTICE THEORY; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Gharan SO, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1098; Gillenwater J., 2012, ADV NEURAL INFORM PR; Hassani Hamed, 2017, ADV NEURAL INFORM PR, P5837; Ito S., 2016, ADV NEURAL INFORM PR, P3855; Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096; Khodabakhsh Ali, 2016, ARXIV161109474; Krause A., 2012, TRACTABILITY PRACTIC, V3, P19; Lacoste-Julien S., 2016, ARXIV PREPRINT ARXIV; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; MOTZKIN TS, 1965, CANADIAN J MATH, V17, P533, DOI 10.4153/CJM-1965-053-6; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Soma T., 2015, P ADV NEURAL INFORM, P847; Soma T, 2014, PR MACH LEARN RES, V32; Sra S., 2012, ADV NEURAL INFORM PR, P530; Staib M., 2017, P 34 INT C MACH LEAR, P3230; TOPKIS DM, 1978, OPER RES, V26, P305, DOI 10.1287/opre.26.2.305; Vondrak J, 2008, ACM S THEORY COMPUT, P67; Xia Wei, 2015, ARXIV151102423; Zass R., 2007, ADV NEURAL INFORM PR, P1561	40	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400047
C	Bigdeli, SA; Jin, MG; Favaro, P; Zwicker, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bigdeli, Siavash A.; Jin, Meiguang; Favaro, Paolo; Zwicker, Matthias			Deep Mean-Shift Priors for Image Restoration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FRAMEWORK; FIELDS	In this paper we introduce a natural image prior that directly represents a Gaussian-smoothed version of the natural image distribution. We include our prior in a formulation of image restoration as a Bayes estimator that also allows us to solve noise-blind image restoration problems. We show that the gradient of our prior corresponds to the mean-shift vector on the natural image distribution. In addition, we learn the mean-shift vector field using denoising autoencoders, and use it in a gradient descent approach to perform Bayes risk minimization. We demonstrate competitive results for noise-blind deblurring, super-resolution, and demosaicing.	[Bigdeli, Siavash A.; Jin, Meiguang; Favaro, Paolo; Zwicker, Matthias] Univ Bern, Bern, Switzerland; [Zwicker, Matthias] Univ Maryland, College Pk, MD 20742 USA	University of Bern; University System of Maryland; University of Maryland College Park	Bigdeli, SA (corresponding author), Univ Bern, Bern, Switzerland.	bigdeli@inf.unibe.ch; jin@inf.unibe.ch; favaro@inf.unibe.ch; zwicker@cs.umd.edu	Jeong, Yongwook/N-7413-2016; Jin, Meiguang/AAC-9958-2021		Swiss National Science Foundation (SNSF) [200021-153324]	Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF))	MJ and PF acknowledge support from the Swiss National Science Foundation (SNSF) on project 200021-153324.	Alain G, 2014, J MACH LEARN RES, V15, P3563; [Anonymous], 2016, ARXIV160803981; [Anonymous], 2017, ARXIV170403264; [Anonymous], [No title captured]; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Bigdeli S.A., 2017, ARXIV170309964; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dabov K., 2006, ELECT IMAGING 2006; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399; Jin M., 2017, COMP VIS PATT REC CV; Khashabi D, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2359774; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Klatzer T, 2016, IEEE INT CONF COMPUT, P43; Krishnan D., 2009, ADV NEURAL INFORM PR, V22, P1033; Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2833, DOI 10.1109/CVPR.2011.5995309; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Malvar HS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P485; Meinhardt Tim, 2017, ARXIV170403488; Miyasawa K., 1961, B I INT STAT, V38, P1; Perrone D, 2016, INT J COMPUT VISION, V117, P159, DOI 10.1007/s11263-015-0857-2; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; Raphan M, 2011, NEURAL COMPUT, V23, P374, DOI 10.1162/NECO_a_00076; Romano Y., 2016, ARXIV161102862; Roth S, 2005, PROC CVPR IEEE, P860; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schmidt U, 2016, IEEE T PATTERN ANAL, V38, P677, DOI 10.1109/TPAMI.2015.2441053; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Shaham TR, 2016, LECT NOTES COMPUT SC, V9910, P136, DOI 10.1007/978-3-319-46466-4_9; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Xiao Lei, 2017, ARXIV170309245; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang H., 2013, ADV NEURAL INFORM PR, P1556; Zhang H., 2014, ADV NEURAL INFORM PR, P3005; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	41	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400073
C	Brown, N; Sandholm, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Brown, Noam; Sandholm, Tuomas			Safe and Nested Subgame Solving for Imperfect-Information Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it in individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold' em poker.	[Brown, Noam; Sandholm, Tuomas] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA	Carnegie Mellon University	Brown, N (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA.	noamb@cs.cmu.edu; sandholm@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]; Carnegie Mellon University; TNG Technology Consulting; Intel; Optimized Markets, Inc.; GreatPoint Ventures; Avenue4Analytics; Rivers Casino	National Science Foundation(National Science Foundation (NSF)); ARO; Carnegie Mellon University; TNG Technology Consulting; Intel(Intel Corporation); Optimized Markets, Inc.; GreatPoint Ventures; Avenue4Analytics; Rivers Casino	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center. The Brains vs. AI competition was sponsored by Carnegie Mellon University, Rivers Casino, GreatPoint Ventures, Avenue4Analytics, TNG Technology Consulting, Artificial Intelligence, Intel, and Optimized Markets, Inc. We thank Kristen Gardner, Marcelo Gutierrez, Theo Gutman-Solo, Eric Jackson, Christian Kroer, Tim Reiff, and the anonymous reviewers for helpful feedback.	[Anonymous], 2012, P 26 AAAI C ART INT; [Anonymous], 2008, P 7 AAMAS; Billings D., 2003, P 18 INT JOINT C ART; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2017, SCIENCE; Brown N, 2017, AAAI CONF ARTIF INTE, P421; Brown Noam, 2015, P INT JOINT C ARTIFI; Burch N, 2014, AAAI CONF ARTIF INTE, P602; Campbell M, 2002, ARTIF INTELL, V134, P57, DOI 10.1016/S0004-3702(01)00129-1; Ganzfried S, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P37; Ganzfried Sam, 2013, P 23 INT JOINT C ART, P120; Ganzfried Sam, 2014, AAAI C ART INT AAAI; Gilpin A., 2007, P 6 AAMAS, P1168; Gilpin A., 2006, P NAT C ART INT AAAI, P1007; Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2; Jackson Eric Griffin, 2014, AAAI WORKSH COMP POK; Johanson M., 2013, CORR; Kroer Christian, 2017, P ACM C EC COMP EC; Moravcik M., 2017, ARXIV170101724V3; Moravcik Matej, 2016, AAAI C ART INT AAAI; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Sandholm T, 2015, AAAI CONF ARTIF INTE, P4127; Sandholm T, 2015, SCIENCE, V347, P122, DOI 10.1126/science.aaa4614; Sandholm T, 2010, AI MAG, V31, P13, DOI 10.1609/aimag.v31i4.2311; Schaeffer J, 2007, SCIENCE, V317, P1518, DOI 10.1126/science.1144079; Schnizlein D, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P278; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645; Waugh Kevin, 2009, P ANN C NEUR INF PRO; Zinkevich M., 2007, ADV NEURAL INFORM PR, V7, P1729	32	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400066
C	Chen, JF; Li, CX; Ru, YZ; Zhu, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Jianfei; Li, Chongxuan; Ru, Yizhong; Zhu, Jun			Population Matching Discrepancy and Applications in Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A differentiable estimation of the distance between two distributions based on samples is important for many deep learning tasks. One such estimation is maximum mean discrepancy (MMD). However, MMD suffers from its sensitive kernel bandwidth hyper-parameter, weak gradients, and large mini-batch size when used as a training objective. In this paper, we propose population matching discrepancy (PMD) for estimating the distribution distance based on samples, as well as an algorithm to learn the parameters of the distributions using PMD as an objective. PMD is defined as the minimum weight matching of sample populations from each distribution, and we prove that PMD is a strongly consistent estimator of the first Wasserstein metric. We apply PMD to two deep learning tasks, domain adaptation and generative modeling Empirical results demonstrate that PMD overcomes the aforementioned drawbacks of MMD, and outperforms MMD on both tasks in terms of the performance as well as the convergence speed.	[Chen, Jianfei; Li, Chongxuan; Ru, Yizhong; Zhu, Jun] Tsinghua Univ, State Key Lab Intell Tech & Sys, TNList Lab, Dept Comp Sci & Tech, Beijing 100084, Peoples R China	Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, State Key Lab Intell Tech & Sys, TNList Lab, Dept Comp Sci & Tech, Beijing 100084, Peoples R China.	chenjian14@mails.tsinghua.edu.cn; licx14@mails.tsinghua.edu.cn; ruyz13@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn	Jeong, Yongwook/N-7413-2016		National NSF of China [61620106010, 61621136008, 61332007]; MIIT Grant of Int. Man. Comp. Stan [2016ZXFB00001]; Youth Top-notch Talent Support Program, Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program	National NSF of China(National Natural Science Foundation of China (NSFC)); MIIT Grant of Int. Man. Comp. Stan; Youth Top-notch Talent Support Program, Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program	This work is supported by the National NSF of China (Nos. 61620106010, 61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No. 2016ZXFB00001), the Youth Top-notch Talent Support Program, Tsinghua Tiangong Institute for Intelligent Computing and the NVIDIA NVAIL Program.	Abadi M, 2015, P 12 USENIX S OPERAT; Agrawal Siddharth, GENERATIVE MOMENT MA; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjovsky M., 2017, ARXIV170107875; Bellemare MG, 2017, ARXIV; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bertsekas D.P., 1998, NETWORK OPTIMIZATION; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Date K, 2016, PARALLEL COMPUT, V57, P52, DOI 10.1016/j.parco.2016.05.012; Drake Doratha, 2003, APPROXIMATION ROM, P21; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Ganin Y, 2016, J MACH LEARN RES, V17; Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, ADV NEURAL INF PROCE; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Huang Gary B., 2007, 0749 U MASS, P7; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li N., 2007, 2007 IEEE 23 INT C D, P106; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Long MS, 2017, PR MACH LEARN RES, V70; Manne F, 2008, LECT NOTES COMPUT SC, V4967, P708; Mohamed Shakir, 2016, ARXIV161003483; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Salimans T, 2016, ADV NEUR IN, V29; Saminger-Platz S., 2017, P INT C LEARN REPR I; Song L., 2011, P 14 INT C ART INT S, P707; Sriperumbudur BK, 2010, IEEE INT SYMP INFO, P1428, DOI 10.1109/ISIT.2010.5513626; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tzeng E., 2014, ARXIV PREPRINT ARXIV; van den Oord A, 2016, PR MACH LEARN RES, V48; Varadarajan VS, 1958, SANKHYA, V19, P15; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wan XJ, 2007, INFORM SCIENCES, V177, P3718, DOI 10.1016/j.ins.2007.02.045; Zeiler Matthew D, 2012, ARXIV12125701	52	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406033
C	Fathony, R; Bashiri, M; Ziebart, BD		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fathony, Rizal; Bashiri, Mohammad; Ziebart, Brian D.			Adversarial Surrogate Losses for Ordinal Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONSISTENCY	Ordinal regression seeks class label predictions when the penalty incurred for mistakes increases according to an ordering over the labels. The absolute error is a canonical example. Many existing methods for this task reduce to binary classification problems and employ surrogate losses, such as the hinge loss. We instead derive uniquely defined surrogate ordinal regression loss functions by seeking the predictor that is robust to the worst-case approximations of training data labels, subject to matching certain provided training data statistics. We demonstrate the advantages of our approach over other surrogate losses based on hinge loss approximations using UCI ordinal prediction tasks.	[Fathony, Rizal; Bashiri, Mohammad; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Fathony, R (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	rfatho2@uic.edu; mbashi4@uic.edu; bziebart@uic.edu	Jeong, Yongwook/N-7413-2016	Ziebart, Brian/0000-0003-4041-6871	Future of Life Institute FLI-RFP-AI1 program [2016-158710]; NSF [1526379]	Future of Life Institute FLI-RFP-AI1 program; NSF(National Science Foundation (NSF))	This research was supported as part of the Future of Life Institute (futureoflife.org) FLI-RFP-AI1 program, grant#2016-158710 and by NSF grant RI-#1526379.	[Anonymous], 2014, ADV NEURAL INFORM PR; [Anonymous], P 25 AAAI; Asif Kaiser, 2015, P C UNC ART INT; Cardoso JS, 2007, J MACH LEARN RES, V8, P1393; Cheng JL, 2008, IEEE IJCNN, P1279, DOI 10.1109/IJCNN.2008.4633963; Chu W, 2005, J MACH LEARN RES, V6, P1019; Chu W., 2005, P 22 INT C MACHINE L, P145; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Crammer Koby, 2001, ADV NEURAL INFORM PR, V14; Dembczynski K., 2007, INT WORKSH MIN COMPL, P169, DOI DOI 10.1007/978-3-540-68416-9_14; Deng WY, 2010, NEUROCOMPUTING, V74, P447, DOI 10.1016/j.neucom.2010.08.022; Dudik M, 2006, LECT NOTES ARTIF INT, V4005, P123, DOI 10.1007/11776420_12; Farnia F., 2016, ADV NEURAL INFORM PR, P4233; Fathony Rizal, 2016, ADV NEURAL INFORM PR, P559; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; HongWang Wei Xing, 2015, ADV NEURAL INFORM PR, P2710; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kriegel, 2006, P 23 INT C MACH LEAR, P1089; Li L., 2006, ADV NEURAL INF PROCE, V19, P865; Lichman M., 2013, UCI MACHINE LEARNING; Lin H, 2008, THESIS; Lin HT, 2006, LECT NOTES ARTIF INT, V4264, P319; Lin Hsuan-Tien, 2014, JMLR WORKSHOP C P, V39, P371; Liu, 2007, INT C ART INT STAT, V2, P291; Mathieson M., 1996, Neural Networks in Financial Engineering. Proceedings of the Third International Conference on Neural Networks in the Capital Markets, P523; MCCULLAGH P, 1980, J ROY STAT SOC B MET, V42, P109; NARULA SC, 1982, INT STAT REV, V50, P317, DOI 10.2307/1402501; Pedregosa F, 2017, J MACH LEARN RES, V18, P1; Ramaswamy Harish G, 2012, ADV NEURAL INFORM PR, P2078; Rennie J. D., 2005, P IJCAI MULT WORKSH, V1, P1; Schmidt Mark, 2015, NONUNIFORM STOCHASTI; Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6; Shashua A., 2003, ADV NEURAL INFORM PR, P961; Sun BY, 2010, IEEE T KNOWL DATA EN, V22, P906, DOI 10.1109/TKDE.2009.170; Tewari A, 2007, J MACH LEARN RES, V8, P1007; TOPSOE F, 1979, KYBERNETIKA, V15, P8; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tu H-H, 2010, P 27 INT C MACH LEAR, P1095	39	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400054
C	Gross, R; Gu, Y; Li, W; Gauci, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gross, Roderich; Gu, Yue; Li, Wei; Gauci, Melvin			Generalizing GANs: A Turing Perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FINITE-STATE MACHINES; COEVOLUTION; BEHAVIOR	Recently, a new class of machine learning algorithms has emerged, where models and discriminators are generated in a competitive setting. The most prominent example is Generative Adversarial Networks (GANs). In this paper we examine how these algorithms relate to the Turing test, and derive what-from a Turing perspective-can be considered their defining features. Based on these features, we outline directions for generalizing GANs-resulting in the family of algorithms referred to as Turing Learning. One such direction is to allow the discriminators to interact with the processes from which the data samples are obtained, making them "interrogators", as in the Turing test. We validate this idea using two case studies. In the first case study, a computer infers the behavior of an agent while controlling its environment. In the second case study, a robot infers its own sensor configuration while controlling its movements. The results confirm that by allowing discriminators to interrogate, the accuracy of models is improved.	[Gross, Roderich; Gu, Yue] Univ Sheffield, Dept Automat Control & Syst Engn, Sheffield, S Yorkshire, England; [Li, Wei] Univ York, Dept Elect, York, N Yorkshire, England; [Gauci, Melvin] Harvard Univ, Wyss Inst Biol Inspired Engn, Cambridge, MA 02138 USA	University of Sheffield; University of York - UK; Harvard University	Gross, R (corresponding author), Univ Sheffield, Dept Automat Control & Syst Engn, Sheffield, S Yorkshire, England.	r.gross@sheffield.ac.uk; ygu16@sheffield.ac.uk; wei.li@york.ac.uk; mgauci@g.harvard.edu	GROSS, Roderich/A-6657-2008	GROSS, Roderich/0000-0003-1826-1375				Arnold DV, 2002, IEEE T EVOLUT COMPUT, V6, P30, DOI 10.1023/A:1015059928466; Bongard J, 2005, J MACH LEARN RES, V6, P1651; Bongard JC, 2005, IEEE T EVOLUT COMPUT, V9, P361, DOI 10.1109/TEVC.2005.850293; Bongard J, 2006, SCIENCE, V314, P1118, DOI 10.1126/science.1133687; Cartlidge J, 2004, EVOL COMPUT, V12, P193, DOI 10.1162/106365604773955148; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; French RM, 2000, TRENDS COGN SCI, V4, P115, DOI 10.1016/S1364-6613(00)01453-4; Gauci M, 2014, INT J ROBOT RES, V33, P1145, DOI 10.1177/0278364914525244; Glover F., 2015, SCHOLARPEDIA, V10, P6532, DOI DOI 10.4249/SCH0LARPEDIA.6532; Goodfellow I., 2016, ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Harnad S., 2000, Journal of Logic, Language and Information, V9, P425, DOI 10.1023/A:1008315308862; HILLIS WD, 1990, PHYSICA D, V42, P228, DOI 10.1016/0167-2789(90)90076-2; Im D., 2016, GENERATING IMAGES RE; Jakobi N, 1995, LECT NOTES ARTIF INT, V929, P704; Juille H., 1998, Genetic Programming 1998. Proceedings of the Third Annual Conference, P519; Li W, 2016, SWARM INTELL-US, V10, P211, DOI 10.1007/s11721-016-0126-1; Li W, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P223; Magnenat S., 2011, ENKI FAST 2D ROBOT S; MILLER GF, 1994, COM ADAP SY, P411; Mondada, 2009, P 9 C AUT ROB SYST C, P59; Nolfi S, 1998, ARTIF LIFE, V4, P311, DOI 10.1162/106454698568620; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rosin CD, 1997, EVOL COMPUT, V5, P1, DOI 10.1162/evco.1997.5.1.1; Saygin AP, 2000, MIND MACH, V10, P463, DOI 10.1023/A:1011288000451; Schawinski K, 2017, MON NOT R ASTRON SOC, V467, pL110, DOI 10.1093/mnrasl/slx008; Turing A.M., 1950, MIND, V49, P433; Vidal E, 2005, IEEE T PATTERN ANAL, V27, P1013, DOI 10.1109/TPAMI.2005.147; Vidal E, 2005, IEEE T PATTERN ANAL, V27, P1026, DOI 10.1109/TPAMI.2005.148	30	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406038
C	Gunasekar, S; Woodworth, B; Bhojanapalli, S; Neyshabur, B; Srebro, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gunasekar, Suriya; Woodworth, Blake; Bhojanapalli, Srinadh; Neyshabur, Behnam; Srebro, Nathan			Implicit Regularization in Matrix Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RANK; NORM	We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix X with gradient descent on a factorization of X. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.	[Gunasekar, Suriya; Woodworth, Blake; Bhojanapalli, Srinadh; Neyshabur, Behnam; Srebro, Nathan] TTI Chicago, Chicago, IL 60637 USA		Gunasekar, S (corresponding author), TTI Chicago, Chicago, IL 60637 USA.	suriya@ttic.edu; blake@ttic.edu; srinadh@ttic.edu; behnam@ttic.edu; nati@ttic.edu	Jeong, Yongwook/N-7413-2016					Amit Y., 2007, ICML 07 P 24 INT C M, P17, DOI DOI 10.1145/1273496.1273499; [Anonymous], 2016, ADV NEURAL INFORM PR; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Bhojanapalli S., 2016, ADV NEURAL INFORM PR; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Foygel R., 2011, JMLR WORKSHOP C P, P315; Jones E., 2001, SCIPY OPEN SOURCE SC; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Keshavan R. H., 2012, THESIS; Keskar Nitish Shirish, 2016, INT C LEARN REPR; Lee JD, 2016, DESCENT ONLY CONVERG; Neyshabur B., 2017, ARXIV170503071; Neyshabur B., 2015, INT C LEARN REPR; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Zeng HQ, 2017, PROC INT CONF RECON	18	4	4	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406022
C	Huggins, JH; Adams, RP; Broderick, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Huggins, Jonathan H.; Adams, Ryan P.; Broderick, Tamara			PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generalized linear models (GLMs)-such as logistic regression, Poisson regression, and robust regression-provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy-including on an advertising data set with 40 million data points and 20,000 covariates.	[Huggins, Jonathan H.; Broderick, Tamara] MIT, CSAIL, Cambridge, MA 02139 USA; [Adams, Ryan P.] Google Brain & Princeton, Princeton, NJ USA	Massachusetts Institute of Technology (MIT)	Huggins, JH (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	jhuggins@mit.edu; rpa@princeton.edu; tbroderick@csail.mit.edu	Jeong, Yongwook/N-7413-2016		ONR [N00014-17-1-2072]; ONR MURI [N00014-11-1-0688]; Google Faculty Research Award; NSF [IIS-1421780]; Alfred P. Sloan Foundation	ONR(Office of Naval Research); ONR MURI(MURIOffice of Naval Research); Google Faculty Research Award(Google Incorporated); NSF(National Science Foundation (NSF)); Alfred P. Sloan Foundation(Alfred P. Sloan Foundation)	JHH and TB are supported in part by ONR grant N00014-17-1-2072, ONR MURI grant N00014-11-1-0688, and a Google Faculty Research Award. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation.	Ahn S., 2012, P 29 INT C MACH LEAR; Alquier P, 2016, STAT COMPUT, V26, P29, DOI 10.1007/s11222-014-9521-x; Bachem A. K. O., 2017, PRACTICAL CORESET CO; Bardenet R, 2017, J MACH LEARN RES, V18, P1; Bardenet R, 2014, PR MACH LEARN RES, V32; Betancourt Michael, 2015, FUNDAMENTAL INCOMPAT; Bierkens J., 2016, ZIG ZAG PROCESS SUPE; Bouchard-Cote A., 2016, BOUNCY PARTICLE SAMP, P1; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Campbell T., 2015, ADV NEURAL INFORM PR, P280; Entezari R., 2016, LIKELIHOOD INFLATING; Feldman D., 2011, ADV NEURAL INFORM PR, V24, P2142; Fithian W, 2014, ANN STAT, V42, P1693, DOI 10.1214/14-AOS1220; Gelman A., 2014, EXPECTATION PROPAGAT; Han L., 2016, LOCAL UNCERTAINTY SA; Hasenclever L, 2017, J MACH LEARN RES, V18; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Huggins Jonathan, 2016, P ADV NEUR INF PROC, P4080; Jaakkola TS., 1997, P MACHINE LEARNING R, VR1, P283; Korattikara A, 2014, PR MACH LEARN RES, V32; Kucukelbir A, 2015, ADV NEUR IN, V28; Li P., 2006, P 12 ACM SIGKDD INT, P287, DOI DOI 10.1145/1150402.1150436; Lucic M., 2017, TRAINING MIXTURE MOD; Maclaurin D, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P543; Mason J. C., 2002, CHEBYSHEV POLYNOMIAL; Minka T.P., 2001, P 17 C UNC ART INT, P362; Nishihara R, 2017, PROCEEDINGS OF THE 16TH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS 2017), P106, DOI 10.1145/3102980.3102998; Pakman A, 2017, PR MACH LEARN RES, V70; Pillai N. S., 2014, ERGODICITY APPROXIMA; Rabinovich M., 2015, VARIATIONAL CONSENSU; Scott Steven L, 2013, EFABBAYES 250 C, V16; Srivastava S, 2015, JMLR WORKSH CONF PRO, V38, P912; Stephanou M, 2017, ELECTRON J STAT, V11, P570, DOI 10.1214/17-EJS1245; Szego G, 1975, ORTHOGONAL POLYNOMIA; Teh Y. W., 2016, J MACH LEARN RES, V17, P193; TIERNEY L, 1986, J AM STAT ASSOC, V81, P82, DOI 10.2307/2287970; Vaart A. W., 1998, ASYMPTOTIC STAT; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; [No title captured]	41	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403066
C	Jiang, B; Tang, J; Ding, C; Gong, YH; Luo, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jiang, Bo; Tang, Jin; Ding, Chris; Gong, Yihong; Luo, Bin			Graph Matching via Multiplicative Update Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					As a fundamental problem in computer vision, graph matching problem can usually be formulated as a Quadratic Programming (QP) problem with doubly stochastic and discrete (integer) constraints. Since it is NP-hard, approximate algorithms are required. In this paper, we present a new algorithm, called Multiplicative Update Graph Matching (MPGM), that develops a multiplicative update technique to solve the QP matching problem. MPGM has three main benefits: (1) theoretically, MPGM solves the general QP problem with doubly stochastic constraint naturally whose convergence and KKT optimality are guaranteed. (2) Empirically, MPGM generally returns a sparse solution and thus can also incorporate the discrete constraint approximately. (3) It is efficient and simple to implement. Experimental results show the benefits of MPGM algorithm.	[Jiang, Bo; Tang, Jin; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China; [Ding, Chris] Univ Texas Arlington, CSE Dept, Arlington, TX 76019 USA; [Gong, Yihong] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian, Shaanxi, Peoples R China	Anhui University; University of Texas System; University of Texas Arlington; Xi'an Jiaotong University	Jiang, B (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.	jiangbo@ahu.edu.cn; tj@ahu.edu.cn; chqding@uta.edu; ygong@mail.xjtu.edu.cn; luobin@ahu.edu.cn	Jeong, Yongwook/N-7413-2016		NBRPC 973 Program [2015CB351705]; National Natural Science Foundation of China [61602001, 61671018, 61572030]; Natural Science Foundation of Anhui Province [1708085QF139]; Natural Science Foundation of Anhui Higher Education Institutions of China [KJ2016A020]; Co-Innovation Center for Information Supply & Assurance Technology, Anhui University; Open Projects Program of National Laboratory of Pattern Recognition	NBRPC 973 Program(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Anhui Province(Natural Science Foundation of Anhui Province); Natural Science Foundation of Anhui Higher Education Institutions of China(National Natural Science Foundation of China (NSFC)); Co-Innovation Center for Information Supply & Assurance Technology, Anhui University; Open Projects Program of National Laboratory of Pattern Recognition	This work is supported by the NBRPC 973 Program (2015CB351705); National Natural Science Foundation of China (61602001,61671018, 61572030); Natural Science Foundation of Anhui Province (1708085QF139); Natural Science Foundation of Anhui Higher Education Institutions of China (KJ2016A020); Co-Innovation Center for Information Supply & Assurance Technology, Anhui University; The Open Projects Program of National Laboratory of Pattern Recognition.	Adamczewski K, 2015, IEEE I CONF COMP VIS, P109, DOI 10.1109/ICCV.2015.21; Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228; Cour Timothee, 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0044; Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277; Ding C, 2008, IEEE DATA MINING, P183, DOI 10.1109/ICDM.2008.130; Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Jiang B, 2015, AAAI CONF ARTIF INTE, P3790; Jiang B, 2014, PATTERN RECOGN, V47, P736, DOI 10.1016/j.patcog.2013.08.024; Jiang Bo, 2017, AAAI; Lee DD, 2001, ADV NEUR IN, V13, P556; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu M., 2009, NIPS, P1114; Leordeanu M., 2011, INT J COMPUT VISION, V96, P1; Luo B, 2003, PATTERN RECOGN, V36, P2213, DOI 10.1016/S0031-3203(03)00084-0; Julian JM, 2012, PATTERN RECOGN, V45, P563, DOI 10.1016/j.patcog.2011.05.008; van Wyk BJ, 2004, IEEE T PATTERN ANAL, V26, P1526, DOI 10.1109/TPAMI.2004.95; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zass R., 2006, P ADV NEUR INF PROC, P1569; Zhang Z, 2016, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2016.135; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667	24	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403025
C	Jin, XJ; Xiao, HX; Shen, XH; Yang, JM; Lin, Z; Chen, Y; Jie, ZQ; Feng, JS; Yan, SC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jin, Xiaojie; Xiao, Huaxin; Shen, Xiaohui; Yang, Jimei; Lin, Zhe; Chen, Yunpeng; Jie, Zequn; Feng, Jiashi; Yan, Shuicheng			Predicting Scene Parsing and Motion Dynamics in the Future	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The ability of predicting the future is important for intelligent systems, e.g. autonomous vehicles and robots to plan early and make decisions accordingly. Future scene parsing and optical flow estimation are two key tasks that help agents better understand their environments as the former provides dense semantic information, i.e. what objects will be present and where they will appear, while the latter provides dense motion information, i.e. how the objects will move. In this paper, we propose a novel model to simultaneously predict scene parsing and optical flow in unobserved future video frames. To our best knowledge, this is the first attempt in jointly predicting scene parsing and motion dynamics. In particular, scene parsing enables structured motion prediction by decomposing optical flow into different groups while optical flow estimation brings reliable pixel-wise correspondence to scene parsing. By exploiting this mutually beneficial relationship, our model shows significantly better parsing and motion prediction results when compared to well-established baselines and individual prediction models on the large-scale Cityscapes dataset. In addition, we also demonstrate that our model can be used to predict the steering angle of the vehicles, which further verifies the ability of our model to learn latent representations of scene dynamics.	[Jin, Xiaojie] NUS, Grad Sch Integrat Sci & Engn NGS, Singapore, Singapore; [Xiao, Huaxin; Chen, Yunpeng; Feng, Jiashi; Yan, Shuicheng] NUS, Dept ECE, Singapore, Singapore; [Shen, Xiaohui; Yang, Jimei; Lin, Zhe] Adobe Res, Seattle, WA USA; [Jie, Zequn] Tencent AI Lab, Bellevue, WA USA; [Yan, Shuicheng] Qihoo AI 360 Inst, Beijing, Peoples R China	National University of Singapore; National University of Singapore; Adobe Systems Inc.	Jin, XJ (corresponding author), NUS, Grad Sch Integrat Sci & Engn NGS, Singapore, Singapore.		Jeong, Yongwook/N-7413-2016; Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022		National University of Singapore startup grant [R-263-000-C08-133]; Ministry of Education of Singapore AcRF Tier One grant [R-263-000-C21-112]; NUS IDS grant [R-263-000-C67-646]	National University of Singapore startup grant(National University of Singapore); Ministry of Education of Singapore AcRF Tier One grant(Ministry of Education, Singapore); NUS IDS grant	The work of Jiashi Feng was partially supported by National University of Singapore startup grant R-263-000-C08-133, Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112 and NUS IDS grant R-263-000-C67-646.	Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Baker Simon, 2007, 2007 11th IEEE International Conference on Computer Vision, P1; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Chen Liang-Chich, 2015, ABS14127062 CORR; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Evgeniou T., 2004, SIGKDD; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Fischer Philipp, 2015, ARXIV150406852, P2; Fouhey D. F., 2014, CVPR; García Herrera Arístides Lázaro, 2017, Rev.Med.Electrón., P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hotz G., 2016, ARXIV PREPRINT ARXIV; Jaeger H., 2002, APPROACH, V5; Jin X., 2017, ICCV; Jin X., 2017, AAAI; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lotter William, 2015, ARXIV151106380; Luo Z., 2017, ARXIV170101821; Mathieu M., 2016, INT C LEARN REPR ICL; Mester R, 2014, IEEE SW SYMP IMAG, P113, DOI 10.1109/SSIAI.2014.6806042; Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3; Neverova N., 2017, ARXIV170307684; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Patraucean Viorica, 2015, ARXIV151106309; Revaud J., 2015, CVPR; Roy A., 2014, CVPR; Schwing A. G., 2015, ARXIV PREPRINT ARXIV; Sevilla-Lara L., 2016, ARXIV160303911 CORR; Socher R., 2011, P 28 INT C INT C MAC, P129; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Villegas Ruben, 2017, ICLR 2017; Villegas Ruben, 2017, ICML; Walker J., 2015, ICCV; Walker J., 2014, CVPR; Yuen J., 2010, ECCV	37	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407001
C	Karimi, MR; Lucic, M; Hassani, N; Krause, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Karimi, Mohammad Reza; Lucic, Mario; Hassani, Named; Krause, Andreas			Stochastic Submodular Maximization: The Case of Coverage Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FUNCTION SUBJECT; APPROXIMATIONS; ALGORITHM	Stochastic optimization of continuous objectives is at the heart of modern machine learning. However, many important problems are of discrete nature and often involve submodular objectives. We seek to unleash the power of stochastic continuous optimization, namely stochastic gradient descent and its variants, to such discrete problems. We first introduce the problem of stochastic submodular optimization, where one needs to optimize a submodular objective which is given as an expectation. Our model captures situations where the discrete objective arises as an empirical risk (e.g., in the case of exemplar-based clustering), or is given as an explicit stochastic model (e.g., in the case of influence maximization in social networks). By exploiting that common extensions act linearly on the class of submodular functions, we employ projected stochastic gradient ascent and its variants in the continuous domain, and perform rounding to obtain discrete solutions. We focus on the rich and widely used family of weighted coverage functions. We show that our approach yields solutions that are guaranteed to match the optimal approximation guarantees, while reducing the computational cost by several orders of magnitude, as we demonstrate empirically.	[Karimi, Mohammad Reza; Lucic, Mario; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Hassani, Named] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Pennsylvania	Karimi, MR (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	mkarimi@ethz.ch; lucic@inf.ethz.ch; hassani@seas.upenn.edu; krausea@ethz.ch	Jeong, Yongwook/N-7413-2016		ERC [StG 307036]	ERC(European Research Council (ERC)European Commission)	The research was partially supported by ERC StG 307036. We would like to thank Yaron Singer for helpful comments and suggestions.	Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2; Bach F., 2010, ABS10104207 CORR; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Badanidiyuru A., 2014, P 25 ANN ACM SIAM S, P1497; BRUCKER P, 1984, OPER RES LETT, V3, P163, DOI 10.1016/0167-6377(84)90010-5; Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Duchi J., 2011, J MACHINE LEARNING R; Ene A, 2016, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2016.34; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46; FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195; Glance N., 2005, P 11 ACM SIGKDD INT, P419, DOI [10.1145/1081870.1081919, DOI 10.1145/1081870.1081919]; Gomez Ryan, 2010, P 27 INT C MACH LEAR; Hassidim Avinatan, 2016, ABS160103095 CORR; Horel T., 2016, NIPS; Iyer Rishabh K., 2013, P 8 C WORKSHOP NEURA, P2436; Iyer Rishabh K., 2015, ABS150607329 ARXIV C; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kingma D.P., 2015, INT C LEARN REPR, P1; Krause A., 2012, TRACTABILITY PRACT A, V3, P8; Krause A., 2005, P 21 C UNCERTAINTY A, P324, DOI DOI 10.5555/3020336.3020377; Krause Andreas, 2005, C UNC ART INT UAI JU; Kulik A, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P545; Kumar K. S. Sesh, 2016, HAL01161759V3; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Mirzasoleiman Baharan, 2015, LAZIER LAZY GREEDY; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748; Seeman L, 2013, ANN IEEE SYMP FOUND, P459, DOI 10.1109/FOCS.2013.56; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Singla Adish, 2016, P C ART INT AAAI FEB; Streeter M., 2008, NIPS; Vondrak J., 2007, SUBMODULARITY COMBIN; Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318; Vondrak J, 2008, ACM S THEORY COMPUT, P67; Wei K., 2014, INT C MACH LEARN ICM	39	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406088
C	Lam, RR; Willcox, KE		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lam, Remi R.; Willcox, Karen E.			Lookahead Bayesian Optimization with Inequality Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GLOBAL OPTIMIZATION	We consider the task of optimizing an objective function subject to inequality constraints when both the objective and the constraints are expensive to evaluate. Bayesian optimization (BO) is a popular way to tackle optimization problems with expensive objective function evaluations, but has mostly been applied to unconstrained problems. Several BO approaches have been proposed to address expensive constraints but are limited to greedy strategies maximizing immediate reward. To address this limitation, we propose a lookahead approach that selects the next evaluation in order to maximize the long-term feasible reduction of the objective function. We present numerical experiments demonstrating the performance improvements of such a lookahead approach compared to several greedy BO algorithms, including constrained expected improvement (EIC) and predictive entropy search with constraint (PESC).	[Lam, Remi R.; Willcox, Karen E.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Lam, RR (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	rlam@mit.edu; kwillcox@mit.edu	Willcox, Karen E/AAH-4519-2021		AFOSR MURI [FA9550-15-1-0038]	AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI)	This work was supported in part by the AFOSR MURI on multi-information sources of multi-physics systems under Award Number FA9550-15-1-0038, program manager Dr. Jean-Luc Cambier.	Adams R., 2015, P 32 INT C MACH LEAR; [Anonymous], 2014, ARXIV14035607; AUDET C, 2000, 4891 AIAA; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1; Bjorkman M, 2000, OPTIM ENG, V1, P373, DOI 10.1023/A:1011584207202; Buffoni M., 2010, 40 FLUID DYN C EXH, P5008; Feliot P, 2017, J GLOBAL OPTIM, V67, P97, DOI 10.1007/s10898-016-0427-3; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Ginsbourger D, 2010, CONTRIB STAT, P89, DOI 10.1007/978-3-7908-2410-0_12; Gonzalez-Hernandez J, 2016, J PEDIATR SURG, V51, P790, DOI 10.1016/j.jpedsurg.2016.02.024; Gramacy RB, 2016, TECHNOMETRICS, V58, P1, DOI 10.1080/00401706.2015.1014065; Gramacy Robert B, 2010, ARXIV10044027; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Lam RR, 2016, ADV NEUR IN, V29; Mockus J., 1978, APPL BAYESIAN METHOD, V2; Osborne M, 2009, LEARNING INTELLIGENT, P1; Picheny V., 2016, NIPS; Picheny V, 2014, JMLR WORKSH CONF PRO, V33, P787; Powell Warren B., 2011, APPROXIMATE DYNAMIC, V842; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Regis RG, 2014, ENG OPTIMIZ, V46, P218, DOI 10.1080/0305215X.2013.765000; SASENA MJ, 2001, CONSTRAINTS, V2, P5; Schonlau M., 1998, NEW DEV APPL EXPT DE, V34, P11, DOI [DOI 10.1214/LNMS/1215456182, 10.1214/lnms/1215456182]; [No title captured]; [No title captured]	27	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401089
C	Lamb, A; Hjelm, RD; Ganin, Y; Cohen, JP; Courville, A; Bengio, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lamb, Alex; Hjelm, R. Devon; Ganin, Yaroslav; Cohen, Joseph Paul; Courville, Aaron; Bengio, Yoshua			GibbsNet: Iterative Adversarial Inference for Deep Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Directed latent variable models that formulate the joint distribution as p(x, z) = p(z) p(x vertical bar z) have the advantage of fast and exact sampling. However, these models have the weakness of needing to specify p (z), often with a simple fixed prior that limits the expressiveness of the model. Undirected latent variable models discard the requirement that p(z) be specified with a prior, yet sampling from them generally requires an iterative procedure such as blocked Gibbs-sampling that may require many steps to draw samples from the joint distribution p(x, z). We propose a novel approach to learning the joint distribution between the data and a latent code which uses an adversarially learned iterative procedure to gradually refine the joint distribution, p(x, z), to better match with the data distribution on each step. GibbsNet is the best of both worlds both in theory and in practice. Achieving the speed and simplicity of a directed latent variable model, it is guaranteed (assuming the adversarial game reaches the virtual training criteria global minimum) to produce samples from p(x, z) with only a few sampling iterations. Achieving the expressiveness and flexibility of an undirected latent variable model, GibbsNet does away with the need for an explicit p(z) and has the ability to do attribute prediction, class-conditional generation, and joint image-attribute modeling in a single model which is not trained for any of these specific tasks. We show empirically that GibbsNet is able to learn a more complex p(z) and show that this leads to improved inpainting and iterative refinement of p(x, z) for dozens of steps and stable generation without collapse for thousands of steps, despite being trained on only a few steps.										Bengio Y., 2013, ABS13061091 CORR; Bengio Y., 2012, ABS12074404 CORR; Bornschein J., 2015, ABS150603877 CORR; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Donahue Jeff, 2017, INT C LEARN REPR ICL; Dumoulin Vincent, 2017, ICLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hjelm D., 2016, ADV NEURAL INFORM PR, P4691; Hjelm R Devon, 2017, ARXIV PREPRINT ARXIV; Huszar F., 2017, ARXIV E PRINTS; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lamb A., 2016, NEURAL INFORM PROCES, V2016; Larsen A.B.L., 2015, ABS151209300 CORR; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Salimans T, 2016, ADV NEUR IN, V29; Sohl-Dickstein Jascha, 2015, ABS150303585 CORR; Song J., 2017, ICLR WORKSH TRACK; Theis Lucas, 2015, ARXIV151101844; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294	26	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405017
C	Levy, KY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Levy, Kfir Y.			Online to Offline Conversions, Universality and Adaptive Minibatch Sizes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present an approach towards convex optimization that relies on a novel scheme which converts adaptive online algorithms into offline methods. In the offline optimization setting, our derived methods are shown to obtain favourable adaptive guarantees which depend on the harmonic sum of the queried gradients. We further show that our methods implicitly adapt to the objective's structure: in the smooth case fast convergence rates are ensured without any prior knowledge of the smoothness parameter, while still maintaining guarantees in the non-smooth setting. Our approach has a natural extension to the stochastic setting, resulting in a lazy version of SGD (stochastic GD), where minibathces are chosen adaptively depending on the magnitude of the gradients. Thus providing a principled approach towards choosing minibatch sizes.	[Levy, Kfir Y.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Levy, KY (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	yehuda.levy@inf.ethz.ch	Jeong, Yongwook/N-7413-2016	Levy, Kfir Yehuda/0000-0003-1236-2626	ETH Zurich Postdoctoral Fellowship; Marie Curie Actions for People COFUND program	ETH Zurich Postdoctoral Fellowship; Marie Curie Actions for People COFUND program	This work was supported by the ETH Zurich Postdoctoral Fellowship and Marie Curie Actions for People COFUND program.	Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658; Cotter A., 2011, P NEURIPS GRAN SPAIN; Dekel O, 2012, J MACH LEARN RES, V13, P165; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hazan E., 2012, P 29 INT C MACH LEAR, P807; Hazan E., 2015, ADV NEURAL INFORM PR, P1594; Jain P., 2016, ARXIV161003774; Juditsky Anatoli, 2008, ARXIV08090813; Kakade S., 2010, LECT NOTES MULTIVARI; Kingma D.P, P 3 INT C LEARNING R; Levin D. A., 2009, MARKOV CHAINS MIXING; Levy K. Y., 2016, ARXIV161104831; Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Mitrinovic D.S., 2013, MEANTHEIR INEQUALI, V31; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Shalev-Shwartz S., 2013, ADV NEURAL INFORM PR, P378; Takac M., 2015, ARXIV150708322; Tieleman T, 2012, COURSERA NEURAL NETW, V4; Zeiler M.D, 2012, CORR ABS12125701	27	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401063
C	Li, XG; Yang, LF; Ge, JS; Haupt, J; Zhang, T; Zhao, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Xingguo; Yang, Lin F.; Ge, Jason; Haupt, Jarvis; Zhang, Tong; Zhao, Tuo			On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MULTISTAGE CONVEX RELAXATION; GENERALIZED LINEAR-MODELS; VARIABLE SELECTION; LASSO; RATES	We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions. Our proposed algorithm integrates the proximal newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming, and enjoys both strong computational and statistical guarantees. Specifically, by leveraging a sophisticated characterization of sparse modeling structures (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations. Numerical experiments are provided to support our theory.	[Li, Xingguo; Haupt, Jarvis] Univ Minnesota, Minneapolis, MN 55455 USA; [Yang, Lin F.; Ge, Jason] Princeton Univ, Princeton, NJ 08544 USA; [Zhang, Tong] Tencent Lab, Bellevue, WA USA; [Li, Xingguo; Zhao, Tuo] Georgia Tech, Atlanta, GA 30332 USA	University of Minnesota System; University of Minnesota Twin Cities; Princeton University; University System of Georgia; Georgia Institute of Technology	Li, XG (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.; Li, XG; Zhao, T (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	lixx1661@umn.edu; tuo.zhao@isye.gatech.edu	Zhang, Tong/HGC-1090-2022		DARPA [YFA N66001-14-1-4047]; NSF [IIS-1447639]; Doctoral Dissertation Fellowship from University of Minnesota	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Doctoral Dissertation Fellowship from University of Minnesota	The authors acknowledge support from DARPA YFA N66001-14-1-4047, NSF Grant IIS-1447639, and Doctoral Dissertation Fellowship from University of Minnesota.	ARMIJO L, 1966, PAC J MATH, V16, P1, DOI 10.2140/pjm.1966.16.1; Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250; Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Boyd S, 2004, CONVEX OPTIMIZATION; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Cristiano A, 2015, P HIPEAC WAPCO AMST; Eloyan A, 2012, FRONT SYST NEUROSCI, V6, DOI 10.3389/fnsys.2012.00061; Fan J., 2015, ARXIV150701037; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Guyon I., 2005, ADV NEURAL INFORM PR, V17, P545; Lee JD, 2014, SIAM J OPTIMIZ, V24, P1420, DOI 10.1137/130921428; Li X., 2016, ARXIV160507950; Li XG, 2016, PR MACH LEARN RES, V48; Li Xingguo, 2015, PICASSO PACKAGE NONC; Loh Po-Ling, 2015, J MACHINE LEARNING R; LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025; Mairal J, 2012, FOUND TRENDS COMPUT, V8, DOI 10.1561/0600000058; MCCULLAGH P, 1984, EUR J OPER RES, V16, P285, DOI 10.1016/0377-2217(84)90282-0; Neale BM, 2012, NATURE, V485, P242, DOI 10.1038/nature11011; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Ning Y., 2014, ARXIV14122295; Pfanzagl J., 1994, PARAMETRIC STAT THEO; Raginsky M, 2010, IEEE T SIGNAL PROCES, V58, P3990, DOI 10.1109/TSP.2010.2049997; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Schraudolph NN, 1999, NEURAL COMPUT, V11, P853, DOI 10.1162/089976699300016467; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; van de Geer SA, 2008, ANN STAT, V36, P614, DOI 10.1214/009053607000000929; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997; Yang Y, 2013, J COMPUT GRAPH STAT, V22, P396, DOI 10.1080/10618600.2012.680324; Yen Ian En-Hsu, 2014, ADV NEURAL INFORM PR, P1008; Yue M.-C., 2016, ARXIV160507522; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang T, 2013, BERNOULLI, V19, P2277, DOI 10.3150/12-BEJ452; Zhang T, 2010, J MACH LEARN RES, V11, P1081; Zhao T., 2014, ARXIV14127477; Zhao T, 2012, J MACH LEARN RES, V13, P1059; ZHOU S, 2009, ARXIV09124045	44	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402077
C	Locatello, F; Tschannen, M; Ratsch, G; Jaggi, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Locatello, Francesco; Tschannen, Michael; Raetsch, Gunnar; Jaggi, Martin			Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NONNEGATIVE MATRIX; SPARSE SOLUTIONS	Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (O(1/t)) convergence on general smooth and convex objectives, and linear convergence (O(e(-t))) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.	[Locatello, Francesco] Swiss Fed Inst Technol, MPI Intelligent Syst, Zurich, Switzerland; [Tschannen, Michael; Raetsch, Gunnar] Swiss Fed Inst Technol, Zurich, Switzerland; [Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Max Planck Society; Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Locatello, F (corresponding author), Swiss Fed Inst Technol, MPI Intelligent Syst, Zurich, Switzerland.	locatelf@ethz.ch; michaelt@nari.ee.ethz.ch; raetsch@inf.ethz.ch; martin.jaggi@epfl.ch	Locatello, Francesco/GQY-6025-2022; Jeong, Yongwook/N-7413-2016	Jaggi, Martin/0000-0003-1579-5558; Locatello, Francesco/0000-0002-4850-0683; Ratsch, Gunnar/0000-0001-5486-8532				Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], [No title captured]; Araujo MCU, 2001, CHEMOMETR INTELL LAB, V57, P65, DOI 10.1016/S0169-7439(01)00119-8; Behr J, 2013, BIOINFORMATICS, V29, P2529, DOI 10.1093/bioinformatics/btt442; Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006; Bruckstein AM, 2008, IEEE T INFORM THEORY, V54, P4813, DOI 10.1109/TIT.2008.929920; Buhlmann P, 2005, SEM STAT ETH ZUR; Buhlmann P, 2010, WIRES COMPUT STAT, V2, P69, DOI 10.1002/wics.55; Burger Martin, 2003, INFINITE DIMENSIONAL; CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472; Cichocki A, 2009, IEICE T FUND ELECTR, VE92A, P708, DOI 10.1587/transfun.E92.A.708; Esser E, 2013, SIAM J IMAGING SCI, V6, P2010, DOI 10.1137/13090540X; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gillis N, 2015, IEEE T GEOSCI REMOTE, V53, P2066, DOI 10.1109/TGRS.2014.2352857; Gillis N, 2014, SIAM J IMAGING SCI, V7, P1420, DOI 10.1137/130946782; Gillis Nicolas, 2016, ARXIV161001349; Grubb Alexander, 2011, ICML, P1209; Guo XW, 2017, AAAI CONF ARTIF INTE, P1948; Hanson R.J., 1995, SOLVING LEAST SQUARE, V15; Harchaoui Z, 2015, MATH PROGRAM, V152, P75, DOI 10.1007/s10107-014-0778-9; Hsieh C.J., 2011, P 17 ACM SIGKDD INT, P1064; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Kim H, 2007, PROCEEDINGS OF THE 7TH IEEE INTERNATIONAL SYMPOSIUM ON BIOINFORMATICS AND BIOENGINEERING, VOLS I AND II, P1147; Kim J., 2012, HIGH PERFORMANCE SCI, P311, DOI DOI 10.1007/978-1-4471-2437-5_16; Kim J, 2014, J GLOBAL OPTIM, V58, P285, DOI 10.1007/s10898-013-0035-4; Kopriva I, 2010, LECT NOTES COMPUT SC, V6365, P490, DOI 10.1007/978-3-642-15995-4_61; Kumar A., 2013, P INT C MACH LEARN, P231; Lacoste-Julien S., 2013, ARXIV13127864; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Laue Soren, 2012, ICML; Lee DD, 2001, ADV NEUR IN, V13, P556; Locatello F, 2017, PR MACH LEARN RES, V54, P860; Makalic E, 2011, LECT NOTES ARTIF INT, V7106, P82, DOI 10.1007/978-3-642-25832-9_9; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Meir R., 2003, Advanced Lectures on Machine Learning. Machine Learning Summer School 2002. Revised Lectures. (Lecture Notes in Artificial Intelligence Vol.2600), P118; Nascimento JMP, 2005, IEEE T GEOSCI REMOTE, V43, P898, DOI 10.1109/TGRS.2005.844293; Peharz Robert, 2010, Proceedings of the 2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP), P83, DOI 10.1109/MLSP.2010.5589219; Pena J, 2017, MATH PROGRAM, V166, P87, DOI 10.1007/s10107-016-1105-4; Pena Javier, 2015, ARXIV151206142; Pogorelov A.V, 1973, EXTRINSIC GEOMETRY C, V35; Ratsch Gunnar, 2001, NIPS, P487; Samarasekera ACJ, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P1, DOI 10.1109/ComManTel.2014.6825568; Sha F., 2002, ADV NEURAL INFORM PR, V15, P1041; Shalev-Shwartz S, 2010, SIAM J OPTIMIZ, V20, P2807, DOI 10.1137/090759574; Shashua A., 2005, P 22 INT C MACHINE L, P792, DOI [10.1145/1102351.1102451, DOI 10.1145/1102351.1102451]; Temlyakov VN, 2015, CONSTR APPROX, V41, P269, DOI 10.1007/s00365-014-9272-0; Temlyakov V, 2014, CONF REC ASILOMAR C, P1331, DOI 10.1109/ACSSC.2014.7094676; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; WANG Z, 2014, ICML, P91; Welling M, 2001, PATTERN RECOGN LETT, V22, P1255, DOI 10.1016/S0167-8655(01)00070-8; Yaghoobi M, 2015, IEEE SIGNAL PROC LET, V22, P1229, DOI 10.1109/LSP.2015.2393637; Yang Yuning, 2015, HIGHER ORDER MATCHIN; Yao Quanming, 2016, IJCAI; Yuan XT, 2013, J MACH LEARN RES, V14, P899	55	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400074
C	Royer, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Royer, Martin			Adaptive Clustering through Semidefinite Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We analyze the clustering problem through a flexible probabilistic model that aims to identify an optimal partition on the sample X-1 , ..., X-n. We perform exact clustering with high probability using a convex semidefinite estimator that interprets as a corrected, relaxed version of K-means. The estimator is analyzed through a non-asymptotic framework and showed to be optimal or near-optimal in recovering the partition. Furthermore, its performances are shown to be adaptive to the problem's effective dimension, as well as to K the unknown number of groups in this partition. We illustrate the method's performances in comparison to other classical clustering algorithms with numerical experiments on simulated high-dimensional data.	[Royer, Martin] Univ Paris Sud, Univ Paris Saclay, Lab Math Orsay, CNRS, F-91405 Orsay, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay	Royer, M (corresponding author), Univ Paris Sud, Univ Paris Saclay, Lab Math Orsay, CNRS, F-91405 Orsay, France.	martin.royer@math.u-psud.fr	Jeong, Yongwook/N-7413-2016		French National research Agency (ANR), "Investissement d'Avenir" program, through the "IDI 2015" project - IDEX Paris-Saclay [ANR-11-IDEX-0003-02]; CNRS PICS funding HighClust	French National research Agency (ANR), "Investissement d'Avenir" program, through the "IDI 2015" project - IDEX Paris-Saclay(French National Research Agency (ANR)); CNRS PICS funding HighClust	This work is supported by a public grant overseen by the French National research Agency (ANR) as part of the "Investissement d'Avenir" program, through the "IDI 2015" project funded by the IDEX Paris-Saclay, ANR-11-IDEX-0003-02. It is also supported by the CNRS PICS funding HighClust. We thank Christophe Giraud for a shrewd, unwavering thesis direction.	Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Azizyan M., 2013, ADV NEURAL INFORM PR, V26, P2139; Banks J, 2016, ARXIV160705222; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Bunea F., 2016, ARXIV160605100; BUNEA F., 2015, ARXIV150801939; Chen YY, 2016, BMC MOL BIOL, V17, DOI 10.1186/s12867-016-0055-y; Chretien S., 2016, ABS160609190 CORR; Dasgupta S, 2007, J MACH LEARN RES, V8, P203; Fei Y., 2017, ARXIV170508391; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; Guedon O., 2014, ABS14114686 CORR; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Mixon Dustin G., 2016, 2016 IEEE Information Theory Workshop (ITW), P211, DOI 10.1109/ITW.2016.7606826; Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983; Royer Martin, 2017, ADMM IMPLEMENTATION; Steinhaus Hugo, 1957, B ACAD POL SCI, V4, P801; Verzelen N., 2014, ARXIV14051478; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967; Yan B., 2016, ARXIV160702675	24	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401080
C	Rudolph, M; Ruiz, F; Athey, S; Blei, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rudolph, Maja; Ruiz, Francisco; Athey, Susan; Blei, David			Structured Embedding Models for Grouped Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S - EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S - EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data.	[Rudolph, Maja; Blei, David] Columbia Univ, New York, NY 10027 USA; [Ruiz, Francisco] Univ Cambridge, Cambridge, England; [Athey, Susan] Stanford Univ, Stanford, CA 94305 USA	Columbia University; University of Cambridge; Stanford University	Rudolph, M (corresponding author), Columbia Univ, New York, NY 10027 USA.	maja@cs.columbia.edu	Jeong, Yongwook/N-7413-2016	Athey, Susan/0000-0001-6934-562X	NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA PPAML [FA8750-14-2-0009]; DARPA SIMPLEX [N66001-15-C-4032]; Alfred P. Sloan Foundation; John Simon Guggenheim Foundation; EU [706760]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA PPAML; DARPA SIMPLEX; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); John Simon Guggenheim Foundation; EU(European Commission)	We thank Elliott Ash and Suresh Naidu for the helpful discussions and for sharing the Senate speeches. This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C-4032, the Alfred P. Sloan Foundation, and the John Simon Guggenheim Foundation. Francisco J. R. Ruiz is supported by the EU H2020 programme (Marie Sklodowska-Curie grant agreement 706760).	Abadi M, 2015, P 12 USENIX S OPERAT; Ammar Waleed, 2016, ARXIV160201925; Arnold BC, 2001, STAT SCI, V16, P249; Arora Sanjeev, 2015, ARXIV150203520; Bamler R, 2017, PR MACH LEARN RES, V70; Barkan O., 2016, 2016 IEEE 26 INT WOR, P1, DOI DOI 10.1109/MLSP.2016.7738886; Bengio Y, 2001, ADV NEUR IN, V13, P932; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Gelman A., 2003, BAYESIAN DATA ANAL, DOI 10.1201/b16018; Gershman SJ, 2014, P 36 ANN C COGN SCI; Glorot X., 2010, PROC MACH LEARN RES, P249; Hamilton WL, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1489; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Kim Yoon., 2014, P ACL 2014 WORKSHOP, P61; Kingma D.P, P 3 INT C LEARNING R; Klementiev Alexandre, 2012, INDUCING CROSSLINGUA; Korattikara A., 2015, ADV NEURAL INFORM PR; Kulkarni V, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P625, DOI 10.1145/2736277.2741627; Levy O, 2014, ADV NEUR IN, V27; Mikolov T., 2013, ICLR WORKSH P; Mikolov T., 2013, P 2013 C N AM CHAPTE, P746, DOI DOI 10.3109/10826089109058901; Mikolov T., 2013, ARXIV PREPRINT ARXIV; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Nasrabadi NM, 2006, PATTERN RECOGN; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rudolph M., 2016, P ANN C NEURAL INFOR, P478; Rudolph Maja, 2017, ARXIV170308052; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yao Zijun, 2017, ARXIV170300607; Zou Will Y, 2013, P 2013 C EMP METH NA, P1393	36	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400024
C	Saatchi, Y; Wilson, AG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Saatchi, Yunus; Wilson, Andrew Gordon			Bayesian GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as label smoothing or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.	[Saatchi, Yunus] Uber AI Labs, San Francisco, CA 94107 USA; [Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Saatchi, Y (corresponding author), Uber AI Labs, San Francisco, CA 94107 USA.				NSF [IIS-1563887]	NSF(National Science Foundation (NSF))	We thank Pavel Izmailov and Ben Athiwaratkun for helping to create a tutorial for the codebase, helpful comments and validation. We also thank Soumith Chintala for helpful advice. We thank NSF IIS-1563887 for support.	BORG I., 2005, MODERN MULTIDIMENSIO, P207; Bottou L., 2017, ARXIV170107875STATML; Chen T., 2014, P INT C MACH LEARN; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Karaletsos T., 2016, ARXIV161205048; Kingma DP, 2 INT C LEARN REPR I, P1; Krizhevsky A., 2010, CIFAR 10; Radford A., 2015, P COMP C; Salimans T., 2016, ABS160603498 CORR; Tran D, 2017, ADV NEUR IN, V30; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wilson A. G., 2016, ADV NEURAL INFORM PR, P2586; Wilson A.G., 2016, ARTIFICIAL INTELLIGE	14	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403067
C	Scarlett, J; Cevher, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Scarlett, Jonathan; Cevher, Volkan			Phase Transitions in the Pooled Data Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RECOVERY; MODELS	In this paper, we study the pooled data problem of identifying the labels associated with a large collection of items, based on a sequence of pooled tests revealing the counts of each label within the pool. In the noiseless setting, we identify an exact asymptotic threshold on the required number of tests with optimal decoding, and prove a phase transition between complete success and complete failure. In addition, we present a novel noisy variation of the problem, and provide an information-theoretic framework for characterizing the required number of tests for general random noise models. Our results reveal that noise can make the problem considerably more difficult, with strict increases in the scaling laws even at low noise levels. Finally, we demonstrate similar behavior in an approximate recovery setting, where a given number of errors is allowed in the decoded labels.	[Scarlett, Jonathan; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Scarlett, J (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.	jonathan.scarlett@epfl.ch; volkancevher@epfl.ch	Jeong, Yongwook/N-7413-2016; Scarlett, Jonathan/AGK-0892-2022		European Commission [CRSII2-147633, SNF 200021-146750]; EPFL Fellows Horizon2020 [665667]	European Commission(European CommissionEuropean Commission Joint Research Centre); EPFL Fellows Horizon2020	This work was supported in part by the European Commission under Grant ERC Future Proof, SNF Sinergia project CRSII2-147633, SNF 200021-146750, and EPFL Fellows Horizon2020 grant 665667.	Aksoylar C, 2017, IEEE T INFORM THEORY, V63, P749, DOI 10.1109/TIT.2016.2605122; Alaoui A. E., 2017, DECODING POOLED DATA; Alaoui A. E., 2016, DECODING POOLED DATA; Atia GK, 2012, IEEE T INFORM THEORY, V58, P1880, DOI 10.1109/TIT.2011.2178156; Chen W. -N., 2017, IEEE INT S INF THEOR; Cover TM, 2006, ELEMENTS INFORM THEO; Csiszar Imre, 2011, INFORM THEORY CODING, VSecond; DUCHI J., 2013, DISTANCE BASED CONTI; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Hwang F., 1993, SERIES APPL MATH, DOI [10.1142/1936, DOI 10.1142/1936]; Malyutov M., 1998, RANDOM OPER STOCHAST, V6, P339; MALYUTOV MB, 1978, MATH NOTES+, V23, P84, DOI 10.1007/BF01104893; Massey J., 1988, INT WORKSH INF THEOR; Reeves G, 2013, IEEE T INFORM THEORY, V59, P3451, DOI 10.1109/TIT.2013.2253852; Reeves G, 2012, IEEE T INFORM THEORY, V58, P3065, DOI 10.1109/TIT.2012.2184848; Scarlett J., 2016, P ACM SIAM S DISC AL; Scarlett J., 2017, IEEE INT C AC SP SIG; Scarlett J, 2017, IEEE T INFORM THEORY, V63, P593, DOI 10.1109/TIT.2016.2606605; Scarlett J, 2016, IEEE T SIGNAL INF PR, V2, P625, DOI 10.1109/TSIPN.2016.2596439; SEBO A, 1985, J STAT PLAN INFER, V11, P23, DOI 10.1016/0378-3758(85)90022-9; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Wang I. -H., 2016, ALL C COMM CONTR COM; Wang IH, 2016, IEEE INT SYMP INFO, P1386, DOI 10.1109/ISIT.2016.7541526	23	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400036
C	Thewlis, J; Bilen, H; Vedaldi, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Thewlis, James; Bilen, Hakan; Vedaldi, Andrea			Unsupervised learning of object frames by dense equivariant image labelling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.	[Thewlis, James; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England; [Bilen, Hakan] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Oxford; University of Edinburgh	Thewlis, J (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford, England.	jdt@robots.ox.ac.uk; hbilen@ed.ac.uk; vedaldi@robots.ox.ac.uk	Bilen, Hakan/AAG-3202-2022; Bilen, Hakan/ACY-3128-2022		AIMS CDT (EPSRC) [EP/L015897/1]; ERC [677195-IDIU]	AIMS CDT (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ERC(European Research Council (ERC)European Commission)	This work acknowledges the support of the AIMS CDT (EPSRC EP/L015897/1) and ERC 677195-IDIU. Clipart: FreePik.	Agrawal P., 2015, P ICCV; [Anonymous], 2017, P ICLR; [Anonymous], [No title captured]; [Anonymous], 2016, P ICLR; Bengio Y., 2009, FDN TRENDS MACHINE L; Bookstein F., 1989, PAMI; Bourlard H., 1988, BIOL CYBERNETICS; Burgos-Artizzu XP, 2013, P ICCV; Cootes T. F., 1995, CVIU; Dalal N., 2005, P CVPR; Darrell Trevor., 2017, P ICLR; Doersch C., 2015, P ICCV; Felzenszwalb P. F., 2010, PAMI; FERGUS R, 2003, P CVPR; Fernando Basura, 2017, P CVPR; Fischer P., 2015, P ICCV; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hinton G. E., 2006, SCIENCE; Horn B. K., 1981, ARTIFICIAL INTELLIGE, V17; Ilg Eddy, 2016, ARXIV; Jaderberg M., 2015, P NIPS; Kanazawa A., 2016, P CVPR; Kemelmacher-Shlizerman Ira, 2012, P CVPR; Koestinger M., 2011, 1 IEEE INT WORKSH BE; Learned-Miller Erik G, 2006, IEEE T PATTERN ANAL; Liu Ce, 2011, PAMI, P3; Liu Z., 2015, P ICCV; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Misra I., 2016, P ECCV; Mobahi Hossein, 2014, P CVPR; Newcombe R. A., 2015, P CVPR; Novotny D., 2017, P ICCV; Pathak D., 2016, P CVPR; Pathak Deepak, 2017, P CVPR; Rocco I., 2017, P CVPR; Schmidt T, 2017, IEEE ROBOT AUTOM LET, V2, P420, DOI 10.1109/LRA.2016.2634089; Sun Y., 2013, P CVPR; Tassa Yuval, CAPSIM MATLAB PHYS E; Thewlis J, 2016, BMVC; Thewlis J., 2017, P ICCV; Weber Markus, 2000, P CVPR; Xiao Shengtao, 2016, P ECCV; Yu Xiang, 2016, P ECCV; Zhang J., 2014, P ECCV; Zhang R., 2016, P ECCV; Zhang W., 2008, P ECCV; Zhang Z., 2014, P ECCV; Zhang Zhanpeng, 2016, PAMI; Zhou T., 2015, P CVPR; Zhou T., 2017, P CVPR; Zhou Tinghui, 2016, P CVPR	52	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400081
C	Wu, G; Say, B; Sanner, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Ga; Say, Buser; Sanner, Scott			Scalable Planning with Tensorflow for Hybrid Nonlinear Domains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains. Furthermore, we remark that Tensorflow is highly scalable, converging to a strong plan on a large-scale concurrent domain with a total of 576,000 continuous action parameters distributed over a horizon of 96 time steps and 100 parallel instances in only 4 minutes. We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradient problems. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optimized toolkits like Tensorflow.	[Wu, Ga; Say, Buser; Sanner, Scott] Univ Toronto, Dept Mech & Ind Engn, Toronto, ON, Canada	University of Toronto	Wu, G (corresponding author), Univ Toronto, Dept Mech & Ind Engn, Toronto, ON, Canada.	wuga@mie.utoronto.ca; bsay@mie.utoronto.ca; ssanner@mie.utoronto.ca	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Agarwal Y., 2010, P 2 ACM WORKSHOP EMB, P1, DOI [DOI 10.1145/1878431.1878433, 10.1145/1878431.1878433]; Balduzzi David, 2016, ARXIV161102345; Bryce D, 2015, AAAI CONF ARTIF INTE, P3247; Cashmore M, 2016, P I C AUTOMAT PLAN S, P79; Coles A, 2013, J ARTIF INTELL RES, V46, P343, DOI 10.1613/jair.3788; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Erickson V. L., 2009, P 1 ACM WORKSH EMB S, P19; Faulwasser T, 2009, LECT NOTES CONTR INF, V384, P335, DOI 10.1007/978-3-642-01094-1_28; Ivankovic F, 2014, P I C AUTOMAT PLAN S, P145; Keller Thomas, 2013, P 23 INT C AUT PLANN; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Linnainmaa S., 1970, REPRESENTATION CUMUL; Lohr J., 2012, P 22 INT C AUT PLANN; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Piotrowski W, 2016, AAAI CONF ARTIF INTE, P4254; Rumelhart David E, COGNITIVE MODELING, V5, P1; Say B, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P750; SCALA E, 2016, ECAI, V285, P655, DOI DOI 10.3233/978-1-61499-672-9-655; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Weinstein Ari, 2012, P 22 INT C AUT PLANN; YEH WWG, 1985, WATER RESOUR RES, V21, P1797, DOI 10.1029/WR021i012p01797; Zeiler Matthew D, 2012, ARXIV12125701	28	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406034
C	Xia, F; Zhang, MJ; Zou, J; Tse, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xia, Fei; Zhang, Martin J.; Zou, James; Tse, David			NeuralFDR: Learning Discovery Thresholds from Hypothesis Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					As datasets grow richer, an important challenge is to leverage the full features in the data to maximize the number of useful discoveries while controlling for false positives. We address this problem in the context of multiple hypotheses testing, where for each hypothesis, we observe a p-value along with a set of features specific to that hypothesis. For example, in genetic association studies, each hypothesis tests the correlation between a variant and the trait. We have a rich set of features for each variant (e.g. its location, conservation, epigenetics etc.) which could inform how likely the variant is to have a true association. However popular empirically-validated testing approaches, such as Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting (IHW), either ignore these features or assume that the features are categorical or uni-variate. We propose a new algorithm, NeuralFDR, which automatically learns a discovery threshold as a function of all the hypothesis features. We parametrize the discovery threshold as a neural network, which enables flexible handling of multi-dimensional discrete and continuous features as well as efficient end-to-end optimization. We prove that NeuralFDR has strong false discovery rate (FDR) guarantees, and show that it makes substantially more discoveries in synthetic and real datasets. Moreover, we demonstrate that the learned discovery threshold is directly interpretable.	[Xia, Fei; Zhang, Martin J.; Zou, James; Tse, David] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Xia, F (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	feixia@stanford.edu; jinye@stanford.edu; jamesz@stanford.edu; dntse@stanford.edu	Xia, Fei/AAW-8782-2021; Jeong, Yongwook/N-7413-2016; Zhang, Martin Jinye/AAF-2206-2019	Xia, Fei/0000-0003-4343-1444; Zhang, Martin Jinye/0000-0003-0006-2466				Ardlie KG, 2015, SCIENCE, V348, P648, DOI 10.1126/science.1262110; Arias-Castro E, 2017, ELECTRON J STAT, V11, P1983, DOI 10.1214/17-EJS1277; Arjovsky M., 2017, ARXIV170107875; Barber, 2016, ARXIV160607926; Benjamini Y, 1997, SCAND J STAT, V24, P407, DOI 10.1111/1467-9469.00072; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Boca Simina M, 2015, BIORXIV; Duchi J, 2011, J MACH LEARN RES, V12, P2121; DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330; Efron B, 2008, ANN APPL STAT, V2, P197, DOI 10.1214/07-AOAS141; Genovese CR, 2006, BIOMETRIKA, V93, P509, DOI 10.1093/biomet/93.3.509; Himes BE, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0099625; HOLM S, 1979, SCAND J STAT, V6, P65; Hu JX, 2010, J AM STAT ASSOC, V105, P1215, DOI 10.1198/jasa.2010.tm09329; IGNATIADIS N., 2017, ARXIV170105179; Ignatiadis N, 2016, NAT METHODS, V13, P577, DOI [10.1038/NMETH.3885, 10.1038/nmeth.3885]; Lei L., 2017, ARXIV171002776; Lei LH, 2016, PR MACH LEARN RES, V48; Lei Lihua, 2016, ARXIV160906035; Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8; Storey JD, 2004, J R STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x	21	4	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401056
C	Yi, JF; Hsieh, CJ; Varshney, KR; Zhang, LJ; Li, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yi, Jinfeng; Hsieh, Cho-Jui; Varshney, Kush R.; Zhang, Lijun; Li, Yao			Scalable Demand-Aware Recommendation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to rating data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world datasets.	[Yi, Jinfeng; Varshney, Kush R.] IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Hsieh, Cho-Jui; Li, Yao] Univ Calif Davis, Davis, CA 95616 USA; [Zhang, Lijun] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China; [Yi, Jinfeng] Tencent AI Lab, Bellevue, WA 98004 USA	International Business Machines (IBM); University of California System; University of California Davis; Nanjing University	Yi, JF (corresponding author), IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.; Yi, JF (corresponding author), Tencent AI Lab, Bellevue, WA 98004 USA.	jinfengyi.ustc@gmail.com; chohsieh@ucdavis.edu; krvarshn@us.ibm.com; zhanglj@lamda.nju.edu.cn; yaoli@ucdavis.edu	Jeong, Yongwook/N-7413-2016	Li, Yao/0000-0002-7195-5774	NSF [IIS-1719097]; Nvidia; TACC	NSF(National Science Foundation (NSF)); Nvidia; TACC	Cho-Jui Hsieh and Yao Li acknowledge the support of NSF IIS-1719097, TACC and Nvidia.	Adomavicius G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P217, DOI 10.1007/978-0-387-85820-3_7; [Anonymous], 2008, P 25 INT C MACH LEAR; Baltrunas L., 2009, WORKSH CONT AW REC S, P25; Bing L, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P179, DOI 10.1109/icdm.2003.1250918; Bobadilla J, 2012, KNOWL-BASED SYST, V26, P225, DOI 10.1016/j.knosys.2011.07.021; Campos PG, 2014, USER MODEL USER-ADAP, V24, P67, DOI 10.1007/s11257-012-9136-x; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; CHATFIELD C, 1973, J AM STAT ASSOC, V68, P828, DOI 10.2307/2284508; Du Nan, 2015, NIPS, P3474; du Plessis MC, 2014, ADV NEUR IN, V27; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2445; Hsieh Cho-Jui, 2014, ICML; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677; Lee Dokyun, 2014, P INT C INF SYST AUC; Li B., 2011, P 22 INT JOINT C ART, P2293, DOI DOI 10.5555/2283696.2283780; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Narita A, 2011, LECT NOTES ARTIF INT, V6912, P501, DOI 10.1007/978-3-642-23783-6_32; Rendle S., 2009, BPR BAYESIAN PERSONA, P452; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699; Sexton Robert L., 2013, EXPLORING EC; STEINER RL, 1976, J MARKETING, V40, P2, DOI 10.2307/1249988; Sun JZ, 2014, IEEE T SIGNAL PROCES, V62, P3499, DOI 10.1109/TSP.2014.2326618; Wang YC, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1265, DOI 10.1145/2783258.2783395; Xiong L., 2010, P SDM COL OH, P211; Yi Jinfeng, 2013, 1 AAAI C HUM COMP CR 1 AAAI C HUM COMP CR	31	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402045
C	Zhuang, CX; Kubilius, J; Hartmann, M; Yamins, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhuang, Chengxu; Kubilius, Jonas; Hartmann, Mitra; Yamins, Daniel			Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RAT BARREL CORTEX; OBJECT LOCALIZATION; LAYER 2/3; RESPONSES; VIBRISSA	In large part, rodents "see" the world through their whiskers, a powerful tactile sense enabled by a series of brain areas that form the whisker-trigeminal system. Raw sensory data arrives in the form of mechanical input to the exquisitely sensitive, actively-controllable whisker array, and is processed through a sequence of neural circuits, eventually arriving in cortical regions that communicate with decision-making and memory areas. Although a long history of experimental studies has characterized many aspects of these processing stages, the computational operations of the whisker-trigeminal system remain largely unknown. In the present work, we take a goal-driven deep neural network (DNN) approach to modeling these computations. First, we construct a biophysically-realistic model of the rat whisker array. We then generate a large dataset of whisker sweeps across a wide variety of 3D objects in highly-varying poses, angles, and speeds. Next, we train DNNs from several distinct architectural families to solve a shape recognition task in this dataset. Each architectural family represents a structurally-distinct hypothesis for processing in the whisker-trigeminal system, corresponding to different ways in which spatial and temporal information can be integrated. We find that most networks perform poorly on the challenging shape recognition task, but that specific architectures from several families can achieve reasonable performance levels. Finally, we show that Representational Dissimilarity Matrices (RDMs), a tool for comparing population codes between neural systems, can separate these higher-performing networks with data of a type that could plausibly be collected in a neurophysiological or imaging experiment. Our results are a proof-of-concept that DNN models of the whisker-trigeminal system are potentially within reach.	[Zhuang, Chengxu] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA; [Kubilius, Jonas] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA; [Kubilius, Jonas] Katholieke Univ Leuven, Brain & Cognit, Leuven, Belgium; [Hartmann, Mitra] Northwestern Univ, Dept Biomed Engn, Evanston, IL 60208 USA; [Hartmann, Mitra] Northwestern Univ, Dept Mech Engn, Evanston, IL 60208 USA; [Yamins, Daniel] Stanford Univ, Stanford Neurosci Inst, Dept Psychol, Stanford, CA 94305 USA; [Yamins, Daniel] Stanford Univ, Stanford Neurosci Inst, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University; Massachusetts Institute of Technology (MIT); KU Leuven; Northwestern University; Northwestern University; Stanford University; Stanford University	Zhuang, CX (corresponding author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.	chengxuz@stanford.edu; qbilius@mit.edu; hartmann@northwestern.edu; yamins@stanford.edu	Hartmann, Mitra/B-6766-2009; Jeong, Yongwook/N-7413-2016		James S. McDonnell Foundation Award [220020469]; NSF Robust Intelligence grant [1703161]; European Union's Horizon 2020 research and innovation programme [705498]; NSF [IOS-0846088, IOS-1558068]	James S. McDonnell Foundation Award; NSF Robust Intelligence grant(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); European Union's Horizon 2020 research and innovation programme; NSF(National Science Foundation (NSF))	This project has sponsored in part by hardware donation from the NVIDIA Corporation, a James S. McDonnell Foundation Award (No. 220020469) and an NSF Robust Intelligence grant (No. 1703161) to DLKY, the European Union's Horizon 2020 research and innovation programme (No. 705498) to JK, and NSF awards (IOS-0846088 and IOS-1558068) to MJZH.	[Anonymous], 2009, SCHOLARPEDIA, DOI DOI 10.4249/SCHOLARPEDIA.7454; Arabzadeh E, 2005, PLOS BIOL, V3, P155, DOI 10.1371/journal.pbio.0030017; ARMSTRONGJAMES M, 1992, J NEUROPHYSIOL, V68, P1345, DOI 10.1152/jn.1992.68.4.1345; Berger J, 2014, PHILOS PSYCHOL, V27, P829, DOI 10.1080/09515089.2013.771241; Bosman LWJ, 2011, FRONT INTEGR NEUROSC, V5, DOI 10.3389/fnint.2011.00053; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Chang Angel X., 2015, ARXIV151203012CSGR P; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Diamond ME, 2008, NAT REV NEUROSCI, V9, P601, DOI 10.1038/nrn2411; Ebara S, 2002, J COMP NEUROL, V449, P103, DOI 10.1002/cne.10277; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8; Hartmann M. J. Z., 2015, SCHOLARPEDIA, V10, P6636, DOI [10. 4249/ scholarpedia. 6636, DOI 10.4249/SCH0LARPEDIA.6636]; Hobbs JA, 2016, FRONT BEHAV NEUROSCI, V9, DOI 10.3389/fnbeh.2015.00356; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huet LA, 2016, IEEE T HAPTICS, V9, P158, DOI 10.1109/TOH.2016.2522432; Inui K, 2004, CEREB CORTEX, V14, P851, DOI 10.1093/cercor/bhh043; Iwamura Y, 1998, CURR OPIN NEUROBIOL, V8, P522, DOI 10.1016/S0959-4388(98)80041-X; Kell A, 2015, SOC NEUROSCIENCE; Kerr JND, 2007, J NEUROSCI, V27, P13316, DOI 10.1523/JNEUROSCI.2210-07.2007; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Knutsen PM, 2006, J NEUROSCI, V26, P8451, DOI 10.1523/JNEUROSCI.1516-06.2006; Kriegeskorte Nikolaus, 2008, Front Syst Neurosci, V2, P4, DOI 10.3389/neuro.06.004.2008; Moore JD, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002253; O'Connor DH, 2010, NEURON, V67, P1048, DOI 10.1016/j.neuron.2010.08.026; Petersen CCH, 2003, J NEUROSCI, V23, P1298, DOI 10.1523/JNEUROSCI.23-04-01298.2003; PONS TP, 1987, SCIENCE, V237, P417, DOI 10.1126/science.3603028; Purves Dale, 2001, NEUROSCIENCE, P3; Quist BW, 2014, J NEUROSCI, V34, P9828, DOI 10.1523/JNEUROSCI.1707-12.2014; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Towal RB, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001120; Von Heimendahl M, 2007, PLOS BIOL, V5, P2696, DOI 10.1371/journal.pbio.0050305; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Yu CX, 2006, PLOS BIOL, V4, P819, DOI 10.1371/journal.pbio.0040124	35	4	4	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402059
C	Zhuang, HL; Wang, C; Wang, YF		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhuang, Honglei; Wang, Chi; Wang, Yifan			Identifying Outlier Arms in Multi-Armed Bandit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				APPROXIMATE	We study a novel problem lying at the intersection of two areas: multi-armed bandit and outlier detection. Multi-armed bandit is a useful tool to model the process of incrementally collecting data for multiple objects in a decision space. Outlier detection is a powerful method to narrow down the attention to a few objects after the data for them are collected. However, no one has studied how to detect outlier objects while incrementally collecting data for them, which is necessary when data collection is expensive. We formalize this problem as identifying outlier arms in a multi-armed bandit. We propose two sampling strategies with theoretical guarantee, and analyze their sampling efficiency. Our experimental results on both synthetic and real data show that our solution saves 70-99% of data collection cost from baseline while having nearly perfect accuracy.	[Zhuang, Honglei] Univ Illinois, Champaign, IL 61820 USA; [Zhuang, Honglei; Wang, Chi] Microsoft Res, Redmond, WA USA; [Wang, Yifan] Tsinghua Univ, Beijing, Peoples R China	University of Illinois System; University of Illinois Urbana-Champaign; Microsoft; Tsinghua University	Zhuang, HL (corresponding author), Univ Illinois, Champaign, IL 61820 USA.	hzhuang3@illinos.edu; wang.chi@microsoft.com; yifan-wa16@mails.tsinghua.edu.cn	Jeong, Yongwook/N-7413-2016		U.S. Army Research Lab [W911NF-09-2-0053]; National Science Foundation [IIS 16-18481, IIS 17-04532, IIS-17-41317]; NIGMS through funds - trans-NIH Big Data to Knowledge (BD2K) initiative [1U54GM114838]	U.S. Army Research Lab(United States Department of DefenseUS Army Research Laboratory (ARL)); National Science Foundation(National Science Foundation (NSF)); NIGMS through funds - trans-NIH Big Data to Knowledge (BD2K) initiative	Part of this work was done while the first author was an intern at Microsoft Research. The first author was sponsored in part by the U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.	Abe N., 2006, P 12 ACM SIGKDD INT, P504, DOI [10.1145/1150402.1150459, DOI 10.1145/1150402.1150459]; Aggarwal C.C., 2008, SDM, P483; Agresti A, 1998, AM STAT, V52, P119, DOI 10.2307/2685469; Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Carpentier A, 2014, ADV NEUR IN, V27; Chen L., 2015, ARXIV151103774; Chen S., 2014, P ADV NEUR INF PROC, P379; Donmez P, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P259; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Gabillon V, 2016, JMLR WORKSH CONF PRO, V51, P1004; Gentile C, 2014, PR MACH LEARN RES, V32, P757; Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1023/B:AIRE.0000045502.10941.a9; Jiang B, 2011, PROC INT CONF DATA, P422, DOI 10.1109/ICDE.2011.5767850; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Kollios G, 2003, IEEE T KNOWL DATA EN, V15, P1170, DOI 10.1109/TKDE.2003.1232271; Korda N, 2016, PR MACH LEARN RES, V48; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Liu HF, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P726, DOI 10.1109/BigData.2016.7840665; Locatelli A, 2016, PR MACH LEARN RES, V48; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Sugiyama M., 2013, INT C NEUR INF PROC, P1; Wu M., 2006, ACM SIGKDD, P767; Zimek A, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P428	30	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405028
C	Abernethy, J; Amin, K; Zhu, RH		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Abernethy, Jacob; Amin, Kareem; Zhu, Ruihao			Threshold Bandit, With and Without Censored Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the Threshold Bandit setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a threshold value. The learner selects one of K actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem, the uncensored and censored case, that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically.	[Abernethy, Jacob; Amin, Kareem] Univ Michigan, Dept Comp Sci, Ann Arbor, MI 48109 USA; [Zhu, Ruihao] MIT, AeroAstro&CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA	University of Michigan System; University of Michigan; Massachusetts Institute of Technology (MIT)	Abernethy, J (corresponding author), Univ Michigan, Dept Comp Sci, Ann Arbor, MI 48109 USA.	jabernet@umich.edu; amkareem@umich.edu; rzhu@mit.edu						Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096; Abernethy Jacob D, 2015, ADV NEURAL INFORM PR, P2188; Agarwal A., 2010, P 13 INT C ART INT S, P9; Amin Kareem, 2012, ARXIV12104847; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Auer P., 2002, J MACHINE LEARNING, V3, P235, DOI DOI 10.1023/A:1013689704352; Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Dvoretzky A., 1956, ANN MATH STAT; FOLDES A, 1981, ANN STAT, V9, P122, DOI 10.1214/aos/1176345337; Ganchev Kuzman, 2010, UAI; Gittins J. C., 2011, MULTIARMED BANDIT AL, V2nd; Huh W. T., 2009, ADAPTIVE DATA DRIVEN; Kaplan E. L., 1958, JASA; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234; Peterson A. V., 1983, ENCY STAT SCI; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8	20	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701006
C	Gao, SY; Steeg, GV; Galstyan, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gao, Shuyang; Steeg, Greg Ver; Galstyan, Aram			Variational Information Maximization for Feature Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MUTUAL INFORMATION	Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.	[Gao, Shuyang; Steeg, Greg Ver; Galstyan, Aram] Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA	University of Southern California	Gao, SY (corresponding author), Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA.	gaos@usc.edu; gregv@isi.edu; galstyan@isi.edu						Balagani KS, 2010, IEEE T PATTERN ANAL, V32, P1342, DOI 10.1109/TPAMI.2010.62; Barber D, 2004, ADV NEUR IN, V16, P201; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Brown G, 2012, J MACH LEARN RES, V13, P27; Cheng HR, 2011, ETRI J, V33, P210, DOI 10.4218/etrij.11.0110.0237; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Das A., 2011, P 28 INT C MACH LEAR, P1057; Dash M., 1997, Intelligent Data Analysis, V1; Gao Shuyang, VARIATIONAL FEATURE; Gentleman R, 2005, STAT BIOL HEALTH, P189; Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; LEWIS DD, 1992, SPEECH AND NATURAL LANGUAGE, P212; Lichman M, 2013, UCI MACHINE LEARNING; Lin DH, 2006, LECT NOTES COMPUT SC, V3951, P68; Liu H., 2012, FEATURE SELECTION KN, V454; Mohamed S., 2015, ADV NEURAL INFORM PR, V28, P2116; Nguyen XV, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P512, DOI 10.1145/2623330.2623611; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Rodriguez-Lujan I, 2010, J MACH LEARN RES, V11, P1491; Ross BC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087357; Vinh N. X., 2015, PATTERN RECOGNITION; Yang HH, 2000, ADV NEUR IN, V12, P687; Zhou YB, 2014, ADV NEUR IN, V27	26	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703080
C	Grill, JB; Valko, M; Munos, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Grill, Jean-Bastien; Valko, Michal; Munos, Remi			Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). But you do not want to StOP with exponential running time, you want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.	[Grill, Jean-Bastien; Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France; [Munos, Remi] Google DeepMind, London, England	Google Incorporated	Grill, JB (corresponding author), INRIA Lille Nord Europe, SequeL Team, Lille, France.	jean-bastien.grill@inria.fr; michal.valko@inria.fr; munos@google.com			French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council; Ecole Normale Superieure in Paris; Inria; Carnegie Mellon University; French National Research Agency [ANR-14-CE24-0010-01, ANR-16-CE23-0003]	French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council(Region Hauts-de-France); Ecole Normale Superieure in Paris; Inria; Carnegie Mellon University; French National Research Agency(French National Research Agency (ANR))	The research presented in this paper was supported by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, a doctoral grant of Ecole Normale Superieure in Paris, Inria and Carnegie Mellon University associated-team project EduBand, and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003)	Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Bubeck S, 2010, C LEARN THEOR; Busoniu Lucian, 2012, INT C ART INT STAT; Gelly S., 2006, TECHNICAL REPORT; Guez A., 2012, NEURAL INFORM PROCES; Hren JF, 2008, LECT NOTES ARTIF INT, V5323, P151, DOI 10.1007/978-3-540-89722-4_12; Kearns Michael, 1999, INT C ART INT STAT; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Silver D., 2010, NIPS, P2164; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Szorenyi Balazs, 2014, NEURAL INFORM PROCES; Walsh TJ, 2010, AAAI CONF ARTIF INTE, P612	17	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701110
C	Huggins, JH; Campbell, T; Broderick, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huggins, Jonathan H.; Campbell, Trevor; Broderick, Tamara			Coresets for Scalable Bayesian Logistic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset - both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.	[Huggins, Jonathan H.; Campbell, Trevor; Broderick, Tamara] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Huggins, JH (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	fjhuggins@mit.edu; tdjc@mit.edu; tbroderick@csail.mit.edu			Office of Naval Research under ONR MURI [N000141110688]; National Defense Science and Engineering Graduate (NDSEG) Fellowship	Office of Naval Research under ONR MURI(MURI); National Defense Science and Engineering Graduate (NDSEG) Fellowship	All authors are supported by the Office of Naval Research under ONR MURI grant N000141110688. JHH is supported by a National Defense Science and Engineering Graduate (NDSEG) Fellowship.	Agarwal P, 2005, COMBINATORIAL COMPUT, P1; [Anonymous], [No title captured]; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bachem O., 2016, AAAI C ART INT; Bardenet R., 2015, MARKOV CHAIN MONTE C; Betancourt M. J., 2015, INT C MACH LEARN; Broderick T., 2013, ADV NEURAL INFORM PR; Campbell T., 2015, ADV NEURAL INFORM PR; Entezari R., 2016, LIKELIHOOD INFLATING; Feldman D., 2011, ADV NEURAL INFORM PR, V24, P2142; Feldman D., 2011, S THEOR COMP; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Gelman A, 2008, ANN APPL STAT, V2, P1360, DOI 10.1214/08-AOAS191; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; Han L., 2016, LOCAL UNCERTAINTY SA; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Lucic M., 2016, INT C ART INT STAT; Maclaurin D., 2014, UNCERTAINTY ARTIFICI; Rabinovich M., 2015, VARIATIONAL CONSENSU; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Scott S. L., 2013, BAYES, V250; SRIVASTAVA S., 2015, INT C ART INT STAT; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1	24	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704011
C	Jain, L; Jamieson, K; Nowak, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jain, Lalit; Jamieson, Kevin; Nowak, Robert			Finite Sample Prediction and Recovery Bounds for Ordinal Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints like "item i is closer to item j than item k". Ordinal constraints like this often come from human judgments. The classic approach to solving this problem is known as non-metric multidimensional scaling. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. The ordinal embedding problem has been studied for decades, but most past work pays little attention to the question of whether accurate embedding is possible, apart from empirical studies. This paper shows that under a generative data model it is possible to learn the correct embedding from noisy distance comparisons. In establishing this fundamental result, the paper makes several new contributions. First, we derive prediction error bounds for embedding from noisy distance comparisons by exploiting the fact that the rank of a distance matrix of points in R-d is at most d + 2. These bounds characterize how well a learned embedding predicts new comparative judgments. Second, we show that the underlying embedding can be recovered by solving a simple convex optimization. This result is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible. Third, two new algorithms for ordinal embedding are proposed and evaluated in experiments.	[Jain, Lalit] Univ Michigan, Ann Arbor, MI 48109 USA; [Jamieson, Kevin] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA	University of Michigan System; University of Michigan; University of California System; University of California Berkeley; University of Wisconsin System; University of Wisconsin Madison	Jain, L (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	lalitj@umich.edu; kjamieson@berkeley.edu; rdnowak@wisc.edu			NSF [CCF-1218189, IIS-1447449]; NIH [1 U54 AI117924-01]; AFOSR [FA9550-13-1-0138]; ONR [N00014-15-1-2620, N00014-13-1-0129]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR(Office of Naval Research)	This work was partially supported by the NSF grants CCF-1218189 and IIS-1447449, the NIH grant 1 U54 AI117924-01, the AFOSR grant FA9550-13-1-0138, and by ONR awards N00014-15-1-2620, and N00014-13-1-0129. We would also like to thank Amazon Web Services for providing the computational resources used for running our simulations.	Agarwal Sameer, 2007, J MACHINE LEARNING R, P11; Arias-Castro E., 2015, ARXIV150102861; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Dattorro J., 2011, CONVEX OPTIMIZATION; Davenport Mark A, 2014, INFORM INFERENCE, V3; Jamieson Kevin G., 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1077; Jamieson K.G., 2015, ADV NEURAL INFORM PR, V28, P2638; Kleindessner M., 2014, 27 C LEARNING THEORY, V35, P40; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P115, DOI 10.1007/BF02289694; Lu Y, 2015, ANN ALLERTON CONF, P1473, DOI 10.1109/ALLERTON.2015.7447183; McFee B, 2011, J MACH LEARN RES, V12, P491; Oymak S., 2015, ARXIV150704793; Park  D., 2015, P INT C MACH LEARN I; Shen J., 2016, ARXIV160501656; SHEPARD RN, 1962, PSYCHOMETRIKA, V27, P125, DOI 10.1007/BF02289630; Tamuz Omer, 2011, ARXIV11051033; Tarazaga P, 2009, LINEAR MULTILINEAR A, V57, P651, DOI 10.1080/03081080802079259; Terada Y, 2014, PR MACH LEARN RES, V32, P847; Tropp Joel A., 2015, INTRO MATRIX CONCENT; van der Maaten L., 2012, MACH LEARN SIGN PROC, P1; WRIGHT S, 2013, NIPS WORKSH GREED AL	21	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704078
C	Kawahara, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kawahara, Yoshinobu			Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PROPER ORTHOGONAL DECOMPOSITION; REDUCTION; SYSTEMS	A spectral analysis of the Koopman operator, which is an infinite dimensional linear operator on an observable, gives a (modal) description of the global behavior of a nonlinear dynamical system without any explicit prior knowledge of its governing equations. In this paper, we consider a spectral analysis of the Koopman operator in a reproducing kernel Hilbert space (RKHS). We propose a modal decomposition algorithm to perform the analysis using finite-length data sequences generated from a nonlinear system. The algorithm is in essence reduced to the calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. The algorithm returns a decomposition of the dynamics into a finite number of modes, and thus it can be thought of as a feature extraction procedure for a nonlinear dynamical system. Therefore, we further consider applications in machine learning using extracted features with the presented analysis. We illustrate the method on the applications using synthetic and real-world data.	[Kawahara, Yoshinobu] Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan; [Kawahara, Yoshinobu] RIKEN, Ctr Adv Integrated Intelligence Res, Tokyo, Japan	Osaka University; RIKEN	Kawahara, Y (corresponding author), Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan.; Kawahara, Y (corresponding author), RIKEN, Ctr Adv Integrated Intelligence Res, Tokyo, Japan.	ykawahara@sanken.osaka-u.ac.jp	KAWAHARA, Yoshinobu/AAM-7540-2020; Kawahara, Yoshinobu/J-2462-2014	KAWAHARA, Yoshinobu/0000-0001-7789-4709; Kawahara, Yoshinobu/0000-0001-7789-4709	JSPS KAKENHI [JP16H01548]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by JSPS KAKENHI Grant Number JP16H01548.	ARNOLDI WE, 1951, Q APPL MATH, V9, P17, DOI 10.1090/qam/42792; Bagheri S, 2013, J FLUID MECH, V726, P596, DOI 10.1017/jfm.2013.249; Bagheri S, 2009, J FLUID MECH, V624, P33, DOI 10.1017/S0022112009006053; Berger E, 2014, IEEE ROMAN, P593, DOI 10.1109/ROMAN.2014.6926317; BONNET JP, 1994, EXP FLUIDS, V17, P307, DOI 10.1007/BF01874409; Brunton BW, 2016, J NEUROSCI METH, V258, P1, DOI 10.1016/j.jneumeth.2015.10.010; Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009; Chen KK, 2012, J NONLINEAR SCI, V22, P887, DOI 10.1007/s00332-012-9130-9; Csorgo M., 1988, LIMIT THEOREMS CHANG; Duke D, 2012, J FLUID MECH, V691, P594, DOI 10.1017/jfm.2011.516; Ghahramani Z., P 1998 C ADV NEUR IN, P431; Holmes P., 1996, TURBULENCE COHERENT; Jovanovic MR, 2014, PHYS FLUIDS, V26, DOI 10.1063/1.4863670; Katayama T., 2005, COMM CONT E, Vvol. 1; Kawahara Y., 2007, ADV NEURAL INF PROCE, V19, P665; Koopman BO, 1931, P NATL ACAD SCI USA, V17, P315, DOI 10.1073/pnas.17.5.315; Kulesza A, 2015, AAAI CONF ARTIF INTE, P2715; Kwok JTY, 2004, IEEE T NEURAL NETWOR, V15, P1517, DOI 10.1109/TNN.2004.837781; Melnyk I, 2015, JMLR WORKSH CONF PRO, V38, P690; Mezic I, 2005, NONLINEAR DYNAM, V41, P309, DOI 10.1007/s11071-005-2824-x; Muld TW, 2012, COMPUT FLUIDS, V57, P87, DOI 10.1016/j.compfluid.2011.12.012; Noack BR, 2003, J FLUID MECH, V497, P335, DOI 10.1017/S0022112003006694; Rowley CW, 2009, J FLUID MECH, V641, P115, DOI 10.1017/S0022112009992059; Rowley CW, 2005, INT J BIFURCAT CHAOS, V15, P997, DOI 10.1142/S0218127405012429; Schmid P. J., 2010, INT J HEAT FLUID FL, V32, P1098; Schmid PJ, 2010, J FLUID MECH, V656, P5, DOI 10.1017/S0022112010001217; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; SIROVICH L, 1987, Q APPL MATH, V45, P561, DOI 10.1090/qam/910462; Song L., P 27 INT C MACH LEAR, P991; Susuki Y, 2014, IEEE T POWER SYST, V29, P899, DOI 10.1109/TPWRS.2013.2287235; Tu J.H., 2014, J COMPUT DYN, V1, P391, DOI [10.3934/jcd.2014.1.391, DOI 10.3934/JCD.2014.1.391]; Van Overschee P., 1996, SUBSPACE IDENTIFICAT; Wang J., 2006, ADV NEURAL INFORM PR, P1441; Williams MO, 2015, J NONLINEAR SCI, V25, P1307, DOI 10.1007/s00332-015-9258-5; Wolf L, 2004, J MACH LEARN RES, V4, P913, DOI 10.1162/1532443041827934	36	4	4	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704107
C	Pazis, J; Parr, R; How, JP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Pazis, Jason; Parr, Ronald; How, Jonathan P.			Improving PAC Exploration Using the Median of Means	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present the first application of the median of means in a PAC exploration algorithm for MDPs. Using the median of means allows us to significantly reduce the dependence of our bounds on the range of values that the value function can take, while introducing a dependence on the (potentially much smaller) variance of the Bellman operator. Additionally, our algorithm is the first algorithm with PAC bounds that can be applied to MDPs with unbounded rewards.	[Pazis, Jason] MIT, Lab Informat & Decis Syst, Cambridge, MA 02139 USA; [Parr, Ronald] Duke Univ, Dept Comp Sci, Durham, NC 27708 USA; [How, Jonathan P.] MIT, Dept Aeronaut & Astronaut, Aerosp Controls Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Duke University; Massachusetts Institute of Technology (MIT)	Pazis, J (corresponding author), MIT, Lab Informat & Decis Syst, Cambridge, MA 02139 USA.	jpazis@mit.edu; parr@cs.duke.edu; jhow@mit.edu			ONR MURI Grant [N000141110688]; National Science Foundation [IIS-1218931]; Boeing Company	ONR MURI Grant; National Science Foundation(National Science Foundation (NSF)); Boeing Company	We would like to thank Emma Brunskill, Tor Lattimore, and Christoph Dann for spotting an error in an earlier version of this paper, as well as the anonymous reviewers for helpful comments and suggestions. This material is based upon work supported in part by The Boeing Company, by ONR MURI Grant N000141110688, and by the National Science Foundation under Grant No. IIS-1218931. Opinions, findings, conclusions or recommendations herein are those of the authors and not necessarily those of the NSF.	BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Guo ZH, 2015, AAAI CONF ARTIF INTE, P2624; HARRISON JM, 1972, ANN MATH STAT, V43, P636, DOI 10.1214/aoms/1177692643; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Maillard  Odalric-Ambrym, 2014, P ADV NEUR INF PROC, V27, P1835; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Osband I., 2014, ADV NEURAL INFORM PR, V27, P1466; Pazis J, 2016, AAAI CONF ARTIF INTE, P1977; Pazis Jason, 2013, P ANN AAAI C ARTIFIC, P774; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szita Istvan, 2010, INT C MACH LEARN ICM, P1031	16	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704101
C	Petrik, M; Ghavamzadeh, M; Chow, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Petrik, Marek; Ghavamzadeh, Mohammad; Chow, Yinlam			Safe Policy Improvement by Minimizing Robust Baseline Regret	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					An important problem in sequential decision-making under uncertainty is to use limited data to compute a safe policy, which is guaranteed to outperform a given baseline strategy. In this paper, we develop and analyze a new model-based approach that computes a safe policy, given an inaccurate model of the system's dynamics and guarantees on the accuracy of this model. The new robust method uses this model to directly minimize the (negative) regret w.r.t. the baseline policy. Contrary to existing approaches, minimizing the regret allows one to improve the baseline policy in states with accurate dynamics and to seamlessly fall back to the baseline policy, otherwise. We show that our formulation is NP-hard and propose a simple approximate algorithm. Our empirical results on several domains further show that even the simple approximate algorithm can outperform standard approaches.	[Petrik, Marek] Univ New Hampshire, Durham, NH 03824 USA; [Ghavamzadeh, Mohammad] Adobe Res, Lille, France; [Ghavamzadeh, Mohammad] INRIA Lille, Lille, France; [Chow, Yinlam] Stanford Univ, Stanford, CA 94305 USA	University System Of New Hampshire; University of New Hampshire; Stanford University	Petrik, M (corresponding author), Univ New Hampshire, Durham, NH 03824 USA.	mpetrik@cs.unh.edu; ghavamza@adobe.com; ychow@stanford.edu						Ahmed AA, 2013, ARAB J GEOSCI, P1; Hansen TD, 2013, J ACM, V60, DOI 10.1145/2432622.2432623; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Kakade S., 2002, P 19 INT C MACH LEAR; Le Tallec Y, 2007, THESIS; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Petrik M., 2014, NEURAL INFORM PROCES; Petrik M, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P692; Pirotta M., 2013, P 30 INT C MACH LEAR; Thomas P., 2015, P 29 C ART INT; Thomas PS, 2015, PR MACH LEARN RES, V37, P2380; Weissman Tsachy, 2003, TECH REP; Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566; Xu H, 2009, IEEE DECIS CONTR P, P3606, DOI 10.1109/CDC.2009.5400796	14	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702041
C	Tamar, A; Wu, Y; Thomas, G; Levine, S; Abbeel, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Tamar, Aviv; Wu, Yi; Thomas, Garrett; Levine, Sergey; Abbeel, Pieter			Value Iteration Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce the value iteration network (VIN): a fully differentiable neural network with a 'planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.	[Tamar, Aviv; Wu, Yi; Thomas, Garrett; Levine, Sergey; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Tamar, A (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.				Siemens; ONR; Army Research Office; NSF CAREER; Viterbi Scholarship, Technion; DARPA PPAML program [FA8750-14-C-0011]	Siemens(Siemens AG); ONR(Office of Naval Research); Army Research Office; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Viterbi Scholarship, Technion; DARPA PPAML program	This research was funded in part by Siemens, by ONR through a PECASE award, by the Army Research Office through the MAST program, and by an NSF CAREER grant. A. T. was partially funded by the Viterbi Scholarship, Technion. Y. W. was partially funded by a DARPA PPAML program, contract FA8750-14-C-0011.	[Anonymous], 2015, NIPS; Bellman RE, 1957, DYNAMIC PROGRAMMING; Bertsekas D. P., 2012, APPROXIMATE DYNAMIC, V2; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Deisenroth M., 2011, ICML; Duan Yan, 2016, ARXIV160406778; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Finn C., 2016, GUIDED POLICY SEARCH; Fukushima K., 1979, Transactions of the Institute of Electronics and Communication Engineers of Japan, Section E (English), VE62, P675; Giusti A., 2016, IEEE ROBOTICS AUTOMA; Guo X., 2016, IJCAI; Guo Xiaoxiao, 2014, NIPS; Ilin R., 2007, ADPRL; Joseph J., 2013, ICRA; Kaelbling LP, 2011, IEEE INT CONF ROBOT, P1470; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Levine S., 2014, NIPS; Levine S., 2016, JMLR, V17; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mnih V, 2016, ASYNCHRONOUS METHODS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Neu G., 2007, UAI; Nogueira R., 2016, ARXIV160202261; Ross S., 2011, P INT C ARTIFICIAL I, P627; Schmidhuber J., 1990, INT JOINT C NEUR NET; Schulman J., 2015, ICML; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Theano Development Team, 2016, ARXIV E PRINTS; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Xu K., 2015, ICML	31	4	4	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700011
C	Vaswani, N; Guo, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Vaswani, Namrata; Guo, Han			Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as "data-dependent noise". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes.	[Vaswani, Namrata; Guo, Han] Iowa State Univ, Ames, IA 50011 USA	Iowa State University	Vaswani, N (corresponding author), Iowa State Univ, Ames, IA 50011 USA.	namrata@iastate.edu; hanguo@iastate.edu						Arora Raman, 2013, ADV NEURAL INFORM PR, P1815; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; Boutsidis C., 2015, P 26 ANN ACM SIAM S, P887, DOI DOI 10.1137/1.9781611973730.61; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chandrasekaran V., 2011, SIAM J OPTIMIZATION, V21; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Fazel M., 2002, MATRIX RANK MINIMIZA; Gillberg Jussi, 2016, J MACHINE LEARNING R; Golub GH, 2000, J COMPUT APPL MATH, V123, P35, DOI 10.1016/S0377-0427(00)00413-1; Hsu D., 2011, IEEE T INFO TH; Karnin Zohar, 2015, P 28 ANN C COMP LEAR, P505; Lin Z., 2009, TECH REP; Lois B., 2015, IEEE INT S INF TH IS; Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR, P2886; Nadler B, 2008, ANN STAT, V36, P2791, DOI 10.1214/08-AOS618; Netrapalli P., 2014, NEURAL INFO PROC SYS; Netrapalli P., 2013, S THEOR COMP STOC; Qiu C., 2010, ALL C COMM CONTR COM; Qiu CL, 2014, IEEE T INFORM THEORY, V60, P5007, DOI 10.1109/TIT.2014.2331344; Shamir Ohad, 2014, ARXIV14092848; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Vaswani N., 2017, PCA DATA DEPENDENT N; Zhan J., 2016, INT C ART INT STAT A	25	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705012
C	Wiebe, N; Kapoor, A; Svore, KM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wiebe, Nathan; Kapoor, Ashish; Svore, Krysta M.			Quantum Perceptron Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points N, namely O(root N). The second algorithm illustrates how the classical mistake bound of O(1/gamma(2)) can be further improved to O(1/root gamma) through quantum means, where gamma denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.	[Wiebe, Nathan; Kapoor, Ashish; Svore, Krysta M.] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Wiebe, N (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	nawiebe@microsoft.com; akapoor@microsoft.com; ksvore@microsoft.com						Amin MH, 2016, ARXIV160102036; Boyer M., 1996, QUANTPH9605034 ARXIV; Brassard G., 2002, CONTEMP MATH, V305, DOI [10.1090/conm/305/05215, DOI 10.1090/CONM/305/05215]; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; Garnerone S, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.230506; Gentile C, 2002, J MACH LEARN RES, V2, P213, DOI 10.1162/15324430260185600; Grover L. K., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P212, DOI 10.1145/237814.237866; Herbrich R, 1999, IJCAI WORKSH SVMS, P23; LEWENSTEIN M, 1994, J MOD OPTIC, V41, P2491, DOI 10.1080/09500349414552331; Li Y., 2002, PROC ICML, V2, P379; Lloyd S., 2013, ARXIV13070411; Lloyd S, 2014, NAT PHYS, V10, P631, DOI [10.1038/NPHYS3029, 10.1038/nphys3029]; Minka T., 2001, THESIS MIT CAMBRIDGE; Novikoff Albert B, 1963, TECHNICAL REPORT; Rebentrost P, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.130503; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Shalev-Shwartz S, 2005, LECT NOTES COMPUT SC, V3559, P264, DOI 10.1007/11503415_18; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Wiebe N., 2015, ARXIV151203145; Wiebe N., 2014, QUANTUM DEEP LEARNIN; Wiebe N, 2015, QUANTUM INF COMPUT, V15, P316	23	4	4	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703037
C	Yu, M; Gupta, V; Kolar, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yu, Ming; Gupta, Varun; Kolar, Mladen			Statistical Inference for Pairwise Graphical Models Using Score Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SELECTION	Probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries. As a result, there is a large body of literature focused on consistent model selection. However, scientists are often interested in understanding uncertainty associated with the estimated parameters, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for edge parameters for pairwise graphical models based on Hyvarinen scoring rule. Hyvarinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form. We prove that the estimator is root n-consistent and asymptotically Normal. This result allows us to construct confidence intervals for edge parameters, as well as, hypothesis tests. We establish our results under conditions that are typically assumed in the literature for consistent estimation. However, we do not require that the estimator consistently recovers the graph structure. In particular, we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes. We illustrate validity of our estimator through extensive simulation studies.	[Yu, Ming; Gupta, Varun; Kolar, Mladen] Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA	University of Chicago	Yu, M (corresponding author), Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA.	mingyu@chicagobooth.edu; varun.gupta@chicagobooth.edu; mladen.kolar@chicagobooth.edu			IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business	IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business(International Business Machines (IBM))	This work is supported by an IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business. This work was completed in part with resources provided by the University of Chicago Research Computing Center.	Arnold B.C., 1999, SPR S STAT; Barber R. F., 2015, ARXIV150207641; Belloni A, 2014, REV ECON STUD, V81, P608, DOI 10.1093/restud/rdt044; Belloni A, 2013, BERNOULLI, V19, P521, DOI 10.3150/11-BEJ410; Chen M., 2015, J AM STAT ASS; Chen S., 2013, ARXIV13110085; Drton M., 2016, ANN REV STAT ITS APP, V3; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003; Jankova J., 2014, ARXIV14036752; Javanmard A, 2014, J MACH LEARN RES, V15, P2869; Lee JD, 2015, J COMPUT GRAPH STAT, V24, P230, DOI 10.1080/10618600.2014.900500; Leeb H., 2007, ECON THEOR, V24, P338; Lin L., 2015, ARXIV150700433; Liu WD, 2015, J MULTIVARIATE ANAL, V135, P153, DOI 10.1016/j.jmva.2014.11.005; Liu WD, 2013, ANN STAT, V41, P2948, DOI 10.1214/13-AOS1169; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Neyman J., 1959, PROBABILITY STATSITI, P213; Parry M, 2012, ANN STAT, V40, P561, DOI 10.1214/12-AOS971; Rudelson M., 2011, RECONSTRUCTION ANISO; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Sriperumbudur B, 2013, ARXIV13123516; Sun S., 2015, ADV NEURAL INFORM PR, V28, P2287; Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221; Wang J., 2016, P AISTATS, V51, P751; Yang EH, 2015, J MACH LEARN RES, V16, P3813; Yang E, 2014, JMLR WORKSH CONF PRO, V33, P1042; Zhang CH, 2014, J R STAT SOC B, V76, P217, DOI 10.1111/rssb.12026	32	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704054
C	Afshar, HM; Domke, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Afshar, Hadi Mohasel; Domke, Justin			Reflection, Refraction, and Hamiltonian Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics with non-smooth functions, leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples, this method improves on traditional HMC.	[Afshar, Hadi Mohasel] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia; [Domke, Justin] NICTA, Canberra, ACT 0200, Australia; [Domke, Justin] Australian Natl Univ, Canberra, ACT 0200, Australia	Australian National University; Australian National University; Australian National University	Afshar, HM (corresponding author), Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia.	hadi.afshar@anu.edu.au; Justin.Domke@nicta.com.au			Australian Government through the Department of Communications; Australian Research Council through the ICT Centre of Excellence Program	Australian Government through the Department of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence Program(Australian Research Council)	NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.	Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Buchdahl H. A, 1993, INTRO HAMILTONIAN OP; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Glen AG, 2004, COMPUT STAT DATA AN, V44, P451, DOI 10.1016/S0167-9473(02)00234-7; Greenwood DT., 1988, PRINCIPLES DYNAMICS; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Lunn D, 2009, STAT MED, V28, P3049, DOI 10.1002/sim.3680; Neal R. M., 2011, HDB MARKOV CHAIN MON, V2; Pakman A., 2013, ADV NEURAL INFORM PR, V26, P2490; Pakman A, 2014, J COMPUT GRAPH STAT, V23, P518, DOI 10.1080/10618600.2013.788448; Roberts GO, 1997, ANN APPL PROBAB, V7, P110, DOI 10.1214/aoap/1034625254; Sanner Scott, 2015, ASS ADVANCEMENT ARTI, P665	12	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101061
C	Berglund, M; Raiko, T; Honkala, M; Karkkainen, L; Vetek, A; Karhunen, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Berglund, Mathias; Raiko, Tapani; Honkala, Mikko; Karkkainen, Leo; Vetek, Akos; Karhunen, Juha			Bidirectional Recurrent Neural Networks as Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult. Recently, two different frameworks, GSN and NADE, provide a connection between reconstruction and probabilistic modeling, which makes the interpretation possible. As far as we know, neither GSN or NADE have been studied in the context of time series before. As an example of an unsupervised task, we study the problem of filling in gaps in high-dimensional time series with complex dynamics. Although unidirectional RNNs have recently been trained successfully to model such time series, inference in the negative time direction is non-trivial. We propose two probabilistic interpretations of bidirectional RNNs that can be used to reconstruct missing gaps efficiently. Our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions, although a bit less accurate than a computationally complex bidirectional Bayesian inference on the unidirectional RNN. We also provide results on music data for which the Bayesian inference is computationally infeasible, demonstrating the scalability of the proposed methods.	[Berglund, Mathias; Raiko, Tapani; Karhunen, Juha] Aalto Univ, Helsinki, Finland; [Honkala, Mikko; Karkkainen, Leo; Vetek, Akos] Nokia Labs, Espoo, Finland	Aalto University; Nokia Corporation; Nokia Finland	Berglund, M (corresponding author), Aalto Univ, Helsinki, Finland.				Nokia; Academy of Finland	Nokia(Nokia Corporation); Academy of Finland(Academy of Finland)	We thank KyungHyun Cho and Yoshua Bengio for useful discussions. The software for the simulations for this paper was based on Theano [3, 7]. Nokia has supported Mathias Berglund and the Academy of Finland has supported Tapani Raiko.	[Anonymous], 2010, PYTH SCI COMP C; Bahdanau D., 2015, INT C LEARN REPR ICL; Baldi P, 1999, BIOINFORMATICS, V15, P937, DOI 10.1093/bioinformatics/15.11.937; Bastien F., 2012, DEEP LEARN UNS FEAT; Bayer J, 2014, ARXIV14117610; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2013, P 26 INT C NEUR INF, P899; Boulanger-Lewandowski N., 2012, P 29 INT C MACH LEAR, P1159, DOI DOI 10.32604/CSSE.2021.014030; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Graves A., 2013, 2013 IEEE INT C AC S; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; Haykin S., 2009, NEURAL NETWORKS LEAR; Hermans M., 2013, P ADV NEUR INF PROC, V26, P190; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Koutnik J., 2014, P 31 INT C MACH LEAR; Maas A. L., 2014, ARXIV14082873; Mikolov Tomas, 2014, P 3 INT C LEARN REPR; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Raiko T, 2001, 8TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, VOLS 1-3, PROCEEDING, P822; Raiko T., 2015, INT C LEARN REPR ICL; Remes U, 2011, IEEE SIGNAL PROC LET, V18, P563, DOI 10.1109/LSP.2011.2163508; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Uria B, 2014, PR MACH LEARN RES, V32	27	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100019
C	Chen, JS; He, J; Shen, YL; Xiao, L; He, XD; Gao, JF; Song, XY; Deng, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chen, Jianshu; He, Ji; Shen, Yelong; Xiao, Lin; He, Xiaodong; Gao, Jianfeng; Song, Xinying; Deng, Li			End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SUBGRADIENT METHODS	We develop a fully discriminative learning approach for supervised Latent Dirich-let Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.	[Chen, Jianshu; Shen, Yelong; Xiao, Lin; He, Xiaodong; Gao, Jianfeng; Song, Xinying; Deng, Li] Microsoft Res, Redmond, WA 98052 USA; [He, Ji] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA	Microsoft; University of Washington; University of Washington Seattle	Chen, JS (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	jianshuc@microsoft.com; jvking@uw.edu; yeshen@microsoft.com; lin.xiao@microsoft.com; xiaohe@microsoft.com; jfgao@microsoft.com; xinson@microsoft.com; deng@microsoft.com						Asuncion A., 2009, P 25 C UNCERTAINTY A, P27, DOI DOI 10.1080/10807030390248483; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bernardo J. M., 2007, BAYESIAN STAT, V8, P3, DOI DOI 10.1007/978-3-642-93220-5_6; Blei D.M., 2007, P 20 INT C NEURAL IN, P121; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blitzer J., P 45 ANN M ASS COMP, P440, DOI DOI 10.1109/IRPS.2011.5784441; Bouchard Guillaume, 2004, 16 IASC INT S COMP S, P721; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hershey J. R., 2014, ARXIV14092574; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Holub A, 2005, PROC CVPR IEEE, P664; Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333; Kapadia S., 1998, THESIS; Lacoste-Julien S., 2008, P 21 INT C NEURAL IN, P897; McAuley Julian John, 2013, P 22 INT C WORLD WID, P897, DOI DOI 10.1145/2488388.2488466; McCallum Andrew Kachites, 2002, MALLET MACHINE LEARN; Nemirovsky D. B., 1983, PROBLEM COMPLEXITY M; Sontag D., 2011, ADV NEURAL INFORM PR, P1008; Stoyanov Veselin, 2011, P AISTATS; Tseng P., 2008, SIAM J OPTIMIZATION; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515; Wallach H. M., 2009, ADV NEURAL INFORM PR, V23, P1973; Wang Y, 2014, P 27 INT C NEUR INF, P1511; Yakhnenko Oksana, 2005, P IEEE ICDM; Zhu J, 2014, J MACH LEARN RES, V15, P1073; Zhu J, 2012, J MACH LEARN RES, V13, P2237	27	4	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103005
C	Chen, PH; Chen, J; Yeshurun, Y; Hasson, U; Haxby, JV; Ramadge, PJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chen, Po-Hsuan; Chen, Janice; Yeshurun, Yaara; Hasson, Uri; Haxby, James V.; Ramadge, Peter J.			A Reduced-Dimension fMRI Shared Response Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				HUMAN BRAIN; SYSTEM	Multi-subject fMRI data is critical for evaluating the generality and validity of findings across subjects, and its effective utilization helps improve analysis sensitivity. We develop a shared response model for aggregating multi-subject fMRI data that accounts for different functional topographies among anatomically aligned datasets. Our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest. Furthermore, by removing the identified shared response, it allows improved detection of group differences. The ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fMRI studies.	[Chen, Po-Hsuan; Ramadge, Peter J.] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA; [Chen, Janice; Yeshurun, Yaara; Hasson, Uri] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Chen, Janice; Yeshurun, Yaara; Hasson, Uri] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA; [Haxby, James V.] Dartmouth Coll, Dept Psychol & Brain Sci, Hanover, NH 03755 USA; [Haxby, James V.] Dartmouth Coll, Ctr Cognit Neurosci, Hanover, NH 03755 USA	Princeton University; Princeton University; Princeton University; Dartmouth College; Dartmouth College	Chen, PH (corresponding author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.							Ahn JH, 2003, NEURAL COMPUT, V15, P57, DOI 10.1162/089976603321043694; Ames D. L., 2014, J COGNITIVE NEUROSCI; Brett M, 2002, NAT REV NEUROSCI, V3, P243, DOI 10.1038/nrn756; Chen J., 2014, ABSTRACTS COGNITIVE; Conroy B., 2009, ADV NEURAL INFORM PR; Conroy B. R., 2013, NEUROIMAGE; Debettencourt MT, 2015, NAT NEUROSCI, V18, P470, DOI 10.1038/nn.3940; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Fischl B, 1999, HUM BRAIN MAPP, V8, P272, DOI 10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4; Griffiths TD, 2002, TRENDS NEUROSCI, V25, P348, DOI 10.1016/S0166-2236(02)02191-4; Hanke M, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.3; Haxby JV, 2011, NEURON, V72, P404, DOI 10.1016/j.neuron.2011.08.026; Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736; He Xu, 2012, Proceedings of the 2012 International Conference on Communication Systems and Network Technologies (CSNT 2012), P229, DOI 10.1109/CSNT.2012.57; Horn R.A., 2013, MATRIX ANAL, VSecond; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Huth AG, 2015, ARXIV150403622; Lorbert A., 2012, ADV NEURAL INFORM P; Manning JR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094914; Margulies DS, 2009, P NATL ACAD SCI USA, V106, P20069, DOI 10.1073/pnas.0905314106; Mazziotta J, 2001, PHILOS T R SOC B, V356, P1293, DOI 10.1098/rstb.2001.0915; Michael AM, 2014, FRONT SYST NEUROSCI, V8, DOI 10.3389/fnsys.2014.00106; Raichle M. E., 2015, ANN REV NEUROSCIENCE, V38; Sabuncu MR, 2010, CEREB CORTEX, V20, P130, DOI 10.1093/cercor/bhp085; Talairach J., 1988, COPLANAR STEREOTAXIC; TOOTELL RBH, 1995, NATURE, V375, P139, DOI 10.1038/375139a0; WATSON JDG, 1993, CEREB CORTEX, V3, P79, DOI 10.1093/cercor/3.2.79; Yeshurun Y., 2014, SOC NEUR ABSTR	29	4	4	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102004
C	Ganti, R; Balzano, L; Willett, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ganti, Ravi; Balzano, Laura; Willett, Rebecca			Matrix Completion Under Monotonic Single Index Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces. In real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation. This paper addresses the challenge of matrix completion in the face of such nonlinearities. Given a few observations of a matrix that are obtained by applying a Lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries. We propose a novel matrix completion method that alternates between low-rank matrix estimation and monotonic function estimation to estimate the missing matrix elements. Mean squared error bounds provide insight into how well the matrix can be estimated based on the size, rank of the matrix and properties of the nonlinear transformation. Empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach.	[Ganti, Ravi] UW Madison, Wisconsin Inst Discovery, Madison, WI 53706 USA; [Balzano, Laura] Univ Michigan, Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA; [Willett, Rebecca] UW Madison, Dept Elect & Comp Engn, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison; University of Michigan System; University of Michigan; University of Wisconsin System; University of Wisconsin Madison	Ganti, R (corresponding author), UW Madison, Wisconsin Inst Discovery, Madison, WI 53706 USA.	gantimahapat@wisc.edu; girasole@umich.edu; rmwillett@wisc.edu						Agarwal A., 2014, P 31 INT C MACH LEAR, P541; Becker S., 2011, LOW RANK MATR OPT S; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Cucuringu Mihai, 2012, THESIS; Dattorro J., 2010, CONVEX OPTIMIZATION; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Elhamifar Ehsan, 2013, TPAMI; Eriksson Brian, 2012, AISTATS; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Horowitz JL, 1996, J AM STAT ASSOC, V91, P1632, DOI 10.2307/2291590; ICHIMURA H, 1993, J ECONOMETRICS, V58, P71, DOI 10.1016/0304-4076(93)90114-K; Kakade S. M., 2011, NIPS; KALAI A. T., 2009, COLT; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Koyejo Oluwasanmi, 2013, P 7 ACM C REC SYST, P49; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Sindhwani Vikas, 2010, ENCY MACHINE LEARNIN; Singh A. A, 2012, SEX ROLES, P1, DOI DOI 10.1109/SCES.2012.6199064; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Tan MK, 2014, PR MACH LEARN RES, V32, P1539; Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768; Wang Z, 2014, PR MACH LEARN RES, V32, P91; Wen  Z., 2012, MATH PROGRAMMING COM; Yang Congyuan, 2015, ICML	27	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102065
C	Goroshin, R; Mathieu, M; LeCun, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Goroshin, Ross; Mathieu, Michael; LeCun, Yann			Learning to Linearize Under Uncertainty	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.	[Goroshin, Ross; Mathieu, Michael; LeCun, Yann] Courant Inst Math Sci, Dept Comp Sci, New York, NY 10012 USA; [LeCun, Yann] Facebook AI Res, New York, NY USA	Facebook Inc	Goroshin, R (corresponding author), Courant Inst Math Sci, Dept Comp Sci, New York, NY 10012 USA.	goroshin@cs.nyu.edu; mathieu@cs.nyu.edu; yann@cs.nyu.edu						Bengio Y., 2012, TECHNICAL REPORT; Cadieu C. F., 2012, NEURAL COMPUTATION; Cohen T. S., 2014, ARXIV14127659; Goroshin Ross, 2014, ARXIV14126056; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Kayser C., 2001, ICANN 2001; Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4; Mobahi H., 2009, ICML; Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Ranzato M.A., 2007, IEEE C COMP VIS PATT, P1; Ranzato MarcAurelio, 2014, ARXIV14126604; Vondrick C., 2015, ARXIV150408023; Wang X., 2015, ARXIV150500687; Wiskott Laurenz, 2002, NEURAL COMPUTATION; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	16	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102099
C	Grill, JB; Valko, M; Munos, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Grill, Jean-Bastien; Valko, Michal; Munos, Remi			Black-box optimization of noisy functions with unknown smoothness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of black-box optimization of a function f of any dimension, given function evaluations perturbed by noise. The function is assumed to be locally smooth around one of its global optima, but this smoothness is unknown. Our contribution is an adaptive optimization algorithm, POO or parallel optimistic optimization, that is able to deal with this setting. POO performs almost as well as the best known algorithms requiring the knowledge of the smoothness. Furthermore, POO works for a larger class of functions than what was previously considered, especially for functions that are difficult to optimize, in a very precise sense. We provide a finite-time analysis of POO's performance, which shows that its error after n evaluations is at most a factor of root ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness.	[Grill, Jean-Bastien; Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Villeneuve Dascq, France; [Munos, Remi] Google DeepMind, London, England	Google Incorporated	Grill, JB (corresponding author), INRIA Lille Nord Europe, SequeL Team, Villeneuve Dascq, France.	jean-bastien.grill@inria.fr; michal.valko@inria.fr; munos@google.com			French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Counci; Ecole Normale Superieure in Paris; Carnegie Mellon University associated-team project EduBand; French National Research Agency project ExTra-Learn [ANR-14-CE24-0010-01]; Inria	French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Counci; Ecole Normale Superieure in Paris; Carnegie Mellon University associated-team project EduBand; French National Research Agency project ExTra-Learn(French National Research Agency (ANR)); Inria	The research presented in this paper was supported by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, a doctoral grant of Ecole Normale Superieure in Paris, Inria and Carnegie Mellon University associated-team project EduBand, and French National Research Agency project ExTra-Learn (n. ANR-14-CE24-0010-01).	Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Azar Mohammad Gheshlaghi, 2014, INT C MACH LEARN INT C MACH LEARN; Bubeck S., 2011, J MACHINE LEARNING R, V12, P1587; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Bubeck Sebastien, 2011, ALGORITHMIC LEARNING; Bull AD, 2015, BERNOULLI, V21, P2289, DOI 10.3150/14-BEJ644; Combes R., 2015, ARXIV E PRINTS; Coquelin P.-A., 2007, BANDIT ALGORIT UNPUB; Kleinberg Robert, 2008, S THEOR COMP; Kocsis L., 2006, EUR C MACH LEARN; Munos R., 2011, NEURAL INFORM PROCES; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Preux Philippe, 2014, C EV COMP; Slivkins Aleksandrs, 2011, NEURAL INFORM PROCES; Valko M, 2013, INT C MACH LEARN PML, P1927	15	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100089
C	Huang, JJ; Qiu, Q; Sapiro, G; Calderbank, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Huang, Jiaji; Qiu, Qiang; Sapiro, Guillermo; Calderbank, Robert			Discriminative Robust Transformation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of training samples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm. Robustness is achieved by encouraging the transform that maps data to features to be a local isometry. This geometric property is shown to improve (K, epsilon)-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization framework is used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets, such as labeled faces in the wild, demonstrate the value of being able to balance discrimination and robustness.	[Huang, Jiaji; Qiu, Qiang; Sapiro, Guillermo; Calderbank, Robert] Duke Univ, Dept Elect Engn, Durham, NC 27708 USA	Duke University	Huang, JJ (corresponding author), Duke Univ, Dept Elect Engn, Durham, NC 27708 USA.	jiaji.huang@duke.edu; qiang.qiu@duke.edu; guillermo.sapiro@duke.edu; robert.calderbank@duke.edu			AFOSR [FA 9550-13-1-0076]; NGA [HM017713-1-0006]; NSF; DoD	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); NGA; NSF(National Science Foundation (NSF)); DoD(United States Department of Defense)	The work of Huang and Calderbank was supported by AFOSR under FA 9550-13-1-0076 and by NGA under HM017713-1-0006. The work of Qiu and Sapiro is partially supported by NSF and DoD.	Bellet A, 2015, NEUROCOMPUTING, V151, P259, DOI 10.1016/j.neucom.2014.09.044; Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41; Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Globerson A., 2005, NIPS; Goldberger J, 2004, ADV NEURAL INF PROCE, V17; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Huang Gary B., 2007, 0749 U MASS, P7; Huang J, 2015, IEEE IC COMP COM NET; Qiu Q, 2015, J MACH LEARN RES, V16, P187; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; Zha Z., 2009, INT JOINT C ART INT	18	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103013
C	Kawaguchi, K; Kaelbling, LP; Lozano-Perez, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kawaguchi, Kenji; Kaelbling, Leslie Pack; Lozano-Perez, Tomas			Bayesian Optimization with Exponential Convergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				GLOBAL MAXIMUM; ALGORITHM	This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence [1] requires access to the delta-cover sampling, which was considered to be impractical [1, 2]. Our approach eliminates both requirements and achieves an exponential convergence rate.	[Kawaguchi, Kenji; Kaelbling, Leslie Pack; Lozano-Perez, Tomas] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Kawaguchi, K (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	kawaguch@mit.edu; lpk@csail.mit.edu; tlp@csail.mit.edu			NSF [1420927]; ONR [N00014-14-1-0486]; ARO [W911NF1410433]; Funai Overseas Scholarship	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO; Funai Overseas Scholarship	The authors would like to thank Dr. Remi Munos for his thoughtful comments and suggestions. We gratefully acknowledge support from NSF grant 1420927, from ONR grant N00014-14-1-0486, and from ARO grant W911NF1410433. Kenji Kawaguchi was supported in part by the Funai Overseas Scholarship. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.	Bubeck S, 2011, LECT NOTES ARTIF INT, V6925, P144, DOI 10.1007/978-3-642-24412-4_14; Carter RG, 2001, OPTIM ENG, V2, P139, DOI 10.1023/A:1013123110266; Dixon L. C. W., 1977, GLOBAL OPTIMA CONVEX; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; JONES DR, 1993, J OPTIMIZ THEORY APP, V79, P157, DOI 10.1007/BF00941892; Kandasamy K., 2015, ARXIV150301673; Kvasov DE, 2003, NUMER MATH, V94, P93, DOI 10.1007/s00211-002-0419-8; MAYNE DQ, 1984, J OPTIMIZ THEORY APP, V42, P19, DOI 10.1007/BF00934131; McDonald DB, 2007, APPL MATH MODEL, V31, P2095, DOI 10.1016/j.apm.2006.08.008; MLADINEO RH, 1986, MATH PROGRAM, V34, P188, DOI 10.1007/BF01580583; Munos R., 2011, P ADV NEUR INF PROC; Murphy K. P., 2012, MACHINE LEARNING PRO, P521; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; SHUBERT BO, 1972, SIAM J NUMER ANAL, V9, P379, DOI 10.1137/0709036; Smola A. J., 2012, P 29 INT C MACH LEAR; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; STRONGIN RG, 1973, ENG CYBERN, V11, P549; SURJANOVIC S, 2014, VIRTUAL LIB SIMULATI; Walsh TJ, 2010, AAAI CONF ARTIF INTE, P612; Wang Z., 2013, INT JOINT C ART INT; Wang ZX, 2014, C IND ELECT APPL, P1005, DOI 10.1109/ICIEA.2014.6931310; Zwolak JW, 2005, IEE P SYST BIOL, V152, P81, DOI 10.1049/ip-syb:20045032	24	4	4	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100083
C	Ma, YA; Chen, TQ; Fox, EB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ma, Yi-An; Chen, Tianqi; Fox, Emily B.			A Complete Recipe for Stochastic Gradient MCMC	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				LANGEVIN	Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers-including stochastic gradient versions-based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.	[Ma, Yi-An; Chen, Tianqi; Fox, Emily B.] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Ma, YA (corresponding author), Univ Washington, Seattle, WA 98195 USA.	yianma@u.washington.edu; tqchen@cs.washington.edu; ebfox@stat.washington.edu		Fox, Emily/0000-0003-3188-9685	ONR [N00014-10-1-0746]; NSF CAREER Award [IIS-1350133]; TerraSwarm Research Center - MARCO; TerraSwarm Research Center - DARPA	ONR(Office of Naval Research); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); TerraSwarm Research Center - MARCO; TerraSwarm Research Center - DARPA	This work was supported in part by ONR Grant N00014-10-1-0746, NSF CAREER Award IIS-1350133, and the TerraSwarm Research Center sponsored by MARCO and DARPA. We also thank Mr. Lei Wu for helping with the proof of Theorem 2 and Professors Ping Ao and Hong Qian for many discussions.	Ahn S., 2012, INT C MACH LEARN ICM; Ahn S., 2014, P 31 INT C MACH LEAR; [Anonymous], 2014, P 31 INT C MACH LEAR; Bardenet R., 2014, P 30 INT C MACH LEAR; Betancourt M., 2015, P 31 INT C MACH LEAR; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Ding N, 2014, ADV NEUR IN, V27; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Feller W, 1950, INTRO PROBABILITY TH; Frank T, 1996, FOKKER PLANCK EQUATI; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Korattikara A., 2014, P 30 INT C MACH LEAR; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Patterson S., 2013, ADV NEURAL INFORM PR, V26; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Shi JH, 2012, J STAT PHYS, V148, P579, DOI 10.1007/s10955-012-0532-8; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xifara T, 2014, STAT PROBABIL LETT, V91, P14, DOI 10.1016/j.spl.2014.04.002; Yin L, 2006, J PHYS A-MATH GEN, V39, P8593, DOI 10.1088/0305-4470/39/27/003; Zwanzig, 2001, NONEQUILIBRIUM STAT	20	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102040
C	Maystre, L; Grossglauser, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Maystre, Lucas; Grossglauser, Matthias			Fast and Accurate Inference of Plackett-Luce Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We show that the maximum-likelihood (ML) estimate of models derived from Luce's choice axiom (e.g., the Plackett-Luce model) can be expressed as the stationary distribution of a Markov chain. This conveys insight into several recently proposed spectral inference algorithms. We take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the Plackett-Luce model. With a simple adaptation, this algorithm can be used iteratively, producing a sequence of estimates that converges to the ML estimate. The ML version runs faster than competing approaches on a benchmark of five datasets. Our algorithms are easy to implement, making them relevant for practitioners at large.	[Maystre, Lucas; Grossglauser, Matthias] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Maystre, L (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	lucas.maystre@epfl.ch; matthias.grossglauser@epfl.ch						[Anonymous], 1998, PAGERANK CITATION RA; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Caron F, 2012, J COMPUT GRAPH STAT, V21, P174, DOI 10.1080/10618600.2012.638220; Dwork C, 2001, P 10 INT C WORLD WID; DYKSTRA O, 1960, BIOMETRICS, V16, P176, DOI 10.2307/2527550; Elo Arpad E, 1978, RATING CHESSPLAYERS; Ford L.R., 1957, AM MATH MON, V64, P28, DOI DOI 10.2307/2308513; Guiver John, 2009, P 26 INT C MACH LEAR; Hajek B, 2014, ADV NEUR IN, V27; Hastie T, 1998, ANN STAT, V26, P451; Hunter DR, 2004, ANN STAT, V32, P384; Kamishima T, 2009, STUD COMPUT INTELL, V165, P261; Kumar R, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P359, DOI 10.1145/2684822.2685310; Levin D., 2008, MARKOV CHAINS MIXING; Luce R, 1959, INDIVIDUAL CHOICE BE; McFadden D., 1973, FRONTIERS ECONOMETRI, P105, DOI DOI 10.1108/EB028592; Negahban S., 2012, ADV NEURAL INFORM PR, V25; PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193; RAO PV, 1967, J AM STAT ASSOC, V62, P194, DOI 10.2307/2282923; Saaty TL, 1980, ANAL HIERARCHY PROCE; Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26; Thurstone L. L., 1927, J ABNORMAL SOCIAL PS, V21, P384, DOI DOI 10.1037/H0065439; Zermelo E, 1929, MATH Z, V29, P436, DOI 10.1007/BF01180541; [No title captured]	24	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100049
C	Mohri, M; Medina, AM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mohri, Mehryar; Medina, Andres Munoz			Revenue Optimization against Strategic Buyers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				REGRET; AUCTIONS	We present a revenue optimization algorithm for posted-price auctions when facing a buyer with random valuations who seeks to optimize his gamma-discounted surplus. In order to analyze this problem we introduce the notion of epsilon-strategic buyer, a more natural notion of strategic behavior than what has been considered in the past. We improve upon the previous state-of-the-art and achieve an optimal regret bound in O(log T + 1/log (1/gamma)) when the seller selects prices from a finite set and provide a regret bound in (O) over tilde (root T + T-1/4/log(1/gamma)) when the prices offered are selected out of the interval [0, 1].	[Mohri, Mehryar] Courant Inst Math Sci, 251 Mercer St, New York, NY 10012 USA; [Medina, Andres Munoz] Google Res, New York, NY 10011 USA	Google Incorporated	Mohri, M (corresponding author), Courant Inst Math Sci, 251 Mercer St, New York, NY 10012 USA.				NSF [CCF-1535987, IIS-1117591]	NSF(National Science Foundation (NSF))	We thank Afshin Rostamizadeh and Umar Syed for useful discussions about the topic of this paper and the NIPS reviewers for their insightful comments. This work was partly funded by NSF IIS-1117591 and NSF CCF-1535987.	Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Amin K., 2014, ADV NEURAL INFORM PR, P622; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Arora R., 2012, P ICML; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bikhchandani S., 2012, BE J THEOR ECON, V12, P1935; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2015, IEEE T INFORM THEORY, V61, P549, DOI 10.1109/TIT.2014.2365772; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Cui Y., 2011, P 17 ACM SIGKDD INT, P265, DOI DOI 10.1145/2020408.2020454; Dani V, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P937, DOI 10.1145/1109557.1109660; Edelman B, 2007, DECIS SUPPORT SYST, V43, P192, DOI 10.1016/j.dss.2006.08.008; Kanoria Y, 2014, LECT NOTES COMPUT SC, V8877, P232, DOI 10.1007/978-3-319-13129-0_17; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Milgrom Paul., 2004, PUTTING AUCTION THEO; MILGROM PR, 1982, ECONOMETRICA, V50, P1089, DOI 10.2307/1911865; Mohri M, 2014, PR MACH LEARN RES, V32; Mohri Mehryar, 2014, ADV NEURAL INFORM PR, P1871; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Nachbar JH, 1997, ECONOMETRICA, V65, P275, DOI 10.2307/2171894; Nachbar JH, 2001, SOC CHOICE WELFARE, V18, P303, DOI 10.1007/PL00007181; VICKREY W, 1961, J FINANC, V16, P8, DOI 10.2307/2977633	23	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103064
C	Mroueh, Y; Voinea, S; Poggio, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mroueh, Youssef; Voinea, Stephen; Poggio, Tomaso			Learning with Group Invariant Features: A Kernel Perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar-integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.	[Mroueh, Youssef] IBM Watson Grp, Armonk, NY 10504 USA; [Voinea, Stephen; Poggio, Tomaso] MIT, CBMM, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mroueh, Y (corresponding author), IBM Watson Grp, Armonk, NY 10504 USA.	mroueh@us.ibm.com; voinea@mit.edu; tp@ai.mit.edu			Nuance Foundation Grant; Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF 1231216]	Nuance Foundation Grant; Center for Brains, Minds and Machines (CBMM) - NSF STC award	Stephen Voinea acknowledges the support of a Nuance Foundation Grant. This work was also supported in part by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216.	Abu-Mostafa Y. S., 1990, Journal of Complexity, V6, P192, DOI 10.1016/0885-064X(90)90006-Y; [Anonymous], 2008, INFORM SCI STAT; Anselmi F., 2013, CORR; Bach F. R., 2015, CORR; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Benzeghiba M, 2007, SPEECH COMMUN, V49, P763, DOI 10.1016/j.specom.2007.02.006; Bo L., 2010, NIPS; Bruna J., 2012, CORR; Cho Y., 2009, NIPS, P342; Haasdonk B., 2005, SCIA; Hinton G., 2011, ICANN 11; Johnson W. B., 1984, C MOD AN PROB; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mairal J., 2014, NIPS; Rahimi A., 2008, NIPS; Rahimi Ali, 2008, P 46 ANN ALL C; Tacchetti A., 2013, CORR; Vapnik V.N, 1998, STAT LEARNING THEORY; Voinea S., 2014, WORD LEVEL INVARIANT, V14, P3201; Wahba G., 1990, SPLINE MODELS OBSERV; Walder C., 2007, NIPS; Williams C. K. I., 2001, NIPS	25	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101058
C	Neu, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Neu, Gergely			Explore no more: Improved high-probability regret bounds for non-stochastic bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHMS	This work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems, focusing on performance guarantees that hold with high probability. Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation. One of these modifications is forcing the learner to sample arms from the uniform distribution at least Omega(root T) times over T rounds, which can adversely affect performance if many of the arms are suboptimal. While it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component. Our result relies on a simple and intuitive loss-estimation strategy called Implicit eXploration (IX) that allows a remarkably clean analysis. To demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework. Finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique.	[Neu, Gergely] INRIA Lille Nord Europe, SequeL Team, Villeneuve Dascq, France		Neu, G (corresponding author), Pompeu Fabra Univ, Dept Informat & Commun Technol, Barcelona, Spain.	gergely.neu@gmail.com			INRIA; French Ministry of Higher Education and Research; FUI project Hermes	INRIA; French Ministry of Higher Education and Research; FUI project Hermes	This work was supported by INRIA, the French Ministry of Higher Education and Research, and by FUI project Hermes. The author wishes to thank Haipeng Luo for catching a bug in an earlier version of the paper, and the anonymous reviewers for their helpful suggestions.	Alon N., 2012, NIPS 25, P1610; Alon N., 2014, ARXIV14098428; Audibert J. -Y., 2009, COLT 2009; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Beygelzimer A, 2011, INT C ART INT STAT, V15, P19; Bubeck S., 2012, MINIMAX POLICIES ONL; Bubeck S, 2012, REGRET ANAL STOCHAST; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hazan E, 2011, J MACH LEARN RES, V12, P1287; Hazan Elad, 2014, J MACHINE LEARNING R, V35, P408; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kocak Tomas, 2014, ADV NEURAL INFORM PR, P613; Mannor Shie, 2011, P NIPS; McMahan H. B., 2009, COLT; Neu G., 2015, C LEARN THEOR, V40, P1360; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Seldin Y., 2012, P WORKSH ON LIN TRAD, V2; Stoltz G., 2012, P ADV NEUR INF PROC, P471; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371	26	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100100
C	Neyshabur, B; Salakhutdinov, R; Srebro, N		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Neyshabur, Behnam; Salakhutdinov, Ruslan; Srebro, Nathan			Path-SGD: Path-Normalized Optimization in Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and Ada-Grad.	[Neyshabur, Behnam; Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA; [Salakhutdinov, Ruslan] Univ Toronto, Dept Stat, Toronto, ON, Canada; [Salakhutdinov, Ruslan] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	Toyota Technological Institute - Chicago; University of Toronto; University of Toronto	Neyshabur, B (corresponding author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	bneyshabur@ttic.edu; rsalakhu@cs.toronto.edu; nati@ttic.edu			NSF [IIS-1302662]; Intel ICRI-CI	NSF(National Science Foundation (NSF)); Intel ICRI-CI	Research was partially funded by NSF award IIS-1302662 and Intel ICRI-CI. We thank Ryota Tomioka and Hao Tang for insightful discussions and Leon Bottou for pointing out the connection to natural gradient.	Duchi J, 2011, J MACH LEARN RES, V12, P2121; Glorot X., 2010, P 13 INT C ART INT S, VVolume 9, P249; He K., 2015, ARXIV150201852; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Martens James, 2015, ICML; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Neyshabur Behnam, 2015, INT C LEARN REPR ICL; Neyshabur Behnam, 2015, COLT; Sanjoy D., 2013, INT C MACH LEARN, P1319, DOI DOI 10.5555/3042817.3043084; Srebro N., 2011, ADV NEURAL INFORM PR, V24, P2645; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z	16	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101057
C	Papa, G; Clemencon, S; Bellet, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Papa, Guillaume; Clemencon, Stephan; Bellet, Aurelien			SGD Algorithms based on Incomplete U-statistics: Large-Scale Minimization of Empirical Risk	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In many learning problems, ranging from clustering to ranking through metric learning, empirical estimates of the risk functional consist of an average over tuples (e.g., pairs or triplets) of observations, rather than over individual observations. In this paper, we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems. We argue that in the large-scale setting, gradient estimates should be obtained by sampling tuples of data points with replacement (incomplete U-statistics) instead of sampling data points without replacement (complete U-statistics based on subsamples). We develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the Stochastic Gradient Descent (SGD) algorithm. It reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost. Beyond the rate bound analysis, experiments on AUC maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach.	[Papa, Guillaume; Clemencon, Stephan] Univ Paris Saclay, CNRS, Telecom ParisTech, LTCI, F-75013 Paris, France; [Bellet, Aurelien] INRIA Lille Nord Europe, Magnet Team, F-59650 Villeneuve Dascq, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay	Papa, G (corresponding author), Univ Paris Saclay, CNRS, Telecom ParisTech, LTCI, F-75013 Paris, France.	guillaume.papa@telecom-paristech.fr; stephan.clemencon@telecom-paristech.fr; aurelien.bellet@inria.fr			chair Machine Learning for Big Data of Telecom ParisTech	chair Machine Learning for Big Data of Telecom ParisTech	This work was supported by the chair Machine Learning for Big Data of Telecom ParisTech, and was conducted when A. Bellet was affiliated with Telecom ParisTech.	Bach F., 2011, NIPS; Bellet A., 2013, ARXIV; Bellet A., 2015, METRIC LEARNING; Bottou L., 2007, NIPS; Clemencon S., 2014, IEEE BIG DATA; Clemencon S., 2013, SDM; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; DEFAZIO A, 2014, NIPS; Fort G., 2014, ESAIMPS; Furnkranz J, 2009, LECT NOTES ARTIF INT, V5781, P359, DOI 10.1007/978-3-642-04180-8_41; Herschtal A., 2004, P 21 INT C MACH LEAR, P49; JANSON S, 1984, Z WAHRSCHEINLICHKEIT, V66, P495, DOI 10.1007/BF00531887; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kar P., 2013, ICML; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Lee A.J., 2019, U STAT THEORY PRACTI; Mairal J., 2014, ARXIV14024419; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Norouzi M., 2012, ADV NEURAL INFORM PR, V25; Pelletier M., 1998, ANN APPL PROB; Qian Q, 2015, MACH LEARN, V99, P353, DOI 10.1007/s10994-014-5456-x; Schmidt M. W., 2012, NIPS; Zhao P., 2011, P 28 INT C MACHINE L, P233; Zhao Peilin, 2015, ICML	28	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101078
C	Besbes, O; Gur, Y; Zeevi, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Besbes, Omar; Gur, Yonatan; Zeevi, Assaf			Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REGRET	In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward "variation" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.	[Besbes, Omar; Zeevi, Assaf] Columbia Univ, New York, NY 10027 USA; [Gur, Yonatan] Stanford Univ, Stanford, CA 94305 USA	Columbia University; Stanford University	Besbes, O (corresponding author), Columbia Univ, New York, NY 10027 USA.	ob2105@columbia.edu; ygur@stanford.edu; assaf@gsb.columbia.edu						Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Azar M. G., 2014, ARXIV14020562; Bergemann D, 1996, ECONOMETRICA, V64, P1125, DOI 10.2307/2171959; Bergemann D, 2005, RAND J ECON, V36, P719; Berry D.A., 1985, BANDIT PROBLEMS SEQU; Bertsimas D, 2000, OPER RES, V48, P80, DOI 10.1287/opre.48.1.80.12444; Besbes O., 2014, WORKING PAPER; Blackwell David, 1956, PAC J MATH, V6, P1, DOI [DOI 10.2140/PJM.1956.6.1, 10.2140/pjm.1956.6.1]; Caro F, 2007, MANAGE SCI, V53, P276, DOI 10.1287/mnsc.1060.0613; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Foster DP, 1999, GAME ECON BEHAV, V29, P7, DOI 10.1006/game.1999.0740; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Garivier A, 2011, LECT NOTES ARTIF INT, V6925, P174, DOI 10.1007/978-3-642-24412-4_16; Gittins J., 1989, MULTIARMED BANDIT AL; Gittins J. C., 1974, DYNAMIC ALLOCATION I; GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148; Guha S, 2007, ANN IEEE SYMP FOUND, P483, DOI 10.1109/FOCS.2007.23; Hannan J., 1957, CONTRIBUTIONS THEORY, V3; Hartland C., 2006, NIPS 2006 WORKSH ONL; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Ortner Ronald, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P214, DOI 10.1007/978-3-642-34106-9_19; Pandey S., 2007, SIAM INT C DATA MINI; PAPADIMITRIOU CH, 1994, STRUCT COMPL TH CONF, P318, DOI 10.1109/SCT.1994.315792; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Slivkins Aleksandrs, 2008, COLT, P343; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; WHITTLE P, 1981, ANN PROBAB, V9, P284, DOI 10.1214/aop/1176994469; Whittle P., 1988, J APPL PROBAB, V25, P287, DOI DOI 10.2307/3214163; ZELEN M, 1969, J AM STAT ASSOC, V64, P131, DOI 10.2307/2283724	32	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101049
C	Gal, Y; van der Wilk, M; Rasmussen, CE		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gal, Yarin; van der Wilk, Mark; Rasmussen, Carl E.			Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.	[Gal, Yarin; van der Wilk, Mark; Rasmussen, Carl E.] Univ Cambridge, Cambridge, England	University of Cambridge	Gal, Y (corresponding author), Univ Cambridge, Cambridge, England.	yg279@cam.ac.uk; mv310@cam.ac.uk; cer54@cam.ac.uk						Arnold S., 1981, WILEY SERIES PROBABI; Asuncion A. U., 2008, ADV NEURAL INFORM PR, P81; Brockwell AE, 2006, J COMPUT GRAPH STAT, V15, P246, DOI 10.1198/106186006X100579; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Gal Y., 2014, P 31 INT C MACH LEAR; Hensman J., 2013, UAI; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; Lovell Dan, 2012, WORKSH BIG LEARN NIP; MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5; Quinonero-Candela J., 2005, J MACH LEARN RES, V6, P2005; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Titsias M., 2010, BAYESIAN GAUSSIAN PR; Titsias M. K, 2009, TECHNICAL REPORT; Wilkinson D. J., 2005, HDB PARALLEL COMPUTI, P477, DOI DOI 10.1201/9781420028683; Williamson Sinead, 2013, P 30 INT C MACH LEAR, P98	16	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103044
C	Gillenwater, J; Kulesza, A; Fox, E; Taskar, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gillenwater, Jennifer; Kulesza, Alex; Fox, Emily; Taskar, Ben			Expectation-Maximization for Learning Determinantal Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard [1]. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix [2], or learning weights for a linear combination of DPPs with fixed kernel matrices [3]. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.	[Gillenwater, Jennifer] Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA; [Kulesza, Alex] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA; [Fox, Emily] Univ Washington, Stat, Seattle, WA 98195 USA; [Taskar, Ben] Univ Washington, Comp Sci & Engn, Seattle, WA 98195 USA	University of Pennsylvania; University of Michigan System; University of Michigan; University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Gillenwater, J (corresponding author), Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA.	jengi@cis.upenn.edu; kulesza@umich.edu; ebfox@stat.washington.edu; taskar@cs.washington.edu		Fox, Emily/0000-0003-3188-9685	ONR [N00014-10-1-0746]	ONR(Office of Naval Research)	This work was supported in part by ONR Grant N00014-10-1-0746.	Affandi R., 2012, C UNC ART INT UAI; Affandi R., 2013, C ART INT STAT AISTA; Affandi R. H., 2013, NIPS; Affandi R. H., 2014, INT C MACH LEARN ICM; Dughmi S., 2009, ELECT COMMERCE; Edelman A., 1998, SIAM J MATRIX ANAL A; Gillenwater J., 2012, EMPIRICAL METHODS NA; Gillenwater J., 2012, NIPS; Hough J., 2006, PROBABILITY SURVEYS, V3; JAMES AT, 1964, ANN MATH STAT, V35, P475, DOI 10.1214/aoms/1177703550; Kang B., 2013, NIPS; Krause A., 2005, C UNC ART INT UAI; Krause A, 2008, J MACH LEARN RES, V9, P235; Kulesza A., 2010, NIPS; Kulesza A., 2012, FDN TRENDS MACHINE L, V5; Kulesza A., 2012, THESIS; Kulesza A., 2011, INT C MACH LEARN ICM; Kulesza Alex, 2011, C UNC ART INT UAI; Levitin Evgeny S, 1966, USSR COMP MATH MATH, V6, P1, DOI DOI 10.1016/0041-5553(66)90114-5; Lin H., 2012, ARXIV PREPRINT ARXIV; Macchi O., 1975, ADV APPL PROBABILITY, V7; Neal R. M., 1998, LEARNING GRAPHICAL M; Nemhauser G. L., 1978, MATH PROGRAMMING, V14; Petersen Kaare Brandt, 2012, TECHNICAL REPORT; Shah A., 2013, C UNC ART INT UAI; Snoek J., 2013, NIPS; Zou J., 2013, NIPS	28	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103015
C	Gunter, T; Osborne, MA; Garnett, R; Hennig, P; Roberts, SJ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gunter, Tom; Osborne, Michael A.; Garnett, Roman; Hennig, Philipp; Roberts, Stephen J.			Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-) parameters (for example to compute the marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.	[Gunter, Tom; Osborne, Michael A.; Roberts, Stephen J.] Univ Oxford, Engn Sci, Oxford, England; [Garnett, Roman] Univ Bonn, Knowledge Discovery & Machine Learning, Bonn, Germany; [Hennig, Philipp] MPI Intelligent Syst, Tubingen, Germany	University of Oxford; University of Bonn; Max Planck Society	Gunter, T (corresponding author), Univ Oxford, Engn Sci, Oxford, England.	tgunter@robots.ox.ac.uk; mosb@robots.ox.ac.uk; rgarnett@uni-bonn.de; phennig@tuebingen.mpg.de; sjrob@robots.ox.ac.uk						Brooks SP, 1998, STAT COMPUT, V8, P319, DOI 10.1023/A:1008820505350; Cowles MK, 1999, J STAT COMPUT SIM, V64, P87, DOI 10.1080/00949659908811968; Diaconis P., 1988, STAT DECISION THEORY, V1, P163, DOI DOI 10.1007/978-1-4613-8768-8_20; Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46; Garnett R., 2012, P 29 INT C MACH LEAR; Gerritsma J, 1981, INT SHIPBUILDING PRO, V28; Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970; Kennedy M, 1998, STAT COMPUT, V8, P365, DOI 10.1023/A:1008832824006; Meng XL, 1996, STAT SINICA, V6, P831; Minka T., 2000, TECHNICAL REPORT; Neal R. M., 1993, PROBABILISTIC INFERE; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Osborne M. A., 2012, ADV NEURAL INFORM PR; Rasmussen C. E., 2003, ADV NEURAL INFORM PR, V15; Rasmussen CE, 2010, J MACH LEARN RES, V11, P3011; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Skilling J, 2004, AIP CONF PROC, V735, P395, DOI 10.1063/1.1835238	18	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102044
C	Irsoy, O; Cardie, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Irsoy, Ozan; Cardie, Claire			Deep Recursive Neural Networks for Compositionality in Language	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture - a deep recursive neural network (deep RNN) - constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.	[Irsoy, Ozan; Cardie, Claire] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Irsoy, O (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	oirsoy@cs.cornell.edu; cardie@cs.cornell.edu			NSF [IIS-1314778]; DARPA DEFT [FA8750-13-2-0015]	NSF(National Science Foundation (NSF)); DARPA DEFT	This work was supported in part by NSF grant IIS-1314778 and DARPA DEFT FA8750-13-2-0015. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF, DARPA or the U.S. Government.	BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2001, ADV NEURAL INFORM PR; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346; Duchi J, 2011, J MACH LEARN RES, V12, P2121; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Glorot X., 2011, P 14 INT C ART INT S, P315; Goller C, 1996, IEEE IJCNN, P347, DOI 10.1109/ICNN.1996.548916; Grefenstette E., 2014, 52 ANN M ASS COMP LI; Hermans M., 2013, P ADV NEUR INF PROC, V26, P190; Hihi S. E., 1995, ADV NEURAL INFORM PR, V8, P493; Hinton G.E., 2012, ARXIV; Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4; Le Quoc V, 2014, ARXIV14054053; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; Socher R., 2013, PROC C EMPIRICAL MET; Socher R., 2011, P 28 INT C INT C MAC, P129; Socher R., 2011, ADV NEURAL INF PROCE, V24, P1; Socher Richard, 2012, P 2012 JOINT C EMP M, P1201, DOI DOI 10.1162/153244303322533223; Socher Richard, 2011, P C EMP METH NAT LAN, P151	23	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103002
C	Long, J; Zhang, N; Darrell, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Long, Jonathan; Zhang, Ning; Darrell, Trevor			Do Convnets Learn Correspondence ?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].	[Long, Jonathan; Zhang, Ning; Darrell, Trevor] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Long, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	jonlong@cs.berkeley.edu; nzhang@cs.berkeley.edu; trevor@cs.berkeley.edu			DARPA; NSF [IIS-1427425, IIS-1212798, IIS-1116411]; Toyota	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Toyota	This work was supported in part by DARPA's MSEE and SMISC programs, by NSF awards IIS-1427425, IIS-1212798, and IIS-1116411, and by support from Toyota.	[Anonymous], 2014, CVPR; [Anonymous], ICCV; [Anonymous], 2012, CVPR; [Anonymous], 2011, CVPR; Belhumeur P. N., 2011, CVPR; Berg T., 2013, CVPR; Bourdev L., 2009, ICCV; Bregler C., 2014, ICLR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Eigen D., 2014, 6 INT C LEARN REPR I; Everingham M., PASCAL VISUAL OBJECT; Felzenszwalb P., 2004, TECHNICAL REPORT; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Fischer P., 2014, ARXIV E PRINTS; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Gkioxari G., 2013, CVPR; Gkioxari G., 2014, CVPR; Huang G. B., 2007, ICCV; Huang G. B., 2012, P C NEUR INF PROC SY, V25; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jones E., 2001, SCIPY OPEN SOURCE SC; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1989, NEURAL COMPUTATION; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Lowe D. G., 1999, ICCV; Sermanet P., 2013, CVPR; Sun Min, 2011, ICCV; Szegedy Christian, 2014, ICLR; Uijlings J., 2013, IJCV; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Vondrick C., 2013, ICCV; Yang Y., 2013, PAMI; Zeiler MD, 2014, ECCV; Zhang Ning, 2014, ICML	35	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101091
C	Naesseth, CA; Lindsten, F; Schott, TB		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Naesseth, Christian A.; Lindsten, Fredrik; Schott, Thomas B.			Sequential Monte Carlo for Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SIMULATION METHODS; PARTICLE; INFERENCE	We propose a new framework for how to use sequential Monte Carlo (SMC) algorithms for inference in probabilistic graphical models (PGM). Via a sequential decomposition of the PGM we find a sequence of auxiliary distributions defined on a monotonically increasing sequence of probability spaces. By targeting these auxiliary distributions using SMC we are able to approximate the full joint distribution defined by the PGM. One of the key merits of the SMC sampler is that it provides an unbiased estimate of the partition function of the model. We also show how it can be used within a particle Markov chain Monte Carlo framework in order to construct high-dimensional block-sampling algorithms for general PGMs.	[Naesseth, Christian A.] Linkoping Univ, Div Automat Control, Linkoping, Sweden; [Lindsten, Fredrik] Univ Cambridge, Dept Engn, Cambridge, England; [Schott, Thomas B.] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden	Linkoping University; University of Cambridge; Uppsala University	Naesseth, CA (corresponding author), Linkoping Univ, Div Automat Control, Linkoping, Sweden.	chran60@isy.liu.se; fsm12@cam.ac.uk; thomas.schon@it.uu.se	Schön, Thomas B/D-4169-2009	Schön, Thomas B/0000-0001-5183-234X	project: Learning of complex dynamical systems - Swedish Research Council [637-2014-466]; project: Probabilistic modeling of dynamical systems - Swedish Research Council [621-2013-5524]	project: Learning of complex dynamical systems - Swedish Research Council; project: Probabilistic modeling of dynamical systems - Swedish Research Council	We would like to thank Iain Murray for his kind and very prompt help in providing the data for the LDA example. This work was supported by the projects: Learning of complex dynamical systems (Contract number: 637-2014-466) and Probabilistic modeling of dynamical systems (Contract number: 621-2013-5524), both funded by the Swedish Research Council.	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bouchard-Cote A, 2012, SYST BIOL, V61, P579, DOI 10.1093/sysbio/syr131; Briers M., 2005, P 8 INT C INF FUS PH; Buntine W, 2009, LECT NOTES ARTIF INT, V5828, P51, DOI 10.1007/978-3-642-05224-8_6; Carbonetto P., 2007, ADV NEURAL INFORM PR, V19; Del Moral P., 2004, PROB APPL S; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Doucet A., 2009, HDB NONLINEAR FILTER, P656; Doucet A., 2001, SEQUENTIAL MONTE CAR; Everitt RG, 2012, J COMPUT GRAPH STAT, V21, P940, DOI 10.1080/10618600.2012.687493; Fearnhead P, 2003, J R STAT SOC B, V65, P887, DOI 10.1111/1467-9868.00421; Frank A., 2009, ADV NEURAL INFORM PR, P826; Hamze F., 2004, P 20 C UNC ART INT U; Hamze F., 2005, ADV NEURAL INFORM PR; Ihler A. T., 2009, P INT C ART INT STAT; Isard M., 2003, P C COMP VIS PATT RE; Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026; Kosterlitz JM, 1973, J PHYS C SOLID STATE, V6, P1181, DOI 10.1088/0022-3719/6/7/010; Lindsten F, 2014, J MACH LEARN RES, V15, P2145; Lindsten F, 2013, FOUND TRENDS MACH LE, V6, P1, DOI 10.1561/2200000045; Naesseth C. A., 2014, P IEEE INF THEOR WOR; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Pitt MK, 2012, J ECONOMETRICS, V171, P134, DOI 10.1016/j.jeconom.2012.06.004; Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Scott G. S., 2009, P 16 INT C ART INT S, P1105; Sudderth E. B., 2003, P C COMP VIS PATT RE; Sudderth EB, 2010, COMMUN ACM, V53, P95, DOI 10.1145/1831407.1831431; Tomita Y, 2002, PHYS REV B, V65, DOI 10.1103/PhysRevB.65.184405; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515	31	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103021
C	Needell, D; Srebro, N; Ward, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Needell, Deanna; Srebro, Nathan; Ward, Rachel			Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We improve a recent guarantee of Bach and Moulines on the linear convergence of SOD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SOD can improve convergence also in other scenarios. Our results are based on a connection between SOD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.	[Needell, Deanna] Claremont Mckenna Coll, Dept Math Sci, Claremont, CA 91711 USA; [Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA; [Srebro, Nathan] Technion, Dept Comp Sci, Haifa, Israel; [Ward, Rachel] Univ Texas Austin, Dept Math, Austin, TX 78712 USA	Claremont Colleges; Claremont Graduate School; Claremont McKenna College; Toyota Technological Institute - Chicago; Technion Israel Institute of Technology; University of Texas System; University of Texas Austin	Needell, D (corresponding author), Claremont Mckenna Coll, Dept Math Sci, Claremont, CA 91711 USA.	dneedell@cmc.edu; nati@ttic.edu; rward@math.utexas.edu						CENSOR Y, 1983, NUMER MATH, V41, P83, DOI 10.1007/BF01396307; Foygel R., 2011, 24 ANN C LEARN THEOR; HANKE M, 1990, LINEAR ALGEBRA APPL, V130, P83, DOI 10.1016/0024-3795(90)90207-S; Herman GT, 2009, ADV PATTERN RECOGNIT, P1, DOI 10.1007/978-1-84628-723-7; HOUNDFIELD GN, 1973, BRIT J RADIOL, V46, P1016, DOI 10.1259/0007-1285-46-552-1016; Kaczmarz S., 1937, B INT LACADEMIE POLO, P335; Moulines E., 2011, ADV NEURAL INF PROCE, V24; Natterer F., 2001, CLASSICS APPL MATH, V32, DOI [10.1137/1.9780898719284, DOI 10.1137/1.9780898719284]; Needell D, 2013, J FOURIER ANAL APPL, V19, P256, DOI 10.1007/s00041-012-9248-z; Needell D, 2010, BIT, V50, P395, DOI 10.1007/s10543-010-0265-5; Nemirovski A, 2005, EFFICIENT METHODS CO; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Schmidt Mark, 2013, ARXIV13092388; Srebro N., 2010, ADV NEURAL INFORM PR; Strohmer T, 2009, J FOURIER ANAL APPL, V15, P262, DOI 10.1007/s00041-008-9030-4; TANABE K, 1971, NUMER MATH, V17, P203, DOI 10.1007/BF01436376; WHITNEY T. M., 1967, SIAM J NUMER ANAL, V4, P109; Zhao P., 2014, STOCHASTIC OPT UNPUB	22	4	4	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101026
C	Qian, J; Saligrama, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Qian, Jing; Saligrama, Venkatesh			Efficient Minimax Signal Detection on Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Several problems such as network intrusion, community detection, and disease outbreak can be described by observations attributed to nodes or edges of a graph. In these applications presence of intrusion, community or disease outbreak is characterized by novel observations on some unknown connected subgraph. These problems can be formulated in terms of optimization of suitable objectives on connected subgraphs, a problem which is generally computationally difficult. We overcome the combinatorics of connectivity by embedding connected subgraphs into linear matrix inequalities (LMI). Computationally efficient tests are then realized by optimizing convex objective functions subject to these LMI constraints. We prove, by means of a novel Euclidean embedding argument, that our tests are minimax optimal for exponential family of distributions on 1-D and 2-D lattices. We show that internal conductance of the connected subgraph family plays a fundamental role in characterizing detectability.	[Qian, Jing] Boston Univ, Div Syst Engn, Brookline, MA 02446 USA; [Saligrama, Venkatesh] Boston Univ, Dept Elect & Comp Engn, Boston, MA 02215 USA	Boston University; Boston University	Qian, J (corresponding author), Boston Univ, Div Syst Engn, Brookline, MA 02446 USA.	jingq@bu.edu; srv@bu.edu		Saligrama, Venkatesh/0000-0002-0675-2268				Addario-Berry L, 2010, ANN STAT, V38, P3063, DOI 10.1214/10-AOS817; Arias-Castro E, 2005, IEEE T INFORM THEORY, V51, P2402, DOI 10.1109/TIT.2005.850056; Arias-Castro E, 2008, ANN STAT, V36, P1726, DOI 10.1214/07-AOS526; Arias-Castro E, 2011, ANN STAT, V39, P278, DOI 10.1214/10-AOS839; Bailly-Bechet M, 2011, P NATL ACAD SCI USA, V108, P882, DOI 10.1073/pnas.1004751108; Boyd S, 2004, CONVEX OPTIMIZATION; Chung F., 1996, SPECTRAL GRAPH THEOR; CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341; Duczmal L, 2004, COMPUT STAT DATA AN, V45, P269, DOI 10.1016/S0167-9473(02)00302-X; Ermis EB, 2010, IEEE T SIGNAL PROCES, V58, P843, DOI 10.1109/TSP.2009.2033300; Glaz J., 2001, SCAN STAT; Kulldorff M., 2006, STAT MED, V25; Patil GP, 2003, STAT SCI, V18, P457, DOI 10.1214/ss/1081443229; Priebe C. E., 2006, COMPUTATIONAL MATH O; Qian J., 2014, INT C ART INT STAT A; Saligrama V., 2012, ARTIFICIAL INTELLIGE, V22; Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917; Sharpnack J., 2013, INT C ART INT STAT; Sharpnack James, 2013, NEURAL INFORM PROCES	19	4	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102086
C	Shrivastava, A; Li, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Shrivastava, Anshumali; Li, Ping			Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present the first provably sublinear time hashing algorithm for approximate Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable distributions for L-2 norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.	[Shrivastava, Anshumali] Cornell Univ, Dept Comp Sci, Comp & Informat Sci, Ithaca, NY 14853 USA; [Li, Ping] Rutgers State Univ, Dept Stat & Biostat, Dept Comp Sci, Piscataway, NJ 08854 USA	Cornell University; Rutgers State University New Brunswick	Shrivastava, A (corresponding author), Cornell Univ, Dept Comp Sci, Comp & Informat Sci, Ithaca, NY 14853 USA.	anshu@cs.cornell.edu; pingli@stat.rutgers.edu			 [NSF-DMS-1444124];  [NSF-III-1360971];  [NSF-Bigdata-1419210];  [ONR-N00014-13-1-0764];  [AFOSR-FA9550-13-1-0137];  [CS6784]	; ; ; ; ; 	The research is partially supported by NSF-DMS-1444124, NSF-III-1360971, NSF-Bigdata-1419210, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137. We appreciate the constructive comments from the program committees of KDD 2014 and NIPS 2014. Shrivastava would also like to thank Thorsten Joachims and the Class of CS6784 (Spring 2014) for valuable feedbacks.	Andoni A., 2004, TECHNICAL REPORT; Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Cremonesi P., 2010, P 2010 ACM C RECOMME, P39, DOI [10.1145/1864708.1864721, DOI 10.1145/1864708.1864721]; Curtin R.R., 2013, P 2013 SIAM INT C DA, P1; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dean T, 2013, PROC CVPR IEEE, P1814, DOI 10.1109/CVPR.2013.237; FRIEDMAN JH, 1974, IEEE T COMPUT, VC 23, P881, DOI 10.1109/T-C.1974.224051; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Koenigstein Noam, 2012, P CIKM 12, P535, DOI DOI 10.1145/2396761.2396831; Koren Y., MATRIX FACTORIZATION; Li P., 2014, ARXIV14038144; Li P., 2011, COMMUN ACM; Li Ping, 2014, ICML; Neyshabur B, 2014, ARXIV14105518; Ram Parikshit, 2012, P 18 ACM SIGKDD INT, P931, DOI DOI 10.1145/2339530.2339677; Shrivastava A., 2014, TECHNICAL REPORT; Shrivastava A., 2014, AISTATS; Shrivastava Anshumali, 2014, ARXIV14105410; Shrivastava Anshumali, 2013, NIPS; Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194	23	4	4	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100108
C	Zhang, CJ; Shah, JA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhang, Chongjie; Shah, Julie A.			Fairness in Multi-Agent Sequential Decision-Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests. This new criterion aims to maximize the worst performance of agents with a consideration on the overall performance. We develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy. This game-theoretic approach formulates this fairness optimization as a two-player zero-sum game and employs an iterative algorithm for finding a Nash equilibrium, corresponding to an optimal fairness policy. We scale up this approach by exploiting problem structure and value function approximation. Our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.	[Zhang, Chongjie; Shah, Julie A.] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Zhang, CJ (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	chongjie@csail.mit.edu; julie_a_shah@csail.mit.edu						Bonald T., 2001, Performance Evaluation Review, V29, P82, DOI 10.1145/384268.378438; Chen Yuqiang, 2010, P 24 AAAI C ART INT; Chevaleyre Y, 2006, INFORM-J COMPUT INFO, V30, P3; Guestrin C, 2003, J ARTIF INTELL RES, V19, P399, DOI 10.1613/jair.1000; Inc. Gurobi Optimization, 2014, GUROBI OPTIMIZER REF; Kash I., 2013, P INT C AUT AG MULT, P351; McLennan A, 2005, GAME ECON BEHAV, V51, P264, DOI 10.1016/j.geb.2004.10.008; McMahan H Brendan, 2003, P 20 INT C MACH LEAR, P536; Nace D, 2008, IEEE COMMUN SURV TUT, V10, P5, DOI 10.1109/SURV.2008.080403; Ogryczak W, 2013, INT J INF TECH DECIS, V12, P1021, DOI 10.1142/S0219622013400075; Porter R, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P664; Procaccia AD, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P239; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rawls, 1971, THEORY JUSTICE; Roijers DM, 2013, J ARTIF INTELL RES, V48, P67, DOI 10.1613/jair.3987; Steuer R., 1986, MULTIPLE CRITERIA OP; Walsh T, 2011, LECT NOTES ARTIF INT, V6992, P292, DOI 10.1007/978-3-642-24873-3_22	17	4	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103039
C	Gentile, C		Thrun, S; Saul, K; Scholkopf, B		Gentile, C			Fast feature selection from microarray expression data via multiplicative large margin algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				CLASSIFICATION; CANCER; PATTERNS	New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classification of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with five known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks.	Univ Insubria, DICOM, I-21100 Varese, Italy	University of Insubria	Gentile, C (corresponding author), Univ Insubria, DICOM, Via Mazzini 5, I-21100 Varese, Italy.							Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Ben-Dor A, 2000, J COMPUT BIOL, V7, P559, DOI 10.1089/106652700750050943; BRADLEY PS, 1998, P 15 INT C MACH LEAR, P82; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Fodor SPA, 1997, SCIENCE, V277, P393, DOI 10.1126/science.277.5324.393; GENTILE C, 2001, IN PRESS MACHINE LEA; GENTILE C, 2001, J MACHINE LEARNING R, V2, P213; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Grove AJ, 2001, MACH LEARN, V43, P173, DOI 10.1023/A:1010844028087; Gruvberger S, 2001, CANCER RES, V61, P5979; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; MANGASARIAN O, 1997, DATA MIN KNOWL DISC, V42, P183; SINGH D, 2002, CANC CELL, V1; TIBSHIRANI R, 1995, J ROY STAT SOC B MET, V1, P267; WESTON J, 2000, P NIPS, V13; WESTON J, 2002, IN PRESS JMLR; XING E, 2001, P 18 ICML	22	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						121	128						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500016
C	Li, YQ; Cichocki, A; Amari, SI; Shishkin, S; Cao, JT; Gu, FJ		Thrun, S; Saul, K; Scholkopf, B		Li, YQ; Cichocki, A; Amari, SI; Shishkin, S; Cao, JT; Gu, FJ			Sparse representation and its applications in blind source separation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					In this paper, sparse representation (factorization) of a data matrix is first discussed. An overcomplete basis matrix is estimated by using the K-means method. We have proved that for the estimated overcomplete basis matrix, the sparse solution (coefficient matrix) with minimum l(1)-norm is unique with probability of one, which can be obtained using a linear programming algorithm. The comparisons of the l(1)-norm solution and the l(0)-norm solution are also presented, which can be used in recoverability analysis of blind source separation (BSS). Next, we apply the sparse rnatrix factorization approach to BSS in the overcomplete case. Generally, if the sources are not sufficiently sparse, we perform blind separation in the time-frequency domain after preprocessing the observed data using the wavelet packets transformation. Third, an EEG experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate its performance. Two almost independent components obtained by the sparse representation method are selected for phase synchronization analysis, and their periods of significant phase synchronization are found which are related to tasks. Finally, concluding remarks review the approach and state areas that require further study.	RIKEN, Brain Sci Inst, Saitama, Saitama 3510198, Japan	RIKEN	Li, YQ (corresponding author), RIKEN, Brain Sci Inst, Saitama, Saitama 3510198, Japan.		Cichocki, Andrzej/AAI-4209-2020; Shishkin, Sergei L./A-3461-2013; Cichocki, Andrzej/A-1545-2015	Shishkin, Sergei L./0000-0002-3257-1022; Cichocki, Andrzej/0000-0002-8364-7226				Chen QW, 2001, HIGH PRESSURE RES, V20, P1, DOI 10.1080/08957950108206146; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Le Van Quyen M, 2001, J NEUROSCI METH, V111, P83, DOI 10.1016/S0165-0270(01)00372-7; Lee TW, 1999, IEEE SIGNAL PROC LET, V6, P87, DOI 10.1109/97.752062; Makeig S, 2002, SCIENCE, V295, P690, DOI 10.1126/science.1066168; Olshausen BA, 2001, ADV NEUR IN, V13, P887; Zibulevsky M., 2000, INDEPENDENT COMPONEN	7	4	4	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						241	248						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500031
C	Morris, QD; Frey, BJ; Paige, CJ		Thrun, S; Saul, K; Scholkopf, B		Morris, QD; Frey, BJ; Paige, CJ			Denoising and untangling graphs using degree priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					This paper addresses the problem of untangling hidden graphs from a set of noisy detections of undirected edges. We present a model of the generation of the observed graph that includes degree-based structure priors on the hidden graphs. Exact inference in the model is intractable; we present an efficient approximate inference algorithm to compute edge appearance posteriors. We evaluate our model and algorithm on a biological graph inference problem.	Univ Toronto, Toronto, ON M5S 3G4, Canada	University of Toronto	Morris, QD (corresponding author), Univ Toronto, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.	quaid@psi.utoronto.ca; frey@psi.utoronto.ca; paige@uhnres.utoronto.ca	Morris, Quaid/ABE-8343-2020	Morris, Quaid/0000-0002-2760-6999				Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Faloutsos M, 1999, COMP COMM R, V29, P251, DOI 10.1145/316194.316229; Freeman W. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1182, DOI 10.1109/ICCV.1999.790414; FREY BJ, 1998, P 35 ALL C COMM CONT; FREY BJ, 2003, PSI200302 U TOR; FREY BJ, 2002, 2001 C ADV NEUR INF, V14; FREY BJ, 1996, P 1996 ALL C COMM CO; GOLDBERG DS, 2003, P NAT AC SCI; Gomez Shawn M, 2002, Pac Symp Biocomput, P413; Jeong H, 2000, NATURE, V407, P651, DOI 10.1038/35036627; JORDAN MI, 2004, UNPUB INTRO LEARNING; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; MacKay DJC, 1996, ELECTRON LETT, V32, P1645, DOI 10.1049/el:19961141; Mezard M, 2002, SCIENCE, V297, P812, DOI 10.1126/science.1073287; Murphy K. P., 1999, UNCERTAINTY ARTIFICI; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rzhetsky A, 2001, BIOINFORMATICS, V17, P988, DOI 10.1093/bioinformatics/17.10.988; Saito R, 2003, BIOINFORMATICS, V19, P756, DOI 10.1093/bioinformatics/btg070; VONMERING C, 2002, NATURE	19	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						385	392						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500049
C	Neal, RM; Beal, MJ; Roweis, ST		Thrun, S; Saul, K; Scholkopf, B		Neal, RM; Beal, MJ; Roweis, ST			Inferring state sequences for non-linear systems with embedded hidden Markov models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of "pools" of candidate states at each time. We then define an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in-these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G3, Canada	University of Toronto	Neal, RM (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G3, Canada.							ACHAN K, 2004, UTMLTR2004001; DOUCET A, 2000, P IEEE INT C AC SPEE, V2, P701; Neal R. M, 2003, 0304 U TOR DEP STAT; Neal RM, 1993, CRGTR931 U TOR DEP C; Scott SL, 2002, J AM STAT ASSOC, V97, P337, DOI 10.1198/016214502753479464	5	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						401	408						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500051
C	Nilim, A; El Ghaoui, L		Thrun, S; Saul, K; Scholkopf, B		Nilim, A; El Ghaoui, L			Robustness in Markov decision problems with uncertain transition matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				PROBABILITIES	Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to real-world problems. We propose an algorithm for solving finite-state and finite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efficient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost.	Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Nilim, A (corresponding author), Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.							Bagnell J. D., 2001, CMURITR0125; ELGHAOURI L, 2004, M0407 UCBERL; FEINBERG EA, 2002, HDB MARKOV DECISION; FERGUSON TS, 1974, ANN STAT, V2, P615, DOI 10.1214/aos/1176342752; GIVAN R, 1997, 4 EUR C PLANN, P234; Putterman M. L., 1994, MARKOV DECISION PROC; SATIA JK, 1973, OPER RES, V21, P728, DOI 10.1287/opre.21.3.728; SHAPIRO A, 2003, IN PRESS OPTIMIZATIO; WHITE CC, 1994, OPER RES, V42, P739, DOI 10.1287/opre.42.4.739; [No title captured]	10	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						839	846						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500105
C	Roman, N; Wang, DL; Brown, GJ		Thrun, S; Saul, K; Scholkopf, B		Roman, N; Wang, DL; Brown, GJ			A classification-based cocktail-party processor	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				SPEECH; SOUND	At a cocktail party, a listener can selectively attend to a single voice and filter out other acoustical interferences. How to simulate this perceptual ability remains a great challenge. This paper describes a novel supervised learning approach to speech segregation, in which a target speech signal is separated from interfering sounds using spatial location cues: interaural time differences (ITD) and interaural intensity differences (IID). Motivated by the auditory masking effect, we employ the notion of an ideal time-frequency binary mask, which selects the target if it is stronger than the interference in a local time-frequency unit. Within a narrow frequency band, modifications to the relative strength of the target source with respect to the interference trigger systematic changes for estimated ITD and IID. For a given spatial configuration, this interaction produces characteristic clustering in the binaural feature space. Consequently, we perform pattern classification in order to estimate ideal binary masks. A systematic evaluation in terms of signal-to-noise ratio as well as automatic speech recognition performance shows that the resulting system produces masks very close to ideal binary ones. A quantitative comparison shows that our model yields significant improvement in performance over an existing approach. Furthermore, under certain conditions the model produces large speech intelligibility improvements with normal listeners.	Ohio State Univ, Dept Comp & Informat Sci, Columbus, OH 43210 USA	Ohio State University	Roman, N (corresponding author), Ohio State Univ, Dept Comp & Informat Sci, Columbus, OH 43210 USA.	niki@cis.ohio-state.edu; dwang@cis.ohio-state.edu; g.brown@dcs.shef.ac.uk		Brown, Guy/0000-0001-8565-5476				Bench J., 1979, SPEECH HEARING TESTS; Blauert J, 1997, SPATIAL HEARING PSYC; Bodden M., 1993, Acta Acustica, V1, P43; Bregman AS, 1990, AUDITORY SCENE ANAL; Bronkhorst AW, 2000, ACUSTICA, V86, P117; BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016; Cooke M, 2001, SPEECH COMMUN, V34, P267, DOI 10.1016/S0167-6393(00)00034-0; Cooke MP, 1993, MODELING AUDITORY PR; Gardner WG, 1994, 280 MIT MED LAB; GLOTIN H, 1999, P EUROSPEECH, P2351; HU G, 2002, P NIPS; Jourjine A., 2000, P ICASSP; Liu C, 2001, J ACOUST SOC AM, V110, P3218, DOI 10.1121/1.1419090; Roman N, 2003, J ACOUST SOC AM, V114, P2236, DOI 10.1121/1.1610463; Wittkop T, 2003, SPEECH COMMUN, V39, P111, DOI 10.1016/S0167-6393(02)00062-6	15	4	4	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1425	1432						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500177
C	Andrieu, C; de Freitas, N; Doucet, A		Dietterich, TG; Becker, S; Ghahramani, Z		Andrieu, C; de Freitas, N; Doucet, A			Rao-Blackwellised particle filtering via data augmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In this paper, we extend the Rao-Blackwellised particle filtering method to more complex hybrid models consisting of Gaussian latent variables and discrete observations. This is accomplished by augmenting the models with artificial variables that enable us to apply Rao-Blackwellisation. Other improvements include the design of an optimal importance proposal distribution and being able to swap the sampling an selection steps to handle outliers. We focus on sequential binary classifiers that consist of linear combinations of basis functions, whose coefficients evolve according to a Gaussian smoothness prior. Our results show significant improvements.	Univ Bristol, Stat Grp, Bristol BS8 1TW, Avon, England	University of Bristol	Andrieu, C (corresponding author), Univ Bristol, Stat Grp, Univ Walk, Bristol BS8 1TW, Avon, England.	C.Andrieu@bristol.ac.uk; jfgf@cs.berkeley.edu; doucet@ee.mu.oz.au		Doucet, Arnaud/0000-0002-7662-419X				ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; ANDRIEU C., 1999, SEQUENTIAL BAYESIAN; CRISAN D, 2000, 381 CUEDFINFENGTR; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Doucet A., 2001, SEQUENTIAL MONTE CAR; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; HOJENSORENSEN P, 2000, IEEE NEURAL NETWORKS; Holmes CC, 1998, NEURAL COMPUT, V10, P1217, DOI 10.1162/089976698300017421; Isard M., 1996, EUR C COMP VIS, P343; Kitagawa G., 1996, LECT NOTES STAT, V116; MCFADDEN D, 1989, ECONOMETRICA, V57, P995, DOI 10.2307/1913621; METROPOLIS N, 1949, J AM STAT ASSOC, V44, P335, DOI 10.2307/2280232; Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	14	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						561	567						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100070
C	Frey, BJ; Kristjansson, TT; Deng, L; Acero, A		Dietterich, TG; Becker, S; Ghahramani, Z		Frey, BJ; Kristjansson, TT; Deng, L; Acero, A			ALGONQUIN - Learning dynamic noise models from noisy speech for robust speech recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					A challenging, unsolved problem in the speech recognition community is recognizing speech signals that are corrupted by loud, highly nonstationary noise. One approach to noisy speech recognition is to automatically remove the noise from the cepstrum sequence before feeding it in to a clean speech recognizer. In previous work published in Eurospeech, we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of estimating the noise-free speech from the noisy speech. We showed how an iterative 2nd order vector Taylor series approximation could be used for probabilistic inference in this model. In many circumstances, it is not possible to obtain examples of noise without speech. Noise statistics may change significantly during an utterance, so that speech-free frames are not sufficient for estimating the noise model. In this paper, we show how the noise model can be learned even when the data contains speech. In particular, the noise model can be learned from the test utterance and then used to denoise the test utterance. The approximate inference technique is used as an approximate E step in a generalized EM algorithm that learns the parameters of the noise model from a test utterance. For both Wall Street Journal data with added noise samples and the Aurora benchmark, we show that the new noise adaptive technique performs as well as or significantly better than the non-adaptive algorithm, without the need for a separate training set of noise examples.	Univ Toronto, Probabil & Stat Inference Grp, Toronto, ON, Canada	University of Toronto	Frey, BJ (corresponding author), Univ Toronto, Probabil & Stat Inference Grp, Toronto, ON, Canada.							ATTIAS H, 2001, ADV NEURAL INFORMATI, V13; BOLL SF, 1979, IEEE T ACOUST SPEECH, V27, P113, DOI 10.1109/TASSP.1979.1163209; Deng L., 2000, 6 INT C SPOK LANG PR; FREY BJ, 2001, P EURO 2001; Gales MJF, 1996, IEEE T SPEECH AUDI P, V4, P352, DOI 10.1109/89.536929; JORDON MI, 1998, LEARNING GRAPHICAL M; MORENO PJ, 1996, SPEECH RECOGNITION N; [No title captured]	9	4	4	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1165	1171						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100145
C	Golland, P		Dietterich, TG; Becker, S; Ghahramani, Z		Golland, P			Discriminative direction for kernel classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In many scientific and engineering applications, detecting and understanding differences between two groups of examples can be reduced to a classical problem of training a classifier for labeling new examples while making as few mistakes as possible. In the traditional classification setting, the resulting classifier is rarely analyzed in terms of the properties of the input data captured by the discriminative model. However, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the statistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we define a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classifier function. We derive the discriminative direction for kernel-based classifiers, demonstrate the technique on several examples and briefly discuss its use in the statistical shape analysis, an application that originally motivated this work.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Golland, P (corresponding author), MIT, Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.							Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S, 1999, NEURAL NETWORKS, V12, P783, DOI 10.1016/S0893-6080(99)00032-5; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Burges CJC, 1999, ADVANCES IN KERNEL METHODS, P89; Golland P, 2000, LECT NOTES COMPUT SC, V1935, P72; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vapnik V.N, 1998, STAT LEARNING THEORY	9	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						745	752						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100093
C	Slonim, N; Friedman, N; Tishby, N		Dietterich, TG; Becker, S; Ghahramani, Z		Slonim, N; Friedman, N; Tishby, N			Agglomerative multivariate information bottleneck	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				CLASSIFICATION	The information bottleneck method is an unsupervised model independent data organization technique. Given a joint distribution P(A, B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. In a recent paper, we introduced a general principled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. We analyze the behavior of these algorithms and apply them to several real-life datasets.	Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Slonim, N (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.							BAKER LD, ACM SIGIR 98; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; ELYANIV R, NIPS 97; FRIEDMAN N, 2001, MULTIVARIATE INFORMA; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; LANG K, ICML 95; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; Slonim N, 2001, MON NOT R ASTRON SOC, V323, P270, DOI 10.1046/j.1365-8711.2001.04125.x; SLONIM N, NIPS 99; SLONIM N, ACM SIGIR 2000; Slonim Noam, 2001, ECIR; TISHBY N, NIPS 00; Tishby N., 1999, P 37 ALL C COMM COMP	15	4	4	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						929	936						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100116
C	Wainwright, MJ; Jaakkola, T; Willsky, AS		Dietterich, TG; Becker, S; Ghahramani, Z		Wainwright, MJ; Jaakkola, T; Willsky, AS			Tree-based reparameterization for approximate inference on loopy graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				PROBABILITY PROPAGATION; BELIEF PROPAGATION; MODELS; CORRECTNESS	We develop a tree-based reparameterization framework that provides a new conceptual view of a large class of iterative algorithms for computing approximate marginals in graphs with cycles. It includes belief propagation (BP), which can be reformulated as a very local form of reparameterization. More generally, we consider algorithms that perform exact computations over spanning trees of the full graph. On the practical side, we find that such tree reparameterization (TRP) algorithms have convergence properties superior to BP. The reparameterization perspective also provides a number of theoretical insights into approximate inference, including a new characterization of fixed points; and an invariance intrinsic to TRP/BP. These two properties enable us to analyze and bound the error between the TRP/BP approximations and the actual marginals. While our results arise naturally from the TRP perspective, most of them apply in an algorithm-independent manner to any local minimum of the Bethe free energy. Our results also have natural extensions to more structured approximations [e.g., 1, 2].	MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Wainwright, MJ (corresponding author), MIT, Dept Elect Engn & Comp Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.			Wainwright, Martin J./0000-0002-8760-2236				Anderson JB, 1998, IEEE J SEL AREA COMM, V16, P297, DOI 10.1109/49.661117; KIKUCHI R, 1951, PHYS REV, V81, P988, DOI 10.1103/PhysRev.81.988; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Minka T. P., 2001, THESIS MIT MEDIA LAB; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Richardson T, 2000, IEEE T INFORM THEORY, V46, P9, DOI 10.1109/18.817505; Rusmevichientong P, 2000, ADV NEUR IN, V12, P575; Wainwright M.J., 2002, THESIS MIT; WAINWRIGHT MJ, 2001, P2510 LIDS; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Weiss Y, 2000, ADV NEUR IN, V12, P673; WELLING M, 2001, UNCERTAINTY ARTIFICA; Yedidia JS, 2001, ADV NEUR IN, V13, P689; YUILLE A, 2001, IN PRESS NEURAL COMP	16	4	4	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1001	1008						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100125
C	Yamasaki, T; Shibata, T		Dietterich, TG; Becker, S; Ghahramani, Z		Yamasaki, T; Shibata, T			Analog soft-pattern-matching classifier using floating-gate MOS technology	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					A flexible pattern-matching analog classifier is presented in conjunction with a robust image representation algorithm called Principal Axes Projection (PAP). In the circuit, the functional form of matching is configurable in terms of the peak position, the peak height and the sharpness of the similarity evaluation. The test chip was fabricated in a 0.6-mum CMOS technology and successfully applied to hand-written pattern recognition and medical radiograph analysis using PAP as a feature extraction pre-processing step for robust image coding. The separation and classification of overlapping patterns is also experimentally demonstrated.	Univ Tokyo, Dept Elect Engn, Sch Engn, Bunkyo Ku, Tokyo 1138656, Japan	University of Tokyo	Yamasaki, T (corresponding author), Univ Tokyo, Dept Elect Engn, Sch Engn, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138656, Japan.	yamasaki@if.t.u-tokyo.ac.jp; shibata@ee.t.u-tokyo.ac.jp						Adachi M, 2001, IC-AI'2001: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS I-III, P229; ANDERSON J, 1993, ADV NEURAL INFORMATI, V5, P765; Cauwenberghs G., 1995, Advances in Neural Information Processing Systems 7, P779; ITO K, 2001, P 2001 INT C SOL STA, P94; Theogarajan L., 1996, 1996 IEEE International Symposium on Circuits and Systems. Circuits and Systems Connecting the World, ISCAS 96 (Cat. No.96CH35876), P543, DOI 10.1109/ISCAS.1996.541653; Theogarajan L, 1997, IEEE T CIRCUITS-II, V44, P977, DOI 10.1109/82.644055; TUTTLE GT, 1993, ISSCC, V36, P38; YAGI M, 2000, 10 EUR SIGN PROC C S, V2, P729; Yamasaki T., 2001, P 27 EUR SOL STAT CI; YAMASAKI T, 2001, P IEEE INT S CIRC SY, V3, P561	10	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1131	1138						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100141
C	Bogacz, R; Brown, MW; Giraud-Carrier, C		Leen, TK; Dietterich, TG; Tresp, V		Bogacz, R; Brown, MW; Giraud-Carrier, C			Emergence of movement sensitive neurons' properties by learning a sparse code for natural moving images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				VISUAL-CORTEX	Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of, 'simple cells' in, V1. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.	Univ Bristol, Dept Comp Sci, Bristol BS8 1UB, Avon, England	University of Bristol	Bogacz, R (corresponding author), Univ Bristol, Dept Comp Sci, Bristol BS8 1UB, Avon, England.	R.Bogacz@bristol.ac.uk; M.W.Brown@bristol.ac.uk; cgc@cs.bris.ac.uk		Giraud-Carrier, Christophe/0000-0002-4129-3239				ATICK JJ, 1993, NEURAL COMPUT, V5, P45, DOI 10.1162/neco.1993.5.1.45; BAKER CL, 1990, VISUAL NEUROSCI, V4, P101, DOI 10.1017/S0952523800002273; Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; BELL AJ, 1996, ADV NEURAL INFORMATI, V9; Blais B, 2000, NEURAL COMPUT, V12, P1057, DOI 10.1162/089976600300015501; Buracas GT, 1998, NEURON, V20, P959, DOI 10.1016/S0896-6273(00)80477-8; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; Hyvarinen A, 1999, NEURAL COMPUT, V11, P1739, DOI 10.1162/089976699300016214; LIVINGSTONE MS, 1987, J NEUROSCI, V7, P3416; Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; OLSHAUSEN BA, 2000, ICA 2000 P JUN 19 22, P603; RAO RPN, 1997, EFFICIENT ENCODING N; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577; [No title captured]	16	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						838	844						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800118
C	Crammer, K; Singer, Y		Leen, TK; Dietterich, TG; Tresp, V		Crammer, K; Singer, Y			Improved output coding for classification using continuous relaxation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				ERROR	Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems. Previous research on output coding has employed, almost solely, predefined discrete codes. We describe an algorithm that improves the performance of Output codes by relaxing them to continuous codes. The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines. We describe experiments with the proposed algorithm, comparing it to standard discrete output codes. The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes.	Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Crammer, K (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.							Aha DW, 1997, AI APPLICATIONS, V11, P13; ALLWEIN E, 2000, MACH LEARN P 17 INT; Berger A., 1999, IJCAI 99 WORKSH MACH; Cohen W.W., 1995, P 12 INT C MACH LEAR, P115, DOI DOI 10.1016/B978-1-55860-377-6.50023-2; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Crammer K., 2000, P 13 ANN C COMP LEAR; Dietterich, 1995, MACHINE LEARNING BIA; Dietterich G.B.T.G., 1999, DATA MINING SPEECH S; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; Hastie T, 1998, ANN STAT, V26, P451; James G, 1998, J COMPUT GRAPH STAT, V7, P377, DOI 10.2307/1390710; Olshen R., 1984, CLASSIFICATION REGRE; PLATT JC, 2000, IN PRESS ADV NEURAL, V12; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309; Schapire R. E., 1997, MACH LEARN P 14 INT, P313; SCHAPIRE RE, 1999, MACH LEARN, V37, P1; Vapnik V.N, 1998, STAT LEARNING THEORY	17	4	4	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						437	443						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800062
C	Dellaert, F; Seitz, SM; Thrun, S; Thorpe, C		Leen, TK; Dietterich, TG; Tresp, V		Dellaert, F; Seitz, SM; Thrun, S; Thorpe, C			Feature correspondence: A Markov chain Monte Carlo approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					When trying to recover 3D structure from a set of images, the most difficult problem is establishing the correspondence between the measurements. Most existing approaches assume that features can be tracked across frames, whereas methods that exploit rigidity constraints to facilitate matching do so only under restricted camera motion. In this paper we propose a Bayesian approach that avoids the brittleness associated with singling out one "best" correspondence, and instead consider the distribution over all possible correspondences. We treat both a fully Bayesian approach that yields a posterior distribution, and a MAP approach that makes use of EM to maximize this posterior. We show how Markov chain Monte Carlo methods can be used to implement these techniques in practice, and present experimental results on real data.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Dellaert, F (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	dellaert@cs.cmu.edu; seitz@cs.cmu.edu; thrun@cs.cmu.edu; cet@cs.cmu.edu		Dellaert, Frank/0000-0002-5532-3566				BEARDSLEY P, 1996, EUR C COMP VIS, P683; Castellanos JA, 2000, MOBILE ROBOT LOCALIZ; DELLAERT F, 2000, IEEE C COMP VIS PATT; Gilks WR, 1996, MARKOV CHAIN MONTE C; Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1; Hartley R. I., 1994, Applications of Invariance in Computer Vision. Second Joint European - US Workshop Proceedings, P237; LEONARD JJ, 1999, P 9 INT S ROB RES SA; PASULA H, 1999, INT JOINT C ART INT; SCOTT GL, 1991, P ROY SOC B-BIOL SCI, V244, P21, DOI 10.1098/rspb.1991.0045; SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3; Tanner M.A., 1996, TOOLS STAT INFERENCE; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torr P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P485, DOI 10.1109/ICCV.1998.710762	13	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						852	858						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800120
C	Grimes, DB; Mozer, MC		Leen, TK; Dietterich, TG; Tresp, V		Grimes, DB; Mozer, MC			The interplay of symbolic and subsymbolic processes in anagram problem solving	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Although connectionist models have provided insights into the nature of perception and motor control, connectionist accounts of higher cognition seldom go beyond an implementation of traditional symbol-processing theories. We describe a connectionist constraint satisfaction model of how people solve anagram problems. The model exploits statistics of English orthography, but also addresses the interplay of subsymbolic and symbolic computation by a mechanism that extracts approximate symbolic representations (partial orderings of letters) from subsymbolic structures and injects the extracted representation back into the model to assist in the solution of the anagram. We show the computational benefit of this extraction-injection process and discuss its relationship to conscious mental processes and working memory. We also account for experimental data concerning the difficulty of anagram solution based on the orthographic structure of the anagram string and the target word.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Grimes, DB (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.							Carroll JohnB., 1967, COMPUTATIONAL ANAL P; MAYZNER MS, 1958, J EXP PSYCHOL, V56, P376, DOI 10.1037/h0041542; MAYZNER MS, 1959, J EXP PSYCHOL, V63, P510; Neal RM, 1993, CRGTR931 U TOR DEP C; SARGENT SS, 1940, ARCH PSYCHOL, V249; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591	6	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						17	23						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800003
C	Hojen-Sorensen, PADR; Winther, O; Hansen, LK		Leen, TK; Dietterich, TG; Tresp, V		Hojen-Sorensen, PADR; Winther, O; Hansen, LK			Ensemble learning and linear response theory for ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We propose a general Bayesian framework for performing independent component analysis (ICA) which relies on ensemble learning and linear response theory known from statistical physics. We apply it to both discrete and continuous sources. For the continuous source the underdetermined (overcomplete) case is studied. The naive mean-field approach fails in this case whereas linear response theory-which gives an improved estimate of covariances-is very efficient. The examples given are for sources without temporal correlations. However, this derivation can easily be extended to treat temporal correlations. Finally, the framework offers a simple way of generating new ICA algorithms without needing to define the prior distribution of the sources explicitly.	Tech Univ Denmark, Dept Math Modelling, DK-2800 Lyngby, Denmark	Technical University of Denmark	Hojen-Sorensen, PADR (corresponding author), Tech Univ Denmark, Dept Math Modelling, DK-2800 Lyngby, Denmark.		Hansen, Lars/E-3174-2013	Hansen, Lars Kai/0000-0003-0442-5877; Winther, Ole/0000-0002-1966-3205				Belouchran A., 1995, P NOLTA, P49; CSATO L, 2000, ADV NEURAL INFORMATI, V12; HANSEN LK, 2000, IN PRESS ADV INDEPEN; HOJENSORENSEN PAD, UNPUB MEAN FIELD APP; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; LAPPALAINEN H, 2000, IN PRESS ADV INDEPEN; Lee T.-W., 1998, INDEPENDENT COMPONEN; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; MACKAY DJC, 1996, MAXIMUM LIKELIHOOD C; OPPER M, 2000, UNPUB PHYS REV LETT; Peterson C., 1987, Complex Systems, V1, P995; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; vanderVeen AJ, 1997, IEEE T SIGNAL PROCES, V45, P1078, DOI 10.1109/78.564198	14	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						542	548						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800077
C	Kakade, S; Dayan, P		Leen, TK; Dietterich, TG; Tresp, V		Kakade, S; Dayan, P			Dopamine bonuses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				NEURONS; REWARD; STIMULI; RESPONSES; SIGNAL	Substantial data support a temporal difference (TD) model of dopamine (DA) neuron activity in which the cells provide a global error signal for reinforcement learning. However, in certain circumstances, DA activity seems anomalous under the TD model, responding to non-rewarding stimuli. We address these anomalies by suggesting that DA cells multiplex information about reward bonuses, including Sutton's exploration bonuses and Ng et al's non-distorting shaping bonuses. We interpret this additional role for DA in terms of the unconditional attentional and psychomotor effects of dopamine, having the computational role of guiding exploration.	Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Kakade, S (corresponding author), Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							[Anonymous], 1992, SEMIN NEUROSCI; Cohen J. D., 1998, PREFRONTAL CORTEX EX; Dayan P, 1996, MACH LEARN, V25, P5, DOI 10.1023/A:1018357105171; Horvitz JC, 1997, BRAIN RES, V759, P251, DOI 10.1016/S0006-8993(97)00265-5; Ikemoto S, 1999, BRAIN RES REV, V31, P6, DOI 10.1016/S0165-0173(99)00023-5; Montague PR, 1996, J NEUROSCI, V16, P1936, DOI 10.1523/jneurosci.16-05-01936.1996; Ng A. Y., 1999, P 16 INT C MACH LEAR; Redgrave P, 1999, TRENDS NEUROSCI, V22, P146, DOI 10.1016/S0166-2236(98)01373-3; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; SCHULTZ W, 1990, J NEUROPHYSIOL, V63, P607, DOI 10.1152/jn.1990.63.3.607; Schultz W, 1998, J NEUROPHYSIOL, V80, P1, DOI 10.1152/jn.1998.80.1.1; SCHULTZ W, 1993, J NEUROSCI, V13, P900; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	15	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						131	137						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800019
C	Lee, TW; Wachtler, T; Sejnowski, T		Leen, TK; Dietterich, TG; Tresp, V		Lee, TW; Wachtler, T; Sejnowski, T			Color opponency constitutes a sparse representation for the chromatic structure of natural scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				INFORMATION; IMAGES; MECHANISMS; LUMINANCE; FILTERS; MACAQUE; CORTEX	The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) axe statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blue-yellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities.	Univ Calif San Diego, Inst Neural Computat, La Jolla, CA 92037 USA	University of California System; University of California San Diego	Lee, TW (corresponding author), Univ Calif San Diego, Inst Neural Computat, 10010 N Torrey Pines Rd, La Jolla, CA 92037 USA.		Sejnowski, Terrence/AAV-5558-2021; Wachtler, Thomas/K-3714-2014	Wachtler, Thomas/0000-0003-2015-6590				BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; BUCHSBAUM G, 1983, PROC R SOC SER B-BIO, V220, P89, DOI 10.1098/rspb.1983.0090; DERRINGTON AM, 1984, J PHYSIOL-LONDON, V357, P241, DOI 10.1113/jphysiol.1984.sp015499; FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559; LENNIE P, 1990, J NEUROSCI, V10, P649; LEWICKI M, 2000, UNPUB NEURAL COMPUTA; LEWICKI MS, 1999, IN PRESS J OPT SOC A; MACLEOD DIA, 1979, J OPT SOC AM, V69, P1183, DOI 10.1364/JOSA.69.001183; MACLEOD DIA, 1998, PLEISTOCHROME OPTIMA; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Parraga CA, 1998, J OPT SOC AM A, V15, P563, DOI 10.1364/JOSAA.15.000563; Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036; STOCKMAN A, 1993, J OPT SOC AM A, V10, P2491, DOI 10.1364/JOSAA.10.002491; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303	15	4	4	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						866	872						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800122
C	Malzahn, D; Opper, M		Leen, TK; Dietterich, TG; Tresp, V		Malzahn, D; Opper, M			Learning curves for Gaussian processes regression: A framework for good approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models.	Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Malzahn, D (corresponding author), Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.	malzahnd@aston.ac.uk; opperm@aston.ac.uk						Barber D, 1997, ADV NEUR IN, V9, P340; CSATO L, 2000, ADV NEURAL INFORMATI, V12; MacKay D.J.C., 1997, GAUSSIAN PROCESSES R; Williams CKI, 1996, ADV NEUR IN, V8, P514	6	4	4	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						273	279						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800039
C	Tchorz, J; Kleinschmidt, M; Kollmeier, B		Leen, TK; Dietterich, TG; Tresp, V		Tchorz, J; Kleinschmidt, M; Kollmeier, B			Noise suppression based on neurophysiologically-motivated SNR estimation for robust speech recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MODULATION; PERCEPTION; MODEL	A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speaker-independent digit recognition experiments and compared to noise suppression by Spectral Subtraction.	Univ Oldenburg, Med Phys Grp, D-26111 Oldenburg, Germany	Carl von Ossietzky Universitat Oldenburg	Tchorz, J (corresponding author), Univ Oldenburg, Med Phys Grp, D-26111 Oldenburg, Germany.							BOLL SF, 1979, IEEE T ACOUST SPEECH, V27, P113, DOI 10.1109/TASSP.1979.1163209; Dau T, 1996, J ACOUST SOC AM, V99, P3623, DOI 10.1121/1.414960; Dau T, 1997, J ACOUST SOC AM, V102, P2892, DOI 10.1121/1.420344; EPHRAIM Y, 1984, IEEE T ACOUST SPEECH, V32, P1109, DOI 10.1109/TASSP.1984.1164453; EWERT S, 1999, UNPUB J ACOUST SOC A; Hermansky H, 1994, IEEE T SPEECH AUDI P, V2, P578, DOI 10.1109/89.326616; KASPER K, 1995, NEURAL NETWORKS SIGN, V5, P272; KASPER K, 1997, P INT C ACOUSTICS SP, V2, P1223; KLEINSCHMIDT M, 2000, IN PRESS SPEECH COMM; KOLLMEIER B, 1994, J ACOUST SOC AM, V95, P1593, DOI 10.1121/1.408546; Langner G, 1997, J COMP PHYSIOL A, V181, P665, DOI 10.1007/s003590050148; Tchorz J, 1999, J ACOUST SOC AM, V106, P2040, DOI 10.1121/1.427950; TCHORZ J, 2000, UNPUB SPEECH COMMUNI	13	4	4	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						821	827						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800116
C	van Vreeswijk, C		Leen, TK; Dietterich, TG; Tresp, V		van Vreeswijk, C			Whence sparseness?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				PRIMARY VISUAL-CORTEX; NATURAL IMAGES; SIMPLE CELLS; FILTERS	It has been shown that the receptive fields of simple cells in V1 can be explained by assuming optimal encoding, provided that an extra constraint of sparseness is added. This finding suggests that there is a reason, independent of optimal representation, for sparseness. However this work used an ad hoc model for the noise. Here I show that if a biologically more plausible noise model, describing neurons as Poisson processes, is used sparseness does not have to be added as a constraint. Thus I conclude that sparseness is not a feature that evolution has striven for, but is simply the result of the evolutionary pressure towards an optimal representation.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	van Vreeswijk, C (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							BADDELEY R, 2000, INFORMATION THEORY B; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; COVER TM, 1991, INFORMATION THEORY; DEAN AF, 1981, EXP BRAIN RES, V44, P437; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; RICHMOND BJ, 1990, J NEUROPHYSIOL, V64, P351, DOI 10.1152/jn.1990.64.2.351; Rolls ET, 1996, J NEUROPHYSIOL, V75, P1982, DOI 10.1152/jn.1996.75.5.1982; SMITH WL, 1951, BIOMETRIKA, V46, P1; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303	10	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						180	186						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800026
C	Jin, CT; Corderoy, A; Carlile, S; van Schaik, A		Solla, SA; Leen, TK; Muller, KR		Jin, CT; Corderoy, A; Carlile, S; van Schaik, A			Spectral cues in human sound localization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The differential contribution of the monaural and interaural spectral cues to human sound localization was examined using a combined psychophysical and analytical approach. The cues to a sound's location were correlated on an individual basis with the human localization responses to a variety of spectrally manipulated sounds. The spectral cues derive from the acoustical filtering of an individual's auditory periphery which is characterized by the measured head-related transfer functions (HRTFs). Auditory localization performance was determined in virtual auditory space (VAS). Psychoacoustical experiments were conducted in which the amplitude spectra of the sound stimulus was varied independently at each ear while preserving the normal timing cues, an impossibility in the free-field environment. Virtual auditory noise stimuli were generated over earphones for a specified target direction such that there was a "false" Aat spectrum at the left eardrum. Using the subject's HRTFs, the sound spectrum at the right eardrum was then adjusted so that either the true right monaural spectral cue or the true interaural spectral cue was preserved. All subjects showed systematic mislocalizations in both the true right and true interaural spectral conditions which was absent in their control localization performance. The analysis of the different cues along with the subjects' localization responses suggests there are significant differences in the use of the monaural and interaural spectral cues and that the auditory system's reliance on the spectral cues varies with the sound condition.	Univ Sydney, Dept Physiol, Sydney, NSW 2006, Australia	University of Sydney	Jin, CT (corresponding author), Univ Sydney, Dept Physiol, Sydney, NSW 2006, Australia.			Jin, Craig/0000-0003-4636-753X; van Schaik, Andre/0000-0001-6140-017X				CARLILE S, 1996, VIRTUAL AUDITORY SPA; Duda R.O., 1997, BINAURAL SPATIAL HEA, P49; GLASBERG BR, 1990, HEARING RES, V47, P103, DOI 10.1016/0378-5955(90)90170-T; Janko J. A., 1997, BINAURAL SPATIAL HEA, P557; MIDDLEBROOKS JC, 1992, J ACOUST SOC AM, V92, P2607, DOI 10.1121/1.404400; WIGHTMAN FL, 1992, J ACOUST SOC AM, V91, P1648, DOI 10.1121/1.402445	6	4	4	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						768	774						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700109
C	Kakade, S; Dayan, P		Solla, SA; Leen, TK; Muller, KR		Kakade, S; Dayan, P			Acquisition in autoshaping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				PIGEONS	Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning. However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude. We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli play a crucial role.	Gatsy Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Kakade, S (corresponding author), Gatsy Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							BALSAM PD, 1988, J EXP PSYCHOL ANIM B, V14, P401; BALSAM PD, 1981, J EXP PSYCHOL ANIM B, V7, P382, DOI 10.1037/0097-7403.7.4.382; DAYAN P, 1997, NEURAL INFORMATION P, V10, P117; GALLISTEL CR, 1999, IN PRESS IS RAT IDEA; GALLISTEL CR, 1999, IN PRESS TIME RATE C; GAMZU ER, 1973, J EXP ANAL BEHAV, V19, P225, DOI 10.1901/jeab.1973.19-225; GIBBON J, 1980, ANIM LEARN BEHAV, V8, P45, DOI 10.3758/BF03209729; GIBBON J, 1977, PSYCHOL REV, V84, P279, DOI 10.1037/0033-295X.84.3.279; GIBBON J, 1977, J EXP PSYCHOL ANIM B, V3, P264, DOI 10.1037/0097-7403.3.3.264; Gibbon J., 1981, AUTOSHAPING CONDITIO, P219; JACOBS RA, 1991, COGNITIVE SCI, V15, P219, DOI 10.1207/s15516709cog1502_2; KAKADE S, 2000, UNPUB; Rescorla RA., 1972, CLASSICAL CONDITION, pp. 64, DOI DOI 10.1101/GR.110528.110; SUTTON RS, 1992, P 7 YAL WORKSH AD LE	14	4	4	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						24	30						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700004
C	Lee, SY; Mozer, MC		Solla, SA; Leen, TK; Muller, KR		Lee, SY; Mozer, MC			Robust recognition of noisy and superimposed patterns via selective attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MODEL	In many classification tasks, recognition accuracy is low because input patterns are corrupted by noise or are spatially or temporally overlapping. We propose an approach to overcoming these limitations based on a model of human selective attention. The model, an early selection filter guided by top-down attentional control, entertains each candidate output class in sequence and adjusts attentional gain coefficients in order to produce a strong response for that class. The chosen class is then the one that obtains the strongest response with the least modulation of attention. We present simulation results on classification of corrupted and superimposed handwritten digit patterns, showing a significant improvement in recognition rates. The algorithm has also been applied in the domain of speech recognition, with comparable results.	Korea Adv Inst Sci & Technol, Brain Sci Res Ctr, Yusong Gu, Taejon 305701, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Lee, SY (corresponding author), Korea Adv Inst Sci & Technol, Brain Sci Res Ctr, Yusong Gu, Taejon 305701, South Korea.							Broadbent Donald Eric, 2013, PERCEPTION COMMUNICA; Cowan N, 1995, ATTENTION MEMORY INT, DOI [10.1093/acprof:oso/9780195119107.001.0001, DOI 10.1093/ACPROF:OSO/9780195119107.001.0001]; FUKUSHIMA K, 1987, APPL OPTICS, V26, P4985, DOI 10.1364/AO.26.004985; Jeong DG, 1996, NEURAL NETWORKS, V9, P1213, DOI 10.1016/0893-6080(96)00042-1; KRUSCHKE JK, 1992, PSYCHOL REV, V99, P22, DOI 10.1037/0033-295X.99.1.22; LEE HJ, 1991, NEURAL COMPUT, V3, P135; LEE SY, 1988, APPL OPTICS, V27, P1921, DOI 10.1364/AO.27.001921; LEE SY, 1997, INT C NEUR INF PROC, P1051; Parasuraman, 1998, ATTENTIVE BRAIN; Pashler HE, 1998, PSYCHOL ATTENTION; RAO RPN, 1998, NEURAL INFORMATION P, V10; TREISMAN AM, 1960, Q J EXP PSYCHOL, V12, P242, DOI 10.1080/17470216008416732	12	4	4	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						31	37						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700005
C	Li, ZP		Solla, SA; Leen, TK; Muller, KR		Li, ZP			Can V1 mechanisms account for figure-ground and medial axis effects?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				PRIMARY VISUAL-CORTEX; INTEGRATION	When a visual image consists of a figure against a background, V1 cells are physiologically observed to give higher responses to image regions corresponding to the figure relative to their responses to the background. The medial axis of the figure also induces relatively higher responses compared to responses to other locations in the figure (except for the boundary between the figure and the background). Since the receptive fields of V1 cells ase very small compared with the global scale of the figure-ground and medial axis effects, it has been suggested that these effects may be caused by feedback from higher visual areas. I show how these effects can be accounted for by V1 mechanisms when the size of the figure is small or is of a certain scale. They are a manifestation of the processes of pre-attentive segmentation which detect and highlight the boundaries between homogeneous image regions.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1E 6BT, England	University of London; University College London	Li, ZP (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, London WC1E 6BT, England.	zhaoping@gatsby.ucl.ac.uk						BLUM H, 1973, J THEOR BIOL, V38, P205, DOI 10.1016/0022-5193(73)90175-6; Gallant Jack L., 1995, P89; GILBERT CD, 1992, NEURON, V9, P1, DOI 10.1016/0896-6273(92)90215-Y; GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698; KAPADIA MK, 1995, NEURON, V15, P843, DOI 10.1016/0896-6273(95)90175-2; KNIERIM JJ, 1992, J NEUROPHYSIOL, V67, P961, DOI 10.1152/jn.1992.67.4.961; LAMME VAF, 1995, J NEUROSCI, V15, P1605; LAMME VAF, 1997, SOC NEUR ABSTR; Lee TS, 1998, VISION RES, V38, P2429, DOI 10.1016/S0042-6989(97)00464-1; Li ZN, 1999, INT J PATTERN RECOGN, V13, P25, DOI 10.1142/S0218001499000033; Li ZP, 1999, NETWORK-COMP NEURAL, V10, P187, DOI 10.1088/0954-898X/10/2/305; Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557; ROCKLAND KS, 1983, J COMP NEUROL, V216, P303, DOI 10.1002/cne.902160307; White E.L., 1989, CORTICAL CIRCUITS, DOI [10.1007/978-1-4684-8721-3, DOI 10.1007/978-1-4684-8721-3]; Zipser K, 1996, J NEUROSCI, V16, P7376	15	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						136	142						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700020
C	Liu, XW; Wang, DL		Solla, SA; Leen, TK; Muller, KR		Liu, XW; Wang, DL			Perceptual organization based on temporal dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SURFACES	A figure-ground segregation network is proposed based on a novel boundary pair representation Nodes in the network are boundary segments obtained through, local grouping. Each node is excitatorily coupled with the neighboring nodes that belong to the same region, and inhibitorily coupled with the corresponding paired node. Gestalt grouping rules are incorporated by modulating connections. The status of a node represents its probability being figural and is updated according to a differential equation. The system solves the figure-ground segregation problem through temporal evolution. Different perceptual phenomena, such as modal and amodal completion, virtual contours, grouping and shape decomposition are then explained through local diffusion. The system eliminates combinatorial optimization and accounts for many psychophysical results with a fixed set of parameters.	Ohio State Univ, Ctr Cognit Sci, Dept Comp & Informat Sci, Columbus, OH 43210 USA	Ohio State University	Liu, XW (corresponding author), Ohio State Univ, Ctr Cognit Sci, Dept Comp & Informat Sci, Columbus, OH 43210 USA.							[Anonymous], 1985, PERCEPTUAL ORG VISUA; Bregman A. S., 1981, PERCEPTUAL ORG, P99; FAHLE M, 1991, BIOL CYBERN, V66, P1, DOI 10.1007/BF00196447; Geiger D, 1998, PROC CVPR IEEE, P118, DOI 10.1109/CVPR.1998.698597; GROSSBERG S, 1985, PERCEPT PSYCHOPHYS, V38, P141, DOI 10.3758/BF03198851; Kanizsa G., 1979, ORG VISION; NAKAYAMA K, 1995, INVITATION COGNITIVE, V2, P1; NITZBERG M, 1993, FILTERING SEMENTATIO; SHAPLEY R, 1987, PERCEPTION ILLUSORY, P109; Williams LR, 1996, COMPUT VIS IMAGE UND, V64, P1, DOI 10.1006/cviu.1996.0043	10	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						38	44						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700006
C	Ng, AY; Parr, R; Koller, D		Solla, SA; Leen, TK; Muller, KR		Ng, AY; Parr, R; Koller, D			Policy search via density estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We propose a new approach to the problem of searching a space of stochastic controllers for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP). Following several other authors, our approach is based on searching in parameterized families of policies (for example, via gradient descent) to optimize solution quality. However, rather than trying to estimate the values and derivatives of a policy directly, we do so indirectly using estimates for the probability densities that the policy induces on states at the different points in time. This enables our algorithms to exploit the many techniques for efficient and robust approximate density propagation in stochastic systems. We show how our techniques can be applied both to deterministic propagation schemes (where the MDP's dynamics are given explicitly in compact form,) and to stochastic propagation schemes (where we have access only to a generative model, or simulator, of the MDP). We present empirical results far both of these variants on complex problems.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ng, AY (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.							BAIRD L, 1999, NIPS, V11; BOUTILIER C, 1999, J ARTIFICIAL INTELLI; Boyen X, 1998, P 14 ANN C UNC ART I, P33; FORBES J, 1995, P IJCAI; Koller D., 1998, P 15 INT C MACH LEAR, P287; MEULEAU N, 1999, P UAI, V15; Randlov Jette, 1998, P ICML; WILLIAMS JK, 1999, NIPS, V11; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	9	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1022	1028						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700144
C	Patel, GN; Cymbalyuk, GS; Calabrese, RL; DeWeerth, SP		Solla, SA; Leen, TK; Muller, KR		Patel, GN; Cymbalyuk, GS; Calabrese, RL; DeWeerth, SP			Bifurcation analysis of a silicon neuron	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We have developed a VLSI silicon neuron and a corresponding mathematical model that is a two state-variable system. We describe the circuit implementation and compare the behaviors observed in the silicon neuron and the mathematical model. We also perform bifurcation analysis of the mathematical model by varying the externally applied current and show that the behaviors exhibited by the silicon neuron under corresponding conditions are in good agreement to those predicted by the bifurcation analysis.	Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Patel, GN (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.	girish.patel@ece.gatech.edu; gcym@biology.emory.edu; rcalabre@biology.emory.edu; steve.deweerth@ece.gatech.edu		Calabrese, Ronald L./0000-0001-7135-3469				DeWeerth SP, 1997, SEVENTEENTH CONFERENCE ON ADVANCED RESEARCH IN VLSI, PROCEEDINGS, P182, DOI 10.1109/ARVLSI.1997.634854; Guckenheimer J., 1983, J APPL MECH, V42, DOI DOI 10.1115/1.3167759; KHIBNIK AI, 1993, PHYSICA D, V62, P360, DOI 10.1016/0167-2789(93)90294-B; KOPELL N, 1988, MATH BIOSCI, V90, P87, DOI 10.1016/0025-5564(88)90059-4; Marder E, 1996, PHYSIOL REV, V76, P687, DOI 10.1152/physrev.1996.76.3.687; Mead, 1989, ANALOG VLSI NEURAL S; MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0; PATEL G, 1999, THESIS GEORGIA I TEC; Rinzel J., 1989, METHODS NEURONAL MOD, P251; SIMONI MF, 1997, 6 ANN COMP NEUR M BI; SKINNER FK, 1993, BIOL CYBERN, V69, P375, DOI 10.1007/BF01185409; SKINNER FK, 1994, COMPUTATION IN NEURONS AND NEURAL SYSTEMS, P223; VANDERPOL B, 1939, SELECTED SCI PAPERS, V2	13	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						731	737						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700104
C	Rodriguez, A; Parr, R; Koller, D		Solla, SA; Leen, TK; Muller, KR		Rodriguez, A; Parr, R; Koller, D			Reinforcement learning using approximate belief states	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The problem of developing good policies for partially observable Markov decision problems (POMDPs) remains one of the most challenging areas of research in stochastic planning. One line of research in this area involves the use of reinforcement learning with belief states, probability distributions over the underlying model states. This is a promising method for small problems, but its application is limited by the intractability of computing or representing a full belief state for large problems. Recent work shows that, in many settings, we can maintain an approximate belief state, which is fairly close to the true belief state. In particular, great success has been shown with approximate belief states that marginalize out correlations between state variables. In this paper, we investigate two methods of full belief state reinforcement learning and one novel method for reinforcement learning using factored approximate belief states. We compare the performance of these algorithms on several well-known problem from the literature. Our results demonstrate the importance of approximate belief state representations for large problems.	SRI Int, Ctr Artificial Intelligence, Menlo Park, CA 94025 USA	SRI International	Rodriguez, A (corresponding author), SRI Int, Ctr Artificial Intelligence, 333 Ravenswood Ave, Menlo Park, CA 94025 USA.							ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X; Bellman RE, 1957, DYNAMIC PROGRAMMING; BOUTILIER C, 1999, J ARTIFICIAL INTELLI; BOYEN X, 1998, P UAI; Cassandra AR, 1998, EXACT APPROXIMATE AL; Littman M, 1996, THESIS BROWN U, Patent No. AAI9709069; LITTMAN M, 1996, P ICML, P362; LOCH J, 1998, P ICML; MCCALLUM AR, 1993, P ICML, P190; PARR R, 1995, P IJCAI; SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071; WIERING M, 1996, HQ LEARNING DISCOVER	12	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1036	1042						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700146
C	Siegelmann, HT; Roitershtein, A; Ben-Hur, A		Solla, SA; Leen, TK; Muller, KR		Siegelmann, HT; Roitershtein, A; Ben-Hur, A			Noisy neural networks and generalizations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In this paper we define a probabilistic computational model which generalizes many noisy neural network models, including the recent work of Maass and Sontag [5]. We identify weak ergodicity as the mechanism responsible for restriction of the computational power of probabilistic models to definite languages, independent of the characteristics of the noise: whether it is discrete or analog, or if it depends on the input or not, and independent of whether the variables are discrete or continuous. We give examples of weakly ergodic models including noisy computational systems with noise depending on the current state and inputs, aggregate models, and computational systems which update in continuous time.	Technion Israel Inst Technol, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Siegelmann, HT (corresponding author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.	iehava@ie.technion.ac.il; roiterst@math.technion.ac.il; asa@tx.technion.ac.il		Roitershtein, Alexander/0000-0001-8207-4289				Casey M, 1996, NEURAL COMPUT, V8, P1135, DOI 10.1162/neco.1996.8.6.1135; Dobrushin R.L., 1956, THEORY PROBAB APPL, P65, DOI 10.1137/1101006; DOBRUSHIN RL, 1956, THEOR PROBAB APPL, V1, P298; Maass W, 1999, NEURAL COMPUT, V11, P771, DOI 10.1162/089976699300016656; Maass W, 1998, NEURAL COMPUT, V10, P1071, DOI 10.1162/089976698300017359; Neveu J., 1964, MATH FDN CALCULUS PR; PAZ A, 1970, ANN MATH STAT, V41, P539, DOI 10.1214/aoms/1177697094; Paz A., 1971, INTRO PROBABILISTIC; RABIN MO, 1963, INFORM CONTROL, V6, P230, DOI 10.1016/S0019-9958(63)90290-0; Siegelmann H.T., 1999, NEURAL NETWORKS ANAL; SIEGELMANN HT, 1999, UNPUB WEAKLY ERGODIC; SIEGELMANN HT, 1999, IN PRESS DISCRETE AP	13	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						335	341						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700048
C	Singer, Y		Solla, SA; Leen, TK; Muller, KR		Singer, Y			Leveraged vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We describe an iterative algorithm for building vector machines used in classification tasks. The algorithm builds on ideas from support vector machines, boosting, and generalized additive models. The algorithm can be used with various continuously differential functions that bound the discrete (0-1) classification loss and is very simple to implement. We test the proposed algorithm with two different loss functions on synthetic and natural data. We also describe a norm penalized version of the algorithm for the exponential loss function used in AdaBaost. The performance of the algorithm on natural data is comparable to support vector machines while typically its running time is shorter than of SVM.	Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel	Hebrew University of Jerusalem	Singer, Y (corresponding author), Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel.							CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; DUFFY N, EUROCOLT 99; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J., 1998, ADDITIVE LOGISTIC RE; KEARNS M, 1994, J ACM, V41, P67, DOI 10.1145/174644.174647; LAFFERTY J, 1999, P 12 ANN C COMP LEAR; MASON L, 1999, DOOM 2; RATSCH G, 1998, ADV NEURAL INFO PROC, V12; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; SCHAPIRE RE, COLT 98; Vapnik V., 1982, ESTIMATION DEPENDENC; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	13	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						610	616						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700087
C	Zemel, RS; Mozer, MC		Solla, SA; Leen, TK; Muller, KR		Zemel, RS; Mozer, MC			A generative model for attractor dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				NETWORKS	Attractor networks, which map an input space to a discrete output space, are useful for pattern completion. However, designing a net to have a given set of attractors is notoriously tricky; training procedures are CPU intensive and often produce spurious attractors and ill-conditioned attractor basins. These difficulties occur because each connection in the network participates in the encoding of multiple attractors. We describe an alternative formulation of attractor networks in which the encoding of knowledge is local, not distributed. Although localist attractor networks have similar dynamics to their distributed counterparts, they are much easier to work with and interpret. We propose a statistical formulation of localist attractor net dynamics, which yields a convergence proof and a mathematical interpretation of model parameters.	Univ Arizona, Dept Psychol, Tucson, AZ 85721 USA	University of Arizona	Zemel, RS (corresponding author), Univ Arizona, Dept Psychol, Tucson, AZ 85721 USA.							Becker S, 1997, J EXP PSYCHOL LEARN, V23, P1059, DOI 10.1037/0278-7393.23.5.1059; GOLDEN R, 1988, NEURAL INFORMATION P, P310; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Kay LM, 1996, INT J NEURAL SYST, V7, P489, DOI 10.1142/S0129065796000476; MATHIS D, 1997, THESIS U COLORADO BO; Mathis DW, 1996, PROCEEDINGS OF THE EIGHTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P324; MCCLELLAND JL, 1981, PSYCHOL REV, V88, P375, DOI 10.1037/0033-295X.88.5.375; McRae K, 1997, J EXP PSYCHOL GEN, V126, P99, DOI 10.1037/0096-3445.126.2.99; Neal R. M., 1998, LEARNING GRAPHICAL M; Redish AD, 1998, NEURAL COMPUT, V10, P73, DOI 10.1162/089976698300017908; RODRIGUES NC, 1997, J PHYSICS A, V30, P7945; Samsonovich A, 1997, J NEUROSCI, V17, P5900; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251	13	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						80	86						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700012
C	Chen, K; Wang, DLL		Kearns, MS; Solla, SA; Cohn, DA		Chen, K; Wang, DLL			Perceiving without learning: from spirals to inside/outside relations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				OSCILLATORS; NETWORK	As a benchmark task, the spiral problem is well known in neural networks. Unlike previous work that emphasizes learning, we approach the problem from a generic perspective that does not involve learning. We point out that the spiral problem is intrinsically connected to the inside/outside problem. A generic solution to both problems is proposed based on oscillatory correlation using a time delay network. Our simulation results are qualitatively consistent with human performance, and we interpret human limitations in terms of synchrony and time delays, both biologically plausible. As a special case, our network without time delays can always distinguish these figures regardless of shape, position, size, and orientation.	Ohio State Univ, Dept Comp & Informat Sci, Columbus, OH 43210 USA	Ohio State University	Chen, K (corresponding author), Ohio State Univ, Dept Comp & Informat Sci, Columbus, OH 43210 USA.		, Ke/ABG-5874-2020; Chen, Ke/C-2560-2008	Chen, Ke/0000-0001-9457-9364				Campbell SR, 1998, PHYSICA D, V111, P151, DOI 10.1016/S0167-2789(97)80010-3; CHEN K, 1997, OSUCISRC897TR38; GROSSBERG S, 1991, NEURAL NETWORKS, V4, P723, DOI 10.1016/0893-6080(91)90053-8; Julesz B., 1995, DIALOGUES PERCEPTION; Lang KJ., 1988, P 1988 CONN MOD SUMM, P52; MILNER P, 1974, PSYCHOL REV, V81, P512; Minsky M., 1988, PERCEPTRONS; Minsky M., 1969, PERCEPTRONS; SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011; TERMAN D, 1995, PHYSICA D, V81, P148, DOI 10.1016/0167-2789(94)00205-5; ULLMAN S, 1984, COGNITION, V18, P97, DOI 10.1016/0010-0277(84)90023-4; Ullman S., 1996, HIGH LEVEL VISION OB; von der Malsburg C, 1981, 812 M PLANCK I BIOPH; Wang DL, 1997, NEURAL COMPUT, V9, P805, DOI 10.1162/neco.1997.9.4.805	14	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						10	16						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700002
C	Gdalyahu, Y; Weinshall, D; Werman, M		Kearns, MS; Solla, SA; Cohn, DA		Gdalyahu, Y; Weinshall, D; Werman, M			A randomized algorithm for pairwise clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				IMAGE SEGMENTATION	We present a stochastic clustering algorithm based on pairwise similarity of datapoints. Our method extends existing deterministic methods, including agglomerative algorithms, min-cut graph algorithms, and connected components. Thus it provides a common framework for all these methods. Our graph-based method differs from existing stochastic methods which are based on analogy to physical systems. The stochastic nature of our method makes it more robust against noise, including; accidental edges and small spurious clusters. We demonstrate the superiority of our algorithm using an example with 3 spiraling bands and a lot of noise.	Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Gdalyahu, Y (corresponding author), Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel.							Blatt M, 1997, NEURAL COMPUT, V9, P1805, DOI 10.1162/neco.1997.9.8.1805; Hart P. E, 1973, PATTERN CLASSIFICATI; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; KARGER DR, 1996, J ACM, V43; KLOCK H, 1996, IAITR968; LINIAL N, 1995, COMBINATORICA, V15, P215, DOI 10.1007/BF01200757; ROSE K, 1993, IEEE T PATTERN ANAL, V15, P785, DOI 10.1109/34.236251; Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407; SMITH SP, 1993, IEEE T PATTERN ANAL, V15, P89, DOI 10.1109/34.184777; WU Z, 1993, IEEE T PATTERN ANAL, V15, P1101, DOI 10.1109/34.244673	11	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						424	430						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700060
C	Leisch, F; Trapletti, A; Hornik, K		Kearns, MS; Solla, SA; Cohn, DA		Leisch, F; Trapletti, A; Hornik, K			Stationarity and stability of autoregressive neural network processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We analyze the asymptotic behavior of autoregressive neural network (AR-NN) processes using techniques from Markov chains and non-linear time series analysis. It is shown that standard AR-NNs without shortcut connections are asymptotically stationary. If linear shortcut connections are allowed, only the shortcut weights determine whether the overall system is stationary, hence standard conditions for linear AR processes can be used.	Vienna Tech Univ, Inst Stat, A-1040 Vienna, Austria	Technische Universitat Wien	Leisch, F (corresponding author), Vienna Tech Univ, Inst Stat, Wiedner Hauptstr 8-10,1071, A-1040 Vienna, Austria.		Leisch, Friedrich/A-6977-2013	Leisch, Friedrich/0000-0001-7278-1983				Brockwell P., 1991, TIME SERIES THEORY M; CHAN KS, 1985, ADV APPL PROBAB, V17, P666, DOI 10.2307/1427125; Cheng H, 1990, SSSA BOOK S, V2, P1; Husmeier D, 1997, NEURAL NETWORKS, V10, P479, DOI 10.1016/S0893-6080(96)00062-7; JONES DA, 1978, P ROY SOC LOND A MAT, V360, P71, DOI 10.1098/rspa.1978.0058; LELAND WE, 1994, IEEE ACM T NETWORK, V2, P1, DOI 10.1109/90.282603; MANDELBROT BB, 1968, SIAM REV, V10, P422, DOI 10.1137/1010093; Tong H, 1990, OXFORD STAT SCI SERI, V6; Wang T, 1996, NEURAL NETWORKS, V9, P957, DOI 10.1016/0893-6080(95)00131-X	9	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						267	273						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700038
C	Mason, L; Bartlett, P; Baxter, J		Kearns, MS; Solla, SA; Cohn, DA		Mason, L; Bartlett, P; Baxter, J			Direct optimization of margins improves generalization in combined classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Cumulative training margin distributions for AdaBoost versus our "Direct Optimization Of Margins" (DOOM) algorithm. The dark curve is AdaBoost, the light curve is DOOM. DOOM sacrifices significant training error for improved test error (horizontal marks on margin = 0 line).	Australian Natl Univ, Dept Syst Engn, Canberra, ACT 0200, Australia	Australian National University	Mason, L (corresponding author), Australian Natl Univ, Dept Syst Engn, Canberra, ACT 0200, Australia.	lmason@syseng.anu.edu.au; bartlett@syseng.anu.edu.au; jon@syseng.anu.edu.au		Bartlett, Peter/0000-0002-8760-3140				Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; BREIMAN L, 1997, 504 U CAL DEP STAT; Grove AJ, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P692; KEOGH E, 1998, UCI REPOITORY MACHIN; MASON L, 1998, IMPROVED GEN EXPLICI; SCHAPIRE RE, 1998, IN PRESS ANN STAT	6	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						288	294						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700041
C	Pelillo, M		Kearns, MS; Solla, SA; Cohn, DA		Pelillo, M			Replicator equations, maximal cliques, and graph isomorphism	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORK	Wie present a new energy-minimization framework for the graph isomorphism problem which is based on an equivalent maximum clique formulation. The approach is centered around a fundamental result proved by Motzkin and Straus ill the mid-1990s, and recently expanded ill various ways, which allows us to formulate the maximum clique problem in terms of a standard quadratic program. To solve the program we use "replicator'' equations, a class of simple continuous- and discrete-time dynamical systems developed ill various branches of theoretical biology. We show how, despite their inability to escape from local solutions, they nevertheless provide experimental results which are competitive with those obtained using more elaborate mean-field annealing heuristics.	Univ Ca Foscari Venezia, Dipartimento Informat, I-30172 Venice, Italy	Universita Ca Foscari Venezia	Pelillo, M (corresponding author), Univ Ca Foscari Venezia, Dipartimento Informat, Via Torino 155, I-30172 Venice, Italy.							Barrow H. G., 1976, Information Processing Letters, V4, P83, DOI 10.1016/0020-0190(76)90049-1; BAUM LE, 1967, B AM MATH SOC, V73, P360, DOI 10.1090/S0002-9904-1967-11751-8; Bomze IM, 1997, J GLOBAL OPTIM, V10, P143, DOI 10.1023/A:1008230200610; BOPPANA RB, 1987, INFORM PROCESS LETT, V25, P127, DOI 10.1016/0020-0190(87)90232-8; Garey M.R., 1979, COMPUTERS INTRACTABI; Gibbons LE, 1997, MATH OPER RES, V22, P754, DOI 10.1287/moor.22.3.754; Hofbauer J., 1995, IMITATION DYNAMICS G; Hofbauer J., 1988, THEORY EVOLUTION DYN; HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141; KREE R, 1988, J PHYS A-MATH GEN, V21, pL813, DOI 10.1088/0305-4470/21/16/006; LOSERT V, 1983, J MATH BIOL, V17, P241, DOI 10.1007/BF00305762; MOTZKIN TS, 1965, CANADIAN J MATH, V17, P533, DOI 10.4153/CJM-1965-053-6; Pelillo M., 1995, Journal of Artificial Neural Networks, V2, P313; PELILLO M, IN PRESS NEURAL COMP; PELILLO M, 1998, COMPUTER VISION ECCV, V2, P3; PELILLO M, 1995, J ARTIF NEURONETW, V0002, P00411; Rangarajan A, 1996, NEURAL COMPUT, V8, P1041, DOI 10.1162/neco.1996.8.5.1041; Rangarajan A, 1996, IEEE T NEURAL NETWOR, V7, P1365, DOI 10.1109/72.548165; Simic PD, 1991, NEURAL COMPUT, V3, P268, DOI 10.1162/neco.1991.3.2.268; Weibull J.W., 1997, EVOLUTIONARY GAME TH; [No title captured]	21	4	4	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						550	556						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700078
C	Suematsu, N; Hayashi, A		Kearns, MS; Solla, SA; Cohn, DA		Suematsu, N; Hayashi, A			A reinforcement learning algorithm in partially observable environments using short-term memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				COMPLEXITY	We describe a Reinforcement Learning algorithm for partially observable environments using short-term memory, which we call BLHT. Since BLHT learns a stochastic model based on Bayesian Learning, the over-fitting problem is reasonably solved. Moreover, BLHT has an efficient implementation. This paper shows that the model learned by BLHT converges to one which provides the most accurate predictions of percepts and rewards, given short-term memory.	Hiroshima City Univ, Fac Comp Sci, Asaminami Ku, Hiroshima 7313194, Japan		Suematsu, N (corresponding author), Hiroshima City Univ, Fac Comp Sci, Asaminami Ku, 3-4-1 Ozuka Higashi, Hiroshima 7313194, Japan.	suematsu@im.hiroshima-cu.ac.jp; akira@im.hiroshima-cu.ac.jp						ABE N, 1992, MACH LEARN, V9, P205, DOI 10.1007/BF00992677; BERTSEKAS DP, 1987, DYANAMIC PROGRAMMING; CHRISMAN L, 1992, P 10 NAT C ART INT; Jaakkola T., 1995, Advances in Neural Information Processing Systems 7, P345; MCCALLUM RA, 1993, P 10 INT C MACH LEAR; PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441; Ron D., 1994, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, COLT 94, P35, DOI 10.1145/180139.181006; SINGH SP, 1995, P 11 MACH LEARN C, P284; SUEMATSU N, 1997, P 14 INT C MACH LEAR, P349	9	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1059	1065						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700149
C	Zhang, LQ; Cichocki, A		Kearns, MS; Solla, SA; Cohn, DA		Zhang, LQ; Cichocki, A			Blind separation of filtered sources using state-space approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In this paper we present a novel approach to multichannel blind separation/generalized deconvolution, assuming that both mixing and demixing models are described by stable linear state-space systems. We decompose the blind separation problem into two process: separation and state estimation. Based on the minimization of Kullback-Leibler Divergence, we develop a novel learning algorithm to train the matrices in the output equation. To estimate the state of the demixing model, we introduce a new concept, called hidden innovation, to numerically implement the Kalman filter. Computer simulations are given to show the validity and high effectiveness of the state-space approach.	RIKEN, Lab Open Informat Sci, Brain Sci Inst, Wako, Saitama 3510198, Japan	RIKEN	Zhang, LQ (corresponding author), RIKEN, Lab Open Informat Sci, Brain Sci Inst, Wako, Saitama 3510198, Japan.	zha@open.brain.riken.go.jp; cia@open.brain.riken.go.jp	Cichocki, Andrzej/AAI-4209-2020; Cichocki, Andrzej/A-1545-2015	Cichocki, Andrzej/0000-0002-8364-7226				Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S., 1996, ADV NEURAL INFORMATI, P752; Amari SI, 1998, P IEEE, V86, P2026, DOI 10.1109/5.720251; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250; Cardoso JF, 1996, IEEE T SIGNAL PROCES, V44, P3017, DOI 10.1109/78.553476; Choi S, 1998, NEURAL NETWORKS FOR SIGNAL PROCESSING VIII, P93, DOI 10.1109/NNSP.1998.710638; Cichocki A, 1996, IEEE T CIRCUITS-I, V43, P894, DOI 10.1109/81.542280; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Gharbi AB, 1997, ISCAS '97 - PROCEEDINGS OF 1997 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS I - IV, P713, DOI 10.1109/ISCAS.1997.608970; Jacobs O.L.R., 1993, INTRO CONTROL THEORY, V2nd; Lee HH, 1997, CLIN DIAGN VIROL, V8, P9, DOI 10.1016/S0928-0197(97)00272-9; ZHANG L, 1998, P IEEE WORKSH NNSP 9, P123	13	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						648	654						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700092
C	Baxter, J; Bartlett, P		Jordan, MI; Kearns, MJ; Solla, SA		Baxter, J; Bartlett, P			The Canonical Distortion Measure in feature space and 1-NN classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We prove that the Canonical Distortion Measure (CDM) [2, 3] is the optimal distance measure to use for 1 nearest-neighbour (1-NN) classification, and show that it reduces to squared Euclidean distance in feature space for function classes that can be expressed as linear combinations of a fixed set of features. PAC-like bounds are given on the sample-complexity required to learn the CDM. An experiment is presented in which a neural network CDM was learnt for a Japanese OCR environment and then used to do 1-NN classification.	Australian Natl Univ, Dept Syst Engn, Canberra, ACT 0200, Australia	Australian National University	Baxter, J (corresponding author), Australian Natl Univ, Dept Syst Engn, Canberra, ACT 0200, Australia.			Bartlett, Peter/0000-0002-8760-3140					0	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						245	251						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700035
C	Burger, M; Graepel, T; Obermayer, K		Jordan, MI; Kearns, MJ; Solla, SA		Burger, M; Graepel, T; Obermayer, K			An annealed self-organizing map for source channel coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	Advances in Neural Information Processing Systems		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We derive and analyse robust optimization schemes for noisy vector quantization on the basis of deterministic annealing. Starting from a cost function for central clustering that incorporates distortions from channel noise we develop a soft topographic vector quantization algorithm (STVQ) which is based on the maximum entropy principle and which performs a maximum-likelihood estimate in an expectation-maximization (EM) fashion. Annealing in the temperature parameter beta leads to phase transitions in the existing code vector representation during the cooling process for which we calculate critical temperatures and modes as a function of eigenvectors and eigenvalues of the covariance matrix of the data and the transition matrix of the channel noise. A whole family of vector quantization algorithms is derived from STVQ, among them a deterministic annealing scheme for Kohonen's self-organizing map (SOM). This algorithm, which we call SSOM, is then applied to vector quantization of image data to be sent via a noisy binary symmetric channel. The algorithm's performance is compared to those of LEG and STVQ. While it is naturally superior to LEG, which does not take into account channel noise, its results compare very well to those of STVQ, which is computationally much more demanding.	Tech Univ Berlin, Dept Comp Sci, D-10587 Berlin, Germany	Technical University of Berlin	Burger, M (corresponding author), Tech Univ Berlin, Dept Comp Sci, FR 2-1,Franklinstr 28-29, D-10587 Berlin, Germany.	burger@cs.tu-berlin.de; graepel2@cs.tu-berlin.de; oby@cs.tu-berlin.de							0	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						430	436						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700061
C	Goodhill, GJ		Jordan, MI; Kearns, MJ; Solla, SA		Goodhill, GJ			Gradients for retinotectal mapping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The initial activity-independent formation of a topographic map in the retinotectal system has long been thought to rely on the matching of molecular cues expressed in gradients in the retina and the tectum. However, direct experimental evidence for the existence of such gradients has only emerged since 1995. The new data has provoked the discussion of a new set of models in the experimental Literature. Here, the capabilities of these models are analyzed, and the gradient shapes they predict in vivo are derived.	Georgetown Univ, Med Ctr, Georgetown Inst Cognit & Computat Sci, Washington, DC 20007 USA	Georgetown University	Goodhill, GJ (corresponding author), Georgetown Univ, Med Ctr, Georgetown Inst Cognit & Computat Sci, 3970 Reservoir Rd, Washington, DC 20007 USA.			Goodhill, Geoffrey/0000-0001-9789-9355					0	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						152	158						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700022
C	Held, M; Buhmann, JM		Jordan, MI; Kearns, MJ; Solla, SA		Held, M; Buhmann, JM			Unsupervised on-line learning of decision trees for hierarchical data analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					An adaptive on-line algorithm is proposed to estimate hierarchical data structures for non-stationary data sources. The approach is based on the principle of minimum cross entropy to derive a decision tree for data clustering and it employs a metalearning idea (learning to learn) to adapt to changes in data characteristics. Its efficiency is demonstrated by grouping non-stationary artifical data and by hierarchical segmentation of LANDSAT images.	Rheinische Friedrich Wilhelms Universitat Bonn, Inst Informatik 3, D-53117 Bonn, Germany	University of Bonn	Held, M (corresponding author), Rheinische Friedrich Wilhelms Universitat Bonn, Inst Informatik 3, Romerstr 164, D-53117 Bonn, Germany.		Buhmann, Joachim/AAU-4760-2020						0	4	4	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						514	520						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700073
C	Kohlmorgen, J; Muller, KR; Pawelzik, K		Jordan, MI; Kearns, MJ; Solla, SA		Kohlmorgen, J; Muller, KR; Pawelzik, K			Analysis of drifting dynamics with neural network hidden Markov models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a method for the analysis of nonstationary time series with multiple operating modes. In particular, it is possible to detect and to model both a switching of the dynamics and a less abrupt, time consuming drift from one mode to another. This is achieved in two steps. First, an unsupervised training method provides prediction experts for the inherent dynamical modes. Then, the trained experts are used in a hidden Markov model that allows to model drifts. An application to physiological wake/sleep data demonstrates that analysis and modeling of real-world time series can be improved when the drift paradigm is taken into account.	GMD FIRST, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Kohlmorgen, J (corresponding author), GMD FIRST, Rudower Chaussee 5, D-12489 Berlin, Germany.		Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685					0	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						735	741						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700104
C	Lewis, MA		Jordan, MI; Kearns, MJ; Solla, SA		Lewis, MA			Visual navigation in a robot using zig-zag behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We implement a model of obstacle avoidance in flying insects on a small, monocular robot. The result is a system that is capable of rapid navigation through a dense obstacle field. The key to the system is the use of zigzag behavior to articulate the body during movement. It is shown that this behavior compensates for a parallax blind spot surrounding the focus of expansion normally found in systems without parallax behavior. The system models the cooperation of several behaviors: halteres-ocular response (similar to VOR), optomotor response, and the parallax field computation and mapping to motor system. The resulting system is neurally plausible, very simple, and should be easily hosted on aVLSI hardware.	Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Lewis, MA (corresponding author), Univ Illinois, Beckman Inst, 405 N Mathews Ave, Urbana, IL 61801 USA.								0	4	4	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						822	828						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700116
C	Mel, BW; Ruderman, DL; Archie, KA		Jordan, MI; Kearns, MJ; Solla, SA		Mel, BW; Ruderman, DL; Archie, KA			Toward a single-cell account for binocular disparity tuning: An energy model may be hiding in your dendrites	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Hubel and Wiesel (1962) proposed that complex cells in visual cortex are driven by a pool of simple cells with the same preferred orientation but different spatial phases. However, a wide variety of experimental results over the past two decades have challenged the pure hierarchical model, primarily by demonstrating that many complex cells receive monosynaptic input from unoriented LGN cells, or do not depend on simple cell input. We recently showed using a detailed biophysical model that nonlinear interactions among synaptic inputs to an excitable dendritic tree could provide the nonlinear subunit computations that underlie complex cell responses (Mel, Ruderman, & Archie, 1997). This work extends the result to the case of complex cell binocular disparity tuning, by demonstrating in an isolated model pyramidal cell (1) disparity tuning at a resolution much finer than the the overall dimensions of the cell's receptive field, and (2) systematically shifted optimal disparity values for rivalrous pairs of light and dark bars-both in good agreement with published reports (Ohzawa, DeAngelis, & Freeman, 1997). Our results reemphasize the potential importance of intradendritic computation for binocular visual processing in particular, and for cortical neurophysiology in general.	Univ So Calif, Dept Biomed Engn, Los Angeles, CA 90089 USA	University of Southern California	Mel, BW (corresponding author), Univ So Calif, Dept Biomed Engn, MC 1451, Los Angeles, CA 90089 USA.	mel@quake.usc.edu; ruderman@salk.edu; karchie@quake.usc.edu							0	4	4	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						208	214						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700030
C	Merz, CJ		Jordan, MI; Kearns, MJ; Solla, SA		Merz, CJ			Combining classifiers using correspondence analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Several effective methods for improving the performance of a single learning algorithm have been developed recently. The general approach is to create a set of learned models by repeatedly applying the algorithm to different versions of the training data, and then combine the learned models' predictions according to a prescribed voting scheme. Little work has been done in combining the predictions of a collection of models generated by many learning algorithms having different representation and/or search strategies. This paper describes a method which uses the strategies of stacking and correspondence analysis to model the relationship between the learning examples and the way in which they are classified by a collection of learned models. A nearest neighbor method is then applied within the resulting representation to classify previously unseen examples. The new algorithm consistently performs as well or better than other combining: techniques on a suite of data sets.	Univ Calif Irvine, Dept Informat & Comp Sci, Irvine, CA 92697 USA	University of California System; University of California Irvine	Merz, CJ (corresponding author), Univ Calif Irvine, Dept Informat & Comp Sci, Irvine, CA 92697 USA.								0	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						591	597						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700084
C	Saul, L; Rahim, M		Jordan, MI; Kearns, MJ; Solla, SA		Saul, L; Rahim, M			Modeling acoustic correlations by factor analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	Advances in Neural Information Processing Systems		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Hidden Markov models (HMMs) for automatic speech recognition rely on high dimensional feature vectors to summarize the short-time properties of speech. Correlations between features can arise when the speech signal is non-stationary or corrupted by noise. We investigate how to model these correlations using factor analysis, a statistical method for dimensionality reduction. Factor analysis uses a small number of parameters to model the covariance structure of high dimensional data. These parameters are estimated by an Expectation-Maximization (EM) algorithm that can be embedded in the training procedures for HMMs. We evaluate the combined use of mixture densities and factor analysis in HMMs that recognize alphanumeric strings. Holding the total number of parameters fixed, we find that these methods, properly combined, yield better models than either method on its own.	AT&T Labs Res, Florham Park, NJ 07932 USA	AT&T	Saul, L (corresponding author), AT&T Labs Res, 180 Pk Ave,D-130, Florham Park, NJ 07932 USA.	lsaul@research.att.com; mazin@research.att.com							0	4	4	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						749	755						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700106
C	Becker, S		Mozer, MC; Jordan, MI; Petsche, T		Becker, S			Learning temporally persistent hierarchical representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A biologically motivated model of cortical self-organization is proposed. Context is combined with bottom-up information via a maximum likelihood cost function. Clusters of one or more units are modulated by a common contextual gating signal; they thereby organize themselves into mutually supportive predictors of abstract contextual features. The model was tested in its ability to discover viewpoint-invariant classes on a set of real image sequences of centered, gradually rotating faces. It performed considerably better than supervised back-propagation at generalizing to novel views from a small number of training examples.			Becker, S (corresponding author), MCMASTER UNIV,DEPT PSYCHOL,HAMILTON,ON L8S 4K1,CANADA.			Becker, Suzanna/0000-0002-2645-070X					0	4	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						824	830						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00116
C	Harris, JG; Chiang, YM		Mozer, MC; Jordan, MI; Petsche, T		Harris, JG; Chiang, YM			An analog implementation of the constant statistics constraint for sensor calibration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We use the constant statistics constraint to calibrate an array of sensors that contains gain and offset variations. This algorithm has been mapped to analog hardware and designed and fabricated with a 2um CMOS technology. Measured results from the chip show that the system achieves invariance to gain and offset variations of the input signal.			Harris, JG (corresponding author), UNIV FLORIDA,DEPT ELECT & COMP ENGN,COMP NEUROENGN LAB,GAINESVILLE,FL 32611, USA.								0	4	5	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						699	705						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00099
C	Leisch, F; Hornik, K		Mozer, MC; Jordan, MI; Petsche, T		Leisch, F; Hornik, K			ARC-LH: A new adaptive resampling algorithm for improving ANN classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We introduce arc-lh, a new algorithm for improvement of ANN classifier performance, which measures the importance of patterns by aggregated network output errors. On several artificial benchmark problems, this algorithm compares favorably with other resample and combine techniques.			Leisch, F (corresponding author), VIENNA TECH UNIV,INST STAT & WAHRSCHEINLICHKEITSTHEORIE,A-1040 VIENNA,AUSTRIA.		Leisch, Friedrich/A-6977-2013	Leisch, Friedrich/0000-0001-7278-1983					0	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						522	528						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00074
C	Orr, GB		Mozer, MC; Jordan, MI; Petsche, T		Orr, GB			Removing noise in on-line search using adaptive batch sizes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Stochastic (on-line) learning can be faster than batch learning. However, at late times, the learning rate must be annealed to remove the noise present in the stochastic weight updates. In this annealing phase, the convergence rate (in mean square) is at best proportional to 1/tau where tau is the number of input presentations. An alternative is to increase the batch size to remove the noise. In this paper we explore convergence for LMS using 1) small but fixed batch sizes and 2) an adaptive batch size. We show that the best adaptive batch schedule is exponential and has a rate of convergence which is the same as for annealing, i.e., at best proportional to 1/tau.			Orr, GB (corresponding author), WILLAMETTE UNIV,DEPT COMP ENGN,900 STATE ST,SALEM,OR 97301, USA.								0	4	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						232	238						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00033
C	Baldi, P; Hornik, K		Touretzky, DS; Mozer, MC; Hasselmo, ME		Baldi, P; Hornik, K			Universal approximation and learning of trajectories using oscillators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CALTECH,DIV BIOL,PASADENA,CA 91125	California Institute of Technology									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						451	457						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00064
C	Bos, S		Touretzky, DS; Mozer, MC; Hasselmo, ME		Bos, S			A realizable learning task which exhibits overfitting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						RIKEN,LAB INFORMAT REPRESENTAT,WAKO,SAITAMA 35101,JAPAN	RIKEN									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						218	224						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00031
C	Coenen, OJMD; Sejnowski, TJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Coenen, OJMD; Sejnowski, TJ			A dynamical model of context dependencies for the vestibulo-ocular reflex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SALK INST BIOL STUDIES,HOWARD HUGHES MED INST,COMPUTAT NEUROBIOL LAB,LA JOLLA,CA 92037	Howard Hughes Medical Institute; Salk Institute			Sejnowski, Terrence/AAV-5558-2021						0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						89	95						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00013
C	Darrell, T; Pentland, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Darrell, T; Pentland, A			Active gesture recognition using learned visual attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,MEDIA LAB,PERCEPTUAL COMP GRP,CAMBRIDGE,MA 02138	Massachusetts Institute of Technology (MIT)									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						858	864						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00121
C	Dasgupta, B; Sontag, ED		Touretzky, DS; Mozer, MC; Hasselmo, ME		Dasgupta, B; Sontag, ED			Ample complexity for learning recurrent perceptron mappings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV WATERLOO,DEPT COMP SCI,WATERLOO,ON N2L 3G1,CANADA	University of Waterloo			Sontag, Eduardo D/J-4420-2012	Sontag, Eduardo D/0000-0001-8020-5783					0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						204	210						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00029
C	EtienneCummings, R; VanderSpiegel, J; Mueller, P		Touretzky, DS; Mozer, MC; Hasselmo, ME		EtienneCummings, R; VanderSpiegel, J; Mueller, P			VLSI model of primate visual smooth pursuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SO ILLINOIS UNIV,DEPT ELECT ENGN,CARBONDALE,IL 62901	Southern Illinois University System; Southern Illinois University									0	4	4	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						706	712						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00100
C	Joseph, SRH; Willshaw, DJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Joseph, SRH; Willshaw, DJ			The role of activity in synaptic competition at the neuromuscular junction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV EDINBURGH,CTR COGNIT SCI,EDINBURGH EH8 9LW,MIDLOTHIAN,SCOTLAND	University of Edinburgh									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						96	102						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00014
C	Kadirkamanathan, V; Kadirkamanathan, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Kadirkamanathan, V; Kadirkamanathan, M			Recursive estimation of dynamic modular RBF networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV SHEFFIELD,AUTOMAT CONTROL & SYST ENG DEPT,SHEFFIELD S1 4DU,S YORKSHIRE,ENGLAND	University of Sheffield									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						239	245						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00034
C	Marshall, JA; Alley, RK; Hubbard, RS		Touretzky, DS; Mozer, MC; Hasselmo, ME		Marshall, JA; Alley, RK; Hubbard, RS			Learning to predict visibility and invisibility from occlusion events	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV N CAROLINA,DEPT COMP SCI,CHAPEL HILL,NC 27599	University of North Carolina; University of North Carolina Chapel Hill									0	4	4	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						816	822						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00115
C	Mel, BW		Touretzky, DS; Mozer, MC; Hasselmo, ME		Mel, BW			SEEMORE: A view-based approach to 3-D object recognition using multiple visual cues	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV SO CALIF,DEPT BIOMED ENGN,LOS ANGELES,CA 90089	University of Southern California									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						865	871						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00122
C	Melia, M; Jordan, MI		Touretzky, DS; Mozer, MC; Hasselmo, ME		Melia, M; Jordan, MI			Learning fine motion by Markov mixtures of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,DEPT ELECT ENGN & COMP SCI,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)			Jordan, Michael I/C-5253-2013						0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1003	1009						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00141
C	Towell, G		Touretzky, DS; Mozer, MC; Hasselmo, ME		Towell, G			Using unlabeled data for supervised learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SIEMENS CORP RES,PRINCETON,NJ 08540	Siemens AG									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						647	653						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00092
C	Wawrzynek, J; Asanovic, K; Kingsbury, B; Beck, J; Johnson, D; Morgan, N		Touretzky, DS; Mozer, MC; Hasselmo, ME		Wawrzynek, J; Asanovic, K; Kingsbury, B; Beck, J; Johnson, D; Morgan, N			SPERT-II: A vector microprocessor system and its application to large problems in backpropagation training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF BERKELEY,DEPT ELECT ENGN & COMP SCI,BERKELEY,CA 94720	University of California System; University of California Berkeley									0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						619	625						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00088
C	West, AHL; Saad, D		Touretzky, DS; Mozer, MC; Hasselmo, ME		West, AHL; Saad, D			Adaptive back-propagation in on-line learning of multilayer networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV EDINBURGH,DEPT PHYS,EDINBURGH EH9 3JZ,MIDLOTHIAN,SCOTLAND	University of Edinburgh				Saad, David/0000-0001-9821-2623					0	4	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						323	329						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00046
C	KUH, A; PETSCHE, T; RIVEST, RL		MOODY, JE; HANSON, SJ; LIPPMANN, RP		KUH, A; PETSCHE, T; RIVEST, RL			INCREMENTALLY LEARNING TIME-VARYING HALF-PLANES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	4	4	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						920	927						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00113
C	SIMARD, P; LECUN, Y		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SIMARD, P; LECUN, Y			REVERSE TDNN - AN ARCHITECTURE FOR TRAJECTORY GENERATION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	4	4	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						579	588						10	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00071
C	Bose, AJ; Gidel, G; Berard, H; Cianflone, A; Vincent, P; Lacoste-Julien, S; Hamilton, WL		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Bose, Avishek Joey; Gidel, Gauthier; Berard, Hugo; Cianflone, Andre; Vincent, Pascal; Lacoste-Julien, Simon; Hamilton, William L.			Adversarial Example Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					The existence of adversarial examples capable of fooling trained neural network classifiers calls for a much better understanding of possible attacks to guide the development of safeguards against them. This includes attack methods in the challenging non-interactive blackbox setting, where adversarial attacks are generated without any access, including queries, to the target model. Prior attacks in this setting have relied mainly on algorithmic innovations derived from empirical observations (e.g., that momentum helps), lacking principled transferability guarantees. In this work, we provide a theoretical foundation for crafting transferable adversarial examples to entire hypothesis classes. We introduce Adversarial Example Games (AEG), a framework that models the crafting of adversarial examples as a min-max game between a generator of attacks and a classifier. AEG provides a new way to design adversarial examples by adversarially training a generator and a classifier from a given hypothesis class (e.g., architecture). We prove that this game has an equilibrium, and that the optimal generator is able to craft adversarial examples that can attack any classifier from the corresponding hypothesis class. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets, outperforming prior state-of-the-art approaches with an average relative improvement of 29.9% and 47.2% against undefended and robust models (Table 2 & 3) respectively.	[Bose, Avishek Joey; Gidel, Gauthier; Berard, Hugo; Cianflone, Andre; Vincent, Pascal; Lacoste-Julien, Simon; Hamilton, William L.] McGill Univ, Mila, Montreal, PQ, Canada; [Vincent, Pascal] Facebook AI Res, Montreal, PQ, Canada	McGill University; Facebook Inc	Bose, AJ (corresponding author), McGill Univ, Mila, Montreal, PQ, Canada.	joey.bose@mail.mcgill.ca; gauthier.gidel@umontreal.ca			Canada CIFAR AI Chair Program; NSERC [RGPIN-2019-05123, RGPIN-2017-06936]; IVADO Fundamental Research Project [PRF-2019-3583139727]; Google Focused Research award; IVADO PhD fellowship; Borealis AI fellowship; Canada Excellence Research Chair in "Data Science for Real-Time Decision-making"	Canada CIFAR AI Chair Program; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); IVADO Fundamental Research Project; Google Focused Research award(Google Incorporated); IVADO PhD fellowship; Borealis AI fellowship; Canada Excellence Research Chair in "Data Science for Real-Time Decision-making"	This work is partially supported by the Canada CIFAR AI Chair Program (held at Mila), NSERC Discovery Grant RGPIN-2019-05123 (held by Will Hamilton at McGill), NSERC Discovery Grant RGPIN-2017-06936, an IVADO Fundamental Research Project grant PRF-2019-3583139727, and a Google Focused Research award (both held at U. Montreal by Simon Lacoste-Julien). Joey Bose was also supported by an IVADO PhD fellowship, Gauthier Gidel by a Borealis AI fellowship and by the Canada Excellence Research Chair in "Data Science for Real-Time Decision-making" (held at Polytechnique by Andrea Lodi), and Andre Cianflone by a NSERC scholarship and a Borealis AI fellowship. Simon Lacoste-Julien and Pascal Vincent are CIFAR Associate Fellows in the Learning in Machines & Brains program. Finally, we thank Facebook for access to computational resources.	Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andriushchenko M., 2020, 16 EUR C COMP VIS EC; Athalye A, 2018, PR MACH LEARN RES, V80; Baluja S., 2018, 32 AAAI C ART INT AA; Berard II., 2020, 8 INT C LEARN REPR I; Bhambri S., 2019, ARXIV191201667; Billingsley, 1968, CONVERGE PROBAB MEAS; Bose A. J., 2019, ARXIV190510864; Bose AJ, 2018, IEEE INT WORKSH MULT; Bruckner M., 2011, P 17 ACM SIGKDD INT; Bruckner M, 2012, J MACH LEARN RES, V13, P2617; Buckman Jacob, 2018, INT C LEARN REPR, DOI DOI 10.1109/TCYB.2016.2523000; Carlini C, 2017, IEEE PES INNOV SMART; Carlini N., 2019, CORR; Carlini Nicholas, 2017, ARXIV171108478; Chakraborty Anirban, 2018, ARXIV PREPRINT ARXIV; Charles Z, 2019, PR MACH LEARN RES, V89; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Croce F., 2019, 8 INT C LEARN REPR I; Croce F., 2020, 37 INT C MACH LEARN; Dhillon G. S., 2018, P INT C LEARN REPR I; Ding, 2020, INT C LEARN REPR; Ding G. W., 2019, ARXIV190207623; Dong YJ, 2020, PLANT BIOSYST, V154, P405, DOI 10.1080/11263504.2019.1651771; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Du J., 2020, 8 INT C LEARN REPR I; Erraqabi A., 2018, ARXIV180104055; Fan K., 1953, P NATL ACAD SCI US; Gidel G., 2019, INT C LEARNING REPRE; Gidel Gauthier, 2020, ARXIV200205820; Gong Z., 2017, ARXIV170504960; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grosse Kathrin, 2017, ARXIV170206280; Guo C, 2019, PR MACH LEARN RES, V97; Guo Chuan, 2018, ICLR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Gao, 2018, ICLR; Huang Z., 2020, INT C LEARN REPR; Ilyas A., 2018, 35 INT C MACH LEARN; Ilyas A., 2019, 7 INT C LEARN REPR I; Ilyas Andrew, 2017, ARXIV171207113; Jang E., 2017, ICLR; Jiang LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P864, DOI 10.1145/3343031.3351088; LeCun Y., 2015, LENET5 CONVOLUTIONAL; Li Y., 2018, 34 AAAI C ART INT AA; Li Y., 2019, 36 INT C MACH LEARN; Liu Yanpei, 2016, ARXIV161102770; Maddison Chris J, 2017, ICLR; Madry A., 2017, 6 INT C LEARN REPR I; Metzen J. H., 2017, 6 INT C LEARN REPR I; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Nash J., 1951, ANN MATH; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Shi YC, 2019, PROC CVPR IEEE, P6512, DOI 10.1109/CVPR.2019.00668; Song Y., 2018, 6 INT C LEARN REPR I; Sun L, 2018, CYBERSECURITY, V1, DOI 10.1186/s42400-018-0012-9; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131; Tu C.-C., 2019, P AAAI C ART INT; Vaswani S, 2019, ADV NEUR IN, V32; WALD A, 1945, ANN MATH, V46, P265, DOI 10.2307/1969022; Wu D., 2020, 8 INT C LEARN REPR I; Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284; Xu H., 2019, ARXIV190908072; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang JY, 2021, COMPUT ECON, V58, P867, DOI 10.1007/s10614-020-10055-9; Zhao Z., 2018, 6 INT C LEARN REPR	76	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													14	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000059
C	Li, YX; Yang, ZH; Wang, YH; Xu, C		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Li, Yanxi; Yang, Zhaohui; Wang, Yunhe; Xu, Chang			Adapting Neural Architectures Between Domains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Neural architecture search (NAS) has demonstrated impressive performance in automatically designing high-performance neural networks. The power of deep neural networks is to be unleashed for analyzing a large volume of data (e.g. ImageNet), but the architecture search is often executed on another smaller dataset (e.g. CIFAR-10) to finish it in a feasible time. However, it is hard to guarantee that the optimal architecture derived on the proxy task could maintain its advantages on another more challenging dataset. This paper aims to improve the generalization of neural architectures via domain adaptation. We analyze the generalization bounds of the derived architecture and suggest its close relations with the validation error and the data distribution distance on both domains. These theoretical analyses lead to AdaptNAS, a novel and principled approach to adapt neural architectures between domains in NAS. Our experimental evaluation shows that only a small part of ImageNet will be sufficient for AdaptNAS to extend its architecture success to the entire ImageNet and outperform state-of-the-art comparison algorithms.	[Li, Yanxi; Xu, Chang] Univ Sydney, Sch Comp Sci, Sydney, NSW, Australia; [Yang, Zhaohui; Wang, Yunhe] Huawei Technol, Noahs Ark Lab, Shenzhen, Peoples R China; [Yang, Zhaohui] Peking Univ, Dept Machine Intelligence, Key Lab Machine Percept MOE, Beijing, Peoples R China	University of Sydney; Huawei Technologies; Peking University	Li, YX (corresponding author), Univ Sydney, Sch Comp Sci, Sydney, NSW, Australia.	yali0722@uni.sydney.edu.au; zhaohuiyang@pku.edu.cn; yunhe.wang@huawei.com; c.xu@sydney.edu.au		Li, Yanxi/0000-0002-0403-8511; Xu, Chang/0000-0002-4756-0609	Australian Research Council [DE180101438]	Australian Research Council(Australian Research Council)	The authors would like to thank the Area Chair and the reviewers for their constructive comments. This work was supported by the Australian Research Council under Project DE180101438.	Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Arjovsky M., 2017, PRINCIPLED METHODS T; Ben-David Shai, 2007, NEURIPS, P7; Blitzer J., 2007, ADV NEURAL INFORM PR, V20, P129; CAI H., 2018, P INT C LEARN REPR; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Yaroslav, 2014, ARXIV14097495; Gidaris P., 2018, P INT C LEARN REPR I, V17; Howard A.G, 2017, ARXIV170404861; Kifer Daniel, 2004, VLDB, DOI DOI 10.1016/B978-012088469-8/50019-X; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y., 2020, INT C MACH LEARN; Liu H., 2018, ARXIV180609055; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Pham H, 2018, PR MACH LEARN RES, V80; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tian Y, 2020, IEEE T PATTERN ANAL; Yan S., 2019, P IEEE INT C COMP VI, P0; Yang ZH, 2020, PROC CVPR IEEE, P1826, DOI 10.1109/CVPR42600.2020.00190; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	26	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000032
C	Roth, K; Kilcher, Y; Hofmann, T		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Roth, Kevin; Kilcher, Yannic; Hofmann, Thomas			Adversarial Training is a Form of Data-dependent Operator Norm Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				ROBUSTNESS	We establish a theoretical link between adversarial training and operator norm regularization for deep neural networks. Specifically, we prove that l(p) -norm constrained projected gradient ascent based adversarial training with an lq-norm loss on the logits of clean and perturbed inputs is equivalent to data-dependent (p, q) operator norm regularization. This fundamental connection confirms the long-standing argument that a network's sensitivity to adversarial examples is tied to its spectral properties and hints at novel ways to robustify and defend against adversarial attacks. We provide extensive empirical evidence on state-of-the-art network architectures to support our theoretical results.	[Roth, Kevin; Kilcher, Yannic; Hofmann, Thomas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Roth, K; Kilcher, Y (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	kevin.roth@inf.ethz.ch; yannic.kilcher@inf.ethz.ch; thomas.hofmann@inf.ethz.ch			ETH Zurich	ETH Zurich(ETH Zurich)	We would like to thank Michael Tschannen, Sebastian Nowozin and Antonio Orvieto for insightful discussions and helpful comments. All authors are directly funded by ETH Zurich.	[Anonymous], 2018, INT C LEARN REPR; Bertsimas D, 2018, EUR J OPER RES, V270, P931, DOI 10.1016/j.ejor.2017.03.051; Bietti A, 2019, PR MACH LEARN RES, V97; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Boyd D. W., 1974, Linear Algebra and Its Applications, V9, P95, DOI 10.1016/0024-3795(74)90029-9; Bubeck S, 2019, PR MACH LEARN RES, V97; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; ElGhaoui L, 1997, SIAM J MATRIX ANAL A, V18, P1035, DOI 10.1137/S0895479896298130; Farnia F., 2018, ARXIV181107457; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; FEINMAN R., 2017, ARXIV170300410; Gao R., 2016, MATH OPER RES; Gilmer J., 2018, INT C LEARN REPR WOR; Goodfellow I. J., 2014, ARXIV14126572; Grosse Kathrin, 2017, ARXIV170206280; Gu S., 2014, ARXIV14125068; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; HIGHAM NJ, 1992, NUMER MATH, V62, P539, DOI 10.1007/BF01396242; Kannan Harini, 2018, ARXIV180306373; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Lyu CC, 2015, IEEE DATA MINING, P301, DOI 10.1109/ICDM.2015.84; Madry Aleksander, 2018, ICLR; Metzen J.H., 2017, ICLR; Miyato T., 2015, ARXIV PREPRINT ARXIV; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Miyato Takeru, 2018, 6 INT C LEARNING REP, P8; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Namkoong H, 2017, ADV NEURAL INFORM PR, V30, P2971; Novak R., 2018, INT C LEARN REPR; Papernot N, 2017, ARXIV161000768; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Raghu M, 2017, PR MACH LEARN RES, V70; Roth K, 2019, PR MACH LEARN RES, V97; Sabour S., 2015, ARXIV151105122; Schmidt L, 2018, ADV NEUR IN, V31; Shaham U, 2018, NEUROCOMPUTING, V307, P195, DOI 10.1016/j.neucom.2018.04.027; Sinha A., 2018, ICLR; TROPP J. A., 2004, THESIS; Tsuzuku Y., 2018, ADV NEURAL INFORM PR; Wong E, 2019, PR MACH LEARN RES, V97; Xu H, 2009, J MACH LEARN RES, V10, P1485; Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198; Yoshida Y, 2017, ARXIV PREPRINT ARXIV; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]	50	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000070
C	Wu, F; Rebeschini, P		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Wu, Fan; Rebeschini, Patrick			A Continuous-Time Mirror Descent Approach to Sparse Phase Retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				CONVERGENCE; NONSMOOTH; ALGORITHM	We analyze continuous-time mirror descent applied to sparse phase retrieval, which is the problem of recovering sparse signals from a set of magnitude-only measurements. We apply mirror descent to the unconstrained empirical risk minimization problem (batch setting), using the square loss and square measurements. We provide a convergence analysis of the algorithm in this non-convex setting and prove that, with the hypentropy mirror map, mirror descent recovers any k-sparse vector x* is an element of R-n with minimum (in modulus) non-zero entry on the order of parallel to X*parallel to(2)/root k from k(2) Gaussian measurements, modulo logarithmic terms. This yields a simple algorithm which, unlike most existing approaches to sparse phase retrieval, adapts to the sparsity level, without including thresholding steps or adding regularization terms. Our results also provide a principled theoretical understanding for Hadamard Wirtinger flow [54], as Euclidean gradient descent applied to the empirical risk problem with Hadamard parametrization can be recovered as a first-order approximation to mirror descent in discrete time.(1)	[Wu, Fan; Rebeschini, Patrick] Univ Oxford, Dept Stat, Oxford, England	University of Oxford	Wu, F (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	fan.wu@stats.ox.ac.uk; patrick.rebeschini@stats.ox.ac.uk			MRC through the OxWaSP CDT programme [EP/L016710/1]; EPSRC	MRC through the OxWaSP CDT programme(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Fan Wu is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1).	Ali A, 2020, PR MACH LEARN RES, V119; Ali A, 2019, PR MACH LEARN RES, V89; Amid E., 2020, ARXIV200210487; Arora Sanjeev, 2019, ADV NEURAL INFORM PR, P7411; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bubeck S., 2015, FDN TRENDS MACHINE L; Bunk O, 2007, ACTA CRYSTALLOGR A, V63, P306, DOI 10.1107/S0108767307021903; Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; CHEN G, 2015, ADV NEURAL INFORM PR, P739; Chen G, 1993, SIAM J OPTIMIZ, V3, P538, DOI 10.1137/0803026; Corbett JV, 2006, REP MATH PHYS, V57, P53, DOI 10.1016/S0034-4877(06)80008-X; Davis D., 2018, ARXIV180202988; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Ghai U., 2020, INT C ALG LEARN THEO, P386; Gunasekar S., 2020, ARXIV200401025; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Hoff PD, 2017, COMPUT STAT DATA AN, V115, P186, DOI 10.1016/j.csda.2017.06.007; Jaganathan K., 2017, OPTICAL COMPRESSIVE, P263; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Kotllowski W., 2019, ARXIV190203035; Krichene W, 2015, PR MACH LEARN RES, V37, P824; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Li Y., 2018, C LEARN THEOR, V75, P2; Ma C, 2018, PR MACH LEARN RES, V80; MAILLARD OA, 2010, JOINT EUR C MACH LEA, V6322, P305; Mei Song, 2019, PROC MACH LEARN RES, V99, P2388; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Netrapalli P, 2015, IEEE T SIGNAL PROCES, V63, P4814, DOI 10.1109/TSP.2015.2448516; Ohlsson H., 2012, P NEUR INF PROC SYST; Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639; Rotskoff G.M., 2018, NEURAL NETWORKS INTE; Shalev-Schwartz S., 2015, FDN TRENDS MACHINE L, V4, P107; Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687; Sirignano J, 2018, J COMPUT PHYS, V375, P1339, DOI 10.1016/j.jcp.2018.08.029; Suggala AS, 2018, ADV NEUR IN, V31; Va~skevi~cius T., 2020, ARXIV200200189; Vaskevicius T, 2019, ADV NEUR IN, V32; Voroninski V., 2016, COMPRESSED SENSING P; Walther A., 1963, OPT ACTA, V10, P41, DOI DOI 10.1080/713817747; Wang G, 2018, IEEE T INFORM THEORY, V64, P773, DOI 10.1109/TIT.2017.2756858; Wang G, 2018, IEEE T SIGNAL PROCES, V66, P479, DOI 10.1109/TSP.2017.2771733; Warmuth M. K., 1998, EL P 5 INT S ART INT; Wu F., 2020, ARXIV200601065; Wu F, 2020, ADV NEUR IN, V33; Yuan ZY, 2019, J COMPUT APPL MATH, V355, P162, DOI 10.1016/j.cam.2019.01.009; Zhang L, 2018, IEEE T SIGNAL PROCES, V66, P5029, DOI 10.1109/TSP.2018.2862395; Zhang S., 2018, ARXIV180604781; Zhao Peng, 2019, ARXIV190309367; Zhou ZY, 2020, SIAM J OPTIMIZ, V30, P687, DOI 10.1137/17M1134925; Zhou ZY, 2017, ADV NEUR IN, V30	60	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000023
C	Wu, Q; Wong, FMF; Li, YH; Liu, ZM; Kanade, V		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Wu, Qiong; Wong, Felix M. F.; Li, Yanhua; Liu, Zhenming; Kanade, Varun			Adaptive Reduced Rank Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				SELECTION; MATRICES	We study the low rank regression problem y = Mx + epsilon, where x and y are d(1) and d(2) dimensional vectors respectively. We consider the extreme high-dimensional setting where the number of observations n is less than d(1)+d(2). Existing algorithms are designed for settings where n is typically as large as rank(M)(d(1)+d(2)). This work provides an efficient algorithm which only involves two SVD, and establishes statistical guarantees on its performance. The algorithm decouples the problem by first estimating the precision matrix of the features, and then solving the matrix denoising problem. To complement the upper bound, we introduce new techniques for establishing lower bounds on the performance of any algorithm for this problem. Our preliminary experiments confirm that our algorithm often out-performs existing baselines, and is always at least competitive.	[Wu, Qiong; Liu, Zhenming] William & Mary, Williamsburg, VA 23185 USA; [Li, Yanhua] Worcester Polytech Inst, Worcester, MA 01609 USA; [Kanade, Varun] Univ Oxford, Oxford, England; [Wong, Felix M. F.] Google, Mountain View, CA 94043 USA	Worcester Polytechnic Institute; University of Oxford; Google Incorporated	Wu, Q (corresponding author), William & Mary, Williamsburg, VA 23185 USA.	qwu05@email.wm.edu		Wu, Qiong/0000-0001-7724-8221	Alan Turing Institute under the EPSRC [EP/N510129/1]; NSF [IIS-1942680, CNS-1952085, CMMI-1831140, DGE-2021871, NSF-2008557, NSF-1835821, NSF-1755769]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); NSF(National Science Foundation (NSF))	We thank anonymous reviewers for helpful comments and suggestions. Varun Kanade is supported in part by the Alan Turing Institute under the EPSRC grant EP/N510129/1. Yanhua Li was supported in part by NSF grants IIS-1942680 (CAREER), CNS-1952085, CMMI-1831140, and DGE-2021871. Qiong Wu and Zhenming Liu are supported by NSF grants NSF-2008557, NSF-1835821, and NSF-1755769. The authors acknowledge William & Mary Research Computing for providing computational resources and technical support that have contributed to the results reported within this paper.	Agarwal Anish, 2019, ADV NEURAL INFORM PR, P9889; Akemann G, 2010, PHYSICA A, V389, P2566, DOI 10.1016/j.physa.2010.02.026; ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580; Ang A., 2014, ASSET MANAGEMENT SYS; Bai Z, 2010, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4419-0661-8; Bamman D, 2014, J SOCIOLING, V18, P135, DOI 10.1111/josl.12080; Batis C, 2016, PUBLIC HEALTH NUTR, V19, P195, DOI 10.1017/S1368980014003103; Bender J., 2013, FDN FACTOR INVESTING; Bunea F, 2011, ANN STAT, V39, P1282, DOI 10.1214/11-AOS876; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Cao LJ, 2003, NEUROCOMPUTING, V55, P321, DOI 10.1016/S0925-2312(03)00433-8; Chen K, 2013, BIOMETRIKA, V100, P901, DOI 10.1093/biomet/ast036; Chen L., 2019, MANAGE SCI; Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111; Donoho D, 2014, ANN STAT, V42, P2413, DOI 10.1214/14-AOS1257; Fan F, 2017, INFORM SCIENCES, V397, P48, DOI 10.1016/j.ins.2017.02.044; Fang CS, 2011, IEEE INTELL SYST, V26, P48, DOI 10.1109/MIS.2011.44; Frank LK, 2015, NUTRIENTS, V7, P5497, DOI 10.3390/nu7075233; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Gavish M, 2014, IEEE T INFORM THEORY, V60, P5040, DOI 10.1109/TIT.2014.2323359; Ghodsi A., 2006, DIMENSIONALITY REDUC, V37; Gu S., 2018, TECHNICAL REPORT; Oliveira RI, 2010, ELECTRON COMMUN PROB, V15, P203, DOI 10.1214/ECP.v15-1544; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1; KATO T, 1987, COMMUN MATH PHYS, V111, P501, DOI 10.1007/BF01238911; Kelly BT, 2019, J FINANC ECON, V134, P501, DOI 10.1016/j.jfineco.2019.05.001; Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894; Ma Zongming, 2014, ARXIV1403; Medeiros Marcelo C, 2012, ESTIMATING HIGH DIME; Mukherjee Ashin, 2011, Stat Anal Data Min, V4, P612, DOI 10.1002/sam.10138; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Nobi A, 2013, J KOREAN PHYS SOC, V62, P569, DOI 10.3938/jkps.62.569; Polk C, 2006, J FINANC ECON, V81, P101, DOI 10.1016/j.jfineco.2005.03.013; Rahim M, 2017, INT WORKSHOP PATTERN; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Sinha Shiladitya, 2013, ARXIV13106998; Stock J. H, 2005, 114 NBER; Stock JH, 2002, J AM STAT ASSOC, V97, P1167, DOI 10.1198/016214502388618960; Stock JH., 2011, DYNAMIC FACTOR MODEL; Tibshirani R, 2011, J R STAT SOC B, V73, P273, DOI 10.1111/j.1467-9868.2011.00771.x; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Urama Thomas Chinwe, 2017, INT C COMP MATH COMP, P89; VELU R., 2013, MULTIVARIATE REDUCED, V136; Wang Liwei, 2015, ARXIV151101699; Wu Y.-J., 2017, ARXIV170506772; Zhou Xinfeng, 2014, ACTIVE EQUITY MANAGE; Zhu XF, 2017, IEEE T BIG DATA, V3, P405, DOI 10.1109/TBDATA.2017.2735991	48	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000043
C	Zhang, C; Zhang, K; Li, YZ		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Zhang, Cheng; Zhang, Kun; Li, Yingzhen			A Causal View on Robustness of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We present a causal view on the robustness of neural networks against input manipulations, which applies not only to traditional classification tasks but also to general measurement data. Based on this view, we design a deep causal manipulation augmented model (deep CAMA) which explicitly models possible manipulations on certain causes leading to changes in the observed effect. We further develop data augmentation and test-time fine-tuning methods to improve deep CAMA's robustness. When compared with discriminative deep neural networks, our proposed model shows superior robustness against unseen manipulations. As a by-product, our model achieves disentangled representation which separates the representation of manipulations from those of other latent causes.	[Zhang, Cheng; Li, Yingzhen] Microsoft Res, Redmond, WA 98052 USA; [Zhang, Kun] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Microsoft; Carnegie Mellon University	Zhang, C (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	Cheng.Zhang@microsoft.com; kunz1@cmu.edu; Yingzhen.Li@microsoft.com			United States Air Force [FA8650-17-C-7715]	United States Air Force(United States Department of Defense)	KZ would like to acknowledge the support by the United States Air Force under Contract No. FA8650-17-C-7715. CZ and YL would like to acknowledge Nathan Jones for his support with computing infrastructure; Tom Ellis and Luke Harries for feedback regarding the manuscript.	Alzantot Moustafa, 2018, EMNLP; Arjovsky Martin, 2019, ARXIV190702893; Athalye A, 2018, PR MACH LEARN RES, V80; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Chen T.Q., 2018, NEURIPS, P2610; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Elsayed Gamaleldin F, 2018, ARXIV180208195, P10; Feinman R., 2017, ARXIV PREPRINT ARXIV; Glymour Clark, 2019, FRONT GENET, P10; Gong MM, 2016, PR MACH LEARN RES, V48; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Heinze-Deml C., 2017, ARXIV171011469; HIGGINS M, 2017, INT C LEARN REPR, V3, DOI DOI 10.1057/S41599-017-0005-4; Kilbertus N., 2018, ARXIV181200524; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Li Y., 2018, ARXIV180206552; Madry Aleksander, 2017, ARXIV; Meinshausen N., 2018, ARXIV180106229; Papernot N., 2018, ARXIV161000768; Parascandolo Giambattista, 2017, ARXIV171200961; Pearl Judea, 2018, BOOK WHY NEW SCI CAU; Pearl Judea, 2009, CAMBRIDGE; Peters J, 2017, ADAPT COMPUT MACH LE; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Samangouei Pouya, 2018, ARXIV180506605; Scholkopf Bernhard, 2012, ICML; Schott Lukas, 2019, INT C LEARN REPR; Song Y, 2019, P COMBUST INST, V37, P667, DOI 10.1016/j.proci.2018.06.115; SONG ZX, 2018, INT C MACH LEARN, P2565; Spirtes P., 2000, CAUSATION PREDICTION; Tramer Florian, 2018, INT C LEARN REPR ICL; Wang J, 2018, ENVIRON TECHNOL, V39, P3055, DOI 10.1080/09593330.2017.1371797; Welling, 2019, ARXIV190510427; Zhang H, 2018, ADV NEUR IN, V31; Zhang K., 2020, ADV NEURAL INFORM PR; Zhang Kun, 2018, NATL SCI REV; ZHANG L, 2013, INT C MACH LEARN, P819; Zhao H., 2019, ARXIV190109453	56	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000013
C	Assran, M; Romoff, J; Ballas, N; Pineau, J; Rabbat, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Assran, Mahmoud; Romoff, Joshua; Ballas, Nicolas; Pineau, Joelle; Rabbat, Michael			Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GO	Multi-simulator training has contributed to the recent success of Deep Reinforcement Learning by stabilizing learning and allowing for higher training throughputs. We propose Gossip-based Actor-Learner Architectures (GALA) where several actor-learners (such as A2C agents) are organized in a peer-to-peer communication topology, and exchange information through asynchronous gossip in order to take advantage of a large number of distributed simulators. We prove that GALA agents remain within an c-ball of one-another during training when using loosely coupled asynchronous communication. By reducing the amount of synchronization between agents, GALA is more computationally efficient and scalable compared to A2C, its fully-synchronous counterpart. GALA also outperforms A3C, being more robust and sample efficient. We show that we can run several loosely coupled GALA agents in parallel on a single GPU and achieve significantly higher hardware utilization and frame-rates than vanilla A2C at comparable power draws.	[Assran, Mahmoud; Romoff, Joshua; Ballas, Nicolas; Pineau, Joelle; Rabbat, Michael] Facebook AI Res, Menlo Pk, CA 94025 USA; [Assran, Mahmoud] McGill Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada; [Romoff, Joshua] McGill Univ, Dept Comp Sci, Montreal, PQ, Canada	Facebook Inc; McGill University; McGill University	Assran, M (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.; Assran, M (corresponding author), McGill Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada.	mahmoud.assran@mail.mcgill.ca; joshua.romoff@mail.mcgill.ca; ballasn@fb.com; jpineau@fb.com; mikerabbat@fb.com						[Anonymous], [No title captured]; Assran M., 2018, ARXIV180308950; Assran M., 2018, THESIS; Assran M, 2019, PR MACH LEARN RES, V97; Blondel VD, 2005, IEEE DECIS CONTR P, P2996; Clemente A. V., 2017, MULTIDISCIPLINARY C; Clemons J., 2017, P INT C LEARN REPR; Espeholt L, 2018, PR MACH LEARN RES, V80; Fujimoto S., 2018, P 35 INT C MACH LEAR, P1582; Gruslys A., 2018, P 6 INT C LEARN REPR; Hadjicostis CN, 2014, IEEE T AUTOMAT CONTR, V59, P763, DOI 10.1109/TAC.2013.2275669; Hesse C., 2017, OPENAI BASELINES; Horgan D, 2018, P 6 INT C LEARN REPR; Kapturowski Steven, 2019, P 7 INT C LEARN REPR; Kuttler H., 2019, ARXIV191003552; Lian X, 2017, P ADV NEUR INF PROC, P5330; Lian XR, 2018, PR MACH LEARN RES, V80; Machado MC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5573; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nedi Angelia, 2015, P IEEE, V106, P953; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pineau J., 2018, MACHINE LEARNING REP; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Seneta E, 1981, NONNEGATIVE MATRICES; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Stooke A., 2018, ARXIV180302811; Sutton R. S., 1998, INTRO REINFORCEMENT, V135; Sutton Richard Stuart, 1984, THESIS, P4; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tian Yuandong, 2019, INT C MACHINE LEARNI, P6244; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Vinyals O., 2019, ALPHASTAR MASTERING; Wolfowitz J., 1963, P AM MATH SOC, V14, P733, DOI [DOI 10.1090/S0002-9939-1963-0154756-3, 10.1090/S0002-9939-1963-0154756-3]	35	3	3	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905002
C	Awasthi, P; Dutta, A; Vijayaraghavan, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Awasthi, Pranjal; Dutta, Abhratanu; Vijayaraghavan, Aravindan			On Robustness to Adversarial Examples and Polynomial Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the design of computationally efficient algorithms with provable guarantees, that are robust to adversarial (test time) perturbations. While there has been an explosion of recent work on this topic due to its connections to test time robustness of deep networks, there is limited theoretical understanding of several basic questions like (i) when and how can one design provably robust learning algorithms? (ii) what is the price of achieving robustness to adversarialexamples in a computationally efficient manner? The main contribution of this work is to exhibit a strong connection between achieving robustness to adversarial examples, and a rich class of polynomial optimization problems, thereby making progress on the above questions. In particular, we leverage this connection to (a) design computationally efficient robust algorithms with provable guarantees for a large class of hypothesis, namely linear classifiers and degree-2 polynomial threshold functions (PTFs), (b) give a precise characterization of the price of achieving robustness in a computationally efficient manner for these classes, (c) design efficient algorithms to certify robustness and generate adversarial attacks in a principled manner for 2-layer neural networks. We empirically demonstrate the effectiveness of these attacks on real data.	[Awasthi, Pranjal] Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08854 USA; [Dutta, Abhratanu; Vijayaraghavan, Aravindan] Northwestern Univ, Dept Comp Sci, Evanston, IL 60208 USA	Rutgers State University New Brunswick; Northwestern University	Awasthi, P (corresponding author), Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08854 USA.	pranjal.awasthi@rutgers.edu; abhratanudutta2020@u.northwestern.edu; aravindv@northwestern.edu	Vijayaraghavan, Aravindan/I-2257-2015		National Science Foundation (NSF) [CCF-1652491, CCF-1637585]; Morrison Fellowship from Northwestern University	National Science Foundation (NSF)(National Science Foundation (NSF)); Morrison Fellowship from Northwestern University	The second and third authors were supported by the National Science Foundation (NSF) under Grant No. CCF-1652491 and CCF-1637585. Additionally, the second author was funded by the Morrison Fellowship from Northwestern University.	Alon N, 2006, INVENT MATH, V163, P499, DOI 10.1007/s00222-005-0465-9; Alon N, 2004, P 30 SIXTHANNUAL ACM, P72; [Anonymous], 1994, INTRO COMPUTATIONAL; Arora S, 2005, ANN IEEE SYMP FOUND, P206, DOI 10.1109/SFCS.2005.57; Attias Idan, 2018, ARXIV181002180; Bhattacharyya C, 2004, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON INTELLIGENT SENSING AND INFORMATION PROCESSING, P433; Bietti A., 2018, ARXIV181000363; Bubeck S., 2018, ARXIV180510204; Bubeck S., 2018, ARXIV181106418; Charikar M, 2004, ANN IEEE SYMP FOUND, P54, DOI 10.1109/FOCS.2004.39; Cullina D., 2018, ARXIV180601471; Diochnos D. I., 2018, P ADV NEU INF PRO SY, P10380; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Feige U., 2015, C LEARN THEOR, P637; Garey M.R., 2002, COMPUTERS INTRACTABI, V29; Gilmer J., 2018, INT C LEARN REPR WOR; Gilmer Justin, 2018, ARXIV180706732; Globerson Amir, 2006, P 23 INT C MACH LEAR, P353, DOI DOI 10.1145/1143844.1143889; Gourdeau Pascale, 2019, ARXIV190905822; Grothendieck A., 1976, RESUME THEORIE METRI; KHIM J, 2018, ARXIV PREPRINT ARXIV; Khot S, 2006, ANN IEEE SYMP FOUND, P217; Khot S, 2007, ANN IEEE SYMP FOUND, P318, DOI 10.1109/FOCS.2007.20; Madry A., 2018, ARXIV PREPRINT ARXIV; Mahloujifar Saeed, 2018, ARXIV180903063; Mahloujifar Saeed, 2018, ARXIV181001407; Nesterov Y, 1998, OPTIM METHOD SOFTW, V9, P141, DOI 10.1080/10556789808805690; O'Donnell R, 2014, ANAL BOOLEAN FUNCTIO; Raghunathan Aditi, 2018, INT C LEARN REPR; Schmidt Ludwig, 2018, ARXIV180411285; Shivaswamy PK, 2006, J MACH LEARN RES, V7, P1283; Sinha A., 2018, CERTIFYING SOME DIST; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Tsipras D., 2018, ROBUSTNESS MAY BE OD; Wong E, 2018, PR MACH LEARN RES, V80; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; Xu H, 2009, J MACH LEARN RES, V10, P1485; Yin Dong, 2018, ARXIV181011914	38	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905041
C	Aybat, NS; Fallah, A; Gurbuzbalaban, M; Ozdaglar, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aybat, Necdet Serhat; Fallah, Alireza; Gurbuzbalaban, Mert; Ozdaglar, Asuman			A Universally Optimal Multistage Accelerated Stochastic Gradient Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION ALGORITHMS; COMPOSITE OPTIMIZATION	We study the problem of minimizing a strongly convex, smooth function when we have noisy estimates of its gradient. We propose a novel multistage accelerated algorithm that is universally optimal in the sense that it achieves the optimal rate both in the deterministic and stochastic case and operates without knowledge of noise characteristics. The algorithm consists of stages that use a stochastic version of Nesterov's method with a specific restart and parameters selected to achieve the fastest reduction in the bias-variance terms in the convergence rate bounds.	[Aybat, Necdet Serhat] Penn State Univ, University Pk, PA 16802 USA; [Fallah, Alireza; Ozdaglar, Asuman] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Gurbuzbalaban, Mert] Rutgers State Univ, Piscataway, NJ USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park; Massachusetts Institute of Technology (MIT); Rutgers State University New Brunswick	Aybat, NS (corresponding author), Penn State Univ, University Pk, PA 16802 USA.	nsa10@psu.edu; afallah@mit.edu; mg1366@rutgers.edu; asuman@mit.edu			NSF [CCF-1814888, CMMI-1635106, DMS-1723085]; Siebel Scholarship	NSF(National Science Foundation (NSF)); Siebel Scholarship	The work of Necdet Serhat Aybat is partially supported by NSF Grant CMMI-1635106. Alireza Fallah is partially supported by Siebel Scholarship. Mert Gurbuzbalaban acknowledges support from the grants NSF DMS-1723085 and NSF CCF-1814888.	[Anonymous], 2012, ADV NEURAL INFORM PR; Arjevani Y, 2016, PR MACH LEARN RES, V48; Aybat N. S., 2018, ARXIV180510579; Bach Francis, 2011, NEURAL INFORM PROCES; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bubeck S., 2015, FDN TRENDS MACHINE L; Cohen M., 2018, P MACHINE LEARNING R, V80, P1019; D'Aspremont A, 2008, SIAM J OPTIMIZ, V19, P1171, DOI 10.1137/060676386; DE KLERK E., 2002, APPL OPTIMIZAT, V65, DOI 10.1007/b105286; Dieuleveut A., 2017, J MACHINE LEARNING R, V18, P3520; Flammarion N., 2015, C LEARN THEOR, P658; Gao X., 2018, ARXIV181207725; Gao Xuefeng, 2018, ARXIV E PRINTS; Hardt M, 2014, ROBUSTNESS VERSUS AC; Hu B, 2017, PR MACH LEARN RES, V70; Hu C., 2009, ADV NEURAL INF PROCE, P781; Jain P., 2018, C LEARN THEOR, P545; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Neelakantan A., 2015, P INT C LEARN REPR; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Raginsky Maxim, 2017, ARXIV170203849; Schmidt M, 2015, JMLR WORKSH CONF PRO, V38, P819; Shi Bin, 2018, ARXIV181008907; Sridharan K., 2009, ADV NEURAL INFORM PR, P1545; Vapnik V., 2013, NATURE STAT LEARNING; Vaswani Sharan, 2018, ARXIV181007288; Wai Hoi-To, 2018, ARXIV180600125; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zhu Z., 2017, ITCS	39	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900015
C	Baby, D; Wang, YX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Baby, Dheeraj; Wang, Yu-Xiang			Online Forecasting of Total-Variation-bounded Sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MINIMAX ESTIMATION	We consider the problem of online forecasting of sequences of length a with total-variation at most C-n, using observations contaminated by independent sigma- subgaussian noise. We design an O(n log n)-time algorithm that achieves a cumulative square error of (O) over tilde (n(1/3)C(n)(2/3)sigma(4/3) + C-n(2)) with high probability. We also prove a lower bound that matches the upper bound in all parameters (up to a log(n) factor). To the best of our knowledge, this is the first polynomial-time algorithm that achieves the optimal O(n(1/3)) rate in forecasting total variation bounded sequences and the first algorithm that adapts to unknown C-n. Our proof techniques leverage the special localized structure of Haar wavelet basis and the adaptivity to unknown smoothness parameters in the classical wavelet smoothing [Donoho et al., 1998]. We also compare our model to the rich literature of dynamic regret minimization and nonstationary stochastic optimization, where our problem can be treated as a special case. We show that the workhorse in those settings - online gradient descent and its variants with a fixed restarting schedule - are instances of a class of linear forecasters that require a suboptimal regret of (Omega) over tilde (root n). This implies that the use of more adaptive algorithms is necessary to obtain the optimal rate.	[Baby, Dheeraj; Wang, Yu-Xiang] UC Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA	University of California System; University of California Santa Barbara	Baby, D (corresponding author), UC Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.	dheeraj@ucsb.edu; yuxiangw@cs.ucsb.edu		Wang, Yu-Xiang/0000-0002-6403-212X	UCSB CS department	UCSB CS department	DB and YW were supported by a start-up grant from UCSB CS department and a gift from Amazon Web Services. The authors thank Yining Wang for a preliminary discussion that inspires the work, and Akshay Krishnamurthy and Ryan Tibshirani for helpful comments to an earlier version of the paper.	Bansal Nikhil, 2015, LIPICS LEIBN INT P I, V40; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408; BICKEL PJ, 1981, ANN STAT, V9, P1301, DOI 10.1214/aos/1176345646; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI [10.1007/s100970100031, DOI 10.1007/S100970100031]; Box G., 1970, TIME SERIES ANAL; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chen Niangjun, 2018, P C LEARNING THEORY, P1574; Chen Xi, 2018, NONSTATIONARY STOCHA; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; De Boor C, 1978, PRACTICAL GUIDE SPLI, V27; DONOHO DL, 1990, ANN STAT, V18, P1416, DOI 10.1214/aos/1176347758; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Donoho DL, 1998, ANN STAT, V26, P879; Gaillard P., 2015, C LEARN THEOR, P764; Hall, 2013, P 30 INT C MACH LEAR, P579; Hazan E., 2007, ELECT C COMPUTATIONA, V14; Hodrick RJ, 1997, J MONEY CREDIT BANK, V29, P1, DOI 10.2307/2953682; Hutter Jan-Christian, 2016, C LEARN THEOR COLT 1; Jadbabaie A, 2015, JMLR WORKSH CONF PRO, V38, P398; Johnstone I.M., 2017, GAUSSIAN ESTIMATION; Kim SJ, 2009, SIAM REV, V51, P339, DOI 10.1137/070690274; Koolen Wouter M, 2015, ADV NEURAL INFORM PR, P2557; Kotlowski Wojciech, 2016, P 29 C LEARN THEOR C, V49, P1165; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; Mammen E, 1997, ANN STAT, V25, P387; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Rakhlin Alexander, 2015, ABS150106598 CORR; Rakhlin Alexander, 2014, C LEARN THEOR, P1232; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sadhanala Veeranjaneyulu, 2016, ADV NEURAL INFORM PR; Scholkopf B., 2001, LEARNING KERNELS SUP; Steidl G, 2006, INT J COMPUT VISION, V70, P241, DOI 10.1007/s11263-006-8066-7; TIBSHIRANI R, 2015, NONPARAMETRIC REGRES; Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Wahba G., 1990, SPLINE MODELS OBSERV, V59; Yang TB, 2016, PR MACH LEARN RES, V48; Zhang L., 2018, ADV NEURAL INFORM PR, P1323; Zhang LJ, 2018, PR MACH LEARN RES, V80; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	43	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902067
C	Balseiro, S; Golrezaei, N; Mahdian, M; Mirrokni, V; Schneider, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Balseiro, Santiago; Golrezaei, Negin; Mahdian, Mohammad; Mirrokni, Vahab; Schneider, Jon			Contextual Bandits With Cross-Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In the classical contextual bandits problem, in each round t, a learner observes some context c, chooses some action a to perform, and receives some reward r(a,t) (c). We consider the variant of this problem where in addition to receiving the reward r(a,t (c)), the learner also learns the values of r(a,t) (c ') for all other contexts c '; i.e., the rewards that would have been achieved by performing that action under different contexts. This variant arises in several strategic settings, such as learning how to bid in non-truthful repeated auctions, which has gained a lot of attention lately as many platforms have switched to running first-price auctions. We call this problem the contextual bandits problem with cross-learning. The best algorithms for the classical contextual bandits problem achieve (O) over tilde(root CKT) regret against all stationary policies, where C is the number of contexts, K the number of actions, and T the number of rounds. We demonstrate algorithms for the contextual bandits problem with cross -learning that remove the dependence on C and achieve regret (O) over tilde(root KT). We simulate our algorithms on real auction data from an ad exchange running first-price auctions (showing that they outperform traditional contextual bandit algorithms).	[Balseiro, Santiago] Columbia Business Sch, New York, NY 10027 USA; [Golrezaei, Negin] MIT, Sloan Sch Management, Cambridge, MA 02139 USA; [Mahdian, Mohammad; Mirrokni, Vahab; Schneider, Jon] Google Res, Mountain View, CA USA	Columbia University; Massachusetts Institute of Technology (MIT); Google Incorporated	Balseiro, S (corresponding author), Columbia Business Sch, New York, NY 10027 USA.	srb2155@columbia.edu; golrezae@mit.edu; mahdian@google.com; mirrokni@google.com; jschnei@google.com						Alon N., 2015, PMLR, V40, P23; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Balseiro Santiago, 2018, ARXIV180909582; Beygelzimer A, 2011, INT C ART INT STAT, V15, P19; Braverman Mark, 2017, ARXIV171109176; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cai Yang, 2017, FOCS; den Boer A.V., 2015, SURV OPER RES MANAG, V20, P1; Dudik Miroslav, 2017, FOCS; Feng Z, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P505, DOI 10.1145/3219166.3219208; Golrezaei N., 2018, DYNAMIC INCENTIVE AW; Kale S., 2010, ADV NEURAL INFORM PR, V23, P1054; Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Mannor S., 2011, NIPS, P684; Mohri M, 2016, J MACH LEARN RES, V17; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Slivkins  Aleksandrs, 2011, P 24 ANN C LEARN THE; VICKREY W, 1961, J FINANC, V16, P8, DOI 10.2307/2977633; Villar SS, 2015, STAT SCI, V30, P199, DOI 10.1214/14-STS504; Weed J., 2016, PROC 29 ANN C LEARN, P1562	24	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901032
C	Basu, D; Data, D; Karakus, C; Diggavi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Basu, Debraj; Data, Deepesh; Karakus, Can; Diggavi, Suhas			Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	Communication bottleneck has been identified as a significant issue in distributed optimization of large-scale learning models. Recently, several approaches to mitigate this problem have been proposed, including different forms of gradient compression or computing local models and mixing them iteratively. In this paper we propose Qsparse-local-SGD algorithm, which combines aggressive sparsification with quantization and local computation along with error compensation, by keeping track of the difference between the true and compressed gradients. We propose both synchronous and asynchronous implementations of Qsparse-local-SGD. We analyze convergence for Qsparse-local-SGD in the distributed case, for smooth non-convex and convex objective functions. We demonstrate that Qsparse-local-SGD converges at the same rate as vanilla distributed SGD for many important classes of sparsifiers and quantizers. We use Qsparse-local-SGD to train ResNet-50 on ImageNet, and show that it results in significant savings over the state-of-the-art, in the number of bits transmitted to reach target accuracy.	[Basu, Debraj] Adobe Inc, San Jose, CA 95110 USA; [Basu, Debraj; Data, Deepesh; Karakus, Can; Diggavi, Suhas] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [Karakus, Can] Amazon Inc, Seattle, WA USA	Adobe Systems Inc.; University of California System; University of California Los Angeles; Amazon.com	Basu, D (corresponding author), Adobe Inc, San Jose, CA 95110 USA.; Basu, D (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.	dbasu@adobe.com; deepeshdata@ucla.edu; cakarak@amazon.com; suhasdiggavi@ucla.edu			NSF [1514531]; UC-NL grant [LFR-18-548554]; Army Research Laboratory [W911NF-17-2-0196]	NSF(National Science Foundation (NSF)); UC-NL grant; Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL))	The authors gratefully thank Navjot Singh for his help with experiments in the early stages of this work. This work was partially supported by NSF grant #1514531, by UC-NL grant LFR-18-548554 and by Army Research Laboratory under Cooperative Agreement W911NF-17-2-0196. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D., 2018, NEURIPS, P5977; Alistarh D, 2017, ADV NEUR IN, V30; [Anonymous], 2012, ICML; BACH F., 2011, ADV NEURAL INFORM PR, P451; Basu Debraj, 2019, ABS190602367 CORR; Bernstein J, 2018, PR MACH LEARN RES, V80; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Chen K, 2016, INT CONF ACOUST SPEE, P5880, DOI 10.1109/ICASSP.2016.7472805; Coppola G., 2015, THESIS; GITLIN RD, 1973, IEEE T CIRCUITS SYST, VCT20, P125, DOI 10.1109/TCT.1973.1083627; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karimireddy SP, 2019, PR MACH LEARN RES, V97; Koloskova A, 2019, PR MACH LEARN RES, V97; Konecny J, 2017, STOCHASTIC DISTRIBUT; LAPIN M, 2015, ADV NEURAL INFORM PR, P325; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin Y., 2018, ICLR; Mania H, 2017, SIAM J OPTIMIZ, V27, P2202, DOI 10.1137/16M1057000; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nguyen LM, 2018, PR MACH LEARN RES, V80; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Seide F, 2014, INTERSPEECH, P1058; Sergeev A., 2018, ABS180205799 CORR; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Stich S. U., 2019, ICLR; Stich S. U., 2018, P 32 INT C NEUR INF, P4452; Strom N, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1488; Suresh AT, 2017, PR MACH LEARN RES, V70; Tang H., 2018, NEURIPS, P7663; Wang H, 2018, J CHEM-NY, V2018, DOI 10.1155/2018/9579872; Wang Jianyu, 2018, ABS180807576 CORR; Wangni J., 2018, ADV NEURAL INFORM PR, P1299; Wen W., 2017, NIPS 17, V30, P1509; Wu JX, 2018, PR MACH LEARN RES, V80; Wu TY, 2018, IEEE T SIGNAL INF PR, V4, P293, DOI 10.1109/TSIPN.2017.2695121; Yu H, 2019, PR MACH LEARN RES, V97; Yu H, 2019, AAAI CONF ARTIF INTE, P5693; Zhang Y., 2013, NEURAL INFORM PROCES, P2328; Zhang YC, 2013, J MACH LEARN RES, V14, P3321	44	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906036
C	Belghazi, MI; Oquab, M; Lecun, Y; Lopez-Paz, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Belghazi, Mohamed Ishmael; Oquab, Maxime; Lecun, Yann; Lopez-Paz, David			Learning about an exponential amount of conditional distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIMENSIONALITY	We introduce the Neural Conditioner (NC), a self-supervised machine able to learn about all the conditional distributions of a random vector X. The NC is a function NC(x . a, a, r) that leverages adversarial training to match each conditional distribution P(X-r vertical bar X-a = x(a)). After training, the NC generalizes to sample conditional distributions never seen, including the joint distribution. The NC is also able to auto-encode examples, providing data representations useful for downstream classification tasks. In sum, the NC integrates different self-supervised tasks (each being the estimation of a conditional distribution) and levels of supervision (partially observed data) seamlessly into a single learning experience.	[Belghazi, Mohamed Ishmael; Oquab, Maxime; Lecun, Yann; Lopez-Paz, David] Facebook AI Res, Paris, France; [Belghazi, Mohamed Ishmael] Montreal Inst Learning Algorithms, Montreal, PQ, Canada	Facebook Inc; Universite de Montreal	Belghazi, MI (corresponding author), Facebook AI Res, Paris, France.; Belghazi, MI (corresponding author), Montreal Inst Learning Algorithms, Montreal, PQ, Canada.	ishmael.belghazi@gmail.com; qas@fb.com; yann@fb.com; dlp@fb.com						BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Belghazi MI, 2018, PR MACH LEARN RES, V80; Belghazi Mohamed Ishmael, 2018, INT C MACH LEARN WOR; Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128; Bordes Florian, 2017, LEARNING GENERATE SA; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Douglas Laura, 2017, UNIVERSAL MARGINALIZ; Dumoulin Vincent, 2016, ARXIV E PRINTS; Garcia-Laencina PJ, 2010, NEURAL COMPUT APPL, V19, P263, DOI 10.1007/s00521-009-0295-6; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goroshin R, 2015, IEEE I CONF COMP VIS, P4086, DOI 10.1109/ICCV.2015.465; Goyal A, 2017, ADV NEUR IN, V30; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS 2017; Hastie T., 2009, UNSUPERVISED LEARNIN, P485; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Ivanov Oleg, 2019, ICLR; Jolliffe I, 2011, INT ENCY STAT SCI, P1094, DOI [10.1007/978-3-642-04898-2_455, DOI 10.1007/978-3-642-04898-2_455]; Kalayeh Mahdi M, 2017, IMPROVING FACIAL ATT; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kolesnikov A, 2019, REVISITING SELF SUPE; Larochelle H., 2011, INT C ART INT STAT; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lever Guy, 2012, INT C MACH LEARN ICM, V5; Lichman M, 2013, UCI MACHINE LEARNING; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Miyato Takeru, 2018, ARXIV180205637; Mobahi H., 2009, P 26 ANN INT C MACHI, P737, DOI DOI 10.1145/1553374.1553469; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48; Pathak D., 2017, ICML; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roweis Sam, 1999, LINEAR HETEROENCODER; Royston P, 2011, J STAT SOFTW, V45, P1, DOI 10.18637/jss.v045.i04; Saxe Andrew M, 2013, ARXIV13126120; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sohl-Dickstein Jascha, 2015, DEEP UNSUPERVISED LE; Song Le, 2009, P 26 INT C MACHINE L, P961, DOI 10.1145/1553374.1553497; Stekhoven DJ, 2012, BIOINFORMATICS, V28, P112, DOI 10.1093/bioinformatics/btr597; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Uria B, 2014, PR MACH LEARN RES, V32; van den Oord Aaron, 2016, PIXEL RECURRENT NEUR; Vapnik V., 1998, STAT LEARNING THEORY; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang K, 2017, ARXIV170502737; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Yoon J., 2018, GAIN MISSING DATA IM; Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40	67	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905038
C	Benton, GW; Maddox, WJ; Salkey, JP; Albinati, J; Wilson, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Benton, Gregory W.; Maddox, Wesley J.; Salkey, Jayson P.; Albinati, Jujlio; Wilson, Andrew Gordon			Function-Space Distributions over Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning (FKL) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modeling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments.	[Benton, Gregory W.; Salkey, Jayson P.; Wilson, Andrew Gordon] NYU, Courant Inst Math Sci, 251 Mercer St, New York, NY 10003 USA; [Maddox, Wesley J.; Wilson, Andrew Gordon] NYU, Ctr Data Sci, New York, NY 10003 USA; [Albinati, Jujlio] Microsoft, Redmond, WA USA; [Albinati, Jujlio] AGW, Mapoon, Qld, Australia	New York University; New York University; Microsoft	Benton, GW (corresponding author), NYU, Courant Inst Math Sci, 251 Mercer St, New York, NY 10003 USA.				Amazon Research Award; Facebook Research; NSF [IIS-1563887, IIS-1910266]; NSF Graduate Research Fellowship [DGE-1650441]	Amazon Research Award; Facebook Research(Facebook Inc); NSF(National Science Foundation (NSF)); NSF Graduate Research Fellowship(National Science Foundation (NSF))	GWB, WJM, JPS, and AGW were supported by an Amazon Research Award, Facebook Research, NSF IIS-1563887, and NSF IIS-1910266. WJM was additionally supported by an NSF Graduate Research Fellowship under Grant No. DGE-1650441.	Adams Ryan P, 2009, ARXIV09124896; Alvarez M. A., 2010, P 13 INT C ART INT S, P25; Alvarez MA, 2011, J MACH LEARN RES, V12, P1459; [Anonymous], INT C LEARN REPR; Belyaev (Belayev Y.K., 1961, P 4 BERK S MATH STAT, P23; Bochner S., 1959, LECT FOURIER INTEGRA; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Cruz-Uribe D, 2002, J INEQUAL PURE APPL, V3; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Dong Kun, 2017, ARXIV171103481; Gardner JR, 2018, ADV NEUR IN, V31; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Herlands William, 2018, ARXIV181011861; HYNDMAN RJ, 2005, TIME SERIES DATA LIB; Jang Phillip A, 2017, ADV NEURAL INFORM PR, V30, P3940; Lizaro-Gredilla Miguel, 2010, J MACHINE LEARNING R, P17; MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133; Menne M J, 2015, CARBON DIOXIDE INFOR; Murray I., 2010, ADV NEURAL INFORM PR, V23, P1732, DOI DOI 10.5555/2997046.2997089; Murray lain, 2010, ARTIFICIAL INTELLIGE; Nguyen TV, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P643; Oliva JB, 2016, JMLR WORKSH CONF PRO, V51, P1078; Rakotomamonjy A., 2007, P 24 INT C MACH LEAR, V227, P775, DOI DOI 10.1145/1273496.1273594; Rasmussen C E, 2006, GAUSSIAN PROCESSES M, V2; Requeima J., 2019, 22 INT C ART INT STA, P1860; Saatci Y., 2012, THESIS U CAMBRIDGE C; Shen Z., 2019, 22 INT C ART INT STA, P3273; Tobar F., 2018, NIPS, V31, P10127; Tobar Felipe, 2015, ADV NEURAL INFORM PR, P9; Tokdar ST, 2007, J STAT PLAN INFER, V137, P34, DOI 10.1016/j.jspi.2005.09.005; WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005; Wilson A., 2013, INT C MACH LEARN, P1067; Wilson A., 2014, P ADV NEUR INF PROC, V4, P3626; Wilson A.G., 2015, ADV NEURAL INFORM PR, V28, P2854; Wilson A.G., 2014, COVARIANCE KERNELS F; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Yang ZC, 2015, JMLR WORKSH CONF PRO, V38, P1098	39	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906060
C	Bhaskara, A; Vadgama, S; Xu, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bhaskara, Aditya; Vadgama, Sharvaree; Xu, Hong			Greedy Sampling for Approximate Clustering in the Presence of Outliers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				K-MEANS; HARDNESS	Greedy algorithms such as adaptive sampling (k-means++) and furthest point traversal are popular choices for clustering problems. One the one hand, they possess good theoretical approximation guarantees, and on the other, they are fast and easy to implement. However, one main issue with these algorithms is the sensitivity to noise/outliers in the data. In this work we show that for k-means and k-center clustering, simple modifications to the well-studied greedy algorithms result in nearly identical guarantees, while additionally being robust to outliers. For instance, in the case of k-means++, we show that a simple thresholding operation on the distances suffices to obtain an O(log k) approximation to the objective. We obtain similar results for the simpler k-center problem. Finally, we show experimentally that our algorithms are easy to implement and scale well. We also measure their ability to identify noisy points added to a dataset.	[Bhaskara, Aditya; Vadgama, Sharvaree; Xu, Hong] Univ Utah, Salt Lake City, UT 84112 USA	Utah System of Higher Education; University of Utah	Bhaskara, A (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.	bhaskaraaditya@gmail.com; sharvaree.vadgama@gmail.com; hxu.hongxu@gmail.com						Aggarwal A, 2009, LECT NOTES COMPUT SC, V5687, P15, DOI 10.1007/978-3-642-03685-9_2; Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15; Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Awasthi P, 2017, J ACM, V63, DOI 10.1145/3006384; Bhaskara Aditya, 2018, ABS180410696 CORR; Charikar M, 2001, SIAM PROC S, P642; Chawla Sanjay, 2013, SDM; Chen Jiecao, 2018, ADV NEURAL INFORM PR, V31, P2248; Chen K, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P826; Dasgupta S, 2008, HARDNESS K MEANS CLU; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; Dasgupta Sanjoy, 2013, LECT NOTES GEOMETRIC; Diakonikolas Ilias, 2016, ABS160406443 CORR; Dua D., 2017, UCI MACHINE LEARNING; Friedman J., 2009, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; Gupta S, 2017, PROC VLDB ENDOW, V10, P757, DOI 10.14778/3067421.3067425; Guruswami V, 2006, ANN IEEE SYMP FOUND, P543; Huber PJ, 2009, WILEY SERIES PROBABI; Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003; Krishnaswamy R, 2018, ACM S THEORY COMPUT, P646, DOI 10.1145/3188745.3188882; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Mettu RR, 2004, MACH LEARN, V56, P35, DOI 10.1023/B:MACH.0000033114.18632.e0; Ostrovsky R, 2012, J ACM, V59, DOI 10.1145/2395116.2395117; Wei Dennis, 2016, ABS160504986 CORR	26	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902074
C	Bistritz, I; Zhou, ZY; Chen, X; Bambos, N; Blanchet, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bistritz, Ilai; Zhou, Zhengyuan; Chen, Xi; Bambos, Nicholas; Blanchet, Jose			Online EXP3 Learning in Adversarial Bandits with Delayed Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Consider a player that in each of T rounds chooses one of K arms. An adversary chooses the cost of each arm in a bounded interval, and a sequence of feedback delays {dt} that are unknown to the player. After picking arm at at round t, the player receives the cost of playing this arm dt rounds later. In cases where t + dt > T, this feedback is simply missing. We prove that the EXP3 algorithm (that uses the delayed feedback upon its arrival) achieves a regret of 0 ln K (KT +( ET I di)) " For the case where Et _1 d t and T are unknown, we propose a novel doubling trick for online learning with delays and prove that this adaptive EXP3 achieves a regret of 0 ln K (K2T +V'( z_,Tt I di)) " We then consider a two player zero -sum game where players experience asynchronous delays. We show that even when the delays are large enough such that players no longer enjoy the "no -regret property", (e.g., where dt = 0 (t log t)) the ergodic average of the strategy profile still converges to the set of Nash equilibria of the game. The result is made possible by choosing an adaptive step size lit that is not summable but is square summable, and proving a "weighted regret bound" for this general case.	[Bistritz, Ilai; Bambos, Nicholas; Blanchet, Jose] Stanford Univ, Stanford, CA 94305 USA; [Zhou, Zhengyuan; Chen, Xi] NYU, Stern Sch Business, New York, NY USA; [Zhou, Zhengyuan] IBM Res, Armonk, NY USA	Stanford University; New York University; International Business Machines (IBM)	Bistritz, I (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	bistritz@stanford.edu; zzhou@stern.nyu.edu; xchen3@stern.nyu.edu; bambos@stanford.edu; jose.blanchet@stanford.edu		Bambos, Nicholas/0000-0001-9250-4553	Koret Foundation; IBM Goldstine Fellowship; NSF [IIS-1845444]	Koret Foundation; IBM Goldstine Fellowship(International Business Machines (IBM)); NSF(National Science Foundation (NSF))	This research was supported by the Koret Foundation grant for Smart Cities and Digital Living. Zhengyuan Zhou gratefully acknowledges IBM Goldstine Fellowship. Xi Chen is supported by NSF via IIS-1845444.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; [Anonymous], 2005, THESIS; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bailey JP, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P321, DOI 10.1145/3219166.3219235; Bowling M., 2005, ADV NEURAL INFORM PR, P209; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cai Y, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P217; Cesa-Bianchi N, 2019, J MACH LEARN RES, V20; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; Joulani P., 2013, ICML, P1453; Langford J., 2009, PROC 22 INT C NEURAL, P2331; Mandel L, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL SYMPOSIUM ON PRINCIPLES AND PRACTICE OF DECLARATIVE PROGRAMMING (PPDP 2015), P6, DOI 10.1145/2790449.2790509; NEU G, 2010, ADV NEURAL INFORM PR, P1804; Pike-Burke C., 2018, P 35 INT C MACH LEAR, P4105; Quanrud K., 2015, P ADV NEUR INF PROC, P1270; Vernade C., 2017, ARXIV170609186; Weinberger MJ, 2002, IEEE T INFORM THEORY, V48, P1959, DOI 10.1109/TIT.2002.1013136; Zhou Z., 2019, ADV NEURAL INFORM PR; Zhou Z., 2017, P C NEUR INF PROC SY, P6171; Zhou ZY, 2018, PR MACH LEARN RES, V80	20	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903003
C	Blanchet, J; Glynn, PW; Yan, J; Zhou, ZQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Blanchet, Jose; Glynn, Peter W.; Yan, Jun; Zhou, Zhengqing			Multivariate Distributionally Robust Convex Regression under Absolute Error Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NONPARAMETRIC APPROACH	This paper proposes a novel non-parametric multidimensional convex regression estimator which is designed to be robust to adversarial perturbations in the empirical measure. We minimize over convex functions the maximum (over Wasserstein perturbations of the empirical measure) of the absolute regression errors. The inner maximization is solved in closed form resulting in a regularization penalty involves the norm of the gradient. We show consistency of our estimator and a rate of convergence of order (O) over tilde (n(-1/d)), matching the bounds of alternative estimators based on square-loss minimization. Contrary to all of the existing results, our convergence rates hold without imposing compactness on the underlying domain and with no a priori bounds on the underlying convex function or its gradient norm.	[Blanchet, Jose; Glynn, Peter W.] Stanford MS&E, Stanford, CA 94305 USA; [Yan, Jun] Stanford Stat, Stanford, CA USA; [Zhou, Zhengqing] Stanford Math, Stanford, CA USA		Blanchet, J (corresponding author), Stanford MS&E, Stanford, CA 94305 USA.	jose.blanchet@stanford.edu; glynn@stanford.edu; junyan65@stanford.edu; zqzhou@stanford.edu						Ait-Sahalia Y, 2003, J ECONOMETRICS, V116, P9, DOI 10.1016/S0304-4076(03)00102-7; Allon G, 2007, J APPL ECONOMET, V22, P795, DOI 10.1002/jae.918; Balazs G, 2015, JMLR WORKSH CONF PRO, V38, P56; Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043; Blanchet J., 2019, MATH OPERATIONS RES; Blanchet J., 2017, P MACH LEARN RES, V77, P97; Blanchet Jose, 2016, ARXIV E PRINTS; Embrechts P., 1997, MODELLING EXTREMAL E, DOI 10.1007/978-3-642-33483-2; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Gao Rui, 2016, ARXIV E PRINTS; Groeneboom P, 2001, ANN STAT, V29, P1653; Han Qiyang, 2016, ARXIV E PRINTS; Hannah LA, 2013, J MACH LEARN RES, V14, P3261; Lim E, 2014, INFORMS J COMPUT, V26, P616, DOI 10.1287/ijoc.2013.0587; Lim E, 2012, OPER RES, V60, P196, DOI 10.1287/opre.1110.1007; Mazumder R, 2019, J AM STAT ASSOC, V114, P318, DOI 10.1080/01621459.2017.1407771; Sinha Aman, 2017, ARXIV PREPRINT ARXIV; VARIAN HR, 1982, ECONOMETRICA, V50, P945, DOI 10.2307/1912771; VARIAN HR, 1984, ECONOMETRICA, V52, P579, DOI 10.2307/1913466	24	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903043
C	Brekelmans, R; Moyer, D; Galstyan, A; Steeg, GV		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Brekelmans, Rob; Moyer, Daniel; Galstyan, Aram; Steeg, Greg Ver			Exact Rate-Distortion in Autoencoders via Echo Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Compression is at the heart of effective representation learning. However, lossy compression is typically achieved through simple parametric models like Gaussian noise to preserve analytic tractability, and the limitations this imposes on learning are largely unexplored. Further, the Gaussian prior assumptions in models such as variational autoencoders (VAEs) provide only an upper bound on the compression rate in general. We introduce a new noise channel, Echo noise, that admits a simple, exact expression for mutual information for arbitrary input distributions. The noise is constructed in a data-driven fashion that does not require restrictive distributional assumptions. With its complex encoding mechanism and exact rate regularization, Echo leads to improved bounds on log-likelihood and dominates beta-VAEs across the achievable range of rate-distortion trade-offs. Further, we show that Echo noise can outperform flow-based methods without the need to train additional distributional transformations.	[Brekelmans, Rob; Moyer, Daniel; Galstyan, Aram; Steeg, Greg Ver] Univ Southern Calif, Informat Sci Inst, Marina Del Rey, CA 90292 USA	University of Southern California	Brekelmans, R (corresponding author), Univ Southern Calif, Informat Sci Inst, Marina Del Rey, CA 90292 USA.	brekelma@isi.edu; moyerd@usc.edu; galstyan@isi.edu; gregv@isi.edu	Thompson, Paul M/C-4194-2018	Thompson, Paul M/0000-0002-4720-8867; Galstyan, Aram/0000-0003-4215-0886; Ver Steeg, Greg/0000-0002-0793-141X				Achille  Alessandro, 2016, ARXIV161101353; Alemi AA, 2018, PR MACH LEARN RES, V80; [Anonymous], 2016, P WORKSH ADV APPR BA; Belghazi I., 2018, ARXIV180104062; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Braithwaite D., 2018, ARXIV180707306; Burda Yuri, 2015, ARXIV150900519; Burgess CP, 2018, ARXIV180403599; Chen T.Q., 2018, NEURIPS, P2610; Chen Xi, 2016, ARXIV161102731; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Gao Shuyang, 2019, AISTATS; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gretton A, 2012, J MACH LEARN RES, V13, P723; Griffith V, 2014, EMERGENCE COMPLEX CO, V9, P159, DOI 10.1007/978-3-642-53734-9_6; Higgins I., 2017, P INT C LEARN REPR T; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kolchinsky A., 2017, ARXIV170502436; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lastras-Montano Luis A, 2018, INFORM THEORETIC LOW; Locatello F., 2019, INT C ADV MACH LEARN, P4114; Mathieu E, 2019, PR MACH LEARN RES, V97; McAllester David, 2018, ARXIV181104251; Papamakarios George, 2017, ARXIV170507057; Phuong M., 2018, MUTUAL AUTOENCODER C; Poole B, 2019, PR MACH LEARN RES, V97; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rezende Danilo Jimenez, 2018, ARXIV181000597; Rosca M., 2018, ARXIV180206847; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Tishby Naftali, 2000, PHYSICS0004057 ARXIV; Tomczak Jakub M, 2017, AISTATS 2018; Tschannen Michael, 2018, ARXIV181205069; vanSteenkiste Sjoerd, 2019, ARXIV190512506; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066; Xiao H., 2017, FASHION MNIST NOVEL; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490; Zhao Shengjia, 2018, ABS180606514 CORR	46	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303083
C	Brendel, W; Rauber, J; Ktimmerer, M; Ustyuzhaninov, I; Bethge, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Brendel, Wieland; Rauber, Jonas; Ktimmerer, Matthias; Ustyuzhaninov, Ivan; Bethge, Matthias			Accurate, reliable and fast robustness evaluation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Throughout the past five years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is significantly impaired by the difficulty of evaluating the robustness of neural network models. Today's methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efficient than current state-of-the-art gradient-based attacks, (c) can be flexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These findings are carefully validated across a diverse set of six different models and hold for L-0, L-1, L-2 and L, in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.	[Brendel, Wieland; Rauber, Jonas; Ktimmerer, Matthias; Ustyuzhaninov, Ivan; Bethge, Matthias] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany; [Rauber, Jonas; Ktimmerer, Matthias; Ustyuzhaninov, Ivan] Int Max Planck Res Sch Intelligent Syst, Tubingen, Germany; [Brendel, Wieland; Rauber, Jonas; Ktimmerer, Matthias; Ustyuzhaninov, Ivan; Bethge, Matthias] Bernstein Ctr Computat Neurosci Tubingen, Tubingen, Germany; [Bethge, Matthias] Max Planck Inst Biol Cybernet, Tubingen, Germany	Eberhard Karls University of Tubingen; Max Planck Society	Brendel, W (corresponding author), Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.; Brendel, W (corresponding author), Bernstein Ctr Computat Neurosci Tubingen, Tubingen, Germany.	wieland.brendel@bethgelab.org			German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tibingen [FKZ: 01GQ1002]; German Research Foundation [DFG CRC 1233]; Tubingen Al Center [FKZ: 01IS 18039A]; International Max Planck Research School for Intelligent Systems (IMPRS-IS); Bosch Forschungsstiftung (Stifterverband) [T 113/30057/17]; Centre for Integrative Neuroscience Tibingen [EXC 307]; Intelligence Advanced Research Projects Activity (JARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PCO0003]	German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tibingen(Federal Ministry of Education & Research (BMBF)); German Research Foundation(German Research Foundation (DFG)); Tubingen Al Center; International Max Planck Research School for Intelligent Systems (IMPRS-IS); Bosch Forschungsstiftung (Stifterverband); Centre for Integrative Neuroscience Tibingen; Intelligence Advanced Research Projects Activity (JARPA) via Department of Interior/Interior Business Center (DoI/IBC)	This work has been funded, in part, by the German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tibingen (FKZ: 01GQ1002) as well as the German Research Foundation (DFG CRC 1233 on "Robust Vision") and the Tubingen Al Center (FKZ: 01IS 18039A). The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting J.R., M.K. and I.U.; J.R. acknowledges support by the Bosch Forschungsstiftung (Stifterverband, T 113/30057/17); M.B. acknowledges support by the Centre for Integrative Neuroscience Tibingen (EXC 307); W.B. and M.B. were supported by the Intelligence Advanced Research Projects Activity (JARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PCO0003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of LARPA, DoI/IBC, or the U.S. Government.	Athalye A., 2018, CORR; Brendel Wieland, 2018, INT C LEARN REPR; Carlini N., 2016, CORR; Chen P.-Y., 2017, ARXIV170904114; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Domahidi A, 2013, 2013 EUROPEAN CONTROL CONFERENCE (ECC), P3077; Dong Y., 2018, P IEEE C COMP VIS PA; Engstrom Logan, 2018, CORR; Goodfellow I. J., 2015, P ICLR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kannan Harini, 2018, CORR; Kolter J. Z., 2017, CORR; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Madry A., 2018, INT C LEARN REPR ICL; Modas Apostolos, 2019, IEEE C COMP VIS PATT; Moosavi-Dezfooli Seyed-Mohsen, 2016, IEEE C COMP VIS PATT; O'Donoghue B, 2016, J OPTIMIZ THEORY APP, V169, P1042, DOI 10.1007/s10957-016-0892-3; Papernot N., 2015, CORR; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Rauber Jonas, 2017, CORR; Rony J, 2018, ARXIV181109600; Schott Lukas, 2019, INT C LEARN REPR; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; van den Oord Aaron, 2018, ARXIV180205666; Wang S., 2018, ARXIV181102625; Zheng Tianhang, 2018, CORR	26	3	3	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904049
C	Bun, M; Kamath, G; Steinke, T; Wu, ZS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bun, Mark; Kamath, Gautam; Steinke, Thomas; Wu, Zhiwei Steven			Private Hypothesis Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE; ESTIMATORS; RATES	We provide a differentially private algorithm for hypothesis selection. Given samples from an unknown probability distribution P and a set of m probability distributions H, the goal is to output, in a epsilon-differentially private manner, a distribution from H whose total variation distance to P is comparable to that of the best such distribution (which we denote by alpha). The sample complexity of our basic algorithm is O(log m/alpha(2) + log m/alpha epsilon), representing a minimal cost for privacy when compared to the non-private algorithm. We also can handle infinite hypothesis classes H by relaxing to (epsilon, delta)-differential privacy. We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes. Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class. As the covering and packing numbers are often closely related, for constant alpha, our algorithms achieve the optimal sample complexity for many classes of interest. Finally, we describe an application to private distribution-free PAC learning.	[Bun, Mark] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA; [Kamath, Gautam] Univ Waterloo, Cheriton Sch Comp Sci, Waterloo, ON, Canada; [Steinke, Thomas] IBM Res, Yorktown Hts, NY USA; [Wu, Zhiwei Steven] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	Boston University; University of Waterloo; International Business Machines (IBM); University of Minnesota System; University of Minnesota Twin Cities	Bun, M (corresponding author), Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.	mbun@bu.edu; g@csail.mit.edu; phs@thomas-steinke.net; zsw@umn.edu		Wu, Steven/0000-0002-8125-8227	Google Research Fellowship, Simons-Berkeley Research Fellowship program; Microsoft Research Fellowship, Simons-Berkeley Research Fellowship program; Patrick J. McGovern Research Fellowship, Simons-Berkeley Research Fellowship program; Google Faculty Research Award; J.P. Morgan Faculty Award; Facebook Research Award	Google Research Fellowship, Simons-Berkeley Research Fellowship program(Google Incorporated); Microsoft Research Fellowship, Simons-Berkeley Research Fellowship program; Patrick J. McGovern Research Fellowship, Simons-Berkeley Research Fellowship program; Google Faculty Research Award(Google Incorporated); J.P. Morgan Faculty Award; Facebook Research Award(Facebook Inc)	The authors would like to thank Shay Moran for bringing to their attention the application to PAC learning mentioned in the supplement, Jonathan Ullman for asking questions which motivated Remark 1, and Clement Canonne for assistance in reducing the constant factor in the approximation guarantee. This work was done while the authors were all affiliated the Simons Institute for the Theory of Computing. MB was supported by a Google Research Fellowship, as part of the Simons-Berkeley Research Fellowship program. GK was supported by a Microsoft Research Fellowship, as part of the Simons-Berkeley Research Fellowship program, and the work was also partially done while visiting Microsoft Research, Redmond. TS was supported by a Patrick J. McGovern Research Fellowship, as part of the Simons-Berkeley Research Fellowship program. ZSW was supported in part by a Google Faculty Research Award, a J.P. Morgan Faculty Award, and a Facebook Research Award.	Acharya J., 2018, P 35 INT C MACH LEAR, P30; Acharya J, 2014, IEEE INT SYMP INFO, P1682, DOI 10.1109/ISIT.2014.6875120; Acharya Jayadev, 2019, 22 INT C ARTIFICIAL, V89, P1120; Acharya Jayadev, 2018, J MACHINE LEARNING R, V19, P2427; [Anonymous], 2018, ARXIV181108382; ANTHONY M, 1995, DISCRETE APPL MATH, V61, P91, DOI 10.1016/0166-218X(94)00008-2; Blum A., 2005, P 24 ACM SIGMOD SIGA, P128, DOI DOI 10.1145/1065167.1065184; Bousquet Olivier, 2019, P 32 ANN C LEARN THE, P318; Bun M, 2018, ACM S THEORY COMPUT, P74, DOI 10.1145/3188745.3188946; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Bun M, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1, DOI 10.1145/2591796.2591877; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Bun Mark, 2019, ARXIV190513229; Dajani A. N., 2017, SEPT 2017 M CENS SCI; Daskalakis C, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P709; Daskalakis Constantinos, 2014, P 27 C LEARN THEOR C, P1183; Devroye L, 1996, ANN STAT, V24, P2499; Devroye L, 1997, ANN STAT, V25, P2626; Devroye L., 2001, SPRINGER SERIES STAT; Diakonikolas I., 2015, ADV NEURAL INFORM PR, P2566; Duchi J. C, 2018, ARXIV180605756; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C., 2016, ARXIV PREPRINT ARXIV; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2009, ACM S THEORY COMPUT, P371; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Gaboardi Marco, 2019, P 22 INT C ART INT S, P2545; Hardt M, 2010, ACM S THEORY COMPUT, P705; Kairouz P, 2016, PR MACH LEARN RES, V48; Kamath G., 2019, P C LEARN THEOR, P1853; Kamath Gautam, 2019, ADV NEURAL INFORM PR; Karwa V., 2018, P 9 INN THEOR COMP S, V94, p44:1; Mahalanabis S, 2008, P 21 ANN C LEARN THE, P503; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Smith A, 2011, ACM S THEORY COMPUT, P813; Steinke T., 2016, J PRIV CONFID, V7, P3; Suresh A. T., 2014, ADV NEURAL INFORM PR, V27, P1395; Thakurta Abhradeep Guha, 2013, P 26 ANN C LEARN THE, P819; Ullman, 2015, COLT, P1588; Wang S., 2016, ARXIV160708025; Wang Y, 2019, ARXIV190204495; YATRACOS YG, 1985, ANN STAT, V13, P768, DOI 10.1214/aos/1176349553; Ye M, 2018, IEEE T INFORM THEORY, V64, P5662, DOI 10.1109/TIT.2018.2809790	46	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300015
C	Campbell, T; Beronov, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Campbell, Trevor; Beronov, Boyan			Sparse Variational Inference: Bayesian Coresets from Scratch	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The proliferation of automated inference algorithms in Bayesian statistics has provided practitioners newfound access to fast, reproducible data analysis and powerful statistical models. Designing automated methods that are also both computationally scalable and theoretically sound, however, remains a significant challenge. Recent work on Bayesian coresets takes the approach of compressing the dataset before running a standard inference algorithm, providing both scalability and guarantees on posterior approximation error. But the automation of past coreset methods is limited because they depend on the availability of a reasonable coarse posterior approximation, which is difficult to specify in practice. In the present work we remove this requirement by formulating coreset construction as sparsity-constrained variational inference within an exponential family. This perspective leads to a novel construction via greedy optimization, and also provides a unifying information-geometric view of present and past methods. The proposed Riemannian coreset construction algorithm is fully automated, requiring no problem-specific inputs aside from the probabilistic model and dataset. In addition to being significantly easier to use than past methods, experiments demonstrate that past coreset constructions are fundamentally limited by the fixed coarse posterior approximation; in contrast, the proposed algorithm is able to continually improve the coreset, providing state-of-the-art Bayesian dataset summarization with orders-of-magnitude reduction in KL divergence to the exact posterior.	[Campbell, Trevor] Univ British Columbia, Dept Stat, Vancouver, BC V6T 1Z4, Canada; [Beronov, Boyan] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada	University of British Columbia; University of British Columbia	Campbell, T (corresponding author), Univ British Columbia, Dept Stat, Vancouver, BC V6T 1Z4, Canada.	trevor@stat.ubc.ca; beronov@cs.ubc.ca			National Sciences and Engineering Research Council of Canada (NSERC); NSERC Discovery Launch Supplement	National Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); NSERC Discovery Launch Supplement	T. Campbell and B. Beronov are supported by National Sciences and Engineering Research Council of Canada (NSERC) Discovery Grants. T. Campbell is additionally supported by an NSERC Discovery Launch Supplement.	Agarwal P, 2005, COMBINATORIAL COMPUT, P1; Ahfock D., 2017, ARXIV170603665; Ahn S., 2012, P 29 INT C MACH LEAR; Alquier Pierre, 2018, ANN STAT; Amari Shun ichi, 2016, INFORM GEOMETRY ITS; Bachem Olivier, 2017, ARXIV170306476; Bardenet R, 2017, J MACH LEARN RES, V18, P1; Bardenet R, 2014, PR MACH LEARN RES, V32; Baydin AG, 2018, J MACH LEARN RES, V18; Betancourt Michael, 2015, FUNDAMENTAL INCOMPAT; Boche H, 2015, APPL NUMER HARMON AN, P1, DOI 10.1007/978-3-319-16042-9_1; Braverman V., 2016, ARXIV161200889; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Campbell T., 2015, ADV NEURAL INFORM PR, P280; Campbell T, 2018, PR MACH LEARN RES, V80; Campbell T, 2019, J MACH LEARN RES, V20; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Chen Yutian, 2010, UAI; Cherief-Abdellatif BE, 2018, ELECTRON J STAT, V12, P2995, DOI 10.1214/18-EJS1475; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Entezari Reihaneh, 2016, ARXIV160502113; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Feldman D, 2011, ACM S THEORY COMPUT, P569; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gelman A., 2021, BAYESIAN DATA ANAL, V3rd ed.; Geppert LN, 2017, STAT COMPUT, V27, P79, DOI 10.1007/s11222-015-9608-z; GUELAT J, 1986, MATH PROGRAM, V35, P110, DOI 10.1007/BF01589445; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Huggins Jonathan, 2016, ADV NEURAL INFORM PR; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Korattikara A, 2014, PR MACH LEARN RES, V32; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Langberg M, 2010, PROC APPL MATH, V135, P598; Locatello F, 2017, ADV NEUR IN, V30; Maclaurin D, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P543; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2; Rabinovich M., 2015, ADV NEURAL INFORM PR, P1207; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Scott SL, 2016, INT J MANAG SCI ENG, V11, P78, DOI 10.1080/17509653.2016.1142191; Srivastava S, 2015, JMLR WORKSH CONF PRO, V38, P912; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; TIERNEY L, 1986, J AM STAT ASSOC, V81, P82, DOI 10.2307/2287970; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang Y, 2019, J LOW FREQ NOISE V A, V38, P1008, DOI 10.1177/1461348418795813; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Yang Yun, 2018, ANN STAT	62	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903013
C	Cardoso, AR; Wang, H; Xu, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cardoso, Adrian Rivera; Wang, He; Xu, Huan			Large Scale Markov Decision Processes with Changing Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider Markov Decision Processes (MDPs) where the rewards are unknown and may change in an adversarial manner. We provide an algorithm that achieves a regret bound of O(root tau(ln vertical bar S vertical bar + ln vertical bar A vertical bar)T ln(T)), where S is the state space, A is the action space, tau is the mixing time of the MDP, and T is the number of periods. The algorithm's computational complexity is polynomial in vertical bar S vertical bar and vertical bar A vertical bar. We then consider a setting often encountered in practice, where the state space of the MDP is too large to allow for exact solutions. By approximating the state-action occupancy measures with a linear architecture of dimension d << vertical bar S vertical bar, we propose a modified algorithm with a computational complexity polynomial in d and independent of vertical bar S vertical bar. We also prove a regret bound for this modified algorithm, which to the best of our knowledge, is the first (O) over tilde(root T) regret bound in the large-scale MDP setting with adversarially changing rewards.	[Cardoso, Adrian Rivera; Wang, He] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; [Xu, Huan] Alibaba Grp, Hangzhou, Peoples R China	University System of Georgia; Georgia Institute of Technology; Alibaba Group	Cardoso, AR (corresponding author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	adrian.riv@gatech.edu; he.wang@isye.gatech.edu; huan.xu@alibaba-inc.com	Wang, He/R-1290-2019					Abbasi- Yadkori Y., 2019, ARXIV190101992; Abbasi-Yadkori Y, 2014, PR MACH LEARN RES, V32, P496; Abernethy J. D., 2009, C LEARN THEOR; [Anonymous], [No title captured]; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Ben-Tal A, 2015, OPER RES, V63, P628, DOI 10.1287/opre.2015.1374; Bertsekas D. P., 2012, APPROXIMATE DYNAMIC, V2; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chatterjee K, 2007, LECT NOTES COMPUT SC, V4855, P473; Chen Y., 2018, INT C MACH LEARN, P833; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; De Farias DP, 2003, OPER RES, V51, P850, DOI 10.1287/opre.51.6.850.24925; Dick T, 2014, PR MACH LEARN RES, V32; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; EVENDAR E, 2005, ADV NEURAL INFORM PR, V18; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Gajane P., 2018, ARXIV180510066; Hazan E, 2012, J MACH LEARN RES, V13, P2903; Helmbold DP, 1998, MATH FINANC, V8, P325, DOI 10.1111/1467-9965.00058; Howard Ronald A., 1960, DYNAMIC PROGRAMMING; Junges S, 2016, LECT NOTES COMPUT SC, V9636, P130, DOI 10.1007/978-3-662-49674-9_8; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kalai A., 2002, J MACHINE LEARNING R, V3, P423; Koutsoupias E, 2009, COMPUT SCI REV, V3, P105, DOI 10.1016/j.cosrev.2009.04.002; Kretinsky J., 2018, ARXIV180408924; Ma Y., 2015, ARXIV151004454; Neu G, 2014, IEEE T AUTOMAT CONTR, V59, P676, DOI 10.1109/TAC.2013.2292137; Puterman M.L., 2014, MARKOV DECISION PROC; Schrijver A., 1998, THEORY LINEAR INTEGE; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328; Wang M., 2017, ARXIV PREPRINT ARXIV; Weber R., 1992, ANN APPL PROBAB, V2, P1024, DOI 10.1214/aoap/1177005588; WHITTLE P, 1980, J ROY STAT SOC B MET, V42, P143; Yu JY, 2009, 2009 INTERNATIONAL CONFERENCE ON GAME THEORY FOR NETWORKS (GAMENETS 2009), P314, DOI 10.1109/GAMENETS.2009.5137416; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	45	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302035
C	Carion, N; Synnaeve, G; Lazaric, A; Usunier, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Carion, Nicolas; Synnaeve, Gabriel; Lazaric, Alessandro; Usunier, Nicolas			A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Effective coordination is crucial to solve multi-agent collaborative (MAC) problems. While centralized reinforcement learning methods can optimally solve small MAC instances, they do not scale to large problems and they fail to generalize to scenarios different from those seen during training. In this paper, we consider MAC problems with some intrinsic notion of locality (e.g., geographic proximity) such that interactions between agents and tasks are locally limited. By leveraging this property, we introduce a novel structured prediction approach to assign agents to tasks. At each step, the assignment is obtained by solving a centralized optimization problem (the inference procedure) whose objective function is parameterized by a learned scoring model. We propose different combinations of inference procedures and scoring models able to represent coordination patterns of increasing complexity. The resulting assignment policy can be efficiently learned on small problem instances and readily reused in problems with more agents and tasks (i.e., zero-shot generalization). We report experimental results on a toy search and rescue problem and on several target selection scenarios in StarCraft: Brood War(1), in which our model significantly outperforms strong rule-based baselines on instances with 5 times more agents and tasks than those seen during training.	[Carion, Nicolas; Lazaric, Alessandro; Usunier, Nicolas] Facebook, Paris, France; [Carion, Nicolas] Univ Paris 09, Lamsade, Paris, France; [Synnaeve, Gabriel] Facebook, Nyc, NY USA	Facebook Inc; UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; Facebook Inc	Carion, N (corresponding author), Facebook, Paris, France.; Carion, N (corresponding author), Univ Paris 09, Lamsade, Paris, France.	alcinos@fb.com; gab@fb.com; lazaric@fb.com; usunier@fb.com						[Anonymous], 2003, IJCAI; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Churchill D., 2012, P 8 AAAI C ART INT I, P112; Churchill D., 2013, P C COMPUTATIONAL IN, P1; Churchill David, 2017, ANAL MODEL BASED HEY; De Witt Christian Schroeder, 2018, ARXIV180311485; Diuk C., 2008, ICML, DOI [10.1145/1390156.1390187, DOI 10.1145/1390156.1390187]; Foerster JN, 2016, ADV NEUR IN, V29; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Guestrin C., 2002, ICML, P227; Hunt J.J., CONTINUOUS CONTROL D; Jiang J, 2018, ARXIV181009202, V2; Jiang JC, 2018, ADV NEUR IN, V31; Lin KX, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1774, DOI 10.1145/3219819.3219993; Lowe R., 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295385; Meuleau N, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P165; Mnih V, 2016, PR MACH LEARN RES, V48; Ontanon S, 2013, IEEE T COMP INTEL AI, V5, P293, DOI 10.1109/TCIAIG.2013.2286295; Proper S, 2009, P 8 INT C AUT AG MUL, V1, P681; Sanner Scott, 2012, ARXIV12066879; Singh A., 2019, ICLR; Singh S, 1998, ADV NEUR IN, V10, P1057; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tesauro G., 2005, P ASS ADVANCEMENT AR, V5, P886; Usunier N., 2017, P INT C LEARN REPR I; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wawrzynski Pawel, 2015, International Journal of Machine Learning and Computing, V5, P91, DOI 10.7763/IJMLC.2015.V5.489; Yang Yaodong, 2018, ARXIV PREPRINT ARXIV, P5571, DOI DOI 10.1115/FEDSM2018-83038; Zambaldi Vinicius, 2018, ARXIV180601830	31	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308018
C	Charpiat, G; Girard, N; Felardos, L; Tarabalka, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Charpiat, Guillaume; Girard, Nicolas; Felardos, Loris; Tarabalka, Yuliya			Input Similarity from the Neural Network Perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RIEMANNIAN METRICS	Given a trained neural network, we aim at understanding how similar it considers any two samples. For this, we express a proper definition of similarity from the neural network perspective (i.e. we quantify how undissociable two inputs A and B are), by taking a machine learning viewpoint: how much a parameter variation designed to change the output for A would impact the output for B as well? We study the mathematical properties of this similarity measure, and show how to estimate sample density with it, in low complexity, enabling new types of statistical analysis for neural networks. We also propose to use it during training, to enforce that examples known to be similar should also be seen as similar by the network. We then study the self-denoising phenomenon encountered in regression tasks when training neural networks on datasets with noisy labels. We exhibit a multimodal image registration task where almost perfect accuracy is reached, far beyond label noise variance. Such an impressive self-denoising phenomenon can be explained as a noise averaging effect over the labels of similar examples. We analyze data by retrieving samples perceived as similar by the network, and are able to quantify the denoising effect without requiring true labels.	[Charpiat, Guillaume; Felardos, Loris] Univ Paris Sud, LRI, INRIA, TAU Team, Paris, France; [Girard, Nicolas; Tarabalka, Yuliya] Univ Cote Azur, INRIA Sophia Antipolis, TITANE Team, Nice, France; [Tarabalka, Yuliya] LuxCarta Technol, Valbonne, France	Inria; UDICE-French Research Universities; Universite Paris Saclay; UDICE-French Research Universities; Universite Cote d'Azur	Charpiat, G (corresponding author), Univ Paris Sud, LRI, INRIA, TAU Team, Paris, France.	guillaume.charpiat@inria.fr; nicolas.girard@inria.fr; loris.felardos@inria.fr; yuliya.tarabalka@inria.fr			French National Research Agency (ANR) [EPITOME ANR-17-CE23-0009]	French National Research Agency (ANR)(French National Research Agency (ANR))	We thank Victor Berger and Adrien Bousseau for useful discussions. This work benefited from the support of the project EPITOME ANR-17-CE23-0009 of the French National Research Agency (ANR).	Chizat L., 2018, NOTE LAZY TRAINING S; Cohen TS, 2016, PR MACH LEARN RES, V48; Drucker H., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P145, DOI 10.1109/IJCNN.1991.155328; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gatys LeonA., 2015, ARXIV, DOI 10.1167/16.12.326; Girard Nicolas, 2019, IGARSS; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Hochreiter S., 1995, Advances in Neural Information Processing Systems 7, P529; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Lehtinen J, 2018, PR MACH LEARN RES, V80; Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211; Maggiori E., 2017, IGARSS; Mnih V., 2012, P 29 INT C MACHINE L, P567, DOI DOI 10.5555/3042573.3042603; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Ollivier Y, 2015, INF INFERENCE, V4, P154, DOI 10.1093/imaiai/iav007; Ollivier Y, 2015, INF INFERENCE, V4, P108, DOI 10.1093/imaiai/iav006; Rifai S., 2011, PROC INT C MACH LEAR; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Sukhbaatar Sainbayar, 2014, ICLR	20	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305035
C	Chaudhuri, R; Fiete, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chaudhuri, Rishidev; Fiete, Ila			Bipartite expander Hopfield networks as self-decoding high-capacity error correcting codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEURAL NETWORKS; INFORMATION CAPACITY; SPIN-GLASS; PROPAGATION; MODELS; GRAPHS; MEMORY	Neural network models of memory and error correction famously include the Hopfield network, which can directly store-and error-correct through its dynamicsarbitrary N-bit patterns, but only for similar to N such patterns. On the other end of the spectrum, Shannon's coding theory established that it is possible to represent exponentially many states (similar to e(N)) using N symbols in such a way that an optimal decoder could correct all noise upto a threshold. We prove that it is possible to construct an associative content-addressable network that combines the properties of strong error correcting codes and Hopfield networks: it simultaneously possesses exponentially many stable states, these states are robust enough, with large enough basins of attraction that they can be correctly recovered despite errors in a finite fraction of all nodes, and the errors are intrinsically corrected by the network's own dynamics. The network is a two-layer Boltzmann machine with simple neural dynamics, low dynamic-range (binary) pairwise synaptic connections, and sparse expander graph connectivity. Thus, quasi-random sparse structures-characteristic of important error-correcting codes-may provide for high-performance computation in artificial neural networks and the brain.	[Chaudhuri, Rishidev] Univ Calif Davis, Ctr Neurosci, Dept Math & Neurobiol Physiol & Behav, Davis, CA 95616 USA; [Fiete, Ila] MIT, Brain & Cognit Sci, Cambridge, MA 02139 USA	University of California System; University of California Davis; Massachusetts Institute of Technology (MIT)	Chaudhuri, R (corresponding author), Univ Calif Davis, Ctr Neurosci, Dept Math & Neurobiol Physiol & Behav, Davis, CA 95616 USA.	rchaudhuri@ucdavis.edu; fiete@mit.edu		Fiete, Ila/0000-0003-4738-2539; Chaudhuri, Rishidev/0000-0003-2301-3261	Simons Foundation; ONR under a YIP award; Google Fellowship	Simons Foundation; ONR under a YIP award; Google Fellowship(Google Incorporated)	We are indebted to Peter Latham for extensive and thought-provoking comments and discussion, as well as suggestions for improving the exposition of our results. We are grateful to Yoram Burak, David Schwab, and Ngoc Tran for many helpful discussions on early parts of this work, and to Yoram Burak and Christopher Hillar for comments on the manuscript. IF is an HHMI Faculty Scholar, a CIFAR Senior Fellow, and acknowledges funding from the Simons Foundation and the ONR under a YIP award. Part of this work was performed by RC and IF in residence at the Simons Institute for the Theory of Computing at Berkeley, where RC was supported by a Google Fellowship.	ABUMOSTAFA YS, 1985, IEEE T INFORM THEORY, V31, P461, DOI 10.1109/TIT.1985.1057069; Alemi Alireza, 2017, ARXIV170507441; Barlow H.B., 1961, SENSORY COMMUNICATIO, V1; BERLEKAMP ER, 1978, IEEE T INFORM THEORY, V24, P384, DOI 10.1109/TIT.1978.1055873; BINDER K, 1986, REV MOD PHYS, V58, P801, DOI 10.1103/RevModPhys.58.801; Buzsaki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Fiete I., 2014, ARXIV14076029; GARDNER E, 1988, J PHYS A-MATH GEN, V21, P271, DOI 10.1088/0305-4470/21/1/031; GROSSBERG S, 1988, NEURAL NETWORKS, V1, P17, DOI 10.1016/0893-6080(88)90021-4; HAMMING RW, 1950, BELL SYST TECH J, V29, P147, DOI 10.1002/j.1538-7305.1950.tb00463.x; Hillar C, 2014, IEEE IMAGE PROC, P4092, DOI 10.1109/ICIP.2014.7025831; Hillar CJ, 2018, J MATH NEUROSCI, V8, DOI 10.1186/s13408-017-0056-2; Hinton G. E., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P448; Hoory S, 2006, B AM MATH SOC, V43, P439, DOI 10.1090/S0273-0979-06-01126-8; HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141; HOPFIELD JJ, 1986, SCIENCE, V233, P625, DOI 10.1126/science.3755256; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; KIRKPATRICK S, 1978, PHYS REV B, V17, P4384, DOI 10.1103/PhysRevB.17.4384; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; Kuncheva L I, 2004, COMBINING PATTERN CL; LARSEN KG, 2016, FOCS, P61, DOI DOI 10.1109/FOCS.2016.16; LITTLE W A, 1974, Mathematical Biosciences, V19, P101, DOI 10.1016/0025-5564(74)90031-5; Lubotzky A, 2012, B AM MATH SOC, V49, P113, DOI 10.1090/S0273-0979-2011-01359-3; Luby MG, 2001, IEEE T INFORM THEORY, V47, P585, DOI 10.1109/18.910576; McDonald D., 2004, PRACTICAL HAZOPS TRI; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; PALM G, 1992, NETWORK-COMP NEURAL, V3, P177, DOI 10.1088/0954-898X/3/2/006; Rich PD, 2014, SCIENCE, V345, P814, DOI 10.1126/science.1255635; Salavati AH, 2014, IEEE T NEUR NET LEAR, V25, P557, DOI 10.1109/TNNLS.2013.2277608; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Sipser M, 1996, IEEE T INFORM THEORY, V42, P1710, DOI 10.1109/18.556667; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Sourlas N, 2001, PHYSICA A, V302, P14, DOI 10.1016/S0378-4371(01)00439-3; TANAKA F, 1980, J PHYS F MET PHYS, V10, P2769, DOI 10.1088/0305-4608/10/12/017; TANK DW, 1986, IEEE T CIRCUITS SYST, V33, P533, DOI 10.1109/TCS.1986.1085953; TREVES A, 1991, NETWORK-COMP NEURAL, V2, P371, DOI 10.1088/0954-898X/2/4/004; TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002; Vinje WE, 2000, SCIENCE, V287, P1273, DOI 10.1126/science.287.5456.1273; Ziv Y, 2013, NAT NEUROSCI, V16, P264, DOI 10.1038/nn.3329	45	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307068
C	Chen, SY; Wang, WY; Pan, SJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Shangyu; Wang, Wenya; Pan, Sinno Jialin			MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Tremendous amount of parameters make deep neural networks impractical to be deployed for edge-device-based real-world applications due to the limit of computational power and storage space. Existing studies have made progress on learning quantized deep models to reduce model size and energy consumption, i.e. converting full-precision weights (r's) into discrete values (q's) in a supervised training manner. However, the training process for quantization is non-differentiable, which leads to either infinite or zero gradients (gr) w.r.t. r. To address this problem, most training-based quantization methods use the gradient w.r.t. q (gq) with clipping to approximate gr by Straight-Through-Estimator (STE) or manually design their computation. However, these methods only heuristically make training-based quantization applicable, without further analysis on how the approximated gradients can assist training of a quantized network. In this paper, we propose to learn gr by a neural network. Specifically, a meta network is trained using gq and r as inputs, and outputs gr for subsequent weight updates. The meta network is updated together with the original quantized network. Our proposed method alleviates the problem of non-differentiability, and can be trained in an end-to-end manner. Extensive experiments are conducted with CIFAR10/100 and ImageNet on various deep networks to demonstrate the advantage of our proposed method in terms of a faster convergence rate and better performance. Codes are released at: https://github.com/csyhhu/MetaQuant	[Chen, Shangyu; Wang, Wenya; Pan, Sinno Jialin] Nanyang Technol Univ, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Chen, SY (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	schen025@e.ntu.edu.sg; wangwy@ntu.edu.sg; sinnopan@ntu.edu.sg			NTU Singapore Nanyang Assistant Professorship (NAP) grant [M4081532.020]; Singapore MOE AcRF Tier-2 grant [MOE2016-T2-2-06]	NTU Singapore Nanyang Assistant Professorship (NAP) grant(Nanyang Technological University); Singapore MOE AcRF Tier-2 grant(Ministry of Education, Singapore)	This work is supported by NTU Singapore Nanyang Assistant Professorship (NAP) grant M4081532.020, and Singapore MOE AcRF Tier-2 grant MOE2016-T2-2-06.	Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2017, CVPR; Bai Y., 2019, INT C LEARN REPR; Courbariaux M., 2015, ADV NEUR IN, P3123; Hou L., 2017, INT C LEARN REPR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hubara I, 2016, ADV NEUR IN, V29; Kingma D.P., 2015, INT C LEARN REPR ICL; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Leng Cong, 2017, AAAI C ART INT; Liu, 2016, ADV NEURAL INFORM PR; Liu Z., 2018, P EUR C COMP VIS ECC, P722; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Zhou AJ, 2018, PROC CVPR IEEE, P9426, DOI 10.1109/CVPR.2018.00982; Zhou Aojun, 2017, INT C LEARN REPR; Zhou Shuchang, 2016, P IEEE C COMP VIS PA; Zhu Chenzhuo, 2016, INT C LEARN REPR	18	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303086
C	Cortes, C; Mohri, M; Storcheus, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cortes, Corinna; Mohri, Mehryar; Storcheus, Dmitry			Regularized Gradient Boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Gradient Boosting ( GB) is a popular and very successful ensemble method for binary trees. While various types of regularization of the base predictors are used with this algorithm, the theory that connects such regularizations with generalization guarantees is poorly understood. We fill this gap by deriving data-dependent learning guarantees for GB used with regularization, expressed in terms of the Rademacher complexities of the constrained families of base predictors. We introduce a new algorithm, called RGB, that directly benefits from these generalization bounds and that, at every boosting round, applies the Structural Risk Minimization principle to search for a base predictor with the best empirical fit versus complexity trade-off. Inspired by Randomized Coordinate Descent we provide a scalable implementation of our algorithm, able to search over large families of base predictors. Finally, we provide experimental results, demonstrating that our algorithm achieves significantly better out-of-sample performance on multiple datasets than the standard GB algorithm used with its regularization.	[Cortes, Corinna] Google Res, New York, NY 10011 USA; [Mohri, Mehryar; Storcheus, Dmitry] Google, New York, NY 10012 USA; [Mohri, Mehryar; Storcheus, Dmitry] Courant Inst, New York, NY 10012 USA	Google Incorporated; Google Incorporated	Cortes, C (corresponding author), Google Res, New York, NY 10011 USA.	corinna@google.com; mohri@google.com; dstorcheus@google.com			NSF [CCF-1535987, IIS-1618662]; Google Research Award	NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated)	We thank our colleagues Natalia Ponomareva and Vitaly Kuznetsov for insightful discussions and feedback. This work was partly supported by NSF CCF-1535987, NSF IIS-1618662, and a Google Research Award.	Caruana Rich, 2004, ICML, DOI DOI 10.1145/1015330.1015432; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cortes C., 2014, P ICML; Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J., 1998, ANN STAT, V28, P2000; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2; Koltchinskii V, 2002, ANN STAT, V30, P1; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lu H., 2018, ARXIV181010158; Mason L, 2000, ADV NEUR IN, V12, P512; Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2; Mohri M., 2018, FDN MACHINE LEARNING; Mohri M., 2018, FDN MACHINE LEARNING; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Quinlan JR, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P725; Rashmi KV, 2015, JMLR WORKSH CONF PRO, V38, P489; Schapire R.E., 1997, P 14 INT C MACHINE L, P322; Schapire RE, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1401; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Sun P, 2014, PR MACH LEARN RES, V32, P1251; VAPNIK V, 1992, ADV NEUR IN, V4, P831	26	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305044
C	Cotter, A; Narasimhan, H; Gupta, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cotter, Andrew; Narasimhan, Harikrishna; Gupta, Maya			On Making Stochastic Classifiers Deterministic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CLASSIFICATION	Stochastic classifiers arise in a number of machine learning problems, and have become especially prominent of late, as they often result from constrained optimization problems, e.g. for fairness, churn, or custom losses. Despite their utility, the inherent randomness of stochastic classifiers may cause them to be problematic to use in practice for a variety of practical reasons. In this paper, we attempt to answer the theoretical question of how well a stochastic classifier can be approximated by a deterministic one, and compare several different approaches, proving lower and upper bounds. We also experimentally investigate the pros and cons of these methods, not only in regard to how successfully each deterministic classifier approximates the original stochastic classifier, but also in terms of how well each addresses the other issues that can make stochastic classifiers undesirable.	[Cotter, Andrew; Narasimhan, Harikrishna; Gupta, Maya] Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA	Google Incorporated	Cotter, A (corresponding author), Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	acotter@google.com; hnarasimhan@google.com; mayagupta@google.com						Agarwal K, 2018, 2018 INTERNATIONAL CONFERENCE ON COMPUTING, POWER AND COMMUNICATION TECHNOLOGIES (GUCON), P60, DOI 10.1109/GUCON.2018.8674988; Angwin J., 2016, MACHINE BIAS; [Anonymous], 1991, SPEECH NAT LANG P WO, DOI [10.3115/112405.112471, DOI 10.3115/112405.112471]; Beutel A., 2019, KDD APPL DATA SCI TR; Chen R. S., 2017, NIPS; Cotter A., 2019, ALGORITHMIC LEARNING; Cotter A, 2019, J MACH LEARN RES, V20; Dheeru D., 2019, UCI MACHINE LEARNING; Dwork C, 2015, ACM S THEORY COMPUT, P117, DOI 10.1145/2746539.2746580; Goh G, 2016, ADV NEUR IN, V29; Gupta M., 2019, NEURIPS; Gupta M., 2019, PAIRWISE FAIRNESS RA; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hastie T, 2009, ELEMENTS STAT LEARNI; Kallus Nathan, 2019, ADV NEURAL INFORM PR, P3433; Kearns M., 2017, PREVENTING FAIRNESS; Kim J-D., 2013, ACL 2013; Lacarelle A, 2007, SPRINGER PROC PHYS, V117, P769; Langford John, 2002, NIPS; Lawrence S, 1998, LECT NOTES COMPUT SC, V1524, P299; Li Xiong, 2015, MACHINE LEARNING; McAllester David, 2003, MACHINE LEARNING; Narasimhan H, 2015, PR MACH LEARN RES, V37, P199; Narasimhan Harikrishna, 2018, INT C ART INT STAT, P1646; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Provost F, 2001, MACH LEARN, V42, P203, DOI 10.1023/A:1007601015854; Rubinfeld Ronitt, 2012, NOTES LECT 5 MIT 6 8; Saunders B, 2008, PHILOSOPHY, V83, P359, DOI 10.1017/S0031819108000727; SHER G, 1980, NOUS, V14, P203, DOI 10.2307/2214861; Sun Y., 2006, ICDM; Wang S, 2012, IEEE T SYST MAN CY B, V42, P1119, DOI 10.1109/TSMCB.2012.2187280; Wightman Linda F, 1998, LSAC NATL LONGITUDIN	34	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902053
C	Deng, Y; Lahaie, S; Mirrokni, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Deng, Yuan; Lahaie, Sebastien; Mirrokni, Vahab			A Robust Non-Clairvoyant Dynamic Mechanism for Contextual Auctions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEARCH	Dynamic mechanisms offer powerful techniques to improve on both revenue and efficiency by linking sequential auctions using state information, but these techniques rely on exact distributional information of the buyers' valuations (present and future), which limits their use in learning settings. In this paper, we consider the problem of contextual auctions where the seller gradually learns a model of the buyer's valuation as a function of the context (e.g., item features) and seeks a pricing policy that optimizes revenue. Building on the concept of a bank account mechanism-a special class of dynamic mechanisms that is known to be revenue-optimal-we develop a non-clairvoyant dynamic mechanism that is robust to both estimation errors in the buyer's value distribution and strategic behavior on the part of the buyer. We then tailor its structure to achieve a policy with provably low regret against a constant approximation of the optimal dynamic mechanism in contextual auctions. Our result substantially improves on previous results that only provide revenue guarantees against static benchmarks.	[Deng, Yuan] Duke Univ, Durham, NC 27710 USA; [Lahaie, Sebastien; Mirrokni, Vahab] Google Res, New York, NY USA	Duke University; Google Incorporated	Deng, Y (corresponding author), Duke Univ, Durham, NC 27710 USA.	ericdy@cs.duke.edu; slahaie@google.com; mirrokni@google.com						Amin K., 2014, ADV NEURAL INFORM PR, P622; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Ashlagi I, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P213, DOI 10.1145/2940716.2940775; Aydin K, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P387, DOI 10.1145/2835776.2835829; Bergemann D., 2011, WILEY ENCY OPERATION; Cai Y, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P459; Cohen MC, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P817, DOI 10.1145/2940716.2940728; den Boer A.V., 2015, SURV OPER RES MANAG, V20, P1; Drutsa A, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P33, DOI 10.1145/3038912.3052700; Drutsa Alexey, 2018, INT C MACH LEARN, P1318; Edelman B, 2007, DECIS SUPPORT SYST, V43, P192, DOI 10.1016/j.dss.2006.08.008; Golrezaei Negin, 2018, DYNAMIC INCENTIVE AW, DOI DOI 10.2139/SSRN.3144034; Koufogiannakis C, 2014, ALGORITHMICA, V70, P648, DOI 10.1007/s00453-013-9771-6; Leme Renato Paes, 2018, CONTEXTUAL SEARCH VI; Mao J., 2018, ADV NEURAL INFORM PR, P5648; Mirrokni V, 2016, ARXIV160508840; Mirrokni V, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P169, DOI 10.1145/3219166.3219224; Mohri M, 2014, PR MACH LEARN RES, V32; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Papadimitriou C., 2016, P 27 ANN ACM SIAM S, P1458; THOMAS J, 1990, J ECON THEORY, V51, P367, DOI 10.1016/0022-0531(90)90023-D; ZHENG CF, 2018, ADV NEURAL INFORM PR, P338	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900027
C	Dhar, S; Cherkassky, V; Shah, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dhar, Sauptik; Cherkassky, Vladimir; Shah, Mohak			Multiclass Learning from Contradictions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUPPORT VECTOR MACHINE; CLASSIFICATION; LEARNABILITY	We introduce the notion of learning from contradictions, a.k.a Universum learning, for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We show that learning from contradictions (using MU-SVM) incurs lower sample complexity compared to multiclass SVM (M-SVM) by deriving the Natarajan dimension for sample complexity for PAC-learnability of MU-SVM. We also propose an analytic span bound for MU-SVM and demonstrate its utility for model selection resulting in similar to 2 - 4x faster computation times than standard resampling techniques. We empirically demonstrate the efficacy of MU-SVM on several real world datasets achieving > 20% improvement in test accuracies compared to M-SVM. Insights into the underlying behavior of MU-SVM using a histograms-of-projections method are also provided.	[Dhar, Sauptik; Shah, Mohak] LG Elect, Santa Clara, CA 95054 USA; [Cherkassky, Vladimir] Univ Minnesota, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Dhar, S (corresponding author), LG Elect, Santa Clara, CA 95054 USA.	sauptik.dhar@lge.com; cherk001@umn.edu; mohak.shah@lge.com						Bonidal R., 2013, THESIS; Chen S, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1016; Cherkassky V, 2011, IEEE T NEURAL NETWOR, V22, P1241, DOI 10.1109/TNN.2011.2157522; Daniely A., 2014, P MACH LEARN RES, V35; Daniely A., 2012, NIPS; Dhar S, 2015, IEEE T CYBERNETICS, V45, P806, DOI 10.1109/TCYB.2014.2336876; Fanty M., 1991, P ADV NEURAL INFORM, P220; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Guermeur Y, 2011, INFORMATICA-LITHUAN, V22, P73; Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427; Japkowicz N., 2011, EVALUATING LEARNING, DOI 10.1017/CBO9780511921803; Lauer F, 2011, J MACH LEARN RES, V12, P2293; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Lei Y., 2017, CORR, Vabs/1706.09814; Lei YW, 2015, ADV NEUR IN, V28; Lu S., 2014, ADV COMPUTER SCI INT, V3, P17; Luntz A., 1969, TECHNICHESKAYA KIBER; Mohri M., 2018, FDN MACHINE LEARNING; Musayeva K, 2019, NEUROCOMPUTING, V342, P6, DOI 10.1016/j.neucom.2018.11.096; Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1023/A:1022605311895; Qi ZQ, 2014, J COMPUT APPL MATH, V263, P288, DOI 10.1016/j.cam.2013.11.003; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shen CH, 2012, IEEE T PATTERN ANAL, V34, P825, DOI 10.1109/TPAMI.2011.240; Sinz F, 2008, ADV NEURAL INFORM PR, P1369; Sinz F., 2007, THESIS; Stallkamp Johannes, 2012, NEURAL NETWORKS; Vapnik V, 2000, NEURAL COMPUT, V12, P2013, DOI 10.1162/089976600300015042; Vapnik V.N., 2006, ESTIMATION DEPENDENC, VVolume 40; Wang Z, 2014, KNOWL-BASED SYST, V70, P376, DOI 10.1016/j.knosys.2014.07.019; Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219; Weston J., 2006, P 23 INT C MACHINE L, P1009; Xu YT, 2016, APPL INTELL, V44, P956, DOI 10.1007/s10489-015-0736-0; Zhang D., 2008, SDM, P323; Zhang X, 2017, AAAI CONF ARTIF INTE, P2907; Zhu CM, 2016, NEUROCOMPUTING, V175, P610, DOI 10.1016/j.neucom.2015.10.102; [No title captured]	39	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900004
C	Eccles, T; Bachrach, Y; Lever, G; Lazaridou, A; Graepel, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Eccles, Tom; Bachrach, Yoram; Lever, Guy; Lazaridou, Angeliki; Graepel, Thore			Biases for Emergent Communication in Multi-agent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LEARNERS; PROGRESS	We study the problem of emergent communication, in which language arises because speakers and listeners must communicate information in order to solve tasks. In temporally extended reinforcement learning domains, it has proved hard to learn such communication without centralized training of agents, due in part to a difficult joint exploration problem. We introduce inductive biases for positive signalling and positive listening, which ease this problem. In a simple one-step environment, we demonstrate how these biases ease the learning problem. We also apply our methods to a more extended environment, showing that agents with these inductive biases achieve better performance, and analyse the resulting communication protocols.	[Eccles, Tom; Bachrach, Yoram; Lever, Guy; Lazaridou, Angeliki; Graepel, Thore] DeepMind, London, England		Eccles, T (corresponding author), DeepMind, London, England.	eccles@google.com; yorambac@google.com; guylever@google.com; angeliki@google.com; thore@google.com						Bansal T., 2018, 6 INT C LEARN REPR I; Bernstein Daniel S., 2000, P 16 C UNCERTAINTY A, P32; Cao K, 2018, INT C LEARN REPR; Cao YC, 2013, IEEE T IND INFORM, V9, P427, DOI 10.1109/TII.2012.2219061; Das A., 2018, ARXIV181011187; ESPEHOLT L., 2018, ARXIV E PRINTS; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Grice Jack., 1968, PHILOS LANGUAGE ARTI, P49; Hansen EA, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P709; Hausknecht M. J., 2016, THESIS; Hinton G., 2012, COURSERA LECT SLIDES; Jaderberg M., 2018, ARXIV180701281; Jaques Natasha, 2019, INT C MACH LEARN; Kitano H., 1997, Proceedings of the First International Conference on Autonomous Agents, P340, DOI 10.1145/267658.267738; Kottur Satwik, 2017, ARXIV170608502; Lauer Martin, 2000, P 17 INT C MACHINE L; Laurent GJ, 2011, INT J KNOWL-BASED IN, V15, P55, DOI 10.3233/KES-2010-0206; Lazaridou A., 2018, ARXIV180403984; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Leibo Joel Z., 2019, CORR; Littman ML, 1994, ICML 1994, P157; Lowe R., 2017, P INT C NEUR INF PRO, P6379; Lowe Ryan, 2019, ARXIV190305168; Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057; Matignon Laetitia, 2012, P 26 AAAI C ART INT; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mordatch I, 2018, AAAI CONF ARTIF INTE, P1495; NEALE S, 1992, LINGUIST PHILOS, V15, P509, DOI 10.1007/BF00630629; SHAPLEY LS, 1953, P NATL ACAD SCI USA, V39, P1095, DOI 10.1073/pnas.39.10.1095; Steels L, 2003, TRENDS COGN SCI, V7, P308, DOI 10.1016/S1364-6613(03)00129-3; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Wagner K, 2003, ADAPT BEHAV, V11, P37, DOI 10.1177/10597123030111003	35	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904073
C	El Balghiti, O; Elmachtoub, AN; Grigas, P; Tewari, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		El Balghiti, Othman; Elmachtoub, Adam N.; Grigas, Paul; Tewari, Ambuj			Generalization Bounds in the Predict-then-Optimize Framework	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The predict-then-optimize framework is fundamental in many practical settings: predict the unknown parameters of an optimization problem, and then solve the problem using the predicted values of the parameters. A natural loss function in this environment is to consider the cost of the decisions induced by the predicted parameters, in contrast to the prediction error of the parameters. This loss function was recently introduced [7] and christened Smart Predict-then-Optimize (SPO) loss. Since the SPO loss is nonconvex and noncontinuous, standard results for deriving generalization bounds do not apply. In this work, we provide an assortment of generalization bounds for the SPO loss function. In particular, we derive bounds based on the Natarajan dimension that, in the case of a polyhedral feasible region, scale at most logarithmically in the number of extreme points, but, in the case of a general convex set, have poor dependence on the dimension. By exploiting the structure of the SPO loss function and an additional strong convexity assumption on the feasible region, we can dramatically improve the dependence on the dimension via an analysis and corresponding bounds that are akin to the margin guarantees in classification problems.	[El Balghiti, Othman] Rayens Capital, Chicago, IL 60606 USA; [Elmachtoub, Adam N.] Columbia Univ, New York, NY 10027 USA; [Grigas, Paul] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Tewari, Ambuj] Univ Michigan, Berkeley, CA 94720 USA	Columbia University; University of California System; University of California Berkeley; University of Michigan System; University of Michigan	El Balghiti, O (corresponding author), Rayens Capital, Chicago, IL 60606 USA.	oe2161@columbia.edu; adam@ieor.columbia.edu; pgrigas@berkeley.edu; tewaria@umich.edu			Rayens Capital; NSF [CMMI-1763000, CCF-1755705, CMMI-1762744]; NSF via CAREER grant [IIS-1452099]; Sloan Research Fellowship	Rayens Capital; NSF(National Science Foundation (NSF)); NSF via CAREER grant; Sloan Research Fellowship(Alfred P. Sloan Foundation)	OB thanks Rayens Capital for their support. AE acknowledges the support of NSF via grant CMMI-1763000. PG acknowledges the support of NSF Awards CCF-1755705 and CMMI-1762744. AT acknowledges the support of NSF via CAREER grant IIS-1452099 and of a Sloan Research Fellowship.	[Anonymous], 2014, ARXIV14025481; [Anonymous], 2015, ADV NEURAL INFORM PR; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bertsekas D., 2015, CONVEX OPTIMIZATION; Daniely A., 2014, COLT, P287; Donti P, 2017, P ADV NEUR INF PROC, V30, P5484; Elmachtoub A. N., 2017, ARXIV171008005; Garber D., 2015, ICML; Guermeur Y, 2007, J MACH LEARN RES, V8, P2551; Journee M, 2010, J MACH LEARN RES, V11, P517; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Koltchinskii V, 2002, ANN STAT, V30, P1; Li J., 2018, ADV NEURAL INFORM PR, V1586; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; Mohri M., 2018, FDN MACHINE LEARNING; Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1023/A:1022605311895; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Tibshirani R., 2015, STAT LEARNING SPARSI; Xiang Y, 2009, ADV NEURAL INFORM PR, V22, P889	21	3	3	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906011
C	Farina, G; Kroer, C; Sandholm, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Farina, Gabriele; Kroer, Christian; Sandholm, Tuomas			Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the performance of optimistic regret-minimization algorithms for both minimizing regret in, and computing Nash equilibria of, zero-sum extensive-form games. In order to apply these algorithms to extensive-form games, a distance-generating function is needed. We study the use of the dilated entropy and dilated Euclidean distance functions. For the dilated Euclidean distance function we prove the first explicit bounds on the strong-convexity parameter for general treeplexes. Furthermore, we show that the use of dilated distance-generating functions enable us to decompose the mirror descent algorithm, and its optimistic variant, into local mirror descent algorithms at each information set. This decomposition mirrors the structure of the counterfactual regret minimization framework, and enables important techniques in practice, such as distributed updates and pruning of cold parts of the game tree. Our algorithms provably converge at a rate of T-1, which is superior to prior counterfactual regret minimization algorithms. We experimentally compare to the popular algorithm CFR+, which has a theoretical convergence rate of T-0.5 in theory, but is known to often converge at a rate of T-1, or better, in practice. We give an example matrix game where CFR+ experimentally converges at a relatively slow rate of T-0.74, whereas our optimistic methods converge faster than T-1. We go on to show that our fast rate also holds in the Kuhn poker game, which is an extensive-form game. For games with deeper game trees however, we find that CFR+ is still faster. Finally we show that when the goal is minimizing regret, rather than computing a Nash equilibrium, our optimistic methods can outperform CFR+, even in deep game trees.	[Farina, Gabriele; Sandholm, Tuomas] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; [Kroer, Christian] Columbia Univ, IEOR Dept, New York, NY 10027 USA; [Sandholm, Tuomas] Strategic Machine Inc, Morristown, NJ USA; [Sandholm, Tuomas] Strategy Robot Inc, Pittsburgh, PA USA; [Sandholm, Tuomas] Optimized Markets Inc, Pittsburgh, PA USA	Carnegie Mellon University; Columbia University	Farina, G (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	gfarina@cs.cmu.edu; christian.kroer@columbia.edu; sandholm@cs.cmu.edu		Kroer, Christian/0000-0002-9009-8683	National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]; Facebook fellowship	National Science Foundation(National Science Foundation (NSF)); ARO; Facebook fellowship(Facebook Inc)	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Gabriele Farina is supported by a Facebook fellowship.	Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2017, SCIENCE; Brown N., 2015, INT C AUT AG MULT SY; Brown N, 2019, AAAI CONF ARTIF INTE, P1829; Brown N, 2019, SCIENCE, V365, P885, DOI 10.1126/science.aay2400; Brown N, 2017, PR MACH LEARN RES, V70; Brown N, 2017, AAAI CONF ARTIF INTE, P421; Brown Noam, 2018, ARXIV180508195; Burch Neil, 2014, AAAI C ART INT AAAI; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chiang C. K, 2012, C LEARN THEOR, V23, P1; Farina G, 2019, PR MACH LEARN RES, V97; Ganzfried Sam, 2015, INT C AUT AG MULT SY; Ganzfried Sam, 2014, AAAI C ART INT AAAI; Gilpin A, 2007, J ACM, V54, DOI 10.1145/1284320.1284324; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hoda S, 2010, MATH OPER RES, V35, P494, DOI 10.1287/moor.1100.0452; Kroer C., 2016, P ACM C EC COMP EC; Kroer C., 2015, P ACM C EC COMP EC; Kroer C, 2018, ADV NEUR IN, V31; Kroer Christian, 2019, ARXIV190310646; Kroer Christian, 2018, MATH PROGRAM, P1; Kroer  Christian, 2014, P ACM C EC COMP EC; Kuhn H. W., 1950, CONTRIBUTIONS THEORY, V1, P97; Lanctot  Marc, 2012, INT C MACH LEARN ICM; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; Moravcik Matej, 2016, AAAI C ART INT AAAI; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Rakhlin Alexander, 2013, ONLINE LEARNING PRED; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Southey F., 2005, P 21 ANN C UNC ART I; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989; Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645; vonStengel B, 1996, GAME ECON BEHAV, V14, P220, DOI 10.1006/game.1996.0050; Zinkevich, 2003, P 20 INT C MACH LEAR, P928; Zinkevich M., 2007, ADV NEURAL INFORM PR, V7, P1729	38	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305024
C	Fellows, M; Mahajan, A; Rudner, TGJ; Whiteson, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fellows, Matthew; Mahajan, Anuj; Rudner, Tim G. J.; Whiteson, Shimon			VIREL: A Variational Inference Framework for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Applying probabilistic models to reinforcement learning (RL) enables the uses of powerful optimisation tools such as variational inference in RL. However, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, for example, the lack of mode capturing behaviour in pseudo-likelihood methods, difficulties learning deterministic policies in maximum entropy RL based approaches, and a lack of analysis when function approximators are used. We propose VIREL, a theoretically grounded inference framework for RL that utilises a parametrised action-value function to summarise future dynamics of the underlying MDP, generalising existing approaches. VIREL also benefits from a mode-seeking form of KL divergence, the ability to learn deterministic optimal polices naturally from inference, and the ability to optimise value functions and policies in separate, iterative steps. Applying variational expectation-maximisation to VIREL, we show that the actor-critic algorithm can be reduced to expectation-maximisation, with policy improvement equivalent to an E-step and policy evaluation to an M-step. We derive a family of actor-critic methods from VIREL, including a scheme for adaptive exploration and demonstrate that our algorithms outperform state-of-the-art methods based on soft value functions in several domains.	[Fellows, Matthew; Mahajan, Anuj; Rudner, Tim G. J.; Whiteson, Shimon] Univ Oxford, Dept Comp Sci, Oxford, England	University of Oxford	Fellows, M; Mahajan, A (corresponding author), Univ Oxford, Dept Comp Sci, Oxford, England.	matthew.fellows@cs.ox.ac.uk; anuj.mahajan@cs.ox.ac.uk			European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme [637713]; NVIDIA; Google DeepMind; Drapers Scholarship; Rhodes Trust; EPSRC	European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme(European Research Council (ERC)); NVIDIA; Google DeepMind(Google Incorporated); Drapers Scholarship; Rhodes Trust; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement number 637713). The experiments were made possible by a generous equipment grant from NVIDIA. Matthew Fellows is funded by the EPSRC. Anuj Mahajan is funded by Google DeepMind and the Drapers Scholarship. Tim G. J. Rudner is funded by the Rhodes Trust and the EPSRC. We would like to thank Yarin Gal and Piotr Milo for helpful comments.	Abdolmaleki Abbas, 2018, INT C LEARN REPR; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Bass R.F., 2013, REAL ANAL GRADUATE S; Beal M.J, 2003, THESIS; Bertsekas D., 1996, ATHENA SCI SERIES OP; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Bishop C.M, 2006, PATTERN RECOGN; Blei DM, 2017, VARIATIONAL INFERENC; Brockman G., 2016, ABS160601540 CORR; Ciosek K., 2018, ARXIV180103326; Ciosek Kamil, 2018, 32 AAAI C ART INT AA; Dayan P, 1997, NEURAL COMPUT, V9, P271, DOI 10.1162/neco.1997.9.2.271; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duan Y, 2016, INT C MACH LEARN, P1329; Fellows M., 2018, P MACHINE LEARNING R, P1486; Foerster J, 2018, PR MACH LEARN RES, V80; Fox CW, 2012, ARTIF INTELL REV, V38, P85, DOI 10.1007/s10462-011-9236-8; Fujimoto S, 2018, PR MACH LEARN RES, V80; Furmston Thomas, 2010, P AISTATS, P241; Geist M, 2019, P MACHINE LEARNING R, V97, P2160; Gu Shixiang, 2016, Q PROP SAMPLE EFFICI, P1; Gunawardana A, 2005, J MACH LEARN RES, V6, P2049; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; Hachiya H, 2009, LECT NOTES ARTIF INT, V5781, P469, DOI 10.1007/978-3-642-04180-8_48; Heess N., 2015, NIPS; Heess N., 2013, EUROPEAN WORKSHOP RE, P45; Heess Nicolas, 2015, LEARNING CONTINUOUS, P1; Jordan M. I., 1999, LEARNING GRAPHICAL M; Kelly J, 2008, GEN FUNCTIONS, P111, DOI [10.1002/9783527618897.ch4, DOI 10.1002/9783527618897.CH4]; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Koller D, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P324; Levine S., 2014, THESIS STANFORD U ST; Levine S, 2018, REINFORCEMENT LEARNI; Levine Sergey, 2013, P 26 INT C NEUR INF, P207; Liberzon D., 2011, CALCULUS VARIATIONS, DOI DOI 10.2307/J.CTVCM4G0S; Lillicrap Timothy P., 2018, ABS180400379 CORR; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mahajan A., 2017, ARXIV170602999; Mahajan A, 2019, MULTIAGENT VARIATION; Mahajan A, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P1619; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Neumann G, 2011, P 28 INT C MACH LEAR, P817; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Peters J., 2007, P 24 INT C MACHINE L, P745; Rawlik K, 2012, ROBOTICS SCI SYSTEMS; Rawlik K, 2010, ABS10093958 CORR; Sallans B, 2004, J MACH LEARN RES, V5, P1063; Schulman J., 2015, ARXIV PREPRINT ARXIV; Schulman J., 2017, **NON-TRADITIONAL**; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2014, ICML ICML 14, P387; Sutton, 2017, INTRO REINFORCEMENT; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2009, ADV NEURAL INFORM PR, V21, P1609; Sutton Richard S., 1998, SUTTON BARTO BOOK RE, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Thomas PS, 2014, PR MACH LEARN RES, V32; Todorov E., 2007, ADV NEURAL INFORM PR, V19, DOI 10.7551/mitpress/7503.003.0176; Toussaint M., 2009, KUNSTLICHE INTELLIGE, V3, P2009; Toussaint M., 2009, P 26 ANN INT C MACHI, P1049, DOI [10.1145%2F1553374.1553508, DOI 10.1145/1553374.1553508, 10.1145/1553374.1553508]; Toussaint Marc, 2006, INT C MACH LEARN ICM, P945; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Turner R. E., 2011, 2 PROBLEMS VARIATION, DOI 10.1017/CBO9780511984679.006; van Hasselt H., 2016, DEEP REINFORCEMENT L, V230, P173, DOI [10.1016/j.artint.2015.09.002, DOI 10.1016/J.ARTINT.2015.09.002]; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Williams R. J, 1993, ANAL SOME INCREMENTA; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Yang Z, 2019, ABS190100137 CORR; Ziebart B. D., 2010, P 27 INT C INT C MAC, P1255; Ziebart B. D., 2008, AAAI, V8, P1433; Ziebart B. D., 2010, THESIS	75	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													15	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307017
C	Feng, J; Cai, QZ; Zhou, ZH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Feng, Ji; Cai, Qi-Zhi; Zhou, Zhi-Hua			Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this work, we consider one challenging training time attack by modifying training data with bounded perturbation, hoping to manipulate the behavior (both targeted or non-targeted) of any corresponding trained classifier during test time when facing clean samples. To achieve this, we proposed to use an auto-encoder-like network to generate such adversarial perturbations on the training data together with one imaginary victim differentiable classifier. The perturbation generator will learn to update its weights so as to produce the most harmful noise, aiming to cause the lowest performance for the victim classifier during test time. This can be formulated into a non-linear equality constrained optimization problem. Unlike GANs, solving such problem is computationally challenging, we then proposed a simple yet effective procedure to decouple the alternating updates for the two networks for stability. By teaching the perturbation generator to hijacking the training trajectory of the victim classifier, the generator can thus learn to move against the victim classifier step by step. The method proposed in this paper can be easily extended to the label specific setting where the attacker can manipulate the predictions of the victim classifier according to some predefined rules rather than only making wrong predictions. Experiments on various datasets including CIFAR-10 and a reduced version of ImageNet confirmed the effectiveness of the proposed method and empirical results showed that, such bounded perturbations have good transferability across different types of victim classifiers.	[Feng, Ji; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China; [Feng, Ji; Cai, Qi-Zhi] Sinovat Ventures AI Inst, Beijing, Peoples R China	Nanjing University	Feng, J (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.; Feng, J (corresponding author), Sinovat Ventures AI Inst, Beijing, Peoples R China.	fengj@lamda.nju.edu.cn; caiqizhi@chuangxin.com; zhouzh@lamda.nju.edu.cn			NSFC [61751306]; National Key R&D Program of China [2018YFB1004300]; Collaborative Innovation Center of Novel Software Technology and Industrialization	NSFC(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; Collaborative Innovation Center of Novel Software Technology and Industrialization	This research was supported by NSFC (61751306), the National Key R&D Program of China (2018YFB1004300), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. The first two authors would like to thank Beijing Sinnovation Ventures Megvii International AI Institute Company Limited for the support.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2012, P 29 INT COFERENCE I; Athalye A, 2018, PR MACH LEARN RES, V80; Ba J., 2017, P 3 INT C LEARN REPR; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Chen X., 2017, ARXIV171205526; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; EYKHOLT K, 2018, CVPR, P1625, DOI DOI 10.1109/CVPR.2018.00175; Goodfellow I. J., 2015, ICLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu T., 2017, ARXIV170806733; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Koh PW, 2017, PR MACH LEARN RES, V70; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munoz-Gonzalez L., 2017, P 10 ACM WORKSHOP AR, P27, DOI [10.1145/3128572.3140451, 10.1145/3128572]; Nelson B., 2008, USENIX WORKSH LARG S; Pearson K., 1901, EDINBURGH DUBLIN PHI, V2, P572; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shafahi Ali, 2018, ADV NEURAL INFORM PR, P3; VAPNIK V, 1992, ADV NEUR IN, V4, P831; Wong E, 2018, PR MACH LEARN RES, V80; Zhang C., 2016, ICLR	29	3	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903059
C	Fiez, T; Jain, L; Jamieson, K; Ratliff, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fiez, Tanner; Jain, Lalit; Jamieson, Kevin; Ratliff, Lillian			Sequential Experimental Design for Transductive Linear Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper we introduce the pure exploration transductive linear bandit problem: given a set of measurement vectors X subset of R-d, a set of items Z subset of R-d, a fixed confidence delta, and an unknown vector theta* epsilon R-d, the goal is to infer argmax(zEz)Z(inverted perpendicular) theta* with probability 1 - delta by making as few sequentially chosen noisy measurements of the form X-inverted perpendicular theta* as possible. When X = Z, this setting generalizes linear bandits, and when X is the standard basis vectors and Z subset of {0, 1}(d), combinatorial bandits. The transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages X a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages Z that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books X a user is queried about may be restricted to known best-sellers even though the goal might be to recommend more esoteric titles Z. In this paper, we provide instance -dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we present the first non -asymptotic algorithm for linear bandits that nearly achieves the information -theoretic lower bound.	[Fiez, Tanner; Ratliff, Lillian] Univ Washington, Elect & Comp Engn, Seattle, WA 98195 USA; [Jain, Lalit; Jamieson, Kevin] Univ Washington, Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Jain, L (corresponding author), Univ Washington, Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.	fiezt@uw.edu; lalitj@cs.washington.edu; jamieson@cs.washington.edu; ratliffl@uw.edu		Ratliff, Lillian/0000-0001-8936-0229				Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Allen-Zhu Z., 2017, ARXIV171105174; [Anonymous], 2005, STAT EXPT; Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Cao T., 2017, ARXIV171108018; Chen L., 2015, ARXIV151103774; Chen Lijie, 2017, ARXIV170601081; Chen S., 2014, P ADV NEUR INF PROC, P379; Dani V., 2008, STOCHASTIC LINEAR OP; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Gabillon Victor, 2012, ADV NEURAL INFORM PR, P3212; He C, 2016, 2016 16th International Workshop on Junction Technology (IWJT), P64, DOI 10.1109/IWJT.2016.7486675; Hill DN, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1813, DOI 10.1145/3097983.3098184; Hoffman MW, 2014, JMLR WORKSH CONF PRO, V33, P365; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jamieson K., 2014, C LEARN THEOR, P423; Karnin Zohar, 2016, ADV NEURAL INFORM PR, P145; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Kaufmann E, 2016, J MACH LEARN RES, V17; Kazerouni A., 2019, ARXIV190508224; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Pukelsheim F, 2006, CLASS APPL MATH, V50, P1, DOI 10.1137/1.9780898719109; Rockafellar R.T., 2015, CONVEX ANAL; Soare M., 2014, ARXIV14096110; Soare M., 2015, THESIS; Szepesvari C., 2016, ARXIV161004491; Tao Chao, 2018, INT C MACH LEARN, P4884; Tsybakov AB, 2004, ANN STAT, V32, P135; Xu L, 2018, CHIN CONTR CONF, P841; Yu K, 2006, P 23 INT C MACH LEAR, ppp1081	34	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902031
C	Garg, D; Ikbal, S; Srivastava, SK; Vishwakarma, H; Karanam, H; Subramaniam, LV		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Garg, Dinesh; Ikbal, Shajith; Srivastava, Santosh K.; Vishwakarma, Harit; Karanam, Hima; Subramaniam, L. Venkata			Quantum Embedding of Knowledge for Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Statistical Relational Learning (SRL) methods are the most widely used techniques to generate distributional representations of the symbolic Knowledge Bases (KBs). These methods embed any given KB into a vector space by exploiting statistical similarities among its entities and predicates but without any guarantee of preserving the underlying logical structure of the KB. This, in turn, results in poor performance of logical reasoning tasks that are solved using such distributional representations. We present a novel approach called Embed2Reason (E2R) that embeds a symbolic KB into a vector space in a logical structure preserving manner. This approach is inspired by the theory of Quantum Logic. Such an embedding allows answering membership based complex logical reasoning queries with impressive accuracy improvements over popular SRL baselines.	[Garg, Dinesh; Ikbal, Shajith; Srivastava, Santosh K.; Karanam, Hima; Subramaniam, L. Venkata] IBM Res AI, New Delhi, India; [Vishwakarma, Harit] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA; [Vishwakarma, Harit] IBM Res, New Delhi, India	University of Wisconsin System; University of Wisconsin Madison; International Business Machines (IBM)	Garg, D (corresponding author), IBM Res AI, New Delhi, India.	garg.dinesh@in.ibm.com; shajmoha@in.ibm.com; sasriva5@in.ibm.com; hvishwakarma@cs.wisc.edu; hkaranam@in.ibm.com; lvsubram@in.ibm.com						BAADER F., 2017, INTRO DESCRIPTION LO; Bach SH, 2017, J MACH LEARN RES, V18; Birkhoff G, 1936, ANN MATH, V37, P823, DOI 10.2307/1968621; Birkhoff Garrett, 1965, AMS C PUBLICATIONS, V25; Bordes A., 2011, AAAI; Bordes A., 2013, ADV NEURAL INFORM PR; Bouraoui Zied, 2017, AAAI; Brachman R.J, 2004, KNOWLEDGE REPRESENTA; Cer Daniel, 2018, CORR; COHEN DW, 1989, INTRO HILBERT SPACE; COLMERAUER A, 1993, SIGPLAN NOTICES, V28, P37, DOI 10.1145/155360.155362; Dalla Chiara M. L., 2008, QUANTUM LOGIC; Demeester T., 2016, EMNLP, P1389, DOI 10.18653/v1/d16-1146; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171, DOI [10.18653/v1/N19-1423, DOI 10.18653/V1/N19-1423]; Diaz G.I., 2018, P EDBT C, P433; Ganter B., 1999, FORMAL CONCEPT ANAL; Getoor L., 2007, ADAPTIVE COMPUTATION; Hamilton William L., 2018, NEURIPS; Hoffman Kenneth, 2015, LINEAR ALGEBRA; Hohenecker Patrick, 2017, CORR; Jameel Shoaib, 2016, ECAI; Kingma D.P., 2015, INT C LEARN REPR, P1; Mikolov T., 2013, CORR; MUGGLETON S, 1990, NEW GENERAT COMPUT, V8, P295; Nickel M., 2017, ADV NEURAL INFORM PR, P6338; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592; Pennington Jeffrey, 2014, EMNLP; Rajpurkar P., 2016, CORR, P2383, DOI [10.18653/v1/D16-1264, DOI 10.18653/V1/D16-1264]; Reddy Siva, 2018, CORR; Rocktaschel Tim, 2014, WORKSH SEM PARS COL; Rocktaschel Tim, 2017, ADV NEURAL INFORM PR, P3791; Rocktaschel Tim, 2015, P 2015 C N AM CHAPT, P1119, DOI [10.3115/v1/N15-1118, DOI 10.3115/V1/N15-1118]; Trouillon T, 2016, PR MACH LEARN RES, V48; Varadarajan VS, 1985, GEOMETRY QUANTUM THE, P2; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wang Z, 2014, AAAI CONF ARTIF INTE, P1112; Widdows D, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P136; Widdows Dominic, 2004, CSLI LECT NOTES SERI, V172; Xie R., 2016, IJCAI, P2965	40	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305057
C	Gartrell, M; Brunel, VE; Dohmatob, E; Krichene, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gartrell, Mike; Brunel, Victor-Emmanuel; Dohmatob, Elvis; Krichene, Syrine			Learning Nonsymmetric Determinantal Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Determinantal point processes (DPPs) have attracted substantial attention as an elegant probabilistic model that captures the balance between quality and diversity within sets. DPPs are conventionally parameterized by a positive semi-definite kernel matrix, and this symmetric kernel encodes only repulsive interactions between items. These so-called symmetric DPPs have significant expressive power, and have been successfully applied to a variety of machine learning tasks, including recommendation systems, information retrieval, and automatic summarization, among many others. Efficient algorithms for learning symmetric DPPs and sampling from these models have been reasonably well studied. However, relatively little attention has been given to nonsymmetric DPPs, which relax the symmetric constraint on the kernel. Nonsymmetric DPPs allow for both repulsive and attractive item interactions, which can significantly improve modeling power, resulting in a model that may better fit for some applications. We present a method that enables a tractable algorithm, based on maximum likelihood estimation, for learning nonsymmetric DPPs from data composed of observed subsets. Our method imposes a particular decomposition of the nonsymmetric kernel that enables such tractable learning algorithms, which we analyze both theoretically and experimentally. We evaluate our model on synthetic and real-world datasets, demonstrating improved predictive performance compared to symmetric DPPs, which have previously shown strong performance on modeling tasks associated with these datasets.	[Gartrell, Mike; Dohmatob, Elvis; Krichene, Syrine] Criteo AI Lab, Paris, France; [Brunel, Victor-Emmanuel] ENSAE ParisTech, Paris, France	Institut Polytechnique de Paris	Gartrell, M (corresponding author), Criteo AI Lab, Paris, France.	m.gartrell@criteo.com; victor.emmanuel.brunel@ensae.fr; e.dohmatob@criteo.com; syrinekrichene@google.com						Affandi R. H., 2014, ICML; Anari N., 2016, JMLR WORKSHOP C P, V49, P103; Borcea J, 2009, J AM MATH SOC, V22, P521; Borodin A., 2009, ARXIV09111153; Brunel V., 2017, P MACH LEARN RES PML, V65, P343; Brunel Victor-Emmanuel, 2018, ADV NEURAL INFORM PR, V31, P7365; Chao WL, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P191; Decreusefond Laurent, 2015, DETERMINANTAL POINT; Dupuy Christophe, 2016, LEARNING DETERMINANT; Gartrell Mike, 2017, AAAI; Gartrell Mike, 2016, RECSYS; Gillenwater J., 2014, THESIS; Gillenwater J., 2014, NIPS; Guo K., 2012, J DATABASE MARKETING, V19, DOI 10.1057/dbm.2012.17; Hu Y, 2008, P 2008 8 IEEE INT C; Kingma D.P., 2015, INT C LEARN REPR, P1; Krause A, 2008, J MACH LEARN RES, V9, P235; Kulesza A., 2011, ICML; Kulesza A., 2013, THESIS; Kulesza A., 2012, FDN TRENDS MACHINE L, V5; Lavancier F, 2015, J R STAT SOC B, V77, P853, DOI 10.1111/rssb.12096; Li Yanen, 2010, P 19 ACM INT C INF K; Lin H., 2012, UNCERTAINTY ARTIFICI; Mariet Zelda, 2015, ICML; Mariet Zelda, 2016, NIPS; Rebeschini Patrick, 2015, P MACHINE LEARNING R, V40, P1480; Tsatsomeros Michael J., 2004, GENERATING DETECTING, P115; Wang Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P617, DOI 10.1145/2934872.2959075; Zhang Cheng, 2017, CORR	29	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306069
C	Golz, P; Kahng, A; Procaccia, AD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Golz, Paul; Kahng, Anson; Procaccia, Ariel D.			Paradoxes in Fair Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Equalized odds is a statistical notion of fairness in machine learning that ensures that classification algorithms do not discriminate against protected groups. We extend equalized odds to the setting of cardinality-constrained fair classification, where we have a bounded amount of a resource to distribute. This setting coincides with classic fair division problems, which allows us to apply concepts from that literature in parallel to equalized odds. In particular, we consider the axioms of resource monotonicity, consistency, and population monotonicity, all three of which relate different allocation instances to prevent paradoxes. Using a geometric characterization of equalized odds, we examine the compatibility of equalized odds with these axioms. We empirically evaluate the cost of allocation rules that satisfy both equalized odds and axioms of fair division on a dataset of FICO credit scores.	[Golz, Paul; Kahng, Anson; Procaccia, Ariel D.] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Golz, P (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.	pgoelz@cs.cmu.edu; akahng@cs.cmu.edu; arielpro@cs.cmu.edu		Golz, Paul/0000-0002-8101-6818; Kahng, Anson/0000-0003-1134-8400	National Science Foundation [IIS-1350598, IIS1714140, CCF-1525932, CCF-1733556]; Office of Naval Research [N00014-16-1-3075, N00014-17-1-2428]; J.P. Morgan AI Research Award; Guggenheim Fellowship	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); J.P. Morgan AI Research Award; Guggenheim Fellowship	This work was partially supported by the National Science Foundation under grants IIS-1350598, IIS1714140, CCF-1525932, and CCF-1733556; by the Office of Naval Research under grants N00014-16-1-3075 and N00014-17-1-2428; by a J.P. Morgan AI Research Award; and by a Guggenheim Fellowship.	Balcan M.-F., 2018, P 33 C NEUR INF PROC; Bard T. F. R., 2007, REPORT C CREDIT SCOR; Barocas Solon, 2018, FAIRNESS MACHINE LEA; Bertsimas D, 2011, OPER RES, V59, P17, DOI 10.1287/opre.1100.0865; Birkhoff G., 1946, U NAC TACUMAN REV SE, V5, P147; Brams Steven J., 1996, FAIR DIVISION CAKE C; Brandt F, 2016, HDB COMPUTATIONAL SO, P57; Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83; Caragiannis I, 2009, LECT NOTES COMPUT SC, V5929, P475, DOI 10.1007/978-3-642-10841-9_45; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Foley D. K, 1967, YALE ECON ESSAYS, V7, P45; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Klamer C., 2010, HDB GROUP DECISION N, P183, DOI 10.1007/978-90-481-9097-3_12; Kleinberg J, 2018, Q J ECON, V133, P237, DOI 10.1093/qje/qjx032; Kleinberg Jon M., 2017, 8 INN THEOR COMP SCI; Moulin H., 2004, FAIR DIVISION COLLEC; Pleiss G., 2017, ADV NEURAL INFORM PR, P5680; Siddiqi N, 2012, CREDIT RISK SCORECAR, V3; Steinhaus H., 1948, ECONOMETRICA, V16, P101; Thomson W, 2011, HBK ECON, V2, P393, DOI 10.1016/S0169-7218(10)00021-3; Von Neumann John, 1953, CONTRIBUTIONS THEORY, V2, P5; Waters A, 2014, AI MAG, V35, P64, DOI 10.1609/aimag.v35i1.2504; Zemel R., 2013, P INT C MACH LEARN, P325	26	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308037
C	Gong, WB; Tschiatschek, S; Turner, RE; Nowozin, S; Hernandez-Lobato, JM; Zhang, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gong, Wenbo; Tschiatschek, Sebastian; Turner, Richard E.; Nowozin, Sebastian; Hernandez-Lobato, Jos Miguel; Zhang, Cheng			Icebreaker: Element-wise Efficient Information Acquisition with a Bayesian Deep Latent Gaussian Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we address the ice-startproblem, i.e., the challenge of deploying machine learning models when only a little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative of the real-world machine learning applications. For instance, in the health-care domain, obtaining every single measurement comes with a cost. We propose Icebreaker, a principled framework for element-wise training data acquisition. Icebreaker introduces a full Bayesian Deep Latent Gaussian Model (BELGAM) with a novel inference method, which combines recent advances in amortized inference and stochastic gradient MCMC to enable fast and accurate posterior inference. By utilizing BELGAM's ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that BELGAM performs significantly better than previous variational autoencoder (VAE) based models, when the data set size is small, using both machine learning benchmarks and real-world recommender systems and health-care applications. Moreover, Icebreaker not only demonstrates improved performance compared to baselines, but it is also capable of achieving better test performance with less training data available.	[Gong, Wenbo; Turner, Richard E.; Hernandez-Lobato, Jos Miguel] Univ Cambridge, Dept Engn, Cambridge, England; [Tschiatschek, Sebastian; Turner, Richard E.; Nowozin, Sebastian; Hernandez-Lobato, Jos Miguel; Zhang, Cheng] Microsoft Res, Cambridge, England; [Nowozin, Sebastian] Google AI, Berlin, Germany	University of Cambridge; Microsoft	Gong, WB (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.; Zhang, C (corresponding author), Microsoft Res, Cambridge, England.	wg242@cam.ac.uk; Cheng.Zhang@microsoft.com						Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Baram Y, 2004, J MACH LEARN RES, V5, P255; Beal M.J., 2003, VARIATIONAL ALGORITH; Chakraborty S, 2013, IEEE DATA MINING, P81, DOI 10.1109/ICDM.2013.69; Chen CY, 2016, JMLR WORKSH CONF PRO, V51, P1051; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dheeru D., 2019, UCI MACHINE LEARNING; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Harutyunyan H., 2017, ARXIV170307771; Havasi M., 2018, ADV NEURAL INFORM PR, P7506; Houlsby N, 2011, STAT; Houlsby N, 2014, PR MACH LEARN RES, V32, P766; Huang S.-J., 2018, ARXIV180205380; Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kapoor A., SELECTIVE SUPERVISIO; Kingma D.P, P 3 INT C LEARNING R; Krause A, 2010, J ARTIF INTELL RES, V39, P633, DOI 10.1613/jair.3089; Krumm J., 2019, TRAFFIC UPDATES SAYI; Lewenberg Y, 2017, AAAI CONF ARTIF INTE, P1396; Li C., 2016, 13 AAAI C ART INT; Li Y, 2018, THESIS; LINDLEY DV, 1956, ANN MATH STAT, V27, P986, DOI 10.1214/aoms/1177728069; Lis`y V., 2017, ARXIV171107364; Ma C., 2019, P INT C MACH LEARN; Ma C., 2018, NIPS WORKSH BAYES DE; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Maltz D., POINTING WAY ACTIVE; McCallum Andrew, 1998, ICML, DOI DOI 10.1023/A:1007692713085; Melville P, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P483, DOI 10.1109/ICDM.2004.10075; Melville P., 2005, 5 IEEE INT C DAT MIN, P4; Nazabal Alfredo, 2018, ARXIV; Pandey NK, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON COMMUNICATION CONTROL AND INTELLIGENT SYSTEMS (CCIS), P213, DOI 10.1109/CCIntelS.2016.7878233; Popkes A.-L., 2019, ARXIV190502599; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Roy N., OPTIMAL ACTIVE LEARN; Ruchansky N, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1025, DOI 10.1145/2783258.2783259; Saar-Tsechansky M, 2009, MANAGE SCI, V55, P664, DOI 10.1287/mnsc.1080.0952; Schein A. I., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P253, DOI 10.1145/564376.564421; Settles B., 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI [10.2200/s00429ed1v01y201207aim018, DOI 10.2200/S00429ED1V01Y201207AIM018]; Shim H., 2018, ADV NEURAL INFORM PR; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Stern D., 2009, INT WORLD WID WEB C; Steyerberg EW, 2008, PLOS MED, V5, P1251, DOI 10.1371/journal.pmed.0050165; Sutherland DJ, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P212; Thahir Mohamed, 2012, BMC Proc, V6 Suppl 7, pS2, DOI 10.1186/1753-6561-6-S7-S2; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Vu D., INTELLIGENT INFORM A; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005; Xu J., 2015, 24 INT JOINT C ART I; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zannone S., 2019, ICML REAL WORLD SEQ; Zhu J., 2017, ARXIV170207956; Zhu X., COMBINING ACTIVE LEA	59	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906047
C	Guo, BC; Han, YX; Wen, JT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Guo, Bichuan; Han, Yuxing; Wen, Jiangtao			AGEM: Solving Linear Inverse Problems via Deep Priors and Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMAGE; ALGORITHM; DOMAIN	In this paper we propose to use a denoising autoencoder (DAE) prior to simultaneously solve a linear inverse problem and estimate its noise parameter. Existing DAE-based methods estimate the noise parameter empirically or treat it as a tunable hyper-parameter. We instead propose autoencoder guided EM, a probabilistically sound framework that performs Bayesian inference with intractable deep priors. We show that efficient posterior sampling from the DAE can be achieved via Metropolis-Hastings, which allows the Monte Carlo EM algorithm to be used. We demonstrate competitive results for signal denoising, image deblurring and image devignetting. Our method is an example of combining the representation power of deep learning with uncertainty quantification from Bayesian statistics.	[Guo, Bichuan; Wen, Jiangtao] Tsinghua Univ, Beijing, Peoples R China; [Han, Yuxing] South China Agr Univ, Guangzhou, Peoples R China	Tsinghua University; South China Agricultural University	Guo, BC (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	gbc16@mails.tsinghua.edu.cn; yuxinghan@scau.edu.cn; jtwen@tsinghua.edu.cn			Natural Science Foundation of China [61521002]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by the Natural Science Foundation of China (Project Number 61521002). We would like to thank Xinyue Liang for discussions on MCMC methods. We also thank the reviewers and the area chair for their valuable comments.	Alain G, 2014, J MACH LEARN RES, V15, P3563; Banham MR, 1997, IEEE SIGNAL PROC MAG, V14, P24, DOI 10.1109/79.581363; Bigdeli S. A., 2017, PROC INT C NEURAL IN, V30, P763; Bigdeli SA, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P33, DOI 10.5220/0006532100330044; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Brifman A, 2016, IEEE IMAGE PROC, P1404, DOI 10.1109/ICIP.2016.7532589; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Chan SH, 2017, IEEE T COMPUT IMAG, V3, P84, DOI 10.1109/TCI.2016.2629286; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dong W., 2018, IEEE TPAMI; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Immerkaer J, 1996, COMPUT VIS IMAGE UND, V64, P300, DOI 10.1006/cviu.1996.0060; Jin M., 2017, P IEEE C COMP VIS PA, P3510; Junyuan X, 2012, ADV NEURAL INF PROCE, P341; Kang S.B., 2000, LNCS, V1843, P640; Knopp T, 2010, IEEE T MED IMAGING, V29, P12, DOI 10.1109/TMI.2009.2021612; Krishnan D., 2009, ADV NEURAL INFORM PR, V22, P1033; Liu W, 2013, IEEE T IMAGE PROCESS, V22, P872, DOI 10.1109/TIP.2012.2219544; Lopez-Fuentes L, 2015, LECT NOTES COMPUT SC, V9095, P450, DOI 10.1007/978-3-319-19222-2_38; Nguyen A., 2017, P IEEE C COMP VIS PA, P4467; Pyatykh S, 2013, IEEE T IMAGE PROCESS, V22, P687, DOI 10.1109/TIP.2012.2221728; Rao AM, 2000, IEEE T SIGNAL PROCES, V48, P1225, DOI 10.1109/78.839971; Rosin PL, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P274, DOI 10.1109/ICCV.1998.710730; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Shah V, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4609; Sonderby Casper Kaae, 2017, ICLR; Steiner B., 2019, ADV NEURAL INFORM PR; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; Wang XR, 2017, INT CONF ACOUST SPEE, P1323, DOI 10.1109/ICASSP.2017.7952371; Wang YK, 2018, J VIS COMMUN IMAGE R, V57, P152, DOI 10.1016/j.jvcir.2018.10.028; WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005; Xu L, 2017, INT GEOSCI REMOTE SE, P3929, DOI 10.1109/IGARSS.2017.8127859; Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zheng YJ, 2009, IEEE T PATTERN ANAL, V31, P2243, DOI 10.1109/TPAMI.2008.263; Zhou Q, 2018, ELECTRON P THEOR COM, P3, DOI 10.4204/EPTCS.278.3	48	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300050
C	Hamelijnck, O; Damoulas, T; Wang, KR; Girolami, MA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hamelijnck, Oliver; Damoulas, Theodoros; Wang, Kangrui; Girolami, Mark A.			Multi-resolution Multi-task Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider evidence integration from potentially dependent observation processes under varying spatio-temporal sampling resolutions and noise levels. We offer a multi-resolution multi-task (MRGP) framework that allows for both inter-task and intra-taskmulti-resolution and multi-fidelity. We develop shallow Gaussian Process (GP) mixtures that approximate the difficult to estimate joint likelihood with a composite one and deep GP constructions that learn mappings between resolutions and naturally handle biases. In doing so, we generalize existing approaches and offer information-theoretic corrections and efficient variational approximations. We demonstrate the competitiveness of MRGPs on synthetic settings and on the challenging problem of hyper-local estimation of air pollution levels across London from multiple sensing modalities operating at disparate spatio-temporal resolutions.	[Hamelijnck, Oliver; Damoulas, Theodoros] Univ Warwick, Dept Comp Sci, Alan Turing Inst, Coventry, W Midlands, England; [Damoulas, Theodoros; Wang, Kangrui] Univ Warwick, Dept Stat, Alan Turing Inst, Coventry, W Midlands, England; [Girolami, Mark A.] Univ Cambridge, Alan Turing Inst, Dept Engn, Cambridge, England	University of Warwick; University of Warwick; University of Cambridge	Hamelijnck, O (corresponding author), Univ Warwick, Dept Comp Sci, Alan Turing Inst, Coventry, W Midlands, England.	ohamelijnck@turing.ac.uk; tdamoulas@turing.ac.uk; kwang@turing.ac.uk; mgirolami@turing.ac.uk		Girolami, Mark/0000-0003-3008-253X	Lloyd's Register Foundation programme on Data Centric Engineering through the London Air Quality project; Alan Turing Institute for Data Science and AI under EPSRC [EP/N510129/1]; Greater London Authority	Lloyd's Register Foundation programme on Data Centric Engineering through the London Air Quality project; Alan Turing Institute for Data Science and AI under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Greater London Authority	O. H., T. D and K.W. are funded by the Lloyd's Register Foundation programme on Data Centric Engineering through the London Air Quality project. This work is supported by The Alan Turing Institute for Data Science and AI under EPSRC grant EP/N510129/1 in collaboration with the Greater London Authority. We would like to thank the anonymous reviewers for their feedback and Libby Rogers, Patrick O'Hara, Daniel Tait and Juan Maronas for their help on multiple aspects of this work.	Adelsberg M., 2018, P KDD 2017 WORKSH AN; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; [Anonymous], 2008, P 25 INT C MACH LEAR; Cutajar K, 2019, NEUR IPS 2018 32 NEU; Damianou A. C, 2013, P 16 INT C ART INT S; Damoulas T, 2019, ARXIV190402063; Deisenroth MP, 2015, PR MACH LEARN RES, V37, P1481; Fox E. B., 2012, P 25 INT C NEUR INF, V1; Fuentes M., 2005, BIOMETRICS; Gelfand AE, 2010, CH CRC HANDB MOD STA, P1, DOI 10.1201/9781420072884; Hensman J., 2013, P 29 C UNC ART INT; Kingma D., 2014, ADAM METHOD STOCHAST; Klinger Tim, 2017, 31 AAAI C ART INT; Krauth K, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Law H. C. L., 2018, ADV NEURAL INFORM PR; Ldizaro-Gredilla M., 2011, P 28 INT C INT C MAC; Lyddon S. P., 2019, BIOMETRIKA; Marecal V., 2015, GEOSCIENTIFIC MODEL; Moreno-Mufioz P., 2018, P 32 INT C NEUR INF; Nguyen T., 2014, P 31 INT C MACH LEAR; Nguyen T., 2013, P 16 INT C ART INT S; Nguyen T. V., 2014, ADV NEURAL INFORM PR, V27; Perdikaris P., 2015, P ROYAL SOC A; Perdikaris P., 2017, P ROYAL SOC A; Rasmussen C. E., 2002, ADV NEURAL INFORM PR, V14; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ribatet M, 2012, STAT SINICA, V22, P813, DOI 10.5705/ss.2009.248; Salimbeni Hugh, 2017, ADV NEURAL INFORM PR; Smith M. T., 2018, ARXIV E PRINTS; Tanaka Yusuke, 2019, NEURIPS 2019; Titsias M.K., 2009, ARTIF INTELL; Varin C., 2011, STAT SINICA; Walder C., 2008, P 25 INT C MACH LEAR; Wilson A. G., 2012, P 29 INT C MACH LEAR; Yousefi F., 2019, NEURIPS 2019; Yuan Chao, 2009, ADV NEURAL INFORM PR	36	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905067
C	Hand, P; Joshi, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hand, Paul; Joshi, Babhru			Global Guarantees for Blind Demodulation with Generative Priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIGNAL RECOVERY; DECONVOLUTION; ALGORITHMS	We study a deep learning inspired formulation for the blind demodulation problem, which is the task of recovering two unknown vectors from their entrywise multiplication. We consider the case where the unknown vectors are in the range of known deep generative models, G((1)) : R-n -> R-l and G((2)) : R-p -> R-l. In the case when the networks corresponding to the generative models are expansive, the weight matrices are random and the dimension of the unknown vectors satisfy l = Omega(n(2) + p(2)), up to log factors, we show that the empirical risk objective has a favorable landscape for optimization. That is, the objective function has a descent direction at every point outside of a small neighborhood around four hyperbolic curves. We also characterize the local maximizers of the empirical risk objective and, hence, show that there does not exist any other stationary points outside of these neighborhood around four hyperbolic curves and the set of local maximizers. We also implement a gradient descent scheme inspired by the geometry of the landscape of the objective function. In order to converge to a global minimizer, this gradient descent scheme exploits the fact that exactly one of the hyperbolic curve corresponds to the global minimizer, and thus points near this hyperbolic curve have a lower objective value than points close to the other spurious hyperbolic curves. We show that this gradient descent scheme can effectively remove distortions synthetically introduced to the MNIST dataset.	[Hand, Paul] Northeastern Univ, Dept Math, Boston, MA 02115 USA; [Hand, Paul] Northeastern Univ, Coll Comp Sci & Informat, Boston, MA 02115 USA; [Joshi, Babhru] Univ British Columbia, Dept Math, Vancouver, BC, Canada	Northeastern University; Northeastern University; University of British Columbia	Hand, P (corresponding author), Northeastern Univ, Dept Math, Boston, MA 02115 USA.; Hand, P (corresponding author), Northeastern Univ, Coll Comp Sci & Informat, Boston, MA 02115 USA.	p.hand@northeastern.edu; b.joshi@math.ubc.ca						Aghasi A, 2020, APPL COMPUT HARMON A, V49, P636, DOI 10.1016/j.acha.2019.03.002; Aghasi A, 2018, ADV NEUR IN, V31; Aghasi A, 2016, OPTICA, V3, P754, DOI 10.1364/OPTICA.3.000754; Asim Muhammad, 2018, CORR; Bahmani S., 2016, P INT C ART INT STAT; Cand~s E., 2012, FOUND COMPUT MATH, P1; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Goldstein Tom, 2016, ARXIV161007531; Hand P., 2017, ARXIV170507576; Hand P, 2018, ADV NEUR IN, V31; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Jalal A., 2017, COMPRESSED SENSING U; Jeong H, 2019, 2019 13TH INTERNATIONAL CONFERENCE ON SAMPLING THEORY AND APPLICATIONS (SAMPTA); Karras T., 2017, PROGR GROWING GANS I; Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268; Lee DD, 2001, ADV NEUR IN, V13, P556; Lee Kiryung, 2017, ARXIV170204342; Li X., 2016, ARXIV160604933V1; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002; Lohit S, 2018, IEEE T COMPUT IMAG, V4, P326, DOI 10.1109/TCI.2018.2846413; Oord A.V.D., 2016, SSW; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Shamshad Fahad, 2018, ABS181211065 CORR; Sonderby C.K., 2017, ICLR, P1; STOCKHAM TG, 1975, P IEEE, V63, P678, DOI 10.1109/PROC.1975.9800; Vershynin R., 2012, COMPRESSEDSENSING TH; Voroninski V., 2016, COMPRESSED SENSING P; Wang, 2016, ARXIV PREPRINT ARXIV; Zhu Jun-Yan, 2017, ICCV	35	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903020
C	Hao, Y; Orlitsky, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hao, Yi; Orlitsky, Alon			The Broad Optimality of Profile Maximum Likelihood	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMAGE REGISTRATION; ENTROPY; SAMPLE; NUMBER; DISTRIBUTIONS; POLYNOMIALS; UNSEEN	We study three fundamental statistical-learning problems: distribution estimation, property estimation, and property testing. We establish the profile maximum likelihood (PML) estimator as the first unified sample-optimal approach to a wide range of learning tasks. In particular, for every alphabet size k and desired accuracy epsilon: Distribution estimation Under l(1) distance, PML yields optimal Theta(k/(epsilon(2)logk)) sample complexity for sorted-distribution estimation, and a PML-based estimator empirically outperforms the Good-Turing estimator on the actual distribution; Additive property estimation For a broad class of additive properties, the PML plug-in estimator uses just four times the sample size required by the best estimator to achieve roughly twice its error, with exponentially higher confidence; alpha-Renyi entropy estimation For integer alpha > 1, the PML plug-in estimator has optimal k(1-1/alpha) sample complexity; for non-integer alpha > 3/4, the PML plug-in estimator has sample complexity lower than the state of the art; Identity testing In testing whether an unknown distribution is equal to or at least e far from a given distribution in l(1) distance, a PML-based tester achieves the optimal sample complexity up to logarithmic factors of k. Most of these results also hold for a near-linear-time computable variant of PML. Stronger results hold for a different and novel variant called truncated PML (TPML).	[Hao, Yi; Orlitsky, Alon] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Hao, Y (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.	yih179@ucsd.edu; alon@ucsd.edu			National Science Foundation (NSF) [CIF-1564355, CIF-1619448]	National Science Foundation (NSF)(National Science Foundation (NSF))	We are grateful to the National Science Foundation (NSF) for supporting this work through grants CIF-1564355 and CIF-1619448.	Acharya J., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1628, DOI 10.1109/ISIT.2012.6283551; Acharya J, 2017, PR MACH LEARN RES, V70; Acharya J, 2018, IEEE INT SYMP INFO, P326; Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435; Acharya J, 2010, IEEE INT SYMP INFO, P1498, DOI 10.1109/ISIT.2010.5513565; Acharya  Jayadev, 2015, ADV NEURAL INFORM PR, P3591; Anevski D, 2017, ANN STAT, V45, P2708, DOI 10.1214/17-AOS1542; Armananzas R, 2008, BIODATA MIN, V1, DOI 10.1186/1756-0381-1-6; Batu T, 2001, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.2001.959920; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Batu T, 2017, ANN IEEE SYMP FOUND, P880, DOI 10.1109/FOCS.2017.86; Braess D, 2004, J APPROX THEORY, V128, P187, DOI 10.1016/j.jat.2004.04.010; Bresler G, 2015, ACM S THEORY COMPUT, P771, DOI 10.1145/2746539.2746631; Canonne C. L., 2017, YOUR DATA IS BIG IS; CARLTON AG, 1969, PSYCHOL BULL, V71, P108, DOI 10.1037/h0026857; CHAO A, 1984, SCAND J STAT, V11, P265; CHAO A, 1992, J AM STAT ASSOC, V87, P210, DOI 10.2307/2290471; Chao A, 2016, WILEY STATSREF STAT, V1, P26, DOI [10.1002/9780470015902.a0026329, 10.1002/9781118445112.stat03432.pub2, DOI 10.1002/9780470015902.A0026329]; Charikar M, 2019, ACM S THEORY COMPUT, P780, DOI 10.1145/3313276.3316398; Chen SF, 1999, COMPUT SPEECH LANG, V13, P359, DOI 10.1006/csla.1999.0128; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Colwell RK, 2012, J PLANT ECOL, V5, P3, DOI 10.1093/jpe/rtr044; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Das H., 2012, THESIS; Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78; Diakonikolas Ilias, 2015, P TWENTYSIXTH ANN AC, P1841, DOI [10.1137/1.9781611973730.123, DOI 10.1137/1.9781611973730.123]; EFRON B, 1976, BIOMETRIKA, V63, P435, DOI 10.2307/2335721; Gerstner W., 2002, SPIKING NEURON MODEL; Goldreich, 2017, INTRO PROPERTY TESTI; GOLDREICH O, 2000, EL C COMP COMPL ECCC; GOLDREICH ODED, 2016, ELECT C COMPUTATIONA, V23, P1; GOOD IJ, 1956, BIOMETRIKA, V43, P45; GRASSBERGER P, 1988, PHYS LETT A, V128, P369, DOI 10.1016/0375-9601(88)90193-4; Haas P. J., 1995, VLDB '95. Proceedings of the 21st International Conference on Very Large Data Bases, P311; Han Yanjun, 2018, P MACH LEARN RES, V75, P3189; Hao Y., 2018, P 32 INT C NEURAL IN, P8848; Hao Y., 2019, ADV NEURAL INFORM PR, P11104; Hao Y., 2019, ARXIV PREPRINT ARXIV; Hao Y, 2019, PR MACH LEARN RES, V97; Hao Y, 2018, IEEE INT SYMP INFO, P1076; Jenssen R, 2003, IEEE IJCNN, P523; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Kallberg David, 2012, Conceptual Modelling and Its Theoretical Foundations. Essays Dedicated to Bernhard Thalheim on the Occasion of his 60th Birthday, P36, DOI 10.1007/978-3-642-28279-9_5; KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331; Kroes I, 1999, P NATL ACAD SCI USA, V96, P14547, DOI 10.1073/pnas.96.25.14547; Ma B, 2000, IEEE IMAGE PROC, P481, DOI 10.1109/ICIP.2000.901000; MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778; Mao CX, 2007, ANN STAT, V35, P917, DOI 10.1214/009053606000001280; MCNEIL DR, 1973, J AM STAT ASSOC, V68, P92, DOI 10.2307/2284147; Neemuchwala H, 2006, INT J IMAG SYST TECH, V16, P130, DOI 10.1002/ima.20079; Obremski M., 2017, APPROXIMATION RANDOM; Orlitsky A, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P306; Orlitsky A., 2004, PROC 20 C UNCERTAINT, P426; Orlitsky A., 2015, ADV NEURAL INFORM PR, P2143; Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113; Orlitsky Alon, 2015, C LEARN THEOR, P1066; Pan S., 2012, THESIS; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987; Pavlichin D. S., 2017, ARXIV171207177; Price, 2018, P 45 INT C AUT LANG; Quinn CJ, 2013, IEEE T SIGNAL PROCES, V61, P3173, DOI 10.1109/TSP.2013.2259161; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Ron Dana, 2010, THEORETICAL COMPUTER, V5, P2; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; THISTED R, 1987, BIOMETRIKA, V74, P445, DOI 10.1093/biomet/74.3.445; Valiant G, 2016, ACM S THEORY COMPUT, P142, DOI 10.1145/2897518.2897641; Valiant G, 2017, SIAM J COMPUT, V46, P429, DOI 10.1137/151002526; Valiant G, 2017, J ACM, V64, DOI 10.1145/3125643; Valiant G, 2011, ACM S THEORY COMPUT, P685; Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81; Vontobel PO, 2012, IEEE INT SYMP INFO; Vontobel PO, 2014, 2014 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA), P489; Wu YH, 2019, ANN STAT, V47, P857, DOI 10.1214/17-AOS1665; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; Xu D., 1999, THESIS; Xu DX, 2010, INFORM SCI STAT, P47, DOI 10.1007/978-1-4419-1570-2_2	77	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902060
C	Heikkila, MA; Jalko, J; Dikmen, O; Honkela, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Heikkila, Mikko A.; Jalko, Joonas; Dikmen, Onur; Honkela, Antti			Differentially Private Markov Chain Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent developments in differentially private (DP) machine learning and DP Bayesian learning have enabled learning under strong privacy guarantees for the training data subjects. In this paper, we further extend the applicability of DP Bayesian learning by presenting the first general DP Markov chain Monte Carlo (MCMC) algorithm whose privacy-guarantees are not subject to unrealistic assumptions on Markov chain convergence and that is applicable to posterior inference in arbitrary models. Our algorithm is based on a decomposition of the Barker acceptance test that allows evaluating the Renyi DP privacy cost of the accept-reject choice. We further show how to improve the DP guarantee through data subsampling and approximate acceptance tests.	[Heikkila, Mikko A.] Univ Helsinki, Helsinki Inst Informat Technol HIIT, Dept Math & Stat, Helsinki, Finland; [Jalko, Joonas] Aalto Univ, Dept Comp Sci, HIIT, Espoo, Finland; [Dikmen, Onur] Halmstad Univ, Ctr Appl Intelligent Syst Res CAISR, Halmstad, Sweden; [Honkela, Antti] Univ Helsinki, HIIT, Dept Comp Sci, Helsinki, Finland	Aalto University; University of Helsinki; Aalto University; Halmstad University; Aalto University; University of Helsinki	Heikkila, MA (corresponding author), Univ Helsinki, Helsinki Inst Informat Technol HIIT, Dept Math & Stat, Helsinki, Finland.	mikko.a.heikkila@helsinki.fi; joonas.jalko@aalto.fi; onur.dikmen@hh.se; antti.honkela@helsinki.fi		Heikkila, Mikko/0000-0001-5753-8643	Academy of Finland [294238, 303815, 313124]	Academy of Finland(Academy of Finland)	This work has been supported by the Academy of Finland [Finnish Center for Artificial Intelligence FCAI and grants 294238, 303815, 313124].	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Bardenet R, 2017, J MACH LEARN RES, V18, P1; Bardenet R, 2014, PR MACH LEARN RES, V32; BARKER AA, 1965, AUST J PHYS, V18, P119, DOI 10.1071/PH650119; Bernstein Garrett, 2018, ADV NEURAL INFORM PR, V31, P2924; Betancourt M, 2015, PR MACH LEARN RES, V37, P533; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Dimitrakakis C, 2017, J MACH LEARN RES, V18; Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21; Dwork C., 2009, J PRIVACY CONFIDENTI, V1, P135; Dwork C., 2016, ARXIV PREPRINT ARXIV; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Foulds J., 2016, P 32 C UNC ART INT U, P192; GEUMLEK J, 2017, PROCEED INGS OF THE, P5295; Gil M, 2013, INFORM SCIENCES, V249, P124, DOI 10.1016/j.ins.2013.06.018; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Honkela A, 2018, BIOL DIRECT, V13, DOI 10.1186/s13062-017-0203-4; Jalko J, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Korattikara A, 2014, PR MACH LEARN RES, V32; Li B, 2019, PR MACH LEARN RES, V89, P557; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Miller JW, 2019, J AM STAT ASSOC, V114, P1113, DOI 10.1080/01621459.2018.1469995; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Park James, 2016, ARXIV161100340; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Seita D, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Wang YX, 2015, PR MACH LEARN RES, V37, P2493; Wang Yu-Xiang, 2019, INT C ART INT STAT A, P2; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Yildirim S, 2019, STAT COMPUT, V29, P947, DOI 10.1007/s11222-018-9847-x; Zhang ZH, 2016, AAAI CONF ARTIF INTE, P2365	34	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304015
