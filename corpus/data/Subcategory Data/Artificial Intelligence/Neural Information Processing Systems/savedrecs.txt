PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Goodfellow, IJ; Pouget-Abadie, J; Mirza, M; Xu, B; Warde-Farley, D; Ozair, S; Courville, A; Bengio, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua			Generative Adversarial Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.	[Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua] Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Goodfellow, IJ (corresponding author), Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada.			Goodfellow, Ian/0000-0003-3937-2322	CIFAR; Canada Research Chairs; Google Fellowship in Deep Learning	CIFAR(Canadian Institute for Advanced Research (CIFAR)); Canada Research Chairs(Canada Research ChairsCGIAR); Google Fellowship in Deep Learning(Google Incorporated)	We would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2 [11] and Theano [6, 1], particularly Frederic Bastien who rushed a Theano feature specifically to benefit this project. Arnaud Bergeron provided much-needed support with LATEX typesetting. We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Quebec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity.	[Anonymous], 2010, PYTH SCI COMP C; Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y., 2014, ICML 14; Bengio Y., 2014, P 30 INT C MACH LEAR; Bengio Y., 2013, ICML 13; Bengio YJF, 2009, LEARNING DEEP ARCHIT; Glorot X., 2011, AISTATS 2011; Goodfellow I. J., 2013, NIPS 2013; Goodfellow I.J., 2013, ARXIV13084214; Goodfellow IJ, 2013, ICML 2013; Gregor K., 2014, ICML 2014; Gutmann Michael, 2010, AISTATS; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Kingma D., P INT C LEARN REPR I, DOI DOI 10.1145/1830483.1830503; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2012, NIPS 2012; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mnih Andriy, 2014, INT C MACH LEARN; Rezende D.J., 2014, PROC INT CONFER ENCE; Rifai S., 2012, ICML 12; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Susskind J, 2010, 2010001 UTML TR; Szegedy C., 2014, ABS13126199 ICLR; Tieleman T., 2012, LECTURE 6 5 MSPROP C; Tu Z, 2007, PROC CVPR IEEE, P500	29	23733	25164	384	1592	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27						2672	2680						9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR		Green Submitted, Bronze			2022-12-19	WOS:000452647101094
C	Ren, SQ; He, KM; Girshick, R; Sun, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ren, Shaoqing; He, Kaiming; Girshick, Ross; Sun, Jian			Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate highquality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_ rcnn.	[Ren, Shaoqing; He, Kaiming; Girshick, Ross; Sun, Jian] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Ren, SQ (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	v-shren@microsoft.com; kahe@microsoft.com; rbg@microsoft.com; jiansun@microsoft.com						[Anonymous], 2007, PASCAL VISUAL OBJECT; Chavali N., 2015, ARXIV150505836; Dai J., 2015, CVPR; Eigen D., 2014, 6 INT C LEARN REPR I; Erhan D., 2014, CVPR; Girshick R., 2015, ARXIV150408083; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; He K., 2014, ECCV; Hosang J., 2014, BMVC; Hosang J., 2015, ARXIV150205082; Jia Y., P ACM MULT, P675; Krizhevsky A., 2012, ADV NEURAL INF PROCE; LeCun Y., 1989, NEURAL COMPUTATION; Lenc K., 2015, ARXIV150606981; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Nair V., 2010, P 27 INT C MACHINE L, P807, DOI DOI 10.5555/3104322.3104425; Ren S., 2015, ARXIV150406066; Russakovsky O., 2014, ARXIV, P1, DOI [10.48550/arXiv.1409.0575, DOI 10.48550/ARXIV.1409.0575]; Simonyan Karen, 2015, INT C LEARN REPR; Szegedy C., 2015, ARXIV14121441V2; Szegedy C., 2013, NIPS; Uijlings J., 2013, IJCV; Zeiler MD, 2014, ECCV; Zitnick C. L., 2014, ECCV	24	21350	24181	324	830	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ	27295650	Green Submitted			2022-12-19	WOS:000450913100006
C	Vaswani, A; Shazeer, N; Parmar, N; Uszkoreit, J; Jones, L; Gomez, AN; Kaiser, L; Polosukhin, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia			Attention Is All You Need	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.	[Vaswani, Ashish; Shazeer, Noam; Kaiser, Lukasz; Polosukhin, Illia] Google Brain, Mountain View, CA 94043 USA; [Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Polosukhin, Illia] Google Res, Mountain View, CA USA; [Gomez, Aidan N.] Univ Toronto, Toronto, ON, Canada	Google Incorporated; Google Incorporated; University of Toronto	Vaswani, A (corresponding author), Google Brain, Mountain View, CA 94043 USA.	avaswani@google.com; noam@google.com; nikip@google.com; usz@google.com; llion@google.com; aidan@cs.toronto.edu; lukaszkaiser@google.com; illia.polosukhin@gmail.com	Jeong, Yongwook/N-7413-2016					[Anonymous], 2016, EMPIRICAL METHODS NA; Bengio Y., 2014, ARXIV14061078; Britz Denny, 2017, ARXIV170303906; Cheng Jianpeng, 2016, EMPIRICAL METHODS NA; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Chung J., 2014, ARXIV14123555; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gehring J., 2017, P ICML; Graves A, 2013, ARXIV13080850; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jozefowicz Rafal, 2016, ARXIV160202410; Kaiser Lukasz, 2016, ICLR; Kaiser Samy Bengio Lukasz, 2016, ADV NEURAL INFORM PR; Kalchbrenner Nal, 2017, ICML, P2; Kim Y., 2017, ABS170200887 CORR; Kingma D.P, P 3 INT C LEARNING R; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Kuchaiev Oleksii, 2017, ARXIV170310722; Lin Z., 2017, ARXIV PREPRINT ARXIV; Luong M., 2015, ARXIV150804025; Paulus Romain, 2017, ARXIV170504304; Press Ofir, 2016, ARXIV160805859; Sennrich Rico, 2015, ARXIV150807909; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Zhou Jie, 2016, ABS160604199 CORR	32	10917	10986	670	1956	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406008
C	Paszke, A; Gross, S; Massa, F; Lerer, A; Bradbury, J; Chanan, G; Killeen, T; Lin, ZM; Gimelshein, N; Antiga, L; Desmaison, A; Kopf, A; Yang, E; DeVito, Z; Raison, M; Tejani, A; Chilamkurthy, S; Steiner, B; Fang, L; Bai, JJ; Chintala, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Paszke, Adam; Gross, Sam; Massa, Francisco; Lerer, Adam; Bradbury, James; Chanan, Gregory; Killeen, Trevor; Lin, Zeming; Gimelshein, Natalia; Antiga, Luca; Desmaison, Alban; Kopf, Andreas; Yang, Edward; DeVito, Zach; Raison, Martin; Tejani, Alykhan; Chilamkurthy, Sasank; Steiner, Benoit; Fang, Lu; Bai, Junjie; Chintala, Soumith			PyTorch: An Imperative Style, High-Performance Deep Learning Library	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.	[Paszke, Adam] Univ Warsaw, Warsaw, Poland; [Gross, Sam; Massa, Francisco; Lerer, Adam; Chanan, Gregory; Lin, Zeming; Yang, Edward; DeVito, Zach; Steiner, Benoit; Chintala, Soumith] Facebook AI Res, Menlo Pk, CA USA; [Bradbury, James] Google, Mountain View, CA 94043 USA; [Gimelshein, Natalia] NVIDIA, Santa Clara, CA USA; [Antiga, Luca] Orobix, Bergamo, Italy; [Desmaison, Alban] Univ Oxford, Oxford, England; [Kopf, Andreas] Xamla, Westphalia, Germany; [Raison, Martin] Nabla, Paris, France; [Tejani, Alykhan] Twitter, San Francisco, CA USA; [Chilamkurthy, Sasank] Qure Ai, Mumbai, Maharashtra, India; [Fang, Lu; Bai, Junjie] Facebook, Menlo Pk, CA USA	University of Warsaw; Facebook Inc; Google Incorporated; Nvidia Corporation; University of Oxford; Twitter, Inc.; Facebook Inc	Paszke, A (corresponding author), Univ Warsaw, Warsaw, Poland.	adam.paszke@gmail.com; sgross@fb.com; fmassa@fb.com; alerer@fb.com; jekbradbury@gmail.com; gchanan@fb.com; killeent@cs.washington.edu; zlin@fb.com; ngimelshein@nvidia.com; luca.antiga@orobix.com; alban@robots.ox.ac.uk; andreas.koepf@xamla.com; ezyang@fb.com; zdevito@cs.stanford.edu; martinraison@gmail.com; atejani@twitter.com; sasankchilamkurthy@gmail.com; benoitsteiner@fb.com; lufang@fb.com; jbai@fb.com; soumith@gmail.com						Abadi M, 2015, P 12 USENIX S OPERAT; Abrams Philip S., 1970, THESIS; [Anonymous], MAXDNN EFFICIENT CON; Baydin A.G., 2017, J MACH LEARN RES, V18, P5595; Bezanson J, 2017, SIAM REV, V59, P65, DOI 10.1137/141000671; Chetlur S., 2014, ARXIV; Collobert R, 2002, TECHNICAL REPORT; Collobert R., 2011, NIPS; DMLC, DLPACK OP MEM TENS S; Evans J., 2006, BSDCAN TECHN BSD C M; Fabian JH, 2000, MESA MG, P117; Gabriel Richard, RISE WORSE IS BETTER; Ghemawat S., TCMALLOC THREAD CACH; Hertz M, 2005, ACM SIGPLAN NOTICES, V40, P313, DOI 10.1145/1103845.1094836; Huang Austin, JUNJI HASHIMOTO SAM; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Johnson Matthew, 2018, JAX; Jones E., 2001, SCIPY OPEN SOURCE SC; Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435; LeCun Y, 2002, TECHNICAL REPORT; LeCun Yann, MNIST HANDWRITTEN DI; Leuck Holger, 1999, IEEE COMP SOC C COMP, P2360; Luitjens Justin, 2014, GPU TECHN C; Maclaurin D., 2016, THESIS HARVARD U; MATLAB and Statistics Toolbox, MATLAB STAT TOOLB; McKinney W., 2010, P 9 PYTH SCI C, P56, DOI [10.25080/Majora-92bf1922-012, 10.25080/Majora-92bf1922-00a, DOI 10.25080/MAJORA-92BF1922-00A]; Neubig Graham, 2017, ARXIV170103980; Oliphant T.E., 2010, NUMPY GUIDE NUMPY, V1, P378, DOI [10.1016/j.jmoldx.2015.02.001, DOI 10.1016/J.JMOLDX.2015.02.001]; Paszke A., 2017, AUTOMATIC DIFFERENTI; Petrantoni Giovanni, NIMTORCH; Piponi D., 2004, Journal of Graphics Tools, V9, P41, DOI 10.1080/10867651.2004.10504901; R Core Team, R LANG ENV STAT COMP; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Seide F, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2135, DOI 10.1145/2939672.2945397; Sermanet P, 2009, PROC INT C TOOLS ART, P693; Synnaeve G, 2018, PROC INT C NEURAL IN, P10761; The Python team, CPYTH GLOB INT LOCK; The PyTorch team, PYT AUT PROF; The PyTorch team, TORCH SCRIPT; Theano Development Team, 2016, ARXIV160502688 THEAN; Tokui Seiya, 2015, P WORKSH MACH LEARN, V5, P1; Vinyals Oriol, 2017, ARXIV170804782	43	5201	5204	64	240	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308009
C	Lee, DD; Seung, HS		Leen, TK; Dietterich, TG; Tresp, V		Lee, DD; Seung, HS			Algorithms for non-negative matrix factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				DESCENT	Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the Expectation-Maximization algorithm. The algorithms can also be interpreted as diagonally resealed gradient descent, where the resealing factor is optimally chosen to ensure convergence.	Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T	Lee, DD (corresponding author), Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.		Lee, Daniel D./B-5753-2013; Patanaik, Amiya/G-4336-2010	Lee, Daniel/0000-0003-4239-8777				Bouman CA, 1996, IEEE T IMAGE PROCESS, V5, P480, DOI 10.1109/83.491321; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559; FOLDIAK P, 1995, HDB BRAIN THEORY NEU, P895; Gersho A., 1992, Vector quantization and signal compression; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee DD, 1997, ADV NEUR IN, V9, P515; LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605; Paatero P, 1997, CHEMOMETR INTELL LAB, V37, P23, DOI 10.1016/S0169-7439(96)00044-5; Press WH, 1993, NUMERICAL RECIPES AR; RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055; SAUL L, 1997, P 2 C EMP METH NAT L, P81; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71	16	4879	5134	8	171	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						556	562						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800079
C	Ng, AY; Jordan, MI; Weiss, Y		Dietterich, TG; Becker, S; Ghahramani, Z		Ng, AY; Jordan, MI; Weiss, Y			On spectral clustering: Analysis and an algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Despite many empirical successes of spectral clustering methods-algorithms that cluster points using eigenvectors of matrices derived from the data-there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.	Univ Calif Berkeley, CS Div, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ng, AY (corresponding author), Univ Calif Berkeley, CS Div, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					Alpert CJ, 1999, DISCRETE APPL MATH, V90, P3, DOI 10.1016/S0166-218X(98)00083-3; CHRISTIANINI N, 2002, NEURAL INFORMATION P, V14; Chung F.R.K., 1997, CBMS REGIONAL C SERI, V92; Kannan R., 2000, P 41 ANN S FDN COMP; MALIK J, 2000, PERCEPTUAL ORG ARTIF; MEILA M, 2001, NEURAL INFORMATION P, V13; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SCOTT GL, 1990, P BRIT MACH VIS C; Spielman Daniel Alan, 1996, P 37 ANN S FDN COMP; Stewart G., 1990, MATRIX PERTURBATION; WEISS YY, 1999, INT C COMP VIS	11	4543	4790	6	137	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						849	856						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100106
C	Lundberg, SM; Lee, SI		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lundberg, Scott M.; Lee, Su-In			A Unified Approach to Interpreting Model Predictions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.	[Lundberg, Scott M.] Univ Washington, Paul G Allen Sch Comp Sci, Seattle, WA 98105 USA; [Lee, Su-In] Univ Washington, Dept Genome Sci, Paul G Allen Sch Comp Sci, Seattle, WA 98105 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Lundberg, SM (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci, Seattle, WA 98105 USA.	slund1@cs.washington.edu; suinlee@cs.washington.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation (NSF) [DBI-135589]; NSF CAREER [DBI-155230]; American Cancer Society [127332-RSG-15-097-01-TBG]; National Institute of Health (NIH) [AG049196]; NSF Graduate Research Fellowship	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); American Cancer Society(American Cancer Society); National Institute of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF Graduate Research Fellowship(National Science Foundation (NSF))	This work was supported by a National Science Foundation (NSF) DBI-135589, NSF CAREER DBI-155230, American Cancer Society 127332-RSG-15-097-01-TBG, National Institute of Health (NIH) AG049196, and NSF Graduate Research Fellowship. We would like to thank Marco Ribeiro, Erik Strumbelj, Avanti Shrikumar, Yair Zick, the Lee Lab, and the NIPS reviewers for feedback that has significantly improved this work.	Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Charnes A, 1988, CHEBYCHEV SHAPLEY VA, P123; Datta A, 2016, P IEEE S SECUR PRIV, P598, DOI 10.1109/SP.2016.42; Lipovetsky S, 2001, APPL STOCH MODEL BUS, V17, P319, DOI 10.1002/asmb.446; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Shapley L. S., 1953, CONTRIBUTIONS THEORY, V28, P307, DOI [10.3390/atmos10110723, DOI 10.1515/9781400881970-018]; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Shrikumar Avanti, 2016, ARXIV160501713; Strumbelj E, 2014, KNOWL INF SYST, V41, P647, DOI 10.1007/s10115-013-0679-x; Young H. P., 1985, International Journal of Game Theory, V14, P65, DOI 10.1007/BF01769885	10	4003	4029	80	80	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404081
C	Drucker, H; Burges, CJC; Kaufman, L; Smola, A; Vapnik, V		Mozer, MC; Jordan, MI; Petsche, T		Drucker, H; Burges, CJC; Kaufman, L; Smola, A; Vapnik, V			Support vector regression machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space.			Drucker, H (corresponding author), AT&T BELL LABS,W LONG BRANCH,NJ 07764, USA.								0	2689	2779	13	147	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						155	161						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00022
C	Belkin, M; Niyogi, P		Dietterich, TG; Becker, S; Ghahramani, Z		Belkin, M; Niyogi, P			Laplacian eigenmaps and spectral techniques for embedding and clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.	Univ Chicago, Dept Math, Chicago, IL 60637 USA	University of Chicago	Belkin, M (corresponding author), Univ Chicago, Dept Math, Hyde Pk, Chicago, IL 60637 USA.							Chung F., 1997, REGIONAL C SERIES MA, V92; CHUNG FRK, IN PRESS COMMUNICATI; ROSENBERG S, 1997, LAPLACIAN RIEMMANNIA; ROWEIS ST, 2000, SCIENCE, V290; Shi J., 2000, IEEE T PAMI, V22; TENEBAUM JB, 2000, SCIENCE, V290	6	2502	2715	5	92	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						585	591						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100073
C	Salimans, T; Goodfellow, I; Zaremba, W; Cheung, V; Radford, A; Chen, X		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Salimans, Tim; Goodfellow, Ian; Zaremba, Wojciech; Cheung, Vicki; Radford, Alec; Chen, Xi			Improved Techniques for Training GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21:3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.				tim@openai.com; ian@openai.com; woj@openai.com; vicki@openai.com; alec@openai.com; peter@openai.com						Abadi M, 2015, P 12 USENIX S OPERAT; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Denton Emily L, 2015, NEURIPS, V2, P4; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Fukumizu K., 2007, ADV NEURAL INF PROCE, P489; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I. J., 2014, ARXIV14126515; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Im D., 2016, GENERATING IMAGES RE; Kingma D. P, 2014, ARXIV13126114; Li Yujia, 2015, ARXIV151105493; Maaloe L, 2016, PR MACH LEARN RES, V48; Miyato T, 2015, ARXIV150700677; Odena A., 2016, SEMISUPERVISED LEARN; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Springenberg Jost Tobias, 2015, ARXIV151106390; Sutskever I., 2015, ARXIV PREPRINT ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Warde-Farley D, 2016, NEURAL INF PROCESS S, P311; Yoo D, 2016, ARXIV160307442; Zhao J., 2015, ARXIV150602351	28	2398	2473	6	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700089
C	Sutton, RS; McAllester, D; Singh, S; Mansour, Y		Solla, SA; Leen, TK; Muller, KR		Sutton, RS; McAllester, D; Singh, S; Mansour, Y			Policy gradient methods for reinforcement learning with function approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.	AT&T Labs Res, Florham Park, NJ 07932 USA	AT&T	Sutton, RS (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Park, NJ 07932 USA.		Ma, Jialin/ABG-2965-2021					Baird III L. C., 1993, WLTR931146; Baird L., 1995, P 12 INT C MACH LEAR, P30, DOI DOI 10.1016/B978-1-55860-377-6.50013-X; BAIRD L, 1999, NIPS, V11; Barto A.G., 1983, IEEE T SYST MAN CYB, V13, P835; BAXTER J, UNPUB DIRECT GRADIEN; Cao XR, 1997, IEEE T AUTOMAT CONTR, V42, P1382, DOI 10.1109/9.633827; Dayan P., 1991, CONNECTIONIST MODELS, P45; Gordon G., 1996, CHATTERING SARSA LAM; GORDON GJ, 1995, P 12 INT C MACH LEAR, P261, DOI DOI 10.1016/B978-1-55860-377-6.50040-2; JAAKKOLA T, 1995, NIPS, V7, P345; Kimura H., 1998, P 15 INT C MACH LEAR, P278; KONDA VR, UNPUB ACTOR CRITIC A; Marbach P., 1998, LIDSP2411 MIT; Singh SP., 1994, MACH LEARN PROC, V1994, P284, DOI [10.1016/c2009-0-27542-8, DOI 10.1016/C2009-0-27542-8]; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tsitsiklis JN, 1996, MACH LEARN, V22, P59, DOI 10.1007/BF00114724; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; WILLIAMS RJ, 1988, NUCCS883 COLL COMP S	20	2293	2389	14	88	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1057	1063						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700149
C	Yosinski, J; Clune, J; Bengio, Y; Lipson, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yosinski, Jason; Clune, Jeff; Bengio, Yoshua; Lipson, Hod			How transferable are features in deep neural networks ?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Many deep neural networks trained on natural images exhibit a curious phenomenon in common on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.	[Yosinski, Jason] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Clune, Jeff] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA; [Bengio, Yoshua] Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ, Canada; [Lipson, Hod] Cornell Univ, Dept Mech & Aerosp Engn, Ithaca, NY 14853 USA	Cornell University; University of Wyoming; Universite de Montreal; Cornell University	Yosinski, J (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.				NASA Space Technology Research Fellowship; DARPA [W911NF-12-1-0449]; NSERC; CIFAR; Ubisoft	NASA Space Technology Research Fellowship; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Ubisoft	The authors would like to thank Kyunghyun Cho and Thomas Fuchs for helpful discussions, Joost Huizinga, Anh Nguyen, and Roby Velez for editing, as well as funding from the NASA Space Technology Research Fellowship (JY), DARPA project W911NF-12-1-0449, NSERC, Ubisoft, and CIFAR (YB is a CIFAR Fellow).	Bengio Y., 2011, JMLR W CP P AISTATS; Bengio Y., 2011, JMLR W CP P UNS TRAN; Caruana R., 1995, LEARNING MANY RELATE, P657; Donahue J., 2013, 31 INT C MACH LEARN; Girshick R., 2014, P IEEE C COMPUTER VI, DOI 10.1109/CVPR.2014.81; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Jia Y., P ACM MULT, P675; Krizhevsky A., 2012, ADV NEURAL INFORM PR; Le Q.V., 2011, NEURIPS, P1017; Lee H., 2009, P 26 ANN INT C MACH; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li K., 2009, CVPR09; Sermanet P., 2014, P 2 INT C LEARN REPR; Zeiler M. D., 2014, EUR C COMP VIS, P818	14	1983	2009	43	199	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101018
C	Zhou, DY; Bousquet, O; Lal, TN; Weston, J; Scholkopf, B		Thrun, S; Saul, K; Scholkopf, B		Zhou, DY; Bousquet, O; Lal, TN; Weston, J; Scholkopf, B			Learning with local and global consistency	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Zhou, DY (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				Anderson J.R., 1983, ARCHITECTURE COGNITI; [Anonymous], 2001, NIPS; BELKIN M, IN PRESS MACHINE LEA; Blum A, 2001, ICML; Chapelle O, 2002, NIPS; Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458; Joachims T., 2003, ICML; Kandola J., 2002, NIPS; Kondor R. I., 2002, ICML; Seeger M., 2000, LEARNING LABELED UNL; SHRAGER J, 1987, SCIENCE, V236, P1092, DOI 10.1126/science.236.4805.1092; SMOLA A, 2003, LEARNING THEORY KERN; Szummer M., 2001, NIPS; Vapnik V.N, 1998, STAT LEARNING THEORY; Zhu X., 2003, ICML	15	1896	2039	2	56	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						321	328						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500041
C	Ke, GL; Meng, Q; Finley, T; Wang, TF; Chen, W; Ma, WD; Ye, QW; Liu, TY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ke, Guolin; Meng, Qi; Finley, Thomas; Wang, Taifeng; Chen, Wei; Ma, Weidong; Ye, Qiwei; Liu, Tie-Yan			LightGBM: A Highly Efficient Gradient Boosting Decision Tree	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LOGISTIC-REGRESSION	Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.	[Ke, Guolin; Wang, Taifeng; Chen, Wei; Ma, Weidong; Ye, Qiwei; Liu, Tie-Yan] Microsoft Res, Redmond, WA 98052 USA; [Meng, Qi] Peking Univ, Beijing, Peoples R China; [Finley, Thomas] Microsoft Redmond, Redmond, WA USA	Microsoft; Peking University	Ke, GL (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	guolin.ke@microsoft.com; qimeng13@pku.edu.cn; tfinely@microsoft.com; taifengw@microsoft.com; wche@microsoft.com; weima@microsoft.com; qiwye@microsoft.com; tie-yan.liu@microsoft.com						Alsabti K., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P2; Appel R., 2013, INT C MACH LEARN, P594, DOI DOI 10.5555/3042817.3043003; Burges C., 2010, LEARNING, V11; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537; Dubout Charles, 2011, NIPS, P1332; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2; Jensen T. R., 2011, GRAPH COLORING PROBL, V39; Jimenez LO, 1999, IEEE T GEOSCI REMOTE, V37, P2653, DOI 10.1109/36.803413; Jin RM, 2003, SIAM PROC S, P119; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Li P., 2007, ADV NEURAL INFORM PR, P897; Li P., 2012, ARXIV12033491; Mehta M., 1996, Advances in Database Technology - EDBT '96. 5th International Conference on Extending Database Technology. Proceedings, P18, DOI 10.1007/BFb0014141; Meng Q, 2016, ADV NEUR IN, V29; Mitchell R, 2017, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.127; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Qin Tao, 2013, CORR; Richardson Matthew, 2007, P 16 INT C WORLD WID, P521, DOI DOI 10.1145/1242572.1242643; Ridgeway G., 2007, UPDATE, V1, P2007; Shafer J, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P544; Shi H., 2007, THESIS; Tyree S., 2011, P 20 INT C WORLD WID, P387, DOI DOI 10.1145/1963405.1963461; Wu Kuan-Wei, 2012, KDDCUP; Yu H.-F., 2010, KDD CUP, P1, DOI DOI 10.1016/J.NEULET.2010.03.079.PUBMED; Zhang H., 2017, ARXIV; Zhou ZH., 2012, ENSEMBLE METHODS FDN, DOI [10.1201/b12207, DOI 10.1201/B12207]	29	1870	1877	114	453	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403021
C	Shi, XJ; Chen, ZR; Wang, H; Yeung, DY; Wong, WK; Woo, WC		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shi, Xingjian; Chen, Zhourong; Wang, Hao; Yeung, Dit-Yan; Wong, Wai-kin; Woo, Wang-chun			Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.	[Shi, Xingjian; Chen, Zhourong; Wang, Hao; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China; [Wong, Wai-kin; Woo, Wang-chun] Hong Kong Observ, Hong Kong, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Shi, XJ (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.	xshiab@cse.ust.hk; zchenbb@cse.ust.hk; hwangaz@cse.ust.hk; dyyeung@cse.ust.hk; wkwong@hko.gov.hk; wcwoo@hko.gov.hk						Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y., 2014, ARXIV14061078; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Bridson R., 2008, FLUID SIMULATION COM; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Cheung P, 2012, 3 WMO INT S NOWC VER, P6; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Douglas R. H., 1990, RADAR METEOROLOGY, P61, DOI DOI 10.1007/978-1-935704-15-7_8; Germann U, 2002, MON WEATHER REV, V130, P2859, DOI 10.1175/1520-0493(2002)130<2859:SDOTPO>2.0.CO;2; Goodfellow I.J., 2015, DEEP LEARNING; Graves A, 2013, ARXIV13080850; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Li PW., 2000, SWIRLS AN EVOLVING N; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Ranzato MarcAurelio, 2014, ARXIV14126604; Reyniers M., 2008, QUANTITATIVE PRECIPI; Sakaino H, 2013, IEEE T GEOSCI REMOTE, V51, P3023, DOI 10.1109/TGRS.2012.2212201; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sun JZ, 2014, B AM METEOROL SOC, V95, P409, DOI 10.1175/BAMS-D-11-00263.1; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tieleman T., 2012, COURSERA COURSE NEUR, V4; Woo W. C., 2014, 27 C SEV LOC STORMS; Xu K, 2015, PR MACH LEARN RES, V37, P2048	26	1719	1742	106	318	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102103
C	Hensel, M; Ramsauer, H; Unterthiner, T; Nessler, B; Hochreiter, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hensel, Martin; Ramsauer, Hubert; Unterthiner, Thomas; Nessler, Bernhard; Hochreiter, Sepp			GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STOCHASTIC-APPROXIMATION; HEAVY BALL; DISTANCE; GRADIENT	Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Frechet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.	[Hensel, Martin] Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria; Johannes Kepler Univ Linz, Inst Bioinformat, A-4040 Linz, Austria	Johannes Kepler University Linz; Johannes Kepler University Linz	Hensel, M (corresponding author), Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria.	mhe@bioinf.jku.at; ramsauer@bioinf.jku.at; unterthiner@bioinf.jku.at; nessler@bioinf.jku.at; hochreit@bioinf.jku.at	Unterthiner, Thomas/K-7231-2018; Heusel, Martin/AAT-5544-2021; Jeong, Yongwook/N-7413-2016; Hochreiter, Sepp/AAI-5904-2020	Unterthiner, Thomas/0000-0001-5361-3087; Hochreiter, Sepp/0000-0001-7449-2528	NVIDIA Corporation; Bayer AG [09/2017]; Zalando SE [01/2016]; Audi.JKU Deep Learning Center; Audi Electronic Venture GmbH; IWT research grant [IWT150865]; H2020 project grant [671555]; FWF [P 28660-N31]	NVIDIA Corporation; Bayer AG(Bayer AG); Zalando SE; Audi.JKU Deep Learning Center; Audi Electronic Venture GmbH; IWT research grant(Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT)); H2020 project grant; FWF(Austrian Science Fund (FWF))	This work was supported by NVIDIA Corporation, Bayer AG with Research Agreement 09/2017, Zalando SE with Research Agreement 01/2016, Audi.JKU Deep Learning Center, Audi Electronic Venture GmbH, IWT research grant IWT150865 (Exaptation), H2020 project grant 671555 (ExCAPE) and FWF grant P 28660-N31.	[Anonymous], ARXIV14126515; Arjovsky M., 2017, ARXIV170107875; Attouch H, 2000, COMMUN CONTEMP MATH, V2, P1, DOI 10.1142/S0219199700000025; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bertsekas DP, 2000, SIAM J OPTIMIZ, V10, P627, DOI 10.1137/S1052623497331063; Bhatnagar S., 2013, LECT NOTES CONTROL I; Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3; Che T, 2017, INT C LEARN REPR ICL; Chelba Ciprian, 2013, ARXIV13123005; Di Castro D, 2010, J MACH LEARN RES, V11, P367; Djork-Arn, ICLR 2016; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; FRECHET M, 1957, CR HEBD ACAD SCI, V244, P689; Gadat S., 2016, ARXIV160904228; Goodfellow I., 2017, ARXIV170100160; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grnarova P., 2017, ARXIV170603269; HIRSCH MW, 1989, NEURAL NETWORKS, V2, P331, DOI 10.1016/0893-6080(89)90018-X; Hjelm R Devon, 2017, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Gulrajani I, 2017, ADV NEUR IN, V30; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Karmakar P., 2017, MATH OPERATIONS RES; Kingma D.P, P 3 INT C LEARNING R; Konda Vijaymohan, 2002, THESIS; Konda VR, 2003, SYST CONTROL LETT, V50, P95, DOI 10.1016/S0167-6911(03)00132-4; Kushner HJ., 2003, STOCHASTIC APPROXIMA; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li CL, 2017, ADV NEUR IN, V30; Li J, 2017, ARXIV170609884; Lim Jae Hyun, 2017, ABS170502894 ARXIV; Liu S., 2017, ADV NEURAL INFORM PR, V31; Metz Luke, 2017, ICLR; Mroueh Y, 2017, ADV NEUR IN, V30; Nagarajan V, 2017, ADV NEUR IN, V30; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Prasad HL, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1371; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ramaswamy A, 2016, STOCHASTICS, V88, P1173, DOI 10.1080/17442508.2016.1215450; Salimans T, 2016, ADV NEUR IN, V29; Theis Lucas, 2016, ICLR; Tolstikhin I, 2017, ADV NEUR IN, V30; Wang Ruohan, 2017, ARXIV170403817; Wasserstein L. N., 1969, PROBL INFORM TRANSM, V5, P47, DOI DOI 10.1016/S0016-0032(33)90010-1; Wu Y, 2017, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON RELIABILITY SYSTEMS ENGINEERING (ICRSE 2017); Zhang JS, 2007, IEEE INFOCOM SER, P222, DOI 10.1109/INFCOM.2007.34	50	1669	1691	16	40	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406067
C	Vapnik, V; Golowich, SE; Smola, A		Mozer, MC; Jordan, MI; Petsche, T		Vapnik, V; Golowich, SE; Smola, A			Support vector method for function approximation, regression estimation, and signal processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems.			Vapnik, V (corresponding author), AT&T BELL LABS,101 CRAWFORDS CORNER,HOLMDEL,NJ 07733, USA.								0	1633	1722	4	72	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						281	287						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00040
C	Hamilton, WL; Ying, R; Leskovec, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hamilton, William L.; Ying, Rex; Leskovec, Jure			Inductive Representation Learning on Large Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.	[Hamilton, William L.; Ying, Rex; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Hamilton, WL (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	wleif@stanford.edu; rexying@stanford.edu; jure@cs.stanford.edu	Jeong, Yongwook/N-7413-2016	Leskovec, Jure/0000-0002-5411-923X	NSF [IIS-1149837]; DARPA SIMPLEX; SAP Stanford Graduate Fellowship; NSERC PGS-D grant; Stanford Data Science Initiative; Huawei; Chan Zuckerberg Biohub	NSF(National Science Foundation (NSF)); DARPA SIMPLEX; SAP Stanford Graduate Fellowship; NSERC PGS-D grant; Stanford Data Science Initiative; Huawei(Huawei Technologies); Chan Zuckerberg Biohub	The authors thank Austin Benson, Aditya Grover, Bryan He, Dan Jurafsky, Alex Ratner, Marinka Zitnik, and Daniel Selsam for their helpful discussions and comments on early drafts. The authors would also like to thank Ben Johnson for his many useful questions and comments on our code. This research has been supported in part by NSF IIS-1149837, DARPA SIMPLEX, Stanford Data Science Initiative, Huawei, and Chan Zuckerberg Biohub. W.L.H. was also supported by the SAP Stanford Graduate Fellowship and an NSERC PGS-D grant. The views and conclusions expressed in this material are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the above funding agencies, corporations, or the U.S. and Canadian governments.	Abadi M, 2015, P 12 USENIX S OPERAT; Arora S., 2017, INT C LEARNING REPRE; Benson AR, 2016, SCIENCE, V353, P163, DOI 10.1126/science.aad9029; Bruna J, 2013, PROC INT C LEARN REP; Cao S., 2015, P 24 ACM INT C INF K, P891, DOI DOI 10.1145/2806416.2806512; Chen J., 2017, ARXIV171010568; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai HJ, 2016, PR MACH LEARN RES, V48; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Duvenaud David K, 2015, P NIPS; ehrek Radim., 2010, P LREC 2010 WORKSH N, P45; GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton William L., 2016, ACL; He K., 2016, EACV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Kingma D.P, P 3 INT C LEARNING R; Kipf Thomas N, 2016, NIPS WORKSHOP BAYESI; Kipf TN, 2016, P INT C LEARN REPR; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Levy O, 2014, ADV NEUR IN, V27; Li Yujia, 2015, ARXIV151105493; Mikolov T., 2013, ARXIV; Ng AY, 2002, ADV NEUR IN, V14, P849; Niepert M, 2016, PR MACH LEARN RES, V48; Page L., 1999, PAGERANK CITATION RA; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Siegal, 1956, NONPARAMETRIC STAT B; Subramanian A, 2005, P NATL ACAD SCI USA, V102, P15545, DOI 10.1073/pnas.0506580102; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Wang X., 2017, 31 AAAI C ART INT; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Xu L., 2017, WWW; Yang Z, 2016, PR MACH LEARN RES, V48; Zitnik M, 2017, BIOINFORMATICS, V33, pI190, DOI 10.1093/bioinformatics/btx252	41	1628	1628	40	129	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401007
C	He, XF; Niyogi, P		Thrun, S; Saul, K; Scholkopf, B		He, XF; Niyogi, P			Locality preserving projections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				EIGENFACES	Many problems in information processing involve some form of dimensionality reduction. In this paper, we introduce Locality Preserving Projections (LPP). These are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set. LPP should be seen as an alternative to Principal Component Analysis (PCA) - a classical linear technique that projects the data along the directions of maximal variance. When the high dimensional data lies on a low dimensional manifold embedded in the ambient space, the Locality Preserving Projections are obtained by finding the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the manifold. As a result, LPP shares many of the data representation properties of nonlinear techniques such as Laplacian Eigenmaps or Locally Linear Embedding. Yet LPP is linear and more crucially is defined everywhere in ambient space rather than just on the training data points. This is borne out by illustrative examples on some high dimensional data sets.	Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA	University of Chicago	He, XF (corresponding author), Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.	xiaofei@cs.uchicago.edu; niyogi@cs.uchicago.edu						Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BELKIN M, 2002, ADV NEURAL INFORMATI, V14; Chung F., 1997, REGIONAL C SERIES MA, V92; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; *YAL U, FAC DAT	8	1479	1527	8	76	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						153	160						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500020
C	Jaderberg, M; Simonyan, K; Zisserman, A; Kavukcuoglu, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Jaderberg, Max; Simonyan, Karen; Zisserman, Andrew; Kavukcuoglu, Koray			Spatial Transformer Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.	[Jaderberg, Max; Simonyan, Karen; Zisserman, Andrew; Kavukcuoglu, Koray] Google DeepMind, London, England	Google Incorporated	Jaderberg, M (corresponding author), Google DeepMind, London, England.	jaderberg@google.com; simonyan@google.com; zisserman@google.com; korayk@google.com						[Anonymous], 2014, ARXIV14127054; [Anonymous], 2011, ICANN; Ba J., 2015, ICLR; Branson S., 2014, BMVC; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Cimpoi M., 2015, CVPR; Cohen T., 2015, ICLR; Erhan D., 2014, CVPR; FREY B, 2001, NIPS; Gens R., 2014, NIPS; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Goodfellow I. J., 2013, ARXIV13126082; Gregor K., 2015, ICML; Hinton G.E., 2012, CORRABS12070580; Hinton G. E., 1981, IJCAI; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Jaderberg M., 2014, NIPS DLW; Kanazawa A., 2014, NIPS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin T.-Y., 2015, ARXIV150407889; Netzer Y., 2011, NIPS DLW; Russakovsky O., 2014, ARXIV, P1, DOI [10.48550/arXiv.1409.0575, DOI 10.48550/ARXIV.1409.0575]; Scholkopf B., 2001, ARXIV150408289; Sohn K., 2012, P 29 INT C MACH LEAR; Stollenga M. F., 2014, NIPS; Tieleman T., 2014, THESIS; Vedaldi A., 2015, CVPR; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Xu K., 2015, ICML; Zhang Xiangyu, 2014, ARXIV14114229	30	1471	1494	29	169	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102003
C	Makeig, S; Bell, AJ; Jung, TP; Sejnowski, TJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Makeig, S; Bell, AJ; Jung, TP; Sejnowski, TJ			Independent component analysis of electroencephalographic data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						USN,HLTH RES CTR,SAN DIEGO,CA 92186	United States Department of Defense; United States Navy; Naval Medical Research Center (NMRC); Naval Health Research Center (NHRC)			Sejnowski, Terrence/AAV-5558-2021						0	1351	1394	0	25	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						145	151						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00021
C	Bengio, Y; Ducharme, R; Vincent, P		Leen, TK; Dietterich, TG; Tresp, V		Bengio, Y; Ducharme, R; Vincent, P			A neural probabilistic language model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				NETWORKS	A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. in the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.	Univ Montreal, Dept Informat & Rech Operat, Ctr Rech Math, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Bengio, Y (corresponding author), Univ Montreal, Dept Informat & Rech Operat, Ctr Rech Math, Montreal, PQ H3C 3J7, Canada.							BAKER D, 1998, SIGIR 98; Bengio Y, 2000, ADV NEUR IN, V12, P400; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Hinton G, 1986, P 8 ANN C COGN SCI S, V1, DOI DOI 10.1016/J.NEUCOM.2013.03.009; Jelinek F., 1980, PATTERN RECOGNITION; KATZ SM, 1987, IEEE T ACOUST SPEECH, V35, P400, DOI 10.1109/TASSP.1987.1165125; MIIKKULAINEN R, 1991, COGNITIVE SCI, V15, P343, DOI 10.1207/s15516709cog1503_2; PACCANARO A, 2000, P INT JOINT C NEUR N; PEREIRA F, 1993, 31ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P183; Schmidhuber J, 1996, IEEE T NEURAL NETWOR, V7, P142, DOI 10.1109/72.478398; Schuatze, 1993, ADV NEURAL INFORM PR, P895	12	1317	1437	3	31	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						932	938						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800131
C	Simonyan, K; Zisserman, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Simonyan, Karen; Zisserman, Andrew			Two-Stream Convolutional Networks for Action Recognition in Videos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multiframe dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.	[Simonyan, Karen; Zisserman, Andrew] Univ Oxford, Visual Geometry Grp, Oxford, England	University of Oxford	Simonyan, K (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford, England.	karen@robots.ox.ac.uk; az@robots.ox.ac.uk			ERC grant VisRec [228180]; NVIDIA Corporation	ERC grant VisRec; NVIDIA Corporation	This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.	[Anonymous], 2012, ABS12120402 CORR; Berg A., 2010, LARGE SCALE VISUAL R; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; CHATFIELD K, 2014, P BMVC; Chen B., 2010, NIPS DEEP LEARN UNS NIPS DEEP LEARN UNS; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Jhuang H, 2007, IEEE I CONF COMP VIS, P1253; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jia Y., 2013, CAFFE OPEN SOURCE CO; Karpathy A., 2014, P CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Laptev Ivan, 2008, P CVPR; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Peng X., 2014, ABS14054506 CORR; Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38; Perronnin F., 2010, P ECCV; Simonyan K., 2013, NIPS; Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11; Wang H., BRIT MACH VIS C LOND, P1; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang Heng, 2013, ICCV WORKSH ACT REC; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Zeiler M., 2013, ABS13112901 CORR	31	1260	1284	16	104	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101024
C	Qi, CR; Yi, L; Su, H; Guibas, LJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Qi, Charles R.; Yi, Li; Su, Hao; Guibas, Leonidas J.			PointNet plus plus : Deep Hierarchical Feature Learning on Point Sets in a Metric Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.	[Qi, Charles R.; Yi, Li; Su, Hao; Guibas, Leonidas J.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Qi, CR (corresponding author), Stanford Univ, Stanford, CA 94305 USA.				Samsung GRO; NSF [IIS-1528025, DMS-1546206]; ONR MURI [N00014-13-1-0341]	Samsung GRO(Samsung); NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research)	The authors would like to acknowledge the support of a Samsung GRO grant, NSF grants IIS-1528025 and DMS-1546206, and ONR MURI grant N00014-13-1-0341.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2011, INT ARCH PHOTOGRAMM; [Anonymous], 2013, COMPUT SCI; [Anonymous], [No title captured]; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Barr A. H, 2002, VISUALIZATION MATH, V3, P52; Belton D., 2006, INT ARCH PHOTOGRAMM, V36, P44; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Chang A.X., 2015, SHAPENET INFORM RICH; Dai A., 2017, ARXIV170204405; Gressin A, 2013, ISPRS J PHOTOGRAMM, V79, P240, DOI 10.1016/j.isprsjprs.2013.02.019; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lian Z., 2015, EUR WORKSH 3D OBJ RE; Luciano L., 2017, PATTERN RECOGNITION; Masci J., 2015, P IEEE INT C COMP VI, P37; Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470; Pauly M, 2006, ACM T GRAPHIC, V25, P177, DOI 10.1145/1138450.1138451; Qi C.R., 2016, ARXIV161200593; Qi C.R., 2016, P COMP VIS PATT REC; Riegler G., 2016, ABS161105009; Rustamov RM, 2009, COMPUT GRAPH FORUM, V28, P1279, DOI 10.1111/j.1467-8659.2009.01505.x; Simard PY, 2003, PROC INT CONF DOC, P958; Su H., 2015, P ICCV; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Vinyals Oriol, 2015, ARXIV151106391; WANG P.-S., 2017, O CNN OCTREE BASED C; Weinmann M, 2015, ISPRS J PHOTOGRAMM, V105, P286, DOI 10.1016/j.isprsjprs.2015.01.016; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Yi L., 2016, IEEE C COMP VIS PATT; Yi L., 2016, SIGGRAPH ASIA	33	1216	1235	117	371	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405018
C	Amari, S; Cichocki, A; Yang, HH		Touretzky, DS; Mozer, MC; Hasselmo, ME		Amari, S; Cichocki, A; Yang, HH			A new learning algorithm for blind signal separation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TOKYO,BUNKYO KU,TOKYO 113,JAPAN	University of Tokyo			Cichocki, Andrzej/AAI-4209-2020						0	1160	1263	0	21	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						757	763						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00107
C	Platt, JC; Cristianini, N; Shawe-Taylor, J		Solla, SA; Leen, TK; Muller, KR		Platt, JC; Cristianini, N; Shawe-Taylor, J			Large margin DAGs for multiclass classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We present a new learning architecture: the Decision Directed Acyclic Graph (DDAG), which is used to combine many two-class classifiers into a multiclass classifier. For an N-class problem, the DDAG contains N(N - 1)/2 classifiers, one for each pair of classes. We present a VC analysis of the case when the node classifiers ale hyperplanes; the resulting bound on the test error depends on N and on the margin achieved at the nodes, but not on the dimension of the space. This motivates an algorithm, DAGSVM, which operates in a kernel-induced feature space and uses two-class maximal margin hyperplanes at each decision-node of the DDAG. The DAGSVM is substantially faster to train and evaluate than either the standard algorithm or Max Wins, while maintaining comparable accuracy to both of these algorithms.	Microsoft Corp, Res, Redmond, WA 98052 USA	Microsoft	Platt, JC (corresponding author), Microsoft Corp, Res, 1 Microsoft Way, Redmond, WA 98052 USA.	jplatt@microsoft.com; nello.cristianini@bristol.ac.uk; j.shawe-taylor@dcs.rhbnc.ac.uk	Platt, John/GOH-2678-2022	Shawe-Taylor, John/0000-0002-2030-0073; Platt, John/0000-0002-5652-5303				BENNETT K, UNPUB ENLARGING MARG; Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197; Friedman J.H., 1996, ANOTHER APPROACH POL; Knerr S, 1990, NATO ASI; Kressel UHG, 1999, ADVANCES IN KERNEL METHODS, P255; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; SHAWETAYLOR J, 1999, ADV NEURAL INFORMATI, V10, P336; Vapnik V.N, 1998, STAT LEARNING THEORY	10	1102	1322	2	34	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						547	553						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700078
C	Sutskever, I; Vinyals, O; Le, QV		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sutskever, Ilya; Vinyals, Oriol; Le, Quoc, V			Sequence to Sequence Learning with Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.	[Sutskever, Ilya; Vinyals, Oriol; Le, Quoc, V] Google, Mountain View, CA 94043 USA	Google Incorporated	Sutskever, I (corresponding author), Google, Mountain View, CA 94043 USA.	ilyasu@google.com; vinyals@google.com; qvl@google.com						[Anonymous], 2012, ICML; [Anonymous], 2012, ARXIV E PRINTS; [Anonymous], P IEEE; Auli M, 2013, EMNLP; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Cho K., 2014, P 2014 C EMP METH NA, P1724; Ciresan D., 2012, CVPR; Dahl G.E., 2012, IEEE T AUDIO SPEECH; Devlin J., 2014, ACL; Durrani Nadir, 2014, WMT; Graves A., 2006, ICML; Graves Alex, 2013, ARXIV13080850 CORR; Hermann Karl Moritz, 2014, ICLR; Hinton G., 2012, IEEE SIGNAL PROCESSI; Hochreiter S., 1997, LSTM CAN SOLVE HARD; Hochreiter S., 2001, FIELD GUIDE DYNAMICA, DOI DOI 10.1109/9780470544037.CH14; Hochreiter S, 1997, NEURAL COMPUTATION; Hochreiter S., 1991, THESIS; Kalchbrenner N., 2013, EMNLP; Krizhevsky A., 2012, ADV NEURAL INF PROCE; LeCun Y., 1998, P IEEE; Mikolov T.A., 2012, STAT LANGUAGE MODELS; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Papineni K., 2002, P ANN M ASS COMP LIN; Pouget-Abadie J, 2014, P 8 WORKSH SYNT SEM; Razborov A., 1992, P 3 SCAND WORKSH ALG; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sundermeyer M., 2010, INTERSPEECH 2010	30	1100	1128	32	137	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101017
C	Defferrard, M; Bresson, X; Vandergheynst, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Defferrard, Michael; Bresson, Xavier; Vandergheynst, Pierre			Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CUTS	In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.	[Defferrard, Michael; Bresson, Xavier; Vandergheynst, Pierre] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Defferrard, M (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	michael.defferrard@epfl.ch; xavier.bresson@epfl.ch; pierre.vandergheynst@epfl.ch						Abadi M, 2015, P 12 USENIX S OPERAT; Bawa Mayank, 2005, P 14 INT C WORLD WID, P651, DOI DOI 10.1145/1060745.1060840; Belkin M, 2008, J COMPUT SYST SCI, V74, P1289, DOI 10.1016/j.jcss.2007.08.006; Bruna J, 2013, PROC INT C LEARN REP; BUI TN, 1992, INFORM PROCESS LETT, V42, P153, DOI 10.1016/0020-0190(92)90140-Q; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Coates A., 2011, ADV NEURAL INFORM PR, P2528, DOI DOI 10.1016/J.PSYCHRES.2009.03.008; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Gavish M., 2010, P 27 INT C MACH LEAR, P367; Gregor K., 2010, ARXIV10060448; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Henaff M, 2015, ARXIV150605163; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Joachims T, 1996, CMUCS96118; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li Yujia, GATED GRAPH SEQUENCE; Mallat S, 1999, WAVELET TOUR SIGNAL, DOI DOI 10.1016/B978-012466606-1/50004-0; Masci J., 2015, P IEEE INT C COMP VI, P37; Mikolov T., 2013, ARXIV; Pasdeloup B, 2015, EUR SIGNAL PR CONF, P1496, DOI 10.1109/EUSIPCO.2015.7362633; Perraudin N., 2016, ARXIV160303030; Ram I, 2011, IEEE T SIGNAL PROCES, V59, P4199, DOI 10.1109/TSP.2011.2158428; Ron D, 2011, MULTISCALE MODEL SIM, V9, P407, DOI 10.1137/100791142; Scarselli F., GRAPH NEURAL NETWORK, V20, P61; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shuman DI, 2016, IEEE T SIGNAL PROCES, V64, P2119, DOI 10.1109/TSP.2015.2512529; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Susnjara A., 2015, ARXIV150904537; Tsitsvero M, 2015, EUR SIGNAL PR CONF, P1506, DOI 10.1109/EUSIPCO.2015.7362635; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z	34	1028	1030	41	146	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700045
C	Han, S; Pool, J; Tran, J; Dally, WJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Han, Song; Pool, Jeff; Tran, John; Dally, William J.			Learning both Weights and Connections for Efficient Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.	[Han, Song; Dally, William J.] Stanford Univ, Stanford, CA 94305 USA; [Pool, Jeff; Tran, John; Dally, William J.] NVIDIA, Santa Clara, CA USA	Stanford University; Nvidia Corporation	Han, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	songhan@stanford.edu; jpool@nvidia.com; johntran@nvidia.com; dally@stanford.edu	Han, Song/AAR-9464-2020	Han, Song/0000-0002-4186-7618				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 1989, ADV NEURAL INFORM PR; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Coates A., 2013, INT C MACHINE LEARNI, P1337; Collins M.D., 2014, ARXIV14121442; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Dentinel Zarembaw, 2014, NEURIPS, P1269; Gong Yunchao, 2014, ARXIV14126115; Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Horowitz Mark, ENERGY TABLE 45NM PR; Jia Y., 2014, P 22 ACM INT C MULT, P675; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin Min, 2013, ARXIV14094842; RAUSCHECKER JP, 1984, HUM NEUROBIOL, V3, P109; Shi QF, 2009, J MACH LEARN RES, V10, P2615; Srinivas S., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.5244/C.29.31; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Vanhoucke V., 2011, P ADV NEUR INF PROC, P1; Walsh CA, 2013, NATURE, V502, P172, DOI 10.1038/502172a; Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516; Yang Zichao, 2014, ARXIV14127149; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519	30	989	1000	19	66	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101044
C	Han, B; Yao, QM; Yu, XR; Niu, G; Xu, M; Hu, WH; Tsang, IW; Sugiyama, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Han, Bo; Yao, Quanming; Yu, Xingrui; Niu, Gang; Xu, Miao; Hu, Weihua; Tsang, Ivor W.; Sugiyama, Masashi			Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called "Co-teaching" for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models.	[Han, Bo; Yu, Xingrui; Tsang, Ivor W.] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia; [Han, Bo; Niu, Gang; Xu, Miao; Sugiyama, Masashi] RIKEN, Tokyo, Japan; [Yao, Quanming] 4Paradigm Inc, Beijing, Peoples R China; [Hu, Weihua] Stanford Univ, Stanford, CA 94305 USA; [Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan	University of Technology Sydney; RIKEN; Stanford University; University of Tokyo	Han, B (corresponding author), Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.; Han, B (corresponding author), RIKEN, Tokyo, Japan.		Sugiyama, Masashi/AEO-1176-2022; Xu, Miao/ABE-6469-2021; Yao, Quanming/Y-6095-2019	Sugiyama, Masashi/0000-0001-6658-6743; Xu, Miao/0000-0001-9409-6960; Yao, Quanming/0000-0001-8944-8618	JST CREST [JPMJCR1403]; ARC [FT130100746, DP180100106, LP150100671]; NSFC [61671481]; NVIDIA Corporation; RIKEN-AIP	JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); ARC(Australian Research Council); NSFC(National Natural Science Foundation of China (NSFC)); NVIDIA Corporation; RIKEN-AIP	MS was supported by JST CREST JPMJCR1403. IWT was supported by ARC FT130100746, DP180100106 and LP150100671. BH would like to thank the financial support from RIKEN-AIP. XRY was supported by NSFC Project No. 61671481. QY would give special thanks to Weiwei Tu and Yuqiang Chen from 4Paradigm Inc. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.	Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Arpit D., 2017, ICML; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Blum A, 2003, J ACM, V50, P506, DOI 10.1145/792538.792543; Blum A, 1998, COLT; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295; Deng J., 2013, CVPR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fan Yang, 2018, ICLR; Freund Y., 1995, EUROPEAN COLT; Freund Y., 1999, JAPANESE SOC ART INT, V14, P1612; Goldberger J., 2017, ICLR; Gong C, 2016, AAAI CONF ARTIF INTE, P1610; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hinton G., 2015, ARXIV 1503 02531; Jiang L., 2018, ICML; Kingma D.P., 2015, INT C LEARN REPR, P1; Kiryo R., 2017, NIPS; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Laine Samuli, 2017, PROC INT C LEARN REP; Li Y., 2017, ICCV; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Ma X., 2018, ICML; Maas Andrew L, 2013, P ICML; Malach E., 2017, NIPS; Masnadi-Shirazi H., 2009, NIPS; Menon A., 2015, ICML; Miyato T., 2016, ICLR; Natarajan N., 2013, NIPS; Patrini G., 2017, CVPR; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Reed S., 2015, ICLR; Ren M., 2018, ICML; Rodrigues Filipe, 2018, AAAI; Scott C, 2014, AISTATS; Szegedy C., 2016, CVPR; Tanaka D., 2018, CVPR; Van Rooyen B., 2015, NIPS; Veit A., 2017, CVPR; Wang Y., 2018, CVPR; Yan Y, 2014, MACH LEARN, V95, P291, DOI 10.1007/s10994-013-5412-1; Yu X., 2018, ECCV; Yu X., 2018, CVPR; Zhang C., 2017, ICLR	45	979	984	4	41	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31									10.5555/3327757.3327944	http://dx.doi.org/10.5555/3327757.3327944			11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003012
C	Ng, AY; Jordan, MI		Dietterich, TG; Becker, S; Ghahramani, Z		Ng, AY; Jordan, MI			On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation-which is borne out in repeated experiments-that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ng, AY (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					Anthony M., 1999, NEURAL NETWORK LEARN, V9; EFRON B, 1975, J AM STAT ASSOC, V70, P892, DOI 10.2307/2285453; Mclachlan GJ., 2005, DISCRIMINANT ANAL ST; Rubinstein Y. D., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P49; Vapnik V.N, 1998, STAT LEARNING THEORY	6	902	931	2	57	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						841	848						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100105
C	Williams, CKI; Seeger, M		Leen, TK; Dietterich, TG; Tresp, V		Williams, CKI; Seeger, M			Using the Nystrom method to speed up kernel machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n(3)), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m < n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m(2) n). We report experiments on the USPS and abalone data sets and show that we can set m much less than n without any significant decrease in the accuracy of the solution.	Univ Edinburgh, Inst Adapt & Neural Computat, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Williams, CKI (corresponding author), Univ Edinburgh, Inst Adapt & Neural Computat, 5 Forrest Hill, Edinburgh EH1 2QL, Midlothian, Scotland.	c.k.i.williams@ed.ac.uk; seeger@dai.ed.ac.uk						Baker C., 1977, NUMERICAL TREATMENT; FINE S, 2000, 21911 RC IBM TJ WATS; Frieze A, 1998, ANN IEEE SYMP FOUND, P370, DOI 10.1109/SFCS.1998.743487; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Smola Alex J, 2000, SPARSE GREEDY MATRIX; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Wahba G., 1990, CBMS NSF REGIONAL C; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807	11	887	912	0	27	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						682	688						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800097
C	Hubara, I; Courbariaux, M; Soudry, D; El-Yaniv, R; Bengio, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hubara, Itay; Courbariaux, Matthieu; Soudry, Daniel; El-Yaniv, Ran; Bengio, Yoshua			Binarized Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.	[Hubara, Itay] Technion Israel Inst Technol, Haifa, Israel; [Courbariaux, Matthieu; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada; [Soudry, Daniel] Columbia Univ, New York, NY 10027 USA	Technion Israel Institute of Technology; Universite de Montreal; Columbia University	Hubara, I (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	itayh@technion.ac.il; matthieu.courbariaux@gmail.com; daniel.soudry@gmail.com; rani@cs.technion.ac.il; yoshua.umontreal@gmail.com			NSERC; Canada Research Chairs; Compute Canada; CIFAR; IBM; Samsung; Israel Science Foundation [1890/14]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada Research Chairs(Canada Research ChairsCGIAR); Compute Canada; CIFAR(Canadian Institute for Advanced Research (CIFAR)); IBM(International Business Machines (IBM)); Samsung(Samsung); Israel Science Foundation(Israel Science Foundation)	We would like to express our appreciation to Elad Hoffer, for his technical assistance and constructive comments. We thank our fellow MILA lab members who took the time to read the article and give us some feedback. We thank the developers of Torch, Collobert et al. (2011) a Lua based environment, and Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library which allowed us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow et al., 2013) and Lasagne (Dieleman et al., 2015), two Deep Learning libraries built on the top of Theano. We thank Yuxin Wu for helping us compare our GPU kernels with cuBLAS. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR. We are also grateful for funding from CIFAR, NSERC, IBM, Samsung. This research was also supported by The Israel Science Foundation (grant No. 1890/14).	Baldassi C, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.128101; Bastien F., 2012, DEEP LEARN UNSUP FEA; Beauchamp M.J., 2006, P 2006 ACM SIGDA 14, P12, DOI DOI 10.1145/1117201.1117204; Bengio Yoshua, 2013, ESTIMATING PROPAGATI, P4; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Chen Tianshi, 2014, P 19 INT C ARCHITECT, P269, DOI [10.1145/2541940, DOI 10.1145/2541940, 10.1145/2654822.2541967, DOI 10.1145/2654822.2541967]; Cheng Zhiyong, 2015, ARXIV150303562; Coates A., 2013, INT C MACHINE LEARNI, P1337; Collobert R., 2011, NIPS; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123; Courbariaux M., 2014, ARXIV E PRINTS; Dieleman S., 2015, LASAGNE 1 RELEASE, V1; Esser S. K., 2015, ADV NEURAL INFORM PR, P1117; Glorot X., 2010, PROC MACH LEARN RES, P249; Gong Yunchao, 2014, ARXIV14126115; Goodfellow I.J., 2013, ARXIV13084214; Govindu G., 2004, Proceedings. 18th International Parallel and Distributed Processing Symposium; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; Hinton G., 2012, NEURAL NETWORKS MACH, V264, P1; Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323; Hubara Itay, 2016, QUANTIZED NEURAL NET, P3; Hwang K, 2014, IEEE WRK SIG PRO SYS, P174; Kim M., 2016, ARXIV E PRINTS; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee C. Y., 2015, ARXIV150908985; Lin Z., 2015, ARXIV E PRINTS; Soudry D., 2014, NIPS 2014; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wan L., 2013, ICML 2013	34	884	903	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704097
C	Elisseeff, A; Weston, J		Dietterich, TG; Becker, S; Ghahramani, Z		Elisseeff, A; Weston, J			A kernel method for multi-labelled classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.	BIOwulf Technol, New York, NY 10007 USA		Elisseeff, A (corresponding author), BIOwulf Technol, 305 Broadway, New York, NY 10007 USA.							Boser B. E., 1992, TRAINING ALGORITHM O, P144; Cristianini N., 2000, INTRO SUPPORT VECTOR; Elisseeff A., 2001, KERNEL METHODS MULTI; JOACHIMS T, 1998, P 10 EUR C MACH LEAR, V1398, P137; MCCALLUM A, 1999, AAAI 99 WORKSH TEXT; PAVLIDIS P, 2001, RECOMB, P242; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; WESTON J, 1998, 9804 U LOND	8	825	909	1	20	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						681	687						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100085
C	Fanti, C; Polito, M; Perona, P		Thrun, S; Saul, K; Scholkopf, B		Fanti, C; Polito, M; Perona, P			An improved scheme for detection and labelling in Johansson displays	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				MODEL	Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can effortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body's centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more significantly, to the introduction of the centroid variable.	CALTECH, Computat Vis Lab, Pasadena, CA 91125 USA	California Institute of Technology	Fanti, C (corresponding author), CALTECH, Computat Vis Lab, Pasadena, CA 91125 USA.	fanti@vision.caltech.edu; marzia.polito@intel.com; perona@vision.caltech.edu						Aji SM, 2000, IEEE T INFORM THEORY, V46, P325, DOI 10.1109/18.825794; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; FREEMAN WT, 2001, IEEE T INFORMATION T, V47, P723; Giudici P, 2003, MACH LEARN, V50, P127, DOI 10.1023/A:1020202028934; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Song Y, 2001, PROC CVPR IEEE, P771; SONG Y, 2001, ADV NEURAL INFORMATI, V14; SONG Y, 2000, P ECCV, V2, P719; Tomasi C, 1991, CMUCS91132; YEDIDIA JS, 2000, ADV NEURAL INFORMATI, V13	10	819	873	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1603	1610						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500199
C	Levy, O; Goldberg, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Levy, Omer; Goldberg, Yoav			Neural Word Embedding as Implicit Matrix Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				EXTRACTING SEMANTIC REPRESENTATIONS; COOCCURRENCE STATISTICS	We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.	[Levy, Omer; Goldberg, Yoav] Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel	Bar Ilan University	Levy, O (corresponding author), Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel.	omerlevy@gmail.com; yoav.goldberg@gmail.com			 [FP7ICT-287923]		This work was partially supported by the EC-funded project EXCITEMENT (FP7ICT-287923). We thank Ido Dagan and Peter Turney for their valuable insights.	Baroni M, 2010, COMPUT LINGUIST, V36, P673, DOI 10.1162/coli_a_00016; Bengio Y, 2001, ADV NEUR IN, V13, P932; Bruni Elia, 2012, ACL; Bullinaria JA, 2007, BEHAV RES METHODS, V39, P510, DOI 10.3758/BF03193020; Bullinaria JA, 2012, BEHAV RES METHODS, V44, P890, DOI 10.3758/s13428-011-0183-8; Caron J, 2001, COMPUTATIONAL INFORMATION RETRIEVAL, P157; CHURCH KW, 1990, 27TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P76; Collobert R., 2008, P 25 ICML, V25, P160, DOI DOI 10.1145/1390156.1390177; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Dagan Ido, 1994, ACL; Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367; Finkelstein Lev, 2002, ACM TOIS; Goldberg Y., 2014, ARXIV PREPRINT ARXIV; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; Kiela Douwe, 2014, WORKSH CONT VECT SPA; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Levy O, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P302, DOI 10.3115/v1/p14-2050; Levy Omer, 2014, CONLL; Mikolov T., 2013, ARXIV; Mikolov Tomas, 2013, NAACL; Mnih A., 2013, ADV NEURAL INFORM PR, V26, P2265; Mnih Andriy, 2009, ADV NEURAL INFORM PR, P1081; SREBRO N, 2003, ICML; Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384; Turney P. D., 2001, MACHINE LEARNING ECM, P491, DOI DOI 10.1007/3-540-44795-4_42; Turney PD, 2012, J ARTIF INTELL RES, V44, P533, DOI 10.1613/jair.3640; Turney Peter D., 2010, J ARTIFICIAL INTELLI	28	786	828	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102038
C	Gulrajani, I; Ahmed, F; Arjovsky, M; Dumoulin, V; Courville, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ishaan Gulrajani; Ahmed, Faruk; Arjovsky, Martin; Dumoulin, Vincent; Courville, Aaron			Improved Training of Wasserstein GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.	[Ishaan Gulrajani; Ahmed, Faruk; Dumoulin, Vincent; Courville, Aaron] Montreal Inst Learning Algorithms, Montreal, PQ, Canada; [Arjovsky, Martin] Courant Inst Math Sci, New York, NY USA	Universite de Montreal	Gulrajani, I (corresponding author), Montreal Inst Learning Algorithms, Montreal, PQ, Canada.	igul222@gmail.com; faruk.ahmed@umontreal.ca; ma4371@nyu.edu; vincent.dumoulin@umontreal.ca; aaron.couryille@umontreal.ca						[Anonymous], 2017, ABS170207983 CORR; Arjovsky M., 2017, ARXIV170107875; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Chelba Ciprian, 2013, ARXIV13123005; Dai Z., 2017, ARXIV170201691; Dumoulin Vincent, 2017, ICLR 2017, P4; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton, 2016, ARXIV PREPRINT ARXIV; Hjelm R Devon, 2017, ARXIV PREPRINT ARXIV; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Jang E., 2016, ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li Jiwei, 2017, P EMNLP; Liang XD, 2017, IEEE I CONF COMP VIS, P3382, DOI 10.1109/ICCV.2017.364; Maddison Chris J, 2016, ARXIV161100712; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Metz Luke, 2016, ARXIV161102163; Odena A., 2016, SEMISUPERVISED LEARN; Poole B., 2016, ARXIV161202780; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; van den Oord Aaron, 2016, ARXIV160605328; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang D., 2016, ARXIV161101722; Warde-Farley D., 2017, IMPROVING GENERATIVE; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu Y., 2016, ARXIV161104273; Yang Z., 2017, ARXIV; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Yu L., 2016, ARXIV160905473	31	785	787	36	127	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405082
C	Chen, X; Duan, Y; Houthooft, R; Schulman, J; Sutskever, I; Abbeel, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Xi; Duan, Yan; Houthooft, Rein; Schulman, John; Sutskever, Ilya; Abbeel, Pieter			InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHM	This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.	[Chen, Xi; Duan, Yan; Houthooft, Rein; Schulman, John; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Chen, Xi; Duan, Yan; Houthooft, Rein; Schulman, John; Sutskever, Ilya; Abbeel, Pieter] OpenAI, San Francisco, CA 94110 USA	University of California System; University of California Berkeley	Chen, X (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.; Chen, X (corresponding author), OpenAI, San Francisco, CA 94110 USA.				ONR through a PECASE award; Berkeley AI Research lab Fellowship; Huawei Fellowship; Ph.D. Fellowship of the Research Foundation - Flanders (FWO)	ONR through a PECASE award; Berkeley AI Research lab Fellowship; Huawei Fellowship(Huawei Technologies); Ph.D. Fellowship of the Research Foundation - Flanders (FWO)(FWO)	We thank the anonymous reviewers. This research was funded in part by ONR through a PECASE award. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Rein Houthooft was supported by a Ph.D. Fellowship of the Research Foundation - Flanders (FWO).	Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Barber D., 2005, ADV NEURAL INFORM PR, P17; Bengio Y., 2013, PATTERN ANAL MACHINE, V35; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bridle John S, 1992, NIPS; Cheung B., 2014, P INT C LEARN REPR W; Desjardins G., 2012, ARXIV12105474; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton, 2010, TECH REP; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni Tejas D, 2015, ADV NEURAL INFORM PR, P2530; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Maaloe Lars, 2016, ICML; Makhzani A., 2015, ARXIV151105644; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Perona P., 2010, ADV NEURAL INFORM PR, P1; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Springenberg Jost Tobias, 2015, ARXIV151106390; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Whitney W. F., 2016, ARXIV160206822; Yang Jimei, 2015, NIPS; Zhu Z., 2014, NIPS, P217	31	764	793	17	92	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703035
C	Mnih, V; Heess, N; Graves, A; Kavukcuoglu, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mnih, Volodymyr; Heess, Nicolas; Graves, Alex; Kavukcuoglu, Koray			Recurrent Models of Visual Attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				EYE-MOVEMENTS	Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.	[Mnih, Volodymyr; Heess, Nicolas; Graves, Alex; Kavukcuoglu, Koray] Google DeepMind, London, England	Google Incorporated	Mnih, V (corresponding author), Google DeepMind, London, England.	vmnih@google.com; heess@google.com; gravesa@google.com; korayk@google.com						Alexe B., 2012, NIPS; Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Blaschko MB, 2008, PROC CVPR IEEE, P93, DOI 10.1109/cvpr.2008.4587586; Butko Nicholas J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2751, DOI 10.1109/CVPRW.2009.5206540; Butko NJ, 2008, INT C DEVEL LEARN, P139, DOI 10.1109/DEVLRN.2008.4640819; Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312; Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Mathe Stefan, 2013, NIPS; Paletta Lucas, 2005, CVPR; Ranzato M., 2014, ARXIV E PRINTS; Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Stanley Kenneth O., 2004, GECCO; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766; van de Sande Koen E. A., 2011, ICCV; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wierstra D, 2007, LECT NOTES COMPUT SC, V4668, P697; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	26	754	847	11	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102103
C	Scholkopf, B; Williamson, R; Smola, A; Shawe-Taylor, J; Platt, J		Solla, SA; Leen, TK; Muller, KR		Scholkopf, B; Williamson, R; Smola, A; Shawe-Taylor, J; Platt, J			Support vector method for novelty detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Suppose you are given some dataset drawn from an underlying probability distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified nu between 0 and 1. We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.	Microsoft Res Ltd, Cambridge, England	Microsoft	Scholkopf, B (corresponding author), Microsoft Res Ltd, 1 Guildhall St, Cambridge, England.		Platt, John/GOH-2678-2022; Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925; Platt, John/0000-0002-5652-5303; Shawe-Taylor, John/0000-0002-2030-0073				BenDavid S, 1997, J COMPUT SYST SCI, V55, P171, DOI 10.1006/jcss.1997.1507; Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327; Scholkopf B., 1995, KDD; SCHOLKOPF B, 1999, UNSUPERVISED LEARNIN, P19; SCHOLKOPF B, 1999, 9987 TR MSR MICR RES; Smola AJ, 1999, LECT NOTES ARTIF INT, V1572, P214; TAX DMJ, 1999, P EUR S ART NEUR NET, P251; Vapnik V., 1963, AUTOMAT REM CONTR+, V24, P774; Vapnik V.N, 1998, STAT LEARNING THEORY	9	718	751	2	20	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						582	588						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700083
C	Sabour, S; Frosst, N; Hinton, GE		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Sabour, Sara; Frosst, Nicholas; Hinton, Geoffrey E.			Dynamic Routing Between Capsules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.	[Sabour, Sara; Frosst, Nicholas; Hinton, Geoffrey E.] Google Brain, Toronto, ON, Canada		Sabour, S (corresponding author), Google Brain, Toronto, ON, Canada.	sasabour@google.com; frosst@google.com; geoffhinton@google.com	Jeong, Yongwook/N-7413-2016					Abadi M., TENSORFLOW LARGE SCA; [Anonymous], 2013, ARXIV201313013557; Ba J., 2014, ARXIV; Chang J.R., 2015, COMPUTER SCI; Ciresan DC, 2011, ARXIV11020183; Goodfellow I. J., 2013, ARXIV13126082; Greff K., 2016, ADV NEURAL INFORM PR; Hinton G.F., 1981, P 7 INT JOINT C ART, V2, P683; Hinton GE, 2000, ADV NEUR IN, V12, P463; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hinton Geoffrey E, 1981, INT JOINT C ART INT, V2; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 2004, PROC CVPR IEEE, P97; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700; Pelli DG, 2004, J VISION, V4, P1136, DOI 10.1167/4.12.12; Wan L., 2013, P INT C MACHINE LEAR, P1058	19	694	698	19	121	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403089
C	Mao, XJ; Shen, CH; Yang, YB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mao, Xiao-Jiao; Shen, Chunhua; Yang, Yu-Bin			Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods.	[Mao, Xiao-Jiao; Yang, Yu-Bin] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China; [Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia	Nanjing University; University of Adelaide	Mao, XJ (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.				Natural Science Foundation of China [61673204, 61273257, 61321491]; Program for Distinguished Talents of Jiangsu Province, China [2013-XXRJ-018]; Fundamental Research Funds for the Central Universities [020214380026]; Australian Research Council Future Fellowship [FT120100969]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Distinguished Talents of Jiangsu Province, China; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Australian Research Council Future Fellowship(Australian Research Council)	This work was in part supported by Natural Science Foundation of China (Grants 61673204, 61273257, 61321491), Program for Distinguished Talents of Jiangsu Province, China (Grant 2013-XXRJ-018), Fundamental Research Funds for the Central Universities (Grant 020214380026), and Australian Research Council Future Fellowship (FT120100969). X.-J. Mao's contribution was made when visiting University of Adelaide. His visit was supported by the joint PhD program of China Scholarship Council.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Chatterjee P, 2009, IEEE T IMAGE PROCESS, V18, P1438, DOI 10.1109/TIP.2009.2018575; Chen F, 2015, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2015.76; Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847; Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Hong S, 2015, ADV NEUR IN, V28; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; Huang Y., 2015, P ADV NEURAL INFORM, V28, P235; Jain V, 2008, P ADV NEUR INF PROC, P769; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Junyuan X, 2012, ADV NEURAL INF PROCE, P341; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Liu HF, 2015, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2015.7298646; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Salvador J, 2015, IEEE I CONF COMP VIS, P325, DOI 10.1109/ICCV.2015.45; Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003; Srivastava Rupesh K, 2015, TRAINING VERY DEEP N, V28, P2377; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang ZY, 2015, IEEE T IMAGE PROCESS, V24, P4359, DOI 10.1109/TIP.2015.2462113; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	34	686	715	13	44	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701029
C	Sukhbaatar, S; Szlam, A; Weston, J; Fergus, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sukhbaatar, Sainbayar; Szlam, Arthur; Weston, Jason; Fergus, Rob			End-To-End Memory Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.	[Sukhbaatar, Sainbayar] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA; [Szlam, Arthur; Weston, Jason; Fergus, Rob] Facebook AI Res, New York, NY USA	New York University; Facebook Inc	Sukhbaatar, S (corresponding author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.	sainbar@cs.nyu.edu; aszlam@fb.com; jase@fb.com; robfergus@fb.com						Atkeson CG, 1995, NEUROCOMPUTING, V9, P243, DOI 10.1016/0925-2312(95)00033-6; Bengio Y, 2001, ADV NEUR IN, V13, P932; Chung J., 2014, ARXIV14123555; Das Sreerupa, 1992, P 14 ANN C COGN SCI; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Goodman J., 2001, CORR; Graves A., 2014, ARXIV14105401; Graves A, 2013, ARXIV13080850; Gregor K., 2015, CORR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Joulin A, 2015, ADV NEUR IN, V28; Koutnik J., 2014, ICML; Mikolov T., 2014, 14127753 ARXIV; Mikolov T., 2012, GOOGLE; Mozer M. C., 1993, ADV NEURAL INFORM PR, P863; Peng B., 2015, 150805508 ARXIV; POLLACK JB, 1991, MACH LEARN, V7, P227, DOI 10.1007/BF00114845; Steinbuch K., 1961, IEEE TRANS ELECTRON, V1, P36, DOI [10.1109/pgec.1963.263588, DOI 10.1109/PGEC.1963.263588]; Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194; Taylor W. K., 1959, PROC IEE PART B RADI, V106, P198; West JG, 2015, SCI SYNTH, P1, DOI 10.1055/sos-SD-220-00001; Weston J., 2015, 150205698 ARXIV; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zaremba Wojciech, 2014, ABS14092329 CORR	25	672	704	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101105
C	Wen, W; Wu, CP; Wang, YD; Chen, YR; Li, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wen, Wei; Wu, Chunpeng; Wang, Yandan; Chen, Yiran; Li, Hai			Learning Structured Sparsity in Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNN's evaluation. Experimental results show that SSL achieves on average 5.1 x and 3.1 x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth reduces a 20-layer Deep Residual Network (ResNet) to 18 layers while improves the accuracy from 91.25% to 92.60%, which is still higher than that of original ResNet with 32 layers. For AlexNet, SSL reduces the error by similar to 1%.	[Wen, Wei; Wu, Chunpeng; Wang, Yandan; Chen, Yiran; Li, Hai] Univ Pittsburgh, Pittsburgh, PA 15260 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Wen, W (corresponding author), Univ Pittsburgh, Pittsburgh, PA 15260 USA.	wew57@pitt.edu; chw127@pitt.edu; yaw46@pitt.edu; yic52@pitt.edu; hal66@pitt.edu	Wen, Wei/AAO-5266-2020; Li, Hai/L-8558-2017; Chen, Yiran/L-4812-2017	Wen, Wei/0000-0003-0027-4821; Li, Hai/0000-0003-3228-6544; Chen, Yiran/0000-0002-1486-8412	NSF [XPS-1337198, CCF-1615475]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF XPS-1337198 and NSF CCF-1615475. The authors thank Drs. Sheng Li and Jongsoo Park for valuable feedback on this work.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chetlur S., 2014, ARXIV; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Dentinel Zarembaw, 2014, NEURIPS, P1269; Feng T, 2015, IEEE IC COMP COM NET; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jia Y., 2014, P 22 ACM INT C MULT, P675; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J, 2010, PROCEEDINGS OF THE 17TH INTERNATIONAL CONGRESS ON SOUND AND VIBRATION; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Robertson D, 2015, ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tai C., 4 INT C LEARN REPR I; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	22	658	682	4	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704029
C	Dai, JF; Li, Y; He, KM; Sun, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Dai, Jifeng; Li, Yi; He, Kaiming; Sun, Jian			R-FCN: Object Detection via Region-based Fully Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [7, 19] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [10], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 x faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.	[Dai, Jifeng; Li, Yi; He, Kaiming; Sun, Jian] Microsoft Res, Redmond, WA 98052 USA; [Li, Yi] Tsinghua Univ, Beijing, Peoples R China	Microsoft; Tsinghua University	Dai, JF (corresponding author), Microsoft Res, Redmond, WA 98052 USA.		Dai, Jifeng/HGU-8741-2022; Dai, Jifeng/AAF-7709-2019	Dai, Jifeng/0000-0002-6785-0785; 				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Dai J., 2016, ARXIV160308678; Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276; Everingham Mark, 2010, IJCV; Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2014, ECCV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lenc K, 2015, ARXIV150606981; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mallat S., 1998, WAVELET TOUR SIGNAL, DOI 10.1016/B978-0-12-374370-1.X0001-8; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren S, 2015, ARXIV150406066, VPP, P1; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy Christian, 2013, ADV NEURAL INFORM PR, P3, DOI DOI 10.5555/2999792.2999897; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	29	655	672	48	195	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703101
C	Eigen, D; Puhrsch, C; Fergus, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Eigen, David; Puhrsch, Christian; Fergus, Rob			Depth Map Prediction from a Single Image using a Multi-Scale Deep Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.	[Eigen, David; Puhrsch, Christian; Fergus, Rob] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA	New York University	Eigen, D (corresponding author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.	deigen@cs.nyu.edu; cpuhrsch@nyu.edu; fergus@cs.nyu.edu			ONR [N00014-13-1-0646]; NSF [1116923, 1149633]; Microsoft Research	ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); Microsoft Research(Microsoft)	The authors are grateful for support from ONR #N00014-13-1-0646, NSF #1116923, #1149633 and Microsoft Research.	Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Fouhey D. F., 2013, ICCV; Geiger A., 2013, INT J ROBOTICS RES I; Hadsell R, 2009, J FIELD ROBOT, V26, P120, DOI 10.1002/rob.20276; Hartley R., 2004, ROBOTICA; Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232; Karsch K., 2014, TPAMI; Konda K., 2013, ARXIV13123429V2; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Ladicky M. P. Lubor, 2014, CVPR; Levin A., 2007, SIGGRAPH; Liu C., 2008, SIFT FLOW DENSE CORR; Memisevic R., 2011, NIPS WORKSH DEEP LAR; Michels Jeff, 2005, P 22 INT C MACH LEAR, P593, DOI DOI 10.1145/1102351.1102426; Saxena A., 2005, NIPS; Saxena A., 2008, TPAMI; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Silberman N., 2012, ECCV; Sinz FH, 2004, LECT NOTES COMPUT SC, V3175, P245; Snavely N., 2006, PHOTO TOURISM EXPLOR; Yamaguchi K., 2012, ARXIV12041393V1	21	649	668	22	104	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102100
C	Jaakkola, TS; Haussler, D		Kearns, MS; Solla, SA; Cohn, DA		Jaakkola, TS; Haussler, D			Exploiting generative models in discriminative classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines fr om generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Jaakkola, TS (corresponding author), MIT, Artificial Intelligence Lab, 545 Technol Sq, Cambridge, MA 02139 USA.							Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Durbin R., 1998, BIOL SEQUENCE ANAL P; Hubbard TJP, 1997, NUCLEIC ACIDS RES, V25, P236, DOI 10.1093/nar/25.1.236; Jaakkola T., 1998, EXPLOITING GENERATIV; MACKAY D, 1997, INTRO GAUSSIAN PROCE; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; WAHBA G, 1990, CBMS NSF REG C SER A	7	640	661	1	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						487	493						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700069
C	Rasmussen, CE		Solla, SA; Leen, TK; Muller, KR		Rasmussen, CE			The infinite Gaussian mixture model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				NONPARAMETRIC PROBLEMS; BAYESIAN-ANALYSIS	In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the "right" number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.	Tech Univ Denmark, Dept Math Modelling, DK-2800 Lyngby, Denmark	Technical University of Denmark	Rasmussen, CE (corresponding author), Tech Univ Denmark, Dept Math Modelling, Bldg 321, DK-2800 Lyngby, Denmark.			Rasmussen, Carl Edward/0000-0001-8899-7850				ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; GILKS WR, 1992, APPL STAT, V41, P337, DOI DOI 10.2307/2347565; Neal RM, 1996, LECT NOTES STAT, V118; NEAL RM, 1998, 4915 U TOR DEP STAT; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; UEDA N, 1998, 11 NIPS; West M., 1994, ASPECTS UNCERTAINTY, P363; WILLIAMS CKI, 1996, NIPS, V8	9	614	632	1	18	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						554	560						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700079
C	Zhang, X; Zhao, JB; Yann, LC		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhang, Xiang; Zhao, Junbo; Yann Lecun			Character-level Convolutional Networks for Text Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.	[Zhang, Xiang; Zhao, Junbo; Yann Lecun] NYU, Courant Inst Math Sci, 719 Broadway,12th Floor, New York, NY 10003 USA	New York University	Zhang, X (corresponding author), NYU, Courant Inst Math Sci, 719 Broadway,12th Floor, New York, NY 10003 USA.	xiang@cs.nyu.edu; junbo.zhao@cs.nyu.edu; yann@cs.nyu.edu			NVIDIA Corporation; Amazon.com Inc	NVIDIA Corporation; Amazon.com Inc	We gratefully acknowledge the support of NVIDIA Corporation with the donation of 2 Tesla K40 GPUs used for this research. We gratefully acknowledge the support of Amazon. com Inc for an AWS in Education Research grant used for this research.	Bottou L., 1989, P EUR, V89, P537; Boureau Y.L., 2010, P 27 INT C MACH LEAR, P111; BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963; Collobert R., 2011, BIGLEARN NIPS WORKSH; Collobert R, 2011, J MACH LEARN RES, V12, P2493; dos Santos Cicero, 2014, COLING, P69; Fellbaum Christiane, 2005, ENCY LANGUAGE LINGUI, P665; Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042; Greff K., 2015, CORR, Vabs/1503.04069; Hinton G.E., 2012, ARXIV; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Joachims T., 1998, P EUROPEAN C MACHINE, P137, DOI [10.1007/bfb0026683, 10.1007/BFb0026683]; Johnson R., 2014, CORR; Kanaris I, 2007, INT J ARTIF INTELL T, V16, P1047, DOI 10.1142/S0218213007003692; Kim Y., 2014, P 2014 C EMP METH NA; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lehmann J., 2014, SEMANTIC WEB J; Lev G, 2015, LECT NOTES COMPUT SC, V9103, P35, DOI 10.1007/978-3-319-19581-0_3; McAuley Julian, 2013, P 7 ACM C REC SYST A, DOI DOI 10.1145/2507157.2507163; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Nair V., 2010, ICML, P807; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Polyak B.T., 1964, USSR COMP MATH MATH+, V4, P1, DOI [10.1016/0041-5553(64)90137-5, DOI 10.1016/0041-5553(64)90137-5]; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Santos C. D., 2014, P 31 INT C MACH LEAR, P1818; Shen Y, 2014, 23 ACM INT C CIKM 20, P101, DOI 10.1145/2661829.2661935; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701; Wang Canhui, 2008, P 17 INT C WORLD WID, P457	32	587	592	5	22	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101042
C	Maron, O; Lozano-Perez, T		Jordan, MI; Kearns, MJ; Solla, SA		Maron, O; Lozano-Perez, T			A framework for multiple-instance learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Multiple-instance learning is a variation on supervised learning, where the task is to learn a concept given positive and negative bags of instances. Each bag may contain many instances, but a bag is labeled positive even if only one of the instances in it falls within the concept. A bag is labeled negative only if all the instances in it are negative. We describe a new general framework, called Diverse Density, for solving multiple-instance learning problems. We apply this framework to learn a simple description of a person from a series of images (bags) containing that person, to a stock selection problem, and to the drug activity prediction problem.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Maron, O (corresponding author), MIT, AI Lab, NE43-755, Cambridge, MA 02139 USA.		Lozano-Perez, Tomas/J-9374-2012	Lozano-Perez, Tomas/0000-0002-8657-2450					0	580	620	0	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						570	576						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700081
C	Cauwenberghs, G; Poggio, T		Leen, TK; Dietterich, TG; Tresp, V		Cauwenberghs, G; Poggio, T			Incremental and decremental support vector machine learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					An on-line recursive algorithm for training support vector machines, one vector at a time, is Presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental "unlearning" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.	Johns Hopkins Univ, Dept ECE, CLSP, Baltimore, MD 21218 USA	Johns Hopkins University	Cauwenberghs, G (corresponding author), Johns Hopkins Univ, Dept ECE, CLSP, Baltimore, MD 21218 USA.	gert@jhu.edu; tp@ai.mit.edu						CAMPBELL C, 2000, P 17 INT C MACH LEAR; CSATO L, 2001, ADV NEURAL INFORMATI, V13; Friess T.-T., 1998, 15 INT C MACH LEARN; JAAKKOLA TS, 1998, P 7 INT WORKSH ART I; Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169; OPPER M, 2000, ADV LARGE MARGIN CLA, P43; Osuna E, 1997, NEURAL NETWORKS FOR SIGNAL PROCESSING VII, P276, DOI 10.1109/NNSP.1997.622408; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Pontil M, 1998, NEURAL COMPUT, V10, P955, DOI 10.1162/089976698300017575; SCHOLKOPF B, 1999, 99035 NEUR; Syed N.A., 1999, P WORKSH SUPP VECT M; VAPNIK V, 2000, ADV LARGE MARGIN CLA; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	13	571	608	8	38	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						409	415						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800058
C	Zhu, JY; Zhang, R; Pathak, D; Darrell, T; Efros, AA; Wang, O; Shechtman, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhu, Jun-Yan; Zhang, Richard; Pathak, Deepak; Darrell, Trevor; Efros, Alexei A.; Wang, Oliver; Shechtman, Eli			Toward Multimodal Image-to-Image Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.	[Zhu, Jun-Yan; Zhang, Richard; Pathak, Deepak; Darrell, Trevor; Efros, Alexei A.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Wang, Oliver; Shechtman, Eli] Adobe Res, San Jose, CA USA	University of California System; University of California Berkeley; Adobe Systems Inc.	Zhu, JY (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.		Jeong, Yongwook/N-7413-2016; Shechtman, Eli/B-2736-2012	Efros, Alexei A./0000-0001-5720-8070	Adobe Inc.; DARPA; DoD MURI award [N000141110688]; NSF [IIS-1633310, IIS-1427425, IIS-1212798]; Facebook Graduate Fellowship; Adobe Research Fellowship; NVIDIA Graduate Fellowship; AFRL; Berkeley Artificial Intelligence Research (BAIR) Lab	Adobe Inc.; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DoD MURI award; NSF(National Science Foundation (NSF)); Facebook Graduate Fellowship(Facebook Inc); Adobe Research Fellowship; NVIDIA Graduate Fellowship; AFRL(United States Department of DefenseUS Air Force Research Laboratory); Berkeley Artificial Intelligence Research (BAIR) Lab	We thank Phillip Isola and Tinghui Zhou for helpful discussions. This work was supported in part by Adobe Inc., DARPA, AFRL, DoD MURI award N000141110688, NSF awards IIS-1633310, IIS-1427425, IIS-1212798, the Berkeley Artificial Intelligence Research (BAIR) Lab, and hardware donations from NVIDIA. JYZ is supported by Facebook Graduate Fellowship, RZ by Adobe Research Fellowship, and DP by NVIDIA Graduate Fellowship.	Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; [Anonymous], 2017, ICLR; [Anonymous], 2017, ICCV; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Bansal A., 2017, PROC INT C LEARN REP; Chen X., 2016, ARXIV160603657, P2172; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Dinh L, 2017, 5 INT C LEARN REPR I; Donahue J., 2016, ARXIV160509782; Dosovitskiy Alexey, 2016, NEURIPS; Dumoulin Vincent, 2016, ARXIV E PRINTS; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265; Ghosh A., 2017, ARXIV170402906; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Iizuka S., 2016, ACM T GRAPH, V35, P4; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Krizhevsky A., 2014, ARXIV; Laffont P.Y., 2014, SIGGRAPH; Larsen ABL, 2016, PR MACH LEARN RES, V48; Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mathieu Michael, 2016, ICLR; Mirza M., 2014, ARXIV; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Radford A., 2016, ICLR 2016 INT C LEAR, DOI DOI 10.1007/S11280-018-0565-2; Reed S, 2016, PR MACH LEARN RES, V48; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Shlens J., 2017, ARXIV170507208; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Sohn Kihyuk, 2015, NEURAL INFORM PROCES; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Walker Jacob, 2016, P EUR C COMP VIS ECC; Xian W., 2017, ARXIV170602823; Xue T., 2016, NIPS; Yang C., 2017, PROC CVPR IEEE, P6721, DOI DOI 10.1109/CVPR.2017.434; Yu A., 2014, CVPR; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	55	558	573	7	16	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400045
C	Andrychowicz, M; Denil, M; Colmenarejo, SG; Hoffman, MW; Pfau, D; Schaul, T; Shillingford, B; de Freitas, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Andrychowicz, Marcin; Denil, Misha; Colmenarejo, Sergio Gomez; Hoffman, Matthew W.; Pfau, David; Schaul, Tom; Shillingford, Brendan; de Freitas, Nando			Learning to learn by gradient descent by gradient descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ADAPTATION; SEARCH; TERM	The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.	[Andrychowicz, Marcin; Denil, Misha; Colmenarejo, Sergio Gomez; Hoffman, Matthew W.; Pfau, David; Schaul, Tom; Shillingford, Brendan; de Freitas, Nando] Google DeepMind, London, England; [Shillingford, Brendan; de Freitas, Nando] Univ Oxford, Oxford, England; [de Freitas, Nando] Canadian Inst Adv Res, Toronto, ON, Canada	Google Incorporated; University of Oxford; Canadian Institute for Advanced Research (CIFAR)	Andrychowicz, M (corresponding author), Google DeepMind, London, England.	marcin.andrychowicz@gmail.com; mdenil@google.com; sergomez@google.com; mwhoffman@google.com; pfau@google.com; schaul@google.com; brendan.shillingford@cs.ox.ac.uk; nandodefreitas@google.com						Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; BENGIO S, 1995, NEURAL PROCESS LETT, V2, P26, DOI 10.1007/BF02279935; Bengio Y., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), DOI 10.1109/IJCNN.1991.155621; Bobolas F., 2009, BRAIN NEURONS; Cotter N. E., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P553, DOI 10.1109/IJCNN.1990.137898; Daniel C., 2016, ASS ADVANCEMENT ARTI; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Feldkamp LA, 1998, P IEEE, V86, P2259, DOI 10.1109/5.726790; Gatys LA, 2015, 150806576 ARXIV, V4, P150806576, DOI 10.1167/16.12.326; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lake B. M., 2016, 160400289 ARXIV; Maley T., 2011, NEURON; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Nemhauser G.L., 1988, INTEGER COMBINATORIA; RIEDMILLER M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P586, DOI 10.1109/ICNN.1993.298623; Runarsson TP, 2000, 2000 IEEE SYMPOSIUM ON COMBINATIONS OF EVOLUTIONARY COMPUTATION AND NEURAL NETWORKS, P59, DOI 10.1109/ECNN.2000.886220; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J, 1997, MACH LEARN, V28, P105, DOI 10.1023/A:1007383707642; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; SCHMIDHUBER J, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P407, DOI 10.1109/ICNN.1993.298591; Schmidhuber J, 1987, THESIS; Schraudolph NN, 1999, IEE CONF PUBL, P569, DOI 10.1049/cp:19991170; SUTTON RS, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P171; Thrun S, 1998, LEARNING TO LEARN, P181; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tseng P, 1998, SIAM J OPTIMIZ, V8, P506, DOI 10.1137/S1052623495294797; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893; Younger A. S., 2001, INT JOINT C NEUR NET; Younger AS, 1999, IEEE T NEURAL NETWOR, V10, P272, DOI 10.1109/72.750553	35	547	562	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703097
C	Konda, VR; Tsitsiklis, JN		Solla, SA; Leen, TK; Muller, KR		Konda, VR; Tsitsiklis, JN			Actor-critic algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				DIFFERENCE	We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.	MIT, Informat & Decis Syst Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Konda, VR (corresponding author), MIT, Informat & Decis Syst Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	konda@mit.edu; jnt@mit.edu	Ma, Jialin/ABG-2965-2021					Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3; Cao XR, 1997, IEEE T AUTOMAT CONTR, V42, P1382, DOI 10.1109/9.633827; GLYNN PW, 1986, P 1986 WINT SIM C, P285; Jaakkola T., 1995, Advances in Neural Information Processing Systems 7, P345; Konda VR, 1999, SIAM J CONTROL OPTIM, V38, P94, DOI 10.1137/S036301299731669X; Marbach P., 1998, THESIS MIT; MARBACH P, UNPUB IEEE T AUTOMAT; Sutton R. S., 1995, REINFORCEMENT LEARNI; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Tsitsiklis JN, 1999, AUTOMATICA, V35, P1799, DOI 10.1016/S0005-1098(99)00099-0; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	12	547	576	2	13	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1008	1014						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700142
C	Liu, MY; Breuel, T; Kautz, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Ming-Yu; Breuel, Thomas; Kautz, Jan			Unsupervised Image-to-Image Translation Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.	[Liu, Ming-Yu; Breuel, Thomas; Kautz, Jan] NVIDIA, Santa Clara, CA 95051 USA	Nvidia Corporation	Liu, MY (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.	mingyul@nvidia.com; tbreuel@nvidia.com; jkautz@nvidia.com	Jeong, Yongwook/N-7413-2016					Arjovsky M, 2017, PR MACH LEARN RES, V70; Cordts M., 2015, CVPR WORKSH FUT DAT, V2, P1; Denton Emily L, 2015, NEURIPS, V2, P4; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Ganin Y., 2016, JMLR, V17, P2096; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Larsen ABL, 2016, PR MACH LEARN RES, V48; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lindvall T., 2002, LECT COUPLING METHOD; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Maaloe L, 2016, PR MACH LEARN RES, V48; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Salimans T, 2016, ADV NEUR IN, V29; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; van den Oord Aaron, 2016, ARXIV160605328; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Zhu Jun-Yan, 2017, ICCV	28	543	551	6	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400067
C	Lu, JS; Yang, JW; Batra, D; Parikh, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lu, Jiasen; Yang, Jianwei; Batra, Dhruv; Parikh, Devi			Hierarchical Question-Image Co-Attention for Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling "where to look" or visual attention, it is equally important to model "what words to listen to" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.	[Lu, Jiasen; Yang, Jianwei; Batra, Dhruv; Parikh, Devi] Virginia Tech, Blacksburg, VA 24061 USA; [Batra, Dhruv; Parikh, Devi] Georgia Inst Technol, Atlanta, GA 30332 USA	Virginia Polytechnic Institute & State University; University System of Georgia; Georgia Institute of Technology	Lu, JS (corresponding author), Virginia Tech, Blacksburg, VA 24061 USA.	jiasenlu@vt.edu; jw2yang@vt.edu; dbatra@vt.edu; parikh@vt.edu			NSF CAREER; ONR; ONR [N00014-14-1-0679]; Sloan Fellowship; ARO; Paul G. Allen Family Foundation; ICTAS Junior Faculty awards; Google; AWS in Education Research grant	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); ONR(Office of Naval Research); Sloan Fellowship(Alfred P. Sloan Foundation); ARO; Paul G. Allen Family Foundation; ICTAS Junior Faculty awards; Google(Google Incorporated); AWS in Education Research grant	This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, a Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and DP, Google Faculty Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor.	Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Collobert R., 2011, NIPS; Das A., 2016, EMNLP; dos Santos C<prime>icero Nogueira, 2016, ABS160203609 CORR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gao HY, 2015, ADV NEUR IN, V28; Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693; Hu B., 2014, P 27 INT C NEUR INF, P2042; Ilievski I., 2016, FOCUSED DYNAMIC ATTE; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krishna Ranjay, 2016, ARXIV160207332; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Ma L., 2016, AAAI; Malinowski M., 2015, P INT C COMP VIS; Ren M., 2015, P 28 INT C NEUR INF, V2, P2953, DOI [10.5555/2969442.2969570, DOI 10.5555/2969442.2969570]; Rocktaschel Tim, 2016, P ICLR; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Xiong C., 2016, P INT C MACHINE LEAR; Xu H., 2015, ARXIV151105234; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yin W., 2016, T ASS COMPUT LINGUIS, P259, DOI DOI 10.1162/TACL_A_00097; Zhang P., 2015, ARXIV151105099; Zhu Y., 2016, P IEEE C COMP VIS PA; Zitnick C. L., 2016, AI MAGAZINE	27	543	562	3	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701059
C	Caruana, R; Lawrence, S; Giles, L		Leen, TK; Dietterich, TG; Tresp, V		Caruana, R; Lawrence, S; Giles, L			Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				SIZE	The conventional wisdom is that backprop nets with excess hidden units generalize poorly, We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high. non-linearity, and backprop, often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when teaming to fit regions of high non-linearity.	Carnegie Mellon Univ, CALD, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Caruana, R (corresponding author), Carnegie Mellon Univ, CALD, 500 Forbes Ave, Pittsburgh, PA 15213 USA.	caruana@cs.emu.edu; lawrence@research.nj.nec.com; giles@ist.psu.edu						Bartlett PL, 1997, ADV NEUR IN, V9, P134; Baum EB, 1989, NEURAL COMPUT, V1, P151, DOI 10.1162/neco.1989.1.1.151; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Hertz J.A, 1992, NIPS 91 P 4 INT C NE, P950; LECUN Y, 1989, ADV NEURAL INFORMATI, V2, P598; Martin GL, 1991, NEURAL COMPUT, V3, P258, DOI 10.1162/neco.1991.3.2.258; MOODY JE, 1992, ADV NEUR IN, V4, P847; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Sejnowski T. J., 1987, Complex Systems, V1, P145; Weigend A., 1994, P 1993 CONN MOD SUMM, V1, P335; Wolpert DH, 1997, NEURAL COMPUT, V9, P1211, DOI 10.1162/neco.1997.9.6.1211; [No title captured]	13	526	531	1	8	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						402	408						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800057
C	Sutton, RS		Touretzky, DS; Mozer, MC; Hasselmo, ME		Sutton, RS			Generalization in reinforcement learning: Successful examples using sparse coarse coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MASSACHUSETTS,AMHERST,MA 01003	University of Massachusetts System; University of Massachusetts Amherst									0	517	531	0	5	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1038	1044						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00146
C	Salimans, T; Kingma, DP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Salimans, Tim; Kingma, Diederik P.			Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.	[Salimans, Tim; Kingma, Diederik P.] OpenAI, San Francisco, CA 94110 USA		Salimans, T (corresponding author), OpenAI, San Francisco, CA 94110 USA.	tim@openai.com; dpkingma@openai.com						Amari S, 1997, ADV NEUR IN, V9, P127; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Cooijmans T., 2016, ARXIV160309025; Desjardins G., 2015, ADV NEURAL INFORM PR, P2071; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow Ian J., 2013, ICML; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Grosse RB, 2015, PR MACH LEARN RES, V37, P2304; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krahenbuhl P., 2015, ARXIV151106856; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lee C.-Y., 2014, ARXIV14095185; Lin M., 2014, ICLR C TRACK; Martens J., 2010, P 27 INT C MACH LEAR, P735; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Mishkin Dmytro, 2015, ICLR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Raiko T, 2012, P INT C ART INT STAT, V22, P924; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Springenberg J.T., 2014, ARXIV14126806; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Zhang SL, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2635	33	509	531	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700078
C	Williams, CKI; Rasmussen, CE		Touretzky, DS; Mozer, MC; Hasselmo, ME		Williams, CKI; Rasmussen, CE			Gaussian processes for regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						ASTON UNIV,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND	Aston University				Rasmussen, Carl Edward/0000-0001-8899-7850					0	509	520	2	31	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						514	520						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00073
C	Luo, WJ; Li, YJ; Urtasun, R; Zemel, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Luo, Wenjie; Li, Yujia; Urtasun, Raquel; Zemel, Richard			Understanding the Effective Receptive Field in Deep Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.	[Luo, Wenjie; Li, Yujia; Urtasun, Raquel; Zemel, Richard] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	University of Toronto	Luo, WJ (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.	wenjie@cs.toronto.edu; yujiali@cs.toronto.edu; urtasun@cs.toronto.edu; zemel@cs.toronto.edu	Luo, Wenjie/AAG-5800-2020					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Chen XZ, 2015, ADV NEUR IN, V28; Eger S, 2013, J INTEGER SEQ, V16; Erhan D, 2009, VISUALIZING HIGHER L, P1341; Glorot X., 2010, PROC MACH LEARN RES, P249; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Kanwisher N, 1997, J NEUROSCI, V17, P4302; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lovsz L, 2003, DISCRETE MATH ELEMEN; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Mordvintsev A., INCEPTIONISM GOING D; Neuschel Thorsten, 2014, J INTEGER SEQUENCES, V17, P3; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ungerleider Leslie G., 1994, Current Opinion in Neurobiology, V4, P157, DOI 10.1016/0959-4388(94)90066-3; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yu F., 2016, P ICLR 2016; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou B., 2014, CORR, V1412, P6856	23	504	531	12	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701060
C	Snell, J; Swersky, K; Zemel, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Snell, Jake; Swersky, Kevin; Zemel, Richard			Prototypical Networks for Few-shot Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.	[Snell, Jake] Univ Toronto, Vector Inst, Toronto, ON, Canada; [Snell, Jake; Swersky, Kevin] Twitter, San Francisco, CA USA; [Zemel, Richard] Univ Toronto, Vector Inst, Canadian Inst Adv Res, Toronto, ON, Canada	University of Toronto; Twitter, Inc.; Canadian Institute for Advanced Research (CIFAR); University of Toronto	Snell, J (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.		Jeong, Yongwook/N-7413-2016		Samsung GRP project; Canadian Institute for Advanced Research	Samsung GRP project(Samsung); Canadian Institute for Advanced Research(Canadian Institute for Advanced Research (CIFAR))	We would like to thank Marc Law, Sachin Ravi, Hugo Larochelle, Renjie Liao, and Oriol Vinyals for helpful discussions. This work was supported by the Samsung GRP project and the Canadian Institute for Advanced Research.	Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bellet Aurelien, 2013, ARXIV13066709; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Edwards H., 2017, NEURAL STAT, P2; Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321; Finn C, 2017, PR MACH LEARN RES, V70; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koch G., 2015, ICML DEEP LEARNING W; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Lake Brenden, 2011, C COGN SCI SOC, P6; Liao Renjie, 2016, ADV NEURAL INFORM PR, P2; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856; Min RQ, 2009, IEEE DATA MINING, P357, DOI 10.1109/ICDM.2009.27; Ravi S., 2017, INT C LEARN REPR, P12; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rippel O., 2016, P ICLR, P1; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salakhutdinov R, 2005, NEURAL INF PROCESS S, P513; Salakhutdinov Ruslan, 2007, J MACHINE LEARNING R, P412, DOI DOI 10.1109/ICCV.2017.74; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Welinder P., 2010, CNSTR2010001 CALTECH; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Zhang ZM, 2016, LECT NOTES COMPUT SC, V9911, P533, DOI 10.1007/978-3-319-46478-7_33	36	497	499	13	67	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404015
C	Mika, S; Scholkopf, B; Smola, A; Muller, KR; Scholz, M; Ratsch, G		Kearns, MS; Solla, SA; Cohn, DA		Mika, S; Scholkopf, B; Smola, A; Muller, KR; Scholz, M; Ratsch, G			Kernel PCA and de-noising in feature spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms. But it can also be considered as a natural generalization of linear principal component analysis. This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA. This is a nontrivial task, as the results provided by kernel PCA live in some high dimensional feature space and need not have pre-images in input space. This work presents ideas for finding approximate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data.	GMD FIRST, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Mika, S (corresponding author), GMD FIRST, Rudower Chaussee 5, D-12489 Berlin, Germany.	mika@first.gmd.de; bs@first.gmd.de; smola@first.gmd.de; klaus@first.gmd.de; scholz@first.gmd.de; raetsch@first.gmd.de	Scholz, Matthias/B-3508-2018; Mueller, Klaus-Robert/Y-3547-2019	Scholz, Matthias/0000-0003-1414-5924; Mueller, Klaus-Robert/0000-0002-3861-7685; Ratsch, Gunnar/0000-0001-5486-8532				Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; BURGES CJC, 1996, P 13 INT C MACH LEAR, P71; Diamantaras K.I., 1996, PRINCIPAL COMPONENT; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Saitoh S., 1988, THEORY REPROD KERNEL; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 1998, DAGM S, P124; Scholkopf Bernhard, 1997, SUPPORT VECTOR LEARN	9	494	521	1	24	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						536	542						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700076
C	Long, MS; Cao, ZJ; Wang, JM; Jordan, MI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Long, Mingsheng; Cao, Zhangjie; Wang, Jianmin; Jordan, Michael I.			Conditional Adversarial Domain Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Adversarial learning has been embedded into deep networks to learn disentangled and transferable representations for domain adaptation. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions native in classification problems. In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks (CDANs) are designed with two novel conditioning strategies: multilinear conditioning that captures the cross-covariance between feature representations and classifier predictions to improve the discriminability, and entropy conditioning that controls the uncertainty of classifier predictions to guarantee the transferability. With theoretical guarantees and a few lines of codes, the approach has exceeded state-of-the-art results on five datasets.	[Long, Mingsheng] Tsinghua Univ, Sch Software, Beijing, Peoples R China; [Cao, Zhangjie; Wang, Jianmin] Tsinghua Univ, KLiss, MOE, Beijing, Peoples R China; [Cao, Zhangjie; Wang, Jianmin] Tsinghua Univ, BNRist, Beijing, Peoples R China; [Cao, Zhangjie; Wang, Jianmin] Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China; [Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; University of California System; University of California Berkeley	Long, MS (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.	mingsheng@tsinghua.edu.cn; caozhangjie14@gmail.com; jimwang@tsinghua.edu.cn; jordan@berkeley.edu	Jordan, Michael I/C-5253-2013; wang, jian/GVS-0711-2022	Jordan, Michael/0000-0001-8935-817X	National Key RAMP;D Program of China [2016YFB1000701]; Natural Science Foundation of China [61772299, 71690231, 61502265]; DARPA Program on Lifelong Learning Machines	National Key RAMP;D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); DARPA Program on Lifelong Learning Machines	We thank Yuchen Zhang at Tsinghua University for insightful discussions. This work was supported by the National Key R&D Program of China (2016YFB1000701), the Natural Science Foundation of China (61772299, 71690231, 61502265) and the DARPA Program on Lifelong Learning Machines.	Arjovsky M., 2017, INT C LEARN REPR ITL; Arjovsky M, 2017, PR MACH LEARN RES, V70; Ben-David S., 2007, ADV NEURAL INFORM PR, V19, P137; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Che T, 2017, INT C LEARN REPR ICL; Chen YH, 2017, IEEE I CONF COMP VIS, P2011, DOI 10.1109/ICCV.2017.220; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Courty N, 2017, ADV NEUR IN, V30; Donahue J, 2014, PR MACH LEARN RES, V32; Ganin Y., 2016, JMLR, V17, P2096; Ganin Yaroslav, 2015, ICML; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Grandvalet Y., 2005, CAP, P529; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffman Judy, 2014, NIPS; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; Huang J., 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0080; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kar P., 2012, ARTIF INTELL, P583; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Liu Ming-Yu, 2017, NIPS; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Long MS, 2016, ADV NEUR IN, V29; Mansour Yishay, 2009, COLT 2009 22 C LEARN; Mirza M., 2014, ARXIV; Odena A, 2017, PR MACH LEARN RES, V70; Oquab M., 2013, IEEE C COMP VIS PATT; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Sha, 2013, P INT C MACH LEARN; SONG L, 2013, P ADV NEUR INF PROC, P3228; Song L., 2010, INT C MACH LEARN ICM; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Song Le, 2009, P 26 INT C MACHINE L, P961, DOI 10.1145/1553374.1553497; Sugiyama M., 2008, NIPS, P1433; Sugiyama M, 2007, J MACH LEARN RES, V8, P985; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819; Zhu Jun-Yan, 2017, ICCV	56	480	489	16	35	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301061
C	Hermann, KM; Kocisky, T; Grefenstette, E; Espeholt, L; Kay, W; Suleyman, M; Blunsom, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hermann, Karl Moritz; Kocisky, Tomas; Grefenstette, Edward; Espeholt, Lasse; Kay, Will; Suleyman, Mustafa; Blunsom, Phil			Teaching Machines to Read and Comprehend	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.	[Hermann, Karl Moritz; Kocisky, Tomas; Grefenstette, Edward; Espeholt, Lasse; Kay, Will; Suleyman, Mustafa; Blunsom, Phil] Google DeepMind, London, England; [Kocisky, Tomas; Blunsom, Phil] Univ Oxford, Oxford, England	Google Incorporated; University of Oxford	Hermann, KM (corresponding author), Google DeepMind, London, England.	kmh@google.com; tkocisky@google.com; etg@google.com; lespeholt@google.com; wkay@google.com; mustafasul@google.com; pblunsom@google.com						Collobert R, 2011, J MACH LEARN RES, V12, P2493; Das Dipanjan, 2013, COMPUT LINGUIST, V40, P9; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hermann Karl Moritz, 2014, P ACL JUN; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062; Mnih Volodymyr, ADV NEURAL INFORM PR, V27; Poon Hoifung, P NAACL HLT 2010 1 I; Richardson Matthew, P EMNLP; Riloff Ellen, P ANLP NAACL WORKSH; Sukhbaatar S., 2015, ABS150308895 CORR; Sutskever Ilya, ADV NEURAL INFORM PR, V27; Svore Krysta, P EMNLP CONLL; Taylor WL, 1953, JOURNALISM QUART, V30, P415, DOI 10.1177/107769905303000401; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Weston J., 2014, ARXIV14103916; Winograd Terry, 1972, UNDERSTANDING NATURA; Woodsend Kristian, 2010, P ACL	19	473	520	2	22	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102093
C	Lakshminarayanan, B; Pritzel, A; Blundell, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lakshminarayanan, Balaji; Pritzel, Alexander; Blundell, Charles			Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.	[Lakshminarayanan, Balaji; Pritzel, Alexander; Blundell, Charles] DeepMind, London, England		Lakshminarayanan, B (corresponding author), DeepMind, London, England.	balajiln@google.com; apritzel@google.com; cblundell@google.com	Jeong, Yongwook/N-7413-2016					Abbasi M, 2017, ARXIV PREPRINT ARXIV; Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300; Amodei D., 2016, CONCRETE PROBLEMS AI; Bernardo J. M., 1994, BAYESIAN THEORY; Bishop C.M., 1994, MIXTURE DENSITY NETW; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Clarke B., 2003, J MACHINE LEARNING R, V4, P683, DOI [10.1162/153244304773936090, DOI 10.1162/153244304773936090]; Dawid A. P., 1982, J AM STAT ASS; DeGroot M. H., 1983, STATISTICIAN; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Gal Y, 2016, PR MACH LEARN RES, V48; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Hasenclever L., 2015, ARXIV151209327; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1699; Hinton G., 2015, ARXIV150302531; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Huang G., 2017, ICLR SUBMISSION; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Korattikara A., 2015, NIPS; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A, 2016, INT C LEARN REPR SAN; Lakshminarayanan B., 2016, THESIS; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee S., 2016, ADV NEURAL INFORM PR, V29, P2119; Lee Stefan, 2015, ARXIV151106314, P2; Li Y., 2015, NIPS; Louizos C, 2016, PR MACH LEARN RES, V48; Mackay D.J.C., 1992, THESIS CALIFORNIA I; Maeda S.-i., 2014, ARXIV PREPRINT ARXIV; Mikolov T., 2013, ARXIV; Minka T., 2000, BAYESIAN MODEL AVERA; Miyato T., 2016, ICLR; Nair V, 2010, P 27 INT C MACHINE L, P807; Neal Radford M, 1996, BAYESIAN LEARNING NE, V118, DOI [10.1007/978-1-4612-0745-0, DOI 10.1007/978-1-4612-0745-0]; NIX DA, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P55, DOI 10.1109/ICNN.1994.374138; Quinonero-Candela J., 2006, MACHINE LEARNING CHA; Rasmussen Carl Edward, 2005, ICML; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Singh S., 2016, ADV NEURAL INFORM PR, V29; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tramer F., 2017, ARXIV; Vinyals Oriol, 2016, ARXIV160604080, P3630; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/nmeth.3547, 10.1038/NMETH.3547]	60	471	473	7	21	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406046
C	Lu, JS; Batra, D; Parikh, D; Lee, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lu, Jiasen; Batra, Dhruv; Parikh, Devi; Lee, Stefan			ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks - visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval - by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.	[Lu, Jiasen; Batra, Dhruv; Parikh, Devi; Lee, Stefan] Georgia Inst Technol, Atlanta, GA 30332 USA; [Lee, Stefan] Oregon State Univ, Corvallis, OR 97331 USA; [Batra, Dhruv; Parikh, Devi] Facebook AI Res, Menlo Pk, CA USA	University System of Georgia; Georgia Institute of Technology; Oregon State University; Facebook Inc	Lu, JS (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.				NSF; AFRL; DARPA; ONR YIPs; ARO PECASE	NSF(National Science Foundation (NSF)); AFRL(United States Department of DefenseUS Air Force Research Laboratory); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR YIPs; ARO PECASE	The Georgia Tech effort was supported in part by NSF, AFRL, DARPA, ONR YIPs, ARO PECASE. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.	Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Agrawal Harsh, 2018, ARXIV181208658; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73; Boden M., 2008, MIND MACHINE HIST CO; Chelba Ciprian, 2014, 15 ANN C INT SPEECH, DOI DOI 10.21437/INTERSPEECH.2014-564; Chen X, 2015, CORR, V1504, P325; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; Das Abhishek, 2018, CVPR; De Vries H., 2017, ADV NEURAL INFORM PR, P6594; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141; Hariharan B., 2017, P IEEE C COMP VIS PA; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jayaraman D, 2015, IEEE I CONF COMP VIS, P1413, DOI 10.1109/ICCV.2015.166; Jayaraman Dinesh, 2018, EUR C COMP VIS, P120; Kazemzadeh Sahar, 2014, EMNLP; Krishna Ranjay, 2016, ARXIV160207332; Lample G., 2019, ARXIV190107291; Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96; Lee Kuang-Huei, 2018, P EUR C COMP VIS; Li Gen, 2019, ARXIV190806066; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Peng, 2018, ARXIV181205252; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556; Shekhar R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P255, DOI 10.18653/v1/P17-1024; Su Weijie, 2019, VL BERT PRETRAINING; Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756; Tan Hao, 2019, P EMNLP, DOI [10.18653/v1/D19-1514, DOI 10.18653/V1/D19-1514]; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Young Peter, 2014, T ASSOC COMPUT LING, V2, P67; Yu L., 2018, CVPR; Zellers Rowan, 2019, P CVPR, P2839; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhou Luowei, 2019, ARXIV190911059; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	46	469	482	14	24	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300002
C	Tarvainen, A; Valpola, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tarvainen, Antti; Valpola, Harri			Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORKS; APPROXIMATION; DROPOUT	The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.	[Tarvainen, Antti; Valpola, Harri] Curious AI Co, Helsinki, Finland		Tarvainen, A (corresponding author), Curious AI Co, Helsinki, Finland.	tarvaina@cai.fi; harri@cai.fi	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Bachman Philip, 2014, ARXIV14124864CSSTAT; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Gal Y, 2016, PR MACH LEARN RES, V48; Gastaldi Xavier, 2017, ARXIV170507485CS; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hu Jie, 2017, ARXIV170901507CS; Huang Gao, 2016, ARXIV160309382CS; Kingma D.P, P 3 INT C LEARNING R; Laine Samuli, 2016, ARXIV161002242CS; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Miyato Takeru, 2017, ARXIV170403976CSSTAT; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Papernot Nicolas, 2015, ARXIV151104508CSSTAT; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Pu Yunchen, 2016, ARXIV160908976CSSTAT; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sajjadi Mehdi, 2016, NEURIPS; Salimans T, 2016, ADV NEUR IN, V29; Singh Saurabh, 2016, ARXIV160506465CS; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wager Stefan, 2013, ARXIV13071493CSSTAT; Wan L., 2013, P INT C MACHINE LEAR, P1058; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Xie Saining, 2016, ARXIV161105431CS; Zhu Xiaojin, 2002, LEARNING LABELED UNL	33	447	447	16	49	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401023
C	Weston, J; Mukherjee, S; Chapelle, O; Pontil, M; Poggio, T; Vapnik, V		Leen, TK; Dietterich, TG; Tresp, V		Weston, J; Mukherjee, S; Chapelle, O; Pontil, M; Poggio, T; Vapnik, V			Feature selection for SVMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				EXPRESSION	We introduce a method of feature selection for Support Vector Machines. The method is based upon finding those features which minimize bounds on the leave-one-out error. This search can be efficiently performed via gradient descent. The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA microarray data.	Barnhill BioInformat com, Savannah, GA 31406 USA		Weston, J (corresponding author), Barnhill BioInformat com, Savannah, GA 31406 USA.							Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5; BRADLEY PS, 1998, P 15 INT C MACH LEAR, P82; CHAPELLE O, 2000, MACHINE LEARNING; EVGENIOU T, 2000, AS C COMP VIS; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Guyon I., 2000, MACHINE LEARNING; JEBARA T, 2000, UNCERTAINITY ARTIFIC; KOHAVI J, 1995, AIJ ISSUE RELEVANCE; Oren M, 1997, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.1997.609319; PAPAGEORGIOU C, 1998, INT C COMP VIS BOMB; Tamayo P., 1999, 1677 AI MIT; Vapnik V.N, 1998, STAT LEARNING THEORY	13	446	464	0	23	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						668	674						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800095
C	Tipping, ME		Solla, SA; Leen, TK; Muller, KR		Tipping, ME			The relevance vector machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				CLASSIFICATION	The support vector machine (SVM) is a state-of-the-art technique for regression and classification, combining excellent generalisation properties with a sparse kernel representation. However, it does suffer from a number of disadvantages, notably the absence of probabilistic outputs, the requirement to estimate a trade-off parameter and the need to utilise 'Mercer' kernel functions. Ln this paper we introduce the Relevance Vector Machine (RVM), a Bayesian treatment of a generalised linear model of identical functional form to the SVM. The RVM suffers from none of the above disadvantages, and examples demonstrate that for comparable generalisation performance, the RVM requires dramatically fewer kernel functions.	Microsoft Res, Cambridge CB2 3NH, England	Microsoft	Tipping, ME (corresponding author), Microsoft Res, St George House,1 Guildhall St, Cambridge CB2 3NH, England.	mtipping@microsoft.com						Berger J. O., 1985, STAT DECISION THEORY; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; MacKay DJC, 1994, ASHRAE T, V100, P1053; Nabney IT, 1999, IEE CONF PUBL, P210, DOI 10.1049/cp:19991110; Neal Radford M, 1996, BAYESIAN LEARNING NE, V118, DOI [10.1007/978-1-4612-0745-0, DOI 10.1007/978-1-4612-0745-0]; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; Vapnik V., 1998, STAT LEARNING THEORY; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807	9	446	491	2	37	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						652	658						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700093
C	Santurkar, S; Tsipras, D; Ilyas, A; Madry, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander			How Does Batch Normalization Help Optimization?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.	[Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Santurkar, S (corresponding author), MIT, Cambridge, MA 02139 USA.	shibani@mit.edu; tsipras@mit.edu; ailyas@mit.edu; madry@mit.edu	Tsipras, Dimitris/AAZ-2505-2021		National Science Foundation (NSF) [IIS-1447786, IIS-1607189, CCF-1563880]; Intel Corporation; NSF [CCF-1553428, CNS-1815221, CCF-1617730, IIS-1741137, CNS-1413920]; Simons Investigator Award; Google Faculty Research Award; MIT-IBM Watson AI Lab research grant; Alfred P. Sloan Research Fellowship; Google Research Award	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Intel Corporation(Intel Corporation); NSF(National Science Foundation (NSF)); Simons Investigator Award; Google Faculty Research Award(Google Incorporated); MIT-IBM Watson AI Lab research grant(International Business Machines (IBM)); Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); Google Research Award(Google Incorporated)	Shibani Santurkar was supported by the National Science Foundation (NSF) under grants IIS-1447786, IIS-1607189, and CCF-1563880, and the Intel Corporation. Dimitris Tsipras was supported in part by the NSF grant CCF-1553428 and the NSF Frontier grant CNS-1413920. Andrew Ilyas was supported in part by NSF awards CCF-1617730 and IIS-1741137, a Simons Investigator Award, a Google Faculty Research Award, and an MIT-IBM Watson AI Lab research grant. Aleksander Madry was supported in part by an Alfred P. Sloan Research Fellowship, a Google Research Award, and the NSF grants CCF-1553428 and CNS-1815221.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Djork-Arn, ICLR 2016; Glorot X., 2010, PROC MACH LEARN RES, P249; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hardt M., 2016, ARXIV161104231; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Im, 2016, ARXIV161204010; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Kingma D.P, P 3 INT C LEARNING R; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Kohler  Jonas, 2018, ARXIV180510694; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li H., 2017, ARXIV171209913; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Morcos A. S., 2018, ICLR POSTER; Nair V, 2010, P 27 INT C MACHINE L, P807; Nesterov Y., 2018, APPL OPTIMIZATION; Rahimi  Ali, 2017, NIPS TEST OF TIME AW; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Ulyanov D., 2016, ARXIV160708022; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1	31	442	455	2	23	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302049
C	Guo, YW; Yao, AB; Chen, YR		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Guo, Yiwen; Yao, Anbang; Chen, Yurong			Dynamic Network Surgery for Efficient DNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of 108x and 17:7x respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.	[Guo, Yiwen; Yao, Anbang; Chen, Yurong] Intel Labs China, Beijing, Peoples R China	Intel Corporation	Guo, YW (corresponding author), Intel Labs China, Beijing, Peoples R China.	yiwen.guo@intel.com; anbang.yao@intel.com; yurong.chen@intel.com	yao, anbang/V-8215-2019					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Courbariaux M., 2015, ADV NEUR IN, P3123; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; Dentinel Zarembaw, 2014, NEURIPS, P1269; Gong Yunchao, 2014, ARXIV14126115; Han S., 2016, P INT C LEARNING REP; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lebedev Vadim, 2015, ICLR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1989, ADV NEURAL INF PROCE, P598; Lodish Harvey, 2000, MOL CELL BIOL NEUROT; Mathieu M., 2013, P 2 INT C LEARN REPR; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Yang Z., 2015, ICCV; Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809	22	437	462	3	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701022
C	Bennett, KP; Demiriz, A		Kearns, MS; Solla, SA; Cohn, DA		Bennett, KP; Demiriz, A			Semi-supervised support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORKS	We introduce a semi-supervised support vector machine ((SVM)-V-3) method. Given a training set of labeled data and a working set of unlabeled data. (SVM)-V-3 constructs a support vector machine using both the training and working sets. We use (SVM)-V-3 to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points ill the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the: classes of the working set data. We propose a general (SVM)-V-3 model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the (SVM)-V-3 model for 1-norm linear support vector machines carl be converted to a mixed-integer program and then solved exactly using integer programming. Results of (SVM)-V-3 and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, (SVM)-V-3 either improved or showed no significant difference in generalization compared to the traditional approach.	Rensselaer Polytech Inst, Dept Math Sci, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Bennett, KP (corresponding author), Rensselaer Polytech Inst, Dept Math Sci, Troy, NY 12180 USA.		Demiriz, Ayhan/AAF-2646-2019	Demiriz, Ayhan/0000-0002-5731-3134				Bennett K.P., 1992, OPT MET SOFTW, V1, P23; BENNETT KP, 1997, IN PRESS GEOMETRY WO; BENNETT KP, 1998, 98100 RPI; Bensaid AM, 1996, PATTERN RECOGN, V29, P859, DOI 10.1016/0031-3203(95)00120-4; BRADLEY PS, 1998, UNPUB; BRADLEY PS, 1998, ICML98; Bredensteiner E. J., 1997, COMPUTATIONAL OPTIMI, V10, P110; BURGES CJC, 1998, INP RESS DATA MINING; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; *CPLEX OPT INC, 1994, US CPLEX CALL LIBR; FRIES TT, 1998, LINEAR PROGRAMMING S; Kernighan B. W, 1993, AMPL MODELING LANGUA; MANGASARIAN OL, 1997, IN PRESS COMPUTATION; ODEWAHN SC, 1992, ASTRON J, V103, P318, DOI 10.1086/116063; OSUNA E, 1997, 1602 AI; PLATT J, 1998, SEQUENTIONAL MINIMAL; VAIDYANATHAN M, 1994, ARTIFICIAL NEURAL NE; Vapnik V., 1974, THEORY PATTERN RECOG; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; VAPNIK VN, 1979, ESTIMATION DEPENDENS	20	431	454	1	18	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						368	374						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700052
C	Mason, L; Baxter, O; Bartlett, P; Frean, M		Solla, SA; Leen, TK; Muller, KR		Mason, L; Baxter, O; Bartlett, P; Frean, M			Boosting algorithms as gradient descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				DECISION	We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.	Australian Natl Univ, Res Sch Informat Sci & Engn, Canberra, ACT 0200, Australia	Australian National University	Mason, L (corresponding author), Australian Natl Univ, Res Sch Informat Sci & Engn, Canberra, ACT 0200, Australia.	lmason@syseng.anu.edu.au; Jonathan.Baxter@anu.edu.au; Peter.Bartlett@anu.edu.au; marcusf@elec.uq.edu.au		Bartlett, Peter/0000-0002-8760-3140				Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Breiman L., 1998, 504 U CAL DEP STAT; DIETTERICH TG, 1998, EXPT COMP 3 METHODS; Drucker H, 1996, ADV NEUR IN, V8, P479; DUFFY N, 1999, IN PRESS COMP LEARN; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; FREUND Y, 1999, IN PRESS P 12 ANN C; Freund Y, 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.5555/3091696.3091715; Friedman J., 1998, ADDITIVE LOGISTIC RE; Friedman J. H., 1999, GREEDY FUNCTION APPR; Grove AJ, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P692; KEOGH E, 1998, UCI REPOSITORY MACHI; MASON L, 1999, IN PRESS MACHINE LEA; MASON L, 1999, IN PRESS LARGE MARGI; Quinlan JR, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P725; RATSCH G, 1998, NCTR1998021 U LOND D; Schapire RE, 1998, ANN STAT, V26, P1651; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901	19	427	440	1	9	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						512	518						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700073
C	Zhang, ZL; Sabuncu, MR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Zhilu; Sabuncu, Mert R.			Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.	[Zhang, Zhilu; Sabuncu, Mert R.] Cornell Univ, Meinig Sch Biomed Engn, Elect & Comp Engn, Ithaca, NY 14853 USA	Cornell University	Zhang, ZL (corresponding author), Cornell Univ, Meinig Sch Biomed Engn, Elect & Comp Engn, Ithaca, NY 14853 USA.	zz452@cornell.edu; msabuncu@cornell.edu	Sabuncu, Mert Rory/ABE-2284-2021		NIH R01 [R01LM012719, R01AG053949]; NSF NeuroNex [1707312]; NSF CAREER [1748377]	NIH R01(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF NeuroNex; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported by NIH R01 grants (R01LM012719 and R01AG053949), the NSF NeuroNex grant 1707312, and NSF CAREER grant (1748377).	Arpit D, 2017, ARXIV170605394; Azadi S., 2015, ARXIV151107069; Bazaraa M.S., 2013, NONLINEAR PROGRAMMIN, Vthird; BOX GEP, 1964, J ROY STAT SOC B, V26, P211, DOI 10.1111/j.2517-6161.1964.tb00553.x; Brooks JP, 2011, OPER RES, V59, P467, DOI 10.1287/opre.1100.0854; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ferrari D, 2010, ANN STAT, V38, P753, DOI 10.1214/09-AOS687; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919; Ghosh A, 2015, NEUROCOMPUTING, V160, P93, DOI 10.1016/j.neucom.2014.09.081; Goldberger Jacob, 2016, TRAINING DEEP NEURAL; Han Bo, 2018, ADV NEURAL INFORM PR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HENDRYCKS Dan, 2018, ARXIV PREPRINT ARXIV; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Jiang L., 2017, ARXIV PREPRINT ARXIV; Jindal I, 2016, IEEE DATA MINING, P967, DOI [10.1109/ICDM.2016.124, 10.1109/ICDM.2016.0121]; Khetan A., 2017, P 6 INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Li Yuncheng, 2017, ARXIV170302391; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460; Masnadi-Shirazi Hamed, 2009, P 21 INT C NEUR INF; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Northcutt Curtis G., 2017, ARXIV170501936; Patrini Giorgio, 2017, STAT, V1050, P22; Rabinovich S. E, 2014, P 3 INT C LEARN REPR; Sukhbaatar Sainbayar, 2014, ARXIV14062080, P2; Tanaka Daiki, 2018, ARXIV180311364; Vahdat A., 2017, ROBUSTNESS LABEL NOI, P5601; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Rooyen B., 2015, P ADV NEURAL INFORM, P10; Veit Andreas, 2017, C COMP VIS PATT REC; Wang Y., 2018, ARXIV180400092; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Zeng W., 2018, ARXIV180309050; Zhang Chiyuan, 2016, ARXIV161103530	42	426	430	8	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003034
C	Roweis, S		Jordan, MI; Kearns, MJ; Solla, SA		Roweis, S			EM algorithms for PCA and SPCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also naturally accommodates missing information. I also introduce a new variant of PCA called sensible principal component analysis (SPCA) which defines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Roweis, S (corresponding author), CALTECH, Pasadena, CA 91125 USA.								0	411	429	0	12	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						626	632						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700089
C	Hochreiter, S; Schmidhuber, J		Mozer, MC; Jordan, MI; Petsche, T		Hochreiter, S; Schmidhuber, J			LSTM can solve hard long time lag problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.			Hochreiter, S (corresponding author), TECH UNIV MUNICH,FAK INFORMAT,D-80290 MUNICH,GERMANY.		Hochreiter, Sepp/AAI-5904-2020	Hochreiter, Sepp/0000-0001-7449-2528					0	411	421	3	27	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						473	479						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00067
C	Yang, Y; Sun, J; Li, HB; Xu, ZB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yang, Yan; Sun, Jian; Li, Huibin; Xu, Zongben			Deep ADMM-Net for Compressive Sensing MRI	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				IMAGE-RECONSTRUCTION	Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of under-sampled data in k-space, and accelerating the data acquisition in MRI. To improve the current MRI system in reconstruction accuracy and computational speed, in this paper, we propose a novel deep architecture, dubbed ADMM-Net. ADMM-Net is defined over a data flow graph, which is derived from the iterative procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the training data for CS-based reconstruction task. Experiments on MRI image reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction accuracies with fast computational speed.	[Yang, Yan; Sun, Jian; Li, Huibin; Xu, Zongben] Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China	Xi'an Jiaotong University	Yang, Y (corresponding author), Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China.	yangyan92@stu.xjtu.edu.cn; jiansun@mail.xjtu.edu.cn; huibinli@mail.xjtu.edu.cn; zbxu@mail.xjtu.edu.cn						[Anonymous], 2010, P INT C MACH LEARN; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Bernstein MA, 2001, J MAGN RESON IMAGING, V14, P270, DOI 10.1002/jmri.1183; Block KT, 2007, MAGN RESON MED, V57, P1086, DOI 10.1002/mrm.21236; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chen C., 2012, ADV NEURAL INFORM PR, P1115; Eksioglu Ender M, 2016, J MATH IMAGING VIS, P1; Fang S, 2010, MAGN RESON MED, V64, P1414, DOI 10.1002/mrm.22392; Hershey J. R., 2014, ARXIV14092574; KAVI KM, 1986, IEEE T COMPUT, V35, P940, DOI 10.1109/TC.1986.1676696; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728; Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391; Qu XB, 2014, MED IMAGE ANAL, V18, P843, DOI 10.1016/j.media.2013.09.007; Qu XB, 2012, MAGN RESON IMAGING, V30, P964, DOI 10.1016/j.mri.2012.02.019; Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Sun J, 2015, IEEE T IMAGE PROCESS, V24, P4148, DOI 10.1109/TIP.2015.2448352; Wang H., 2014, ADV NEURAL INFORM PR, P181; Yang JF, 2010, IEEE J-STSP, V4, P288, DOI 10.1109/JSTSP.2010.2042333; Zhan Zhifang, 2016, IEEE T BIOMEDICAL EN	22	410	419	13	64	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703042
C	KROGH, A; HERTZ, JA		MOODY, JE; HANSON, SJ; LIPPMANN, RP		KROGH, A; HERTZ, JA			A SIMPLE WEIGHT DECAY CAN IMPROVE GENERALIZATION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Krogh, Anders/M-1541-2014						0	410	411	0	5	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						950	957						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00117
C	Bengio, Y; Paiement, JFO; Vincent, P; Delalleau, O; Le Roux, N; Ouimet, M		Thrun, S; Saul, K; Scholkopf, B		Bengio, Y; Paiement, JFO; Vincent, P; Delalleau, O; Le Roux, N; Ouimet, M			Out-of-sample extensions for LLE, isomap, MDS, eigenmaps, and spectral clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				DIMENSIONALITY REDUCTION	Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data.	Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Bengio, Y (corresponding author), Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada.	bengioy@iro.umontreal.ca; paiemeje@iro.umontreal.ca; vincentp@iro.umontreal.ca; delalle@iro.umontreal.ca; lerouxni@iro.umontreal.ca; ouimema@iro.umontreal.ca						Baker C., 1977, NUMERICAL TREATMENT; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bengio Y, 2003, SPECTRAL CLUSTERING; Cox T.F., 1994, MULTIDIMENSIONAL SCA; DESILVA V, 2003, ADV NEURAL INFORM PR, V15, P705; GOWER JC, 1968, BIOMETRIKA, V55, P582, DOI 10.2307/2334268; NG AY, 2002, ADV NEURAL INFORMATI, V14; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; SAUL L, 2002, J MACHINE LEARNING R, V4, P119; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SHAWETAYLOR J, 2003, ADV NEURAL INFORMATI, V15; Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Weiss Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P975, DOI 10.1109/ICCV.1999.790354; Williams C, 2000, P 17 INT C MACH LEAR	16	400	427	2	17	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						177	184						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500023
C	Li, YY; Bu, R; Sun, MC; Wu, W; Di, XH; Chen, BQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Yangyan; Bu, Rui; Sun, Mingchao; Wu, Wei; Di, Xinhan; Chen, Baoquan			PointCNN: Convolution On X -Transformed Points	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.	[Li, Yangyan; Sun, Mingchao; Wu, Wei] Shandong Univ, Jinan, Shandong, Peoples R China; [Di, Xinhan] Huawei Inc, Shenzhen, Peoples R China; [Chen, Baoquan] Peking Univ, Beijing, Peoples R China	Shandong University; Huawei Technologies; Peking University	Li, YY (corresponding author), Shandong Univ, Jinan, Shandong, Peoples R China.				National Key Research and Development Program of China [2017YFB1002603]; National Basic Research grant (973) [2015CB352501]; National Science Foundation of China [61772317]; "Qilu" Young Talent Program of Shandong University	National Key Research and Development Program of China; National Basic Research grant (973)(National Basic Research Program of China); National Science Foundation of China(National Natural Science Foundation of China (NSFC)); "Qilu" Young Talent Program of Shandong University	Yangyan would like to thank Leonidas Guibas from Stanford University and Mike Haley from Autodesk Research for insightful discussions, and Noa Fish from Tel Aviv University and Thomas Schattschneider from Technical University of Hamburg for proof reading. The work is supported in part by National Key Research and Development Program of China grant No. 2017YFB1002603, the National Basic Research grant (973) No. 2015CB352501, National Science Foundation of China General Program grant No. 61772317, and "Qilu" Young Talent Program of Shandong University.	Abadi M, 2015, P 12 USENIX S OPERAT; ARMENI I, 2016, PROC CVPR IEEE, P1534, DOI DOI 10.1109/CVPR.2016.170; Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Ben-Shabat  Yizhak, 2018, ARXIV171108241; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cruz Rodrigo Santa, 2017, CVPR; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; Dieleman S, 2016, PR MACH LEARN RES, V48; Djork-Arn, ICLR 2016; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Graham B., 2017, ARXIV171110275; Graham Benjamin, 2017, ARXIV170601307; Groh  Fabian, 2018, ARXIV180307289; Ha David, 2017, ARXIV170403477; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hua Binh-Son, 2018, CVPR; Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kingma D.P, P 3 INT C LEARNING R; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Landrieu L., 2017, ABS171109869 CORR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li YY, 2016, ADV NEUR IN, V29; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Maron  Haggai, 2017, ACM T GRAPHIC, V36; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qi Charles R, 2017, ARXIV170602413; Ravanbakhsh S, 2016, ARXIV PREPRINT ARXIV; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sabour Sara, 2017, PROC 31 INT C NEURAL; Shao TJ, 2020, IEEE T VIS COMPUT GR, V26, P2403, DOI 10.1109/TVCG.2018.2887262; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409; Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wu  Shihao, 2015, TOG, V34; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yi L., 2017, ARXIV PREPRINT ARXIV; Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697; Yi  Li, 2016, TOG, V35; Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3; Zaheer Manzil, 2017, P ADV NEUR INF PROC, P3394	58	390	390	12	61	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300076
C	Finn, C; Goodfellow, I; Levine, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Finn, Chelsea; Goodfellow, Ian; Levine, Sergey			Unsupervised Learning for Physical Interaction through Video Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.	[Finn, Chelsea; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Goodfellow, Ian] OpenAI, San Francisco, CA USA; [Finn, Chelsea; Levine, Sergey] Google Brain, Mountain View, CA USA	University of California System; University of California Berkeley; Google Incorporated	Finn, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	cbfinn@eecs.berkeley.edu; ian@openai.com; slevine@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; Battaglia P. W., 2013, P NATL ACAD SCI USA, V110; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Boots B., 2014, INT C ROB AUT ICAR; Brubaker M. A., 2009, INT C COMP VIS ICCV; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; De Brabandere B, 2016, ADV NEUR IN, V29; Eslami S., 2016, NEURAL INFORM PROCES; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Huang D. - A., 2014, EUR C COMP VIS ECCV; Ionescu C., 2014, PAMI, V36; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Karpathy A., 2014, COMPUER VISION PATTE; Kingma D.P, P 3 INT C LEARNING R; Lange S, 2012, IEEE IJCNN; Lerer A, 2016, PR MACH LEARN RES, V48; Lotter William, 2016, ARXIV160508104; Mathieu Michael, 2016, ICLR; Mottaghi R., 2016, EUR C COMP VIS ECCV; Mottaghi R., 2015, COMPUTER VISION PATT; Ranzato MarcAurelio, 2014, ARXIV14126604; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Vondrick Carl, 2015, ABS150408023 CORR; Walker J., 2014, COMPUTER VISION PATT; Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Yuen J., 2010, EUR C COMP VIS ECCV	30	387	402	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701018
C	Santoro, A; Raposo, D; Barrett, DGT; Malinowski, M; Pascanu, R; Battaglia, P; Lillicrap, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Santoro, Adam; Raposo, David; Barrett, David G. T.; Malinowski, Mateusz; Pascanu, Razvan; Battaglia, Peter; Lillicrap, Timothy			A simple neural network module for relational reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties.	[Santoro, Adam; Raposo, David; Barrett, David G. T.; Malinowski, Mateusz; Pascanu, Razvan; Battaglia, Peter; Lillicrap, Timothy] DeepMind, London, England		Santoro, A (corresponding author), DeepMind, London, England.	adamsantoro@google.com; draposo@google.com; barrettdavid@google.com; mateuszm@google.com; razp@google.com; peterbattaglia@google.com; countzero@google.com	Jeong, Yongwook/N-7413-2016; Malinowski, Mateusz/AAI-8855-2020	Malinowski, Mateusz/0000-0003-3909-0149				Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Battaglia P, 2016, NIPS; Garnelo M., 2016, P ADV NEUR INF PROC; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Henaff Mikael, 2017, 5 INT C LEARN REPR I; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217; Kemp C, 2008, P NATL ACAD SCI USA, V105, P10687, DOI 10.1073/pnas.0802631105; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li Yujia, 2016, P INT C LEARN REPR I, P2; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Malinowski M., 2016, ARXIV160502697; Mikolov T., 2015, ARXIV150205698, V1502, P05698; NEWELL A, 1980, COGNITIVE SCI, V4, P135, DOI 10.1016/S0364-0213(80)80015-2; Rae JW, 2016, ADV NEUR IN, V29; Ren M., 2015, ADV NEURAL INFORM PR, V28, P2953; Santoro A, 2017, ARXIV PREPRINT ARXIV; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Weston J., 2015, ICLR; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10	24	386	393	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405005
C	Cristianini, N; Shawe-Taylor, J; Elisseeff, A; Kandola, J		Dietterich, TG; Becker, S; Ghahramani, Z		Cristianini, N; Shawe-Taylor, J; Elisseeff, A; Kandola, J			On kernel-target alignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA			kernels; alignment; eigenvectors; eigenvalues; transduction		We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction.				nello@support-vector.net; john@cs.rhul.ac.uk						Cristianini N., 2000, NCTR00080; Cristianini N., 2000, INTRO SUPPORT VECTOR; Devroye L., 1996, APPL MATH; MCDIARMID C, 1989, LOND MATH S, V141, P148	4	383	408	3	13	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						367	373						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100046
C	Ying, R; You, JX; Morris, C; Ren, X; Hamilton, WL; Leskovec, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ying, Rex; You, Jiaxuan; Morris, Christopher; Ren, Xiang; Hamilton, William L.; Leskovec, Jure			Hierarchical Graph Representation Learning with Differentiable Pooling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PREDICTION	Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs-a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DIFFPOOL, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DIFFPOOL learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DIFFPOOL yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.	[Ying, Rex; You, Jiaxuan; Hamilton, William L.; Leskovec, Jure] Stanford Univ, Stanford, CA 94305 USA; [Morris, Christopher] TU Dortmund Univ, Dortmund, Germany; [Ren, Xiang] Univ Southern Calif, Los Angeles, CA 90089 USA	Stanford University; Dortmund University of Technology; University of Southern California	Ying, R (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	rexying@stanford.edu; jiaxuan@stanford.edu; christopher.morris@udo.edu; xiangren@usc.edu; wleif@stanford.edu; jure@cs.stanford.edu	Morris, Christopher/AAG-8965-2021; You, Jiaxuan/ABC-7506-2020	Morris, Christopher/0000-0002-0465-1068	DARPA SIMPLEX, Stanford Data Science Initiative, Huawei, JD; Chan Zuckerberg Biohub; German Science Foundation (DFG) within the Collaborative Research Center [SFB 876]	DARPA SIMPLEX, Stanford Data Science Initiative, Huawei, JD; Chan Zuckerberg Biohub; German Science Foundation (DFG) within the Collaborative Research Center(German Research Foundation (DFG))	This research has been supported in part by DARPA SIMPLEX, Stanford Data Science Initiative, Huawei, JD and Chan Zuckerberg Biohub. Christopher Morris is funded by the German Science Foundation (DFG) within the Collaborative Research Center SFB 876 "Providing Information by Resource-Constrained Data Analysis", project A6 "Resource-efficient Graph Mining". The authors also thank Marinka Zitnik for help in visualizing the high-level illustration of the proposed methods.	Bianchini M, 2001, IEEE T NEURAL NETWOR, V12, P1464, DOI 10.1109/72.963781; Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Dai HJ, 2016, PR MACH LEARN RES, V48; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dobson Paul D, 2003, J Mol Biol, V330, P771; Duvenaud David K, 2015, P NIPS; Feragen Aasa, 2013, NIPS, P216; Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097; Fout A, 2017, ADV NEUR IN, V30; Gilmer J, 2017, PR MACH LEARN RES, V70; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hamilton WL, 2017, REPRESENTATION LEARN; Jin W., 2017, P 31 INT C NEURAL IN, P2607; Kersting Kristian, 2016, BENCHMARK DATA SETS; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kriege NM, 2016, ADV NEUR IN, V29; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lei T, 2017, PR MACH LEARN RES, V70; Li Yujia, 2016, P INT C LEARN REPR I, P2; Liao RJ, 2018, INT C LEARN REPR; Lusci A, 2013, J CHEM INF MODEL, V53, P1563, DOI 10.1021/ci400187y; Merkwirth C, 2005, J CHEM INF MODEL, V45, P1159, DOI 10.1021/ci049613b; Niepert M, 2016, PR MACH LEARN RES, V48; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schlichtkrull M., 2018, EXT SEM WEB C; Schutt K.T., 2017, ADV NEURAL INF PROCE, V30, P992; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy Christian, 2014, P 2 INT C LEARNING R; Velickovic P., 2018, P INT C LEARN REPR, DOI DOI 10.17863/CAM.48429; Verma Saurabh, 2018, ABS180508090 CORR; Vinyals O., 2015, INT C LEARN REPR; Yanardag P., 2015, ADV NEURAL INFORM PR, P2134; Zhang M, 2018, P ASME INT C OCEAN	40	374	381	5	29	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304078
C	van den Oord, A; Vinyals, O; Kavukcuoglu, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		van den Oord, Aaron; Vinyals, Oriol; Kavukcuoglu, Koray			Neural Discrete Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.	[van den Oord, Aaron; Vinyals, Oriol; Kavukcuoglu, Koray] DeepMind, London, England		van den Oord, A (corresponding author), DeepMind, London, England.	avdnoord@google.com; vinyals@google.com; korayk@google.com	Jeong, Yongwook/N-7413-2016					Agustsson E, 2017, ADV NEUR IN, V30; Ba J, 2014, ADV NEURAL INFORM PR; Beattie C., 2016, ARXIV161203801; Bengio Y., 2011, P 14 INT C ART INT S, P233; Bengio Yoshua, 2013, ARXIV13083432; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Burda Yuri, 2015, ARXIV150900519; Chen Xi, 2016, ARXIV161102731; Denton E., 2016, ARXIV161106430; Dinh L, 2016, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2013, ARXIV PREPRINT ARXIV, V2; Gulrajani I., 2016, P INT C LEARN REPR; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hoffman Judy, 2013, JUDY; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jang E., 2016, ARXIV; Kalchbrenner N, 2016, ARXIV161000527; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Maddison Chris J, 2016, ARXIV161100712; Mehri S., 2016, ARXIV161207837; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mnih Andriy, 2016, CORR; Oord A.V.D., 2016, SSW; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Santoro A, 2016, PR MACH LEARN RES, V48; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Theis Lucas, 2017, INT C LEARN REPR; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Yang Zhilin, 2017, ICLR	41	372	375	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406037
C	Choi, E; Bahadori, MT; Kulas, JA; Schuetz, A; Stewart, WF; Sun, JM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Choi, Edward; Bahadori, Mohammad Taha; Kulas, Joshua A.; Schuetz, Andy; Stewart, Walter F.; Sun, Jimeng			RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INFORMATION-TECHNOLOGY; COSTS	Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.	[Choi, Edward; Bahadori, Mohammad Taha; Kulas, Joshua A.; Sun, Jimeng] Georgia Inst Technol, Atlanta, GA 30332 USA; [Schuetz, Andy; Stewart, Walter F.] Sutter Hlth, Sacramento, CA USA	University System of Georgia; Georgia Institute of Technology	Choi, E (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	mp2893@gatech.edu; bahadori@gatech.edu; jkulas3@gatech.edu; schueta1@sutterhealth.org; stewarwf@sutterhealth.org; jsun@cc.gatech.edu	Choi, Edward/AAC-8825-2020	Choi, Edward/0000-0002-5958-3509; Sun, Jimeng/0000-0003-1512-6426				BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Black AD, 2011, PLOS MED, V8, DOI 10.1371/journal.pmed.1000387; Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613; Chaudhry B, 2006, ANN INTERN MED, V144, P742, DOI 10.7326/0003-4819-144-10-200605160-00125; Che Zhengping, 2015, ARXIV151203542; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Choi  E., 2016, KDD; Choi E, 2015, JMLR WORKSHOP C P, P301; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Esteban C, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI), P93, DOI 10.1109/ICHI.2016.16; Fleisher AS, 2007, NEUROLOGY, V68, P1588, DOI 10.1212/01.wnl.0000258542.58725.4c; Gallego B, 2015, J COMP EFFECT RES, V4, P191, DOI 10.2217/cer.15.12; Ghosh J., 1992, AEROSPACE SENSING, P449; Goldzweig CL, 2009, HEALTH AFFAIR, V28, pW282, DOI 10.1377/hlthaff.28.2.w282; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jha A. K., 2009, N ENGL J MED; Karpathy A., 2015, ARXIV150602078; Kho AN, 2012, J AM MED INFORM ASSN, V19, P212, DOI 10.1136/amiajnl-2011-000439; Kingma D.P, P 3 INT C LEARNING R; Le Q.V., 2015, ABS150400941 CORR; Le Q.V., 2013, ICASSP; Lipton Z.C., 2015, ICLR, P1; Martins AFT, 2016, PR MACH LEARN RES, V48; Mnih V, 2014, ADV NEUR IN, V27; Saria S., 2010, NIPS PREDICTIVE MODE; Schulam Peter, 2015, AMIA Annu Symp Proc, V2015, P143; Sun J, 2012, ACM SIGKDD EXPLOR NE, V14, P16, DOI DOI 10.1145/2408736.2408740; W. K. C. D. Information, MED SPAN EL DRUG FIL; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zeiler Matthew D, 2012, ARXIV12125701	36	370	379	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702068
C	Long, MS; Zhu, H; Wang, JM; Jordan, MI		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Long, Mingsheng; Zhu, Han; Wang, Jianmin; Jordan, Michael I.			Unsupervised Domain Adaptation with Residual Transfer Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.	[Long, Mingsheng; Zhu, Han; Wang, Jianmin] Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China; [Long, Mingsheng; Zhu, Han; Wang, Jianmin] Tsinghua Univ, TNList, Beijing, Peoples R China; [Long, Mingsheng; Zhu, Han; Wang, Jianmin] Tsinghua Univ, Sch Software, Beijing, Peoples R China; [Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Tsinghua University; Tsinghua University; Tsinghua University; University of California System; University of California Berkeley	Long, MS (corresponding author), Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China.; Long, MS (corresponding author), Tsinghua Univ, TNList, Beijing, Peoples R China.; Long, MS (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.	mingsheng@tsinghua.edu.cn; zhuhan10@gmail.com; jimwang@tsinghua.edu.cn; jordan@berkeley.edu	Jordan, Michael I/C-5253-2013; wang, jian/GVS-0711-2022	Jordan, Michael/0000-0001-8935-817X	National Natural Science Foundation of China [61502265, 61325008]; National Key R&D Program of China [2016YFB1000701, 2015BAF32B01]; TNList Key Project	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; TNList Key Project	This work was supported by the National Natural Science Foundation of China (61502265, 61325008), National Key R&D Program of China (2016YFB1000701, 2015BAF32B01), and TNList Key Project.	[Anonymous], 2007, P 15 ACM INT C MULTI; [Anonymous], 2015, CVPR, DOI DOI 10.1109/ICCV.2015.170; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Donahue J, 2014, PR MACH LEARN RES, V32; Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Ganin Yaroslav, 2015, ICML; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Grandvalet Yves, 2004, NIPS, P529; Gretton A, 2012, J MACH LEARN RES, V13, P723; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoffman Judy, 2014, NIPS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Long MS, 2015, PR MACH LEARN RES, V37, P97; Mansour Yishay, 2009, COLT 2009 22 C LEARN; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Oquab M., 2013, CVPR; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzeng E., 2014, ARXIV PREPRINT ARXIV; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Wang X., 2014, NIPS; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang Kun, 2013, ICML	30	370	373	6	19	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700074
C	Bolukbasi, T; Chang, KW; Zou, J; Saligrama, V; Kalai, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bolukbasi, Tolga; Chang, Kai-Wei; Zou, James; Saligrama, Venkatesh; Kalai, Adam			Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.	[Bolukbasi, Tolga; Saligrama, Venkatesh] Boston Univ, 8 St Marys St, Boston, MA 02215 USA; [Chang, Kai-Wei; Zou, James; Saligrama, Venkatesh; Kalai, Adam] Microsoft Res New England, 1 Mem Dr, Cambridge, MA USA	Boston University; Microsoft	Bolukbasi, T (corresponding author), Boston Univ, 8 St Marys St, Boston, MA 02215 USA.	tolgab@bu.edu; kw@kwchang.net; jamesyzou@gmail.com; srv@bu.edu; adam.kalai@microsoft.com			NSF [CNS-1330008, CCF-1527618]; ONR [50202168]; NGA [HM1582-09-1-0037]; DHS [2013-ST-061-ED0001]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); NGA; DHS(United States Department of Homeland Security (DHS))	This material is based upon work supported in part by NSF Grants CNS-1330008, CCF-1527618, by ONR Grant 50202168, NGA Grant HM1582-09-1-0037 and DHS 2013-ST-061-ED0001	Angwin J., 2016, MACHINE BIAS; Barocas Solon., 2014, BIG DATAS DISPARATE; Beigman  E., 2009, ACL; Datta  A., 2015, P PRIVACY ENHANCING; Dwork C., 2012, ITCS, P214; Eisenstein J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0113114; Faruqui Manaal, 2015, NAACL; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; Finkelstein Lev, 2001, P 10 INT C WORLD WID, P406, DOI DOI 10.1145/371920.372094; Greenwald AG, 1998, J PERS SOC PSYCHOL, V74, P1464, DOI 10.1037/0022-3514.74.6.1464; Hansen  C., SNN AD INT S MACH LE; Hines C. P., 1977, PAPERS LANGUAGE VARI, P303; Holmes  J., 2008, HDB LANGUAGE GENDER, V25; Irsoy  O., 2014, NIPS; Jakobson  R., 1990, LANGUAGE; Kay  M., 2015, HUMAN FACTORS COMPUT; Lei  T., 2016, NAACL; Levy Omer, 2014, CONLL; Mikolov T., 2013, ARXIV; Mikolov T., 2013, P 2013 C N AM CHAPTE, P746, DOI DOI 10.3109/10826089109058901; Mikolov Tomas, NIPS; Nalisnick  E., 2016, WWW; Nosek BA, 2002, GROUP DYN-THEOR RES, V6, P101, DOI 10.1037//1089-2699.6.1.101; Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Ross K, 2011, MEDIA CULT SOC, V33, P1148, DOI 10.1177/0163443711418272; RUBENSTEIN H, 1965, COMMUN ACM, V8, P627, DOI 10.1145/365628.365657; Sapir  E., 1985, SELECTED WRITINGS E, V342; Schmidt Ben, 2015, REJECTING GENDER BIN; Sweeney L., 2013, DISCRIMINATION ONLIN, V11, P10, DOI [DOI 10.1145/2460276.2460278, 10.1145/2460276.2460278]; Torralba  A., 2012, CVPR; Turney PD, 2012, J ARTIF INTELL RES, V44, P533, DOI 10.1613/jair.3640; Wagner Claudia, 2015, 9 INT AAAI C WEB SOC; Yogatama  D., 2015, ICML; Zliobaite Indre, 2015, ARXIV151100148	36	365	365	5	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701085
C	Chorowski, J; Bahdanau, D; Serdyuk, D; Cho, K; Bengio, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chorowski, Jan; Bahdanau, Dzmitry; Serdyuk, Dmitriy; Cho, Kyunghyun; Bengio, Yoshua			Attention-Based Models for Speech Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.	[Chorowski, Jan] Univ Wroclaw, Wroclaw, Poland; [Bahdanau, Dzmitry] Jacobs Univ Bremen, Bremen, Germany; [Serdyuk, Dmitriy; Cho, Kyunghyun; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada	University of Wroclaw; Jacobs University; Universite de Montreal	Chorowski, J (corresponding author), Univ Wroclaw, Wroclaw, Poland.	jan.chorowski@ii.uni.wroc.pl	Chorowski, Jan/AAF-5251-2020	Chorowski, Jan/0000-0002-1570-7610	National Science Center (Poland) [Sonata 8 2014/15/D/ST6/04402]; CIFAR; NSERC; Calcul Quebec; Compute Canada; Canada Research Chairs	National Science Center (Poland)(National Science Centre, Poland); CIFAR(Canadian Institute for Advanced Research (CIFAR)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Calcul Quebec; Compute Canada; Canada Research Chairs(Canada Research ChairsCGIAR)	The authors would like to acknowledge the support of the following agencies for research funding and computing support: National Science Center (Poland) grant Sonata 8 2014/15/D/ST6/04402, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR. D. Bahdanau also thanks Planet Intelligent Systems GmbH and Yandex.	Bahdanau D, 2015, P 3 ICLR; Bastien F., 2012, DEEP LEARN UNS FEAT; Bergstra J., 2010, P SCIPY; Cho Kyunghyun, 2014, EMNLP; Chorowski J., 2014, ARXIV14121602CSSTAT; Gales M, 2007, FOUND TRENDS SIGNAL, V1, P195, DOI 10.1561/2000000004; Garofolo J.S., 1993, TIMIT ACOUSTIC PHONE, DOI 10.35111/17gk-bn40; Goodfellow I.J., 2013, ARXIV13084214; Graves A., 2014, P 31 ICML; Graves A., 2006, P 23 ICML 06; Graves A., 2012, P 29 ICML; Graves A., 2011, P 24 NIPS; Graves A., 2014, NEURAL TURING MACHIN; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Graves Alex, 2013, ARXIV13080850 CORR; Gulcehre Caglar, 2015, USING MONOLINGUAL CO; Hannun A., 2014, ARXIV14125567; Hinton GE, 2012, IMPROVING NEURAL NET; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; LeCun Y., 1998, P IEEE; Mnih V., 2014, P 27 NIPS; Povey D, 2011, IEEE 2011 WORKSH AUT; Sukhbaatar S., 2015, ABS150308895 CORR; Toth L., 2014, P ICASSP; van Merrienboer B, 2015, ARXIV150600619CSSTAT; Vinyals O, 2014, P 27 NIPS; Weston J., 2014, ARXIV14103916; Xu K., 2015, P 32 ICML; Zeiler M.D, 2012, CORR ABS12125701	30	362	370	2	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101106
C	Zhang, Q; Goldman, SA		Dietterich, TG; Becker, S; Ghahramani, Z		Zhang, Q; Goldman, SA			EM-DD: An improved multiple-instance learning technique	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a new multiple-instance (MI) learning technique (EM-DD) that combines EM with the diverse density (DD) algorithm. EM-DD is a general-purpose MI algorithm that can be applied with boolean or real-value labels and makes real-value predictions. On the boolean Musk benchmarks, the EM-DD algorithm without any tuning significantly outperforms all previous algorithms. EM-DD is relatively insensitive to the number of relevant attributes in the data set and scales up well to large bag sizes. Furthermore, EM-DD provides a new framework for MI learning, in which the MI problem is converted to a single-instance setting by using EM to estimate the instance responsible for the label of the bag.	Washington Univ, Dept Comp Sci, St Louis, MO 63130 USA	Washington University (WUSTL)	Zhang, Q (corresponding author), Washington Univ, Dept Comp Sci, St Louis, MO 63130 USA.							AMAR RA, 2001, P 18 INT C MACH LEAR, P3; Auer P., 1997, P 14 INT C MACHINE L, P21; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Maron O., 1998, P 15 INT C MACH LEAR, P341; MARON O, 1998, NEURAL INFORMATION P, V10; MARON O, 1998, 1639 MIT AI; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; RAMON J, 2000, P ICML 2000 WORKSH A; RAY S, 2001, P 18 INT C MACH LEAR, P425; RUFFO G, 2000, THESIS U TURIN TORIN; Wang Jun, 2000, ICML, P1119	12	360	371	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1073	1080						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100134
C	Attias, H		Solla, SA; Leen, TK; Muller, KR		Attias, H			A variational Bayesian framework for graphical models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				NETWORKS	This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation.	UCL, Gatsby Unit, London WC1N 3AR, England	University of London; University College London	Attias, H (corresponding author), UCL, Gatsby Unit, 17 Queen Sq, London WC1N 3AR, England.	hagai@gatsby.ucl.ac.uk						Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458; BISHOP CM, 1999, P 9 ICANN; Chickering DM, 1997, MACH LEARN, V29, P181, DOI 10.1023/A:1007469629108; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; JAAKKOLA T, 1997, STAT ARTIFICIAL INTE, V6; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; WATERHOUSE S, 1996, NIPS 8	9	357	369	5	16	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						209	215						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700030
C	Feurer, M; Springenberg, JT; Klein, A; Blum, M; Eggensperger, K; Hutter, F		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Feurer, Matthias; Springenberg, Jost Tobias; Klein, Aaron; Blum, Manuel; Eggensperger, Katharina; Hutter, Frank			Efficient and Robust Automated Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.	[Feurer, Matthias; Springenberg, Jost Tobias; Klein, Aaron; Blum, Manuel; Eggensperger, Katharina; Hutter, Frank] Univ Freiburg, Dept Comp Sci, Freiburg, Germany	University of Freiburg	Feurer, M (corresponding author), Univ Freiburg, Dept Comp Sci, Freiburg, Germany.	feurerm@cs.uni-freiburg.de; springj@cs.uni-freiburg.de; kleinaa@cs.uni-freiburg.de; mblum@cs.uni-freiburg.de; eggenspk@cs.uni-freiburg.de; fh@cs.uni-freiburg.de			German Research Foundation (DFG) under Priority Programme Autonomous Learning (SPP 1527) [HU 1900/3-1]; German Research Foundation (DFG) under Emmy Noether grant [HU 1900/2-1]; German Research Foundation (DFG) under BrainLinks-BrainTools Cluster of Excellence [EXC 1086]	German Research Foundation (DFG) under Priority Programme Autonomous Learning (SPP 1527)(German Research Foundation (DFG)); German Research Foundation (DFG) under Emmy Noether grant(German Research Foundation (DFG)); German Research Foundation (DFG) under BrainLinks-BrainTools Cluster of Excellence(German Research Foundation (DFG))	This work was supported by the German Research Foundation (DFG), under Priority Programme Autonomous Learning (SPP 1527, grant HU 1900/3-1), under Emmy Noether grant HU 1900/2-1, and under the BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086).	[Anonymous], 2014, P ICML 13; Bardenet R., P ICML 13, P199; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Brazdil P., 2009, COGNITIVE TECHNOLOGI; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brochu E., 2010, TUTORIAL BAYESIAN OP; Caruana R, 2006, IEEE DATA MINING, P828; Caruana Rich, 2004, ICML, DOI DOI 10.1145/1015330.1015432; Eggensperger K., 2013, NIPS WORK BAYESIAN O; Eliasmith, 2014, ICML WORKSH AUTOML, P50, DOI DOI 10.25080/MAJORA-14BD3278-006; Feurer M, 2015, AAAI CONF ARTIF INTE, P1128; Gomes TAF, 2012, NEUROCOMPUTING, V75, P3, DOI 10.1016/j.neucom.2011.07.005; Guyon I., 2015, P IJCNN 15; Guyon I, 2010, J MACH LEARN RES, V11, P61; Hall M., 2008, WEKA DATA MINING SOF, V11, P10, DOI [10.1145/1656274.1656278, DOI 10.1145/1656274.1656278]; Hamerly G, 2004, ADV NEUR IN, V16, P281; Hutter F., 2011, P LION 11, P507; Kalousis A., 2002, THESIS; Lacoste A., 2014, P 31 INT C MACH LEAR, P611; Michie Donald, 1994, MACHINE LEARNING NEU, P2; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pfahringer Bernhard, 2000, P 17 INT C MACH LEAR, P743, DOI [10.5555/645529.658105, DOI 10.5555/645529.658105]; Reif M, 2012, MACH LEARN, V87, P357, DOI 10.1007/s10994-012-5286-7; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Yogatama D, 2014, JMLR WORKSH CONF PRO, V33, P1077	28	355	359	4	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102021
C	Kingma, DP; Rezende, DJ; Mohamed, S; Welling, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kingma, Diederik P.; Rezende, Danilo J.; Mohamed, Shakir; Welling, Max			Semi-supervised Learning with Deep Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The ever-increasing size of modem data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.	[Kingma, Diederik P.; Welling, Max] Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands; [Rezende, Danilo J.; Mohamed, Shakir] Google Deepmind, London, England	University of Amsterdam; Google Incorporated	Kingma, DP (corresponding author), Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands.	D.P.Kingma@uva.nl; dlanilor@google.com; shakir@google.com; M.Welling@uva.nl						Adams R. P., 2009, P INT C MACH LEARN I; BLUM A, 2004, P INT C MACH LEARN I; Dayan P, 2000, HDB BRAIN THEORY NEU, V44; Dietterich T. G., 1995, CS9501101 ARXIV; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fergus R., 2009, ADV NEURAL INFORM PR; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Kemp C., 2003, ADV NEURAL INFORM PR; Kingma D., P INT C LEARN REPR I, DOI DOI 10.1145/1830483.1830503; Li P., 2009, P ESANN2009, P11; Liang P., 2005, THESIS MIT; Liu Y., 2013, P INT; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Pal C., 2005, ADV NEURAL INFORM PR; Pitelis Nikolaos, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P565, DOI 10.1007/978-3-662-44851-9_36; Ranzato M., 2008, P 25 INT C MACHINE L, P792, DOI DOI 10.1145/1390156.1390256; Rezende D. J., 2014, JMLR W CP, V32; Rifai S., 2011, P ADV NEUR INF PROC; Rosenberg C., 2005, P 7 IEEE WORKSH APPL; Shi M, 2011, BIOINFORMATICS, V27, P3017, DOI 10.1093/bioinformatics/btr502; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, P3048; Tang Yichuan, 2013, ADV NEURAL INFORM PR, P530; Wang Y., 2009, ADV NEURAL INFORM PR, P2008, DOI DOI 10.1097/EDE.0B013E318231D67A; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Zhu X., 2006, TECHNICAL REPORT; Zhu Xiaojin., 2003, P ICLR, P912	26	347	348	7	22	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101023
C	Zhang, MH; Chen, YX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Muhan; Chen, Yixin			Link Prediction Based on Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a "heuristic" that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel gamma-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the gamma-decaying theory, we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.	[Zhang, Muhan; Chen, Yixin] Washington Univ, Dept CSE, St Louis, MO 63130 USA	Washington University (WUSTL)	Zhang, MH (corresponding author), Washington Univ, Dept CSE, St Louis, MO 63130 USA.	muhan@wustl.edu; chen@cse.wustl.edu			National Science Foundation [III-1526012, SCH-1622678]; National Institute of Health [1R21HS024581]	National Science Foundation(National Science Foundation (NSF)); National Institute of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The work is supported in part by the III-1526012 and SCH-1622678 grants from the National Science Foundation and grant 1R21HS024581 from the National Institute of Health.	ACKLAND R, 2005, BLOGTALK DOWN 2005 C; Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1; Aicher C, 2015, J COMPLEX NETW, V3, P221, DOI 10.1093/comnet/cnu026; Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; [Anonymous], 2010, ADV NEURAL INF PROCE; Bar-Yossef Z., 2008, P 17 ACM INT C INF K, P279, DOI 10.1145/1458082.1458122; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132; Brin S, 2012, COMPUT NETW, V56, P3825, DOI 10.1016/j.comnet.2012.10.007; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Chen Y., 2004, P 13 ACM INT C INF K, P381; Costa Fabrizio, 2010, INT C MACHINE LEARNI, P255, DOI DOI 10.1016/J.NEUROPHARM.2007.07.003; Dai HJ, 2016, PR MACH LEARN RES, V48; Duran Alberto Garcia, 2017, NIPS, V30, P5119; Duvenaud David K, 2015, P NIPS; Gilmer Justin, 2017, ARXIV170401212; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton WL, 2017, P 31 INT C NEUR INF, P1025; Jeh G, 2002, P 8 ACM SIGKDD INT C, V02, P538; Jeh G., 2003, P 12 INT C WORLD WID, P271, DOI DOI 10.1145/775152.775191; Katz L., 1953, PSYCHOMETRIKA, V18, P39, DOI [10.1007/BF02289026, DOI 10.1007/BF02289026, DOI 10.1016/j.clinph.2016.12.016]; Kipf T.N., 2016, VARIATIONAL GRAPH AU; Kipf TN, 2016, P INT C LEARN REPR; Koren Y., 2008, P 14 ACM SIGKDD INT, P426, DOI DOI 10.1145/1401890.1401944; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Kovacs I. A., 2018, BIORXIV; Krevl Andrej, 2015, LARGE NETWORK DATASE; Kriege N.M., 2012, INT C MACHINE LEARNI, P291; Lai Y-A, 2017, ADV NEURAL INFORM PR, P5263; Lao N, 2010, MACH LEARN, V81, P53, DOI 10.1007/s10994-010-5205-8; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027; Mahoney Matt, 2011, LARGE TEXT COMPRESSI; Monti F., 2017, NIPS, P3700; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592; Nickel Maximilian, 2014, ADV NEURAL INFORM PR, P1179; Niepert M, 2016, PR MACH LEARN RES, V48; Oyetunde Tolutola, 2016, BIOINFORMATICS; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Qiu Jiezhong, 2017, ARXIV171002971; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771; Ribeiro LFR, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P385, DOI 10.1145/3097983.3098061; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Spring N, 2004, IEEE ACM T NETWORK, V12, P2, DOI 10.1109/TNET.2003.822655; Stark C, 2006, NUCLEIC ACIDS RES, V34, pD535, DOI 10.1093/nar/gkj109; Sugiyama M, 2015, ADV NEURAL INFORM PR, P1639; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; von Mering C, 2002, NATURE, V417, P399, DOI 10.1038/nature750; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Xu Jia, 2010, Proceedings of the 12th Asia Pacific Web Conference (APWEB 2010), P157, DOI 10.1109/APWeb.2010.47; Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438; Zhang MH, 2018, AAAI CONF ARTIF INTE, P4430; Zhang M, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P575, DOI 10.1145/3097983.3097996; Zhao H, 2017, PR MACH LEARN RES, V70; Zhou T, 2009, EUR PHYS J B, V71, P623, DOI 10.1140/epjb/e2009-00335-8	61	346	357	4	14	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305020
C	Lopez-Paz, D; Ranzato, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lopez-Paz, David; Ranzato, Marc'Aurelio			Gradient Episodic Memory for Continual Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.	[Lopez-Paz, David; Ranzato, Marc'Aurelio] Facebook Artificial Intelligence Res, Menlo Pk, CA 94025 USA	Facebook Inc	Lopez-Paz, D (corresponding author), Facebook Artificial Intelligence Res, Menlo Pk, CA 94025 USA.	dlp@fb.com; ranzato@fb.com	Jeong, Yongwook/N-7413-2016					Aljundi R., 2016, CVPR; [Anonymous], 2015, CVPR; [Anonymous], 2010, AAAI; [Anonymous], 2009, ICML; Balcan M.-F., 2015, COLT; Baroni M., 2017, COMMAI EVALUATING 1; Baxter Jonathan Baxter, 2000, JAIR; Ben-David S., 2010, MACHINE LEARNING J; Bertinetto L., 2016, NIPS; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Denoyer L., 2015, DEEP SEQUENTIAL NEUR; Dorn W. S., 1960, Q APPL MATH; Eigen D., 2014, ICLR; FEIFEI L, 2003, ICCV; Fernando C., 2017, PATHNET EVOLUTION CH; French R., 1999, TRENDS COGNITIVE SCI; Goodfellow I.J., 2013, EMPIRICAL INVESTIGAT; He K, 2016, 2016 IEEE C COMP VIS, DOI [10.1109/cvpr.2016.90, 10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton Geoffrey, NIPS DEEP LEARN REP; Jung Heechul, 2016, LESS FORGETTING LEAR; Kirkpatrick J., 2017, PNAS; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lampert C. H., 2009, CVPR; Li Z., 2016, ECCV; Lucic M., 2017, TRAINING MIXTURE MOD; McClelland J. L., 1995, PSYCHOL REV; McCloskey M., 1989, PSYCHOL LEARNING MOT; Mikolov T., 2015, ROADMAP MACHINE INTE; Oquab M., 2014, CVPR; Palatucci M., 2009, ADV NEURAL INFORM PR, V22; Pan S. J., 2010, TKDE; Pentina A., 2016, NIPS; Peters J., 2016, J ROYAL STAT SOC; Ratcliff R., 1990, PSYCHOL REV; Rebuffi S.A., 2017, CVPR; Ring M. B., 1994, THESIS; Ring M. B., 1997, MACHINE LEARNING; Rusu A. A., 2016, NIPS; Ruvolo P., 2013, ICML; Santoro A, 2016, ONE SHOT LEARNING ME; Schaul T., 2015, ICML; Scholkopf B., 2001, LEARNING KERNELS SUP; Scholkopf B., 2016, LEARNING THEORY APPR; Sutton R. S., 2011, 10 INT C AUT AG MULT; Thrun S., 1994, P IEEE RSJ GI C INT; Thrun S., 2012, LEARNING LEARN; Thrun S., 1996, NIPS; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; Triki A. Rannen, 2017, ENCODER BASED LIFELO; Vapnik V.N, 1998, STAT LEARNING THEORY; Vinyals O., 2016, NIPS; Zenke F., 2017, IMPROVED MULTITASK L	52	341	344	2	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406052
C	Alistarh, D; Grubic, D; Li, JZ; Tomioka, R; Vojnovic, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alistarh, Dan; Grubic, Demjan; Li, Jerry Z.; Tomioka, Ryota; Vojnovic, Milan			QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to its excellent scalability properties. A fundamental barrier when parallelizing SGD is the high bandwidth cost of communicating gradient updates between nodes; consequently, several lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always converge. In this paper, we propose Quantized SGD (QSGD), a family of compression schemes with convergence guarantees and good practical performance. QSGD allows the user to smoothly trade off communication bandwidth and convergence time: nodes can adjust the number of bits sent per iteration, at the cost of possibly higher variance. We show that this trade-off is inherent, in the sense that improving it past some threshold would violate information-theoretic lower bounds. QSGD guarantees convergence for convex and non-convex objectives, under asynchrony, and can be extended to stochastic variance-reduced techniques. When applied to training deep neural networks for image classification and automated speech recognition, QSGD leads to significant reductions in end-to-end training time. For instance, on 16GPUs, we can train the ResNet-152 network to full accuracy on ImageNet 1.8x faster than the full-precision variant.	[Alistarh, Dan] IST Austria, Klosterneuburg, Austria; [Alistarh, Dan; Grubic, Demjan] Swiss Fed Inst Technol, Zurich, Switzerland; [Grubic, Demjan] Google, Mountain View, CA USA; [Li, Jerry Z.] MIT, Cambridge, MA 02139 USA; [Tomioka, Ryota] Microsoft Res, Cambridge, England; [Vojnovic, Milan] London Sch Econ, London, England	Institute of Science & Technology - Austria; Swiss Federal Institutes of Technology Domain; ETH Zurich; Google Incorporated; Massachusetts Institute of Technology (MIT); Microsoft; University of London; London School Economics & Political Science	Alistarh, D (corresponding author), IST Austria, Klosterneuburg, Austria.; Alistarh, D (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	dan.alistarh@ist.ac.at; demjangrubic@gmail.com; jerryzli@mit.edu; ryoto@microsoft.com; M.Vojnovic@lse.ac.uk			Swiss National Fund Ambizione Fellowship; NSF CAREER Award [CCF-1453261, CCF-1565235]; Google Faculty Research Award; NSF Graduate Research Fellowship	Swiss National Fund Ambizione Fellowship; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Faculty Research Award(Google Incorporated); NSF Graduate Research Fellowship(National Science Foundation (NSF))	The authors would like to thank Martin Jaggi, Ce Zhang, Frank Seide and the CNTK team for their support during the development of this project, as well as the anonymous NIPS reviewers for their careful consideration and excellent suggestions. Dan Alistarh was supported by a Swiss National Fund Ambizione Fellowship. Jerry Li was supported by the NSF CAREER Award CCF-1453261, CCF-1565235, a Google Faculty Research Award, and an NSF Graduate Research Fellowship. This work was developed in part while Dan Alistarh, Jerri Li and Milan Vojnovic were with Microsoft Research Cambridge, UK.	Abadi M, 2015, P 12 USENIX S OPERAT; Acero A., 2012, ACOUSTICAL ENV ROBUS, V201; Agarwal Amit, 2014, MSRTR2014 112; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjevani Y., 2015, ADV NEURAL INFORM PR, V28, P1756; Bubeck S., 2015, FDN TRENDS MACHINE L; Burges, 1998, MNIST DATABASE HANDW; Chilimbi T., 2014, OSDI, P571; Dan Alistarh, 2016, ARXIV161002132; De Sa C., 2015, NIPS, P2674; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duchi J. C., 2015, NIPS; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hubara I, 2016, ADV NEUR IN, V29; Iandola FN, 2016, PROC CVPR IEEE, P2592, DOI 10.1109/CVPR.2016.284; Konecny J., 2017, ARXIV170701155; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Neelakantan A., 2015, ARXIV151106807; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schreier R., 2005, UNDERSTANDING DELTA; Seide F, 2014, INT CONF ACOUST SPEE; Strom N, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1488; Suresh Ananda Theertha, 2016, ARXIV161100429; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tokui S., CHAINER NEXT GENERAT; Tsitsiklis John N, 1987, J COMPLEXITY, V3; Wen W., 2017, ADV NEURAL INFORM PR, P1, DOI DOI 10.1109/ICC.2017.7997306; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zhang HT, 2017, PR MACH LEARN RES, V70; Zhang S., 2015, P ADV NEURAL INFORM, P685, DOI DOI 10.1145/3207677.3277958; Zhang Y., 2013, NEURAL INFORM PROCES, P2328; Zhou S., 2016, ARXIV160606160	43	337	342	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401072
C	Veit, A; Wilber, M; Belongie, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Veit, Andreas; Wilber, Michael; Belongie, Serge			Residual Networks Behave Like Ensembles of Relatively Shallow Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ARCHITECTURE	In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.	[Veit, Andreas] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; Cornell Univ, Cornell Tech, Ithaca, NY 14853 USA	Cornell University; Cornell University	Veit, A (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	av443@cornell.edu; mjw285@cornell.edu; sjb344@cornell.edu		Belongie, Serge/0000-0002-0388-5217	AOL through the Connected Experiences Laboratory; NSF Graduate Research Fellowship [NSF DGE-1144153]; Google Focused Research award	AOL through the Connected Experiences Laboratory; NSF Graduate Research Fellowship(National Science Foundation (NSF)); Google Focused Research award(Google Incorporated)	We would like to thank Sam Kwak and Theofanis Karaletsos for insightful feedback. We also thank the reviewers of NIPS 2016 for their very constructive and helpful feedback and for suggesting the paper title. This work is partly funded by AOL through the Connected Experiences Laboratory (Author 1), an NSF Graduate Research Fellowship award (NSF DGE-1144153, Author 2), and a Google Focused Research award (Author 3).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DRUCKER H, 1994, NEURAL COMPUT, V6, P1289, DOI 10.1162/neco.1994.6.6.1289; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Hochreiter  Sepp, 1991, THESIS; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Malik J., 1990, J OPTICAL SOC AM; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Serre T, 2007, P NATL ACAD SCI USA, V104, P6424, DOI 10.1073/pnas.0700622104; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	23	337	350	4	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704080
C	Zhu, J; Rosset, S; Hastie, T; Tibshirani, R		Thrun, S; Saul, K; Scholkopf, B		Zhu, J; Rosset, S; Hastie, T; Tibshirani, R			1-norm support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				CLASSIFICATION; SELECTION; CANCER	The standard 2-norm SVM is known for its good performance in two-class classipoundcation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVNI may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an efpoundcient algorithm that computes the whole solution path of the 1-norm SVNI, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Zhu, J (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.			Hastie, Trevor/0000-0002-0164-3142				BRADLEY P, 1998, ICML 98; EVGENIOU T, 1999, ADV LARGE MARGIN CLA; FRIEDMAN J, 2004, IN PRESS ANN STAT; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; HASTIE TR, 2001, ELEMENTS STAT LEANRI; Mukherjee S., 1999, 1677 MIT; ROSSET S, 2003, TECHNICAL REPORT DEP; SONG M, J CHEM INFORMATI SEP; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Wahba G, 1999, ADVANCES IN KERNEL METHODS, P69; ZHU J, 2003, IN PRESS CLASSIFICAT; ZHU J, 2003, THESIS STANFORD U	14	335	350	4	20	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						49	56						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500007
C	van der Merwe, R; Doucet, A; de Freitas, N; Wan, E		Leen, TK; Dietterich, TG; Tresp, V		van der Merwe, R; Doucet, A; de Freitas, N; Wan, E			The unscented particle filter	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				CARLO	In this paper, we propose a new particle filter based on sequential importance sampling. The algorithm uses a bank of unscented filters to obtain the importance proposal distribution. This proposal has two very "nice" properties. Firstly, it makes efficient use of the latest available information and, secondly, it can have heavy tails. As a result, we find that the algorithm outperforms standard particle filtering and other nonlinear filtering methods very substantially. This experimental finding is in agreement with the theoretical convergence proof for the algorithm. The algorithm also includes resampling and (possibly) Markov chain Monte Carlo (MCMC) steps.	Oregon Grad Inst, Beaverton, OR 97006 USA		van der Merwe, R (corresponding author), Oregon Grad Inst, POB 91000, Beaverton, OR 97006 USA.		Zarei, Jafar/E-2965-2010	Doucet, Arnaud/0000-0002-7662-419X				Anderson B. D. O., 1979, OPTIMAL FILTERING; CRISAN D, 2000, 381 CUED F INFENG TR; de Freitas JFG, 2000, NEURAL COMPUT, V12, P955, DOI 10.1162/089976600300015664; DEFREITAS JFG, 1999, THESIS CAMBRIDGE U C; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Doucet A., 2001, SEQUENTIAL MONTE CAR; Gelman A, 2013, BAYESIAN DATA ANAL, P16; JULIER SJ, 1997, P AER 11 INT S AER D, V2; Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179; Thrun S, 2000, ADV NEUR IN, V12, P1064; VANDERMERWE R, 2000, 380 CUED F INFENG TR	12	335	340	1	22	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						584	590						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800083
C	Nowozin, S; Cseke, B; Tomioka, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Nowozin, Sebastian; Cseke, Botond; Tomioka, Ryota			f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.	[Nowozin, Sebastian; Cseke, Botond; Tomioka, Ryota] Microsoft Res, Machine Intelligence & Percept Grp, Redmond, WA 98052 USA	Microsoft	Nowozin, S (corresponding author), Microsoft Res, Machine Intelligence & Percept Grp, Redmond, WA 98052 USA.	Sebastian.Nowozin@microsoft.com; Botond.Cseke@microsoft.com; ryoto@microsoft.com						ALI SM, 1966, J ROY STAT SOC B, V28, P131; Bishop C.M., 1994, MIXTURE DENSITY NETW; Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Djork-Arn, ICLR 2016; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Gauthier J., 2014, CLASS PROJECT STANFO, V2014, P5; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, ARXIV13080850; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Huszar Ferenc, 2015, ABS151105101 CORR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, ARXIV14020030; Larochelle H., 2011, INT C ART INT STAT; Lemarechal Claude, 2012, FUNDAMENTALS CONVEX; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liese F, 2006, IEEE T INFORM THEORY, V52, P4394, DOI 10.1109/TIT.2006.881731; MACKAY DJC, 1995, NUCL INSTRUM METH A, V354, P73, DOI 10.1016/0168-9002(94)00931-7; Makhzani A., 2015, ARXIV151105644; Minka, 2005, DIVERGENCE MEASURES; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Nielsen F, 2014, IEEE SIGNAL PROC LET, V21, P10, DOI 10.1109/LSP.2013.2288355; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reid MD, 2011, J MACH LEARN RES, V12, P731; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salimans T, 2016, ADV NEUR IN, V29; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Theis Lucas, 2015, ARXIV151101844; Uria Benigno, 2013, P 26 INT C NEURAL IN; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7	35	334	349	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700030
C	Kulkarni, TD; Whitney, WF; Kohli, P; Tenenbaum, JB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kulkarni, Tejas D.; Whitney, William F.; Kohli, Pushmeet; Tenenbaum, Joshua B.			Deep Convolutional Inverse Graphics Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs.	[Kulkarni, Tejas D.; Whitney, William F.; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA; [Kohli, Pushmeet] Microsoft Res, Cambridge, England	Massachusetts Institute of Technology (MIT); Microsoft	Kulkarni, TD (corresponding author), MIT, Cambridge, MA 02139 USA.	tejask@mit.edu; wwhitney@mit.edu; pkohli@microsoft.com; jbt@mit.edu			MIT Center for Brains, Minds, and Machines (CBMM)	MIT Center for Brains, Minds, and Machines (CBMM)	We thank Thomas Vetter for access to the Basel face model. We are grateful for support from the MIT Center for Brains, Minds, and Machines (CBMM). We also thank Geoffrey Hinton and Ilker Yildrim for helpful feedback and discussions.	Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Cohen T, 2014, PR MACH LEARN RES, V32, P1755; Desjardins G., 2012, ARXIV12105474; Dosovitskiy A., 2015, ARXIV14115928; Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646, DOI DOI 10.5555/2984093.2984166; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni T. D., 2014, ARXIV14071339; Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Mansinghka V., 2013, ADV NEURAL INFORM PR; Paysan P., 2009, 3D FACE MODEL POSE I; Ranzato Marc'Aurelio., 2007, PROC CVPR IEEE, P1, DOI [10.1109/CVPR.2007.383157, DOI 10.1109/CVPR.2007.383157]; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; TANG Y, 2012, INT C MACH LEARN, V1206, P6445, DOI DOI 10.1016/J.NEUNET.2005.06.042; Tieleman T., 2012, LECT 6 5 RMSPROP COU; Tieleman T., 2014, THESIS; Vincent P, 2010, J MACH LEARN RES, V11, P3371	23	334	337	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101110
C	De Brabandere, B; Jia, X; Tuytelaars, T; Gool, LV		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		De Brabandere, Bert; Jia, Xu; Tuytelaars, Tinne; Luc Van Gool			Dynamic Filter Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.	[De Brabandere, Bert; Jia, Xu; Tuytelaars, Tinne; Luc Van Gool] Katholieke Univ Leuven, IMinds, ESAT PSI, Leuven, Belgium; [Luc Van Gool] Swiss Fed Inst Technol, D ITET, Zurich, Switzerland	IMEC; KU Leuven; Swiss Federal Institutes of Technology Domain; ETH Zurich	De Brabandere, B (corresponding author), Katholieke Univ Leuven, IMinds, ESAT PSI, Leuven, Belgium.	bert.debrabandere@esat.kuleuven.be; xu.jia@esat.kuleuven.be; tinne.tuytelaars@esat.kuleuven.be; vangool@vision.ee.ethz.ch	Tuytelaars, Tinne/B-4319-2015	Tuytelaars, Tinne/0000-0003-3307-9723	FWO [G.0696.12N]; EU FP7 project Europa2; iMinds ICON project Footwork; bilateral Toyota project	FWO(FWO); EU FP7 project Europa2; iMinds ICON project Footwork; bilateral Toyota project	This work was supported by FWO through the project G.0696.12N "Representations and algorithms for captation, visualization and manipulation of moving 3D objects, subjects and scenes", the EU FP7 project Europa2, the iMinds ICON project Footwork and bilateral Toyota project.	Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Flynn J., 2015, ARXIV150606825; Gomez F., 2005, ICANN; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han B., 2016, P CVPR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Larsen ABL, 2016, PR MACH LEARN RES, V48; Mathieu Michael, 2016, ICLR; Oh J., 2015, P ADV NEUR INF PROC, P2863; Patraucean Viorica, 2015, ARXIV151106309; Ranzato MarcAurelio, 2014, ARXIV14126604; Riegler G., 2015, ICCV; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Srivastava Rupesh K, 2015, TRAINING VERY DEEP N, V28, P2377; Taylor G. W., 2009, ICML; Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Yang J., 2015, P ADV NEUR INF PROC; Yim J., 2015, IEEE C COMP VIS PATT	24	328	333	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704102
C	Zhou, DY; Weston, J; Gretton, A; Bousquet, O; Scholkopf, B		Thrun, S; Saul, K; Scholkopf, B		Zhou, DY; Weston, J; Gretton, A; Bousquet, O; Scholkopf, B			Ranking on data manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks. Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data. The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data. Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Zhou, DY (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.	dengyong.zhou@tuebingen.mpg.de; jason.weston@tuebingen.mpg.de; arthur.gretton@tuebingen.mpg.de; olivier.bousquet@tuebingen.mpg.de; bernhard.scholkopf@tuebingen.mpg.de	Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925; Gretton, Arthur/0000-0003-3169-7624				Albert R, 1999, NATURE, V401, P130, DOI 10.1038/43601; Brin S., 1998, P PAP PRES 7 INT WOR; Duda R.O., 2000, PATTERN CLASSIFICATI; Golub G. H., 1996, MATRIX COMPUTATIONS; Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Zhou D.Y., 2003, P NIPS 2003 VANC BC, P321	8	328	357	0	14	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						169	176						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500022
C	Prokhorenkova, L; Gusev, G; Vorobev, A; Dorogush, AV; Gulin, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Prokhorenkova, Liudmila; Gusev, Gleb; Vorobev, Aleksandr; Dorogush, Anna Veronika; Gulin, Andrey			CatBoost: unbiased boosting with categorical features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DECISION TREES; GRADIENT	This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.	[Prokhorenkova, Liudmila; Gusev, Gleb; Vorobev, Aleksandr; Dorogush, Anna Veronika; Gulin, Andrey] Yandex, Moscow, Russia; [Prokhorenkova, Liudmila; Gusev, Gleb] Moscow Inst Phys & Technol, Dolgoprudnyi, Russia	Moscow Institute of Physics & Technology	Prokhorenkova, L (corresponding author), Yandex, Moscow, Russia.; Prokhorenkova, L (corresponding author), Moscow Inst Phys & Technol, Dolgoprudnyi, Russia.	ostroumova-la@yandex-team.ru; gleb57@yandex-team.ru; alvor88@yandex-team.ru; annaveronika@yandex-team.ru; gulin@yandex-team.ru	Gusev, Gleb G./C-8263-2014					Bottou U, 2004, ADV NEUR IN, V16, P217; Breiman L, 2001, MACH LEARN, V45, P261, DOI 10.1023/A:1017934522171; Breiman L., 1996, OUT OF BAG ESTIMATIO; Caruana R., 2006, P 23 INT C MACH LEAR, P161; Cestnik B., 1990, ECAI 90. Proceedings of the 9th European Conference on Artificial Intelligence, P147; Chapelle O, 2015, ACM T INTEL SYST TEC, V5, DOI 10.1145/2532128; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Ferov M., 2016, ARXIV160905610; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Friedman J., 2015, PACKAGE GLASSO; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2; Gulin A., 2011, YAHOO LEARNING RANK, V14, P63; Ke G., 2017, P ADV NEURAL INFORM, V30, P3146; KEARNS M, 1994, J ACM, V41, P67, DOI 10.1145/174644.174647; Langford J, 2009, J MACH LEARN RES, V10, P777; LightGBM, 2017, OPT SPLIT CAT FEAT; LightGBM, 2017, CAT FEAT SUPP; Ling XL, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P689, DOI 10.1145/3041021.3054192; Lou Y, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1893, DOI 10.1145/3097983.3098175; Mason L, 2000, ADV NEUR IN, V12, P512; Micci-Barreca D., 2001, SIGKDD EXPLOR NEWSL, V3, P27, DOI [10.1145/507533.507538, DOI 10.1145/507533.507538]; Olshen R., 1984, CLASSIFICATION REGRE; Roe BP, 2005, NUCL INSTRUM METH A, V543, P577, DOI 10.1016/j.nima.2004.12.018; Rokach L, 2005, IEEE T SYST MAN CY C, V35, P476, DOI 10.1109/TSMCC.2004.843247; RUBIN DB, 1981, ANN STAT, V9, P130, DOI 10.1214/aos/1176345338; Wu FF, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087764; Wu QA, 2010, INFORM RETRIEVAL, V13, P254, DOI 10.1007/s10791-009-9112-1; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819; Zhang O, 2015, WINNING DATA SCI COM; Zhang YR, 2015, TRANSPORT RES C-EMER, V58, P308, DOI 10.1016/j.trc.2015.02.019	31	327	327	30	75	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001020
C	Courbariaux, M; Bengio, Y; David, JP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Courbariaux, Matthieu; Bengio, Yoshua; David, Jean-Pierre			BinaryConnect: Training Deep Neural Networks with binary weights during propagations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.	[Courbariaux, Matthieu; David, Jean-Pierre] Ecole Polytech Montreal, Montreal, PQ, Canada; [Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada	Universite de Montreal; Polytechnique Montreal; Universite de Montreal	Courbariaux, M (corresponding author), Ecole Polytech Montreal, Montreal, PQ, Canada.	matthieu.courbariaux@polymtl.ca; yoshua.bengio@gmail.com; jean-pierre.david@polymtl.ca			NSERC; Canada Research Chairs; CIFAR; Compute Canada	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada Research Chairs(Canada Research ChairsCGIAR); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Compute Canada	We thank the reviewers for their many constructive comments. We also thank Roland Memisevic for helpful discussions. We thank the developers of Theano [42, 43], a Python library which allowed us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 [44] and Lasagne, two Deep Learning libraries built on the top of Theano. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bartol T. M., 2015, BIORXIV; Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y, 2001, ADV NEUR IN, V13, P932; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Chen Tianshi, 2014, P 19 INT C ARCHITECT, P269, DOI [10.1145/2541940, DOI 10.1145/2541940, 10.1145/2654822.2541967, DOI 10.1145/2654822.2541967]; Chen YJ, 2014, INT SYMP MICROARCH, P609, DOI 10.1109/MICRO.2014.58; Cheng Zhiyong, 2015, ARXIV150303562; Collobert Ronan, 2004, THESIS; Courbariaux M., 2015, ICLR 2015 WORKSH; David JP, 2007, IEEE T COMPUT, V56, P1308, DOI 10.1109/TC.2007.1084.; Dean J., 2012, NIPS 2012; Devlin J., 2014, P ACL 2014; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Glorot X., 2011, AISTATS 2011; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I.J., 2013, ARXIV13084214; Goodfellow Ian J., 2013, 13024389 U MONTR; Graham Benjamin, 2014, ARXIV14096070; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Gupta Suyog, 2015, ICML 2015; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hwang K, 2014, IEEE WRK SIG PRO SYS, P174; Jonghong Kim, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7510, DOI 10.1109/ICASSP.2014.6855060; Kim SK, 2009, I C FIELD PROG LOGIC, P367, DOI 10.1109/FPL.2009.5272262; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee C.-Y., 2014, ARXIV14095185; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Minka Thomas P, 2001, UAI 2001; Muller L. K., 2015, ARXIV150405767; Nair V, 2010, P 27 INT C MACHINE L, P807; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Raina Rajat, 2009, ICML 2009; Sainath T., 2013, ICASSP 2013; Soudry D., 2014, NIPS 2014; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Srivastava Nitish, 2013, THESIS; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tang Yichuan, 2013, INT C MACH LEARN, P163; Wan L., 2013, ICML 2013	44	319	325	3	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100015
C	Smith, V; Chiang, CK; Sanjabi, M; Talwalkar, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Smith, Virginia; Chiang, Chao-Kai; Sanjabi, Maziar; Talwalkar, Ameet			Federated Multi-Task Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.	[Smith, Virginia] Stanford Univ, Stanford, CA 94305 USA; [Chiang, Chao-Kai; Sanjabi, Maziar] USC, Los Angeles, CA USA; [Talwalkar, Ameet] CMU, Mt Pleasant, MI USA	Stanford University; University of Southern California	Smith, V (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	smithv@stanford.edu; chaokaic@usc.edu; maziarsanjabi@gmail.com; talwalkar@cmu.edu	Jeong, Yongwook/N-7413-2016					Ahmed A., 2014, C WEB SEARCH DAT MIN; Ando RK, 2005, J MACH LEARN RES, V6, P1817; Anguita D., 2013, ESANN, V3, P3, DOI DOI 10.1007/978-3-642-40728-4_54; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Aslan O., 2014, ADV NEURAL INFORM PR; Baytas I. M., 2016, INT C DAT MIN; Bonomi F., 2012, SIGCOMM WORKSH MOB C; Carroll Aaron, 2010, P USENIXATC, V14; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen J., 2011, C KNOWL DISC DAT MIN; Deshpande A, 2005, VLDB J, V14, P417, DOI 10.1007/s00778-005-0159-3; Duarte MF, 2004, J PARALLEL DISTR COM, V64, P826, DOI 10.1016/j.jpdc.2004.03.020; Evgeniou A., 2007, ADV NEURAL INF PROCE, V19, P41, DOI DOI 10.2139/SSRN.1031158; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Lopez PG, 2015, ACM SIGCOMM COMP COM, V45, P37, DOI 10.1145/2831347.2831354; Goncalves AR, 2016, J MACH LEARN RES, V17, P1; Hong K., 2013, SIGCOMM WORKSH MOB C; Hsieh C.-J., 2014, NEURAL INFORM PROCES, V27; Huang J., 2013, ACM SIGCOMM C; Jacob L., 2009, NEURAL INFORM PROCES; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Jin X., 2015, C INF KNOWL MAN; Kim S, 2009, PLOS GENET, V5, DOI 10.1371/journal.pgen.1000587; Konecn J., 2016, ARXIV161005492; Kuflik T., 2012, UBIQUITOUS DISPLAY E, P7, DOI DOI 10.1007/978-3-642-27663-7_2; Kumar A., 2012, INT C MACH LEARN; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Liu S., 2017, C KNOWL DISC DAT MIN; Ma CX, 2015, PR MACH LEARN RES, V37, P1973; Madden S., 2002, S OP SYST DES IMPL; Madden SR, 2005, ACM T DATABASE SYST, V30, P122, DOI 10.1145/1061318.1061322; Mairal J., 2014, NEURAL INFORM PROCES; Mateos-Nunez D., 2015, IFAC WORKSH DISTR ES; McMahan Brendan, 2015, ARXIV151103575; McMahan H. B., 2017, C ART INT STAT; Miettinen A. P., 2010, USENIX C HOT TOP CLO; Qi H., 2017, INT C LEARN REPR; Rahman S. A., 2015, C PERV COMP TECHN HE; Rashidi P, 2009, IEEE T SYST MAN CY A, V39, P949, DOI 10.1109/TSMCA.2009.2025137; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Shalev-Shwartz S., 2007, INT C MACH LEARN; Singelee D., 2011, ACM C WIR NETW SEC; Smith V., 2016, J MACH LEARN RES, V18, P1; Takac M., 2013, INT C MACH LEARN; Tsai C.-Y., 2016, NEURAL INFORM PROCES; van Berkel CH, 2009, DES AUT TEST EUROPE, P1260; Wang H., 2013, NEURAL INFORM PROCES; Wang J., 2016, C ART INT STAT; Wang J., 2016, ARXIV160302185; Zhang Y., 2010, C UNC ART INT; Zhang YC, 2017, PR MACH LEARN RES, V70; Zhou J., 2011, NEURAL INFORM PROCES	56	312	318	6	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404048
C	Muller, R; Kornblith, S; Hinton, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Muller, Rafael; Kornblith, Simon; Hinton, Geoffrey			When Does Label Smoothing Help?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.	[Muller, Rafael; Kornblith, Simon; Hinton, Geoffrey] Google Brain, Toronto, ON, Canada		Muller, R (corresponding author), Google Brain, Toronto, ON, Canada.	rafaelmuller@google.com						Baum E.B., 1988, NEURAL INFORMATION P, P52; Chelombiev C. J., 2019, ARXIV190209037, P1; Chorowski J, 2017, INTERSPEECH, P523, DOI 10.21437/Interspeech.2017-343; Fahlman Scott E., 1988, TECHNICAL REPORT; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; Hinton G., 2015, ARXIV150302531; Kornblith Simon, 2018, ARXIV180508974; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar Aviral, 2019, ARXIV190300802; Ott M, 2018, PR MACH LEARN RES, V80; Pereyra Gabriel, 2017, ARXIV170106548; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Solla S. A., 1988, Complex Systems, V2, P625; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tishby Naftali, 2000, PHYSICS0004057 ARXIV; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Xie LX, 2016, PROC CVPR IEEE, P4753, DOI 10.1109/CVPR.2016.514; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	25	311	321	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304067
C	Hamerly, G; Elkan, C		Thrun, S; Saul, K; Scholkopf, B		Hamerly, G; Elkan, C			Learning the k in k-means	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					When clustering a dataset, the right number k of clusters to use is often not obvious, and choosing k automatically is a hard algorithmic problem. In this paper we present an improved algorithm for learning k while clustering. The G-means algorithm is based on a statistical test for the hypothesis that a subset of data follows a Gaussian distribution. G-means runs k-means with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each k-means center are Gaussian. Two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix. Additionally, G-means only requires one intuitive parameter, the standard statistical significance level a. We present results from experiments showing that the algorithm works well, and better than a recent method based on the BIC penalty for model complexity. In these experiments, we show that the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model's complexity.	Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Hamerly, G (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.							Bischof H, 1999, PATTERN ANAL APPL, V2, P59, DOI 10.1007/s100440050015; DELCORSO GM, 2000, SIAM J MATRIX ANAL A, P143; DING C, 2002, P 2 IEEE INT C DAT M; Farnstrom F., 2000, ACM SIGKDD EXPLORATI, V2, P51, DOI DOI 10.1145/360402.360419; HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; KASS RE, 1995, J AM STAT ASSOC, V90, P928, DOI 10.2307/2291327; KEARNS MJ, 1995, COMPUTATIONAL LEARNI, P21; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Newman C. B. D., 1998, UCI REPOSITORY MACHI; NG AY, 2002, NEURAL INFORMATION P, V14; PELLEG D, 2000, P 17 INT C MACH LEAR; SAND P, 2001, P 18 INT C MACH LEAR; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; STEPHENS MA, 1974, J AM STAT ASSOC, V69, P730, DOI 10.2307/2286009	16	308	325	0	10	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						281	288						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500036
C	Sohn, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sohn, Kihyuk			Improved Deep Metric Learning with Multi-class N-pair Loss Objective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SIMILARITY	Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N - pair loss. The proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples - more specifically, N - 1 negative examples - and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only N pairs of examples, instead of (N + 1) x N. We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.	[Sohn, Kihyuk] NEC Labs Amer Inc, Princeton, NJ 08540 USA	NEC Corporation	Sohn, K (corresponding author), NEC Labs Amer Inc, Princeton, NJ 08540 USA.	ksohn@nec-labs.com						Best-Rowden L, 2014, IEEE T INF FOREN SEC, V9, P2144, DOI 10.1109/TIFS.2014.2359577; Chechik G, 2010, J MACH LEARN RES, V11, P1109; Chopra S., 2005, CVPR; Cui Y., 2016, CVPR; Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384; Goldberger J., 2004, NIPS; Hadsell R., 2006, P CVPR; Huang G. B., 2008, CVPR WORKSH; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jia Y., 2014, ARXIV14085093; Kingma D.P., 2015, INT C LEARN REPR, P1; Krause J., 2013, ICCV WORKSH; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Liu J., 2015, ABS150607310 CORR; LOWE DG, 1995, NEURAL COMPUT, V7, P72, DOI 10.1162/neco.1995.7.1.72; Manning C. D., 2008, INTRO INFORM RETRIEV, V1; Norouzi M., 2012, NIPS; Oh Song H., 2016, CVPR; Parkhi O. M., 2015, BMVC; Schroff F., 2015, CVPR; Simonyan Karen, 2015, INT C LEARN REPR; Sun Y., 2014, NIPS; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Taigman Y., 2014, CVPR; Wah C., 2011, CNST2011001 CALTECH; Wang J, 2014, CVPR; Weinberger K. Q, 2005, NIPS; Weston Jason, 2011, 22 INT JOINT C ART I; Xie S., 2015, CVPR; Xing E. P., 2003, DISTANCE METRIC LEAR; Yi D, 2014, ARXIV PREPRINT ARXIV; Zhang Xiaofan, 2016, CVPR	32	302	302	2	22	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701057
C	Lawrence, ND		Thrun, S; Saul, K; Scholkopf, B		Lawrence, ND			Gaussian process latent variable models for visualisation of high dimensional data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				GTM	In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to 'twin kernel PCA' in which a mapping between feature spaces occurs.	Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England	University of Sheffield	Lawrence, ND (corresponding author), Univ Sheffield, Dept Comp Sci, Regent Court,211 Portobello St, Sheffield S1 4DP, S Yorkshire, England.	neil@dcs.shef.ac.uk		Lawrence, Neil/0000-0001-9258-1030				BECKER S, 2003, ADV NEURAL INFORMATI, V15; BISHOP CM, 1993, NUCL INSTRUM METH A, V327, P580, DOI 10.1016/0168-9002(93)90728-Z; Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; Bishop CM, 1997, ADV NEUR IN, V9, P354; LAWRENCE ND, 2003, ADV NEURAL INFORMATI, V15, P625; Nabney I.T., 2001, NETLAB ALGORITHMS PA, V1st ed.; Paccanaro A, 2002, ADV NEUR IN, V14, P857; Roweis S, 2002, ADV NEUR IN, V14, P889; Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217; Tipping ME, 2001, ADV NEUR IN, V13, P633; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196	11	300	306	1	11	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						329	336						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500042
C	Nickel, M; Kiela, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nickel, Maximilian; Kiela, Douwe			Poincare Embeddings for Learning Hierarchical Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space - or more precisely into an n-dimensional Poincare ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincare embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.	[Nickel, Maximilian; Kiela, Douwe] Facebook AI Res, New York, NY 10003 USA	Facebook Inc	Nickel, M (corresponding author), Facebook AI Res, New York, NY 10003 USA.	maxn@fb.com; dkiela@fb.com						Adcock AB, 2013, IEEE DATA MINING, P1, DOI 10.1109/ICDM.2013.77; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Asta DM, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P102; Boguna Marian, 2010, Nat Commun, V1, P62, DOI 10.1038/ncomms1063; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bordes A., 2013, ADV NEURAL INFORM PR; Bouchard Guillaume, 2015, AAAI SPRING S CIT; Clauset A, 2008, NATURE, V453, P98, DOI 10.1038/nature06830; Demeester T., 2016, EMNLP, P1389, DOI 10.18653/v1/d16-1146; Firth JohnRupert., 1957, STUDIES LINGUISTIC A, P168; Gromov Mikhael, 1987, MATH SCI RES I PUBL, V8, P75, DOI 10.1007/978-1-4613-9586-7_3; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; Joulin Armand, 2016, ARXIV160701759; JulieWeeds Daoud Clarke, 2014, P COLING 2014 25 INT, P2249; Kiela D, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P119; Kleinberg R, 2007, IEEE INFOCOM SER, P1902, DOI 10.1109/INFCOM.2007.221; Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106; Mikolov T., 2013, ARXIV; Miller George A, 1998, WORDNET ELECT LEXICA; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M, 2016, AAAI CONF ARTIF INTE, P1955; Nickel Maximilian, 2014, ADV NEURAL INFORM PR, P1179; Ontrup J, 2006, NEURAL NETWORKS, V19, P751, DOI 10.1016/j.neunet.2006.05.015; Paccanaro A, 2001, IEEE T KNOWL DATA EN, V13, P232, DOI 10.1109/69.917563; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Ravasz E, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.026112; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Riedel Sebastian, 2013, P 2013 C N AM CHAPT, P74, DOI DOI 10.4218/ETRIJ.2018-0553; Steyvers M, 2005, COGNITIVE SCI, V29, P41, DOI 10.1207/s15516709cog2901_3; Sun Ke, 2015, ADV NEURAL INFORM PR, V28, P100; Trouillon T, 2016, PR MACH LEARN RES, V48; Vendrov I, 2015, ARXIV151106361; Vilnis Luke, 2015, INT C LEARN REPR; Vulic Ivan, 2016, ARXIV160802117; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592; Zipf G. K., 1949, HUMAN BEHAVIOUR PRIN	40	294	300	3	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406040
C	Shin, H; Lee, JK; Kim, J; Kim, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Shin, Hanul; Lee, Jung Kwon; Kim, Jaehong; Kim, Jiwon			Continual Learning with Deep Generative Replay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MEMORY; STABILITY	Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.	[Shin, Hanul] MIT, Cambridge, MA 02139 USA; [Shin, Hanul; Lee, Jung Kwon; Kim, Jaehong; Kim, Jiwon] SK T Brain, Seoul, South Korea	Massachusetts Institute of Technology (MIT)	Shin, H (corresponding author), MIT, Cambridge, MA 02139 USA.; Shin, H (corresponding author), SK T Brain, Seoul, South Korea.	skyshin@mit.edu; jklee@sktbrain.com; xhark@sktbrain.com; jk@sktbrain.com	Jeong, Yongwook/N-7413-2016					Abraham WC, 2005, TRENDS NEUROSCI, V28, P73, DOI 10.1016/j.tins.2004.12.003; Ans B, 1997, CR ACAD SCI III-VIE, V320, P989, DOI 10.1016/S0764-4469(97)82472-9; BALDWIN DA, 1993, CHILD DEV, V64, P711, DOI 10.2307/1131213; Bornstein MH, 2010, DEV PSYCHOL, V46, P350, DOI 10.1037/a0018411; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Fagott J, 2006, P NATL ACAD SCI USA, V103, P17564, DOI 10.1073/pnas.0605184103; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Gelbard-Sagiv H, 2008, SCIENCE, V322, P96, DOI 10.1126/science.1164685; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Goodfellow I., 2016, ARXIV; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grutzendler J, 2002, NATURE, V420, P812, DOI 10.1038/nature01276; Gulrajani I, 2017, P NIPS 2017; Hattori M, 2014, NEUROCOMPUTING, V134, P262, DOI 10.1016/j.neucom.2013.08.044; Hinton G.E., 1987, P 9 ANN C COGNITIVE, P177; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee S W, 2017, ARXIV170308475; Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mocanu D. C., 2016, CORR, Patent No. [ArXiv161005555Cs, 161005555]; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; O'Neill J, 2010, TRENDS NEUROSCI, V33, P220, DOI 10.1016/j.tins.2010.01.006; O'Reilly RC, 2002, TRENDS COGN SCI, V6, P505, DOI 10.1016/S1364-6613(02)02005-3; Ramirez S, 2013, SCIENCE, V341, P387, DOI 10.1126/science.1239073; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; Robins A., 1993, Proceedings 1993 The First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems, P65, DOI 10.1109/ANNES.1993.323080; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310; Stickgold R, 2007, SLEEP MED, V8, P331, DOI 10.1016/j.sleep.2007.03.011	35	288	291	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403006
C	Jung, TP; Humphries, C; Lee, TW; Makeig, S; McKeown, MJ; Iragui, V; Sejnowski, TJ		Jordan, MI; Kearns, MJ; Solla, SA		Jung, TP; Humphries, C; Lee, TW; Makeig, S; McKeown, MJ; Iragui, V; Sejnowski, TJ			Extended ICA removes artifacts from electroencephalographic recordings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Severe contamination of electroencephalographic (EEG) activity by eye movements, blinks, muscle, heart and line noise is a serious problem for EEG interpretation and analysis. Rejecting contaminated EEG segments results in a considerable loss of information and may be impractical for clinical data. Many methods have been proposed to remove eye movement and blink artifacts from EEG recordings. Often regression in the time or frequency domain is performed on simultaneous EEG and electrooculographic (EOG) recordings to derive parameters characterizing the appearance and spread of EOG artifacts in the EEG channels. However, EOG records also contain brain signals [1, 2], so regressing out EOG activity inevitably involves subtracting a portion of the relevant EEG signal from each recording as well. Regression cannot be used to remove muscle noise or line noise, since these have no reference channels. Here, pre propose a new and generally applicable method for removing a wide variety of artifacts from EEG records. The method is based on an extended version of a previous Independent Component Analysis (ICA) algorithm [3, 4] for performing blind source separation on linear mixtures of independent source signals with either sub-Gaussian or super-Gaussian distributions. Our results show that ICA can effectively detect, separate and remove activity in EEG records from a wide variety of artifactual sources, With results comparing favorably to those obtained using regression-based methods.	Salk Inst, Howard Hughes Med Inst, San Diego, CA 92186 USA	Howard Hughes Medical Institute; Salk Institute	Jung, TP (corresponding author), Salk Inst, Howard Hughes Med Inst, POB 85800, San Diego, CA 92186 USA.		Sejnowski, Terrence/AAV-5558-2021; McKeown, Martin J./Z-1253-2018	McKeown, Martin J./0000-0002-4048-0817; Humphries, Colin/0000-0002-5131-8743					0	286	292	0	10	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						894	900						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700126
C	Yedidia, JS; Freeman, WT; Weiss, Y		Leen, TK; Dietterich, TG; Tresp, V		Yedidia, JS; Freeman, WT; Weiss, Y			Generalized belief propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Belief propagation (BP) Was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. We show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. More importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. Ve illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP.	MERL, Cambridge, MA 02139 USA		Yedidia, JS (corresponding author), MERL, 201 Broadway, Cambridge, MA 02139 USA.							FREEMAN W, 1999, 7 INT C COMP VIS, P1182; JORDAN M, 1998, LEARNING GRAPHICAL M; Kabashima Y, 1998, EUROPHYS LETT, V44, P668, DOI 10.1209/epl/i1998-00524-7; KIKUCHI R, 1951, PHYS REV, V81, P988, DOI 10.1103/PhysRev.81.988; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Murphy K. P., 1999, P UNC AI; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Richardson T, 2000, IEEE T INFORM THEORY, V46, P9, DOI 10.1109/18.817505; [No title captured]; 1994, PROGR THEOR PHYS SUP, V115	10	284	293	0	10	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						689	695						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800098
C	Kakade, S		Dietterich, TG; Becker, S; Ghahramani, Z		Kakade, S			A natural policy gradient	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.	Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Kakade, S (corresponding author), Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; BAXTER J, 1999, DIRECT GRADIENT BASE; Dayan P, 1997, NEURAL COMPUT, V9, P271, DOI 10.1162/neco.1997.9.2.271; KAKADE S, 2001, IN PRESS COLT; KONDA V, 2000, ADV NEURAL INFORMATI, P12; MACKAY DJC, 1996, MAXIMUM LIKELIHOOD C; MARBACH P, 1998, SIMULATION BASED OPT; SUTTON R, 2000, NEURAL INFORMATION P, P13; Tsitsiklis J, 1996, NEURO DYNAMIC PROGRA; Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129	10	282	290	0	9	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1531	1538						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100190
C	Lee, K; Lee, K; Lee, H; Shin, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Kimin; Lee, Kibok; Lee, Honglak; Shin, Jinwoo			A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.	[Lee, Kimin; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Seoul, South Korea; [Lee, Kibok; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA; [Lee, Honglak] Google Brain, Mountain View, CA USA; [Shin, Jinwoo] Altrics, Rosheim, France	Korea Advanced Institute of Science & Technology (KAIST); University of Michigan System; University of Michigan; Google Incorporated	Lee, K (corresponding author), Korea Adv Inst Sci & Technol, Seoul, South Korea.				Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIT) [R0132-15-1005]; National Research Council of Science & Technology (NST) - Korea government (MSIP) [CRC-15-05-ETRI]; DARPA Explainable AI (XAI) program [313498]; Sloan Research Fellowship; Kwanjeong Educational Foundation Scholarship	Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIT); National Research Council of Science & Technology (NST) - Korea government (MSIP); DARPA Explainable AI (XAI) program; Sloan Research Fellowship(Alfred P. Sloan Foundation); Kwanjeong Educational Foundation Scholarship	This work was supported in part by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.R0132-15-1005, Content visual browsing technology in the online and offline environments), National Research Council of Science & Technology (NST) grant by the Korea government (MSIP) (No. CRC-15-05-ETRI), DARPA Explainable AI (XAI) program #313498, Sloan Research Fellowship, and Kwanjeong Educational Foundation Scholarship.	Amodei D., 2016, CONCRETE PROBLEMS AI; Amodei D, 2016, PR MACH LEARN RES, V48; Chrabaszcz P., 2017, ARXIV PREPRINT ARXIV; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dezfooli Moosavi, 2016, CVPR; Evtimov Ivan, 2018, CVPR; Feinman R., 2017, ARXIV PREPRINT ARXIV; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Guo Chuan, 2017, ARXIV171100117; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Lasserre J. A., 2006, CVPR; Lee K., 2017, ICML; Lee Kibok, 2018, CVPR; Lee Kimin, 2018, INT C LEARN REPR; Liang Shiyu, 2018, INT C LEARN REPR; Ma Xingjun, 2018, INT C LEARN REPR; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Netzer Y., 2011, NIPS DLW; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinyals Oriol, 2016, ARXIV160604080, P3630; Yu F., 2015, ARXIVABS150603365 CO	32	281	285	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001069
C	Oreshkin, BN; Rodriguez, P; Lacoste, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Oreshkin, Boris N.; Rodriguez, Pau; Lacoste, Alexandre			TADAM: Task dependent adaptive metric for improved few-shot learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100.	[Oreshkin, Boris N.; Rodriguez, Pau; Lacoste, Alexandre] Element AI, Montreal, PQ, Canada; [Rodriguez, Pau] UAB, CVC, Barcelona, Spain	Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Oreshkin, BN (corresponding author), Element AI, Montreal, PQ, Canada.	boris@elementai.com; pau.rodriguez@elementai.com; allac@elementai.com	Rodríguez López, Pau/I-2762-2015	Rodríguez López, Pau/0000-0002-1689-8084	Spanish project (MINECO/FEDER) [TIN2015-65464-R]; Generalitat de Catalunya [2016FI B 01163]	Spanish project (MINECO/FEDER)(Spanish Government); Generalitat de Catalunya(Generalitat de Catalunya)	Authors acknowledge the support of the Spanish project TIN2015-65464-R (MINECO/FEDER), the 2016FI B 01163 grant of Generalitat de Catalunya. Authors would like to thank Nicolas Chapados, Adam Salvail and Rachel Samson as well as anonymous reviewers for their careful reading of the manuscript and for providing constructive feedback and valuable suggestions.	Alonso HM, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P44; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Bauer  M., 2017, ARXIV170600326; Carey S., 1978, PAPERS REPORTS CHILD; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Edwards Harrison, 2016, ICLR; Fink M, 2005, ADV NEURAL INFORM PR, P449; Finn C, 2017, PR MACH LEARN RES, V70; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Hinton G., 2015, COMPUTER SCI; Koch Gregory, 2015, P ICML DEEP LEARN WO, V2; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lacoste  A., 2017, ARXIV171205016; Lake B.M., 2013, ADV NEURAL INFORM PR; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Mishra N., 2018, INT C LEARN REPR; Munkhdalai  T., 2018, ICML; Perez E., 2018, AAAI; Perez E., 2017, ABS170703017 CORR; Ramachandran P., 2018, ICLR; Ravi S., 2017, OPTIMIZATION MODEL F; Ren M., 2018, P 6 INT C LEARN REPR; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J, 1997, MACH LEARN, V28, P105, DOI 10.1023/A:1007383707642; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shyam P, 2017, PR MACH LEARN RES, V70; Snell J., 2017, ADV NEURAL INFORM PR, P4080; Sung F., 2018, CVPR; Taigman Y, 2015, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2015.7298891; Thrun S, 1998, LEARNING TO LEARN, P181; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang Y., 2018, CVPR	34	279	287	5	22	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300067
C	Foerster, JN; Assael, YM; de Freitas, N; Whiteson, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Foerster, Jakob N.; Assael, Yannis M.; de Freitas, Nando; Whiteson, Shimon			Learning to Communicate with Deep Multi-Agent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.	[Foerster, Jakob N.; Assael, Yannis M.; de Freitas, Nando; Whiteson, Shimon] Univ Oxford, Oxford, England; [de Freitas, Nando] Canadian Inst Adv Res, CIFAR NCAP Program, London, England; [de Freitas, Nando] Google DeepMind, London, England	University of Oxford; Google Incorporated	Foerster, JN (corresponding author), Univ Oxford, Oxford, England.	jakob.foerster@cs.ox.ac.uk; yannis.assael@cs.ox.ac.uk; nandodefreitas@google.com; shimon.whiteson@cs.ox.ac.uk	Assael, Yannis/E-8160-2013	Assael, Yannis/0000-0001-7408-3847; Foerster, Jakob/0000-0001-9688-2498				[Anonymous], ARXIV160507736; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung J., 2014, ARXIV14123555; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Giles CL, 2002, LECT NOTES ARTIF INT, V2564, P377; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hausknecht Matthew, 2015, 2015 AAAI FALL S SER; Hinton G, 2011, TOP COGN SCI, V3, P74, DOI 10.1111/j.1756-8765.2010.01109.x; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kasai T, 2009, 2008 IEEE CONFERENCE ON SOFT COMPUTING IN INDUSTRIAL APPLICATIONS SMCIA/08, P1; Kraemer L, 2016, NEUROCOMPUTING, V190, P82, DOI 10.1016/j.neucom.2016.01.031; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Melo F. S., 2011, MULTIAGENT SYSTEMS, P189; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Narasimhan K., 2015, ARXIV150608941; Oliehoek FA, 2008, J ARTIF INTELL RES, V32, P289, DOI 10.1613/jair.2447; Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2; Shoham Y., 2009, MULTIAGENT SYSTEMS A; Studdert-Kennedy M., 2005, LANGUAGE ORIGINS PER; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Wu W., 2002, TECHNICAL REPORT; Zawadzki E., 2014, 14018074 ARXIV	26	273	284	4	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700007
C	VAPNIK, V		MOODY, JE; HANSON, SJ; LIPPMANN, RP		VAPNIK, V			PRINCIPLES OF RISK MINIMIZATION FOR LEARNING-THEORY	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	271	279	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						831	838						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00102
C	Wainwright, MJ; Simoncelli, EP		Solla, SA; Leen, TK; Muller, KR		Wainwright, MJ; Simoncelli, EP			Scale mixtures of Gaussians and the statistics of natural images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				WAVELET	The statistics of photographic images, when represented using multiscale (wavelet) bases, exhibit two striking types of non-Gaussian behavior. First, the marginal densities of the coefficients have extended heavy tails. Second, the joint densities exhibit variance dependencies not captured by second-order models. We examine properties of the class of Gaussian scale mixtures, and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients. This class of model suggests a Markov structure, in which wavelet coefficients are linked by hidden scaling variables corresponding to local image structure. We derive an estimator for these hidden variables, and show that a nonlinear "normalization" procedure can be used to Gaussianize the coefficients.	MIT, Stochast Syst Grp, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Wainwright, MJ (corresponding author), MIT, Stochast Syst Grp, Bldg 35-425, Cambridge, MA 02139 USA.			Wainwright, Martin J./0000-0002-8760-2236; Simoncelli, Eero/0000-0002-1206-527X				ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; Buccigrossi RW, 1999, IEEE T IMAGE PROCESS, V8, P1688, DOI 10.1109/83.806616; CHOU KC, 1994, IEEE T AUTOMAT CONTR, V39, P479, DOI 10.1109/9.280747; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Huang Jinggang, 1999, CVPR; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332; ROMBERG J, 1999, P IEEE ICIP KOB JAP; RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814; Simoncelli Eero P., 1996, P IEEE INT C IM PROC, V1, P379; Simoncelli EP, 1999, ADV NEUR IN, V11, P153; SIMONCELLI EP, 1995, P INT C IM PROC, V3, P444, DOI DOI 10.1109/ICIP.1995.537667; SIMONCELLI EP, 1997, 31 AS C SIGN SYST CO, P673; TEWFIK AH, 1992, IEEE T INFORM THEORY, V38, P904, DOI 10.1109/18.119750; Zetzsche C, 1993, INT S SOC INF DISPL, VXXIV, P933	15	268	283	0	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						855	861						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700121
C	Smola, AJ; Friess, TT; Scholkopf, B		Kearns, MS; Solla, SA; Cohn, DA		Smola, AJ; Friess, TT; Scholkopf, B			Semiparametric support vector and linear programming machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Semiparametric models are useful tools in the case where domain knowledge exists about the function to be estimated or emphasis is put onto understandability of the model. We extend two learning algorithms - Support Vector machines and Linear Programming machines to this case and give experimental results for SV machines.	GMD First, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Smola, AJ (corresponding author), GMD First, Rudower Chaussee 5, D-12489 Berlin, Germany.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				Bickel P.J., 1994, EFFICIENT ADAPTIVE E; Boser B., 1992, COMPUTATIONAL LEARNI, P144, DOI DOI 10.1145/130385.130401; CHEN S, 1995, 479 STANF U DEP STAT; FRIESS TT, 1998, RR720 TR U SHEFF; KIMELDORF GS, 1971, ANN MATH STAT, V2, P495; MICCHELLI CA, 1986, CONSTR APPROX, V2, P11, DOI 10.1007/BF01893414; Smola AJ, 1998, ALGORITHMICA, V22, P211, DOI 10.1007/PL00013831; Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X; VANDERBEI RJ, 1994, SOR9415 TR PRINC U S; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	10	268	276	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						585	591						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700083
C	Wilson, AC; Roelofs, R; Stern, M; Srebro, N; Recht, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wilson, Ashia C.; Roelofs, Rebecca; Stern, Mitchell; Srebro, Nathan; Recht, Benjamin			The Marginal Value of Adaptive Gradient Methods in Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.	[Wilson, Ashia C.; Roelofs, Rebecca; Stern, Mitchell; Recht, Benjamin] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Srebro, Nathan] Toyota Technol Inst, Chicago, IL USA	University of California System; University of California Berkeley; Toyota Technological Institute - Chicago	Wilson, AC (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	ashia@berkeley.edu; roelofs@berkeley.edu; mitchell@berkeley.edu; nati@ttic.edu; brecht@berkeley.edu	Jeong, Yongwook/N-7413-2016		DOE [AC02-05CH11231]; NSF Graduate Research Fellowships; NSF [IIS-13-02662, IIS-15-46500, CCF-1359814]; Inter ICRI-RI award; Google Faculty Award; ONR [N00014-14-1-0024, N00014-17-1-2191]; DARPA Fundamental Limits of Learning (Fun LoL) Program; Sloan Research Fellowship	DOE(United States Department of Energy (DOE)); NSF Graduate Research Fellowships(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Inter ICRI-RI award; Google Faculty Award(Google Incorporated); ONR(Office of Naval Research); DARPA Fundamental Limits of Learning (Fun LoL) Program; Sloan Research Fellowship(Alfred P. Sloan Foundation)	The authors would like to thank Pieter Abbeel, Moritz Hardt, Tomer Koren, Sergey Levine, Henry Milner, Yoram Singer, and Shivaram Venkataraman for many helpful comments and suggestions. RR is generously supported by DOE award AC02-05CH11231. MS and AW are supported by NSF Graduate Research Fellowships. NS is partially supported by NSF-IIS-13-02662 and NSF-IIS-15-46500, an Inter ICRI-RI award and a Google Faculty Award. BR is generously supported by NSF award CCF-1359814, ONR awards N00014-14-1-0024 and N00014-17-1-2191, the DARPA Fundamental Limits of Learning (Fun LoL) Program, a Sloan Research Fellowship, and a Google Faculty Award.	Choe Do Kook, 2016, P 2016 C EMP METH NA, P2331, DOI DOI 10.18653/V1/D16-1257; Cross James, 2016, P 2016 C EMP METH NA, P1, DOI DOI 10.1109/FIE.2016.7757720; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hardt M, 2016, PR MACH LEARN RES, V48; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Karparthy Andrej, PEEK TRENDS MACHINE; Keskar N.S., 2017, ICLR; Kingma D.P, P 3 INT C LEARNING R; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Ma SY, 2017, ADV NEUR IN, V30; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Mnih V, 2016, PR MACH LEARN RES, V48; Neyshabur Behnam, 2015, NEURAL INFORM PROCES; Neyshabur Behnam, 2015, INT C LEARN REPR ICL; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Reed S, 2016, PR MACH LEARN RES, V48; Sutskever Ilya, 2013, P INT C MACH LEARN I; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2	22	267	269	2	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404022
C	Chen, XZ; Kundu, K; Zhu, YK; Berneshawi, A; Ma, HM; Fidler, S; Urtasun, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chen, Xiaozhi; Kundu, Kaustav; Zhu, Yukun; Berneshawi, Andrew; Ma, Huimin; Fidler, Sanja; Urtasun, Raquel			3D Object Proposals for Accurate Object Class Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors, ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.	[Chen, Xiaozhi; Ma, Huimin] Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China; [Kundu, Kaustav; Zhu, Yukun; Berneshawi, Andrew; Fidler, Sanja; Urtasun, Raquel] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	Tsinghua University; University of Toronto	Chen, XZ (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China.	chenxz12@mails.tsinghua.edu.cn; kkundu@cs.toronto.edu; yukun@cs.toronto.edu; andrew.berneshawi@mail.utoronto.ca; mhmpub@tsinghua.edu.cn; fidler@cs.toronto.edu; urtasun@cs.toronto.edu			NSFC [61171113]; NSERC; Toyota Motor Corporation	NSFC(National Natural Science Foundation of China (NSFC)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Toyota Motor Corporation	The work was partially supported by NSFC 61171113, NSERC and Toyota Motor Corporation.	Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Banica D., 2013, CORR; Benenson R, 2013, PROC CVPR IEEE, P3666, DOI 10.1109/CVPR.2013.470; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479; Everingham M., PASCAL VISUAL OBJECT; Fidler S., 2013, CVPR; Geiger A., 2011, ADV NEURAL INFORM PR, V24; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gonzalez A., 2015, IV; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hosang J., 2015, ARXIV150205082; Hosang J., 2015, TAKING DEEPER LOOK P; Karpathy  A., 2013, ICRA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee T., 2015, ICCV; Li B., 2014, ECCV; Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179; Long Chengjiang, 2014, ACCV, P3000; Ohn-Bar E., 2015, IEEE T INTELLIGENT T; Oneata D., 2014, ECCV; Paisitkriangkrai S., 2014, ARXIV14095209; Pedro F. F., 2010, PAMI, V32, P1627, DOI [10.1109/TPAMI.2009.167, DOI 10.1109/TPAMI.2009.167]; Pepik B., 2015, IEEE T PATTERN ANAL; Pepik B., 2013, CVPR; Premebida C., 2014, IROS; Schwing AG, 2013, IEEE I CONF COMP VIS, P353, DOI 10.1109/ICCV.2013.51; Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; van de Sande Koen E. A., 2011, ICCV; Wang SL, 2015, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2015.7299022; Xiang Y, 2015, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2015.7298800; Xu J., 2014, ARXIV14085400; Yamaguchi K, 2014, LECT NOTES COMPUT SC, V8693, P756, DOI 10.1007/978-3-319-10602-1_49; Yebes J., 2014, IV; Zhang S., 2015, ARXIV150105759; Zhu YX, 2015, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2015.7298903; Zia M., 2015, IJCV; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	45	267	282	6	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100012
C	Ghiasi, G; Lin, TY; Le, QV		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ghiasi, Golnaz; Lin, Tsung-Yi; Le, Quoc V.			DropBlock: A regularization method for convolutional networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78:13% accuracy, which is more than 1:6% improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from 36:8% to 38:4%	[Ghiasi, Golnaz; Lin, Tsung-Yi; Le, Quoc V.] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Ghiasi, G (corresponding author), Google Brain, Mountain View, CA 94043 USA.							[Anonymous], 2018, CORR; Cubuk E. D., 2018, CORR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Devries T., 2017, CORR; Gal Y., 2016, ADV NEURAL INFORM PR, P1019; Gastaldi X., 2017, CORR; Goodfellow Ian J., 2013, MAXOUT NETWORKS, P2; Han D, 2017, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR.2017.668; Hariharan Bharath, 2011, P IEEE C COMP VIS PA; Hu Jie, 2018, CVPR, V1, P5; Huang G., 2017, CVPR; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Ioffe S., 2015, P 32 INT C MACH LEAR, P448; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krueger D., 2016, CORR; Lin T., 2017, ICCV; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Lin T.-Y., 2017, CVPR; Peng Chao, 2018, CVPR; Shakhnarovich G, 2017, INT C LEARN REPR; Shen Z., 2017, ICCV; Simonyan K., 2015, ADV NEURAL INFORM PR, VNIPS 2015; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2016, CVPR; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Szegedy C, 2017, 31 AAAI C ART INT, P1, DOI DOI 10.5555/3298023.3298188; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tompson J., 2015, CVPR; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang X., 2017, CVPR; Wu Yuxin, 2018, ECCV; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yamada Y., 2018, CORR; Zhou Bolei, 2018, CVPR; Zoph Barret, 2017, CVPR	36	265	278	6	31	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005032
C	Blei, DM; Griffiths, TL; Jordan, MI; Tenenbaum, JB		Thrun, S; Saul, K; Scholkopf, B		Blei, DM; Griffiths, TL; Jordan, MI; Tenenbaum, JB			Hierarchical topic models and the nested chinese restaurant process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				MIXTURE	We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting-which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.	Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Blei, DM (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					Aldous D. J., 1985, ECOLE DETE PROBABILI, P1, DOI DOI 10.1007/BFB0099421; BEAL M, ADV NEURAL INFORMATI, V14; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Griffiths T.L., 2002, P 24 ANN C COGN SCI; Hofmann T, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P682; Ishwaran H, 2003, STAT SINICA, V13, P1211; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Pitman J., 2002, COMBINATORIAL STOCHA; RASMUSSEN C, ADV NEURAL INFORMATI, V14; ROWEIS S, 1987, NIPS ABSTR; SEGAL E, ADV NEURAL INFORMATI, V14; WEST M, ASPECTS UNCERTAINTY	14	265	282	2	31	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						17	24						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500003
C	Zha, HY; He, XF; Ding, C; Simon, H; Gu, M		Dietterich, TG; Becker, S; Ghahramani, Z		Zha, HY; He, XF; Ding, C; Simon, H; Gu, M			Spectral relaxation for K-means clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function. A coordinate descend method is then used to find local minima. In this paper we show that the minimization can be reformulated as a trace maximization problem associated with the Gram matrix of the data vectors. Furthermore, we show that a relaxed version of the trace maximization problem possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix, and the cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix. As a by-product we also derive a lower bound for the minimum of the sum-of-squares cost function.	Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Zha, HY (corresponding author), Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA.							Bradley PS., 2000, MICROSOFT RES REDMON, V20, P0; BRADLEY PS, 1998, P 15 INT C MACH LEAR, P91; GIROLANI M, 2001, IN PRESS IEEE T NEUR; Golub Gene H., 2013, MATRIX COMPUTATION, V3; GU M, 2001, CSE01007 PENNS STAT; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Lovasz L., 1986, MATCHING THEORY; McCallum A.K., BOW TOOLKIT STAT LAN; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SLONIM N, 2000, P SIGIR 2000; Stewart G., 1990, MATRIX PERTURBATION	11	264	274	0	8	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1057	1064						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100132
C	Hu, BT; Lu, ZD; Li, H; Chen, QC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hu, Baotian; Lu, Zhengdong; Li, Hang; Chen, Qingcai			Convolutional Neural Network Architectures for Matching Natural Language Sentences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.	[Hu, Baotian; Chen, Qingcai] Harbin Inst Technol, Shenzhen Grad Sch, Dept Comp Sci & Technol, Xili, Peoples R China; [Lu, Zhengdong; Li, Hang] Huawei Technol Co Ltd, Noahs Ark Lab, Sha Tin, Hong Kong, Peoples R China; [Hu, Baotian] Huawei Technol, Noahs Ark Lab, Hong Kong, Peoples R China	Harbin Institute of Technology; Huawei Technologies; Huawei Technologies	Hu, BT (corresponding author), Harbin Inst Technol, Shenzhen Grad Sch, Dept Comp Sci & Technol, Xili, Peoples R China.	baotianchina@gmail.com; lu.zhengdong@huawei.com; hangli.hl@huawei.com; qingcai.chen@hitsz.edu.cn	Hu, Baotian/AAA-4102-2022	Hu, Baotian/0000-0001-7490-684X	National Natural Science Foundation of China [61173075]; China National 973 project [2014CB340301]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China National 973 project(National Basic Research Program of China)	B. Hu and Q. Chen are supported in part by National Natural Science Foundation of China 61173075. Z. Lu and H. Li are supported in part by China National 973 project 2014CB340301.	Abdel-Hamid O., 2012, P ICASSP; [Anonymous], P ICCV; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bordes A, 2014, MACH LEARN, V94, P233, DOI 10.1007/s10994-013-5363-6; Brown P. F., 1993, Computational Linguistics, V19, P263; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Kalchbrenner N., 2014, P ACL BALT US; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Lu Z., 2013, ADV NIPS; Mikolov T., 2013, ARXIV; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Rich C., 2000, ADV NIPS; Rus V., 2008, P FLAIRS C; Shen Y., 2014, P WWW; Socher R., 2011, P 28 INT C INT C MAC, P129; Socher R., 2011, ADV NIPS; Socher Richard, 2011, P C EMP METH NAT LAN, P151; Song Y., 2014, P AAAI; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Wang B., 2010, P ACL; Wang H., 2013, P EMNLP SEATTL WASH; Wu W, 2013, J MACH LEARN RES, V14, P2519; Xue X., 2008, P SIGIR 08 NEW YORK	28	263	279	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103001
C	Nam, W; Dollar, P; Han, JH		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Nam, Woonhyun; Dollar, Piotr; Han, Joon Hee			Local Decorrelation for Improved Pedestrian Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.	[Nam, Woonhyun] StradVision Inc, Pohang, South Korea; [Dollar, Piotr] Microsoft Res, Redmond, WA USA; [Nam, Woonhyun; Han, Joon Hee] POSTECH, Pohang, South Korea	Microsoft; Pohang University of Science & Technology (POSTECH)	Nam, W (corresponding author), StradVision Inc, Pohang, South Korea.	woonhyun.nam@stradvision.com; pdollar@microsoft.com; joonhan@postech.ac.kr						Benenson R, 2013, PROC CVPR IEEE, P3666, DOI 10.1109/CVPR.2013.470; Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310; Box G. E. P., 1970, Time series analysis, forecasting and control; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dollar P., 2012, ECCV; Dollar P, 2009, BRIT MACHINE VISION, DOI [10.5244/C.23.91, DOI 10.5244/C.23.91]; Dollar P., 2012, PAMI, V34; Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33; Hastie T, 2009, ELEMENTS STAT LEARNI; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Levi D, 2013, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2013.127; Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406; Marin J., 2013, ICCV; Mathias M., 2013, ICCV; Mathias Markus, 2013, IJCNN; Menze B. H., 2011, MACHINE LEARNING KNO; MURTHY S, 1994, J ARTIFICIAL INTELLI; Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257; Park D, 2013, PROC CVPR IEEE, P2882, DOI 10.1109/CVPR.2013.371; Park D, 2010, LECT NOTES COMPUT SC, V6314, P241, DOI 10.1007/978-3-642-15561-1_18; RAY WD, 1970, IEEE T INFORM THEORY, V16, P663, DOI 10.1109/TIT.1970.1054565; Rodriguez J. J., 2006, PAMI, V28; Schapire RE, 1998, ANN STAT, V26, P1651; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465; Shen CH, 2013, INT J COMPUT VISION, V103, P326, DOI 10.1007/s11263-013-0608-1; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Yan JJ, 2013, PROC CVPR IEEE, P3033, DOI 10.1109/CVPR.2013.390; Zeng XY, 2013, IEEE I CONF COMP VIS, P121, DOI 10.1109/ICCV.2013.22	36	262	280	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101090
C	Aytar, Y; Vondrick, C; Torralba, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Aytar, Yusuf; Vondrick, Carl; Torralba, Antonio			SoundNet: Learning Sound Representations from Unlabeled Video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.	[Aytar, Yusuf; Vondrick, Carl; Torralba, Antonio] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Aytar, Y (corresponding author), MIT, Cambridge, MA 02139 USA.	yusuf@csail.mit.edu; vondrick@mit.edu; torralba@mit.edu			NSF [1524817]; Google PhD fellowship	NSF(National Science Foundation (NSF)); Google PhD fellowship(Google Incorporated)	We thank MIT TIG, especially Garrett Wollman, for helping store 26 TB of video. We are grateful for the GPUs donated by NVidia. This work was supported by NSF grant #1524817 to AT and the Google PhD fellowship to CV.	[Anonymous], 2016, NIPS; Aytar Y., 2011, ICCV; Aytar Yusuf, 2015, CVIU; Ba J., 2014, NIPS; Barchiesi Daniele, 2015, SPM; Bertin-Mahieux T., 2011, ISMIR; Cakir Emre, 2015, IJCNN; Castrejon Lluis, 2016, CVPR; Chen C., 2013, CVPR; Gupta Saurabh, 2015, ARXIV150700448; Hannun A., 2014, ARXIV14125567; Hershey S., 2016, CORR; Hertel Lars, 2016, COMP TIME FREQUENCY; Hinton Geoffrey, NIPS DEEP LEARN REP; Huang J.-T., 2013, ICASSP; Ioffe S., 2015, INT C MACH LEARN, P448, DOI [10.5555/3045118.3045167, DOI 10.5555/3045118.3045167]; King DB, 2015, ACS SYM SER, V1214, P1; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kuettel D., 2012, CVPR; LeCun Y., 1998, IEEE; Lee H., 2009, NIPS; Li David, 2013, AASP CHALLENGE; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Maaten L. v. d, 2008, JMLR; McLoughlin Ian, 2015, ASL; Ngiam J., 2011, ICML; Nguyen P. X., 2016, OPEN WORLD MICROVIDE; Owens A., 2015, ARXIV151208512; Piczak Karol J, 2015, ACM MULTIMEDIA; Piczak Karol J, 2015, MLSP; Rakotomamonjy Alain, 2015, TASLP; Roma Guido, 2013, WASPAA; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Salamon Justin, 2015, ICASSP; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Stowell Dan, 2015, TM; Sutskever I., 2014, NEURIPS; Thomee B., 2016, COMMUNICATIONS ACM; Van den Oord Aaron, 2013, NIPS; Walker J., 2015, ICCV; Zhou B., 2014, NIPS	42	261	269	1	14	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701003
C	Sohn, K; Yan, XC; Lee, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sohn, Kihyuk; Yan, Xinchen; Lee, Honglak			Learning Structured Output Representation using Deep Conditional Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.	[Sohn, Kihyuk] NEC Labs Amer Inc, Princeton, NJ 08540 USA; [Sohn, Kihyuk; Yan, Xinchen; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA	NEC Corporation; University of Michigan System; University of Michigan	Sohn, K (corresponding author), NEC Labs Amer Inc, Princeton, NJ 08540 USA.	ksohn@nec-labs.com; xcyan@umich.edu; honglak@umich.edu	Yan, Xinchen/AAY-4481-2020	Yan, Xinchen/0000-0003-1019-5537	ONR [N00014-13-1-0762]; NSF CAREER grant [IIS-1453651]	ONR(Office of Naval Research); NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported in part by ONR grant N00014-13-1-0762 and NSF CAREER grant IIS-1453651. We thank NVIDIA for donating a Tesla K40 GPU.	Andrew G., 2013, ICML; [Anonymous], 2010, CALIFORNIA I TECHNOL; Bengio Y., 2014, ICML; Ciresan D., 2012, NIPS; Farabet C., 2012, ICML; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Goodfellow I., 2013, NIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2014, ECCV; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Huang G.B., 2008, WORKSHOP FACESREAL L; Huang G. B., 2008, CVPR WORKSH PERC ORG; Kae A., 2013, CVPR; Kingma D.P., 2013, ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2014, NIPS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2011, INT C ART INT STAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295; Li Y., 2013, CVPR; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Pinheiro P., 2013, ICML; Rezende D.J., 2014, PROC INT CONFER ENCE; Salakhutdinov R., 2009, AISTATS; Sermanet Pierre, 2013, ICLR; Simonyan K., 2014, ICLR; Sohn K., 2014, NIPS; Song HO, 2015, IEEE T PATTERN ANAL, V37, P1001, DOI 10.1109/TPAMI.2014.2353631; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Szegedy C., 2013, NIPS; Tang C., 2013, P ADV NEUR INF PROC, P503; Vedaldi A., 2015, ACMMM; Vincent Pascal., 2008, ICML; Wang N., 2012, CVPR; Yang J., 2014, CVPR	37	260	260	4	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101035
C	Bachman, P; Hjelm, RD; Buchwalter, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bachman, Philip; Hjelm, R. Devon; Buchwalter, William			Learning Representations by Maximizing Mutual Information Across Views	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views - e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect.	[Bachman, Philip; Buchwalter, William] Microsoft Res, Albuquerque, NM 87110 USA; [Hjelm, R. Devon] Microsoft Res, MILA, Albuquerque, NM 87110 USA	Microsoft; Microsoft	Bachman, P (corresponding author), Microsoft Res, Albuquerque, NM 87110 USA.	phil.bachman@gmail.com; devon.hjelm@microsoft.com; wibuch@microsoft.com						Arandjelovic Relja, 2017, INT C COMP VIS ICCV; Arandjelovic Relja, 2018, EUR C COMP VIS ECCV; Berthelot David, 2019, ARXIV190502249CSLG; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue Jeff, 2014, INT C MACH LEARN ICM; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Dwibedi Debidatta, 2018, INT C INT ROB SYST I; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Gidaris Spyros, 2018, ARXIV180307728; Goyal Priya, 2019, ARXIV190501235CSCV; Gutmann Michael, 2010, INT C ART INT STAT A; Haarnoja T, 2017, PR MACH LEARN RES, V70; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Kaiming, 2018, ARXIV181108883VSCV; Henaff Olivier J, 2019, ARXIV190509272CSLG; Hjelm R Devon, 2019, INT C LEARN REPR; Ji Xu, 2019, ARXIV180706653; Kakade Sham M, 2007, ANN C LEARN THEOR CO; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202; Lim Sungbim, 2019, ARXIV190500397CSLG; Logeswaran Lajanugen, 2018, 6 INT C LEARN REPR I; Ma Zhuang, 2018, C EMP METH NAT LANG; McAllester David, 2018, ARXIV181104251CSIT; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Pennington Jeffrey, 2014, EMPIRICAL METHODS NA; Poole Ben, 2019, INT C MACH LEARN ICM; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sermanet Pierre, 2017, INT C ROB AUT ICRA; Sridharan Karthik, 2008, ANN C LEARN THEOR CO; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Tian Yonglong, 2019, ARXIV190605849CSLG; van den Oord Aaron, 2018, ARXIV180703748; Vondrick Carl, 2018, EUR C COMP VIS ECCV; Wang Weiran, 2015, ICML; Yang Z., 2018, AAAI C ARTIF INTELL, P1, DOI DOI 10.1109/ICMMT.2018.8563476; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	47	259	261	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907022
C	Ramachandran, P; Parmar, N; Vaswani, A; Bello, I; Levskaya, A; Shlens, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ramachandran, Prajit; Parmar, Niki; Vaswani, Ashish; Bello, Irwan; Levskaya, Anselm; Shlens, Jonathon			Stand-Alone Self-Attention in Vision Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEOCOGNITRON	Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox. Code for this project is made available.(1)	[Ramachandran, Prajit; Parmar, Niki; Vaswani, Ashish; Bello, Irwan; Levskaya, Anselm; Shlens, Jonathon] Google Res, Brain Team, Mountain View, CA 94043 USA	Google Incorporated	Ramachandran, P (corresponding author), Google Res, Brain Team, Mountain View, CA 94043 USA.	prajit@google.com; nikip@google.com; avaswani@google.com						Bartunov S, 2018, ADV NEUR IN, V31; Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YP, 2018, ADV NEUR IN, V31; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Cohen Taco S, 2019, ARXIV190204615; Cohen Taco S, 2018, ICLR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Gehring J., 2017, P ICML; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Gonzalez R. C., 2002, DIGITAL IMAGE PROCES, V141; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu H., 2019, ARXIV190411491; Hu JS, 2018, IEEE ICC; Huang C.-Z. A., 2018, ADV NEURAL PROCESSIN; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jie H., 2017, P IEEE C COMP VIS PA, P99; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loshchilov I., 2017, P INT C LEARNING REP; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Oord A. v. d., 2016, ARXIV160903499; Parmar Niki, 2018, PR MACH LEARN RES, P4055; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ruderman D. L, 1994, ADV NEURAL INFORM PR, P551; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans Tim, 2017, ARXIV170105517; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shaw Peter, 2018, P 2018 C N AM CHAPT, P464, DOI DOI 10.18653/V1/N18-2074; Shazeer Noam, 2018, CORR; Sifre Laurent, 2014, THESIS; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan M., 2018, P IEEE C COMP VIS PA; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wu Felix, 2019, ARXIV190110430; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	69	257	258	6	14	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300007
C	Lian, XR; Zhang, C; Zhang, H; Hsieh, CJ; Zhang, W; Liu, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lian, Xiangru; Zhang, Ce; Zhang, Huan; Hsieh, Cho-Jui; Zhang, Wei; Liu, Ji			Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONSENSUS	Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart? Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.	[Lian, Xiangru; Liu, Ji] Univ Rochester, Rochester, NY 14627 USA; [Zhang, Ce] Swiss Fed Inst Technol, Zurich, Switzerland; [Zhang, Huan; Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA; [Zhang, Wei] IBM TJ Watson Res Ctr, Yorktown Hts, NY USA; [Liu, Ji] Tencent AI Lab, Bellevue, WA USA	University of Rochester; Swiss Federal Institutes of Technology Domain; ETH Zurich; University of California System; University of California Davis; International Business Machines (IBM)	Lian, XR (corresponding author), Univ Rochester, Rochester, NY 14627 USA.	xiangru@yandex.com; ce.zhang@inf.ethz.ch; victzhang@gmail.com; chohsieh@ucdavis.edu; weiz@us.ibm.com; ji.liu.uwisc@gmail.com	Jeong, Yongwook/N-7413-2016	Zhang, Ce/0000-0002-8105-7505	NSF [CCF1718513, IIS-1719097]; Swiss National Science Foundation [NRP 75 407540_167266]; IBM Zurich; Mercedes-Benz Research & Development North America; Oracle Labs; Swisscom; Chinese Scholarship Council; Department of Computer Science at ETH Zurich; TACC	NSF(National Science Foundation (NSF)); Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); IBM Zurich(International Business Machines (IBM)); Mercedes-Benz Research & Development North America; Oracle Labs; Swisscom; Chinese Scholarship Council(China Scholarship Council); Department of Computer Science at ETH Zurich; TACC	Xiangru Lian and Ji Liu are supported in part by NSF CCF1718513. Ce Zhang gratefully acknowledge the support from the Swiss National Science Foundation NRP 75 407540_167266, IBM Zurich, Mercedes-Benz Research & Development North America, Oracle Labs, Swisscom, Chinese Scholarship Council, the Department of Computer Science at ETH Zurich, the GPU donation from NVIDIA Corporation, and the cloud computation resources from Microsoft Azure for Research award program. Huan Zhang and Cho-Jui Hsieh acknowledge the support of NSF IIS-1719097 and the TACC computation resources.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Aybat N. S., 2015, ARXIV151208122; Aysal TC, 2009, IEEE T SIGNAL PROCES, V57, P2748, DOI 10.1109/TSP.2009.2016247; BACH F., 2011, ADV NEURAL INFORM PR, P451; Bianchi P, 2013, IEEE T INFORM THEORY, V59, P7405, DOI 10.1109/TIT.2013.2275131; Boyd S, 2005, IEEE INFOCOM SER, P1653; Carli R, 2010, AUTOMATICA, V46, P70, DOI 10.1016/j.automatica.2009.10.032; Chenoweth JM, 2016, FLA MUS NAT HIST-RIP, P1; Crammer K, 2006, J MACH LEARN RES, V7, P551; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Dekel O, 2012, J MACH LEARN RES, V13, P165; Fagnani F., 2008, IEEE J SELECTED AREA, V26; Feng MW, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2413, DOI 10.1145/2983323.2983377; Feyzmahdavian H. R., 2015, ASYNCHRONOUS MINIBAT; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lan G., 2017, ARXIV170103961; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Lian X., 2016, ADV NEURAL INFORM PR, P3054; Lin Z., 2017, ARXIV PREPRINT ARXIV; Lu J, 2010, P AMER CONTR CONF, P301; Mokhtari A, 2016, J MACH LEARN RES, V17; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nvidia, NCCL OPT PRIM COLL M; Olfati-Saber R, 2007, P IEEE, V95, P215, DOI 10.1109/JPROC.2006.887293; Ram S.S., 2010, RECENT ADV OPTIMIZAT, P51; Ram SS, 2010, J OPTIMIZ THEORY APP, V147, P516, DOI 10.1007/s10957-010-9737-7; Ram SS, 2009, IEEE DECIS CONTR P, P3581, DOI 10.1109/CDC.2009.5399485; Ram SS, 2009, INT CONF ACOUST SPEE, P3653, DOI 10.1109/ICASSP.2009.4960418; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Schenato L, 2007, IEEE DECIS CONTR P, P4060; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shi W., LINEAR CONVERGENCE A; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Sirb B, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P76, DOI 10.1109/BigData.2016.7840591; Srivastava K, 2011, IEEE J-STSP, V5, P772, DOI 10.1109/JSTSP.2011.2118740; Wu T., 2016, ARXIV161200150; Yan F, 2013, IEEE T KNOWL DATA EN, V25, P2483, DOI 10.1109/TKDE.2012.191; Yang TB, 2014, MACH LEARN, V95, P183, DOI 10.1007/s10994-013-5418-8; Yong Zhuang, 2013, P 7 ACM C REC SYST, P249, DOI DOI 10.1145/2507157.2507164; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Zhang H., 2016, HOGWILD NEW MECH DEC; Zhang RL, 2014, PR MACH LEARN RES, V32, P1701; Zhang S., 2015, P ADV NEURAL INFORM, P685, DOI DOI 10.1145/3207677.3277958; Zhang W., 2016, IEEE INT C DAT MIN; Zhang W., 2016, PROC 25 INT JOINT C, P2350; Zhang W., 2017, P 25 ACM INT C INF K; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	53	257	261	3	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405040
C	Berthelot, D; Carlini, N; Goodfellow, I; Oliver, A; Papernot, N; Raffel, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Berthelot, David; Carlini, Nicholas; Goodfellow, Ian; Oliver, Avital; Papernot, Nicolas; Raffel, Colin			MixMatch: A Holistic Approach to Semi-Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that guesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using MixUp. MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success. We release all code used in our experiments.	[Berthelot, David; Carlini, Nicholas; Goodfellow, Ian; Oliver, Avital; Papernot, Nicolas; Raffel, Colin] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Berthelot, D (corresponding author), Google Res, Mountain View, CA 94043 USA.	dberth@google.com; ncarlini@google.com; ian-academic@mailfence.com; avitalo@google.com; papernot@google.com; craffel@google.com						Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Aila T, 2017, 5 INT C LEARN REPR; [Anonymous], 2018, IEEE T PAMI; [Anonymous], 2019, CORR; BELKIN M, 2002, ADV NEURAL INFORM PR; Ben Athiwaratkun, 2018, ARXIV180605594; Bengio Yoshua, 2006, LABEL PROPAGATION QU; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Carin L, 2016, ADV NEURAL INFORM PR; Chapelle O., 2006, IEEE T NEURAL NETW, V20, P542; Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052; COATES A, 2011, INT C MACH LEARN; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Cubuk E. D., 2018, ARXIV180509501; Denton E. L., 2016, ARXIV161106430; DeVries T., 2017, P 2017 COMPUTER VISI; DWORK CYNTHIA, 2016, J PRIVACY CONFIDENTI, V7, P17, DOI DOI 10.29012/JPC.V7I3.405; Gammerman A., 1998, P 14 C UNC ART INT; Gastaldi Xavier, 2017, 5 INT C LEARN REPR W; Goodfellow I., 2016, DEEP LEARNING; Goodfellow Ian J., 2011, NIPS WORKSH CHALL LE; Grandvalet Y., 2005, CAP, V367, P281; Hinton G., 1993, P 6 ANN ACM C COMP L; Ji Xu, 2018, ARXIV180706653; Joachims T., 1999, INT C MACH LEARN; Joachims Thorsten, 2003, INT C MACH LEARN; Kingma D. P, 2014, ADV NEURAL INFORM PR; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lakshminarayanan B., 2017, ADV NEURAL INFORM PR; Lasserre Julia A., 2006, IEEE COMP SOC C COMP; Lee Dong-Hyun, 2013, WORKSH CHALL REPR LE, V3; Liu Bin, 2018, ARXIV181208781; Loshchilov I., 2017, ARXIV171105101; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Nissim K., 2015, CORR; Odena Augustus, 2016, CONDITIONAL IMAGE SY; Oliver A., 2018, ADV NEURAL INFORM PR, V31, P3235; Papernot N., 2018, ICLR P, P1; Papernot N., 2017, ICLR; Rasmus A., 2015, ADV NEURAL INFORM PR; Sajjadi Mehdi, 2016, ADV NEURAL INFORM PR; Salakhutdinov R., 2007, ADV NEURAL INFORM PR; Salimans T, 2016, ADV NEURAL INFORM PR; Simard Patrice Y, 2003, P INT C DOC AN REC; Tarvainen A., 2017, ADV NEURAL INFORM PR; Zhang Guodong, 2018, ARXIV PREPRINT ARXIV; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31; Zhao J., 2015, ARXIV150602351; Zhu X., 2003, INT C MACH LEARN	49	255	256	8	34	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305009
C	Burges, CJC; Scholkopf, B		Mozer, MC; Jordan, MI; Petsche, T		Burges, CJC; Scholkopf, B			Improving the accuracy and speed of support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Support Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inversion for ill-posed problems. Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing interest. In this paper we combine tyro such techniques on a pattern recognition problem. The method for improving generalization performance (the ''virtual support vector'' method) does so by incorporating known invariances of the problem. This method achieves a drop in the error rate on 10,000 NIST test digit images of 1.4% to 1.0%. The method for improving the speed (the ''reduced set'' method) does so by approximating the support vector decision surface. We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine. The combined approach yields a machine which is both 22 times faster than the original machine, and which has better generalization performance, achieving 1.1% error. The virtual support vector method is applicable to any SVM problem with known invariances. The reduced set method is applicable to any support vector machine.			Burges, CJC (corresponding author), AT&T BELL LABS,LUCENT TECHNOL,ROOM 3G429,101 CRAWFORDS CORNER RD,HOLMDEL,NJ 07733, USA.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925					0	253	265	0	15	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						375	381						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00053
C	Thrun, S		Touretzky, DS; Mozer, MC; Hasselmo, ME		Thrun, S			Is learning the n-th thing any easier than learning the first?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CARNEGIE MELLON UNIV,DEPT COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University									0	253	263	0	7	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						640	646						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00091
C	Ho, J; Ermon, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ho, Jonathan; Ermon, Stefano			Generative Adversarial Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.	[Ho, Jonathan] OpenAI, San Francisco, CA 94110 USA; [Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Ho, J (corresponding author), OpenAI, San Francisco, CA 94110 USA.	hoj@openai.com; ermon@cs.stanford.edu			NSF Graduate Research Fellowship [DGE-114747]	NSF Graduate Research Fellowship(National Science Foundation (NSF))	We thank Jayesh K. Gupta, John Schulman, and the anonymous reviewers for assistance, advice, and critique. This work was supported by the SAIL-Toyota Center for AI Research and by a NSF Graduate Research Fellowship (grant no. DGE-114747).	BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156; Boyd S, 2004, CONVEX OPTIMIZATION; Brockman G., 2016, OPENAI GYM; Finn C, 2016, P 33 INT C MACH LEAR, P49; Geramifard A., 2015, JMLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hiriart-Urruty J.B., 1996, CONVEX ANAL MINIMIZA, V305-06; Ho J., 2016, P 33 INT C MACH LEAR; Kingma D.P, P 3 INT C LEARNING R; Levine S., 2011, C NEURAL INFORM PROC, V24, P19; Levine S., 2012, INT C MACHINE LEARNI, P475; Levine S, 2014, ADV NEUR IN, V27; Moore A.W., 1990, TECHNICAL REPORT; Ng A., 2004, P 21 INT C MACH LEAR; Ng A. Y., 2000, ICML; Nguyen X, 2009, ANN STAT, V37, P876, DOI 10.1214/08-AOS595; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Puterman M.L., 2014, MARKOV DECISION PROC; Ratliff ND, 2009, AUTON ROBOT, V27, P25, DOI 10.1007/s10514-009-9121-3; Ross S., 2010, PROC 13 INT C ARTIF, V9, P661; Ross St<prime>ephane, 2011, AISTATS; Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Syed U., 2007, ADV NEURAL INFORM PR; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Ziebart B. D., 2010, P 27 INT C INT C MAC, P1255; Ziebart B. D., 2008, AAAI AAAI 08	30	246	249	15	25	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703027
C	Li, H; Xu, Z; Taylor, G; Studer, C; Goldstein, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Hao; Xu, Zheng; Taylor, Gavin; Studer, Christoph; Goldstein, Tom			Visualizing the Loss Landscape of Neural Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.	[Li, Hao; Xu, Zheng; Goldstein, Tom] Univ Maryland, College Pk, MD 20742 USA; [Taylor, Gavin] US Naval Acad, Annapolis, MD 21402 USA; [Studer, Christoph] Cornell Univ, Ithaca, NY 14853 USA	University System of Maryland; University of Maryland College Park; United States Department of Defense; United States Navy; United States Naval Academy; Cornell University	Li, H (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	haoli@cs.umd.edu; xuzh@cs.umd.edu; taylor@usna.edu; studer@cornell.edu; tomg@cs.umd.edu	Li, Hao/X-1188-2019		Office of Naval Research [N00014-17-1-2078]; DARPA Lifelong Learning Machines [FA8650-18-2-7833]; DARPA YFA [D18AP00055]; Sloan Foundation; ONR [N0001418WX01582]; DOD HPC Modernization Program; Xilinx, Inc.; NSF [ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, ECCS-1824379]	Office of Naval Research(Office of Naval Research); DARPA Lifelong Learning Machines; DARPA YFA; Sloan Foundation(Alfred P. Sloan Foundation); ONR(Office of Naval Research); DOD HPC Modernization Program(United States Department of Defense); Xilinx, Inc.; NSF(National Science Foundation (NSF))	Li, Xu, and Goldstein were supported by the Office of Naval Research (N00014-17-1-2078), DARPA Lifelong Learning Machines (FA8650-18-2-7833), DARPA YFA (D18AP00055), and the Sloan Foundation. Taylor was supported by ONR (N0001418WX01582), and the DOD HPC Modernization Program. Studer was supported in part by Xilinx, Inc. and by the NSF under grants ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, and ECCS-1824379.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chaudhari Pratik, 2017, ICLR POSTER; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; DeSoham, 2017, AISTATS; Dinh L, 2017, PR MACH LEARN RES, V70; Dziugaite G. K., 2017, UAI; FREEMAN C. D., 2017, P 5 INT C LEARN REPR; Gallagher M, 2003, IEEE T SYST MAN CY B, V33, P28, DOI 10.1109/TSMCB.2003.808183; Glorot X., 2010, PROC MACH LEARN RES, P249; Goldstein Tom, 2016, ARXIV161007531; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goyal Priya, 2017, ARXIV170602677; Haeffele B, 2017, PROC IEEE C COMPUT V, P7331; HARDT M., 2017, P 5 INT C LEARN REPR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Im, 2016, ARXIV161204010; Kawaguchi K., 2017, ARXIV171005468V4STAT, DOI DOI 10.2196/jmir.5870; Keskar N.S., 2017, ICLR; KROGH A, 1992, ADV NEUR IN, V4, P950; Liao Qianli, 2017, ARXIV170309833; Lipton Zachary C, 2016, ICLR WORKSH; Lorch Eliana, 2016, ICML WORKSHOP VISUAL; Nguyen Q, 2017, PR MACH LEARN RES, V70; Safran I, 2016, PR MACH LEARN RES, V48; Smith L. N., 2017, ARXIV170204283; Soltanolkotabi M., 2017, ARXIV170704926; Soudry Daniel, 2017, ARXIV170205777; Swirszcz G., 2016, ARXIV PREPRINT ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tian YD, 2017, PR MACH LEARN RES, V70; Xie Bo, 2017, AISTATS; YuN C., 2017, ARXIV170702444; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414	43	245	251	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000085
C	Klambauer, G; Unterthiner, T; Mayr, A; Hochreiter, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Klambauer, Guenter; Unterthiner, Thomas; Mayr, Andreas; Hochreiter, Sepp			Self-Normalizing Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DEEP; CLASSIFICATION	Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance-even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep.	[Klambauer, Guenter] Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria; Johannes Kepler Univ Linz, Inst Bioinformat, A-4040 Linz, Austria	Johannes Kepler University Linz; Johannes Kepler University Linz	Klambauer, G (corresponding author), Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria.	klambauer@bioinf.jku.at; unterthiner@bioinf.jku.at; mayr@bioinf.jku.at; hochreit@bioinf.jku.at	Jeong, Yongwook/N-7413-2016; Unterthiner, Thomas/K-7231-2018; Hochreiter, Sepp/AAI-5904-2020	Unterthiner, Thomas/0000-0001-5361-3087; Hochreiter, Sepp/0000-0001-7449-2528				Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1; BRADLEY RC, 1981, J MULTIVARIATE ANAL, V11, P1, DOI 10.1016/0047-259X(81)90128-7; Ciresan D, 2015, IEEE IJCNN; Clevert D.-A., 2015, 5 INT C LEARN REPR; Desjardins G., 2015, ADV NEURAL INFORM PR, P2071; Dugan P., 2016, ARXIV160500982; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Graves A., 2008, ADV NEURAL INFORM PR, P545, DOI DOI 10.1007/978-1-4471-4072-6; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huval B, 2015, ARXIV150401716; Korolev V, 2012, SCAND ACTUAR J, P81, DOI 10.1080/03461238.2010.485370; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Mayr A, 2016, FRONT ENV SCI-SWITZ, V3, DOI 10.3389/fenvs.2015.00080; Sak H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1468; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wainberg M., 2016, J MACHINE LEARNING R, V17, P1	30	243	243	2	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401002
C	Kleinberg, J		Dietterich, TG; Becker, S; Ghahramani, Z		Kleinberg, J			Small-world phenomena and the dynamics of information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA						Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Kleinberg, J (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.							ACHLIOPTAS D, 2001, P 42 IEEE S FDN COMP; Adamic LA, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.046135; [Anonymous], 1990, 6 DEGREES SEPARATION; BOLOOBAS B, 1988, SIAM J DISC MATH, V1; CHAKRABARTI S, 1999, P 8 INT WORLD WID WE; CHO J, 1998, P 7 INT WORLD WID WE; CLARKE I, 2000, INT WORKSH DES ISS A; COHN D, 2000, ADV NEUR INF P SYS N, P13; DILIGENTI M, 2000, P 26 INT C VER LARG; GETOOR L, 2001, P 18 INT C MACH LEAR; KEMPE D, 2001, P 33 ACM S THEOR COM; KILLWORTH P, 1978, SOCIAL NETWORKS, V1; KLEINBERG J, 1999, P 32 ACM S THEOR COM; Kleinberg JM, 2000, NATURE, V406, P845, DOI 10.1038/35022643; Kochen Manfred, 1989, SMALL WORLD; MILGRAM S, 1967, PSYCHOL TODAY, V1, P61; NEWMAN MEJ, IN PRESS P NATL ACAD; Oram Andy., 2001, PEER TO PEER HARNESS; PUNIYANI A, INT WALKS SCALE FREE; Ratnasamy S., 2001, P ACM SIGCOMM; Rowstron A., 2001, IFIP ACM INT C DISTR; STOICA I, 2001, P ACM SIGCOMM; WATTS D, 2001, COMMUNICATION    DEC; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; ZHANG H, 2002, P IEEE INF	25	242	244	2	8	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						431	438						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100054
C	Raghu, M; Zhang, CY; Kleinberg, J; Bengio, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Raghu, Maithra; Zhang, Chiyuan; Kleinberg, Jon; Bengio, Samy			Transfusion: Understanding Transfer Learning for Medical Imaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIABETIC-RETINOPATHY; VALIDATION	Transfer learning from natural image datasets, particularly IMAGENET, using standard large models and corresponding pretrained weights has become a de-facto method for deep learning applications to medical imaging. However, there are fundamental differences in data sizes, features and task specifications between natural image classification and the target medical tasks, and there is little understanding of the effects of transfer. In this paper, we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to IMAGENET architectures. Investigating the learned representations and features, we find that some of the differences from transfer learning are due to the over-parametrization of standard models rather than sophisticated feature reuse. We isolate where useful feature reuse occurs, and outline the implications for more efficient model exploration. We also explore feature independent benefits of transfer arising from weight scalings.	[Raghu, Maithra; Kleinberg, Jon] Cornell Univ, Ithaca, NY 14853 USA; [Raghu, Maithra; Zhang, Chiyuan; Bengio, Samy] Google Brain, Mountain View, CA 94043 USA	Cornell University; Google Incorporated	Raghu, M (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.; Raghu, M (corresponding author), Google Brain, Mountain View, CA 94043 USA.	maithrar@gmail.com; chiyuan@google.com; kleinber@cs.cornell.edu; bengio@google.com	zhang, chi/GRX-3610-2022					Aarti Bagul, 2017, Arxiv, DOI arXiv:1711.05225; Abramoff MD, 2016, INVEST OPHTH VIS SCI, V57, P5200, DOI 10.1167/iovs.16-19964; Ahsan H, 2015, DIABETES METAB SYND, V9, P51, DOI 10.1016/j.dsx.2014.09.011; American Academy of Ophthalmology, 2002, INT CLIN DIAB RET DI; [Anonymous], 2017, TENS SLIM INC V3; De Fauw J, 2018, NAT MED, V24, P1342, DOI 10.1038/s41591-018-0107-6; Ding Y, 2019, RADIOLOGY, V290, P456, DOI 10.1148/radiol.2018180958; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Geirhos R., 2019, P INT C LEARNING REP, P1; Gotmare Akhilesh, 2018, ARXIV181013243; Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Kaiming, 2018, ARXIV181108883; Huh Minyoung, 2016, ARXIV160808614; Irvin J, 2019, AAAI CONF ARTIF INTE, P590; Khosravi Pegah, 2018, ROBUST AUTOMATED ASS, DOI [10.1101/394882, DOI 10.1101/394882]; Kornblith Simon, 2018, ARXIV180508974; Kudugunta Sneha Reddy, 2019, ARXIV190902197; Magill M., 2018, P 32 INT C NEUR INF, P4075; Morcos Ari S., 2018, NEURIPS; Ngiam Jiquan, 2018, DOMAIN ADAPTIVE TRAN, P1; Pasa F, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42557-4; Raghu M, 2018, ARXIV180701771; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Saphra Naomi, 2018, ARXIV181100225; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Topol EJ, 2019, NAT MED, V25, P44, DOI 10.1038/s41591-018-0300-7; van der Heijden AA, 2018, ACTA OPHTHALMOL, V96, P63, DOI 10.1111/aos.13613; Voita E, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4396; Wang X., 2017, P IEEE C COMPUTER VI, P2097, DOI 10.1109/CVPR.2017.369; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang Chiyuan, 2019, ARXIV190201996	33	240	242	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303035
C	Taskar, B; Guestrin, C; Koller, D		Thrun, S; Saul, K; Scholkopf, B		Taskar, B; Guestrin, C; Koller, D			Max-margin Markov networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their Popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M-3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M-3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in Structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.	Stanford Univ, Stanford, CA 94305 USA	Stanford University	Taskar, B (corresponding author), Stanford Univ, Stanford, CA 94305 USA.							ALTUN Y, 2003, P ICML; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; COLLINS M, 2001, IWPT; COWELL RG, 1999, PROBABILISTIC NETWOR; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; KASSEL R, 1995, THESIS MIT SPOK LANG; Lafferty J., 2001, P ICML01; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Platt J.C., 1999, NIPS; TASKAR B, 2002, P UAI02; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Yedidia J.S., 2000, NIPS; Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713	13	239	247	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						25	32						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500004
C	Sato, A; Yamada, K		Touretzky, DS; Mozer, MC; Hasselmo, ME		Sato, A; Yamada, K			Generalized learning vector quantization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						NEC CORP LTD,INFORMAT TECHNOL RES LABS,MIYAMAE KU,KAWASAKI,KANAGAWA 216,JAPAN	NEC Corporation									0	239	245	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						423	429						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00060
C	Sollich, P; Krogh, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Sollich, P; Krogh, A			Learning with ensembles: How over-fitting can be useful	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV EDINBURGH,DEPT PHYS,EDINBURGH EH8 9YL,MIDLOTHIAN,SCOTLAND	University of Edinburgh			Krogh, Anders/M-1541-2014; Sollich, Peter/ABC-2993-2020; Sollich, Peter/H-2174-2011	Sollich, Peter/0000-0003-0169-7893					0	239	267	0	5	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						190	196						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00027
C	DeBonet, JS; Isbell, CL; Viola, P		Mozer, MC; Jordan, MI; Petsche, T		DeBonet, JS; Isbell, CL; Viola, P			MIMIC: Finding optima by estimating probability densities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, experience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization landscape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to refine our estimate of the structure. Our technique obtains significant speed gains over other randomized optimization procedures.			DeBonet, JS (corresponding author), MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139, USA.								0	238	241	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						424	430						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00060
C	Lowe, R; Wu, Y; Tamar, A; Harb, J; Abbeel, P; Mordatch, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lowe, Ryan; Wu, Yi; Tamar, Aviv; Harb, Jean; Abbeel, Pieter; Mordatch, Igor			Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.	[Lowe, Ryan; Harb, Jean] McGill Univ, Montreal, PQ H3A 2T5, Canada; [Lowe, Ryan; Harb, Jean; Abbeel, Pieter; Mordatch, Igor] OpenAI, San Francisco, CA 94110 USA; [Wu, Yi; Tamar, Aviv; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA	McGill University; University of California System; University of California Berkeley	Lowe, R (corresponding author), McGill Univ, Montreal, PQ H3A 2T5, Canada.; Lowe, R; Mordatch, I (corresponding author), OpenAI, San Francisco, CA 94110 USA.; Wu, Y (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	ryan.lowe@cs.mcgillca; jxwuyi@gmail.com; mordatch@openai.com	Jeong, Yongwook/N-7413-2016		Vanier CGS Scholarship; Samsung Advanced Institute of Technology	Vanier CGS Scholarship; Samsung Advanced Institute of Technology(Samsung)	The authors would like to thank Jacob Andreas, Smitha Milli, Jack Clark, Jakob Foerster, and others at OpenAI and UC Berkeley for interesting discussions related to this paper, as well as Jakub Pachocki, Yura Burda, and Joelle Pineau for comments on the paper draft. We thank Tambet Matiisen for providing the code base that was used for some early experiments associated with this paper. Ryan Lowe is supported in part by a Vanier CGS Scholarship and the Samsung Advanced Institute of Technology. Finally, we'd like to thank OpenAI for fostering an engaging and productive research environment.	[Anonymous], [No title captured]; [Anonymous], 2016, ARXIV161006918; Assael Y. M, 2016, ABS160506676 CORR; Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P106; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Chalkiadakis Georgios, 2003, P 2 INT JOINT C AUT, P709; Foerster J, 2017, ARXIV170508926; Frank MC, 2012, SCIENCE, V336, P998, DOI 10.1126/science.1218633; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta J. K., 2017, COOPERATIVE MULTIAGE; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Jang Eric, 2017, P 5 INT C LEARN REPR; Junling Hu, 1998, Proceedings of the Second International Conference on Autonomous Agents, P239; Lauer Martin, 2000, P 17 INT C MACHINE L; Lazaridou Angeliki, 2016, ARXIV161207182; Leibo J. Z., 2017, ABS170203037 CORR; Levine S., 2015, ARXIV150400702; Littman ML, 1994, ICML 1994, P157; Matignon L., 2012, AAAI; Matignon L, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P64, DOI 10.1109/IROS.2007.4399095; Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mordatch I., 2017, ARXIV170304908; Omidshafiei S., 2017, ABS170306182 CORR; Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2; Peng P., 2017, ABS170310069 CORR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Sukhbaatar S, 2017, ARXIVABS170305407 CO; Sutton R. S., 2000, ADV NEURAL INFORM PR; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Tesauro G, 2004, ADV NEUR IN, V16, P871; Thomas P. S., 2011, P 28 INT C MACH LEAR, P137; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	36	237	237	56	183	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406044
C	Sonderby, CK; Raiko, T; Maaloe, L; Sonderby, SK; Winther, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sonderby, Casper Kaae; Raiko, Tapani; Maaloe, Lars; Sonderby, Soren Kaae; Winther, Ole			Ladder Variational Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch-normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.	[Sonderby, Casper Kaae; Sonderby, Soren Kaae; Winther, Ole] Univ Copenhagen, Bioinformat Ctr, Dept Biol, Copenhagen, Denmark; [Raiko, Tapani] Aalto Univ, Dept Comp Sci, Espoo, Finland; [Maaloe, Lars; Winther, Ole] Tech Univ Denmark, Dept Appl Math & Comp Sci, Lyngby, Denmark	University of Copenhagen; Aalto University; Technical University of Denmark	Sonderby, CK (corresponding author), Univ Copenhagen, Bioinformat Ctr, Dept Biol, Copenhagen, Denmark.	casperkaae@gmail.com; tapani.raiko@aalto.fi; larsma@dtu.dk; skaaesonderby@gmail.com; olwi@dtu.dk						Bornschein J., 2015, ARXIV150603877; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Burda Yuri, 2015, ARXIV150900519; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Dieleman S., 2015, LASAGNE 1 RELEASE, V1; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lake B.M., 2013, ADV NEURAL INFORM PR; LeCun Y., 2004, COMPUTER VISION PATT; Maaloe L, 2016, PR MACH LEARN RES, V48; MacKay DJ., 2001, LOCAL MINIMA SYMMETR; Raiko T., 2007, J MACHINE LEARNING R, V8; Raiko T., 2014, ADV NEURAL INFORM PR; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Theano Development Team, 2016, ARXIV160502688 THEAN; Tran D., 2015, ARXIV151106499; Valpola H., 2015, ARXIV14117783; van den Broeke G., 2016, THESIS; van den Oord A, 2016, PR MACH LEARN RES, V48	24	236	240	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702022
C	Craven, MW; Shavlik, JW		Touretzky, DS; Mozer, MC; Hasselmo, ME		Craven, MW; Shavlik, JW			Extracting tree-structured representations of trained networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV WISCONSIN,DEPT COMP SCI,MADISON,WI 53706	University of Wisconsin System; University of Wisconsin Madison									0	236	242	1	3	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						24	30						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00004
C	Yan, XC; Yang, JM; Yumer, E; Guo, YJ; Lee, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yan, Xinchen; Yang, Jimei; Yumer, Ersin; Guo, Yijie; Lee, Honglak			Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the perspective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.	[Yan, Xinchen; Guo, Yijie; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA; [Yang, Jimei; Yumer, Ersin] Adobe Res, San Jose, CA USA; [Lee, Honglak] Google Brain, Mountain View, CA USA	University of Michigan System; University of Michigan; Adobe Systems Inc.; Google Incorporated	Yan, XC (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	xcyan@umich.edu; jimyang@adobe.com; yumer@adobe.com; guoyijie@umich.edu; honglak@umich.edu	Yan, Xinchen/AAY-4481-2020	Yan, Xinchen/0000-0003-1019-5537	NSF CAREER [IIS-1453651]; ONR [N00014-13-1-0762]; Sloan Research Fellowship	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); Sloan Research Fellowship(Alfred P. Sloan Foundation)	This work was supported in part by NSF CAREER IIS-1453651, ONR N00014-13-1-0762, Sloan Research Fellowship, and a gift from Adobe. We acknowledge NVIDIA for the donation of GPUs. We also thank Yuting Zhang, Scott Reed, Junhyuk Oh, Ruben Villegas, Seunghoon Hong, Wenling Shang, Kibok Lee, Lajanugen Logeswaran, Rui Zhang and Yi Zhang for helpful comments and discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chang Angel X., 2015, ARXIV151203012CSGR P; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Collobert R., 2011, NIPS; Girdhar R., 2016, ARXIV160308637; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Lee H., 2009, P ANN INT C MACH LEA, P609; Memisevic Roland, 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383036; Michalski V, 2014, ADV NEUR IN, V27; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Rezende DJ, 2016, ADV NEUR IN, V29; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Yang Jimei, 2015, NIPS; Yumer E., 2016, ECCV	24	234	236	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701063
C	Hyvarinen, A		Jordan, MI; Kearns, MJ; Solla, SA		Hyvarinen, A			New approximations of differential entropy for independent component analysis and projection pursuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We derive a first-order approximation of the density of maximum entropy for a continuous 1-D random variable, given a number of simple constraints. This results in a density expansion which is somewhat similar to the classical polynomial density expansions by Gram-Charlier and Edge North. Using this approximation of density, an approximation of 1-D differential entropy is derived. The approximation of entropy is both more exact and more robust against outliers than the classical approximation based on the polynomial density expansions, without being computationally more expensive. The approximation has applications, for example, in independent component analysis and projection pursuit.	Helsinki Univ Technol, Lab Comp & Informat Sci, FIN-02015 HUT, Finland	Aalto University	Hyvarinen, A (corresponding author), Helsinki Univ Technol, Lab Comp & Informat Sci, POB 2200, FIN-02015 HUT, Finland.								0	234	252	0	16	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						273	279						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700039
C	Blankertz, B; Curio, G; Muller, KR		Dietterich, TG; Becker, S; Ghahramani, Z		Blankertz, B; Curio, G; Muller, KR			Classifying single trial EEG: Towards brain computer interfacing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				POTENTIALS; MOVEMENTS	Driven by the progress in the field of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming finger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100-230ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classification accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classifiers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).	Fraunhofer FIRST IDA, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Blankertz, B (corresponding author), Fraunhofer FIRST IDA, Kekulestr 7, D-12489 Berlin, Germany.		Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685; Curio, Gabriel/0000-0002-3377-7735; Blankertz, Benjamin/0000-0002-2437-4846				Bayliss JD, 2000, ADV NEUR IN, V12, P3; BEISTEINER R, 1995, EVOKED POTENTIAL, V96, P183, DOI 10.1016/0168-5597(94)00226-5; Bennett K.P., 1992, OPT MET SOFTW, V1, P23; Birbaumer N, 1999, NATURE, V398, P297, DOI 10.1038/18581; Cui RQ, 1999, NEUROIMAGE, V9, P124, DOI 10.1006/nimg.1998.0388; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; LANG W, 1989, EXP BRAIN RES, V74, P99; Makeig S, 2000, IEEE T REHABIL ENG, V8, P208, DOI 10.1109/86.847818; Mika S, 2001, ADV NEUR IN, V13, P591; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Penny WD, 2000, IEEE T REHABIL ENG, V8, P214, DOI 10.1109/86.847820; Peters BO, 2001, IEEE T BIO-MED ENG, V48, P111, DOI 10.1109/10.900270; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; VIDAL JJ, 1973, ANNU REV BIOPHYS BIO, V2, P157, DOI 10.1146/annurev.bb.02.060173.001105; Wolpaw JR, 2000, IEEE T REHABIL ENG, V8, P222, DOI 10.1109/86.847823; 1999, ILOG SOLVER ILOG CPL	17	232	239	0	11	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						157	164						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100020
C	Crammer, K; Singer, Y		Dietterich, TG; Becker, S; Ghahramani, Z		Crammer, K; Singer, Y			Pranking with ranking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We discuss the problem of ranking instances. In our framework each instance is associated with a rank or a rating, which is an integer from I to k. Our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance's true rank. We describe a simple and efficient online algorithm, analyze its performance in the mistake bound model, and prove its correctness. We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering. In the experiments we performed, our algorithm outperforms online algorithms for regression and classification applied to ranking.	Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Crammer, K (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.							Cohen WW, 1999, J ARTIF INTELL RES, V10, P243, DOI 10.1613/jair.587; CRAMMER K, 2001, P 14 ANN C COMP LEAR; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; FREUND Y, 1998, MACHINE LEARNING; Herbrich R., 2000, ADV LARGE MARGIN CLA; Kemeny J.G., 1962, MATH MODELS SOCIAL S; McJones P., 1997, EACHMOVIE COLLABORAT; Vapnik V.N, 1998, STAT LEARNING THEORY; WIDROW B, 1988, 1960 IRE WESCON CONV	9	232	242	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						641	647						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100080
C	Roweis, ST		Leen, TK; Dietterich, TG; Tresp, V		Roweis, ST			One microphone source separation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Source separation, or computational auditory scene analysis, attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment. Unmixing algorithms such as ICA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available. I present a technique called refiltering which recovers sources by a nonstationary reweighting ("masking") of frequency sub-bands from a single recording, and argue for the application of statistical algorithms to learning this masking function. I present results of a simple factorial HMM system which learns on recordings of single speakers and can then separate mixtures using only one observation signal by computing the masking function and then refiltering.	Univ Coll London, Gatsby Unit, London, England	University of London; University College London	Roweis, ST (corresponding author), Univ Coll London, Gatsby Unit, Mortimer St, London, England.							ATTIAS H, 2001, SPEECH DENOISING DER; Bregman A.S., 1994, AUDITORY SCENE ANAL; BROWN G, 2000, NIPS, V12; BROWN GJ, 1994, COMPUTER SPEECH LANG, V8; CAUWENBERGHS G, 1999, IEEE S CIRC SYST ISC; ELLIS DPW, 1994, P 12 INT C PATT REC; Freeman W. T., 1999, TR9908 MITS EL RES L; GALES MJF, 1996, IEEE T SPEECH AUDIO, V4; Ghahramani Z., 1997, MACHINE LEARNING, V29; SHI J, 1997, IEEE C COMP VIS PATT; VARGA AP, 1990, IEEE C AC SPEECH SIG; WAN EA, 1998, IEEE C AC SPEECH SIG	12	226	236	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						793	799						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800112
C	Collins, M; Duffy, N		Dietterich, TG; Becker, S; Ghahramani, Z		Collins, M; Duffy, N			Convolution kernels for Natural Language	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.	AT&T Labs Res, Florham Pk, NJ 07932 USA	AT&T	Collins, M (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.							Aizerman M. A., 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678; Bod R, 1998, GRAMMAR EXPERIENCE B; CHARMIAK E, 1997, AI MAGAZINE, V18; COLLINS M, 2001, UCSCCRL0101; COLLINS M, 2000, P 17 INT C MACH LEAR; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; FREUND Y, 1998, MACHINE LEARNING; Haussler D., 1999, CONVOLUTION KERNELS; JOHNSON M, IN PRESS COMPUTATION; JOHNSON M, 1999, P 37 ANN M ASS COMP; LODHI H, 2001, IN PRESS ADV NEURAL, V13; Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327; Watkins C, 2000, ADV NEUR IN, P39; [No title captured]	16	225	229	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						625	632						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100078
C	Chen, GB; Choi, WG; Yu, X; Han, T; Chandraker, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Guobin; Choi, Wongun; Yu, Xiang; Han, Tony; Chandraker, Manmohan			Learning Efficient Object Detection Models with Knowledge Distillation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Despite significant accuracy improvement in convolutional neural networks (CNN) based object detectors, they often require prohibitive runtimes to process an image for real-time applications. State-of-the-art models often use very deep networks with a large number of floating point operations. Efforts such as model compression learn compact models with fewer number of parameters, but with much reduced accuracy. In this work, we propose a new framework to learn compact and fast object detection networks with improved accuracy using knowledge distillation [20] and hint learning [34]. Although knowledge distillation has demonstrated excellent improvements for simpler classification setups, the complexity of detection poses new challenges in the form of regression, region proposals and less voluminous labels. We address this through several innovations such as a weighted cross-entropy loss to address class imbalance, a teacher bounded loss to handle the regression component and adaptation layers to better learn from intermediate teacher distributions. We conduct comprehensive empirical evaluation with different distillation configurations over multiple datasets including PASCAL, KITTI, ILSVRC and MS-COCO. Our results show consistent improvement in accuracy-speed trade-offs for modern multi-class detection models.	[Chen, Guobin; Choi, Wongun; Yu, Xiang; Chandraker, Manmohan] NEC Labs Amer, Princeton, NJ 08540 USA; [Chen, Guobin; Han, Tony] Univ Missouri, Columbia, MO 65211 USA; [Chandraker, Manmohan] Univ Calif San Diego, La Jolla, CA 92093 USA	NEC Corporation; University of Missouri System; University of Missouri Columbia; University of California System; University of California San Diego	Chen, GB (corresponding author), NEC Labs Amer, Princeton, NJ 08540 USA.		Chen, Guobin/AAN-1575-2021; Chandraker, Manmohan/AAU-4762-2021; Jeong, Yongwook/N-7413-2016					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ashraf K., 2016, CORR; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Chen X, 2015, CORR, V1504, P325; Cheng Y, 2015, IEEE I CONF COMP VIS, P2857, DOI 10.1109/ICCV.2015.327; Dai J, 2016, ADV NEURAL INF PROCE; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Dentinel Zarembaw, 2014, NEURIPS, P1269; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick R. B., 2012, DISCRIMINATIVELY TRA, V5; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gong Yunchao, 2014, ARXIV14126115; Gupta Saurabh, 2015, ARXIV150700448; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hinton G., 2015, ARXIV150302531; Hubara I, 2016, ADV NEUR IN, V29; Iandola F.N., 2016, ARXIV; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; JitendraMalik R. J. T., RICH FEATURE HIERARC; Kim K., 2016, CORR; Kim Yong-Deok, 2015, COMPRESSION DEEP CON; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lebedev V., 2015, 3 INT C LEARNING REP; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Romero Adriana, 2014, ARXIV14126550; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen J., 2016, ARXIV161200478; Su J.-C., 2016, ARXIV160400433; Xiang Y, 2015, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2015.7298800; Zhang Chiyuan, 2016, ARXIV161103530; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579; Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809	41	224	228	7	23	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400071
C	Belongie, S; Malik, J; Puzicha, J		Leen, TK; Dietterich, TG; Tresp, V		Belongie, S; Malik, J; Puzicha, J			Shape context: A new descriptor for shape matching and object recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We develop an approach to object recognition based on matching shapes and using a resulting measure of similarity in a nearest neighbor classifier. The key algorithmic problem here is that of finding pointwise correspondences between an image shape and a stored prototype shape. We introduce a new shape descriptor, the shape context, which makes this possible, using a simple and robust algorithm. The shape context at a point captures the distribution over relative positions of other shape points and thus summarizes global shape in a rich, local descriptor. We demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes. Once shapes are aligned, shape contexts are used to define a robust score for measuring shape similarity. We have used this score in a nearest-neighbor classifier for recognition of hand written digits as well as 3D objects, using exactly the same distance function. On the benchmark MNIST dataset of handwritten digits, this yields an error rate of 0.63%, outperforming other published techniques.	Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Belongie, S (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	sjb@cs.berkeley.edu; malik@cs.berkeley.edu; puzicha@cs.berkeley.edu						Amit Y, 1997, IEEE T PATTERN ANAL, V19, P1300, DOI 10.1109/34.632990; BELONGIE S, 2001, SHAPE MATCHING OBJEC; BICKEL PJ, 1969, ANN MATH STAT, V40, P1, DOI 10.1214/aoms/1177697800; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Burges CJC, 1997, ADV NEUR IN, V9, P375; Chui HL, 2000, PROC CVPR IEEE, P44, DOI 10.1109/CVPR.2000.854733; Huttenlocher DP, 1999, IEEE T PATTERN ANAL, V21, P951, DOI 10.1109/34.790437; Johnson AE, 1997, PROC CVPR IEEE, P684, DOI 10.1109/CVPR.1997.609400; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; Oren M, 1997, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.1997.609319; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Veltkamp R., 1999, UUCS199927; Wahba G., 1990, SPLINE MODELS OBSERV; [No title captured]	17	221	235	0	13	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						831	837						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800117
C	Kim, JH; Jun, J; Zhang, BT		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kim, Jin-Hwa; Jun, Jaehyun; Zhang, Byoung-Tak			Bilinear Attention Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.	[Kim, Jin-Hwa] SK T Brain, Seoul, South Korea; [Kim, Jin-Hwa; Jun, Jaehyun; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea; [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea	Seoul National University (SNU)	Kim, JH (corresponding author), SK T Brain, Seoul, South Korea.	jnhwkim@sktbrain.com; jhjun@bi.snu.ac.kr; btzhang@bi.snu.ac.kr		Kim, Jin-Hwa/0000-0002-0423-0415	College of Humanities, Seoul National University; Korea government [IITP-2017-0-01772-VTT, IITP-R0126-16-1072-SW.StarLab, 2018-0-00622-RMI, KEIT-10060086-RISF]	College of Humanities, Seoul National University; Korea government(Korean Government)	We would like to thank Kyoung-Woon On, Bohyung Han, Hyeonwoo Noh, Sungeun Hong, Jaesun Park, and Yongseok Choi for helpful comments and discussion. Jin-Hwa Kim was supported by 2017 Google Ph.D. Fellowship in Machine Learning and Ph.D. Completion Scholarship from College of Humanities, Seoul National University. This work was funded by the Korea government (IITP-2017-0-01772-VTT, IITP-R0126-16-1072-SW.StarLab, 2018-0-00622-RMI, KEIT-10060086-RISF). The part of computing resources used in this study was generously shared by Standigm Inc.	Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Bengio Y., 2014, ARXIV14061078; Fukui Akira, 2016, ARXIV160601847; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinami Ryota, 2017, ARXIV171109509; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Ilievski Ilija, 2017, SIMPLE LOSS FUNCTION; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kim Jin-Hwa, 2016, ADV NEURAL INFORM PR, P1; Kingma D.P, P 3 INT C LEARNING R; Krishna Ranjay, 2016, ARXIV160207332; Lim W., 2017, P INT C LEARN REPR; Lu JS, 2016, ADV NEUR IN, V29; Nair V, 2010, P 27 INT C MACHINE L, P807; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Pirsiavash H., 2009, P ADV NEUR INF PROC, P1482; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Teney D., 2017, ARXIV170802711; Trott A, 2018, INT C LEARN REPR; Veit A, 2016, ADV NEUR IN, V29; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Wang MZ, 2016, LECT NOTES COMPUT SC, V9912, P696, DOI 10.1007/978-3-319-46484-8_42; Wang P, 2017, PROC CVPR IEEE, P3909, DOI 10.1109/CVPR.2017.416; Wolf Lior, 2007, IEEE C COMP VIS PATT; Xu H., 2016, P EUR C COMP VIS ECC; Yeh RA, 2017, ADV NEUR IN, V30; Young P., 2014, P TACL, V2, P67, DOI 10.1162/tacl_a_00166; Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340; Zhang JM, 2016, LECT NOTES COMPUT SC, V9908, P543, DOI 10.1007/978-3-319-46493-0_33; Zhang Y., 2018, LEARNING COUNT OBJEC; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	42	220	230	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301054
C	Nguyen, A; Dosovitskiy, A; Yosinski, J; Brox, T; Clune, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Anh Nguyen; Dosovitskiy, Alexey; Yosinski, Jason; Brox, Thomas; Clune, Jeff			Synthesizing the preferred inputs for neurons in neural networks via deep generator networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right-similar to why we study the human brain-and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).				anguyen8@uwyo.edu; dosovits@cs.uni-freiburg.de; jason@geometric.ai; brox@cs.uni-freiburg.de; jeffclune@uwyo.edu		Yosinski, Jason/0000-0002-4701-0199	NSF CAREER award [CAREER: 1453549]; NASA Space Technology Research Fellowship; NSF [1527232]; ERC Starting Grant VideoLearn [279401]	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NASA Space Technology Research Fellowship; NSF(National Science Foundation (NSF)); ERC Starting Grant VideoLearn	The authors would like to thank Yoshua Bengio for helpful discussions and Bolei Zhou for providing images for our study. Jeff Clune was supported by an NSF CAREER award (CAREER: 1453549) and a hardware donation from the NVIDIA Corporation. Jason Yosinski was supported by the NASA Space Technology Research Fellowship and NSF grant 1527232. Alexey Dosovitskiy and Thomas Brox acknowledge funding by the ERC Starting Grant VideoLearn (279401).	Alain G, 2014, J MACH LEARN RES, V15, P3563; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Dosovitskiy Alexey, 2016, NEURIPS; Erhan D., 2009, 4323 U MONTR DEPT IR; Goodfellow I.J., 2015, DEEP LEARNING; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Mahendran A., 2016, COMPUTER VISION PATT; Mordvintsev A., GOOGLE RES BLOG; Nguyen A., 2016, VIS DEEP LEARN WORKS; Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Simonyan K., 2014, CLR WORKSH; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Theis Lucas, 2016, ICLR; Wei D., 2015, THE CNN, V6, P6; Yosinski J., 2015, ICML DEEP LEARN WORK; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou B., 2015, INT C LEARN REPR CLR; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	28	218	227	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704044
C	Heskes, T		Mozer, MC; Jordan, MI; Petsche, T		Heskes, T			Practical confidence and prediction intervals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We propose a new method to compute prediction intervals. Especially far small data sets the width of a prediction interval does not only depend on the variance of the target distribution, but also on the accuracy of our estimator of the mean of the target, i.e., on the width of the confidence interval. The confidence interval follows from the variation in an ensemble of neural networks, each of them trained and stopped on bootstrap replicates of the original data set. A second improvement is the use of the residuals on validation patterns instead of on training patterns for estimation of the variance of the target distribution. As illustrated on a synthetic example, our method is better than existing methods with regard to extrapolation and interpolation in data regimes with a limited amount of data, and yields prediction intervals which actual confidence levels are closer to the desired confidence levels.			Heskes, T (corresponding author), UNIV NIJMEGEN,REAL WORLD COMP PARTNERSHIP,NOVEL FUNCT SNN LAB,GEERT GROOTEPLEIN 21,NL-6525 EZ NIJMEGEN,NETHERLANDS.		Heskes, Tom/A-1443-2010	Heskes, Tom/0000-0002-3398-5235					0	218	218	2	11	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						176	182						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00025
C	Donoho, D; Stodden, V		Thrun, S; Saul, K; Scholkopf, B		Donoho, D; Stodden, V			When does non-negative matrix factorization give a correct decomposition into parts?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				MOLECULAR LINE SPECTRA; ATMOSPHERIC AEROSOL; ALASKA	We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. For such databases there is a generative model in terms of 'parts' and NMF correctly identifies the 'parts'. We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Donoho, D (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.							CRAIG MD, 1994, IEEE T GEOSCI REMOTE, V32, P542, DOI 10.1109/36.297973; JUVELA M, 1994, ASTR SOC P, V65, P176; Juvela M, 1996, MON NOT R ASTRON SOC, V280, P616; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Plumbley M, 2002, IEEE SIGNAL PROC LET, V9, P177, DOI 10.1109/LSP.2002.800502; Polissar AV, 1998, J GEOPHYS RES-ATMOS, V103, P19035, DOI 10.1029/98JD01365; Polissar AV, 1998, J GEOPHYS RES-ATMOS, V103, P19045, DOI 10.1029/98JD01212; Rockefellar R T, 1970, CONVEX ANAL; SIZE W, 1987, USE ABUSE STAT METHO, P33	9	217	223	0	11	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1141	1148						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500142
C	Schaal, S		Mozer, MC; Jordan, MI; Petsche, T		Schaal, S			Learning from demonstration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely attempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based reinforcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor.			Schaal, S (corresponding author), GEORGIA TECH RES INST,COLL COMP,801 ATLANTIC DR,ATLANTA,GA 30332, USA.								0	217	219	0	11	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1040	1046						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00146
C	Wang, YB; Long, MS; Wang, JM; Gao, ZF; Yu, PS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Yunbo; Long, Mingsheng; Wang, Jianmin; Gao, Zhifeng; Yu, Philip S.			PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical frames, where spatial appearances and temporal variations are two crucial structures. This paper models these structures by presenting a predictive recurrent neural network (PredRNN). This architecture is enlightened by the idea that spatiotemporal predictive learning should memorize both spatial appearances and temporal variations in a unified memory pool. Concretely, memory states are no longer constrained inside each LSTM unit. Instead, they are allowed to zigzag in two directions: across stacked RNN layers vertically and through all RNN states horizontally. The core of this network is a new Spatiotemporal LSTM (ST-LSTM) unit that extracts and memorizes spatial and temporal representations simultaneously. PredRNN achieves the state-of-the-art prediction performance on three video prediction datasets and is a more general framework, that can be easily extended to other predictive learning tasks by integrating with other architectures.	[Wang, Yunbo; Long, Mingsheng; Wang, Jianmin; Gao, Zhifeng; Yu, Philip S.] Tsinghua Univ, Sch Software, Beijing, Peoples R China	Tsinghua University	Long, MS (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.	wangyb15@mails.tsinghua.edu.cn; mingsheng@tsinghua.edu.cn; jimwang@tsinghua.edu.cn; gzf16@mails.tsinghua.edu.cn; psyu@uic.edu	wang, jian/GVS-0711-2022		National Key R&D Program of China [2016YFB1000701]; National Natural Science Foundation of China [61772299, 61325008, 61502265, 61672313]; TNList Fund	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); TNList Fund	This work was supported by the National Key R&D Program of China (2016YFB1000701), National Natural Science Foundation of China (61772299, 61325008, 61502265, 61672313) and TNList Fund.	Abadi M, 2015, P 12 USENIX S OPERAT; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; De Brabandere B, 2016, ADV NEUR IN, V29; Denton Emily L, 2015, NEURIPS, V2, P4; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kalchbrenner Nal, 2017, ICML, P2; Kingma D.P, P 3 INT C LEARNING R; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Mathieu Michael, 2016, ICLR; Ranzato MarcAurelio, 2014, ARXIV14126604; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; van den Oord Aaron, 2016, ARXIV160605328; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Viorica P .atr., 2016, ICLR WORKSH; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6	28	215	229	9	38	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400084
C	Han, B; Yao, JC; Niu, G; Zhou, MY; Tsang, I; Zhang, Y; Sugiyama, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Han, Bo; Yao, Jiangchao; Niu, Gang; Zhou, Mingyuan; Tsang, Ivorw.; Zhang, Ya; Sugiyama, Masashi			Masking: A New Perspective of Noisy Supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					It is important to learn various types of classifiers given training data with noisy labels. Noisy labels, in the most popular noise model hitherto, are corrupted from ground-truth labels by an unknown noise transition matrix. Thus, by estimating this matrix, classifiers can escape from overfitting those noisy labels. However, such estimation is practically difficult, due to either the indirect nature of two-step approaches, or not big enough data to afford end-to-end approaches. In this paper, we propose a human-assisted approach called "Masking" that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. To this end, we derive a structure-aware probabilistic model incorporating a structure prior, and solve the challenges from structure extraction and structure alignment. Thanks to Masking, we only estimate unmasked noise transition probabilities and the burden of estimation is tremendously reduced. We conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise structures as well as the industrial-level Clothing1M with agnostic noise structure, and the results show that Masking can improve the robustness of classifiers significantly.	[Han, Bo; Yao, Jiangchao; Tsang, Ivorw.] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia; [Han, Bo; Niu, Gang; Sugiyama, Masashi] RIKEN, Ctr Adv Intelligence Project, Tokyo, Japan; [Yao, Jiangchao; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA; [Sugiyama, Masashi] Univ Tokyo, Grad Sch Frontier Sci, Tokyo, Japan	University of Technology Sydney; RIKEN; Shanghai Jiao Tong University; University of Texas System; University of Texas Austin; University of Tokyo	Han, B; Yao, JC (corresponding author), Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.; Han, B (corresponding author), RIKEN, Ctr Adv Intelligence Project, Tokyo, Japan.; Yao, JC (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China.		Zhang, Ya/Y-8255-2019; Sugiyama, Masashi/AEO-1176-2022; Zhou, Mingyuan/AAE-8717-2021	Zhang, Ya/0000-0002-5390-9053; Sugiyama, Masashi/0000-0001-6658-6743; 	International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study; ARC [FT130100746, DP180100106, LP150100671]; U.S. National Science Foundation [IIS-1812699]; High Technology Research and Development Program of China [2015AA015801]; NSFC [61521062]; STCSM [18DZ2270700]; RIKEN-AIP; SJTU-CMIC; UTS-CAI	International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study; ARC(Australian Research Council); U.S. National Science Foundation(National Science Foundation (NSF)); High Technology Research and Development Program of China(National High Technology Research and Development Program of China); NSFC(National Natural Science Foundation of China (NSFC)); STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); RIKEN-AIP; SJTU-CMIC; UTS-CAI	MS was supported by the International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study. IWT was supported by ARC FT130100746, DP180100106 and LP150100671. MZ acknowledges the support of Award IIS-1812699 from the U.S. National Science Foundation. YZ was supported by the High Technology Research and Development Program of China (2015AA015801), NSFC (61521062), and STCSM (18DZ2270700). BH would like to thank the financial support from RIKEN-AIP. JY would like to thank the financial support SJTU-CMIC and UTS-CAI. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.	Ait-Sahalia Y, 2010, J AM STAT ASSOC, V105, P1504, DOI 10.1198/jasa.2010.tm10163; Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Arpit D., 2017, ICML; Azadi S., 2016, ICLR; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Cha Y., 2012, SIGIR; Deng J., 2013, CVPR; Dgani  Y., 2018, ISBI; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Fergus R, 2015, ICLR WORKSH; Goldberger J., 2017, ICLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodman N., 1952, PHILOS REV, V61, P160; Grigolini P, 2009, PHYSICA A, V388, P4192, DOI 10.1016/j.physa.2009.06.024; Gulrajani Ishaan, 2017, NIPS; Han  B., 2016, ECML PKDD; Hendrycks Dan, 2018, NIPS; Huang J., 2007, NIPS; Jiang L., 2018, ICML; Laine Samuli, 2017, PROC INT C LEARN REP; Li Wen, 2017, ARXIV170802862; Li Y., 2017, ICCV; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Liu  W., 2011, CVPR; Ma X., 2018, ICML; Malach E., 2017, NIPS; Masnadi-Shirazi H., 2009, NIPS; Menon A., 2015, ICML; Michalski R.S., 2013, MACHINE LEARNING ART; Miyato T., 2016, ICLR; Natarajan N., 2013, NIPS; Patrini G., 2017, CVPR; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Reed S., 2015, ICLR; Ren M., 2018, ICML; Rockova  V., 2017, ARXIV170800085; Rodrigues Filipe, 2018, AAAI; Scott C, 2014, AISTATS; Tanaka D., 2018, CVPR; Tarvainen A., 2017, NIPS; Tran  D., 2017, NIPS; Veit A., 2017, CVPR; Wang Y., 2018, CVPR; Welinder P., 2010, NIPS; Xiao T., 2015, CVPR; Yan Y, 2014, MACH LEARN, V95, P291, DOI 10.1007/s10994-013-5412-1; Zhang C., 2017, ICLR; Zhang Z., 2018, NIPS	49	210	210	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000035
C	Fout, A; Byrd, J; Shariat, B; Ben-Hur, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fout, Alex; Byrd, Jonathon; Shariat, Basir; Ben-Hur, Asa			Protein Interface Prediction using Graph Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task.	[Fout, Alex; Byrd, Jonathon; Shariat, Basir; Ben-Hur, Asa] Colorado State Univ, Dept Comp Sci, Ft Collins, CO 80525 USA	Colorado State University	Fout, A (corresponding author), Colorado State Univ, Dept Comp Sci, Ft Collins, CO 80525 USA.	fout@colostate.edu; jonbyrd@colostate.edu; basir@cs.colostate.edu; asa@cs.colostate.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [DBI-1564840]	National Science Foundation(National Science Foundation (NSF))	This work was supported by the National Science Foundation under grant no DBI-1564840.	Abadi M, 2015, P 12 USENIX S OPERAT; Ahmad S, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0029104; Angermueller C, 2016, MOL SYST BIOL, V12, DOI 10.15252/msb.20156651; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Aumentado-Armstrong TT, 2015, ALGORITHM MOL BIOL, V10, DOI 10.1186/s13015-015-0033-9; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Bloem P., 2018, P 15 EUR SEM WEB C E, P593, DOI [10.1007/978-3-319-93417-4_38, DOI 10.1007/978-3-319-93417-4_38]; Bronstein Michael M, 2017, IEEE SIG P MAGAZINE; Duvenaud David K, 2015, P NIPS; Esmaielbeiki R, 2015, BRIEF BIOINFORM, V17, P1; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Minhas FUA, 2014, PROTEINS, V82, P1142, DOI 10.1002/prot.24479; Niepert M, 2016, PR MACH LEARN RES, V48; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Sael L, 2008, PROTEINS, V72, P1259, DOI 10.1002/prot.22030; Sael L, 2010, CH CRC DATA MIN KNOW, P89; Schrodinger L., 2015, PYMOL MOL GRAPHICS S, DOI DOI 10.1007/S13398-014-0173-7.2; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Springenberg J.T., 2014, ARXIV14126806; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vreven T, 2015, J MOL BIOL, V427, P3031, DOI 10.1016/j.jmb.2015.07.016	25	210	216	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406058
C	Shi, XJ; Gao, ZH; Lausen, L; Wang, H; Yeung, DY; Wong, WK; Woo, WC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Shi, Xingjian; Gao, Zhihan; Lausen, Leonard; Wang, Hao; Yeung, Dit-Yan; Wong, Wai-kin; Woo, Wang-chun			Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.	[Shi, Xingjian; Gao, Zhihan; Lausen, Leonard; Wang, Hao; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Wong, Wai-kin; Woo, Wang-chun] Hong Kong Observ, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Shi, XJ (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	xshiab@cse.ust.hk; zgaoag@cse.ust.hk; lelausen@cse.ust.hk; hwangaz@cse.ust.hk; dyyeung@cse.ust.hk; wkwong@hko.gov.hk; wcwoo@hko.gov.hk	Jeong, Yongwook/N-7413-2016		General Research Fund from the Research Grants Council [16207316]; Innovation and Technology Fund from Innovation and Technology Commission in Hong Kong [ITS/205/15FP]; Hong Kong PhD Fellowship	General Research Fund from the Research Grants Council; Innovation and Technology Fund from Innovation and Technology Commission in Hong Kong; Hong Kong PhD Fellowship	This research has been supported by General Research Fund 16207316 from the Research Grants Council and Innovation and Technology Fund ITS/205/15FP from the Innovation and Technology Commission in Hong Kong. The first author has also been supported by the Hong Kong PhD Fellowship.	Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Ballas Nicolas, 2016, ICLR; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; De Brabandere B, 2016, ADV NEUR IN, V29; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hogan RJ, 2010, WEATHER FORECAST, V25, P710, DOI 10.1175/2009WAF2222350.1; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; LEE H, 2017, ATMOSPHERE-BASEL, V8, DOI DOI 10.3390/ATMOS8010011; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; MARSHALL JS, 1948, J METEOROL, V5, P165, DOI 10.1175/1520-0469(1948)005<0165:TDORWS>2.0.CO;2; Mathieu Michael, 2016, ICLR; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Ranzato MarcAurelio, 2014, ARXIV14126604; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sun JZ, 2014, B AM METEOROL SOC, V95, P409, DOI 10.1175/BAMS-D-11-00263.1; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Woo WC, 2017, ATMOSPHERE-BASEL, V8, DOI 10.3390/atmos8030048; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Yu F., 2016, P ICLR 2016	30	209	221	10	58	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405068
C	Hardt, M; Price, E; Srebro, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hardt, Moritz; Price, Eric; Srebro, Nathan			Equality of Opportunity in Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.	[Hardt, Moritz] Google, Mountain View, CA 94043 USA; [Price, Eric] UT Austin, Austin, TX USA; [Srebro, Nathan] TTI Chicago, Chicago, IL USA; [Price, Eric] OpenAI, San Francisco, CA USA	Google Incorporated; University of Texas System; University of Texas Austin	Hardt, M (corresponding author), Google, Mountain View, CA 94043 USA.	m@mrtz.org; ecprice@cs.utexas.edu; nati@ttic.edu						[Anonymous], 2016, BIG DAT REP ALG SYST; Barocas S., 2016, CALIFORNIA LAW REV, V104; Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Kleinberg Jon, 2016, P INN THEOR COMP SCI; Pedreshi Dino, 2008, P 14 ACM SIGKDD; Podesta J., 2014, BIG DATA SEIZING OPP; US Federal Reserve, 2007, C CRED SCOR ITS EFF; Valera Isabel, 2015, ABS150705259 CORR; Wasserman L, 2010, ALL STAT CONCISE COU; Zliobaite Indre, 2015, ABS150505723 CORR	11	206	206	3	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703010
C	Ma, LQ; Jia, X; Sun, QR; Schiele, B; Tuytelaars, T; Van Gool, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ma, Liqian; Jia, Xu; Sun, Qianru; Schiele, Bernt; Tuytelaars, Tinne; Van Gool, Luc			Pose Guided Person Image Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper proposes the novel Pose Guided Person Generation Network (PG(2)) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG(2) utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128x64 re-identification images and 256x256 fashion photos show that our model generates high-quality person images with convincing details.	[Ma, Liqian; Van Gool, Luc] Katholieke Univ Leuven, PSI, TRACE, Toyota Res Europe, Leuven, Belgium; [Jia, Xu; Tuytelaars, Tinne] Katholieke Univ Leuven, PSI, IMEC, Leuven, Belgium; [Sun, Qianru; Schiele, Bernt] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany; [Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland	KU Leuven; IMEC; KU Leuven; Max Planck Society; Swiss Federal Institutes of Technology Domain; ETH Zurich	Ma, LQ (corresponding author), Katholieke Univ Leuven, PSI, TRACE, Toyota Res Europe, Leuven, Belgium.	liqian.ma@esat.kuleuven.be; xu.jia@esat.kuleuven.be; qsun@mpi-inf.mpg.de; schiele@mpi-inf.mpg.de; tuytrlaars@esat.kuleuven.be; luc.vangool@esat.kuleuven.be	Tuytelaars, Tinne/B-4319-2015; Jeong, Yongwook/N-7413-2016	Tuytelaars, Tinne/0000-0003-3307-9723; 	Toyota Motors Europe; FWO Structure from Semantics project; KU Leuven GOA project CAMETRON; German Research Foundation (DFG) [CRC 1223]	Toyota Motors Europe; FWO Structure from Semantics project; KU Leuven GOA project CAMETRON; German Research Foundation (DFG)(German Research Foundation (DFG))	We gratefully acknowledge the support of Toyota Motors Europe, FWO Structure from Semantics project, KU Leuven GOA project CAMETRON, and German Research Foundation (DFG CRC 1223). We would like to thank Bo Zhao for his helpful discussions.	Arjovsky M., 2017, ARXIV170107875; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Chen CY, 2014, PROC CVPR IEEE, P2011, DOI 10.1109/CVPR.2014.258; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Mathieu Michael, 2016, ICLR; Mirza M., 2014, ARXIV; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Quan Tran Minh, 2016, 161205360 ARXIV; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2016, PR MACH LEARN RES, V48; Reed Scott, 2016, TECHNICAL REPORT; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rui Huang, 2017, 170404086 ARXIV; Salimans T, 2016, ADV NEUR IN, V29; Shi Wenzhe, REAL TIME SINGLE IMA; Uria Benigno, 2016, 160502226 ARXIV; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu Jia, 2016, BMVC; Xun Huang, 2016, ARXIV161204357; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yang Jimei, 2015, NIPS; Yim J, 2015, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2015.7298667; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhao Bo, 2017, 170404886 ARXIV; Zhe Cao, 2016, 161108050 ARXIV; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133	37	204	213	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400039
C	Defazio, A; Bach, F; Lacoste-Julien, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Defazio, Aaron; Bach, Francis; Lacoste-Julien, Simon			SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.	[Defazio, Aaron] Australian Natl Univ, Ambiata, Canberra, ACT, Australia; [Bach, Francis; Lacoste-Julien, Simon] Ecole Normale Super, Sierra Project Team, INRIA, Paris, France	Australian National University; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Defazio, A (corresponding author), Australian Natl Univ, Ambiata, Canberra, ACT, Australia.				MSR-Inria Joint Centre; European Research Council [239993]	MSR-Inria Joint Centre; European Research Council(European Research Council (ERC)European Commission)	The first author completed this work while under funding from NICTA. This work was partially supported by the MSR-Inria Joint Centre and a grant by the European Research Council (SIERRA project 239993).	Combettes P. L., 2011, PROXIMAL SPLITTING M; Defazio A., 2014, THESIS AUSTR NATL U; Defazio A., 2014, P 31 INT C MACH LEAR; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; JOHNSON R., 2013, NIPS; Konecny J., 2013, ARXIV PREPRINT ARXIV; Mairal Julien, 2014, TECHNICAL REPORT; Nesterov, 1998, INTRO LECT CONVEX PR; Schmidt M., 2013, HAL0086005 INRIA; Shalev- Shwartz Shai, 2013, TECHNICAL REPORT; Suzuki Taiji, 2014, P 31 INT C MACH LEAR; Tseng P, 2014, J OPTIMIZ THEORY APP, V160, P832, DOI 10.1007/s10957-013-0409-2; Xiao Lin, 2014, TECHNICAL REPORT	14	204	204	5	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100037
C	Oliver, A; Odena, A; Raffel, C; Cubuk, ED; Goodfellow, IJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Oliver, Avital; Odena, Augustus; Raffel, Colin; Cubuk, Ekin D.; Goodfellow, Ian J.			Realistic Evaluation of Deep Semi-Supervised Learning Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that SSL algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often under reported, SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and performance can degrade substantially when the unlabeled dataset contains out-of-distribution examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.	[Oliver, Avital; Odena, Augustus; Raffel, Colin; Cubuk, Ekin D.; Goodfellow, Ian J.] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Oliver, A (corresponding author), Google Brain, Mountain View, CA 94043 USA.	avitalo@google.com; augustusodena@google.com; craffel@google.com; cubuk@google.com; goodfellow@google.com						Bachman Philip, 2014, ADV NEURAL INFORM PR, P2; Belkin M, 2002, ADV NEUR IN, V14, P585; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bengio Yoshua, 2006, LABEL PROPAGATION QU; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Chrabaszcz P., 2017, ARXIV PREPRINT ARXIV; Coates Adam, 2011, P 28 INT C MACH LEAR, P921; Dai Andrew M., 2015, ADV NEURAL INFORM PR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J, 2014, PR MACH LEARN RES, V32; Fedus William, 2018, INT C LEARN REPR; Forster Dennis, NEURAL COMPUTATION, P1; Gammerman A., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P148; Ganin Y., 2016, JMLR, V17, P2096; Gastaldi Xavier, 2017, ARXIV170507485; Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow Ian J., 2011, P NIPS WORKSH CHALL; Grandvalet Y., 2005, CAP, P529; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henderson P, 2018, AAAI CONF ARTIF INTE, P3207; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Joachims T., 2003, P 20 INT C MACH LEAR, P290, DOI DOI 10.1145/2612669.2612699; Ke Rosemary Nan, 2017, REPRODUCIBILITY MACH; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Laine Samuli, 2017, P INT C LEARN REPR I, P3; Lasserre J.A., 2006, IEEE COMP SOC C COMP, P87, DOI DOI 10.1109/CVPR.2006.227; Lee D., 2013, INT C MACH LEARN ICM; Lucic Mario, 2017, ARXIV171110337; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; MCLACHLAN GJ, 1975, J AM STAT ASSOC, V70, P365; Melis Gabor, 2018, ICLR; Miyato T., 2017, ABS170403976 CORR; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Odena A., 2016, SEMISUPERVISED LEARN; Pereyra Gabriel, 2017, ARXIV170106548; Rosenberg C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P29; Sajjadi M, 2016, IEEE IMAGE PROC, P1908, DOI 10.1109/ICIP.2016.7532690; Sajjadi Mehdi, 2016, NEURIPS; Salakhutdinov R., 2007, NIPS, V7, P1249; Salimans T, 2016, ADV NEUR IN, V29; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy Christian, 2014, P 2 INT C LEARNING R; Tarvainen Antti, 2017, CORR, Vabs/1703; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhu X., 2003, INT C MACH LEARN	53	202	205	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303025
C	Newell, A; Huang, Z; Deng, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Newell, Alejandro; Huang, Zhiao; Deng, Jia			Associative Embedding: End-to-End Learning for Joint Detection and Grouping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets.	[Newell, Alejandro; Deng, Jia] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA; [Huang, Zhiao] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China	University of Michigan System; University of Michigan; Tsinghua University	Newell, A (corresponding author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.	alnewell@umich.edu; hza14@mails.tsinghua.edu.cn; jiadeng@umich.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [1734266]; Institute for Interdisciplinary Information Sciences, Tsinghua University	National Science Foundation(National Science Foundation (NSF)); Institute for Interdisciplinary Information Sciences, Tsinghua University	This work is partially supported by the National Science Foundation under Grant No. 1734266. ZH is partially supported by the Institute for Interdisciplinary Information Sciences, Tsinghua University.	Abadi M, 2015, P 12 USENIX S OPERAT; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64; Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44; Cao Zhe, 2017, COMP VIS PATT REC CV; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601; Fan XC, 2015, PROC CVPR IEEE, P1347, DOI 10.1109/CVPR.2015.7298740; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Frome A, 2007, IEEE I CONF COMP VIS, P94; Frome Andrea, 2013, NEURIPS; Gkioxari G, 2016, LECT NOTES COMPUT SC, V9908, P728, DOI 10.1007/978-3-319-46493-0_44; Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35; Han F, 2009, IEEE T PATTERN ANAL, V31, P59, DOI 10.1109/TPAMI.2008.55; Harley Adam W, 2016, INT C LEARN REPR WOR; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Hu PY, 2016, PROC CVPR IEEE, P5600, DOI 10.1109/CVPR.2016.604; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Insafutdinov Eldar, 2016, ARXIV161201465; Iqbal Umar, 2016, ARXIV160808526; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Krahenbuhl P., 2011, P NIPS, P1; Levinkov E, 2017, PROC CVPR IEEE, P1904, DOI 10.1109/CVPR.2017.206; Lifshitz I, 2016, LECT NOTES COMPUT SC, V9906, P246, DOI 10.1007/978-3-319-46475-6_16; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maire M, 2011, IEEE I CONF COMP VIS, P2142, DOI 10.1109/ICCV.2011.6126490; Maire M, 2010, LECT NOTES COMPUT SC, V6312, P450, DOI 10.1007/978-3-642-15552-9_33; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ning Guanghan, 2017, ARXIV170502407; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Yu SX, 2009, PROC CVPR IEEE, P2302, DOI 10.1109/CVPRW.2009.5206673	42	200	215	5	28	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402032
C	Chen, YP; Kalantidis, Y; Li, JS; Yan, SC; Feng, JS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Yunpeng; Kalantidis, Yannis; Li, Jianshu; Yan, Shuicheng; Feng, Jiashi			A(2)-Nets: Double Attention Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work, we propose the "double attention block", a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos, enabling subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps, where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task, a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number of parameters and less FLOPs. On the action recognition task, our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works.	[Chen, Yunpeng; Li, Jianshu; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore; [Kalantidis, Yannis] Facebook Res, Singapore, Singapore; [Yan, Shuicheng] Natl Univ Singapore, Qihoo 360 AI Inst, Singapore, Singapore	National University of Singapore; Facebook Inc; National University of Singapore	Chen, YP (corresponding author), Natl Univ Singapore, Singapore, Singapore.	chenyunpeng@u.nus.edu; yannisk@fb.com; jianshu@u.nus.edu; eleyans@nus.edu.sg; elefjia@nus.edu.sg	Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022	Chen, Yunpeng/0000-0002-9830-8980				Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YB, 2018, LECT NOTES COMPUT SC, V11205, P275, DOI 10.1007/978-3-030-01246-5_17; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Girdhar R, 2017, ADV NEUR IN, V30; Girshick R., 2015, ICCV; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kay W., 2017, ARXIV PREPRINT ARXIV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Ma Ningning, 2018, P EUR C COMP VIS ECC; Paszke Adam, 2017, PYTORCH TENSORS DYNA, P6; Sandler Mark, 2018, ARXIV180104381, DOI DOI 10.1109/CVPR.2018.00474; Soomro K., 2012, ARXIV; Sun G., 2018, P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745; Tran  Du, 2018, 2018 IEEE C COMP VIS; Tran Du, 2017, ARXIV170805038; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang X, 2018, COMPANION OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'18), DOI 10.1145/3180308.3180320; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634	26	199	201	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300033
C	Yang, HH; Moody, J		Solla, SA; Leen, TK; Muller, KR		Yang, HH; Moody, J			Data visualization and feature selection: New algorithms for nongaussian data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO			feature selection; joint mutual information; ICA; visualization; classification	MUTUAL INFORMATION; BLIND SEPARATION	Data visualization and feature selection methods are proposed based on the joint mutual information and ICA. The visualization methods can find many good 2-D projections for high dimensional data interpretation, which cannot be easily found by the other existing methods. The new variable selection method is found to be better in eliminating redundancy in the inputs than other methods based on simple mutual information. The efficacy of the methods is illustrated on a radar signal analysis problem to find 2-D viewing coordinates for data visualization and to select inputs for a neural network classifier.	Oregon Grad Inst Sci & Technol, Beaverton, OR 97006 USA		Yang, HH (corresponding author), Oregon Grad Inst Sci & Technol, 20000 NW,Walker Rd, Beaverton, OR 97006 USA.							Amari S, 1996, ADV NEUR IN, V8, P757; BARROWS G, 1996, IEEE INT S TIM FREQ, P249; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; BONNLANDER BV, 1996, THESIS U COLORADO; JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X; MOODY J, 1994, NATO ASI SERIES F; PI H, 1994, NEURAL COMPUT, V6, P509, DOI 10.1162/neco.1994.6.3.509; Yang HH, 1997, NEURAL COMPUT, V9, P1457, DOI 10.1162/neco.1997.9.7.1457	8	199	208	1	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						687	693						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700098
C	Smyth, P		Mozer, MC; Jordan, MI; Petsche, T		Smyth, P			Clustering sequences with hidden Markov models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper discusses a probabilistic model-based approach to clustering sequences, using hidden Markov models (HMMs). The problem can be framed as a generalization of the standard mixture model approach to clustering in feature space. Two primary issues are addressed. First, a novel parameter initialization procedure is proposed, and second, the more difficult problem of determining the number of clusters K, from the data, is investigated. Experimental results indicate that the proposed techniques are useful for revealing hidden cluster structure in data sets of sequences.			Smyth, P (corresponding author), UNIV CALIF IRVINE,IRVINE,CA 92697, USA.			Smyth, Padhraic/0000-0001-9971-8378					0	199	203	1	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						648	654						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00092
C	Parr, R; Russell, S		Jordan, MI; Kearns, MJ; Solla, SA		Parr, R; Russell, S			Reinforcement learning with hierarchies of machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and "behavior-based" or "teleo-reactive" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.	Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Parr, R (corresponding author), Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.								0	197	202	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1043	1049						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700147
C	Cao, KD; Wei, CL; Gaidon, A; Arechiga, N; Ma, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cao, Kaidi; Wei, Colin; Gaidon, Adrien; Arechiga, Nikos; Ma, Tengyu			Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains(1).	[Cao, Kaidi; Wei, Colin; Ma, Tengyu] Stanford Univ, Stanford, CA 94305 USA; [Gaidon, Adrien; Arechiga, Nikos] Toyota Res Inst, Ann Arbor, MI USA	Stanford University; Toyota Motor Corporation	Cao, KD (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	kaidicao@stanford.edu; colinwei@stanford.edu; adrien.gaidon@tri.global; nikos.arechiga@tri.global; tengyuma@stanford.edu			Toyota Research Institute ("TRI")	Toyota Research Institute ("TRI")	Toyota Research Institute ("TRI") provided funds and computational resources to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. We thank Percy Liang and Michael Xie for helpful discussions in various stages of this work.	[Anonymous], 2017, ARXIV171206541; Arora S, 2018, PR MACH LEARN RES, V80; Azizzadenesheli K., 2019, INT C LEARN REPR; Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011; Byrd J, 2019, PR MACH LEARN RES, V97; Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544; Carmon Yair, 2019, ARXIV190702056; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Cui Yin, 2019, IEEE C COMP VIS PATT; Duchi John C, DISTRIBUTIONALLY ROB; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Goyal Priya, 2017, ARXIV170602677; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Hashimoto TB, 2018, PR MACH LEARN RES, V80; He H, 2013, IMBALANCED LEARNING: FOUNDATIONS, ALGORITHMS, AND APPLICATIONS, P1, DOI 10.1002/9781118646106; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinnefeld J.H., 2018, ARXIV180909245; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Huang Chen, 2019, IEEE T PATTERN ANAL; Japkowicz N., 2002, Intelligent Data Analysis, V6, P429; Khan S, 2019, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2019.00019; Kingma D.P, P 3 INT C LEARNING R; Koltchinskii V, 2002, ANN STAT, V30, P1; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li BY, 2019, AAAI CONF ARTIF INTE, P8577; Li Y., 2002, PROC ICML, V2, P379; Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lipton Zachary, 2018, ARXIV180203916, P3122; Liu J, 2016, IEEE SYS MAN CYBERN, P1753, DOI 10.1109/SMC.2016.7844491; LIU W, 2017, ADV SOC SCI EDUC HUM, V99, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Liu Yu, 2017, ARXIV171000870; Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264; Ma T, 2019, ADV NEURAL INFORM PR, P9725; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Merler Michele, 2019, ARXIV190110436; Nagarajan V., 2019, ARXIV190513344, p2019a; Paszke A., 2017, AUTOMATIC DIFFERENTI; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen Li, 2016, ECCV; Shu J, 2019, ADV NEUR IN, V32; Soudry D, 2018, J MACH LEARN RES, V19; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Thomee B., 2015, ARXIV150301817; Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914; Van Horn Grant, 2017, ARXIV170901450; WANG LT, 2017, ADV NEURAL INFORM PR, P290; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Wei C., 2020, PROC INT C LEARN REP; Wei Colin, 2018, ARXIV181005369; Yan ZY, 2019, IEEE INT CONF MULTI, P402, DOI 10.1109/ICMEW.2019.00075; Zhong Q, 2016, CVPR WORKSH; Zou Yang, 2018, ARXIV181007911	61	196	201	4	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301054
C	Rasmussen, CE; Ghahramani, Z		Dietterich, TG; Becker, S; Ghahramani, Z		Rasmussen, CE; Ghahramani, Z			Infinite mixtures of Gaussian process experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an infinite number of Experts. Inference in this model may be done efficiently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets - thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Rasmussen, CE (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.			Rasmussen, Carl Edward/0000-0001-8899-7850				Duane S, 1987, PHYS LETT B, V55, P2774; GIBBS MN, 1997, THESIS U CAMBRIDGE; GILKS WR, 1992, APPL STAT, V41, P337, DOI DOI 10.2307/2347565; GOLDBERG PW, 1998, NIPS, V10; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; NEAL RM, 1998, 4915 U TOR DEP STAT; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; SMOLA AJ, 2001, NIPS, V13; TRESP V, 2001, NIPS, V13; WILLIAMS CKI, 1996, NIPS, V8; WILLIAMS CKI, 2001, NIPS, V13	12	196	200	1	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						881	888						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100110
C	Gal, Y; Ghahramani, Z		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gal, Yarin; Ghahramani, Zoubin			A Theoretically Grounded Application of Dropout in Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.	[Gal, Yarin; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England	University of Cambridge	Gal, Y (corresponding author), Univ Cambridge, Cambridge, England.	yg279@cam.ac.uk; zg201@cam.ac.uk						[Anonymous], 2015, KERAS; [Anonymous], 2010, PYTH SCI COMP C; [Anonymous], 1993, P 6 ANN C COMPUTATIO, DOI DOI 10.1145/168304.168306; [Anonymous], 2005, ACL; Ba J., 2017, P 3 INT C LEARN REPR; Balan A. Korattikara, 2015, NIPS; Barber D., 1998, Neural Networks and Machine Learning. Proceedings, P215; Bayer J., 2013, ARXIV13110701; Bluche T., 2015, ICDAR; Blundell C., 2015, ICML; Cho Kyunghyun, 2014, EMNLP; Gal Y., 2015, ARXIV; Gal Y., 2015, ARXIV150602142; Graves A., 2013, ICASSP; Graves Alex, 2011, NIPS; Gunther J, 2015, IEEE WORK APPL SIG; Hernandez-Lobato J. M., 2015, ICML; Hinton G.E., 2012, COMPUT SCI, V3, P212, DOI DOI 10.9774/GLEAF.978-1-909493-38-42; Hochreiter S., 1997, NEURAL COMPUTATION, V9; Kalchbrenner N., 2013, EMNLP; Kingma D. P., 2015, NIPS; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Neal R. M., 2012, BAYESIAN LEARNING NE; Pachitariu M., 2013, ARXIV13015650; Pham V., 2014, ICFHR; Rezende D.J., 2014, ICML; Srivastava N., 2014, JMLR; Sundermeyer M., 2012, INTERSPEECH; Sutskever I., 2014, NEURIPS; Sutskever I., 2014, ARXIV	30	194	198	0	14	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701098
C	Wang, A; Pruksachatkun, Y; Nangia, N; Singh, A; Michael, J; Hill, F; Levy, O; Bowman, SR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Alex; Pruksachatkun, Yada; Nangia, Nikita; Singh, Amanpreet; Michael, Julian; Hill, Felix; Levy, Omer; Bowman, Samuel R.			SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.	[Wang, Alex; Pruksachatkun, Yada; Nangia, Nikita; Bowman, Samuel R.] NYU, New York, NY 10003 USA; [Singh, Amanpreet; Levy, Omer] Facebook AI Res, New York, NY USA; [Michael, Julian] Univ Washington, Seattle, WA 98195 USA; [Hill, Felix] DeepMind, London, England	New York University; Facebook Inc; University of Washington; University of Washington Seattle	Pruksachatkun, Y (corresponding author), NYU, New York, NY 10003 USA.	glue-benchmark-admin@googlegroups.com			NVIDIA Corporation; DeepMind; National Science Foundation [DGE 1342536]; Samsung Advanced Institute of Technology; Samsung Electronics	NVIDIA Corporation; DeepMind; National Science Foundation(National Science Foundation (NSF)); Samsung Advanced Institute of Technology(Samsung); Samsung Electronics(Samsung)	We thank the original authors of the included datasets in SuperGLUE for their cooperation in the creation of the benchmark, as well as those who proposed tasks and datasets that we ultimately could not include. This work was made possible in part by a donation to NYU from Eric and Wendy Schmidt made by recommendation of the Schmidt Futures program. We gratefully acknowledge the support of the NVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research, and funding from DeepMind for the hosting of the benchmark platform. AW is supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1342536. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. SB is partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from Pattern Recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure).	Bach Stephen H., 2018, SIGMOD; Bentivogli Luisa, 2009, TEXT AN C TAC; Buechel S., 2018, P 2018 C EMP METH NA; Callison-Burch Chris, 2006, P C EUR CHAPT ASS CO; Cer Daniel, 2017, ARXIV170800055; Choi E., 2018, EMPIRICAL METHODS NA; Choi Eunsol, 2018, P ASS COMP LING ACL; Clark Kevin, 2019, P ASS COMP LING ACL; Collobert R., 2008, MACH LEARN P 25 INT; Conneau A., 2017, P 2017 C EMP METH NA, P670; Conneau Alexis, 2018, P 11 LANG RES EV C; Dagan I, 2006, LECT NOTES ARTIF INT, V3944, P177; Dai A.M., 2015, ADV NEURAL INFORM PR; De Marneffe Marie-Catherine, 2019, P SINN BEDEUTUNG; Devlin Jacob, 2019, INNAACL HL T I; Dolan William B., 2005, P IWP; Faruqui Manaal, 2018, P C EMP METH NAT LAN; Furlanello Tommaso, 2018, INT C MACH LEARN ICM; Gardner Matt, 2017, P WORKSH NLP OP SOUR; GIAMPICCOLO D, 2007, P ACL PASCAL WORKSH; Gonen Hila, 2019, ARXIV190303862; Haim R.B., 2006, P 2 PASCAL CHALL WOR; Hill Felix, 2016, P 2016 C N AM CHAPT, P1367, DOI DOI 10.18653/V1/N16-1162; Hinton G., 2015, 150302531 ARXIV; Jia Robin, 2017, P 2017 C EMP METH NA, P2021, DOI DOI 10.18653/V1/D17-1215; Khashabi Daniel, 2018, P C N AM CHAPT ASS C; Kingma D.P, P 3 INT C LEARNING R; Kiros Ryan, 2015, P NIPS; Kitaev N, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3499; Kocijan V, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4837; Levesque H., 2012, P KR ROM IT; Liu C, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON MEDICINE AND BIOPHARMACEUTICALS, P1226; Liu NF, 2019, P C N AM CHAPT ASS C; Liu X., 2019, ARXIV190111504; Liu Xiaodong, 2019, ARXIV190409482; Lu K., 2018, ARXIV180711714; McCann B., 2018, 180608730 ARXIV; McCann B, 2017, ADV NEURAL INFORM PR; McCoy Richard T., 2019, P SOC COMP LING SCIL; Miller George A, 1995, COMMUNICATIONS ACM; Naik Aakanksha, 2018, INT C COMP LING COLI; Nangia Nikita, 2019, P ASS COMP LING ACL; Paszke Adam, 2017, ADV NEURAL INFORM PR; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Phang Jason, 2018, 181101088 ARXIV; Pilehvar Mohammad Taher, 2019, P C N AM CHAPT ASS C; Poliak A., 2018, P 2018 C EMP METH NA; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Radford Alec, 2018, IMPROVING LANG UNPUB; Rajpurkar P., 2016, CORR, P2383, DOI [10.18653/v1/D16-1264, DOI 10.18653/V1/D16-1264]; Roemmele M., 2011, 2011 AAAI SPRING S S; Rudinger Rachel, 2018, P 2018 C N AM CHAPT, P8, DOI DOI 10.18653/V1/N18-2002; Sap Maarten, 2019, P C EMP METH NAT LAN; Schneider N., 2015, P NAACL HLT DENV COL; Schuler Karin Kipper, 2005, THESIS; Socher R., 2013, PROC C EMPIRICAL MET; Tenney Ian, 2019, INT C LEARN REPR ICL; Thomas McCoy R., 2019, P ASS COMP LING ACL; Trivedi H., 2019, P NAACL 2019, DOI [10.18653/v1/N19-1302, DOI 10.18653/V1/N19-1302]; Wang Alex, 2019, GLUE MULTITASK BENCH; Wang Alex, 2019, JIANT 1 2 SOFTWARE T; Warstadt Alex, 2019, T ASS COMPUTATIONAL; Webster Kellie, 2018, T ASS COMPUTATIONAL; Williams Adina, 2018, P C N AM CHAPT ASS C; Yang ZR, 2019, ADV NEUR IN, V32; Zanzotto FM, 2017, ACM T INTERACT INTEL, V7, DOI 10.1145/2885501; Zellers Rowan, 2018, SWAG LARGE SCALE ADV; Zhang Sheng, 2018, 181012885 ARXIV; Zhang Yuan, 2019, P C N AM CHAPT ASS C; Zhao Jieyu, 2018, ARXIV180406876	72	192	192	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													15	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303028
C	Viola, P; Jones, M		Dietterich, TG; Becker, S; Ghahramani, Z		Viola, P; Jones, M			Fast and robust classification using asymmetric AdaBoost and a detector cascade	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final detector with many desirable features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classifiers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields significant improvements in performance over conventional AdaBoost. The final face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of I in a 1,000,000.	Mitsubishi Elect Res Lab, Cambridge, MA USA		Viola, P (corresponding author), Mitsubishi Elect Res Lab, Cambridge, MA USA.	viola@merl.com; mjones@merl.com						Cortes C, 1995, MACH LEARN, P20; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; John G.H., 1994, MACHINE LEARNING P 1, DOI 10.1016/B978-1-55860-335-6.50023-4; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Schapire RE, 1998, ANN STAT, V26, P1651; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; SCHNEIDERMAN H, 2000, COMPUTER VISION PATT; VIOLA P, 2001, P IEEE WORKSH STAT C	8	192	218	0	16	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1311	1318						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100163
C	Hu, J; Shen, L; Albanie, S; Sun, G; Vedaldi, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hu, Jie; Shen, Li; Albanie, Samuel; Sun, Gang; Vedaldi, Andrea			Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					While the use of bottom-up local operators in convolutional neural networks (CNNs) matches well some of the statistics of natural images, it may also prevent such models from capturing contextual long-range feature interactions. In this work, we propose a simple, lightweight approach for better context exploitation in CNNs. We do so by introducing a pair of operators: gather, which efficiently aggregates feature responses from a large spatial extent, and excite, which redistributes the pooled information to local features. The operators are cheap, both in terms of number of added parameters and computational complexity, and can be integrated directly in existing architectures to improve their performance. Experiments on several datasets show that gather-excite can bring benefits comparable to increasing the depth of a CNN at a fraction of the cost. For example, we find ResNet-50 with gather-excite operators is able to outperform its 101-layer counterpart on ImageNet with no additional learnable parameters. We also propose a parametric gather-excite operator pair which yields further performance gains, relate it to the recently-introduced Squeeze-and-Excitation Networks, and analyse the effects of these changes to the CNN feature activation statistics.	[Hu, Jie; Sun, Gang] Momenta, Cambridge, MA 02142 USA; [Shen, Li; Albanie, Samuel; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England	University of Oxford	Hu, J (corresponding author), Momenta, Cambridge, MA 02142 USA.	hujie@momenta.ai; lishen@robots.ox.ac.uk; albanie@robots.ox.ac.uk; sungang@momenta.ai; vedaldi@robots.ox.ac.uk	Albanie, Samuel/AAC-9729-2020	Albanie, Samuel/0000-0003-1732-9198; Zhou, Xiangyun/0000-0001-8973-9079	ESPRC AIMS CDT; ERC [638009-IDIU]	ESPRC AIMS CDT; ERC(European Research Council (ERC)European Commission)	The authors would like to thank Andrew Zisserman and Aravindh Mahendran for many helpful discussions. Samuel Albanie is supported by ESPRC AIMS CDT. Andrea Vedaldi is supported by ERC 638009-IDIU.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Hanson A., 1978, COMPUTER VISION SYST; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4; Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P77; Hock Howard S, 1974, PERCEPTION PSYCHOPHY; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Ke TW, 2017, PROC CVPR IEEE, P4067, DOI 10.1109/CVPR.2017.433; Kligvasser I, 2018, PROC CVPR IEEE, P2433, DOI 10.1109/CVPR.2018.00258; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee H., 2009, P ANN INT C MACH LEA, P609; Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu W, 2016, INT WORKS EARTH OB; Luo WJ, 2016, ADV NEUR IN, V29; Morcos A. S., 2018, ICLR POSTER; MURPHY K, 2004, NIPS; Novotny D, 2018, PROC CVPR IEEE, P3637, DOI 10.1109/CVPR.2018.00383; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Strat Thomas M, 1991, IEEE TPMI; Sun G., 2018, P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wolf L., 2006, IJCV; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang J, 2007, MULTIMEDIA C EXHIBIT, P197, DOI [10.1145/1290082.1290111, DOI 10.1145/1290082.1290111]; Yu F., 2016, P ICLR 2016; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	50	190	197	5	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004001
C	Jiang, L; Meng, DY; Yu, SI; Lan, ZZ; Shan, SG; Hauptmann, AG		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Jiang, Lu; Meng, Deyu; Yu, Shoou-, I; Lan, Zhenzhong; Shan, Shiguang; Hauptmann, Alexander G.			Self-Paced Learning with Diversity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizer. This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.	[Jiang, Lu; Meng, Deyu; Yu, Shoou-, I; Lan, Zhenzhong; Shan, Shiguang; Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Meng, Deyu] Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Shaanxi, Peoples R China; [Shan, Shiguang] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China	Carnegie Mellon University; Xi'an Jiaotong University; Chinese Academy of Sciences; Institute of Computing Technology, CAS	Jiang, L (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	lujiang@cs.cmu.edu; dymeng@mail.xjtu.edu.cn; iyu@cs.cmu.edu; lanzhzh@cs.cmu.edu; sgshan@ict.ac.cn; alex@cs.cmu.edu	Yu, Shoou-I/Z-4199-2019	Shan, Shiguang/0000-0002-8348-392X	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center [D11PC20068]; 973 Program of China [3202013CB329404]; NSFC project [61373114]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center; 973 Program of China(National Basic Research Program of China); NSFC project(National Natural Science Foundation of China (NSFC))	This work was partially supported by Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20068. Deyu Meng was partially supported by 973 Program of China (3202013CB329404) and the NSFC project (61373114). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.	Basu S., 2013, AAAI; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brendel W., 2011, ICCV; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2; Gaidon A., 2012, BMVC; Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330; Jiang L., 2014, ICMR; Jiang L, 2012, MM; Jiang L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P547, DOI 10.1145/2647868.2654918; Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31; Khan Faisal, 2011, ADV NEURAL INFORM PR, P1449; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Kumar M., 2011, ICCV; Lan Z., 2013, TRECVID; Lan Zhenzhong, 2014, ARXIV14087071; Lapedriza A., 2013, ABS13116510 CORR; Lee Yong Jae, 2011, CVPR; Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Over P., 2013, TRECVID; Spitkovsky V. I., 2009, NIPS; Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308; Tang Kevin, 2012, ADV NEURAL INFORM PR, P647; Vig E., 2012, ECCV; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	29	190	192	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103019
C	Liu, R; Lehman, J; Molino, P; Such, FP; Frank, E; Sergeev, A; Yosinski, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Rosanne; Lehman, Joel; Molino, Piero; Such, Felipe Petroski; Frank, Eric; Sergeev, Alex; Yosinski, Jason			An intriguing failing of convolutional neural networks and the CoordConv solution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x, y) Cartesian space and coordinates in one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10-100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24% better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.	[Liu, Rosanne; Lehman, Joel; Molino, Piero; Such, Felipe Petroski; Frank, Eric; Yosinski, Jason] Uber AI Labs, San Francisco, CA 94107 USA; [Sergeev, Alex] Uber Technol, Seattle, WA USA	Uber Technologies, Inc.	Liu, R (corresponding author), Uber AI Labs, San Francisco, CA 94107 USA.	rosanne@uber.com; joel.lehman@uber.com; piero@uber.com; felipe.such@uber.com; mysterefrank@uber.com; asergeev@uber.com; yosinski@uber.com	Lehman, Joel/AAH-9977-2019	Lehman, Joel/0000-0002-9535-1123; Yosinski, Jason/0000-0002-4701-0199				Banino Andrea, 2018, NATURE, P1; Brust Clemens-Alexander, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P510; Cueva C. J., 2018, ARXIV E PRINTS; Dan Horgan, 2018, ARXIV180300933; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Franzius M, 2007, PLOS COMPUT BIOL, V3, P1605, DOI 10.1371/journal.pcbi.0030166; Gehring J., 2017, P ICML; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hoover AK, 2009, CONNECT SCI, V21, P227, DOI 10.1080/09540090902733871; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Levine S, 2016, J MACH LEARN RES, V17; Li Chunyuan, 2018, INT C LEARN REPR APR; Lyu Y, 2018, ARXIV180405164; Mnih V., 2013, ARXIV E PRINTS; Mnih V, 2016, PR MACH LEARN RES, V48; Nguyen A., 2016, ARXIV E PRINTS; Oord A.V.D., 2016, SSW; Parmar N, 2018, PR MACH LEARN RES, V80; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Santoro A, 2017, ADV NEUR IN, V30; Sergeev A., 2018, ARXIV E PRINTS; Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8; Touretzky, 1990, ADV NEURAL INFORM PR, V2, P524, DOI [10.1007/978-1-4899-7687-1_33, DOI 10.1007/978-1-4899-7687-1_33]; Ulyanov D, 2020, INT J COMPUT VISION, V128, P1867, DOI 10.1007/s11263-020-01303-4; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629	36	189	191	5	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004019
C	You, JX; Liu, BW; Ying, R; Pande, V; Leskovec, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		You, Jiaxuan; Liu, Bowen; Ying, Rex; Pande, Vijay; Leskovec, Jure			Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DRUG DISCOVERY	Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.	[You, Jiaxuan; Ying, Rex; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Liu, Bowen] Stanford Univ, Dept Chem, Stanford, CA 94305 USA; [Pande, Vijay] Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University	You, JX (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	jiaxuan@stanford.edu; liubowen@stanford.edu; rexying@stanford.edu; pande@stanford.edu; jure@cs.stanford.edu	You, Jiaxuan/ABC-7506-2020; Pande, Vijay/ABE-8145-2020		DARPA SIMPLEX; ARO MURI; Stanford Data Science Initiative; Chan Zuckerberg Biohub; NIH [R01 GM062868, U19 AI109662]	DARPA SIMPLEX; ARO MURI(MURI); Stanford Data Science Initiative; Chan Zuckerberg Biohub; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The authors thank Xiang Ren, Marinka Zitnik, Jiaming Song, Joseph Gomes, Amir Barati Farimani, Peter Eastman, Franklin Lee, Zhenqin Wu and Paul Wender for their helpful discussions. This research has been supported in part by DARPA SIMPLEX, ARO MURI, Stanford Data Science Initiative, Huawei, JD, and Chan Zuckerberg Biohub. The Pande Group acknowledges the generous support of Dr. Anders G. Froseth and Mr. Christian Sundt for our work on machine learning. The Pande Group is broadly supported by grants from the NIH (R01 GM062868 and U19 AI109662) as well as gift funds and contributions from Folding@homedonors.	[Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2013, INT C MACH LEARN; Aspuru-Guzik A, 2017, OBJECTIVE REINFORCED; Bickerton GR, 2012, NAT CHEM, V4, P90, DOI [10.1038/nchem.1243, 10.1038/NCHEM.1243]; Bjerrum EJ., 2017, MOL GENERATION RECUR; Bleicher KH, 2003, NAT REV DRUG DISCOV, V2, P369, DOI 10.1038/nrd1086; Brockman G., 2016, OPENAI GYM; Dai H., 2018, P 6 INT C LEARN REPR; Ertl  P., 2017, ABS171207449 CORR; Ertl P, 2009, J CHEMINFORMATICS, V1, DOI 10.1186/1758-2946-1-8; Gilmer J., 2017, NEURAL MESSAGE PASSI; Gomez-Bombarelli Rafael, 2016, ACS CENTRAL SCI; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Irwin JJ, 2012, J CHEM INF MODEL, V52, P1757, DOI 10.1021/ci3001277; Jin W., 2018, P 35 INT C MACHINE L; Kakade S., 2002, INT C MACH LEARN; Kingma D.P., 2015, INT C LEARN REPR, P1; Kipf T. N., 2016, ICLR; Kirkpatrick P, 2004, NATURE, V432, P823, DOI 10.1038/432823a; Kusner M. J., 2017, INT C MACH LEARN, V70; Landrum G., 2006, RDKIT OPEN SOURCE CH; Li Y., 2018, ARXIV; Li  Y., 2018, MULTIOBJECTIVE DE NO; Lin JH, 1997, PHARMACOL REV, V49, P403; Lipinski CA, 1997, ADV DRUG DELIVER REV, V23, P3, DOI 10.1016/S0169-409X(96)00423-1; Liu BW, 2017, ACS CENTRAL SCI, V3, P1103, DOI 10.1021/acscentsci.7b00303; Maclaurin D., 2015, ADV NEURAL INFORM PR; Olivecrona M, 2017, J CHEMINFORMATICS, V9, DOI 10.1186/s13321-017-0235-x; Polishchuk PG, 2013, J COMPUT AID MOL DES, V27, P675, DOI 10.1007/s10822-013-9672-4; Rogers D, 2010, J CHEM INF MODEL, V50, P742, DOI 10.1021/ci100050t; SanchezLengeling  B., 2017, OPTIMIZING DISTRIBUT, V8; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Schulman J., 2017, **NON-TRADITIONAL**; Segall MD, 2012, CURR PHARM DESIGN, V18, P1292; Segler MHS, 2018, ACS CENTRAL SCI, V4, P120, DOI 10.1021/acscentsci.7b00512; Simonovsky Martin, 2018, ARXIV180203480; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; Wildman SA, 1999, J CHEM INF COMP SCI, V39, P868, DOI 10.1021/ci990307l; Yang  X., 2017, CHEMTS EFFICIENT PYT; You J., 2018, ARXIV180208773, P1; Yu L., 2017, AAAI	44	189	194	5	21	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000087
C	Ovadia, Y; Fertig, E; Ren, J; Nado, Z; Sculley, D; Nowozin, S; Dillon, JV; Lakshminarayanan, B; Snoek, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ovadia, Yaniv; Fertig, Emily; Ren, Jie; Nado, Zachary; Sculley, D.; Nowozin, Sebastian; Dillon, Joshua V.; Lakshminarayanan, Balaji; Snoek, Jasper			Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modem machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.	[Ovadia, Yaniv; Fertig, Emily; Ren, Jie; Nado, Zachary; Sculley, D.; Nowozin, Sebastian; Dillon, Joshua V.; Snoek, Jasper] Google Res, New York, NY 02115 USA; [Lakshminarayanan, Balaji] DeepMind, London, England	Google Incorporated	Snoek, J (corresponding author), Google Res, New York, NY 02115 USA.; Lakshminarayanan, B (corresponding author), DeepMind, London, England.	yovadia@google.com; emilyaf@google.com; jjren@google.com; znado@google.com; dsculley@google.com; nowozin@google.com; jvdillon@google.com; balajiln@google.com; jsnoek@google.com						Alemi A. A., 2018, UNCERTAINTY VARIATIO; Amodei D., 2016, CONCRETE PROBLEMS AI; Behrmann J., 2018, ARXIV181100995; BISHOP CM, 1994, IEE P-VIS IMAGE SIGN, V141, P217, DOI 10.1049/ip-vis:19941330; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Bojarski Mariusz, 2016, arXiv; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Chelba Ciprian, 2013, ARXIV13123005; DeGroot M. H., 1983, STATISTICIAN; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Gal Y, 2016, PR MACH LEARN RES, V48; Geifman Y., 2017, NEURIPS; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hendrycks D., 2019, ICLR, P1; Hensman J., 2015, INT C ART INT STAT J; Herntrndez-Lobato J. M., 2015, ICML; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; J Ren, 2015, ARXIV190602845; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Lang K., 1995, MACHINE LEARNING; Liang Shiyu, 2018, INT C LEARN REPR; Lipton Zachary C., 2018, ARXIV180703341; Louizos C, 2017, PR MACH LEARN RES, V70; Louizos C, 2016, PR MACH LEARN RES, V48; MacKay D. J., 1999, STAT NEURAL NETWORKS; Mackay D.J.C., 1992, THESIS CALIFORNIA I; Naeini MP, 2015, AAAI CONF ARTIF INTE, P2901; Nalisnick E, 2019, PR MACH LEARN RES, V97; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Platt JC, 2000, ADV NEUR IN, P61; Quinonero-Candela J., 2006, MACHINE LEARNING CHA; Rahimi A., 2017, ADDENDUM ALCHEMY; Riquelme Carlos, 2018, ICLR; Sculley D., 2018, WINNERS CURSE PACE P; Shafaei A., 2018, ARXIV180904729; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Sugiyama M., 2017, DATASET SHIFT MACHIN; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wen Yeming, 2018, ARXIV180304386; Wu A., 2019, ICLR; [No title captured]	57	188	188	1	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905062
C	Ratner, A; De Sa, C; Wu, S; Selsam, D; Re, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ratner, Alexander; De Sa, Christopher; Wu, Sen; Selsam, Daniel; Re, Christopher			Data Programming: Creating Large Training Sets, Quickly	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can "denoise" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.	[Ratner, Alexander; De Sa, Christopher; Wu, Sen; Selsam, Daniel; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Ratner, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ajratner@stanford.edu; cdesa@stanford.edu; senwu@stanford.edu; dselsam@stanford.edu; chrismre@stanford.edu			DARPA [FA8750-13-2-0039, FA8750-12-2-0335]; NSF [IIS-1247701, CCF-1337375, IIS-1353606]; DOE [108845]; ONR [N000141210041, N000141310129]; NIH [U54EB020405]; DARPA's SIMPLEX program; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; Toshiba;  [NSFCCF-1111943]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE)); ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); DARPA's SIMPLEX program; Oracle; NVIDIA; Huawei(Huawei Technologies); SAP Labs; Sloan Research Fellowship(Alfred P. Sloan Foundation); Moore Foundation(Gordon and Betty Moore Foundation); American Family Insurance; Google(Google Incorporated); Toshiba; 	Thanks to Theodoros Rekatsinas, Manas Joglekar, Henry Ehrenberg, Jason Fries, Percy Liang, the DeepDive and DDLite users and many others for their helpful conversations. The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSFCCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; DARPA's SIMPLEX program; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba. The views and conclusions expressed in this material are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, AFRL, NSF, ONR, NIH, or the U.S. Government.	Alfonseca E., P ACL; Angeli Gabor, 2014, TAC KBP, V695; Balsubramani A., 2015, ADV NEURAL INFORM PR, P1351; Berend D., 2014, NIPS; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bootkrajang Jakramate, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P143, DOI 10.1007/978-3-642-33460-3_15; Bunescu Razvan, 2007, ANN M ASS COMP LING, P576; Craven M, 1999, Proc Int Conf Intell Syst Mol Biol, P77; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Dogan R. I., P 2012 WORKSH BIOM N; Ehrenberg Henry R., 2016, HILDA PROC WORK HUM, P13, DOI [10.1145/2939502.2939515, DOI 10.1145/2939502.2939515]; Gao H, 2011, IEEE INTELL SYST, V26, P10, DOI 10.1109/MIS.2011.52; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffmann R., P ACL; Joglekar M., DAT ENG ICDE 2015 IE; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Krishna Ranjay, 2016, ARXIV160207332; Krogel MA, 2004, MACH LEARN, V57, P61, DOI 10.1023/B:MACH.0000035472.73496.0c; LUGOSI G, 1992, PATTERN RECOGN, V25, P79, DOI 10.1016/0031-3203(92)90008-7; Mallory E. K., 2015, BIOINFORMATICS; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; Natarajan N., ADV NEURAL INFORM PR, V26; Parisi F, 2014, P NATL ACAD SCI USA, V111, P1253, DOI 10.1073/pnas.1219097111; Riedel S, 2010, LECT NOTES ARTIF INT, V6323, P148, DOI 10.1007/978-3-642-15939-8_10; Roth B., P 22 ACM C KNOWL MAN; Roth Benjamin, 2013, EMNLP, P24; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Shin J, 2015, PROC VLDB ENDOW, V8, P1310, DOI 10.14778/2809974.2809991; Surdeanu Mihai, 2014, P TEXT AN C TAC2014; Takamatsu S., P ACL; Verga Patrick, 2015, ARXIV151106396; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260	33	186	187	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG	29872252				2022-12-19	WOS:000458973704048
C	Scholkopf, B		Leen, TK; Dietterich, TG; Tresp, V		Scholkopf, B			The kernel trick for distances	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				SUPPORT VECTOR; REGULARIZATION	A method is described which, like the kernel trick in support vector machines (SVMs), lets us generalize distance-based algorithms to operate in feature spaces, usually nonlinearly related to the input space. This is done by identifying a class of kernels which can be represented as norm-based distances in Hilbert spaces. It turns out that common kernel algorithms, such as SVMs and kernel PCA, are actually really distance based algorithms and can be run with that class of kernels, too. As well as providing a useful new insight into how these algorithms work, the present work can form the basis for conceiving new algorithms.	Microsoft Res, Cambridge, England	Microsoft	Scholkopf, B (corresponding author), Microsoft Res, 1 Guildhall St, Cambridge, England.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				Aizerman M. A., 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678; BERG C, 1984, HARMONIC ANAL SEMIGR; BOSER B, 1992, P 5 ANN WORKSH COMP, V5, P144; HAUSSLER D, 1999, UCSCCRL9910 U CAL SA; Schoenberg IJ, 1938, T AM MATH SOC, V44, P522, DOI 10.2307/1989894; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 1999, ADV KERNEL METHODS S; Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X; Smola AJ, 1999, ADV NEUR IN, V11, P585; Torgerson W.S., 1958, THEORY METHODS SCALI; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Wahba G., 1990, CBMS NSF REGIONAL C, V59; WATKINS C, 2000, COMMUNICATION	15	185	189	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						301	307						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800043
C	Razavi, A; van den Oord, A; Vinyals, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Razavi, Ali; van den Oord, Aaron; Vinyals, Oriol			Generating Diverse High-Fidelity Images with VQ-VAE-2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.	[Razavi, Ali; van den Oord, Aaron; Vinyals, Oriol] DeepMind, London, England		Razavi, A (corresponding author), DeepMind, London, England.	alirazavi@google.com; avdnoord@google.com; vinyals@google.com						Agustsson Eirikur, 2018, CORR; Azadi S., 2019, INT C LEARN REPR; Barratt S., 2018, ARXIV180101973; Bauer M., 2019, 22 INT C ART INT STA; Brock A., 2019, INT C LEARNING REPRE; Chen Xi, 2017, PIXELSNAIL IMPROVED, P12; Cheng XD, 2016, IEEE VTS VEH TECHNOL, DOI 10.1109/VTCSpring.2016.7504065; De Fauw Jeffrey, 2019, CORR; Dieleman S., 2018, ADV NEURAL INFORM PR, P7989; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heusel M., 2017, ADV NEURAL INFORM PR, V30, P6626; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Durk P, 2018, ADV NEURAL INFORM PR, P10236; Kolesnikov A, 2017, PR MACH LEARN RES, V70; Kynkaanniemi T, 2019, ADV NEUR IN, V32; Larochelle H., 2011, INT C ART INT STAT; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Menick J., 2019, P INT C LEARN REPR; Mentzer Fabian, 2018, CORR; Minnen David, 2018, ARXIV180902736; Oord A. v. d., 2016, ARXIV160903499; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Ravuri Suman, 2019, ARXIV190510887; Reed S, 2017, PR MACH LEARN RES, V70; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, STOCHASTIC BACKPROPA, V32; Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR, P5234; Salimans T, 2016, ADV NEUR IN, V29; Santurkar Shibani, 2017, CORR; Taigman Y., 2016, ARXIV161102200; Theis L, 2015, ADV NEURAL INFORM PR, P1927; Theis Lucas, 2016, ICLR; van den Oord A, 2016, PR MACH LEARN RES, V48; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals Oriol, 2017, CORR; WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089; Zhu Jun-Yan, 2017, ICCV	41	184	185	6	15	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906051
C	Singh, B; Najibi, M; Davis, LS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Singh, Bharat; Najibi, Mahyar; Davis, Larry S.			SNIPER: Efficient Multi-Scale Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present SNIPER, an algorithm for performing efficient multi-scale training in instance level visual recognition tasks. Instead of processing every pixel in an image pyramid, SNIPER processes context regions around ground-truth instances (referred to as chips) at the appropriate scale. For background sampling, these context-regions are generated using proposals extracted from a region proposal network trained with a short learning schedule. Hence, the number of chips generated per image during training adaptively changes based on the scene complexity. SNIPER only processes 30% more pixels compared to the commonly used single scale training at 800x1333 pixels on the COCO dataset. But, it also observes samples from extreme resolutions of the image pyramid, like 1400x2000 pixels. As SNIPER operates on resampled low resolution chips (512x512 pixels), it can have a batch size as large as 20 on a single GPU even with a ResNet-101 backbone. Therefore it can benefit from batch-normalization during training without the need for synchronizing batch-normalization statistics across GPUs. SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct. Our implementation based on Faster-RCNN with a ResNet-101 backbone obtains an mAP of 47:6 % on the COCO dataset for bounding box detection and can process 5 images per second during inference with a single GPU. Code is available at https://github.com/mahyarnajibi/SNIPER/.	[Singh, Bharat; Najibi, Mahyar; Davis, Larry S.] Univ Maryland, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Singh, B (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	bharat@cs.umd.edu; najibi@cs.umd.edu; lsd@cs.umd.edu			Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via DOI/IBC [D17PC00287, D17PC00345]	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via DOI/IBC	The authors would like to thank an Amazon Machine Learning gift for the AWS credits used for this research. The research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via DOI/IBC Contract Numbers D17PC00287 and D17PC00345. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied of IARPA, DOI/IBC or the U.S. Government.	Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Boda V. P., 2018, IEEE T INFORM THEORY; Boda VP, 2017, IEEE T INFORM THEORY, V63, P563, DOI 10.1109/TIT.2016.2633256; Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Chen CW, 2018, IEEE INT SYMP CIRC S, DOI [10.1109/ISCAS.2018.8351436, 10.1145/3265723.3265730]; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Ge DJ, 2017, IEEE INT CONF FUZZY; Gidaris S, 2016, PROC CVPR IEEE, P789, DOI 10.1109/CVPR.2016.92; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hinton G., 2015, ARXIV150302531; Krasin Ivan, 2017, OPENIMAGES PUBLIC DA, V2, P18; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Luo WJ, 2016, ADV NEUR IN, V29; Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522; Narang S., 2018, ICLR; Peng C, 2018, PROC CVPR IEEE, P6181, DOI 10.1109/CVPR.2018.00647; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu F., 2016, P ICLR 2016; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	42	184	200	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003083
C	Li, YJ; Fang, C; Yang, JM; Wang, ZW; Lu, X; Yang, MH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Yijun; Fang, Chen; Yang, Jimei; Wang, Zhaowen; Lu, Xin; Yang, Ming-Hsuan			Universal Style Transfer via Feature Transforms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.	[Li, Yijun; Yang, Ming-Hsuan] UC Merced, Merced, CA 95343 USA; [Fang, Chen; Yang, Jimei; Wang, Zhaowen; Lu, Xin] Adobe Res, San Jose, CA USA; [Yang, Ming-Hsuan] NVIDIA Res, Santa Clara, CA USA	University of California System; University of California Merced; Adobe Systems Inc.	Li, YJ (corresponding author), UC Merced, Merced, CA 95343 USA.	yli62@ucmerced.edu; cfang@adobe.com; jimyang@adobe.com; zhawang@adobe.com; xinl@adobe.com; mhyang@ucmerced.edu	Yang, Ming-Hsuan/AAE-7350-2019; Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304	NSF CAREER [1149783]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work is supported in part by the NSF CAREER Grant #1149783, gifts from Adobe and NVIDIA.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, NIPS; Champandard Alex J., 2016, SEMANTIC STYLE TRANS, P2; Chen D., 2017, CVPR; Chen Tian Qi, 2016, ARXIV161204337; Cimpoi M., 2014, CVPR; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Frigo O., 2016, CVPR; Gatys L. A., 2015, NIPS; GATYS L. A., 2016, CVPR; Gatys L. A., 2017, CVPR; Ghiasi Golnaz, 2017, BMVC; Gonzalez R.C., 2008, DIGITAL IMAGE PROCES; HERTZMANN A., 2001, SIGGRAPH; Hossain M, 2016, PROJ RHEA, P3; Huang X., 2017, ICCV; Johnson J, 2016, ECCV; Karayev S., 2014, BRIT MACH VIS C BMVC; Lempitsky V., 2016, ARXIV160708022V3; Li C., 2016, CVPR; Li C., 2016, ECCV; Li Y., 2017, CVPR; Liao J., 2017, ARXIV170501088; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Luan F., 2017, CVPR; Shih Y., 2013, SIGGRAPH; Shih YiChang, 2014, SIGGRAPH; Ulyanov D., 2017, CVPR; Vedaldi Andrea, 2016, TEXTURE NETWORKS FEE; Wang H., 2017, ARXIV170307255; Wang X., 2017, CVPR; WILMOT P, 2017, ARXIV170108893, P3	32	183	191	2	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400037
C	Beal, MJ; Ghahramani, Z; Rasmussen, CE		Dietterich, TG; Becker, S; Ghahramani, Z		Beal, MJ; Ghahramani, Z; Rasmussen, CE			The infinite hidden Markov model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NONPARAMETRIC PROBLEMS	We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite-consider, for example, symbols being possible words appearing in English text.	UCL, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Beal, MJ (corresponding author), UCL, Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.	m.beal@gatsby.ucl.ac.uk; zoubin@gatsby.ucl.ac.uk; edward@gatsby.ucl.ac.uk		Rasmussen, Carl Edward/0000-0001-8899-7850				ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; Bauman Peto L.C., 1995, NAT LANG ENG, V1, P289; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; MacKay D.J.C., 1997, BOOK ENSEMBLE LEARNI; Neal RM, 1998, 9815 U TOR DEP STAT; RASMUSSEN CE, 2000, ADV NEURAL INFORMATI, V12; Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI [10.1109/MASSP.1986.1165342, 10.1002/0471250953.bia03as18]; Stolcke A., 1993, P ADV NEUR INF PROC, P11	8	182	184	1	10	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						577	584						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100072
C	Faul, AC; Tipping, ME		Dietterich, TG; Becker, S; Ghahramani, Z		Faul, AC; Tipping, ME			Analysis of sparse Bayesian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyperparameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model.	Microsoft Res, Cambridge CB2 3NH, England	Microsoft	Faul, AC (corresponding author), Microsoft Res, St George House,1 Guildhall St, Cambridge CB2 3NH, England.			Faul, Anita/0000-0002-5911-2109				Bishop C.M., 2000, 16 C UNCERTAINTY ART, P46; CHEN S, 1995, 479 STANF U DEP STAT; GRANDVALET Y, 1998, PERSPECTIVES NEURAL, P201; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Smola A, 1999, IEE CONF PUBL, P575, DOI 10.1049/cp:19991171; Tipping ME, 2000, ADV NEUR IN, V12, P652; TIPPING ME, 2001, ADV NEURAL INFORMATI, V13; TIPPING ME, UNPUB NIPS 01; Vapnik V.N, 1998, STAT LEARNING THEORY	9	182	186	2	21	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						383	389						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100048
C	Hastie, T; Tibshirani, R		Jordan, MI; Kearns, MJ; Solla, SA		Hastie, T; Tibshirani, R			Classification by pairwise coupling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in simulated datasets. The classifiers used include linear discriminants and nearest neighbors: application to support vector machines is also briefly described.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Hastie, T (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.			Hastie, Trevor/0000-0002-0164-3142					0	182	188	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						507	513						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700072
C	Nazari, M; Oroojlooy, A; Takac, M; Snyder, LV		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nazari, Mohammadreza; Oroojlooy, Afshin; Takac, Martin; Snyder, Lawrence V.			Reinforcement Learning for Solving the Vehicle Routing Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NUMBER	We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single policy model that finds near-optimal solutions for a broad range of problem instances of similar size, only by observing the reward signals and following feasibility rules. We consider a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems.	[Nazari, Mohammadreza; Oroojlooy, Afshin; Takac, Martin; Snyder, Lawrence V.] Lehigh Univ, Dept Ind & Syst Engn, Bethlehem, PA 18015 USA	Lehigh University	Nazari, M (corresponding author), Lehigh Univ, Dept Ind & Syst Engn, Bethlehem, PA 18015 USA.	mon314@lehigh.edu; afo214@lehigh.edu; takac@lehigh.edu; lvs2@lehigh.edu	Takac, Martin/AAA-8564-2022	Takac, Martin/0000-0001-7455-2025				[Anonymous], 2018, GOOGL OPT TOOLS OR T; Applegate D. L., 2006, TRAVELING SALESMAN P; Archetti C, 2008, OPER RES COMPUT SCI, V43, P103, DOI 10.1007/978-0-387-77778-8_5; Bello I., 2016, ARXIV161109940; Busoniu L, 2010, STUD COMPUT INTELL, V310, P183; Chen Kan, 2015, ARXIV151105960; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Christofides N., 1976, TECHNICAL REPORT; CLARKE G, 1964, OPER RES, V12, P568, DOI 10.1287/opre.12.4.568; Dai H., 2017, ADV NEURAL INFORM PR; Dai HJ, 2016, PR MACH LEARN RES, V48; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fukasawa R, 2006, MATH PROGRAM, V106, P491, DOI 10.1007/s10107-005-0644-x; Glorot X., 2010, PROC MACH LEARN RES, P249; Glover F., 1998, HDB COMBINATORIAL OP, P2093, DOI [10.1007/978-1-4613-0303-9_33, DOI 10.1007/978-1-4613-0303-933]; Golden B, 2008, OPER RES COMPUT SCI, V43, P1, DOI 10.1007/978-0-387-77778-8; Gurobi Optimization LLC, 2020, GUROBI OPTIMIZER REF; Hong S, 2016, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2016.349; Jean S., 2015, USING VERY LARGE TAR; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Laporte G., 2000, International Transactions in Operational Research, V7, P285, DOI 10.1111/j.1475-3995.2000.tb00200.x; LAPORTE G, 1992, EUR J OPER RES, V59, P345, DOI 10.1016/0377-2217(92)90192-C; Luong M., 2015, ARXIV150804025; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Neubig G., 2017, ARXIV PREPRINT ARXIV; Ritzinger U, 2016, INT J PROD RES, V54, P215, DOI 10.1080/00207543.2015.1043403; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P81, DOI 10.1109/TNN.2008.2005141; Snyder Lawrence V, 2018, FUNDAMENTALS SUPPLY; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Toth P, 2002, SIAM MONOG DISCR MAT, P1; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Vinyals Oriol, 2016, 4 INT C LEARN REPR I, DOI DOI 10.48550/ARXIV.1511.01844; Voudouris C, 1999, EUR J OPER RES, V113, P469, DOI 10.1016/S0377-2217(98)00099-X; WREN A, 1972, OPER RES QUART, V23, P333, DOI 10.2307/3007888; Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685; Xu K, 2015, PR MACH LEARN RES, V37, P2048	40	179	188	18	66	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004040
C	Minka, TP		Leen, TK; Dietterich, TG; Tresp, V		Minka, TP			Automatic choice of dimensionality for PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained. By interpreting PCA as density estimation, we show how to use Bayesian model selection to estimate the true dimensionality of the data. The resulting estimate is simple to compute yet guaranteed to pick the correct dimensionality, given enough data. The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly. But after choosing an appropriate parameterization and applying Laplace's method, an accurate and practical estimator is obtained. In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster.	MIT, Media Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Minka, TP (corresponding author), MIT, Media Lab, 20 Ames St, Cambridge, MA 02139 USA.							Bishop C. M., 1998, NEURAL INFORMATION P, V11, P382; BREGLER C, 1994, NIPS, P43; Everson R, 2000, IEEE T SIGNAL PROCES, V48, P2083, DOI 10.1109/78.847792; FUKUNAGA K, 1971, IEEE T COMPUT, VC 20, P176, DOI 10.1109/T-C.1971.223208; GHAHRAMANI Z, 1999, NEURAL INFORMATION P, V12; Ghahramani Zoubin, 1996, CRGTR961 U TOR; JAMES AT, 1954, ANN MATH STAT, V25, P40, DOI 10.1214/aoms/1177728846; Kass R.E., 1993, 254 U WASH; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; MINKA T, 1999, 514 MIT MED LAB VIS; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; MOGHADDAM B, 1998, NEURAL INFORMATION P, V11, P910; RAJAN JJ, 1997, IEE VISION IMAGE SIG, V144, P166; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; TIPPING ME, 1999, J ROYAL STAT SOC B, V61	15	179	192	0	10	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						598	604						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800085
C	Bishop, CM		Kearns, MS; Solla, SA; Cohn, DA		Bishop, CM			Bayesian PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					The technique of principal component analysis (PCA) has recently been expressed as the maximum likelihood solution for a generative latent variable model. Ln this paper we use this probabilistic reformulation as the basis for a Bayesian treatment of PCA. Our key result is that effective dimensionality of the latent space (equivalent to the number of retained principal components) can be determined automatically as part of the Bayesian inference procedure. An important application of this framework is to mixtures of probabilistic PCA models, in which each component can determine its own effective complexity.	Microsoft Res, Cambridge CB2 3NH, England	Microsoft	Bishop, CM (corresponding author), Microsoft Res, St George House,1 Guildhall St, Cambridge CB2 3NH, England.							Bishop, 1995, NEURAL NETWORKS PATT; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; TIPPING ME, 1997, P IEE 5 INT C ART NE, P13; TIPPLING ME, 1997, UNPUB J ROYAL STAT B	4	179	181	0	10	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						382	388						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700054
C	Lu, Z; Pu, HM; Wang, FC; Hu, ZQ; Wang, LW		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lu, Zhou; Pu, Hongming; Wang, Feicheng; Hu, Zhiqiang; Wang, Liwei			The Expressive Power of Neural Networks: A View from the Width	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.	[Lu, Zhou; Pu, Hongming; Wang, Feicheng] Peking Univ, Dept Math, Beijing, Peoples R China; [Hu, Zhiqiang; Wang, Liwei] Peking Univ, Key Lab Machine Percept, MOE, Sch EECS, Beijing, Peoples R China; [Lu, Zhou; Wang, Feicheng; Wang, Liwei] Peking Univ, Ctr Data Sci, Beijing Inst Big Data Res, Beijing, Peoples R China	Peking University; Peking University; Peking University	Lu, Z (corresponding author), Peking Univ, Dept Math, Beijing, Peoples R China.; Lu, Z (corresponding author), Peking Univ, Ctr Data Sci, Beijing Inst Big Data Res, Beijing, Peoples R China.	1400010739@pku.edu.cn; 1400010621@pku.edu.cn; 1400010604@pku.edu.cn; huzq@pku.edu.cn; wanglw@cis.pku.edu.cn	Jeong, Yongwook/N-7413-2016		National Basic Research Program of China (973 Program) [2015CB352502]; NSFC [61573026]; Center for Data Science, Beijing Institute of Big Data Research in Peking University	National Basic Research Program of China (973 Program)(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC)); Center for Data Science, Beijing Institute of Big Data Research in Peking University	This work was partially supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and Center for Data Science, Beijing Institute of Big Data Research in Peking University. We would like to thank the anonymous reviewers for their valuable comments on our paper.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; BARRON AR, 1994, MACH LEARN, V14, P115, DOI 10.1023/A:1022650905902; Cohen N., 2016, C LEARNING THEORY, V49, P698; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; Harvey Nick, 2017, COLT 2017; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Nguyen Q, 2017, PR MACH LEARN RES, V70; Srikant R., 2017, ICLR 2017; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Telgarsky M.., 2016, C LEARNING THEORY, P1517; Yarotsky D., 2016, ARXIV161001145	16	178	182	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406030
C	Kingma, DP; Dhariwal, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kingma, Diederik P.; Dhariwal, Prafulla			Glow: Generative Flow with Invertible 1 x 1 Convolutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 x 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openi/glow.	[Kingma, Diederik P.; Dhariwal, Prafulla] OpenAI, San Francisco, CA 94110 USA; [Kingma, Diederik P.] Google AI, San Francisco, CA USA		Kingma, DP (corresponding author), OpenAI, San Francisco, CA 94110 USA.; Kingma, DP (corresponding author), Google AI, San Francisco, CA USA.							Cobo, 2017, ARXIV171110433; Deco G., 1995, ADV NEURAL INFORM PR, P247; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, ARXIV13080850; Grover A, 2018, AAAI CONF ARTIF INTE, P3069; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2018, VARIATIONAL AUTOENCO; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Oord A.V.D., 2016, SSW; Papamakarios George, 2017, ARXIV170507057; Parmar Niki, 2018, ARXIV180205751, P2; Reed S, 2017, PR MACH LEARN RES, V70; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T., 2017, GRADIENT CHECKPOINTI; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7	28	176	176	1	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004074
C	Huang, YP; Cheng, YL; Bapna, A; Firat, O; Chen, MX; Chen, DH; Lee, H; Ngiam, J; Le, QV; Wu, YH; Chen, ZF		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huang, Yanping; Cheng, Youlong; Bapna, Ankur; Firat, Orhan; Chen, Mia Xu; Chen, Dehao; Lee, HyoukJoong; Ngiam, Jiquan; Le, Quoc V.; Wu, Yonghui; Chen, Zhifeng			GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batchsplitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.				huangyp@google.com; ylc@google.com; ankurbpn@google.com; orhanf@google.com; miachen@google.com; dehao@google.com; hyouklee@google.com; jngiam@google.com; qvl@google.com; yonghui@google.com; zhifengc@google.com						Arivazhagan Naveen, 2019, ARXIV190705019; Auli M., 2019, ABS190110430 CORR; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chen M. X., 2018, ABS180409849 CORR; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Cubuk Ekin D., 2018, ARXIV180509501; Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Gehring J., 2017, P ICML; Griewank A, 2000, ACM T MATH SOFTWARE, V26, P19, DOI 10.1145/347837.347846; Harlap A., 2018, ARXIV PREPRINT ARXIV; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kornblith Simon, 2018, ARXIV180508974; Krizhevsky A., 2014, CORR; Lee S, 2014, ADV NEUR IN, V27; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; McCann Bryan, 2017, ARXIV170800107; Mirhoseini A, 2017, PR MACH LEARN RES, V70; Ngiam Jiquan, 2018, DOMAIN ADAPTIVE TRAN, P1; Peng Chao, 2017, CVPR, V7; Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041; Peters M., 2018, ACL; PETROWSKI A, 1993, IEEE T NEURAL NETWOR, V4, P970, DOI 10.1109/72.286892; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Shazeer N, 2018, ADV NEUR IN, V31; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002; Wu Yonghui, 2017, T ASS COMPUTATIONAL; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255; Zhang Han, 2019, ICLR; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	46	175	179	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300010
C	Kazemi, SM; Poole, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kazemi, Seyed Mehran; Poole, David			SimplE Embedding for Link Prediction in Knowledge Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at https ://github.com/Mehran-k/SimplE.	[Kazemi, Seyed Mehran; Poole, David] Univ British Columbia, Vancouver, BC, Canada	University of British Columbia	Kazemi, SM (corresponding author), Univ British Columbia, Vancouver, BC, Canada.	smkazemi@cs.ubc.ca; poole@cs.ubc.ca						Abadi M, 2015, P 12 USENIX S OPERAT; Bordes A., 2013, ARXIV13047158; Bordes A., 2013, ADV NEURAL INFORM PR; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Das Rajarshi, 2017, NIPS WORKSH AKBC; Dat Quoc Nguyen, 2017, ARXIV170308098; de Raedt L., 2016, SYNTHESIS LECT ARTIF, DOI DOI 10.2200/S00692ED1V01Y201601AIM032; Dettmers T, 2018, AAAI CONF ARTIF INTE, P1811; Ding Boyang, 2018, P 56 ANN M ASS COMP; Dong XL, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P601, DOI 10.1145/2623330.2623623; Evans C., 2008, P 2008 ACM SIGMOD IN, P1247, DOI [DOI 10.1145/1376616.1376746, 10.1145/1376616]; Feng J, 2016, FIFTEENTH INTERNATIONAL CONFERENCE ON THE PRINCIPLES OF KNOWLEDGE REPRESENTATION AND REASONING, P557; Getoor Lise, 2007, INTRO STAT RELATIONA; Guo S., 2016, P 2016 C EMPIRICAL M, P192; Hayashi K., 2017, ARXIV170205563; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Ji GL, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P687; Kadlec Rudolf, 2017, P 2 WORKSH REPR LEAR, P69, DOI [DOI 10.18653/V1/W17-2609, 10.18653/v1/W17-2609]; Lao N, 2010, MACH LEARN, V81, P53, DOI 10.1007/s10994-010-5205-8; Lin Yankai, 2015, EMNLP; Lin YC, 2015, ADV SOC SCI EDUC HUM, V39, P2181; Liu Hanxiao, 2018, AAAI; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Minervini P, 2017, LECT NOTES ARTIF INT, V10534, P668, DOI 10.1007/978-3-319-71249-9_40; Nguyen Dat Quoc, 2016, MARK; Nickel M., 2012, P 21 INT C WORLD WID, P271; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M, 2016, AAAI CONF ARTIF INTE, P1955; Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592; Nickel Maximilian, 2014, ADV NEURAL INFORM PR, P1179; Rocktaschel T., 2014, P ACL 2014 WORKSH SE, P45, DOI DOI 10.3115/V1/W14-2409; Rocktaschel Tim, 2017, ADV NEURAL INFORM PR, P3791; Santoro A, 2017, ADV NEUR IN, V30; Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38; Seyed Mehran Kazemi, 2018, AAAI; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Trouillon T, 2017, P INT WORKSH STAT RE; Trouillon T, 2016, PR MACH LEARN RES, V48; Trouillon Theo, 2017, ARXIV170206879; Wang Q, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1859; Wang Q, 2017, IEEE T KNOWL DATA EN, V29, P2724, DOI 10.1109/TKDE.2017.2754499; Wang YX, 2018, AAAI CONF ARTIF INTE, P5561; Wang Z., 2014, AAAI, V14, P1112; Wei Zhuoyu, 2015, P 24 ACM INT C INFOR, P1331; Yang Bishan, 2015, 3 INT C LEARN REPR I; Zhang Hanwang, 2017, PROC CVPR IEEE, P5532, DOI [DOI 10.1109/CVPR.2017.331, DOI 10.1109/CVPR.2018.00611]	47	174	179	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304031
C	Ilyas, A; Santurkar, S; Tsipras, D; Engstrom, L; Tran, B; Madry, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Engstrom, Logan; Tran, Brandon; Madry, Aleksander			Adversarial Examples are not Bugs, they are Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.	[Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Engstrom, Logan; Tran, Brandon; Madry, Aleksander] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Ilyas, A (corresponding author), MIT, Cambridge, MA 02139 USA.	ailyas@mit.edu; shibani@mit.edu; tsipras@mit.edu; engstrom@mit.edu; btran115@mit.edu; madry@mit.edu	Tsipras, Dimitris/AAZ-2505-2021		NSF [CCF-1553428, CCF-1563880, CNS-1413920, CNS-1815221, IIS-1447786, IIS-1607189]; Microsoft Corporation; Intel Corporation; MIT-IBM Watson AI Lab research grant; Analog Devices Fellowship	NSF(National Science Foundation (NSF)); Microsoft Corporation(Microsoft); Intel Corporation(Intel Corporation); MIT-IBM Watson AI Lab research grant(International Business Machines (IBM)); Analog Devices Fellowship	Work supported in part by the NSF grants CCF-1553428, CCF-1563880, CNS-1413920, CNS-1815221, IIS-1447786, IIS-1607189, the Microsoft Corporation, the Intel Corporation, the MIT-IBM Watson AI Lab research grant, and an Analog Devices Fellowship.	Athalye A, 2018, PR MACH LEARN RES, V80; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Bubeck S., 2018, ARXIV180510204; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Carlini N., 2019, CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Charles Z, 2019, PR MACH LEARN RES, V89; Cohen J, 2019, PR MACH LEARN RES, V97; Daskalakis Constantinos, 2019, FDN COMPUTER SCI FOC; Ding G. W., 2019, INT C LEARN REPR; Engstrom L., 2017, NIPS 2017 WORKSH MAC; Engstrom L., 2019, DISTILL, V4, DOI [10.23915/distill.00019, DOI 10.23915/DISTILL.00019]; Fawzi A., 2018, ADV NEURAL INFORM PR; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Ford N, 2019, PR MACH LEARN RES, V97; Geirhos R., 2019, ICLR; Gilmer J., 2018, INT C LEARN REPR WOR; Goh Gabriel., 2019, DISTILL, V4, DOI [10.23915/distill.00019.3, DOI 10.23915/DISTILL.00019.3]; Goh Gabriel, 2019, DISTILL, DOI [10.23915/distill.00019.2, DOI 10.23915/DISTILL.00019.2]; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He Warren, 2017, 11 USENIX WORKSH OFF; Hendrycks Dan, 2019, PROC INT C LEARN REP; Hinton G., 2015, NIPS WORKSH, P1; Jetley S., 2018, P ADV NEUR INF PROC, P10749; Kim B., 2019, ARXIV190311626; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Laurent B., 2000, ANN STAT; Lecuyer M., 2019, 2019 IEEE S SEC PRIV; Liu Y., 2017, 5 INT C LEARN REPR I; Madry A., 2018, P ICLR VANC BC CAN; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Mahloujifar S., 2018, ARXIV180903063; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Nakkiran P., 2019, ARXIV190100532; Nakkiran Preetum, 2019, DISTILL, DOI [10.23915/distill.00019.5, DOI 10.23915/DISTILL.00019.5]; Olah C., 2017, DISTILL, V2; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Raghunathan A., 2018, INT C LEARN REPR; RECHT B, 2019, ICML; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schmidt L, 2018, ADV NEUR IN, V31; Shafahi A., 2019, P 7 INT C LEARN REPR; Shamir A., 2019, ARXIV190110861; Smilkov D., 2017, SMOOTHGRAD REMOVING; Stutz David, 2019, COMPUTER VISION PATT, P6976; Suggala Arun Sai, 2019, C ART INT STAT AISTA; Szegedy Christian, 2014, P 2 INT C LEARNING R; Tanay T., 2016, ARXIV PREPRINT ARXIV; Tramer F., 2017, ARXIV170403453; Turner A., 2019, INT C LEARN REPR; Uesato Jonathan, 2018, P 35 INT C MACH LEAR; Wang Tongzhou, 2018, ARXIV181110959; Wong E, 2018, PR MACH LEARN RES, V80; Xiao K., 2019, ICLR; Zou Haosheng, 2018, GEOM MACH LEARN ICML	60	173	173	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300012
C	MOODY, JE		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MOODY, JE			THE EFFECTIVE NUMBER OF PARAMETERS - AN ANALYSIS OF GENERALIZATION AND REGULARIZATION IN NONLINEAR LEARNING-SYSTEMS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	173	174	0	3	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						847	854						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00104
C	Gardner, JR; Pleiss, G; Bindel, D; Weinberger, KQ; Wilson, AG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gardner, Jacob R.; Pleiss, Geoff; Bindel, David; Weinberger, Kilian Q.; Wilson, Andrew Gordon			GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHM	Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n(3)) to O(n(2)). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.	[Gardner, Jacob R.; Pleiss, Geoff; Bindel, David; Weinberger, Kilian Q.; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Gardner, JR (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	jrg365@cornell.edu; geoff@cs.cornell.edu; bindel@cs.cornell.edu; kqw4@cornell.edu; andrew@cornell.edu			NSF [IIS-1563887]; Facebook Research; National Science Foundation [III-1618134, III-1526012, IIS-1149882, IIS-1724282, TRIPODS-1740822]; Bill and Melinda Gates Foundation; Office of Naval Research; SAP America Inc.	NSF(National Science Foundation (NSF)); Facebook Research(Facebook Inc); National Science Foundation(National Science Foundation (NSF)); Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Office of Naval Research(Office of Naval Research); SAP America Inc.	JRG and AGW are supported by NSF IIS-1563887 and by Facebook Research. GP and KQW are supported in part by the III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822 grants from the National Science Foundation. In addition, they are supported by the Bill and Melinda Gates Foundation, the Office of Naval Research, and SAP America Inc.	Abadi M, 2015, P 12 USENIX S OPERAT; Asuncion A, 2007, UCI MACHINE LEARNING; Avron H, 2011, J ACM, V58, DOI 10.1145/1944345.1944349; Bach F., 2013, COLT; Bindel D., 2017, ADV NEURAL INFORM PR, V30, P6327; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Chaudhari P, 2016, ARXIV161101838; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Cunningham J. P., 2008, ICML; Cutajar K., 2016, ICML; Datta BN., 2010, NUMERICAL LINEAR ALG, V116; Demmel JW, 1997, APPL NUMERICAL LINEA, V56; Fitzsimons JF, 2016, ARXIV160800117; Gardner J. R., 2018, AISTATS; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Golub GH, 2010, PRINC SER APPL MATH, P1; Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072; Harbrecht H, 2012, APPL NUMER MATH, V62, P428, DOI 10.1016/j.apnum.2011.10.001; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J., 2015, ICML; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; HUTCHINSON MF, 1990, COMMUN STAT SIMULAT, V19, P433, DOI 10.1080/03610919008812866; Izmailov P, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P876; Jia Y., 2014, P 22 ACM INT C MULT, P675; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lanczos C., 1950, ITERATION METHOD SOL; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Murray I., 2009, ICML WORKSH NUM MATH; OLEARY DP, 1980, LINEAR ALGEBRA APPL, V29, P293, DOI 10.1016/0024-3795(80)90247-5; Paige C. C., 1970, BIT (Nordisk Tidskrift for Informationsbehandling), V10, P183, DOI 10.1007/BF01936866; PARLETT BN, 1980, LINEAR ALGEBRA APPL, V29, P323, DOI 10.1016/0024-3795(80)90248-7; Pleiss G., 2018, ICML; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen C., 2006, GAUSSIAN PROCESSES M, V1; Saad Y., 2003, ITERATIVE METHODS SP, P82; Saatci Y., 2012, THESIS U CAMBRIDGE C; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974; Van der Vorst H.A., 2003, ITERATIVE KRYLOV MET, V13; Wathen AJ, 2015, NUMER ALGORITHMS, V70, P709, DOI 10.1007/s11075-015-9970-0; Wilson A.G., 2014, THESIS U CAMBRIDGE C; Wilson A.G., 2015, ARXIV151101870; Wilson AG, 2016, ADV NEUR IN, V29; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775	52	171	171	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002015
C	Wang, RJ; Li, X; Ling, CX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Robert J.; Li, Xiang; Ling, Charles X.			Pelee: A Real-Time Object Detection System on Mobile Devices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					An increasing need of running Convolutional Neural Network (CNN) models on mobile devices with limited computing power and memory resource encourages studies on efficient model design. A number of efficient architectures have been proposed in recent years, for example, MobileNet, ShuffleNet, and MobileNetV2. However, all these models are heavily dependent on depthwise separable convolution which lacks efficient implementation in most deep learning frameworks. In this study, we propose an efficient architecture named PeleeNet, which is built with conventional convolution instead. On ImageNet ILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and over 1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile, PeleeNet is only 66% of the model size of MobileNet. We then propose a real-time object detection system by combining PeleeNet with Single Shot MultiBox Detector (SSD) method and optimizing the architecture for fast speed. Our proposed detection system(2), named Pelee, achieves 76.4% mAP (mean average precision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of 23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size.	[Wang, Robert J.; Li, Xiang; Ling, Charles X.] Univ Western Ontario, Dept Comp Sci, London, ON N6A 3K7, Canada	Western University (University of Western Ontario)	Ling, CX (corresponding author), Univ Western Ontario, Dept Comp Sci, London, ON N6A 3K7, Canada.	jwan563@uwo.ca; lxiang2@uwo.ca; charles.ling@uwo.ca						Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Jonathan, 2016, ABS161110012 CORR; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Khosla Aditya, 2011, 1 WORKSH FIN GRAIN V, P6; Kwak N., 2017, ARXIV170705031; Loshchilov I., 2017, P INT C LEARNING REP; Pleiss G., 2017, ABS170706990 CORR; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	20	171	181	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301091
C	Neil, D; Pfeiffer, M; Liu, SC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Neil, Daniel; Pfeiffer, Michael; Liu, Shih-Chii			Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				TIME	Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.	[Neil, Daniel] Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland; Swiss Fed Inst Technol, CH-8057 Zurich, Switzerland	University of Zurich; Swiss Federal Institutes of Technology Domain; ETH Zurich	Neil, D (corresponding author), Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.	dneil@ini.uzh.ch; pfeiffer@ini.uzh.ch; shih@ini.uzh.ch						Bengio Y., 2014, ARXIV14061078; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Buzsaki G, 2009, RHYTHMS BRAIN, DOI DOI 10.1093/ACPROF:OSO/9780195301069.001.0001; Cauwenberghs G, 1996, IEEE T NEURAL NETWOR, V7, P346, DOI 10.1109/72.485671; Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044; Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184; Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005; Dieleman S., 2015, LASAGNE 1 RELEASE, V1; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302; Graves A, 2013, ARXIV13080850; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hannun A.Y., 2014, ARXIV14125567, P1; He Kaiming, 2015, IEEE INT C COMP VIS; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Itseez, 2015, OP SOURC COMP VIS LI; Kingma D.P, P 3 INT C LEARNING R; Koutnik Jan, 2014, ARXIV14023511; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039; Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037; O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178; Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]; Pearlmutter BA, 1989, NEURAL COMPUT, V1, P263, DOI 10.1162/neco.1989.1.2.263; Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153; Semeniuta S., 2016, P COLING 2016 26 INT, P1757; Wand M., 2016, ARXIV160108188; Xu K, 2015, PR MACH LEARN RES, V37, P2048	29	171	175	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702057
C	Cortes, C; Mohri, M		Thrun, S; Saul, K; Scholkopf, B		Cortes, C; Mohri, M			AUC optimization vs. error rate minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classification algorithm. However, the objective function optimized in most of these algorithms is the error rate and not the AUC value. We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate. Our results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable. Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values. We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets demonstrating the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.	AT&T Labs Res, Florham Pk, NJ 07932 USA	AT&T	Cortes, C (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.							CHAUCHAT JH, 2001, TARGETING CUSTOMER G; EGAN JP, 1975, SIGNAL DETECTION THE; FERRI C, 2002, ICML 2002; FREUND Y, 1995, P 2 EUR C COMP LEARN, V2; FREUND YR, 1998, ICML 98; GREEN DM, 1966, SIGNAL DETECTION THE; Hanley J. A., 1982, RADIOLOGY; MOZER MC, 2002, NIPS 2002; Olshen R., 1984, CLASSIFICATION REGRE; PERLICH C, 2003, J MACHINE LEARNING R; PIATETSKYSHAPIR.G, 2000, SIGKDD EXPLORATIONS; PROVOST F, 1997, KDD 97; ROSSET S, 2001, KDD 2001; ROSSET S, 1999, THESIS TEL AVIV U; YAN L, 2003, ICML 2003	15	171	177	0	9	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						313	320						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500040
C	Schultz, M; Joachims, T		Thrun, S; Saul, K; Scholkopf, B		Schultz, M; Joachims, T			Learning a distance metric from relative comparisons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					This paper presents a method for learning a distance metric from relative comparison such as "A is closer to B than A is to C". Taking a Support Vector Machine (SVM) approach, we develop an algorithm that provides a flexible way of describing qualitative training data as a set of constraints. We show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard methods for SVM training. We empirically evaluate the performance and the modelling flexibility of the algorithm on a collection of text documents.	Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Schultz, M (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.			Joachims, Thorsten/0000-0003-3654-3683				BUJA A, IN PRESS J COMPUTATI; COHN D, 2003, TR20031892 CORN U; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cox T.F., 1994, MULTIDIMENSIONAL SCA; CRAVEN M, 1998, P 15 NAT C ART INT A; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Joachims T., 2002, P ACM C KNOWL DISC D; Tsang I., 2003, P INT C ART NEUR NET; Vapnik V.N, 1998, STAT LEARNING THEORY; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616; XING EP, 2002, ADV NEURAL INFORMATI	11	171	173	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						41	48						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500006
C	Ren, Y; Ruan, YJ; Tan, X; Qin, T; Zhao, S; Zhao, Z; Liu, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ren, Yi; Ruan, Yangjun; Tan, Xu; Qin, Tao; Zhao, Sheng; Zhao, Zhou; Liu, Tie-Yan			FastSpeech: Fast, Robust and Controllable Text to Speech	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SYNTHESIS SYSTEM	Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.(3)	[Ren, Yi; Ruan, Yangjun; Zhao, Zhou] Zhejiang Univ, Hangzhou, Peoples R China; [Tan, Xu; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Redmond, WA USA; [Zhao, Sheng] Microsoft STC Asia, Beijing, Peoples R China	Zhejiang University; Microsoft	Zhao, Z (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.	rayeren@zju.edu.cn; ruanyj3107@zju.edu.cn; xuta@microsoft.com; taoqin@microsoft.com; Sheng.Zhao@microsoft.com; zhaozhou@zju.edu.cn; tyliu@microsoft.com		Qin, Tao/0000-0002-9095-0776	National Natural Science Foundation of China [61602405, 61836002]; China Knowledge Centre of Engineering Sciences and Technology	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Knowledge Centre of Engineering Sciences and Technology	This work was supported by the National Natural Science Foundation of China under Grant No.61602405, No.61836002. This work was also supported by the China Knowledge Centre of Engineering Sciences and Technology.	Arik SO, 2017, PR MACH LEARN RES, V70; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gehring J., 2017, P ICML; GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317; Gu Jiatao, 2017, ARXIV171102281; Guo J., 2019, AAAI; Ito K, 2017, LJ SPEECH DATASET; Jin ZY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2251; Kim Yoon, 2016, ARXIV160607947, DOI [10.18653/v1/D16-1139, DOI 10.18653/V1/D16-1139]; Li Hao, 2018, ARXIV180609276; Li N., 2018, ARXIV180908895; Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457; Peng K., 2019, ARXIV190508459; Ping W, 2018, INT C LEARNING REPRE; Ping W., 2019, INT C LEARN REPR ICL; Prenger R, 2019, INT CONF ACOUST SPEE, P3617, DOI 10.1109/ICASSP.2019.8683143; Ren Y., 2019, ICML; Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779; van den Oord A., 2017, PARALLEL WAVENET FAS, P3918; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang YF, 2019, AAAI CONF ARTIF INTE, P1401; Wang YX, 2017, INTERSPEECH, P4006, DOI 10.21437/Interspeech.2017-1452; Wu Z., 2016, MERLIN OPEN SOURCE N, P202, DOI DOI 10.21437/SSW.2016-33	28	170	173	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303019
C	Slonim, N; Tishby, N		Solla, SA; Leen, TK; Muller, KR		Slonim, N; Tishby, N			Agglomerative information bottleneck	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We introduce a novel distributional clustering algorithm that maximizes the mutual information per cluster between data and given categories. This algorithm can be considered as a bottom up hard version of the recently introduced "Information Bottleneck Method". The algorithm is compared with the top-down soft version of the information bottleneck method and a relationship between the hard and soft results is established. We demonstrate the algorithm on the 20 Newsgroups data set. For a subset of two newsgroups we achieve compression by 3 orders of magnitudes loosing only 10% of the original mutual information.	Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Slonim, N (corresponding author), Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel.							BAKER LD, 1998, ACM SIGIR 98; Brown P. F., 1992, Computational Linguistics, V18, P467; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; ELYANIV R, 1998, ADV NEURAL INFORMATI; HOFMANN T, 1999, ADV NEURAL INFORMATI; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; PEREIRA F, 1993, 31ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P183; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; TISHBY N, 1998, UNPUB INFORMATION BO	9	170	171	1	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						617	623						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700088
C	Agustsson, E; Mentzer, F; Tschannen, M; Cavigelli, L; Timofte, R; Benini, L; Van Gool, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Agustsson, Eirikur; Mentzer, Fabian; Tschannen, Michael; Cavigelli, Lukas; Timofte, Radu; Benini, Luca; Van Gool, Luc			Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.	[Agustsson, Eirikur; Mentzer, Fabian; Tschannen, Michael; Cavigelli, Lukas; Timofte, Radu; Benini, Luca; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Timofte, Radu] Merantix, Berlin, Germany; [Van Gool, Luc] Katholieke Univ Leuven, Leuven, Belgium	Swiss Federal Institutes of Technology Domain; ETH Zurich; KU Leuven	Agustsson, E (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	aeirikur@vision.ee.ethz.ch; mentzerf@vision.ee.ethz.ch; michaelt@nari.ee.ethz.ch; cavigelli@iis.ee.ethz.ch; timofter@vision.ee.ethz.ch; benini@iis.ee.ethz.ch; vangool@vision.ee.ethz.ch	Cavigelli, Lukas/M-1836-2015; Timofte, Radu/AAK-6022-2021; Cavigelli, Lukas/T-6355-2019; Jeong, Yongwook/N-7413-2016	Cavigelli, Lukas/0000-0003-1767-7715; Timofte, Radu/0000-0002-1478-0402; BENINI, LUCA/0000-0001-8068-3806	EUs Horizon 2020 programme [687757 - REPLICATE]; NVIDIA Corporation; ETH Zurich; Armasuisse	EUs Horizon 2020 programme; NVIDIA Corporation; ETH Zurich(ETH Zurich); Armasuisse	This work was supported by EUs Horizon 2020 programme under grant agreement No 687757 - REPLICATE, by NVIDIA Corporation through the Academic Hardware Grant, by ETH Zurich, and by Armasuisse.	Allgower E.L., 2012, NUMERICAL CONTINUATI, V13; Balle Johannes, 2016, ARXIV160705006; Balle Johannes, 2016, ARXIV161101704; Choi Yoojin, 2016, ARXIV161201543; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; Hubara Itay, 2016, QUANTIZED NEURAL NET, P3; Johnston N, 2017, ARXIV170310114; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2011, P EUR S ART NEUR NET, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; ROSE K, 1992, IEEE T INFORM THEORY, V38, P1249, DOI 10.1109/18.144705; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shi W., 2016, ARXIV160907009; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Taubman DS, 2001, JPEG 2000 IMAGE COMP; Theis L., 2017, P INT C LEARN REPR I; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Toderici G., 2016, ARXIV160805148; Toderici George, 2015, ARXIV151106085; Ullrich K., 2017, 5 INT C LEARN REPPR; WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wen W, 2016, ADV NEUR IN, V29; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771; Wohlhart Paul, 2013, IEEE C COMP VIS PATT; YAIR E, 1992, IEEE T SIGNAL PROCES, V40, P294, DOI 10.1109/78.124940; Zhou A, 2017, INCREMENTAL NETWORK	41	167	170	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401018
C	Zhang, LF; Han, JQ; Wang, H; Saidi, WA; Car, R; E, WN		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Linfeng; Han, Jiequn; Wang, Han; Saidi, Wissam A.; Car, Roberto; E, Weinan			End-to-end Symmetry Preserving Inter-atomic Potential Energy Model for Finite and Extended Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MOLECULAR-DYNAMICS; GROWTH	Machine learning models are changing the paradigm of molecular modeling, which is a fundamental tool for material science, chemistry, and computational biology. Of particular interest is the inter-atomic potential energy surface (PES). Here we develop Deep Potential - Smooth Edition (DeepPot-SE), an end-to-end machine learning-based PES model, which is able to efficiently represent the PES of a wide variety of systems with the accuracy of ab initio quantum mechanics models. By construction, DeepPot-SE is extensive and continuously differentiable, scales linearly with system size, and preserves all the natural symmetries of the system. Further, we show that DeepPot-SE describes finite and extended systems including organic molecules, metals, semiconductors, and insulators with high fidelity.	[Zhang, Linfeng; Han, Jiequn; Car, Roberto; E, Weinan] Princeton Univ, Program Appl & Computat Math, Princeton, NJ 08544 USA; [Wang, Han] Inst Appl Phys & Computat Math, Beijing, Peoples R China; [Wang, Han] CAEP Software Ctr High Performance Numer Simulat, Beijing, Peoples R China; [Saidi, Wissam A.] Univ Pittsburgh, Dept Mech Engn & Mat Sci, Pittsburgh, PA USA; [Car, Roberto] Princeton Univ, Dept Chem, Princeton, NJ 08544 USA; [Car, Roberto] Princeton Univ, Dept Phys, Princeton, NJ 08544 USA; [Car, Roberto] Princeton Univ, Princeton Inst Sci & Technol Mat, Princeton, NJ 08544 USA; [E, Weinan] Princeton Univ, Dept Math, Princeton, NJ 08544 USA; [E, Weinan] Beijing Inst Big Data Res, Beijing, Peoples R China	Princeton University; Chinese Academy of Sciences; Institute of Applied Physics & Computational Mathematics - China; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Princeton University; Princeton University; Princeton University; Princeton University	Wang, H (corresponding author), Inst Appl Phys & Computat Math, Beijing, Peoples R China.; Wang, H (corresponding author), CAEP Software Ctr High Performance Numer Simulat, Beijing, Peoples R China.	wang_han@iapcm.ac.cn; alsaidi@pitt.edu; weinan@math.princeton.edu	Han, Jiequn/AAC-2489-2019	Han, Jiequn/0000-0002-3553-7313	ONR [N00014-13-1-0338]; DOE [DE-SC0008626, DE-SC0009248]; NSFC [U1430237, 91530322]; National Science Foundation of China [91530322, 11501039]; National Key Research and Development Program of China [2016YFB0201200, 2016YFB0201203]; Science Challenge Project [JCKY2016212A502]; National Science Foundation [DMR-1809085, NSF OCI-1053575]; Office of Science of the U.S. Department of Energy [DE-AC02-05CH11231]; Argonne Leadership Computing Facility, DOE Office of Science User Facility [DE-AC02-06CH11357]	ONR(Office of Naval Research); DOE(United States Department of Energy (DOE)); NSFC(National Natural Science Foundation of China (NSFC)); National Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; Science Challenge Project; National Science Foundation(National Science Foundation (NSF)); Office of Science of the U.S. Department of Energy(United States Department of Energy (DOE)); Argonne Leadership Computing Facility, DOE Office of Science User Facility	We thank the anonymous reviewers for their careful reading of our manuscript and insightful comments and suggestions. The work of L. Z., J. H., and W. E is supported in part by ONR grant N00014-13-1-0338, DOE grants DE-SC0008626 and DE-SC0009248, and NSFC grants U1430237 and 91530322. The work of R. C. is supported in part by DOE grant DE-SC0008626. The work of H. W. is supported by the National Science Foundation of China under Grants 11501039 and 91530322, the National Key Research and Development Program of China under Grants 2016YFB0201200 and 2016YFB0201203, and the Science Challenge Project No. JCKY2016212A502. W.A.S. acknowledges financial support from National Science Foundation (DMR-1809085). We are grateful for computing time provided in part by the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation (# NSF OCI-1053575), the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357, the National Energy Research Scientific Computing Center (NERSC), which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231, and the Terascale Infrastructure for Groundbreaking Research in Science and Engineering (TIGRESS) High Performance Computing Center and Visualization Laboratory at Princeton University.	Bartok AP, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1701816; Bartok AP, 2013, PHYS REV B, V87, DOI 10.1103/PhysRevB.87.184115; Bartok AP, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.136403; Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401; Cantor B, 2004, MAT SCI ENG A-STRUCT, V375, P213, DOI 10.1016/j.msea.2003.10.257; CAR R, 1985, PHYS REV LETT, V55, P2471, DOI 10.1103/PhysRevLett.55.2471; Chmiela S, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1603015; Cohen Taco, 2018, ICLR; DAW MS, 1984, PHYS REV B, V29, P6443, DOI 10.1103/PhysRevB.29.6443; Gilmer J, 2017, PR MACH LEARN RES, V70; Gong C, 2013, ACS NANO, V7, P11350, DOI 10.1021/nn4052138; Han J., 2018, ARXIV180707014; Han JQ, 2018, COMMUN COMPUT PHYS, V23, P629, DOI 10.4208/cicp.OA-2017-0213; Huang X, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms2472; Hutter J, 2014, WIRES COMPUT MOL SCI, V4, P15, DOI 10.1002/wcms.1159; Kingma D.P, P 3 INT C LEARNING R; Ko HY, 2018, PHYS REV MATER, V2, DOI 10.1103/PhysRevMaterials.2.055603; KOHN W, 1965, PHYS REV, V140, P1133, DOI 10.1103/PhysRev.140.A1133; Morawietz T, 2016, P NATL ACAD SCI USA, V113, P8368, DOI 10.1073/pnas.1602375113; Saidi WA, 2015, CRYST GROWTH DES, V15, P3190, DOI 10.1021/acs.cgd.5b00269; Saidi WA, 2015, CRYST GROWTH DES, V15, P642, DOI 10.1021/cg5013395; Saidi WA, 2014, J CHEM PHYS, V141, DOI 10.1063/1.4893875; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Schutt K.T., 2017, ADV NEURAL INF PROCE, V30, P992; Shi YL, 2018, J PHYS CHEM LETT, V9, P2972, DOI 10.1021/acs.jpclett.8b01233; Smith JS, 2017, CHEM SCI, V8, P3192, DOI 10.1039/c6sc05720a; Tkatchenko A, 2012, PHYS REV LETT, V108, DOI [10.1103/PhysRevLett.108.058301, 10.1103/PhysRevLett.108.236402]; Wang H, 2018, COMPUT PHYS COMMUN, V228, P178, DOI 10.1016/j.cpc.2018.03.016; Weyl H., 1939, CLASSICAL GROUPS THE; Yao K, 2017, J PHYS CHEM LETT, V8, P2689, DOI 10.1021/acs.jpclett.7b01072; Yeh JW, 2004, ADV ENG MATER, V6, P299, DOI 10.1002/adem.200300567; Zaheer Manzil, 2017, ADV NEURAL INFORM PR, V2, P8; Zhang F, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15687; Zhang LF, 2018, PHYS REV LETT, V120, DOI 10.1103/PhysRevLett.120.143001	35	165	168	16	25	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304045
C	Zhuang, ZW; Tan, MK; Zhuang, BH; Liu, J; Guo, Y; Wu, QY; Huang, JZ; Zhu, JH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhuang, Zhuangwei; Tan, Mingkui; Zhuang, Bohan; Liu, Jing; Guo, Yong; Wu, Qingyao; Huang, Junzhou; Zhu, Jinhui			Discrimination-aware Channel Pruning for Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either train from scratch with sparsity constraints on channels, or minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. In this paper, we investigate a simple-yet-effective method called discrimination-aware channel pruning (DCP) to choose those channels that really contribute to discriminative power. To this end, we introduce additional discrimination-aware losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error. Last, we propose a greedy algorithm to conduct channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example, on ILSVRC-12, our pruned ResNet-50 with 30% reduction of channels outperforms the baseline model by 0.39% in top-1 accuracy.	[Zhuang, Zhuangwei; Tan, Mingkui; Liu, Jing; Guo, Yong; Wu, Qingyao; Zhu, Jinhui] South China Univ Technol, Guangzhou, Guangdong, Peoples R China; [Zhuang, Bohan] Univ Adelaide, Adelaide, SA, Australia; [Huang, Junzhou] Univ Texas Arlington, Arlington, TX 76019 USA; [Huang, Junzhou] Tencent AI Lab, Bellevue, WA USA	South China University of Technology; University of Adelaide; University of Texas System; University of Texas Arlington	Tan, MK; Zhu, JH (corresponding author), South China Univ Technol, Guangzhou, Guangdong, Peoples R China.	z.zhuangwei@mail.scut.edu.cn; mingkuitan@scut.edu.cn; bohan.zhuang@adelaide.edu.au; seliujing@mail.scut.edu.cn; guo.yong@mail.scut.edu.cn; qyw@scut.edu.cn; jzhuang@uta.edu; csjhzhu@scut.edu.cn	Zhuang, Bohan/AAL-5022-2021; Tan, Mingkui/T-2667-2019; Liu, Jing/V-7155-2019	Zhuang, Bohan/0000-0002-0074-0303	National Natural Science Foundation of China (NSFC) [61876208, 61502177, 61602185]; Recruitment Program for Young Professionals, Guangdong Provincial Scientific and Technological funds [2017B090901008, 2017A010101011, 2017B090910005]; Fundamental Research Funds for the Central Universities [D2172480]; Pearl River S&T Nova Program of Guangzhou [201806010081]; CCF-Tencent Open Research Fund [RAGR20170105]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Recruitment Program for Young Professionals, Guangdong Provincial Scientific and Technological funds; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Pearl River S&T Nova Program of Guangzhou; CCF-Tencent Open Research Fund; Program for Guangdong Introducing Innovative and Enterpreneurial Teams	This work was supported by National Natural Science Foundation of China (NSFC) (61876208, 61502177 and 61602185), Recruitment Program for Young Professionals, Guangdong Provincial Scientific and Technological funds (2017B090901008, 2017A010101011, 2017B090910005), Fundamental Research Funds for the Central Universities D2172480, Pearl River S&T Nova Program of Guangzhou 201806010081, CCF-Tencent Open Research Fund RAGR20170105, and Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183.	Alvarez Jose M, 2016, ADV NEURAL INFORM PR, P2270; Bahmani S, 2013, J MACH LEARN RES, V14, P807; Cao JZ, 2018, PR MACH LEARN RES, V80; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dentinel Zarembaw, 2014, NEURIPS, P1269; Gong Yunchao, 2014, ARXIV14126115; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo YW, 2016, ADV NEUR IN, V29; Guo YC, 2018, AAAI CONF ARTIF INTE, P6870; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu H., 2016, ARXIV PREPRINT ARXIV; Huang Gary B., 2007, 0749 U MASS, P7; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1; Li H., 2017, P INT C LEARN REPR I, P1; Liu J, 2014, PR MACH LEARN RES, V32; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Nair V, 2010, P 27 INT C MACHINE L, P807; Parkhi Omkar M., 2015, BRIT MACH VIS C; Paszke Adam, 2017, PYTORCH TENSORS DYNA, P6; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Sindhwani V., 2015, ADV NEURAL INFORM PR, P3088; Srinivas S, 2017, IEEE COMPUT SOC CONF, P455, DOI 10.1109/CVPRW.2017.61; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Sun Y., 2015, ARXIV PREPRINT ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tan M., 2016, ARXIV161101773; Tan MK, 2015, IEEE T SIGNAL PROCES, V63, P727, DOI 10.1109/TSP.2014.2385036; Tan MK, 2014, J MACH LEARN RES, V15, P1371; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wen W, 2016, ADV NEUR IN, V29; Ye Jianbo, 2018, INT C LEARN REPR; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958; Yuan XT, 2014, PR MACH LEARN RES, V32, P127; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579; Zhou A, 2017, INCREMENTAL NETWORK; Zhou S., 2016, ARXIV160606160; Zhu Chenzhuo, 2017, ICLR; Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826	57	165	172	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300081
C	Adebayo, J; Gilmer, J; Muelly, M; Goodfellow, I; Hardt, M; Kim, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Adebayo, Julius; Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been			Sanity Checks for Saliency Maps	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings(2) .	[Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been] Google Brain, Mountain View, CA USA; [Hardt, Moritz] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Adebayo, Julius] Google Residency Program, Berkeley, CA USA	Google Incorporated; University of California System; University of California Berkeley	Adebayo, J (corresponding author), Google Residency Program, Berkeley, CA USA.	juliusad@mit.edu; gilmer@google.com; muelly@google.com; goodfellow@google.com; mrtz@google.com; beenkim@google.com						Alain Guillaume, 2016, ARXIV161001644; Alber Maximilian, 2018, INT C LEARN REPR; Ancona Marco, 2018, P 6 ICLR; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Casillas J., 2013, INTERPRETABILITY ISS, V128; Chen JB, 2018, PR MACH LEARN RES, V80; Dabkowski P, 2017, ADV NEUR IN, V30; Doshi-Velez F, 2017, ARXIV171101134; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Ghorbani A, 2017, ARXIV171010547; Goodman B, 2017, AI MAG, V38, P50, DOI 10.1609/aimag.v38i3.2741; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Lakkaraju H., 2017, ARXIV PREPRINT ARXIV; Lundberg SM, 2017, ADV NEUR IN, V30; Mahendran A, 2016, LECT NOTES COMPUT SC, V9910, P120, DOI 10.1007/978-3-319-46466-4_8; Meng Qingjie, 2018, AUTOMATIC SHADOW DET; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Nie Weili, 2018, ICML; Oztireli A. C., 2018, INT C LEARN REPR ICL; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Saxe A, 2011, P 28 INT C MACH LEAR, V28, P1089, DOI DOI 10.5555/3104482.3104619; Selvaraju Ramprasaath R, 2016, ARXIV161107450; Shrikumar Avanti, 2016, ARXIV160501713; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Smilkov D, 2017, ARXIV; Springenberg J.T., 2014, ARXIV14126806; Sundararajan M, 2017, PR MACH LEARN RES, V70; Ulyanov D, 2020, INT J COMPUT VISION, V128, P1867, DOI 10.1007/s11263-020-01303-4; Vellido A, 2012, P 20 INT S ART NEUR, V12, P163; Wang Fulton, 2015, ARXIV151005189; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang Chiyuan, 2017, P 5 ICLR; Zhu Xiaojin, 2016, ICML WORKSH REL MACH; Zintgraf Luisa M., 2017, P ICLR	36	162	162	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004010
C	Meila, M; Shi, JB		Leen, TK; Dietterich, TG; Tresp, V		Meila, M; Shi, JB			Learning segmentation by random walks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present a new view of image segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation. In particular, we prove that the Normalized Cut method arises naturally from our framework. Finally, the framework provides a principled method for learning the similarity function as a combination of features.	Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Meila, M (corresponding author), Univ Washington, Seattle, WA 98195 USA.							Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; KANNAN R, 2000, P 41 S FDN COMP SCI; Kemeny J. G., 1983, FINITE MARKOV CHAINS; KLEINBERG J, 1997, AUTHORITATIVE SOURCE; MAILA M, 2001, P INT WORKSH AI STAT; MALIK J, 2000, INT J COMPUTER VISIO; Perona P., 1998, EUR C COMP VIS EUR C COMP VIS; SCOTT GL, 1990, P BRIT MACH VIS C; Shi J., 2000, IEEE T PATTERN ANAL; WEISS YY, 1999, INT C COMP VIS	10	161	169	1	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						873	879						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800123
C	Luo, RQ; Tian, F; Qin, T; Chen, EH; Liu, TY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Luo, Renqian; Tian, Fei; Qin, Tao; Chen, Enhong; Liu, Tie-Yan			Neural Architecture Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 2.11% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. The best discovered architectures on both tasks are successfully transferred to other tasks such as CIFAR-100 and WildText-2. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 3.53%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.	[Luo, Renqian; Chen, Enhong] Univ Sci & Technol China, Hefei, Anhui, Peoples R China; [Tian, Fei; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft	Luo, RQ (corresponding author), Univ Sci & Technol China, Hefei, Anhui, Peoples R China.	lrq@mail.ustc.edu.cn; fetia@microsoft.com; taoqin@microsoft.com; cheneh@ustc.edu.cn; tie-yan.liu@microsoft.com		Qin, Tao/0000-0002-9095-0776				[Anonymous], 2018, INT C LEARN REPR; [Anonymous], 2017, ARXIV170300548; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Baker B., 2017, INT C LEARN REPR; Baker Bowen, 2018, WORKSHOP TRACK P; Bender G, 2018, PR MACH LEARN RES, V80; Cai H., 2017, ARXIV170704873; Cho K., 2014, P 2014 C EMP METH NA, P1724; Deng Boyang, 2017, ARXIV171203351; DeVries T., 2017, P 2017 COMPUTER VISI; Fahlman S.E., 1990, ADV NEURAL INFORM PR, P524; Gal Y., 2016, ADV NEURAL INFORM PR, P1019; Gastaldi X., 2017, CORR; Grave Edouard, 2016, ARXIV161204426; Grosse Roger, 2012, 28 C UNC ART INT CAT, P306; Han Cai, 2018, ARXIV180602639; Huang F., 2017, ARXIV170604964; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Inan H., 2016, ARXIV161101462; Labaka G, 2018, INT C LEARN REPR; Liu C., 2017, ARXIV171200559; Liu H., 2018, ARXIV180609055; Liu Hanxiao, 2018, INT C LEARN REPR; Maharaj T., 2016, ARXIV160601305; Melis G, 2018, INT C LEARN REPR ICL; Merity Stephen, 2017, CORR; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Pham H, 2018, 35 INT C MACH LEARN; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Real E., 2018, ARXIV180201548; Real E, 2017, PR MACH LEARN RES, V70; Simonyan K., 2018, INT C LEARN REPR; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Sutskever I., 2014, ARXIV; Wu Lijun, 2018, ADV NEURAL INFORM PR, P6465; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xing EP, 2018, ARXIV180207191; Yang Fan, 2018, INT C LEARN REPR; Yang Z., 2018, BREAKING SOFTMAX BOT; Zoph B., 2016, ICLR; Zoph Barret, 2018, P IEEE C COMP VIS PA	46	160	164	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002037
C	Dugas, C; Bengio, Y; Belisle, F; Nadeau, C; Garcia, R		Leen, TK; Dietterich, TG; Tresp, V		Dugas, C; Bengio, Y; Belisle, F; Nadeau, C; Garcia, R			Incorporating second-order functional knowledge for better option pricing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.	CIRANO, Montreal, PQ H3A 2A5, Canada	Universite de Montreal	Dugas, C (corresponding author), CIRANO, Montreal, PQ H3A 2A5, Canada.	dugas@iro.umontreal.ca; bengioy@iro.umontreal.ca; belislfr@iro.umontreal.ca; nadeauc@iro.umontreal.ca; garciar@cirano.qc.ca						Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Cybenko G., 1988, CONTINUOUS VALUED NE; DUGAS C, 2000, 1176 U MONTR DEP INF; GARCIA R, 1998, 98S35 CIRANO; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Moody J, 1994, STAT NEURAL NETWORKS	6	160	162	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						472	478						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800067
C	Ge, YX; Li, ZW; Zhao, HY; Yin, GJ; Yi, S; Wang, XG; Li, HS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ge, Yixiao; Li, Zhuowan; Zhao, Haiyu; Yin, Guojun; Yi, Shuai; Wang, Xiaogang; Li, Hongsheng			FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.(double dagger double dagger)	[Ge, Yixiao; Wang, Xiaogang; Li, Hongsheng] Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China; [Li, Zhuowan; Zhao, Haiyu; Yin, Guojun; Yi, Shuai] SenseTime Res, Hong Kong, Peoples R China; [Li, Zhuowan] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Yin, Guojun] Univ Sci & Technol China, Hefei, Anhui, Peoples R China	Chinese University of Hong Kong; Johns Hopkins University; Chinese Academy of Sciences; University of Science & Technology of China, CAS	Li, HS (corresponding author), Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China.	yxge@link.cuhk.edu.hk; zli110@jhu.edu; zhaohaiyu@sensetime.com; gjyin@mail.ustc.edu.cn; yishuai@sensetime.com; xgwang@ee.cuhk.edu.hk; hsli@ee.cuhk.edu.hk			SenseTime Group Limited; General Research Fund - Research Grants Council of Hong Kong [CUHK14213616, CUHK14206114, CUHK14205615, CUHK14203015, CUHK14239816, CUHK419412, CUHK14207814, CUHK14208417, CUHK14202217]; Hong Kong Innovation and Technology Support Program [ITS/121/15FX]	SenseTime Group Limited; General Research Fund - Research Grants Council of Hong Kong(Hong Kong Research Grants Council); Hong Kong Innovation and Technology Support Program	This work is supported by SenseTime Group Limited, the General Research Fund sponsored by the Research Grants Council of Hong Kong (Nos. CUHK14213616, CUHK14206114, CUHK14205615, CUHK14203015, CUHK14239816, CUHK419412, CUHK14207814, CUHK14208417, CUHK14202217), the Hong Kong Innovation and Technology Support Program (No. ITS/121/15FX).	Bak S, 2017, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR.2017.171; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696; CHEN DP, 2016, CVPR, P1268, DOI DOI 10.1109/CVPR.2016.142; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kaneko T, 2017, PROC CVPR IEEE, P7006, DOI 10.1109/CVPR.2017.741; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211; Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Ma LQ, 2017, ADV NEUR IN, V30; Mirza M., 2014, ARXIV; Nowozin S, 2016, ADV NEUR IN, V29; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Salimans T, 2016, ADV NEUR IN, V29; Shen Yantao, 2018, PERSON REIDENTIFICAT; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Shi ZY, 2015, PROC CVPR IEEE, P4184, DOI 10.1109/CVPR.2015.7299046; Siarohin A., 2017, CVPR; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Tran L., 2017, CVPR; Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; Zanfir M, 2018, PROC CVPR IEEE, P5391, DOI 10.1109/CVPR.2018.00565; Zhang H., 2017, ICCV; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783; Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhou Jiahuan, 2017, CVPR	48	158	164	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301023
C	Li, CX; Xu, K; Zhu, J; Zhang, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Chongxuan; Xu, Kun; Zhu, Jun; Zhang, Bo			Triple Generative Adversarial Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players-a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.	[Li, Chongxuan; Xu, Kun; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Ctr Bioinspired Comp Res, Dept Comp Sci & Tech, TNList Lab,State Key Lab Intell Tech & Sys, Beijing 100084, Peoples R China	Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, Ctr Bioinspired Comp Res, Dept Comp Sci & Tech, TNList Lab,State Key Lab Intell Tech & Sys, Beijing 100084, Peoples R China.	licx14@mails.tsinghua.edu.cn; xu-k16@mails.tsinghua.edu.cn; dcszj@mail.tsinghua.edu.cn; dcszb@mail.tsinghua.edu.cn	Jeong, Yongwook/N-7413-2016		National NSF of China [61620106010, 61621136008, 61332007]; MIIT Grant of Int. Man. Comp. Stan [2016ZXFB00001]; Youth Top-notch Talent Support Program; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Project from Siemens	National NSF of China(National Natural Science Foundation of China (NSFC)); MIIT Grant of Int. Man. Comp. Stan; Youth Top-notch Talent Support Program; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Project from Siemens	The work is supported by the National NSF of China (Nos. 61620106010, 61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No. 2016ZXFB00001), the Youth Top-notch Talent Support Program, Tsinghua Tiangong Institute for Intelligent Computing, the NVIDIA NVAIL Program and a Project from Siemens.	Denton Emily L, 2015, NEURIPS, V2, P4; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Laine Samuli, 2016, ARXIV161002242; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Chongxuan, 2016, ARXIV161107119; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Maaloe L, 2016, PR MACH LEARN RES, V48; Miyato T, 2015, ARXIV150700677; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Odena A., 2016, SEMISUPERVISED LEARN; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salimans T, 2016, ADV NEUR IN, V29; Springenberg Jost Tobias, 2015, ARXIV151106390; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Theano Development Team, 2016, ARXIV160502688 THEAN; Theis Lucas, 2015, ARXIV151101844; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yang Jimei, 2015, NIPS	29	158	166	2	16	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404016
C	Louizos, C; Ullrich, K; Welling, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Louizos, Christos; Ullrich, Karen; Welling, Max			Bayesian Compression for Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SHRINKAGE; SELECTION; NETWORKS	Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.	[Louizos, Christos] Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands; [Ullrich, Karen] Univ Amsterdam, Amsterdam, Netherlands; [Welling, Max] Univ Amsterdam, CIFAR, Amsterdam, Netherlands	University of Amsterdam; University of Amsterdam; University of Amsterdam	Louizos, C (corresponding author), Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands.	c.louizos@uva.nl; k.ullrich@uva.nl; m.welling@uva.nl	Jeong, Yongwook/N-7413-2016		TNO; NWO; Google	TNO; NWO(Netherlands Organization for Scientific Research (NWO)); Google(Google Incorporated)	We would like to thank Dmitry Molchanov, Dmitry Vetrov, Klamer Schutte and Dennis Koelma for valuable discussions and feedback. This research was supported by TNO, NWO and Google.	Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; Armagan Artin, 2011, Adv Neural Inf Process Syst, V24, P523; Azarkhish Erfan, 2017, ARXIV170106420; BEALE EML, 1959, ANN MATH STAT, V30, P1145, DOI 10.1214/aoms/1177706099; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Burges, 1998, MNIST DATABASE HANDW; Carvalho CM, 2010, BIOMETRIKA, V97, P465, DOI 10.1093/biomet/asq017; Chai S., 2017, ARXIV170308595; Chen W., 2015, ARXIV PREPRINT ARXIV; Courbariaux M., 2014, ARXIV PREPRINT ARXIV; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Dong XY, 2017, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR.2017.205; Figueiredo MAT, 2002, ADV NEUR IN, V14, P697; Gal Y, 2016, PR MACH LEARN RES, V48; Gong Y, 2015, ICLR; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Grunwald P. D., 2007, MINIMUM DESCRIPTION; Guo YW, 2016, ADV NEUR IN, V29; Gupta S., 2015, ABS150202551 CORR, P392; Gysel P., 2016, THESIS; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; Hinton G., 2015, ARXIV150302531; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Honkela A, 2004, IEEE T NEURAL NETWOR, V15, P800, DOI 10.1109/TNN.2004.828762; Howard A.G., 2017, MOBILENETS EFFICIENT; Iandola Forrest, 2017, 2017 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS), DOI 10.1145/3125502.3125606; Ingraham J. B., 2016, ARXIV160203807; Karaletsos T., 2015, ARXIV150507765; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lawrence ND, 2002, PERSP NEURAL COMP, P128; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin D. D., 2016, WORKSH ICML; Lin D.D., 2015, ARXIV PREPRINT ARXIV; Louizos C., 2018, ICLR; Louizos C., 2015, THESIS U AMSTERDAM A; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; Mellempudi N., 2017, ARXIV170501462; Merolla P., 2016, ARXIV160601981; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Molchanov D, 2017, PR MACH LEARN RES, V70; Neal R. M., 2012, BAYESIAN LEARNING NE; Neville SE, 2014, ELECTRON J STAT, V8, P1113, DOI 10.1214/14-EJS910; Peterson C., 1987, Complex Systems, V1, P995; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; RISSANEN J, 1986, ANN STAT, V14, P1080, DOI 10.1214/aos/1176350051; Scardapane Simone, 2016, ARXIV160700485; Shi S., 2017, ARXIV170407724; Smyth, 2015, ARXIV150603208; Sonderby C. K., 2016, 33 INT C MACH LEARN; Srinivas S., 2016, ARXIV161106791; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Ullrich Karen, 2017, ICLR; Venkatesh G., 2016, ARXIV161000324; Wallace C. S., 1990, ICCI 90 ADV COMPUTIN, P72; Wen W, 2016, ADV NEUR IN, V29; Yang TJ, 2017, PROC CVPR IEEE, P6071, DOI 10.1109/CVPR.2017.643; Zhu Chenzhuo, 2017, ICLR	72	158	160	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403035
C	Smola, AJ; Bartlett, P		Leen, TK; Dietterich, TG; Tresp, V		Smola, AJ; Bartlett, P			Sparse greedy Gaussian process regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n(2)m), storage is O(nm), the cost for prediction is O(n) and the cost to compute confidence bounds is O(nm), where n much less than m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems.	Australian Natl Univ, RSISE, Canberra, ACT 0200, Australia	Australian National University	Smola, AJ (corresponding author), Australian Natl Univ, RSISE, GPO Box 4, Canberra, ACT 0200, Australia.			Bartlett, Peter/0000-0002-8760-3140				FINE S, 2000, EFFICIENT SVM TRAINI; Gibbs M., 1997, EFFICIENT IMPLEMENTA; Girosi F, 1998, NEURAL COMPUT, V10, P1455, DOI 10.1162/089976698300017269; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; WILLIAMS CKI, 2000, P 17 INT C MACH LEAR, P1159	8	157	160	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						619	625						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800088
C	Tenenbaum, JB		Jordan, MI; Kearns, MJ; Solla, SA		Tenenbaum, JB			Mapping a manifold of perceptual observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Nonlinear dimensionality reduction is formulated here as the problem of trying to find a Euclidean feature-space embedding of a set of observations that preserves as closely as possible their intrinsic metric structure - the distances between points on the observation manifold as measured along geodesic paths. Our isometric feature mapping procedure, or isomap, is able to reliably recover low-dimensional nonlinear structure in realistic perceptual data sets, such as a manifold of face images, where conventional global mapping methods find only local minima. The recovered map provides a canonical set of globally meaningful features, which allows perceptual transformations such as interpolation, extrapolation, and analogy - highly nonlinear transformations in the original observation space - to be computed with simple linear operations in feature space.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Tenenbaum, JB (corresponding author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.								0	157	165	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						682	688						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700097
C	Miller, DJ; Uyar, HS		Mozer, MC; Jordan, MI; Petsche, T		Miller, DJ; Uyar, HS			A mixture of experts classifier with learning based on both labelled and unlabelled data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We address statistical classifier design given a mixed training set consisting of a small labelled feature set and a (generally larger) set of unlabelled features. This situation arises, e.g., for medical images, where although training features may be plentiful, expensive expertise is required to extract their class labels. We propose a classifier structure and learning algorithm that make effective use of unlabelled data to improve performance. The learning is based on maximization of the total data Likelihood, i.e. over both the labelled and unlabelled data subsets. Two distinct EM learning algorithms are proposed, differing in the EM formalism applied for unlabelled data. The classifier, based on a joint probability model for features and labels, is a ''mixture of experts'' structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training. The scope of application for the new method is greatly extended by the observation that test data, or any new data to classify, is in fact additional, unlabelled data - thus, a combined learning/classification operation - much akin to what is done in image segmentation - can be invoked whenever there is new data to classify. Experiments with data sets from the UC Irvine database demonstrate that the new learning algorithms and structure achieve substantial performance gains over alternative approaches.			Miller, DJ (corresponding author), PENN STATE UNIV,DEPT ELECT ENGN,UNIVERSITY PK,PA 16802, USA.								0	157	177	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						571	577						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00081
C	Blei, DM; Ng, AY; Jordan, MI		Dietterich, TG; Becker, S; Ghahramani, Z		Blei, DM; Ng, AY; Jordan, MI			Latent Dirichlet allocation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.	Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Blei, DM (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.		Nikolenko, Sergey/AAU-3615-2020; Nikolenko, Sergey I/I-7696-2013; Jordan, Michael I/C-5253-2013	Nikolenko, Sergey/0000-0001-7787-2251; Nikolenko, Sergey I/0000-0001-7787-2251; 				COHN D, 2001, ADV NEURAL INFORMATI, V13; GREEN PJ, 1998, MODELLING HETEROGENE; Hofmann Thomas, 1999, INT ACM SIG C RES DE; Jiang Thomas J., 1992, J COMPUT GRAPH STAT, V1, P231; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; POPESCUL A, 2001, UNCERTAINTY ARTIFICI	7	156	162	6	68	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						601	608						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100075
C	Shu, J; Xie, Q; Yi, LX; Zhao, Q; Zhou, SP; Xu, ZB; Meng, DY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shu, Jun; Xie, Qi; Yi, Lixuan; Zhao, Qian; Zhou, Sanping; Xu, Zongben; Meng, Deyu			Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net.	[Shu, Jun; Xie, Qi; Yi, Lixuan; Zhao, Qian; Zhou, Sanping; Xu, Zongben; Meng, Deyu] Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China; [Meng, Deyu] Macau Univ Sci & Technol, Taipa, Macao, Peoples R China	Xi'an Jiaotong University; Macau University of Science & Technology	Meng, DY (corresponding author), Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China.; Meng, DY (corresponding author), Macau Univ Sci & Technol, Taipa, Macao, Peoples R China.	dymeng@mail.xjtu.edu.cn			China NSFC [61661166011, 11690011, 61603292, 61721002, U1811461]	China NSFC(National Natural Science Foundation of China (NSFC))	This research was supported by the China NSFC projects under contracts 61661166011, 11690011, 61603292, 61721002,U1811461. The authors would also like to thank anonymous reviewers for their constructive suggestions on improving the paper, especially on the proofs and theoretical analysis of our paper.	Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2018, SMALL SAMPLE LEARNIN; Arpit D, 2017, PR MACH LEARN RES, V70; Azadi S., 2016, ICLR; Bi Wei, 2014, UAI; Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011; Chang H. -S., 2017, ADV NEURAL INFORM PR, V30, P1002; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Chen CW, 2018, IEEE INT SYMP CIRC S, DOI [10.1109/ISCAS.2018.8351436, 10.1145/3265723.3265730]; Csaji B. C., 2001, THESIS; Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432; Cui Yin, 2019, CVPR, P9268; De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986; Dehghani Mostafa, 2017, NEURIPS WORKSH; Dehghani Mostafa, 2018, ICLR; Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205; Elkan C., 2001, INT JOINT C ART INT, V17, P973, DOI DOI 10.5555/1642194.1642224; Eshratifar A. E., 2018, ARXIV181008178; Fan Y., 2018, ARXIV PREPRINT ARXIV; Finn C, 2017, PR MACH LEARN RES, V70; Franceschi L, 2018, PR MACH LEARN RES, V80; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Galar M, 2012, IEEE T SYST MAN CY C, V42, P463, DOI 10.1109/TSMCC.2011.2161285; Gold JR, 2017, PLAN HIST ENVIRON SE, P1; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; He Haibo, 2008, IEEE TRANSACTIONS ON; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hendrycks Dan, 2018, ADV NEURAL INFORM PR, P10456; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Jiang L., 2018, ICML; Jiang L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P547, DOI 10.1145/2647868.2654918; Jiang Lu, 2014, NEURIPS; Johnson JM, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0192-5; Kawaguchi K., 2017, ARXIV171005468V4STAT, DOI DOI 10.2196/jmir.5870; Khan Salman H, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3573, DOI 10.1109/TNNLS.2017.2732482; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112; Li YH, 2017, IEEE I CONF COMP VIS, P2098, DOI 10.1109/ICCV.2017.229; Ma XJ, 2018, PR MACH LEARN RES, V80; Mairal Julien, 2013, NEURIPS; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Nichol Alex, 2018, ARXIV180302999; Novak R., 2018, ICLR; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240; Ravi S., 2017, INT C LEARN REPR, P12; Reed Scott E., 2015, P 3 INT C LEARN REPR; Ren Mengye, 2018, ARXIV PREPRINT ARXIV; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sukhbaatar Sainbayar, 2015, ICLR; Sun YM, 2007, PATTERN RECOGN, V40, P3358, DOI 10.1016/j.patcog.2007.04.009; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; Vandat A, 2017, ADV NEUR IN, V30; Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696; Wang YB, 2017, ADV NEUR IN, V30; Wu LJ, 2018, ADV NEUR IN, V31; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Yao JC, 2019, AAAI CONF ARTIF INTE, P9103; Zadrozny  B., 2004, INT C MACH LEARN ICM, DOI 10.1145/1015330.1015425; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhang Xiao, 2017, ICCV; Zhang ZL, 2018, ADV NEUR IN, V31; Zhuang BH, 2017, PROC CVPR IEEE, P2915, DOI 10.1109/CVPR.2017.311	69	155	157	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301086
C	Lin, J; Rao, YM; Lu, JW; Zhou, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lin, Ji; Rao, Yongming; Lu, Jiwen; Zhou, Jie			Runtime Neural Pruning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. Unlike existing neural pruning methods which produce a fixed pruned model for deployment, our method preserves the full ability of the original network and conducts pruning according to the input image and current feature maps adaptively. The pruning is performed in a bottom-up, layer-by-layer manner, which we model as a Markov decision process and use reinforcement learning for training. The agent judges the importance of each convolutional kernel and conducts channel-wise pruning conditioned on different samples, where the network is pruned more when the image is easier for the task. Since the ability of network is fully preserved, the balance point is easily adjustable according to the available resources. Our method can be applied to off-the-shelf network structures and reach a better tradeoff between speed and accuracy, especially with a large pruning rate.	[Lin, Ji; Rao, Yongming; Lu, Jiwen; Zhou, Jie] Tsinghua Univ, Dept Automat, Beijing, Peoples R China	Tsinghua University	Lin, J (corresponding author), Tsinghua Univ, Dept Automat, Beijing, Peoples R China.	lin-j14@mails.tsinghua.edu.cn; raoyongming95@gmail.com; lujiwen@tsinghua.edu.cn; zhou@tsnghua.edu.cn	Rao, Yongming/U-8310-2019; Jeong, Yongwook/N-7413-2016; Lu, Jiwen/C-5291-2009	Rao, Yongming/0000-0003-3952-8753; Lu, Jiwen/0000-0002-6121-5529	National Natural Science Foundation of China [61672306]; National 1000 Young Talents Plan Program	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National 1000 Young Talents Plan Program	We would like to thank Song Han, Huazhe (Harry) Xu, Xiangyu Zhang and Jian Sun for their generous help and insightful advice. This work is supported by the National Natural Science Foundation of China under Grants 61672306 and the National 1000 Young Talents Plan Program. The corresponding author of this work is Jiwen Lu.	Almahairi Amjad, 2015, ARXIV151107838; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 1989, ADV NEURAL INFORM PR; Anwar Sajid, 2015, ARXIV151208571; BELLMAN R, 1956, P NATL ACAD SCI USA, V42, P767, DOI 10.1073/pnas.42.10.767; Benbouzid Djalel, 2012, ARXIV12066387; Bengio Emmanuel, 2015, CORR; Bengio Yoshua, 2013, ARXIV13083432; Bolukbasi T, 2017, PR MACH LEARN RES, V70; Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286; Chetlur S., 2014, ARXIV; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Figurnov Michael, 2016, ARXIV161202297; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He, 2012, ADV NEURAL INFORM PR, P3149; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu H., 2016, ARXIV PREPRINT ARXIV; Huang Gary B., 2007, 0749 U MASS, P7; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jia Y., 2014, P 22 ACM INT C MULT, P675; Karayev S., 2012, ADV NEURAL INFORM PR, V25, P890; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kumar Neeraj, 2011, IEEE Trans Pattern Anal Mach Intell, V33, P1962, DOI 10.1109/TPAMI.2011.48; Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Leroux Sam, 2017, KNOWL INF SYST, P1; Littman ML, 2015, NATURE, V521, P445, DOI 10.1038/nature14540; Liu Lanlan, 2017, ARXIV170100299; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Murray K., 2015, ARXIV150805051; Odena Augustus, 2017, ARXIV170207780; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Strom N., 1997, FREE SPEECH J, V5, P2; Sun C, 2016, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2016.379; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Wen W, 2016, ADV NEUR IN, V29; Zhang F., 2015, ARXIV151103791; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579	46	155	160	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402023
C	Liu, D; Wen, BH; Fan, YC; Loy, CC; Huang, TS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Ding; Wen, Bihan; Fan, Yuchen; Loy, Chen Change; Huang, Thomas S.			Non-Local Recurrent Network for Image Restoration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many classic methods have shown non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are: (1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. (2) We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images. (3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice [41] that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with many fewer parameters. The code is available at https://github.com/Ding-Liu/NLRN.	[Liu, Ding; Wen, Bihan; Fan, Yuchen; Huang, Thomas S.] Univ Illinois, Champaign, IL 61820 USA; [Loy, Chen Change] Nanyang Technol Univ, Singapore, Singapore	University of Illinois System; University of Illinois Urbana-Champaign; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Liu, D (corresponding author), Univ Illinois, Champaign, IL 61820 USA.	dingliu2@illinois.edu; bwen3@illinois.edu; yuchenf4@illinois.edu; ccloy@ntu.edu.sg; t-huang1@illinois.edu	Wen, Bihan/B-3123-2017	Wen, Bihan/0000-0002-6874-6453; Loy, Chen Change/0000-0001-5345-1591				[Anonymous], 2017, ADV NEURAL INFORM PR; Bevilacqua Marco, 2012, LOW COMPLEXITY SINGL; Buades A., 2005, CVPR; Burger H.C., 2012, CVPR; Chandra S., 2017, ICCV; Chang H., 2004, CVPR; Chen F., 2015, ICCV; Chen Y.-C., 2017, IEEE TPAMI, V2, P7; Dabov K., 2007, IEEE TIP; Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954; Dong C., 2016, ECCV; Dong Chao, 2014, ECCV; Gehring Jonas, 2017, ICML; Glasner D., 2009, ICCV; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Han W., 2018, CVPR; Harley A. W., 2017, ICCV; He K., 2016, ECCV; Huang Jia-Bin, 2015, CVPR; Kim Jiwon, 2016, CVPR; Lai W., 2017, CVPR; Lefkimmiatis S., 2017, CVPR; Lim B., 2017, CVPR WORKSH; Liu D, 2016, IEEE T IMAGE PROCESS, V25, P3194, DOI 10.1109/TIP.2016.2564643; Liu Ding, 2018, IJCAI; Mairal J., 2009, ICCV; Mao X, 2016, NIPS; Martin D. R., 2001, ICCV; Qiao P., 2017, ACM MULT C; Rudin L. I., 1994, ICIP; Santoro A, 2017, NIPS; Schulter S., 2015, CVPR; Shi W., 2016, CVPR; Tai Y., 2017, CVPR; Tai Y., 2017, ICCV; Timofte R., 2013, ICCV; Tomasi C., 1998, BILATERAL FILTERING; Tong T., 2017, ICCV; Wang X., 2017, ARXIV171107971; Wang Z., 2004, IEEE TIP; Wang Zhaowen, 2015, ICCV; Wen B., 2015, IJCV; Xu Jun, 2015, ICCV; Yang J., 2010, IEEE TIP; Zeyde R., 2010, INT C CURV SURF; Zhang K, 2017, CVPR; Zhang Kaihao, 2017, IEEE TIP; Zheng S., 2015, ICCV; Zoran Daniel, 2011, ICCV	51	153	158	1	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301064
C	Girdhar, R; Ramanan, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Girdhar, Rohit; Ramanan, Deva			Attentional Pooling for Action Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII dataset with 12.5% relative improvement. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.	[Girdhar, Rohit; Ramanan, Deva] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Girdhar, R (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.		Jeong, Yongwook/N-7413-2016		National Science Foundation (NSF) [CNS-1518865, IIS-1618903]; Defense Advanced Research Projects Agency (DARPA) [HR001117C0051]; Intel Science and Technology Center for Visual Cloud Systems (ISTC-VCS)	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Intel Science and Technology Center for Visual Cloud Systems (ISTC-VCS)	Authors would like to thank Olga Russakovsky for initial review. This research was supported in part by the National Science Foundation (NSF) under grant numbers CNS-1518865 and IIS-1618903, and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0051. Additional support was provided by the Intel Science and Technology Center for Visual Cloud Systems (ISTC-VCS). Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view(s) of their employers or the above-mentioned funding sources.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baluch F., 2011, TRENDS NEUROSCIENCES; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Chao YW, 2015, IEEE I CONF COMP VIS, P1017, DOI 10.1109/ICCV.2015.122; Cheron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368; Courtney PG, 2015, IEEE COMP SEMICON; Delaitre V., 2010, BMVC, DOI DOI 10.5244/C.24.97; Delaitre Vincent, 2011, NIPS; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Everingham Mark, 2010, IJCV; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Feichtenhofer Christoph, 2016, NIPS; Fowlkes C., 2010, P IEEE COMP SOC C CO, P9; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Girdhar Rohit, 2017, PROC CVPR IEEE, P971, DOI DOI 10.1109/CVPR.2017.337; Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Huang X., 2015, ICCV; Jiang Y., 2013, THUMOS CHALLENGE ACT; Kay W., 2017, ARXIV PREPRINT ARXIV; Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008; Lim W., 2017, P INT C LEARN REPR; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Maji S., 2011, CVPR, DOI DOI 10.1109/CVPR.2011.5995631; Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25; Navalpakkam V., 2006, CVPR; Perronnin F, 2007, PROC CVPR IEEE, P2272; Piergiovanni A., 2017, AAAI; Pishchulin L, 2014, LECT NOTES COMPUT SC, V8753, P678, DOI 10.1007/978-3-319-11752-2_56; Ramanan D., 2003, NIPS; Ronchi M. R., 2015, BMVC; Rutishauser U., 2004, P 2004 IEEE COMP SOC, DOI [DOI 10.1109/CVPR.2004.1315142, 10.1109/cvpr.2004.1315142]; Santoro A, 2017, ADV NEUR IN, V30; Sharmin S, 2016, CLOUD BASED DYNAMIC, P1; Shi Y., 2016, ARXIV161105215; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Song SJ, 2017, AAAI CONF ARTIF INTE, P4263; Soomro K., 2012, ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Ullman S., 1984, COGNITION; Varol Gul, 2016, ARXIV160404494; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Yang W., 2010, CVPR; Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386; Yao Bangpeng, 2010, CVPR, DOI DOI 10.1109/CVPR.2010.5540234; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316	56	152	152	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400004
C	Wu, JJ; Lu, E; Kohli, P; Freeman, WT; Tenenbaum, JB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Jiajun; Lu, Erika; Kohli, Pushmeet; Freeman, William T.; Tenenbaum, Joshua B.			Learning to See Physics via Visual De-animation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a paradigm for understanding physical scenes without human annotations. At the core of our system is a physical world representation that is first recovered by a perception module and then utilized by physics and graphics engines. During training, the perception module and the generative models learn by visual de-animation - interpreting and reconstructing the visual information stream. During testing, the system first recovers the physical world state, and then uses the generative models for reasoning and future prediction. Even more so than forward simulation, inverting a physics or graphics engine is a computationally hard problem; we overcome this challenge by using a convolutional inversion network. Our system quickly recognizes the physical world state from appearance and motion cues, and has the flexibility to incorporate both differentiable and non-differentiable physics and graphics engines. We evaluate our system on both synthetic and real datasets involving multiple physical scenes, and demonstrate that our system performs well on both physical state estimation and reasoning problems. We further show that the knowledge learned on the synthetic dataset generalizes to constrained real images.	[Wu, Jiajun; Freeman, William T.; Tenenbaum, Joshua B.] MIT CSAIL, Cambridge, MA 02139 USA; [Lu, Erika] Univ Oxford, Oxford, England; [Kohli, Pushmeet] DeepMind, London, England; [Freeman, William T.] Google Res, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); University of Oxford; Google Incorporated	Wu, JJ (corresponding author), MIT CSAIL, Cambridge, MA 02139 USA.		Wu, JiaJun/GQH-7885-2022; Jeong, Yongwook/N-7413-2016		NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Center for Brain, Minds and Machines (NSF) [1231216]; Toyota Research Institute; Shell; MIT Advanced Undergraduate Research Opportunities Program (SuperUROP); Samsung	NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); Center for Brain, Minds and Machines (NSF); Toyota Research Institute; Shell(Royal Dutch Shell); MIT Advanced Undergraduate Research Opportunities Program (SuperUROP); Samsung(Samsung)	We thank Michael Chang, Donglai Wei, and Joseph Lim for helpful discussions. This work is supported by NSF #1212849 and #1447476, ONR MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF #1231216), Toyota Research Institute, Samsung, Shell, and the MIT Advanced Undergraduate Research Opportunities Program (SuperUROP).	Agrawal P., 2016, ADV NEURAL INFORM PR, P5074; [Anonymous], 2010, BULLET PHYS ENGINE; Ba J., 2015, ICLR 2015 C TRACK P; Bai JM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185562; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Battaglia Peter W, 2016, ARXIV161200222; Brubaker MA, 2010, INT J COMPUT VISION, V87, P140, DOI 10.1007/s11263-009-0274-5; Chang MB., 2017, 5 INT C LEARN REPR I; Collobert R., 2011, NIPS; Denil M., 2017, P INT C LEARN REPR; Ehrhardt S., 2017, ARXIV170300247; Eslami SM, 2016, NEURIPS, V1; Fragkiadaki K., 2016, 6 INT C LEARN REPR I; Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13; Hamrick J.B., 2017, INT C LEARN REPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Huang Jonathan, 2015, ICLR WORKSH; Jia ZY, 2015, IEEE T PATTERN ANAL, V37, P905, DOI 10.1109/TPAMI.2014.2359435; Kitani KM, 2017, COMPUT VIS PATT REC, P273, DOI 10.1016/B978-0-12-809276-7.00014-X; Kulkarni TD, 2015, CVPR; Kulkarni TD, 2015, ADV NEUR IN, V28; Kyriazis N, 2013, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2013.9; Lerer A, 2016, PR MACH LEARN RES, V48; Mathieu Michael, 2016, ICLR; Mnih A, 2016, PR MACH LEARN RES, V48; Mottaghi R., 2016, CVPR; Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17; Pinto L, 2016, LECT NOTES COMPUT SC, V9906, P3, DOI 10.1007/978-3-319-46475-6_1; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Rezende DJ, 2016, ADV NEUR IN, V29; Salzmann M, 2011, IEEE I CONF COMP VIS, P2064, DOI 10.1109/ICCV.2011.6126480; Shao TJ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661288; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vondrak M, 2013, IEEE T PATTERN ANAL, V35, P52, DOI 10.1109/TPAMI.2012.61; Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281; Wenbin Li, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2606, DOI 10.1109/ICRA.2017.7989304; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu J., 2016, PROC BRIT MACH VIS C, V2, P7; Wu J., 2015, ADV NEURAL INFORM PR; Wu JJ, 2017, PROC CVPR IEEE, P7035, DOI 10.1109/CVPR.2017.744; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zhang Renqiao, 2016, COGSCI; Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	47	152	153	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400015
C	Chen, LC; Collins, MD; Zhu, YK; Papandreou, G; Zoph, B; Schroff, F; Adam, H; Shlens, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Liang-Chieh; Collins, Maxwell D.; Zhu, Yukun; Papandreou, George; Zoph, Barret; Schroff, Florian; Adam, Hartwig; Shlens, Jonathon			Searching for Efficient Multi-Scale Architectures for Dense Image Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7% on Cityscapes (street scene parsing), 71.3% on PASCAL-Person-Part (person-part segmentation), and 87.9% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.	[Chen, Liang-Chieh; Collins, Maxwell D.; Zhu, Yukun; Papandreou, George; Zoph, Barret; Schroff, Florian; Adam, Hartwig; Shlens, Jonathon] Google Inc, Mountain View, CA 94043 USA	Google Incorporated	Chen, LC (corresponding author), Google Inc, Mountain View, CA 94043 USA.							Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Baker Bowen, 2017, ICLR; Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970; Berg A.C., 2015, ARXIV150604579; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen L, 2014, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2014.135; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen Tianqi, 2016, ICLR; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fang HS, 2018, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2018.00015; Fangting Xia, 2017, CVPR; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Fourure Damien, 2017, BMVC; Fu J., 2017, ARXIV170804943; Giusti A, 2013, IEEE IMAGE PROC, P4034, DOI 10.1109/ICIP.2013.6738831; Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043; Guan Melody, 2018, ICML, P4095; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He XM, 2004, PROC CVPR IEEE, P695; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Holschneider M., 1990, WAVELETS, p286?297, DOI DOI 10.1007/978-3-642-75988-8_28; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Lin D, 2018, LECT NOTES COMPUT SC, V11207, P622, DOI 10.1007/978-3-030-01219-9_37; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2018, LECT NOTES COMPUT SC, V11210, P203, DOI 10.1007/978-3-030-01231-1_13; Liu Hanxiao, 2018, ICLR; Liu Hanxiao, 2018, ARXIV180609055; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959; Negrinho R., 2017, ARXIV170408792; Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17; Papandreou G, 2015, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2015.7298636; Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189; Pinheiro PO, 2014, PR MACH LEARN RES, V32; Qi  H., 2017, ICCV COCO CHALL WORK; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Real E, 2017, PR MACH LEARN RES, V70; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Saxena S., 2016, ABS160602492 CORR; Schwing A. G., 2015, ARXIV PREPRINT ARXIV; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Sifre Laurent, 2014, THESIS; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vanhoucke Vincent, 2014, ICLR INV TALK, V1, P2; Wang M., 2016, ARXIV160804337; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388; Yao J, 2012, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2012.6247739; Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199; Yu F., 2016, P ICLR 2016; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhong ZX, 2018, AAAI CONF ARTIF INTE, P5714; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	99	151	153	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003027
C	Ghahramani, Z; Beal, MJ		Solla, SA; Leen, TK; Muller, KR		Ghahramani, Z; Beal, MJ			Variational inference for Bayesian mixtures of factor analysers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We present an algorithm that infers the model structure of a mixture of factor analysers using an efficient and deterministic variational approximation to full Bayesian integration over model parameters. This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i.e. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to overfitting. Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples. By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exact predictive density, and the KL divergence between the variational posterior and the true posterior, not only in this model but for variational approximations in general.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Ghahramani, Z (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							ATTIAS H, 1999, P 15 C UNC ART INT; Bishop C. M., 1999, P 9 INT C ART NEUR N; GHAHRAMANI Z, 1999, GCNUTR1999006 U COLL; Ghahramani Z., 1996, CRGTR961 U TOR DEP C; MacKay D.J.C., 1997, BOOK ENSEMBLE LEARNI; Neal R. M., 1998, Neural Networks and Machine Learning. Proceedings, P97; RASMUSSEN CE, 2000, ADV NEUR INF P SYS, V12; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; Roberts SJ, 1998, IEEE T PATTERN ANAL, V20, P1133, DOI 10.1109/34.730550; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; UEDA N, 1999, ADV NEUR INF P SYS, V11; WATERHOUSE S, 1995, ADV NEUR INF P SYS, V7	13	151	153	1	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						449	455						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700064
C	Phillips, PJ		Kearns, MS; Solla, SA; Cohn, DA		Phillips, PJ			Support vector machines applied to face recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Face recognition is a K class problem, where K is the number of known individuals; and support vector machines (SVMs) are a binary classification method. By reformulating the face recognition problem and reinterpreting the output of the SVM classifier, we developed a SVM-based face recognition algorithm. The face recognition problem is formulated as a problem in difference space, which models dissimilarities between two facial images. In difference space we formulate face recognition as a two class problem. The classes are: dissimilarities between faces of the same person, and dissimilarities between faces of different people. By modifying the interpretation of the decision surface generated by SVM, we generated a similarity metric between faces that is learned from examples of differences between faces. The SVM-based algorithm is compared with a principal component analysis (PCA) based algorithm on a difficult set of images from the FERET database. Performance was measured for both verification and identification scenarios. The identification performance for SVM is 77-78% versus 54% for PCA. For verification. the equal error rate is 7% for SVM and 13% for PCA.	NIST, Gaithersburg, MD 20899 USA	National Institute of Standards & Technology (NIST) - USA	Phillips, PJ (corresponding author), NIST, Bldg 225,Rm A216, Gaithersburg, MD 20899 USA.	jonathon@nist.gov						BURGES CJC, 1998, UNPUB DATA MINING KN; MOGHADDAM B, 1994, P SOC PHOTO-OPT INS, V2277, P12, DOI 10.1117/12.191877; Moghaddam B, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P30, DOI 10.1109/AFGR.1998.670921; MOON H, 1998, EMPIRICAL EVALUATION; Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	7	151	157	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						803	809						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700113
C	Crites, RH; Barto, AG		Touretzky, DS; Mozer, MC; Hasselmo, ME		Crites, RH; Barto, AG			Improving elevator performance using reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MASSACHUSETTS,DEPT COMP SCI,AMHERST,MA 01003	University of Massachusetts System; University of Massachusetts Amherst									0	151	153	0	5	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1017	1023						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00143
C	Bartlett, PL; Foster, DJ; Telgarsky, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bartlett, Peter L.; Foster, Dylan J.; Telgarsky, Matus			Spectrally-normalized margin bounds for neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized spectral complexity: their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.	[Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Bartlett, Peter L.] Queensland Univ Technol, Brisbane, Qld, Australia; [Foster, Dylan J.] Cornell Univ, Ithaca, NY 14853 USA; [Telgarsky, Matus] Univ Illinois, Urbana, IL 61801 USA	University of California System; University of California Berkeley; Queensland University of Technology (QUT); Cornell University; University of Illinois System; University of Illinois Urbana-Champaign	Bartlett, PL (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Bartlett, PL (corresponding author), Queensland Univ Technol, Brisbane, Qld, Australia.	peter@berkeley.edu; djf244@cornell.edu; mjt@illinois.edu	Jeong, Yongwook/N-7413-2016		NVIDIA GPU grant; NDSEG fellowship; NSF [IIS-1619362]; Australian Research Council [FL110100281]; ARC Centre of Excellence for Mathematical and Statistical Frontiers	NVIDIA GPU grant; NDSEG fellowship; NSF(National Science Foundation (NSF)); Australian Research Council(Australian Research Council); ARC Centre of Excellence for Mathematical and Statistical Frontiers(Australian Research Council)	The authors thank Srinadh Bhojanapalli, Ryan Jian, Behnam Neyshabur, Maxim Raginsky, Andrew J. Risteski, and Belinda Tzen for useful conversations and feedback. The authors thank Ben Recht for giving a provocative lecture at the Simons Institute, stressing the need for understanding of both generalization and optimization of neural networks. M.T. and D.F. acknowledge the use of a GPU machine provided by Karthik Sridharan and made possible by an NVIDIA GPU grant. D.F. acknowledges the support of the NDSEG fellowship. P.B. gratefully acknowledges the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. The authors thank the Simons Institute for the Theory of Computing Spring 2017 program on the Foundations of Machine Learning. Lastly, the authors are grateful to La Burrita (both the north and the south Berkeley campus locations) for upholding the glorious tradition of the California Burrito.	Anthony M., 1999, NEURAL NETWORK LEARN, V9; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett P. L., 2017, ARXIV170608498; Bartlett Peter L., 1996, NIPS; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Cisse M., 2017, ICML; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Goodfellow IJ, 2014, ARXIV14126572STATML; He K., 2016, ECCV; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Mohri M., 2018, FDN MACHINE LEARNING; Neyshabur Behnam, 2017, ABS170901953 CORR; Neyshabur Behnam, 2017, ABS170709564 CORR; Neyshabur Behnam, 2015, COLT; Pisier G., 1980, SEMINAIRE SEM AN FON, V5, P1; Schapire R.E., 1997, P 14 INT C MACHINE L, P322; Shalev-Shwartz Shai, 2008, COLT; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Vapnik V., 1982, ESTIMATION DEPENDENC; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Zhang C., 2017, ICLR; Zhang T, 2004, J MACH LEARN RES, V5, P1225; Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713	25	150	152	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406031
C	Szummer, M; Jaakkola, T		Dietterich, TG; Becker, S; Ghahramani, Z		Szummer, M; Jaakkola, T			Partially labeled classification with Markov random walks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems.	MIT, AI Lab & CBCL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Szummer, M (corresponding author), MIT, AI Lab & CBCL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	szummer@ai.mit.edu; tommi@ai.mit.edu						Chawla Shuchi, 2001, LEARNING LABELED UNL; JAAKKOLA T, 1999, NIPS, V12; KONDOR I, 2001, IN PRESS DIFFUSION K; SZUMMER M, 2000, 13 NIPS; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; TISHBY N, 2000, NIPS, V13	7	150	157	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						945	952						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100118
C	Poole, B; Lahiri, S; Raghu, M; Sohl-Dickstein, J; Ganguli, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Poole, Ben; Lahiri, Subhaneil; Raghu, Maithra; Sohl-Dickstein, Jascha; Ganguli, Surya			Exponential expressivity in deep neural networks through transient chaos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.	[Poole, Ben; Lahiri, Subhaneil; Ganguli, Surya] Stanford Univ, Stanford, CA 94305 USA; [Raghu, Maithra; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA USA	Stanford University; Google Incorporated	Poole, B (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	benpoole@stanford.edu; sulahiri@stanford.edu; maithra@google.com; jaschasd@google.com; sganguli@stanford.edu	Lahiri, Subhaneil/AGI-0957-2022	Lahiri, Subhaneil/0000-0003-2028-6635; Ganguli, Surya/0000-0002-9264-7551				Bengio, 2011, ADV NEURAL INFORM PR, P666, DOI DOI 10.5555/2986459.2986534; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bianchini M, 2014, IEEE T NEUR NET LEAR, V25, P1553, DOI 10.1109/TNNLS.2013.2293637; DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010; Eldan R., 2015, COMPUT SCI; Hannun A.Y., 2014, ARXIV14125567, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee J. M., 2006, RIEMANNIAN MANIFOLDS, DOI 10.1007/b98852; Martens J., 2013, ADV NEURAL INFORM PR, P2877; McIntosh Lane T., 2016, ADV NEURAL INFORM PR; Mhaskar Hrushikesh N., 2016, ARXIV160300988; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Piech C., 2015, ADV NEURAL INFORM PR, P505; Raghu M., 2016, ARXIV160605336; SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259; Telgarsky M, 2015, ARXIV150908101V2CSLG	18	149	149	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702069
C	Opitz, DW; Shavlik, JW		Touretzky, DS; Mozer, MC; Hasselmo, ME		Opitz, DW; Shavlik, JW			Generating accurate and diverse members of a neural-network ensemble	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MINNESOTA,DEPT COMP SCI,DULUTH,MN 55812	University of Minnesota System; University of Minnesota Duluth									0	149	163	0	4	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						535	541						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00076
C	Bengio, S; Vinyals, O; Jaitly, N; Shazeer, N		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bengio, Samy; Vinyals, Oriol; Jaitly, Navdeep; Shazeer, Noam			Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.	[Bengio, Samy; Vinyals, Oriol; Jaitly, Navdeep; Shazeer, Noam] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Bengio, S (corresponding author), Google Res, Mountain View, CA 94043 USA.	bengio@google.com; vinyals@google.com; ndjaitly@google.com; noam@google.com						[Anonymous], 2006, P HUM LANG TECHN C N; Bahdanau Dzmitry, 2015, ICLR 2015; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2009, P INT C MACH LEARN I; Chorowski J., NIPS 2014 WORKSHOP D; Collins M., 2004, P ASS COMPUT LING AC; Cui Y., 2015, MICROSOFT COCO CAPTI; Donahue J., 2015, IEEE C COMP VIS PATT; Fang H., 2015, IEEE C COMP VIS PATT; Goldberg Y., 2012, P COLING; Hal Daume III, 2009, MACHINE LEARNING J; Hochreiter S., 1997, NEURAL COMPUTATION, V9; JAITLY N., 2014, THESIS; Karpathy A., 2015, IEEE C COMP VIS PATT; Kiros Ryan, 2015, ARXIV14112539; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Lin T., 2014, ECCV; loffe S., 2015, P INT C MACH LEARN I; Mao Junhua, 2015, INT C LEARN REPR ICL INT C LEARN REPR ICL; Povey D., IEEE 2011 WORKSH AUT; Ross S., 2011, P WORKSH ART INT STA; Sutskever I., 2014, NIPS, V27; Vedantam R., 2015, IEEE C COMP VIS PATT; Venkatraman A., 2015, 29 AAAI C ART INT AA; Vinyals O., 2015, IEEE C COMP VIS PATT; VINYALS O, 2014, GRAMMAR FOREIGN LANG	26	148	153	4	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102104
C	Johnson, MJ; Duvenaud, D; Wiltschko, AB; Datta, SR; Adams, RP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Johnson, Matthew James; Duvenaud, David; Wiltschko, Alexander B.; Datta, Sandeep R.; Adams, Ryan P.			Composing graphical models with neural networks for structured representations and fast inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.	[Johnson, Matthew James; Duvenaud, David] Harvard Univ, Cambridge, MA 02138 USA; [Wiltschko, Alexander B.; Adams, Ryan P.] Harvard Univ, Twitter, Cambridge, MA 02138 USA; [Datta, Sandeep R.] Harvard Med Sch, Boston, MA 02115 USA	Harvard University; Harvard University; Twitter, Inc.; Harvard University; Harvard Medical School	Johnson, MJ (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	mattjj@seas.harvard.edu; dduvenaud@seas.harvard.edu; awiltsch@fas.harvard.edu; srdatta@hms.harvard.edu; rpa@seas.harvard.edu		Datta, Sandeep/0000-0002-8068-3862				Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S.-I., 2007, METHODS INFORM GEOME; Archer E., 2015, ARXIV PREPRINT ARXIV; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Deng L, 2004, IMA VOL MATH APPL, V138, P115; Deng L, 1999, COMPUTATIONAL MODELS, P199; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Iwata T., 2013, P 29 C UNC ART INT U, P311; Johnson MJ, 2014, PR MACH LEARN RES, V32, P1854; Kingma D. P, 2014, ARXIV13126114; Koller D., 2009, PROBABILISTIC GRAPHI; MacKay DJC, 1999, STATISTICS AND NEURAL NETWORKS, P129; Martens J., 2015, ARXIV14121193; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sontag D., 2015, ARXIV151105121; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wiltschko AB, 2015, NEURON, V88, P1121, DOI 10.1016/j.neuron.2015.11.031	22	147	149	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703015
C	Srivastava, RK; Greff, K; Schmidhuber, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Srivastava, Rupesh Kumar; Greff, Klaus; Schmidhuber, Juergen			Training Very Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.	[Srivastava, Rupesh Kumar; Greff, Klaus; Schmidhuber, Juergen] USI, SUPSI, Swiss AI Lab IDSIA, Lugano, Switzerland	Universita della Svizzera Italiana	Srivastava, RK (corresponding author), USI, SUPSI, Swiss AI Lab IDSIA, Lugano, Switzerland.	rupesh@idsia.ch; klaus@idsia.ch; juergen@idsia.ch	Peters, Jan/P-6027-2019	Peters, Jan/0000-0002-5266-8091	EU project NASCENCE [FP7-ICT-317662]	EU project NASCENCE	We thank NVIDIA Corporation for their donation of GPUs and acknowledge funding from the EU project NASCENCE (FP7-ICT-317662). We are grateful to Sepp Hochreiter and Thomas Unterthiner for helpful comments and Jan Koutnik for help in conducting experiments.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bianchini M., 2014, IEEE T NEURAL NETWOR; Ciresan D. C., 2011, IJCAI; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1049/cp:19991218, 10.1162/089976600300015015]; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow Ian J., 2013, ARXIV13024389CSSTAT; Graham Benjamin, 2014, ARXIV14096070; Graves A, 2013, ARXIV13080850; Hastad J., 1991, Computational Complexity, V1, P113, DOI 10.1007/BF01272517; Hastad J, 1987, COMPUTATIONAL LIMITA; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1997, FR ART INT, V37, P65; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kalchbrenner N., 2015, ARXIV150701526CS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Martens James, 2014, ARXIV14117717CSSTAT; Martin Joyce A, 2012, NCHS Data Brief, P1; Masci Jonathan, 2015, INT C LEARN REPR; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Raiko T, 2012, P INT C ART INT STAT, V22, P924; Romero Adriana, 2014, ARXIV14126550CS; Saxe Andrew M., 2013, ARXIV13126120CONDMAT; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; Seide F, 2013, ARXIV PREPRINT ARXIV; Springenberg J.T., 2014, ARXIV14126806; Srivastava R.K., 2015, ARXIV150500387CS; Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310; Stollenga M.F., 2014, ADV NEURAL INFORM PR, P3545; Sussillo David, 2014, ARXIV14126558CSSTAT; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]	38	147	148	5	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101109
C	Denton, E; Zaremba, W; Bruna, J; LeCun, Y; Fergus, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Denton, Emily; Zaremba, Wojciech; Bruna, Joan; LeCun, Yann; Fergus, Rob			Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1% of the original model.	[Denton, Emily; Zaremba, Wojciech; Bruna, Joan; LeCun, Yann; Fergus, Rob] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA	New York University	Denton, E (corresponding author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.	denton@cs.nyu.edu; zaremba@cs.nyu.edu; bruna@cs.nyu.edu; lecun@cs.nyu.edu; fergus@cs.nyu.edu			ONR [N00014-13-1-0646]; NSF [1116923, 1149633]; Microsoft Research	ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); Microsoft Research(Microsoft)	The authors are grateful for support from ONR #N00014-13-1-0646, NSF #1116923, #1149633 and Microsoft Research.	Denil M., 2013, ARXIV13060543; Hinton G.E., 2012, COMPUT SCI, V3, P212, DOI DOI 10.9774/GLEAF.978-1-909493-38-42; Jaderberg M., 2014, ARXIV14053866; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le Q.V., 2011, ARXIV11126209; Li K., 2009, CVPR09; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Mathieu M., 2013, P 2 INT C LEARN REPR; Ngiam J., 2010, ADV NEURAL INF PROCE, V23, P10; Sermanet P., 2013, COMPUT VIS PATTERN R; Zeiler M. D., 2014, EUR C COMP VIS, P818; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474; Zhang T, 2001, SIAM J MATRIX ANAL A, V23, P534, DOI 10.1137/S0895479899352045	15	147	148	5	14	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102105
C	Rao, RS; Bhattacharya, N; Thomas, N; Duan, Y; Chen, X; Canny, J; Abbeel, P; Song, YS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rao, Roshan; Bhattacharya, Nicholas; Thomas, Neil; Duan, Yan; Chen, Xi; Canny, John; Abbeel, Pieter; Song, Yun S.			Evaluating Protein Transfer Learning with TAPE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PREDICTION; CONTACT; CLASSIFICATION	Machine learning applied to protein sequences is an increasingly popular area of research. Semi-supervised learning for proteins has emerged as an important paradigm due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We bench-mark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end,	[Rao, Roshan; Bhattacharya, Nicholas; Thomas, Neil; Canny, John; Abbeel, Pieter; Song, Yun S.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Duan, Yan; Chen, Xi] Covariant Ai, Berkeley, CA USA	University of California System; University of California Berkeley	Rao, RS (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	roshan_rao@berkeley.edu; nick_bhat@berkeley.edu; nthomas@berkeley.edu; rocky@covariant.ai; peter@covariant.ai; canny@berkeley.edu; pabbeel@berkeley.edu; yss@berkeley.edu	Rao, Roshan/AAO-4719-2021	Rao, Roshan/0000-0003-4412-3742; Thomas, Neil/0000-0002-9089-1921	Berkeley Deep Drive; DARPA XAI; NIH; Packard Fellowship for Science and Engineering; Open Philanthropy Project; Chan-Zuckerberg Biohub	Berkeley Deep Drive; DARPA XAI; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Packard Fellowship for Science and Engineering; Open Philanthropy Project; Chan-Zuckerberg Biohub	We thank Philippe Laban, David Chan, Jeffrey Spence, Jacob West-Roberts, Alex Crits-Cristoph, Surojit Biswas, Ethan Alley, Mohammed AlQuraishi, Grigory Khimulya, and Kevin Yang for valuable input on this paper. We thank the AWS Educate program for providing us with the resources to train our models. Additionally, we acknowledge funding from Berkeley Deep Drive, Chan-Zuckerberg Biohub, DARPA XAI, NIH, the Packard Fellowship for Science and Engineering, and the Open Philanthropy Project.	Alley Ethan C, 2019, BIORXIV; AlQuraishi Mohammed, 2019, BIORXIV; ALTSCHUH D, 1988, PROTEIN ENG, V2, P193, DOI 10.1093/protein/2.3.193; Altschul SF, 1997, NUCLEIC ACIDS RES, V25, P3389, DOI 10.1093/nar/25.17.3389; ALTSCHUL SF, 1990, J MOL BIOL, V215, P403, DOI 10.1006/jmbi.1990.9999; ANFINSEN CB, 1961, P NATL ACAD SCI USA, V47, P1309, DOI 10.1073/pnas.47.9.1309; [Anonymous], 1984, PURE APPL CHEM, V56, P595; Bateman A, 2019, NUCLEIC ACIDS RES, V47, pD506, DOI 10.1093/nar/gky1049; Bepler T., 2018, INT C LEARNING REPRE; Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235; Brenner SE, 1998, P NATL ACAD SCI USA, V95, P6073, DOI 10.1073/pnas.95.11.6073; Castelle CJ, 2018, CELL, V172, P1181, DOI 10.1016/j.cell.2018.02.016; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Creighton T.E., 1993, PROTEINS STRUCTURES; Cuff JA, 1999, PROTEINS, V34, P508, DOI 10.1002/(SICI)1097-0134(19990301)34:4<508::AID-PROT10>3.0.CO;2-4; Donahue Jeff, 2013, J MACHINE LEARNING R; Drozdetskiy A, 2015, NUCLEIC ACIDS RES, V43, pW389, DOI 10.1093/nar/gkv332; Durbin Richard, 1998, ANAL PROBABILISTIC M; Eddy SR, 1998, BIOINFORMATICS, V14, P755, DOI 10.1093/bioinformatics/14.9.755; Eiband M, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P211, DOI 10.1145/3172944.3172961; El-Gebali S, 2019, NUCLEIC ACIDS RES, V47, pD427, DOI 10.1093/nar/gky995; Fox NK, 2014, NUCLEIC ACIDS RES, V42, pD304, DOI 10.1093/nar/gkt1240; Heinzinger Michael, 2019, BIORXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hou J, 2018, BIOINFORMATICS, V34, P1295, DOI 10.1093/bioinformatics/btx780; Joosten Robbie P, 2011, Nucleic Acids Res, V39, pD411, DOI 10.1093/nar/gkq1105; Kim DE, 2014, PROTEINS, V82, P208, DOI 10.1002/prot.24374; Klausen M.S., 2019, PROTEINS STRUCTURE F; Krause B., 2016, ARXIV PREPRINT ARXIV; Liu JJ, 2019, NATURE, V566, P218, DOI 10.1038/s41586-019-0908-x; Ma JZ, 2015, BIOINFORMATICS, V31, P3506, DOI 10.1093/bioinformatics/btv472; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Moult J, 2018, PROTEINS, V86, P7, DOI 10.1002/prot.25415; Perdigao N, 2015, P NATL ACAD SCI USA, V112, P15898, DOI 10.1073/pnas.1508380112; Peters Matthew E., 2018, P 2018 C N AM CHAPT, V1, P2227, DOI DOI 10.18653/V1/N18-1202; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Remmert M, 2012, NAT METHODS, V9, P173, DOI [10.1038/NMETH.1818, 10.1038/nmeth.1818]; Riesselman AJ, 2018, NAT METHODS, V15, P816, DOI 10.1038/s41592-018-0138-4; Rives A., 2019, BIORXIV, DOI 10.1101/622803https://www.biorxiv.org/content/10.1101/622803v3.; Rocklin GJ, 2017, SCIENCE, V357, P168, DOI 10.1126/science.aan0693; Rost B, 1999, PROTEIN ENG, V12, P85, DOI 10.1093/protein/12.2.85; Sarkisyan KS, 2016, NATURE, V533, P397, DOI 10.1038/nature17995; Schaarschmidt J, 2018, PROTEINS, V86, P51, DOI 10.1002/prot.25407; Seemayer S, 2014, BIOINFORMATICS, V30, P3128, DOI 10.1093/bioinformatics/btu500; Shin H., 2006, SEMISUPERVISED LEARN; Soding J, 2005, NUCLEIC ACIDS RES, V33, pW244, DOI 10.1093/nar/gki408; Stevens R., 2003, DRUG DISC WORLD, V4, P35; Tavares LS, 2013, FRONT MICROBIOL, V4, DOI 10.3389/fmicb.2013.00412; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vazquez E, 2009, CURR PHARM DESIGN, V15, P893, DOI 10.2174/138161209787582084; Wang A, 2019, ADV NEUR IN, V32; Wang Alex, 2018, ARXIV180407461, DOI DOI 10.18653/V1/W18-5446; Weston J, 2004, ADV NEUR IN, V16, P595; Whitehead TA, 2012, NAT BIOTECHNOL, V30, P543, DOI 10.1038/nbt.2214; Yang K. K., 2018, ARXIV181110775; Yang KK, 2018, BIOINFORMATICS, V34, P2642, DOI 10.1093/bioinformatics/bty178; Yang YD, 2018, BRIEF BIOINFORM, V19, P482, DOI 10.1093/bib/bbw129; YANOFSKY C, 1964, SCIENCE, V146, P1593, DOI 10.1126/science.146.3651.1593; Yu Fisher, IEEE C COMP VIS PATT	60	144	145	11	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE	33390682				2022-12-19	WOS:000535866901033
C	Wu, YH; Mansimov, E; Liao, S; Grosse, R; Ba, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Yuhuai; Mansimov, Elman; Liao, Shun; Grosse, Roger; Ba, Jimmy			Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also the method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the Mu-JoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2-to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines.	[Wu, Yuhuai; Liao, Shun; Grosse, Roger; Ba, Jimmy] Univ Toronto, Vector Inst, Toronto, ON, Canada; [Mansimov, Elman] NYU, New York, NY 10003 USA	University of Toronto; New York University	Wu, YH (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	ywu@cs.toronto.edu; mansimov@cs.nyu.edu; sliao3@cs.toronto.edu; rgrosse@cs.toronto.edu; jimmy@psi.utoronto.ca	Jeong, Yongwook/N-7413-2016					Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], 2016, ICML; Ba J., 2017, ICLR; Bagnell J. A., 2003, IJCAI; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Brockman G., 2016, OPENAI GYM; Grosse R., 2016, ICML; Gu S., 2017, ICLR; Heess N., 2017, ABS170702286 CORR; Jaderberg M., 2017, ICLR; Kakade S. M., 2002, ADV NEURAL INFORM PR; Kingma D.P., 2015, INT C LEARN REPR, P1; Lillicrap Timothy P, 2016, ICLR; Martens J., 2010, ICML 10; Martens James, 2015, ICML; Martens James, 2014, NEW INSIGHTS PERSPEC; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Schraudolph Nicol N., 2002, NEURAL COMPUTATION; Schulman J, 2016, INT C LEARNING REPRE; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, P 32 INT C MACH LEAR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton Richard S, 2000, ADV NEURAL INFORM PR, V12; Todorov E., 2012, IEEE RSJ INT C INT R; Wang Z., 2016, ICLR; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	28	144	149	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405035
C	Ha, D; Schmidhuber, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ha, David; Schmidhuber, Jurgen			Recurrent World Models Facilitate Policy Evolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PRIMARY VISUAL-CORTEX; REPRESENTATION; CREATIVITY; CURIOSITY	A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatiotemporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper: https://worldmodels.github.io	[Ha, David] Google Brain, Tokyo, Japan; [Schmidhuber, Jurgen] IDSIA USI & SUPSI, Swiss AI Lab, NNAISENSE, Manno, Switzerland	Universita della Svizzera Italiana	Ha, D (corresponding author), Google Brain, Tokyo, Japan.	hadavid@google.com; juergen@idsia.ch			SNF project RNNAISSANCE [200021_165675]; ERC Advanced Grant [742870]	SNF project RNNAISSANCE; ERC Advanced Grant(European Research Council (ERC))	We would like to thank Blake Richards, Kory Mathewson, Chris Olah, Kai Arulkumaran, Denny Britz, Kyle McDonald, Ankur Handa, Elwin Ha, Nikhil Thorat, Daniel Smilkov, Alex Graves, Douglas Eck, Mike Schuster, Rajat Monga, Vincent Vanhoucke, Jeff Dean and Natasha Jaques for their thoughtful feedback. This work was partially funded by SNF project RNNAISSANCE (200021_165675) and by an ERC Advanced Grant (no: 742870).	Alvernaz Samuel, 2017, 2017 IEEE Conference on Computational Intelligence and Games (CIG), P1, DOI 10.1109/CIG.2017.8080408; [Anonymous], 2017, ARXIV170907857; Bartol TM, 2015, ELIFE, V4, DOI 10.7554/eLife.10778; Bishop C. M., 1995, NEURAL NETWORKS PATT; Brockman G., 2016, OPENAI GYM; Carter S., 2016, DISTILL; Chang L, 2017, CELL, V169, P1013, DOI 10.1016/j.cell.2017.05.011; Chiappa Silvia, 2017, ARXIV170402254; Consalvo M., 2007, CHEATING GAINING ADV; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Depeweg S., 2016, ARXIV160507127; Dosovitskiy A., 2016, ARXIV161101779; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; French R.M., 1994, ADV NEURAL INFORM PR, P1176; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gauci J, 2010, NEURAL COMPUT, V22, P1860, DOI 10.1162/neco.2010.06-09-1042; Gemici M., 2017, ARXIV PREPRINT ARXIV; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Gomez F, 2008, J MACH LEARN RES, V9, P937; Gomez FJ, 2005, GECCO 2005: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOLS 1 AND 2, P491; Gottlieb J, 2013, TRENDS COGN SCI, V17, P585, DOI 10.1016/j.tics.2013.09.001; Graves A., 2015, HALLUCINATION RECURR; Graves A, 2013, ARXIV13080850; Guzdial M, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3707; Ha David, 2017, ICLR; Ha David, 2017, EVOLVING STABLE STRA; Ha David, 2018, ICLR; Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398; Hansen N., 2016, ARXIV160400772; Hausknecht M, 2014, IEEE T COMP INTEL AI, V6, P355, DOI 10.1109/TCIAIG.2013.2294713; Hein D., 2017, ARXIV170909480; Higgins I, 2017, PR MACH LEARN RES, V70; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hunermann J., 2017, SELF DRIVING CARS BR; Jang S., 2017, REINFORCEMENT CAR RA; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Keller GB, 2012, NEURON, V74, P809, DOI 10.1016/j.neuron.2012.03.040; KELLEY HJ, 1960, ARSJ-AM ROCKET SOC J, V30, P947, DOI 10.2514/8.5282; Kempka M, 2016, IEEE CONF COMPU INTE; Khan M., 2016, CAR RACING USING REI; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Klimov Oleg, 2016, CARRACING V0; Koutnik J, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P1061; Lau B., 2016, USING KERAS DEEP DET; Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025; Leinweber M, 2017, NEURON, V95, P1420, DOI 10.1016/j.neuron.2017.08.036; Linnainmaa S., 1970, REPRESENTATION CUMUL; Long-Ji Lin, 1993, THESIS; Maus GW, 2013, NEURON, V78, P554, DOI 10.1016/j.neuron.2013.03.010; McAllister RT, 2017, ADV NEUR IN, V30; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mobbs D, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00055; MUNRO PW, 1987, P 9 ANN C COGN SCI S, P165; Nagabandi A., 2017, ARXIV170802596; Nguyen D., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P357, DOI 10.1109/IJCNN.1989.118723; Nortmann N, 2015, CEREB CORTEX, V25, P1427, DOI 10.1093/cercor/bht318; Oord A.V.D., 2016, SSW; Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271; Paquette P, 2016, DOOMTAKECOVER V0; Parker M, 2012, IEEE T COMP INTEL AI, V4, P44, DOI 10.1109/TCIAIG.2012.2184109; Pathak D, 2017, PR MACH LEARN RES, V70; Pi HJ, 2013, NATURE, V503, P521, DOI 10.1038/nature12676; Prieur L., 2017, DEEP Q LEARNING RACE; Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687; Rechenberg I, 1978, SIMULATIONSMETHODEN, P83; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Robinson T., 1989, P 11 C COGN SCI SOC, P836; Salimans T., 2017, ARXIV170303864; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X; Schmidhuber J., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P253, DOI 10.1109/IJCNN.1990.137723; SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605; Schmidhuber J., 1994, FKI94 TUM DEP INF; Schmidhuber J, 1990, MAKING WORLD DIFFERE; Schmidhuber J., 1990, P 1 INT C SIM AD BEH, P222; Schmidhuber J., 1991, ADV NEURAL INFORM PR, V3, P500; Schmidhuber J., 2015, ARXIV151109249; Schmidhuber J., 2018, ARXIV180208864; Schmidhuber J, 2006, CONNECT SCI, V18, P173, DOI 10.1080/09540090600768658; Schmidhuber J, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00313; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Schwefel H., 1977, NUMERICAL OPTIMIZATI; Sehnke F, 2010, NEURAL NETWORKS, V23, P551, DOI 10.1016/j.neunet.2009.12.004; Silver D., 2016, ARXIV161208810; Srivastava RK, 2013, NEURAL NETWORKS, V41, P130, DOI 10.1016/j.neunet.2013.01.022; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Suarez Joseph, 2017, ADV NEURAL INFORM PR, P3269; Such F. P., 2017, ARXIV171206567; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wahlstrom N., 2015, DEEP LEARN WORKSH 32; Wahlstrom N., 2015, 17 IFAC S SYST ID SY; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Watters N, 2017, ARXIV170601433; Werbos P., 1982, SYSTEM MODELING OPTI, V2, P762, DOI [10.1007/BFb0006203, DOI 10.1007/BFB0006203]; Werbos Paul J, 1987, P IEEE INT C SYST MA; WERBOS PJ, 1989, PROCEEDINGS OF THE 28TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-3, P260, DOI 10.1109/CDC.1989.70114; Wiering M, 2012, ADAPT LEARN OPTIM, V12, P1, DOI 10.1007/978-3-642-27645-3; Wu Y., 2018, INT C LEARN REPR	107	143	144	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302046
C	Shen, TX; Lei, T; Barzilay, R; Jaakkola, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Shen, Tianxiao; Lei, Tao; Barzilay, Regina; Jaakkola, Tommi			Style Transfer from Non-Parallel Text by Cross-Alignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.	[Shen, Tianxiao; Barzilay, Regina; Jaakkola, Tommi] MIT CSAIL, Cambridge, MA 02139 USA; [Lei, Tao] ASAPP Inc, New York, NY USA	Massachusetts Institute of Technology (MIT)	Shen, TX (corresponding author), MIT CSAIL, Cambridge, MA 02139 USA.	tianxiao@csail.mit.edu; tao@asapp.com; regina@csail.mit.edu; tommi@csail.mit.edu	Jeong, Yongwook/N-7413-2016		MIT Lincoln Laboratory	MIT Lincoln Laboratory	We thank Nicholas Matthews for helping to facilitate human evaluations, and Zhiting Hu for sharing his code. We also thank Jonas Mueller, Arjun Majumdar, Olga Simek, Danelle Shah, MIT NLP group and the reviewers for their helpful comments. This work was supported by MIT Lincoln Laboratory.	[Anonymous], 2017, ABS170207983 CORR; Brown P. F., 1990, Computational Linguistics, V16, P79; Dou Q., 2012, P 2012 JOINT C EMP M, P266; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal Kartik, 2017, ARXIV170406970; Hjelm R Devon, 2017, ARXIV PREPRINT ARXIV; Hu ZT, 2017, PR MACH LEARN RES, V70; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jang E., 2016, ARXIV; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Klein G, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P67, DOI 10.18653/v1/P17-4012; Kusner Matt J, 2016, ARXIV161104051; Lantao Yu, 2016, ARXIV160905473; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Maddison Chris J, 2016, ARXIV161100712; Makhzani A., 2015, ARXIV151105644; Mueller J, 2017, PR MACH LEARN RES, V70; Nuhn M., 2013, P 51 ANN M ASS COMP, V1, P615; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Schmaltz Allen, 2016, P 2016 C EMP METH NA, P2319, DOI DOI 10.18653/V1/D16-1255; Taigman Y., 2016, ARXIV161102200; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zhu Jun-Yan, 2017, ICCV	30	143	148	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406086
C	du Plessis, MC; Niu, G; Sugiyama, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		du Plessis, Marthinus C.; Niu, Gang; Sugiyama, Masashi			Analysis of Learning from Positive and Unlabeled Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHM	Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than 2 root 2 times the fully supervised case. These theoretical findings are also validated through experiments.	[du Plessis, Marthinus C.; Sugiyama, Masashi] Univ Tokyo, Tokyo 1130033, Japan; [Niu, Gang] Baidu Inc, Beijing 100085, Peoples R China	University of Tokyo; Baidu	du Plessis, MC (corresponding author), Univ Tokyo, Tokyo 1130033, Japan.	niugang@baidu.com; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	JST CREST program; 973 Program [2014CB340505]; KAKENHI [23120004]	JST CREST program(Core Research for Evolutional Science and Technology (CREST)Japan Science & Technology Agency (JST)); 973 Program(National Basic Research Program of China); KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	MCdP is supported by the JST CREST program, GN was supported by the 973 Program No. 2014CB340505 and MS is supported by KAKENHI 23120004.	Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Collobert R., 2006, P 23 INT C MACHINE L, P201, DOI DOI 10.1145/1143844.1143870.; du Plessis M. C., 2014, IEICE T INFORM SYS D; Duda R.O., 2001, PATTERN CLASSIFICATI; Elkan C., 2001, INT JOINT C ART INT, V17, P973, DOI DOI 10.5555/1642194.1642224; Elkan Charles, 2008, P 14 ACM SIGKDD INT, P213, DOI DOI 10.1145/1401890.1401920; Ghosh A., 2014, ABS14033610 CORR; Hido S, 2008, IEEE DATA MINING, P223, DOI 10.1109/ICDM.2008.49; Li WK, 2011, IEEE T GEOSCI REMOTE, V49, P717, DOI 10.1109/TGRS.2010.2058578; Mohri M., 2018, FDN MACHINE LEARNING; Suzumura S, 2014, PR MACH LEARN RES, V32, P1098; Van Trees H. L, 2004, DETECTION ESTIMATION; Vapnik V.N., 2000, NATURE STAT LEARNING, DOI DOI 10.1007/978-1-4757-3264-1_1; Welling M, 2009, P 12 INT C ART INT S, P464	15	143	144	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102070
C	Goldberg, PW; Williams, CKI; Bishop, CM		Jordan, MI; Kearns, MJ; Solla, SA		Goldberg, PW; Williams, CKI; Bishop, CM			Regression with input-dependent noise: A Gaussian process treatment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Gaussian processes provide natural non-parametric prior distributions over regression functions. In this paper we consider regression problems where there is noise on the output, and the variance of the noise depends on the inputs. If we assume that the noise is a smooth function of the inputs, then it is natural to model the noise variance using a second Gaussian process, in addition to the Gaussian process governing the noise-free output value. We show that prior uncertainty about the parameters controlling both processes can be handled and that the posterior distribution of the noise rate can be sampled from using Markov chain Monte Carlo methods. Our results on a synthetic data set give a posterior noise variance that well-approximates the true variance.	Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England	University of Warwick	Goldberg, PW (corresponding author), Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.								0	143	148	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						493	499						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700070
C	Goyal, A; Lamb, A; Zhang, Y; Zhang, SZ; Courville, A; Bengio, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Goyal, Anirudh; Lamb, Alex; Zhang, Ying; Zhang, Saizheng; Courville, Aaron; Bengio, Yoshua			Professor Forcing: A New Algorithm for Training Recurrent Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.	[Goyal, Anirudh; Lamb, Alex; Zhang, Ying; Zhang, Saizheng; Courville, Aaron] Univ Montreal, MILA, Montreal, PQ, Canada; [Bengio, Yoshua] CIFAR, Toronto, ON, Canada	Universite de Montreal; Canadian Institute for Advanced Research (CIFAR)	Goyal, A (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.	anirudhgoyal9119@gmail.com; alex6200@gmail.com; ying.zhlisa@gmail.com; saizhenglisa@gmail.com; aaron.courville@gmail.com; yoshua.umontreal@gmail.com						Ajakan H., 2014, ARXIV E PRINTS; Al-Rfou R., 2016, CORR; Bahdanau D., 2016, ARXIV E PRINTS; Bahdanau Dzmitry, 2015, ARXIV150804395, P4945; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bengio Y., 2014, ARXIV14061078; Bengio Yoshua, 2013, ARXIV E PRINTS; Brebisson A., 2016, CONDITIONAL HANDWRIT; Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Daume III H., 2009, ARXIV E PRINTS; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ganin Yaroslav, 2015, ARXIV E PRINTS; Germain M, 2015, PR MACH LEARN RES, V37, P881; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2013, TECHNICAL REPORT; Graves A., 2012, STUDIES IN COMPUTATI; Graves A, 2013, ARXIV13080850; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huszar F., 2015, ARXIV E PRINTS; Kingma D.P, P 3 INT C LEARNING R; Larochelle H., 2011, NEURAL AUTOREGRESSIV; Liwicki M, 2005, PROC INT CONF DOC, P956, DOI 10.1109/ICDAR.2005.132; Mikolov T., 2012, CONTEXT DEPENDENT RE; Mikolov Tomas, 2010, RECURRENT NEURAL NET; Murray I., 2009, ADV NEURAL INFORM PR, P1137; Oord A.V.D., 2016, SSW; Raiko T., 2014, ADV NEURAL INFORM PR; Ross S., 2010, ARXIV E PRINTS; Salimans Tim, 2014, ICML; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Theis Lucas, 2015, ARXIV151101844; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Xu K, 2015, PR MACH LEARN RES, V37, P2048	35	142	143	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700063
C	Shafahi, A; Najibi, M; Ghiasi, A; Xu, Z; Dickerson, J; Studer, C; Davis, LS; Taylor, G; Goldstein, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shafahi, Ali; Najibi, Mahyar; Ghiasi, Amin; Xu, Zheng; Dickerson, John; Studer, Christoph; Davis, Larry S.; Taylor, Gavin; Goldstein, Tom			Adversarial Training for Free!	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our "free" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks.	[Shafahi, Ali; Najibi, Mahyar; Ghiasi, Amin; Xu, Zheng; Dickerson, John; Davis, Larry S.; Goldstein, Tom] Univ Maryland, College Pk, MD 20742 USA; [Studer, Christoph] Cornell Univ, Ithaca, NY 14853 USA; [Taylor, Gavin] US Naval Acad, Annapolis, MD 21402 USA	University System of Maryland; University of Maryland College Park; Cornell University; United States Department of Defense; United States Navy; United States Naval Academy	Shafahi, A (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	ashafahi@cs.umd.edu; najibi@cs.umd.edu; amin@cs.umd.edu; xuzh@cs.umd.edu; john@cs.umd.edu; studer@cornell.edu; lsd@umiacs.umd.edu; taylor@usna.edu; tomg@cs.umd.edu			DARPA GARD; DARPA QED; DARPA L2M; YFA program; AFOSR MURI; Office of the Director of National Intelligence (ODNI); IARPA [2014-14071600012]; Xilinx, Inc.; US NSF [ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, ECCS-1824379]; Office of Naval Research [N0001418WX01582]; Department of Defense High Performance Computing Modernization Program	DARPA GARD; DARPA QED; DARPA L2M; YFA program; AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI); Office of the Director of National Intelligence (ODNI); IARPA; Xilinx, Inc.; US NSF(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); Department of Defense High Performance Computing Modernization Program(United States Department of Defense)	Goldstein and his students were supported by DARPA GARD, DARPA QED for RML, DARPA L2M, and the YFA program. Additional support was provided by the AFOSR MURI program. Davis and his students were supported by the Office of the Director of National Intelligence (ODNI), and IARPA (2014-14071600012). Studer was supported by Xilinx, Inc. and the US NSF under grants ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, and ECCS-1824379. Taylor was supported by the Office of Naval Research (N0001418WX01582) and the Department of Defense High Performance Computing Modernization Program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	[Anonymous], 2017, ARXIV171209196; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Athalye Anish, 2018, ICML; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Buckman Jacob, 2018, P ICLR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cohen Jeremy M, 2019, ARXIV190202918; Dhillon GS., 2018, P 6 INT C LEARN REPR; Engstrom Logan, 2018, ARXIV180710272; Fischer, 2018, AAAI; Goodfellow I. J., 2015, ICLR; Guo Chuan, 2017, ARXIV171100117; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; Jakubovitz D, 2018, EUR C COMP VIS; Kannan Harini, 2018, ARXIV180306373; Kurakin Alexey, 2016, ICLR; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Lecuyer M., 2018, ARXIV180203471; Li B., 2018, ABS180903113 CORR; Li H, 2018, ADV NEUR IN, V31; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Lin Ji, 2019, ICLR; Ma Xingjun, 2018, INT C LEARN REPR; Madry A., 2017, ICLR; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Mosbach Marius, 2018, ARXIV181012042; Poursaeed Omid, 2018, CVPR; Qian H., 2018, ARXIV180207896; Raghunathan A., 2018, NEURIPS; Raghunathan Aditi, 2018, INT C LEARN REPR; Ross Andrew Slavin, 2018, AAAI C ART INT; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shafahi Ali, 2019, ICLR; Shafahi Ali, 2018, ARXIV181111304; Shafahi Ali, 2019, LABEL SMOOTHING LOGI; Song Y, 2017, ARXIV171010766; Szegedy C., 2013, ICLR; Tramer F., 2017, ARXIV170507204; Tsipras Dimitris, 2018, ARXIV180512152; van den Oord Aaron, 2018, ARXIV180205666; Wang Shiqi, 2018, ARXIV18110262; Wong E, 2018, ARXIV180512514; Wong Eric, 2017, ICML; Xiao Chaowei, 2018, IJCAI; Xie C, 2019, CVPR; Xie Cihang, 2017, ARXIV171101991; Xu W., 2017, P 25 ANN NETW DISTR, DOI DOI 10.14722/NDSS.2018.23198; Yu F., 2018, ARXIV181000144; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang D., 2019, ARXIV PREPRINT ARXIV; Zhang H., 2019, ICML	51	141	147	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303036
C	Kumar, K; Kumar, R; de Boissiere, T; Gestin, L; Teoh, WZ; Sotelo, J; de Brebisson, A; Bengio, Y; Courville, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumar, Kundan; Kumar, Rithesh; de Boissiere, Thibault; Gestin, Lucas; Teoh, Wei Zhen; Sotelo, Jose; de Brebisson, Alexandre; Bengio, Yoshua; Courville, Aaron			MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 108OTi GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.	[Kumar, Kundan; Kumar, Rithesh; de Boissiere, Thibault; Gestin, Lucas; Teoh, Wei Zhen; Sotelo, Jose; de Brebisson, Alexandre] Lyrebird AI, San Francisco, CA 94102 USA; [Kumar, Kundan; Sotelo, Jose; de Brebisson, Alexandre; Bengio, Yoshua; Courville, Aaron] Mila, Montreal, PQ, Canada; [Kumar, Kundan; Bengio, Yoshua; Courville, Aaron] Univ Montreal, Montreal, PQ, Canada	Universite de Montreal	Kumar, K (corresponding author), Lyrebird AI, San Francisco, CA 94102 USA.; Kumar, K (corresponding author), Mila, Montreal, PQ, Canada.; Kumar, K (corresponding author), Univ Montreal, Montreal, PQ, Canada.	kundan@descript.com; rithesh@descript.com			NSERC; Canada CIFAR AI Chairs; Canada Research Chairs; IVADO	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada CIFAR AI Chairs; Canada Research Chairs(Canada Research ChairsCGIAR); IVADO	The authors would like to thank NSERC, Canada CIFAR AI Chairs, Canada Research Chairs and IVADO for funding.	[Anonymous], 2019, ARXIV190208710; Arik SO, 2017, PR MACH LEARN RES, V70; Cobo, 2017, ARXIV171110433; Dieleman S., 2018, ADV NEURAL INFORM PR, P7989; Donahue C, 2018, ARXIV PREPRINT ARXIV; Dosovitskiy Alexey, 2016, NEURIPS; Engel J, 2017, PR MACH LEARN RES, V70; GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Ito K, 2017, LJ SPEECH DATASET; Jia Z., 2018, ARXIV180406826 CS; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kalchbrenner N., 2018, ARXIV180208435; Karras T., 2018, ARXIV181204948; Karras T, 2017, ARXIV171010196; Kingma D. P., 2018, P ADV NEUR INF PROC, P10215; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Lempitsky V., 2016, ARXIV160708022V3; Lim Jae Hyun, 2017, ABS170502894 ARXIV; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mathieu M., 2016, INT C LEARN REPR ICL; Mehri S., 2016, ARXIV161207837; Miyato T., 2018, INT C LEARN REPR, P2; Mor N., 2019, INT C LEARN REPR; Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457; Neekhara P., 2019, ARXIV190407944; Odena A., 2016, DISTILL, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003]; Park Taesung, 2019, ARXIV190307291; Ping W., 2018, INT C LEARN REPR; Ping Wei, 2017, ICLR; Prenger R., 2019, ICASSP 2019 2019 IEE, P3617; Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779; Sotelo J., 2017, CHAR2WAV END TO END; Thickstun J., 2018, INT C AC SPEECH SIGN; Van Den Oord A., 2016, SSW, V125; van den Oord Aaron, 2017, CORR, P6306; Veaux C., 2017, CSTR VCTK CORPUS ENG; Wang T.-C., 2018, ADV NEURAL INFORM PR; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang Y., 2017, ARXIV170310135; Yamamoto R., 2019, ARXIV190404472; Yamamoto R., 2019, R9Y9 WAVENET VOCODER; Zhang H., 2018, ARXIV180508318; Zhou Tinghui, 2018, ARXIV180807371; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	50	140	142	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906055
C	Michel, P; Levy, O; Neubig, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Michel, Paul; Levy, Omer; Neubig, Graham			Are Sixteen Heads Really Better than One?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headedattention is a driving force behind many recent state-of-the-art natural language processing (NLP) models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention(1).	[Michel, Paul; Neubig, Graham] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA; [Levy, Omer] Facebook Artificial Intelligence Res, Seattle, WA USA	Carnegie Mellon University; Facebook Inc	Michel, P (corresponding author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.	pmichell@cs.cmu.edu; omerlevy@fb.com; gneubig@cs.cmu.edu						Ahmed Karim, 2017, ARXIV171102132; Anwar S., 2017, JETC; Bengio Y., 2014, ARXIV14061078; Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8; Cettolo Mauro, 2015, P 2014 INT WORKSH SP; Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI [10.1109/CIS.2016.133, 10.1109/CIS.2016.0134]; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Dai Zihang, 2019, ARXIV190102860, P2; Devlin J., 2018, P 2019 C N AM CHAPT, DOI [DOI 10.18653/V1/N19-1423, 10.18653/v1/N19-1423]; Dolan William B., 2005, P 3 INT WORKSH PAR I; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; Kim Yoon, 2016, ARXIV160607947, DOI [10.18653/v1/D16-1139, DOI 10.18653/V1/D16-1139]; Koehn P, 2007, 45 ANN M ASS COMP LI, P177, DOI DOI 10.3115/1557769.1557821; Koehn Philipp, 2004, EMNLP; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; Michel Paul, 2018, P 2018 C EMP METH NA, P543, DOI DOI 10.18653/V1/D18-1050; Molchanov P., 2017, P INT C LEARN REPR I, P1; Murray Kenton, 2015, P 2015 C EMP METH NA, P908; Neubig Graham, 2019, M N AM CHAPT ASS COM; Ott Myle, 2018, P 3 C MACH TRANSL RE, P1, DOI DOI 10.18653/V1/W18-6301; Paulus Romain, 2017, ARXIV170504304; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Raganato Alessandro, 2018, P 2018 EMNLP WORKSH, P7; See Abigail, 2016, P 20 SIGNLL C COMP N, P291, DOI DOI 10.18653/V1/K16-1029; Shen Tao, 2018, P 32 M ASS ADV ART I; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Strubell Emma, 2018, P 2018 C EMP METH NA, P5027, DOI [DOI 10.18653/V1/D18-1548, 10.18653/v1/D18-1548]; T_ackstr_om O, 2016, P 2016 C EMP METH NA; Tang G., 2018, PROC C EMPIR METHODS, DOI DOI 10.18653/V1/D18-1458; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Voita E, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1198; Warstadt Alex, 2018, ARXIV180512471; Williams A., 2018, P C N AM CHAPT ASS C, V1, P1112, DOI DOI 10.18653/V1/N18-1101	38	140	142	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905066
C	Chua, K; Calandra, R; McAllister, R; Levine, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chua, Kurtland; Calandra, Roberto; McAllister, Rowan; Levine, Sergey			Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PREDICTIVE CONTROL; BACKPROPAGATION	Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).	[Chua, Kurtland; Calandra, Roberto; McAllister, Rowan; Levine, Sergey] Univ Calif Berkeley, Berkeley Artificial Intelligence Res, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Chua, K (corresponding author), Univ Calif Berkeley, Berkeley Artificial Intelligence Res, Berkeley, CA 94720 USA.	kchua@berkeley.edu; roberto.calandra@berkeley.edu; rmcallister@berkeley.edu; svlevine@berkeley.edu	Calandra, Roberto/X-1789-2019	Calandra, Roberto/0000-0001-9430-8433				Abbeel P., 2006, MACHINE LEARNING P 2, P1, DOI DOI 10.1145/1143844.1143845; Agrawal P., 2016, ADV NEURAL INFORM PR, P5074; Atkeson C. G., 1997, INT C ROB AUT ICRA; Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Botev ZI, 2013, HANDB STAT, V31, P35, DOI 10.1016/B978-0-444-53859-8.00003-5; BROOKS SH, 1958, OPER RES, V6, P244, DOI 10.1287/opre.6.2.244; Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626; Camacho EF., 2013, MODEL PREDICTIVE CON; Candela JQ, 2003, INT CONF ACOUST SPEE, P701; Chebotar Y, 2017, PR MACH LEARN RES, V70; Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218; Depeweg S., 2016, ARXIV E PRINTS; Depeweg Stefan, 2018, ICML; Draeger A., 1995, IEEE Control Systems Magazine, V15, P61, DOI 10.1109/37.466261; Efron B., 1994, INTRO BOOTSTRAP; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Fu J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4019, DOI 10.1109/IROS.2016.7759592; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gal Y, 2017, PROC 31 INT C NEURAL, P3584; Girard A, 2002, ADV NEURAL INFORM PR, V15, P529; Grancharova A, 2008, AUTOMATICA, V44, P1621, DOI 10.1016/j.automatica.2008.04.002; Gu SX, 2016, PR MACH LEARN RES, V48; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Haarnoja T, 2018, PR MACH LEARN RES, V80; Hernandez E., 1990, Proceedings of the 1990 American Control Conference (IEEE Cat. No.90CH2896-9), P2454; Hernandez-Lobato J. M., 2016, INT C MACH LEARN ICM, V48, P1511; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hesse C., 2017, OPENAI BASELINES; Higuera JCG, 2018, IEEE INT C INT ROBOT, P2538, DOI 10.1109/IROS.2018.8594018; Kamthe S, 2018, PR MACH LEARN RES, V84; Ko J, 2007, IEEE INT CONF ROBOT, P742, DOI 10.1109/ROBOT.2007.363075; Kober J., 2009, PROC NEURAL INF PROC, P849; Kocijan J, 2004, P AMER CONTR CONF, P2214; Kupcsik A, 2013, AAAI; Kurutach T., 2018, P INT C LEARN REPR; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Le, 2017, ARXIV PREPRINT ARXIV; Lenz I, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Lin L, 1992, THESIS; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; McAllister RT, 2017, ADV NEUR IN, V30; MILLER WT, 1990, IEEE T ROBOTIC AUTOM, V6, P1, DOI 10.1109/70.88112; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mordatch I, 2016, IEEE INT CONF ROBOT, P242, DOI 10.1109/ICRA.2016.7487140; Nagabandi A., 2017, ARXIV170802596; Neal R. M., 2012, BAYESIAN LEARNING NE; Nguyen-Tuong D., 2008, ADV NEURAL INFORM PR, P1193; Osband I, 2016, NIPS WORKSH BAYES DE; Parmas P, 2018, PR MACH LEARN RES, V80; Punjani A, 2015, IEEE INT CONF ROBOT, P3223, DOI 10.1109/ICRA.2015.7139643; Rasmussen CE, 2004, ADV NEUR IN, V16, P751; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Thrun Sebastian B, 1992, EFFICIENT EXPLORATIO; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109	60	140	142	4	16	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304074
C	Denton, E; Birodkar, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Denton, Emily; Birodkar, Vighnesh			Unsupervised Learning of Disentangled Representations from Video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a new model DRNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.	[Denton, Emily; Birodkar, Vighnesh] NYU, Dept Comp Sci, New York, NY 10003 USA	New York University	Denton, E (corresponding author), NYU, Dept Comp Sci, New York, NY 10003 USA.	denton@cs.nyu.edu; vighneshbirodkar@nyu.edu	Jeong, Yongwook/N-7413-2016		Google Fellowship	Google Fellowship(Google Incorporated)	We thank Rob Fergus, Will Whitney and Jordan Ash for helpful comments and advice. Emily Denton is grateful for the support of a Google Fellowship	Agrawal P., 2016, ADV NEURAL INFORM PR, P5074; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chiappa S., 2017, ICLR; Cricri F., 2016, ABS161201756 CORR; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl Will, 2016, ARXIV161204440; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Kalchbrenner N, 2016, ARXIV161000527; Kingma D.P, P 3 INT C LEARNING R; Kulkarni TD, 2015, ADV NEUR IN, V28; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; LeCun Y, 2004, PROC CVPR IEEE, P97; Liu Ce, 2009, THESIS, P2; Mathieu Michael, 2015, ARXIV151105440; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranzato MarcAurelio, 2014, ARXIV14126604; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Salimans T, 2016, ADV NEUR IN, V29; Salimans Tim, 2017, ARXIV170105517; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; van den Oord A, 2016, PR MACH LEARN RES, V48; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Whitney W. F., 2016, ARXIV150204623; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614	40	139	140	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404047
C	Scholkopf, B; Simard, P; Smola, A; Vapnik, V		Jordan, MI; Kearns, MJ; Solla, SA		Scholkopf, B; Simard, P; Smola, A; Vapnik, V			Prior knowledge in Support Vector kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions.	Max Planck Inst Biol Kybernet, D-72076 Tubingen, Germany	Max Planck Society	Scholkopf, B (corresponding author), Max Planck Inst Biol Kybernet, D-72076 Tubingen, Germany.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925					0	139	149	0	13	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						640	646						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700091
C	Kosaraju, V; Sadeghian, A; Martin-Martin, R; Reid, I; Rezatofighi, SH; Savarese, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kosaraju, Vineet; Sadeghian, Amir; Martin-Martin, Roberto; Reid, Ian; Rezatofighi, S. Hamid; Savarese, Silvio			Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Predicting the future trajectories of multiple interacting agents in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human's future trajectory. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions by better modelling the social interactions of pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns reliable feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans' paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.	[Kosaraju, Vineet; Sadeghian, Amir; Martin-Martin, Roberto; Rezatofighi, S. Hamid; Savarese, Silvio] Stanford Univ, Stanford, CA 94305 USA; [Sadeghian, Amir] Aibee Inc, Beijing, Peoples R China; [Reid, Ian; Rezatofighi, S. Hamid] Univ Adelaide, Adelaide, SA, Australia	Stanford University; University of Adelaide	Kosaraju, V (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	vineetk@stanford.edu		Martin-Martin, Roberto/0000-0002-9586-2759; Reid, Ian/0000-0001-7790-6423	TRI gift; ONR [1165419-10-TDAUZ]; Nvidia; Samsung	TRI gift; ONR(Office of Naval Research); Nvidia; Samsung(Samsung)	The research reported in this publication was supported by funding from the TRI gift, ONR (1165419-10-TDAUZ), Nvidia, and Samsung.	Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Alahi A, 2014, PROC CVPR IEEE, P2211, DOI 10.1109/CVPR.2014.283; Amirian Javad, 2019, ABS190409507 CORR; [Anonymous], 2017, ADV NEURAL INFORM PR; Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365; Ballan L, 2016, LECT NOTES COMPUT SC, V9905, P697, DOI 10.1007/978-3-319-46448-0_42; Bartoli F., 2017, ARXIV170502503; Casanova A., 2018, ABS171010903 CORR; Chen X, 2016, ADV NEUR IN, V29; Fernando T., 2017, ARXIV170205552; Fernando T., 2017, ARXIV170304706; Forestier Sebastien, 2016, 30 ANN C NEUR INF PR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta Agrim, 2018, ARXIV180310892; Hasan I, 2018, IEEE WINT CONF APPL, P1178, DOI 10.1109/WACV.2018.00134; HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282; Hug R., 2018, ARXIV180405546; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Kantorovitch J, 2014, J ASSIST TECHNOL, V8, P64, DOI 10.1108/JAT-08-2013-0024; Kipf T. N., 2017, ABS160902907 CORR; Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15; Lee N, 2017, DESIRE DISTANT FUTUR; Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x; Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493; Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641; Mirza M., 2014, ARXIV PREPRINT ARXIV; Morris BT, 2008, IEEE T CIRC SYST VID, V18, P1114, DOI 10.1109/TCSVT.2008.927109; Mould N., 2014, J POLICING INTELLIGE, V9, P151, DOI DOI 10.1080/18335330.2014.940819; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; Pellegrini S, 2010, LECT NOTES COMPUT SC, V6311, P452, DOI 10.1007/978-3-642-15549-9_33; Robicquet A, 2016, LECT NOTES COMPUT SC, V9912, P549, DOI 10.1007/978-3-319-46484-8_33; Sadeghian A., 2017, ARXIV171110061; Sadeghian Amir, 2019, CVPR; Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu Jun-Yan, 2017, NIPS	38	137	143	2	20	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300013
C	Bradley, PS; Mangasarian, OL; Street, WN		Mozer, MC; Jordan, MI; Petsche, T		Bradley, PS; Mangasarian, OL; Street, WN			Clustering via concave minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The problem of assigning m points in the n-dimensional real space R-n to k clusters is formulated as that of determining if centers in R-n such that the sum of distances of each point to the nearest center is minimized. If a polyhedral distance is used, the problem can be formulated as that of minimizing a piecewise-linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program: minimizing a bilinear function on a polyhedral set. A fast finite k-Median Algorithm consisting of solving few linear programs in closed form leads to a stationary point of the bilinear program. Computational testing an a number of real-world databases was carried out. On the Wisconsin Diagnostic Breast Cancer (WDBC) database, k-Median training set correctness was comparable to that of the k-Mean Algorithm, however its testing set correctness was better. Additionally, on the Wisconsin Prognostic Breast Cancer (WPBC) database, distinct and clinically important survival curves were extracted by the k-Median Algorithm, whereas the k-Mean Algorithm failed to obtain such distinct survival curves for the same database.			Bradley, PS (corresponding author), UNIV WISCONSIN,DEPT COMP SCI,1210 W DAYTON ST,MADISON,WI 53706, USA.								0	137	149	1	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						368	374						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00052
C	Shrestha, SB; Orchard, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shrestha, Sumit Bam; Orchard, Garrick			SLAYER: Spike Layer Error Reassignment in Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPUTATIONAL POWER; NETWORKS; NEURON; RESUME	Configuring deep Spiking Neural Networks (SNNs) is an exciting research avenue for low power spike event based computation. However, the spike generation function is non-differentiable and therefore not directly compatible with the standard error backpropagation algorithm. In this paper, we introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers. We describe and release a GPU accelerated software implementation of our method which allows training both fully connected and convolutional neural network (CNN) architectures. Using our software, we compare our method against existing SNN based learning approaches and standard ANN to SNN conversion techniques and show that our method achieves state of the art performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS datasets.	[Shrestha, Sumit Bam; Orchard, Garrick] Natl Univ Singapore, Temasek Labs NUS, Singapore 117411, Singapore	National University of Singapore	Shrestha, SB (corresponding author), Natl Univ Singapore, Temasek Labs NUS, Singapore 117411, Singapore.	tslsbs@nus.edu.sg; tslgmo@nus.edu.sg	Bhatia, Sumit/HGE-1687-2022					Abdollahi M, 2011, BIOMED CIRC SYST C, P269, DOI 10.1109/BioCAS.2011.6107779; Amir A., 2017, IEEE C COMP VIS PATT; Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0; Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184; Dauwels J, 2008, ADV NEUROINFORMATION, P177; Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359; Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758; DIEHL PU, 2015, IEEE IJCNN; Esser S. K., 2015, ADV NEURAL INFORM PR, P1117; Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113; Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638; GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738; Gerstner W., 2002, SPIKING NEURON MODEL; Gutig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643; Hunsberger E., 2015, CORR; Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846; Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508; Leonard R.G., 1993, TIDIGITS SPEECH CORP; Liu Q., 2017, CORR; Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1; Maass W., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P54, DOI 10.1145/267460.267477; Maass W, 1996, ADV NEUR IN, V8, P211; McKennoch S, 2006, IEEE IJCNN, P3970; Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642; Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128; O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178; Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]; Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212; Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005; Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901; Ramesh Bharath, 2017, CORR; Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682; Schrauwen Benjamin, 2004, NEUR NETW 2004 P 200, V1; Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010; Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938; Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95; Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221; Zenke F., 2017, ARXIV170511146	38	136	138	4	13	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301040
C	Yang, MH; Roth, D; Ahuja, N		Solla, SA; Leen, TK; Muller, KR		Yang, MH; Roth, D; Ahuja, N			A SNoW-based face detector	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				RECOGNITION	A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Yang, MH (corresponding author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.	mhyang@vision.ai.uiuc.edu; danr@cs.uiuc.edu; ahuja@vision.ai.uiuc.edu						Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BLUM A, 1992, MACH LEARN, V9, P373, DOI 10.1023/A:1022653502461; CARLESON A, 1999, UIUCDCSR992101; Colmenarez AJ, 1997, PROC CVPR IEEE, P782, DOI 10.1109/CVPR.1997.609415; Golding AR, 1999, MACH LEARN, V34, P107, DOI 10.1023/A:1007545901558; GRAHAM DB, 1998, NATO ASI SERIES F, V163, P446; HALLINAN P, 1995, THESIS HARVARD U; HELMBOLD DP, 1995, J COMPUT SYST SCI, V50, P551, DOI 10.1006/jcss.1995.1044; Kivinen J., 1995, P ANN ACM S THEOR CO; LITTLESTONE N, 1991, PROCEEDINGS OF THE FOURTH ANNUAL WORKSHOP ON COMPUTATIONAL LEARNING THEORY, P147; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; MUNOZ M, 1999, EMNLP VLC 99 JOINT S; Osuna E, 1997, PROC CVPR IEEE, P130, DOI 10.1109/CVPR.1997.609310; PHILLIPS PJ, 1998, NATO ASI SERIES F, V163, P244; Qian RJ, 1997, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.1997.609318; Rajagopalan AN, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P640, DOI 10.1109/ICCV.1998.710785; Roth D, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P806; Roth D, 1998, COLING ACL 98 17 INT, P1136; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Rowley HA, 1998, PROC CVPR IEEE, P38, DOI 10.1109/CVPR.1998.698585; SAMARIA F, 1994, THESIS U CAMBRIDGE; Schneiderman H, 1998, PROC CVPR IEEE, P45, DOI 10.1109/CVPR.1998.698586; Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; YANG MH, 1999, P IEEE INT C IM PROC; YANG MH, 2000, P 4 IEEE INT C AUT F	27	136	143	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						862	868						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700122
C	Roth, K; Lucchi, A; Nowozin, S; Hofmann, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Roth, Kevin; Lucchi, Aurelien; Nowozin, Sebastian; Hofmann, Thomas			Stabilizing Training of Generative Adversarial Networks through Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.	[Roth, Kevin; Lucchi, Aurelien; Hofmann, Thomas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Nowozin, Sebastian] Microsoft Res, Cambridge, England	Swiss Federal Institutes of Technology Domain; ETH Zurich; Microsoft	Roth, K (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	kevin.roth@inf.ethz.ch; aurelien.lucchi@inf.ethz.ch; sebastian.Nowozin@microsoft.com; thomas.hofmann@inf.ethz.ch	Jeong, Yongwook/N-7413-2016		Microsoft Research through its PhD Scholarship Programme	Microsoft Research through its PhD Scholarship Programme(Microsoft)	We would like to thank Devon Hjelm for pointing out that the regularizer works well with ResNets. KR is thankful to Yannic Kilcher, Lars Mescheder and the dalab team for insightful discussions. Big thanks also to Ishaan Gulrajani and Taehoon Kim for their open-source GAN implementations. This work was supported by Microsoft Research through its PhD Scholarship Programme.	Amari S.-I., 2007, METHODS INFORM GEOME; An GZ, 1996, NEURAL COMPUT, V8, P643, DOI 10.1162/neco.1996.8.3.643; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arjovsky M, 2017, PR MACH LEARN RES, V70; BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108; Bouchacourt D, 2016, ADV NEURAL INFORM PR, P352; Che Tong, 2016, ARXIV161202136; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS, P1; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Metz Luke, 2016, INT C LEARN REPR ICL; Minka, 2005, DIVERGENCE MEASURES; Narayanan H., 2010, NEURIPS, P1786; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Nowozin S, 2016, ADV NEUR IN, V29; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reid MD, 2011, J MACH LEARN RES, V12, P731; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Scott DW, 2015, WILEY SER PROBAB ST, P1, DOI 10.1002/9781118575574; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Sriperumbudur B. K., 2009, ARXIV PREPRINT ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7	31	135	137	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402007
C	Jayaraman, D; Grauman, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Jayaraman, Dinesh; Grauman, Kristen			Zero-Shot Recognition with Unreliable Attributes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses-even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.	[Jayaraman, Dinesh; Grauman, Kristen] Univ Texas Austin, Austin, TX 78701 USA	University of Texas System; University of Texas Austin	Jayaraman, D (corresponding author), Univ Texas Austin, Austin, TX 78701 USA.	dineshj@cs.utexas.edu; grauman@cs.utexas.edu	Jayaraman, Dinesh/AAI-2527-2021	Jayaraman, Dinesh/0000-0002-6888-3095	NSF [IIS-1065390]; ONR ATL	NSF(National Science Foundation (NSF)); ONR ATL	We thank Christoph Lampert and Felix Yu for helpful discussions and sharing their code. This research is supported in part by NSF IIS-1065390 and ONR ATL.	Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Farhadi Ali, 2009, CVPR; Frome Andrea, 2013, NEURIPS; Gardenfors P., 2000, CONCEPTUAL SPACES GE, V106; Jayaraman D, 2014, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2014.211; Kankuekul P., 2012, CVPR; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Larochelle H., 2008, AAAI, V1, P3; Mahajan D., 2011, ICCV; Maji S., 2011, CVPR, DOI DOI 10.1109/CVPR.2011.5995631; Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313; Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35; Mittelman R, 2013, CVPR; Olaru C, 2003, FUZZY SET SYST, V138, P221, DOI 10.1016/S0165-0114(03)00089-7; OSHERSON D, 1994, THEOR DECIS, V36, P103, DOI 10.1007/BF01079209; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63; Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627; Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121; Rosch E., 1978, COGNITION CATEGORIZA, P27, DOI DOI 10.1037/0012-1649.16.5.391; Sharmanska V, 2012, LECT NOTES COMPUT SC, V7576, P242, DOI 10.1007/978-3-642-33715-4_18; Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6; Tsang S, 2011, IEEE T KNOWL DATA EN, V23, P64, DOI 10.1109/TKDE.2009.175; Turakhia N., 2013, ICCV; Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105; Yu X., 2010, ECCV	31	134	143	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100069
C	Nachum, O; Gu, SX; Lee, H; Levine, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nachum, Ofir; Gu, Shixiang; Lee, Honglak; Levine, Sergey			Data-Efficient Hierarchical Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FRAMEWORK	Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations,(1) learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.(2)	[Nachum, Ofir; Gu, Shixiang; Lee, Honglak; Levine, Sergey] Google Brain, Mountain View, CA 94043 USA; [Gu, Shixiang] Univ Cambridge, Cambridge, England; [Gu, Shixiang] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Levine, Sergey] Univ Calif Berkeley, Berkeley, CA USA	Google Incorporated; University of Cambridge; Max Planck Society; University of California System; University of California Berkeley	Nachum, O (corresponding author), Google Brain, Mountain View, CA 94043 USA.	ofirnachum@google.com; shanegu@google.com; honglak@google.com; slevine@google.com						[Anonymous], 2016, P ADV NEUR INF PROC; [Anonymous], ARXIV170506366; [Anonymous], 2018, INT C LEARN REPR; [Anonymous], 2018, ARXIV180209464; Barto AG, 2003, DISCRETE EVENT DYN S, V13, P343; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Daniel C., 2012, ARTIF INTELL, P273; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Duan Y, 2016, INT C MACH LEARN, P1329; Florensa Carlos, 2017, ARXIV170403012; Fujimoto S., 2018, ARXIV180209477; Gu Shixiang, 2016, ARXIV161102247; Haarnoja T., 2018, P 35 INT C MACH LEAR; Harb Jean, 2017, ARXIV170904571; Heess N., 2017, ABS170702286 CORR; Heess N., 2016, ARXIV161005182; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Klein U., 2004, P 21 INT C MACH LEAR, P71, DOI DOI 10.1145/1015330.1015355; Konidaris G, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P895; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Levine Sergey, 2018, TEMPORAL DIFFERENCE; Levy A, 2017, ARXIV PREPRINT ARXIV; Lillicrap T., 2018, INT C LEARN REPR; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mahadevan S, 2007, J MACH LEARN RES, V8, P2169; Nachum O., 2017, ARXIV170701891; Parr R, 1998, ADV NEUR IN, V10, P1043; Pong V., 2018, INT C LEARN REPR ICL; Precup D., 2000, TEMPORAL ABSTRACTION; Rajeswaran A., 2017, ARXIV PREPRINT ARXIV; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Shixiang Gu, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3389, DOI 10.1109/ICRA.2017.7989385; Sigaud O., 2018, ARXIV180304706; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tessler C., 2017, AAAI, V3, P6; Vecerik Mel, 2017, LEVERAGING DEMONSTRA; Vezhnevets A. S., 2017, ARXIV170301161	43	133	139	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303031
C	Paciorek, CJ; Schervish, MJ		Thrun, S; Saul, K; Scholkopf, B		Paciorek, CJ; Schervish, MJ			Nonstationary covariance functions for Gaussian process regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				BAYESIAN REGRESSION; SPLINES	We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matern stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from fixing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP fitting may allow for implementation of the method on larger datasets.	Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Paciorek, CJ (corresponding author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.	paciorek@alumni.cmu.edu; mark@stat.cmu.edu						Biller C, 2000, J COMPUT GRAPH STAT, V9, P122, DOI 10.2307/1390616; BRUNTZ SM, 1974, S ATM DIFF AIR POLL, P125; Damian D, 2001, ENVIRONMETRICS, V12, P161, DOI 10.1002/1099-095X(200103)12:2<161::AID-ENV452>3.0.CO;2-G; Denison DGT, 1998, STAT COMPUT, V8, P337, DOI 10.1023/A:1008824606259; DiMatteo I, 2001, BIOMETRIKA, V88, P1055, DOI 10.1093/biomet/88.4.1055; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Higdon D, 1999, BAYESIAN STATISTICS 6, P761; Holmes CC, 2001, J ROY STAT SOC B, V63, P3, DOI 10.1111/1467-9868.00272; MACKAY D, 1995, INTERPOLATION MODELS; Paciorek C. J., 2003, THESIS CARNEGIE MELL; RASMUSSEN CE, 2002, ADV NEURAL INFORMATI, V14; SCHMIDT AM, 2000, 49800 U SHEFFI; SEEGER M, 2003, WORKSH AI STAT, V9; SMOLA AJ, 2001, ADV NEURAL INFORMATI, V13; Stein M.L., 1999, INTERPOLATION SPATIA; Tresp V, 2001, ADV NEUR IN, V13, P654; VIVARELLI F, 1999, ADV NEURAL INFORMATI, V11; Wood SA, 2002, BIOMETRIKA, V89, P513, DOI 10.1093/biomet/89.3.513	20	133	134	1	8	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						273	280						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500035
C	Yun, S; Jeong, M; Kim, R; Kang, J; Kim, HJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yun, Seongjun; Jeong, Minbyul; Kim, Raehyun; Kang, Jaewoo; Kim, Hyunwoo J.			Graph Transformer Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-called meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.	[Yun, Seongjun; Jeong, Minbyul; Kim, Raehyun; Kang, Jaewoo; Kim, Hyunwoo J.] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea	Korea University	Kang, J; Kim, HJ (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.	ysj5419@korea.ac.kr; minbyuljeong@korea.ac.kr; raehyun@korea.ac.kr; kangj@korea.ac.kr; hyunwoojkim@korea.ac.kr		Kim, Hyunwoo/0000-0002-2181-9264	National Research Foundation of Korea (NRF) - Korea government (MSIT) [NRF-2019R1G1A1100626, NRF-2016M3A9A7916996, NRF-2017R1A2A1A17069645]	National Research Foundation of Korea (NRF) - Korea government (MSIT)	This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2019R1G1A1100626, NRF-2016M3A9A7916996, NRF-2017R1A2A1A17069645).	[Anonymous], 2016, P 22 ACM SIGKDD INT; Bhagat S, 2011, SOCIAL NETWORK DATA ANALYTICS, P115; Bloem P., 2018, P 15 EUR SEM WEB C E, P593, DOI [10.1007/978-3-319-93417-4_38, DOI 10.1007/978-3-319-93417-4_38]; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Chen J., 2018, INT C LEARN REPR; Chen J., 2017, ARXIV171010568; Chen YP, 2018, ADV NEUR IN, V31; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Dong YX, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P135, DOI 10.1145/3097983.3098036; Duvenaud David K, 2015, P NIPS; Gilmer J, 2017, PR MACH LEARN RES, V70; Hamilton W. L., 2017, ABS170602216 CORR; Henaff M., 2015, ABS150605163 CORR; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kim R., 2019, HATS HIERARCHICAL GR; Kipf T.N., 2017, INT C LEARN REPR; Kipf Thomas N, 2016, NIPS WORKSHOP BAYESI; Ktena S. I., 2017, CORR; Lee Junghwan, 2019, CORR; Lei T, 2017, PR MACH LEARN RES, V70; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Linmei H., 2019, P 2019 C EMP METH NA; Liu Z., 2018, ARXIV180200910, V33, P4424; Monti F., 2017, ADV NEURAL INFORM PR, P3697; Monti F., 2016, ARXIV161108402; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Scarselli F., 2009, IEEE T NEURAL NETWOR; Shi C, 2017, IEEE T KNOWL DATA EN, V29, P17, DOI 10.1109/TKDE.2016.2598561; Tang J., 2015, WWW; van den Berg R., 2018, P 24 ACM SIGKDD INT; Velickovic P., 2018, INT C LEARN REPR; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Wang X, 2019, ABS190307293 CORR; Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989; Xu Bingbing, 2019, 7 INT C LEARN REPR; Ying R., 2018, ABS180608804 CORR; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890; Zhang CX, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P793, DOI 10.1145/3292500.3330961; Zhang YZ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P399, DOI 10.1145/3178876.3186106; Zhao S, 2018, INT J GENOMICS, V2018, DOI 10.1155/2018/8931651	42	132	137	3	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903058
C	Rezende, DJ; Eslami, SMA; Mohamed, S; Battaglia, P; Jaderberg, M; Heess, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rezende, Danilo Jimenez; Eslami, S. M. Ali; Mohamed, Shakir; Battaglia, Peter; Jaderberg, Max; Heess, Nicolas			Unsupervised Learning of 3D Structure from Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A key goal of computer vision is to recover the underlying 3D structure that gives rise to 2D observations of the world. If endowed with 3D understanding, agents can abstract away from the complexity of the rendering process to form stable, disentangled representations of scene elements. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet [2], and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained jointly, end-to-end, and directly from 2D images without any use of ground-truth 3D labels. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner.	[Rezende, Danilo Jimenez; Eslami, S. M. Ali; Mohamed, Shakir; Battaglia, Peter; Jaderberg, Max; Heess, Nicolas] Google DeepMind, London, England	Google Incorporated	Rezende, DJ (corresponding author), Google DeepMind, London, England.	danilor@google.com; aeslami@google.com; shakir@google.com; peterbattaglia@google.com; jaderberg@google.com; heess@google.com		Mohamed, Shakir/0000-0002-1184-5776				Burda Yuri, 2015, ICLR; Chang Angel X., 2015, ARXIV151203012CSGR P; Choy C. B., 2016, 160400449 ARXIV; Del Pero L, 2012, PROC CVPR IEEE, P2719, DOI 10.1109/CVPR.2012.6247994; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Eslami S., 2016, ATTEND INFER REPEAT; Gregor K., 2016, 160408772 ARXIV; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jiajun W., 2016, 161007584 ARXIV; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P, P 3 INT C LEARNING R; Kulkarni T.D., 2014, NIPS; Kulkarni T. D., 2014, ARXIV14071339; LeCun Yann, MNIST DATABASE HANDW; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Mansinghka V., 2013, ADV NEURAL INFORM PR; Mnih A., 2016, 160206725 ARXIV; OpenGL Architecture Review Board, 1993, OPENGL REF MAN OFF R; Rezende D. Jimenez, 2016, 160305106 ARXIV; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Sundareswara R, 2008, J VISION, V8, DOI 10.1167/8.5.12; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wingate D., 2011, NEURAL INFORM PROCES, P1152; Wu J., 2015, ADV NEURAL INF PROCE, V28, P1, DOI DOI 10.1007/978-3-319-26532-2_15; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Zhou T., 2016, VIEW SYNTHESIS APPEA	29	132	134	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705014
C	Yuille, AL; Rangarajan, A		Dietterich, TG; Becker, S; Ghahramani, Z		Yuille, AL; Rangarajan, A			The Concave-Convex procedure (CCCP)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				STATISTICAL PHYSICS; ALGORITHM	We introduce the Concave-Convex procedure (CCCP) which constructs discrete time iterative dynamical systems which are guaranteed to monotonically decrease global optimization/energy functions. It can be applied to (almost) any optimization problem and many existing algorithms can be interpreted in terms of CCCP. In particular, we prove relationships to some applications of Legendre transform techniques. We then illustrate CCCP by applications to Potts models, linear assignment, EM algorithms, and Generalized Iterative Scaling (GIS). CCCP can be used both as a new way to understand existing optimization algorithms and as a procedure for generating new algorithms.	Smith Kettlewell Eye Res Inst, San Francisco, CA 94115 USA	The Smith-Kettlewell Eye Research Institute	Yuille, AL (corresponding author), Smith Kettlewell Eye Res Inst, 2318 Fillmore St, San Francisco, CA 94115 USA.							DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379; Durbin R, 1989, NEURAL COMPUT, V1, P348, DOI 10.1162/neco.1989.1.3.348; ELFADEL IM, 1995, NEURAL COMPUT, V7, P1079, DOI 10.1162/neco.1995.7.5.1079; HATHAWAY RJ, 1986, STAT PROBABIL LETT, V4, P53, DOI 10.1016/0167-7152(86)90016-7; KOSOWSKY JJ, 1994, NEURAL NETWORKS, V7, P477, DOI 10.1016/0893-6080(94)90081-7; MJOLSNESS E, 1990, NEURAL NETWORKS, V3, P651, DOI 10.1016/0893-6080(90)90055-P; Rangarajan A, 1996, NEURAL COMPUT, V8, P1041, DOI 10.1162/neco.1996.8.5.1041; RANGARAJAN A, 1996, P NIPS 96 DENV COL; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591; WAUGH FR, 1993, PHYS REV E, V47, P4524, DOI 10.1103/PhysRevE.47.4524; YUILLE A, 2002, IN PRESS NEURAL COMP; YUILLE AL, 1994, NEURAL COMPUT, V6, P341, DOI 10.1162/neco.1994.6.3.341; Yuille AL, 1990, NEURAL COMPUT, V2, P1, DOI 10.1162/neco.1990.2.1.1	13	130	130	3	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1033	1040						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100129
C	Likhachev, M; Gordon, G; Thrun, S		Thrun, S; Saul, K; Scholkopf, B		Likhachev, M; Gordon, G; Thrun, S			ARA*: Anytime A* with provable bounds on sub-optimality	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				SEARCH	In real world planning problems, time for deliberation is often limited. Anytime planners are well suited for these problems: they find a feasible solution quickly and then continually work on improving it until time runs out. In this paper we propose an anytime heuristic search, ARA*, which tunes its performance bound based on available search time. It starts by finding a suboptimal solution quickly using a loose bound, then tightens the bound progressively as time allows. Given enough time it finds a provably optimal solution. While improving its bound, ARA* reuses previous search efforts and, as a result, is significantly more efficient than other anytime search methods. In addition to our theoretical analysis, we demonstrate the practical utility of ARA* with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Likhachev, M (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.							[Anonymous], 2003, CMUCS03148; Bonet B, 2001, ARTIF INTELL, V129, P5, DOI 10.1016/S0004-3702(01)00108-4; DEAN TL, 1988, P NAT C ART INT AAAI; HAEHNEL D, 2003, COMMUNICATION; KOENIG S, 2002, ADV NEURAL INFORMATI, V14; KORF RE, 1993, ARTIF INTELL, V62, P41, DOI 10.1016/0004-3702(93)90045-D; Pearl J., 1984, INTELLIGENT SEARCH S; Zhou R., 2002, P NAT C ART INT AAAI; Zilberstein Shlomo, 1995, IMPRECISE APPROXIMAT, P43	9	129	133	2	16	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						767	774						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500096
C	Ji, P; Zhang, T; Li, HD; Salzmann, M; Reid, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ji, Pan; Zhang, Tong; Li, Hongdong; Salzmann, Mathieu; Reid, Ian			Deep Subspace Clustering Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SEGMENTATION	We present a novel deep neural network architecture for unsupervised subspace clustering. This architecture is built upon deep auto-encoders, which non-linearly map the input data into a latent space. Our key idea is to introduce a novel self-expressive layer between the encoder and the decoder to mimic the "self-expressiveness" property that has proven effective in traditional subspace clustering. Being differentiable, our new self-expressive layer provides a simple but effective way to learn pairwise affinities between all data points through a standard back-propagation procedure. Being nonlinear, our neural-network based method is able to cluster data points having complex (often nonlinear) structures. We further propose pre-training and fine-tuning strategies that let us effectively learn the parameters of our subspace clustering networks. Our experiments show that our method significantly outperforms the state-of-the-art unsupervised subspace clustering techniques.	[Ji, Pan; Reid, Ian] Univ Adelaide, Adelaide, SA, Australia; [Zhang, Tong; Li, Hongdong] Australian Natl Univ, Canberra, ACT, Australia; [Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland	University of Adelaide; Australian National University; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Ji, P (corresponding author), Univ Adelaide, Adelaide, SA, Australia.		Ji, Pan/AAE-6930-2020	Reid, Ian/0000-0001-7790-6423; Salzmann, Mathieu/0000-0002-8347-8637	Australian Research Council (ARC) through the Centre of Excellence in Robotic Vision [CE140100016]; Australian Research Council (ARC) through Laureate Fellowship [FL130100102]; ARC [DP150104645]	Australian Research Council (ARC) through the Centre of Excellence in Robotic Vision(Australian Research Council); Australian Research Council (ARC) through Laureate Fellowship(Australian Research Council); ARC(Australian Research Council)	This research was supported by the Australian Research Council (ARC) through the Centre of Excellence in Robotic Vision, CE140100016, and through Laureate Fellowship FL130100102 to IDR. TZ was supported by the ARC's Discovery Projects funding scheme (project DP150104645).	Abadi M, 2015, P 12 USENIX S OPERAT; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Cai D., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383054; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9; Ciptadi A, 2009, IEEE I CONF COMP VIS, P1765, DOI 10.1109/ICCV.2009.5459394; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Favaro P, 2011, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2011.5995365; Feng JS, 2014, PROC CVPR IEEE, P3818, DOI 10.1109/CVPR.2014.482; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Ho J, 2003, PROC CVPR IEEE, P11, DOI 10.1109/cvpr.2003.1211332; Ji P, 2015, IEEE I CONF COMP VIS, P4687, DOI 10.1109/ICCV.2015.532; Ji P, 2014, IEEE WINT CONF APPL, P461, DOI 10.1109/WACV.2014.6836065; Kanatani K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P586, DOI 10.1109/ICCV.2001.937679; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Li CG, 2015, PROC CVPR IEEE, P277, DOI 10.1109/CVPR.2015.7298624; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26; Ma Y, 2007, IEEE T PATTERN ANAL, V29, P1546, DOI 10.1109/TP'AMI.2007.1085; Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7; Mo QY, 2012, LECT NOTES COMPUT SC, V7578, P402, DOI 10.1007/978-3-642-33786-4_30; Nene, 1996, CUCS00596 COL U DEP; Ng AY, 2002, ADV NEUR IN, V14, P849; Ochs Peter, 2012, CVPR; Patel VM, 2014, IEEE IMAGE PROC, P2849, DOI 10.1109/ICIP.2014.7025576; Patel VM, 2013, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2013.35; Peng X., 2016, IJCAI, P1925; Purkait P, 2014, LECT NOTES COMPUT SC, V8692, P672, DOI 10.1007/978-3-319-10593-2_44; Rao SR, 2008, PROC CVPR IEEE, P743; Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z; Vidal R, 2014, PATTERN RECOGN LETT, V43, P47, DOI 10.1016/j.patrec.2013.08.006; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Xiao SJ, 2016, IEEE T NEUR NET LEAR, V27, P2268, DOI 10.1109/TNNLS.2015.2472284; Xie JY, 2016, PR MACH LEARN RES, V48; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94; Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005; Yin M, 2016, PROC CVPR IEEE, P5157, DOI 10.1109/CVPR.2016.557; You C, 2016, PROC CVPR IEEE, P3918, DOI 10.1109/CVPR.2016.425; You C, 2016, PROC CVPR IEEE, P3928, DOI 10.1109/CVPR.2016.426	52	128	133	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400003
C	Fraccaro, M; Sonderby, SK; Paquet, U; Winther, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Fraccaro, Marco; Sonderby, Soren Kaae; Paquet, Ulrich; Winther, Ole			Sequential Neural Models with Stochastic Layers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.	[Fraccaro, Marco; Winther, Ole] Tech Univ Denmark, Lyngby, Denmark; [Sonderby, Soren Kaae; Winther, Ole] Univ Copenhagen, Copenhagen, Denmark; [Paquet, Ulrich] Google DeepMind, London, England	Technical University of Denmark; University of Copenhagen; Google Incorporated	Fraccaro, M (corresponding author), Tech Univ Denmark, Lyngby, Denmark.			Winther, Ole/0000-0002-1966-3205	Microsoft Research	Microsoft Research(Microsoft)	We thank Casper Kaae Sonderby and Lars Maaloe for many fruitful discussions, and NVIDIA Corporation for the donation of TITAN X and Tesla K40 GPUs. Marco Fraccaro is supported by Microsoft Research through its PhD Scholarship Programme.	Archer E., 2015, ARXIV PREPRINT ARXIV; Bastien F., 2012, DEEP LEARN UNS FEAT; Bayer J, 2014, ARXIV14117610; Bengio Y., 2014, ARXIV14061078; Boulanger-Lewandowski N, 2012, P 29 INT C MACH LEAR, P1159; Chung J., 2014, ARXIV14123555; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dieleman S., 2015, LASAGNE 1 RELEASE, V1; Doucet A, 2001, STAT ENG IN, P3; Gan Z., 2015, NIPS, P2458; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; King S., 2013, 9 ANN BLIZZARD CHALL; Kingma D.P, P 3 INT C LEARNING R; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Paisley J., 2012, ARXIV12066430; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Sontag D., 2015, ARXIV151105121; van Amersfoort J. R., 2014, ARXIV14126581	24	127	127	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700004
C	Bartlett, PL		Mozer, MC; Jordan, MI; Petsche, T		Bartlett, PL			For valid generalization, the size of the weights is more important than the size of the network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper shows that if a large neural network is used for a pattern classification problem, and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the sire of the weights rather than the number of weights. More specifically, consider an l-layer feed-forward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A. The misclassification probability converges to an error estimate (that is closely related to squared error on the training set) at rate O((cA)(l(l+1)/2) root(log n)/m) ignoring log factors, where m is the number of training patterns, n is the input dimension, and c is a constant. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training.			Bartlett, PL (corresponding author), AUSTRALIAN NATL UNIV,RES SCH INFORMAT SCI & ENGN,DEPT SYST ENGN,CANBERRA,ACT 0200,AUSTRALIA.			Bartlett, Peter/0000-0002-8760-3140					0	127	130	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						134	140						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00019
C	Monti, F; Bronstein, MM; Bresson, X		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Monti, Federico; Bronstein, Michael M.; Bresson, Xavier			Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationary structures on user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score matrix. Our neural network system is computationally attractive as it requires a constant number of parameters independent of the matrix size. We apply our method on several standard datasets, showing that it outperforms state-of-the-art matrix completion techniques.	[Monti, Federico; Bronstein, Michael M.] Univ Svizzera Italiana, Lugano, Switzerland; [Bresson, Xavier] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore	Universita della Svizzera Italiana; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Monti, F (corresponding author), Univ Svizzera Italiana, Lugano, Switzerland.	federico.monti@usi.ch; michael.bronstein@usi.ch; xbresson@ntu.edu.sg	Jeong, Yongwook/N-7413-2016		ERC [724228, 307047]; Google Faculty Research Award; Nvidia equipment grant; Harvard Institute for Advanced Study; TU Munich Institute for Advanced Study - German Excellence Initiative; European Union Seventh Framework Programme [291763]; NRF Fellowship [NRFF2017-10]	ERC(European Research Council (ERC)European Commission); Google Faculty Research Award(Google Incorporated); Nvidia equipment grant; Harvard Institute for Advanced Study; TU Munich Institute for Advanced Study - German Excellence Initiative; European Union Seventh Framework Programme(European Commission); NRF Fellowship	FM and MB are supported in part by ERC Starting Grant No. 307047 (COMET), ERC Consolidator Grant No. 724228 (LEMAN), Google Faculty Research Award, Nvidia equipment grant, Radcliffe fellowship from Harvard Institute for Advanced Study, and TU Munich Institute for Advanced Study, funded by the German Excellence Initiative and the European Union Seventh Framework Programme under grant agreement No. 291763. XB is supported in part by NRF Fellowship NRFF2017-10.	Benzi K., 2016, P ICASSP; Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844; Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Breese J., 1998, P UNC ART INT; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J, 2013, PROC INT C LEARN REP; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dror G., 2012, KDD CUP; Duvenaud David K, 2015, P NIPS; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; Henaff M, 2015, ARXIV150605163; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jain P., 2013, ABS13060626 CORR; Jamali M., 2010, P REC SYST; Kalofolias V., 2014, MATRIX COMPLETION GR; Kingma D.P, P 3 INT C LEARNING R; Kipf T. N., 2017, P INT C LEARN REPR; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Ktena S. I., 2017, P MICCAI; Kuang D., 2016, CORR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Yujia, 2016, P INT C LEARN REPR I, P2; Ma H., 2011, P WEB SEARCH DAT MIN; Masci J., 2015, P IEEE INT C COMP VI, P37; Miller B. N., 2003, P INT US INT; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Parisot  S., 2017, P MICCAI; Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325; Rao N., 2015, P NIPS; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Sedhain S, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P111, DOI 10.1145/2740908.2742726; Seo Y., 2016, STRUCTURED SEQUENCE; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Srebro N., 2004, P NIPS; Suhara Y., 2016, P INT C COMP SOC SCI; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Vestner M., 2017, P CVPR; Xu M., 2013, P NIPS; Yanez F, 2017, INT CONF ACOUST SPEE, P2257, DOI 10.1109/ICASSP.2017.7952558; Zheng Y., 2016, P ICML	44	126	137	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403074
C	Schmidt, L; Santurkar, S; Tsipras, D; Talwar, K; Madry, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Schmidt, Ludwig; Santurkar, Shibani; Tsipras, Dimitris; Talwar, Kunal; Madry, Aleksander			Adversarially Robust Generalization Requires More Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high "standard" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of "standard" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.	[Schmidt, Ludwig] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Santurkar, Shibani; Tsipras, Dimitris; Madry, Aleksander] MIT, Cambridge, MA 02139 USA; [Talwar, Kunal] Google Brain, Mountain View, CA USA	University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT); Google Incorporated	Schmidt, L (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	ludwig@berkeley.edu; shibani@mit.edu; tsipras@mit.edu; kunal@google.com; madry@mit.edu	Tsipras, Dimitris/AAZ-2505-2021		Google PhD fellowship; Microsoft Research fellowship at the Simons Institute for the Theory of Computing; National Science Foundation (NSF) [IIS-1447786, IIS-1607189, CCF-1563880]; Intel Corporation; NSF [CCF-1553428, CNS-1815221, CNS-1413920]; Alfred P. Sloan Research Fellowship; Google Research Award	Google PhD fellowship(Google Incorporated); Microsoft Research fellowship at the Simons Institute for the Theory of Computing(Microsoft); National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Intel Corporation(Intel Corporation); NSF(National Science Foundation (NSF)); Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); Google Research Award(Google Incorporated)	During this research project, Ludwig Schmidt was supported by a Google PhD fellowship and a Microsoft Research fellowship at the Simons Institute for the Theory of Computing. Ludwig was also an intern in the Google Brain team. Shibani Santurkar is supported by the National Science Foundation (NSF) under grants IIS-1447786, IIS-1607189, and CCF-1563880, and the Intel Corporation. Dimitris Tsipras was supported in part by the NSF grant CCF-1553428 and the NSF Frontier grant CNS-1413920. Aleksander Madry was supported in part by an Alfred P. Sloan Research Fellowship, a Google Research Award, and the NSF grants CCF-1553428 and CNS-1815221.	[Anonymous], 2016, ARXIV160704311; [Anonymous], 2017, TENSOR FLOW MODELS R; Arnab A., 2018, CVPR; Athalye A, 2018, PR MACH LEARN RES, V80; Behzadan Vahid, 2017, Machine Learning and Data Mining in Pattern Recognition. 13th International Conference, MLDM 2017. Proceedings: LNAI 10358, P262, DOI 10.1007/978-3-319-62416-7_19; Bellet A, 2015, NEUROCOMPUTING, V151, P259, DOI 10.1016/j.neucom.2014.09.044; BenTal A, 2009, PRINC SER APPL MATH, P1; Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bshouty Nader H., 1999, ALGORITHMIC LEARNING, DOI [10.1007/3-540-46769-6_17, DOI 10.1007/3-540-46769-6_17]; Carlini N., 2016, ARXIV160804644; Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009; Carlini N, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P513; Cisse M, 2017, ADV NEUR IN, V30; Cubuk E.D., 2017, ARXIV171102846; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; EricWong Zico, 2018, INT C MACH LEARN, P5286; Fawzi A., 2018, ADV NEURAL INFORM PR; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Gilmer J., 2018, INT C LEARN REPR WOR; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Grosse K, 2016, ARXIV PREPRINT ARXIV; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He Warren, 2017, P WOOT; Huang Alex, 2018, SEC PRIV WORKSH SPW; Huang S., 2017, 5 INT C LEARN REPR I; Huber P., 1981, ROBUST STAT; Jia Robin, 2017, P 2017 C EMP METH NA, P2021, DOI DOI 10.18653/V1/D17-1215; KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052; Kearns Michael J., 1994, MACH LEARN, DOI [10.1023/A:1022615600103, DOI 10.1023/A:1022615600103]; Kos J, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P36, DOI 10.1109/SPW.2018.00014; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Madry A., 2018, P ICLR VANC BC CAN; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Narodytska N., 2017, C COMP VIS PATT REC; Netzer Y., 2011, NIPS DLW; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Papernot Nicolas, 2018, IEEE EUR S SEC PRIV; Papernot Nicolas, 2017, ABS170403453 CORR; Raghunathan A., 2018, INT C LEARN REPR; Rigollet P, 2017, LECT NOTES; Schmidt L., 2017, ROTATION TRANSLATION; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Sinha A., 2018, ICLR; Song Linfeng, 2017, CORR; Su J., 2017, ABS171008864 CORR; Szegedy Christian, 2014, P 2 INT C LEARNING R; Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131; Wald Abraham, 1945, ANN MATH; Wang Yizhen, 2018, INT C MACH LEARN ICM; Xiao Chaowei, 2018, ARXIV180102612; Xu H, 2012, MACH LEARN; Xu Huan, 2009, J MACHINE LEARNING R; Xu W., 2017, P 25 ANN NETW DISTR, DOI DOI 10.14722/NDSS.2018.23198; Xu X., 2017, ARXIV PREPRINT ARXIV; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang G., 2017, C COMP COMM SEC CCS	62	125	126	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305006
C	Levine, S; Abbeel, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Levine, Sergey; Abbeel, Pieter			Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.	[Levine, Sergey; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94709 USA	University of California System; University of California Berkeley	Levine, S (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94709 USA.	svlevine@eecs.berkeley.edu; pabbeel@eecs.berkeley.edu		Levine, Sergey/0000-0001-6764-2743	DARPA Young Faculty Award [D13AP0046]	DARPA Young Faculty Award	This research was partly funded by a DARPA Young Faculty Award #D13AP0046.	Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Coates A., 2008, INT C MACH LEARN ICM; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Engel Y., 2005, ADV NEURAL INFORM PR; Fletcher Roger, 1987, PRACTICAL METHODS OP, DOI 10.1002/9781118723203; Ijspeert AJ, 2003, ADV NEURAL INFORM PR, V2003, P1547; Khansari-Zadeh S. M., 2010, INT C ROB AUT ICRA; Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721; Kober J, 2009, IEEE INT CONF ROBOT, P2509; Levine S., 2014, INT C MACH LEARN ICM; Levine S., 2013, ADV NEURAL INFORM PR; Levine Sergey, 2013, ICML; Lioutikov R., 2014, INT C ROB AUT; Peters J., 2007, EUR S ART NEUR NETW; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Rawlik Konrad, 2012, ROBOTICS SCI SYSTEMS; Rubinstein R., 2004, CROSS ENTROPY METHOD; Schneider J, 2003, P INT JOINT C ART IN; Song J, 2004, I C CONT AUTOMAT ROB, P2223; Stulp F., 2012, INT C MACH LEARN ICM; Tassa Y, 2012, IEEE INT C INT ROBOT, P4906, DOI 10.1109/IROS.2012.6386025; Theodorou E, 2010, IEEE INT CONF ROBOT, P2397, DOI 10.1109/ROBOT.2010.5509336; Toussaint M., 2009, P 26 ANN INT C MACHI, P1049, DOI [10.1145%2F1553374.1553508, DOI 10.1145/1553374.1553508, 10.1145/1553374.1553508]	25	125	132	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102005
C	Dasgupta, S; Littman, ML; McAllester, D		Dietterich, TG; Becker, S; Ghahramani, Z		Dasgupta, S; Littman, ML; McAllester, D			PAC generalization bounds for co-training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The rule-based bootstrapping introduced by Yarowsky, and its co-training variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justifies both the use of confidences - partial rules and partial labeling of the unlabeled data - and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of k labels for k greater than or equal to 2.										Blum A, 1998, COLT; Collins Michael, 1999, EMNLP; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Nigam K., 2000, CIKM; Yarowsky David, 1995, ACL	6	125	134	1	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						375	382						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100047
C	Wan, EA; van der Merwe, R; Nelson, AT		Solla, SA; Leen, TK; Muller, KR		Wan, EA; van der Merwe, R; Nelson, AT			Dual estimation and the unscented transformation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Dual estimation refers to the problem of simultaneously estimating the state of a dynamic system and the model which gives rise to the dynamics. Algorithms include expectation-maximization (EM), dual Kalman filtering, and joint Kalman methods. These methods have recently been explored in the context of nonlinear modeling, where a neural network is used as the functional form of the unknown model. Typically, an extended Kalman filter (EKF) or smoother is used for the part of the algorithm that estimates the clean state given the current estimated model. An EKF may also be used to estimate the: weights of the network. This paper points out the flaws in using the EKE and proposes an improvement based on a new approach called the unscented transformation (UT) [3]. A substantial performance gain is achieved with the same order of computational complexity as that of the standard EKE The approach is illustrated on several dual estimation methods.	Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, Beaverton, OR 97006 USA		Wan, EA (corresponding author), Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, 20000 NW Walker Rd, Beaverton, OR 97006 USA.							DEFREITAS JFG, 1998, TR328 CAMBR U ENG DE; GHAHRAMANI Z, 1999, ADV NEURAL INFORMATI, V11; Julier S.J., 1997, P 11 INT S AER DEF S; KOPP RE, 1963, AIAA J, V1, P2300, DOI 10.2514/3.2056; MATTHEWS MB, 1990, INTERNATIONAL NEURAL NETWORK CONFERENCE, VOLS 1 AND 2, P115; Nelson A.T., 1999, THESIS OREGON GRADUA; Singhal S., 1989, ADV NEURAL INFORM PR, P133; WAN E, 1997, ADV NEURAL INFORMATI, V9	8	125	131	0	8	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						666	672						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700095
C	Platt, JC		Kearns, MS; Solla, SA; Cohn, DA		Platt, JC			Using analytic QP and sparseness to speed training of support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) problem. This paper proposes an algorithm for training SVMs: Sequential Minimal Optimization, or SMO. SMO breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable. Thus, SMO does not require a numerical QP library. SMO's computation time is dominated by evaluation of the kernel, hence kernel optimizations substantially quicken SMO. For the MNIST database, SMO is 1.7 times as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be 1500 times faster than the PCG chunking algorithm.	Microsoft Corp, Res, Redmond, WA 98052 USA	Microsoft	Platt, JC (corresponding author), Microsoft Corp, Res, 1 Microsoft Way, Redmond, WA 98052 USA.	jplatt@microsoft.com	Platt, John/GOH-2678-2022	Platt, John/0000-0002-5652-5303				BURGES CJC, 1998, DATA MINING KNOWLEDG, V2; Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169; Kaufman L, 1999, ADVANCES IN KERNEL METHODS, P147; LeCun Yann, MNIST HANDWRITTEN DI; Merz C, 1998, UCI REPOSITORY MACHI; Osuna E., 1997, P IEEE NEUR NETW SIG; Platt J, 1998, MSRTR9814; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Vapnik V., 1982, ESTIMATION DEPENDENC	9	125	167	2	20	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						557	563						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700079
C	Rangapuram, SS; Seeger, M; Gasthaus, J; Stella, L; Wang, YY; Januschowski, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rangapuram, Syama Sundar; Seeger, Matthias; Gasthaus, Jan; Stella, Lorenzo; Wang, Yuyang; Januschowski, Tim			Deep State Space Models for Time Series Forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from large collection of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.	[Rangapuram, Syama Sundar; Seeger, Matthias; Gasthaus, Jan; Stella, Lorenzo; Wang, Yuyang; Januschowski, Tim] Amazon Res, Seattle, WA USA		Rangapuram, SS (corresponding author), Amazon Res, Seattle, WA USA.	rangapur@amazon.com; matthis@amazon.com; gasthaus@amazon.com; stellalo@amazon.com; yuyawang@amazon.com; tjnsch@amazon.com						[Anonymous], ICML TIM SER WORKSH; [Anonymous], 2014, ICLR; Athanasopoulos G, 2011, INT J FORECASTING, V27, P822, DOI 10.1016/j.ijforecast.2010.04.009; Barber D., 2012, BAYESIAN REASONING M; BOX GEP, 1964, J ROY STAT SOC B, V26, P211, DOI 10.1111/j.2517-6161.1964.tb00553.x; BOX GEP, 1968, ROY STAT SOC C-APP, V17, P91; Chapados N, 2014, PR MACH LEARN RES, V32, P1395; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Durbin J, 2012, TIME SERIES ANAL STA, V38; Flunkert Valentin, 2017, ABS170404110 CORR; FRACCARO M, 2017, ADV NEURAL INFORM PR, P3604; Graves Alex, 2013, ARXIV13080850 CORR; Hyndman RJ, 2008, SPRINGER SER STAT, P3; Karl Maximilian, 2017, ICLR; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Makridakis S, 2018, INT J FORECASTING, V34, P802, DOI 10.1016/j.ijforecast.2018.06.001; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Seeger Matthias, 2017, ABS170404110 CORR; Seeger MW., 2016, ADV NEURAL INFORM PR, P4646; Sontag D., 2015, ARXIV151105121; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; van den Oord A., 2016, GENERATIVE MODEL RAW; Wen R., 2017, MULTIHORIZON QUANTIL; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847; Zaheer M, 2017, PR MACH LEARN RES, V70; Zheng X, 2017, ARXIV171111179CSLG; NON TRADITIONAL REF	29	124	126	14	34	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002034
C	Stich, SU; Cordonnier, JB; Jaggi, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Stich, Sebastian U.; Cordonnier, Jean-Baptiste; Jaggi, Martin			Sparsified SGD with Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far. In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory). That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications.	[Stich, Sebastian U.; Cordonnier, Jean-Baptiste; Jaggi, Martin] Ecole Polytech Fed Lausanne, Machine Learning & Optimizat Lab MLO, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Stich, SU (corresponding author), Ecole Polytech Fed Lausanne, Machine Learning & Optimizat Lab MLO, Lausanne, Switzerland.		Nidadavolu, Divya/HGU-5657-2022	Jaggi, Martin/0000-0003-1579-5558	SNSF [200021_175796]; Microsoft Research JRC project 'Coltrain'; Google Focused Research Award	SNSF(Swiss National Science Foundation (SNSF)); Microsoft Research JRC project 'Coltrain'(Microsoft); Google Focused Research Award(Google Incorporated)	We would like to thank Dan Alistarh for insightful discussions in the early stages of this project and Frederik Kunstner for his useful comments on the various drafts of this manuscript. We acknowledge funding from SNSF grant 200021_175796, Microsoft Research JRC project 'Coltrain', as well as a Google Focused Research Award.	Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D, 2018, PODC'18: PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P169, DOI 10.1145/3212734.3212763; Alistarh D, 2017, ADV NEUR IN, V30; Alistarh Dan, 2018, NEURIPS 2018; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bottou Leon, 2012, STOCHASTIC GRADIENT, V7700, P430; Chen CC, 2018, AAAI CONF ARTIF INTE, P257; Cordonnier Jean-Baptiste, 2018, THESIS; De Sa Christopher, 2015, Adv Neural Inf Process Syst, V28, P2656; Dryden N, 2016, PROCEEDINGS OF 2016 2ND WORKSHOP ON MACHINE LEARNING IN HPC ENVIRONMENTS (MLHPC), P1, DOI [10.1109/MLHPC.2016.4, 10.1109/MLHPC.2016.004]; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Dunner C., 2017, ADV NEURAL INFORM PR, P4258; Goyal Priya, 2017, ARXIV170602677; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2370; Jones E., 2001, SCIPY OPEN SOURCE SC; Kingma D.P, P 3 INT C LEARNING R; Lacoste-Julien Simon, 2012, CORR; Leblond R, 2017, PR MACH LEARN RES, V54, P46; Leblond Remi, 2018, CORR; Lin Yujun, 2018, ICLR 2018; Mania H, 2017, SIAM J OPTIMIZ, V27, P2202, DOI 10.1137/16M1057000; Moulines Eric, 2011, P NIPS, V24, P451; Na T, 2017, IEEE IJCNN, P3716, DOI 10.1109/IJCNN.2017.7966324; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; RAKHLIN A., 2012, P INT C MACH LEARN, P1571; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Ruppert D., 1988, TECHNICAL REPORT; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Seide F, 2014, INTERSPEECH, P1058; Shamir O., 2013, INT C MACH LEARN, P71; Sonnenburg S., 2008, 25 INT C MACH LEARN, V10, P1937; Stich Sebastian U., 2018, CORR; Strom N, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1488; Sun X, 2017, PR MACH LEARN RES, V70; Tang Hanlin, 2018, NEURIPS 2018; Wangni Jianqiao, 2018, NEURIPS 2018; Wen W., 2017, ADV NEURAL INFORM PR, P1, DOI DOI 10.1109/ICC.2017.7997306; Wu JX, 2018, PR MACH LEARN RES, V80; You Yang, 2017, ARXIV170803888; Zhang HT, 2017, PR MACH LEARN RES, V70; Zhang YC, 2012, IEEE DECIS CONTR P, P6792, DOI 10.1109/CDC.2012.6426691; Zhaoguang Pan, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7285868	46	124	128	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304046
C	Li, YZ; Yuan, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Yuanzhi; Yuan, Yang			Convergence Analysis of Two-layer Neural Networks with ReLU Activation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called "identity mapping". We prove that, if input follows from Gaussian distribution, with standard O(1/root d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the "identity mapping" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in "two phases": In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.	[Li, Yuanzhi] Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA; [Yuan, Yang] Cornell Univ, Comp Sci Dept, Ithaca, NY 14853 USA	Princeton University; Cornell University	Li, YZ (corresponding author), Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA.	yuanzhil@cs.princeton.edu; yangyuan@cs.cornell.edu	Li, Yuan/GXV-1310-2022; Jeong, Yongwook/N-7413-2016					Andoni A, 2014, PR MACH LEARN RES, V32, P1908; [Anonymous], 2014, CORR; [Anonymous], 2017, AISTATS; Arora S, 2014, PR MACH LEARN RES, V32; BREIMAN L, 1993, IEEE T INFORM THEORY, V39, P999, DOI 10.1109/18.256506; Choromanska A., 2015, COLT, P1756; Choromanska A., 2015, AISTATS; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Glorot X., 2011, P 14 INT C ART INT S, P315; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Goel Surbhi, 2017, ARXIV E PRINTS; Goel Surbhi, 2016, CORR; Goel Surbhi, 2017, NIPS 2017; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hardt M., 2016, CORR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Janzamin M., 2015, ARXIV150608473; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kingma D.P, P 3 INT C LEARNING R; Klusowski J. M., 2016, ARXIV E PRINTS; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Livni R., 2014, NIPS, V1, P855; Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924; Nair V., 2010, ICML, P807; Pan X., 2016, INT C MACHINE LEARNI, P2427; Pascanu Razvan, 2013, CORR; Rudelson M., 2010, ARXIV E PRINTS; Saad D, 1996, ADV NEUR IN, V8, P302; Safran I, 2016, PR MACH LEARN RES, V48; Saxe Andrew M., 2013, CORR; Sedghi Hanie, 2015, ICLR; Shamir Ohad, 2016, CORR; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tian Yuandong, 2016, ICLR 2017 UNPUB; Zhang Yuchen, 2015, CORR; Zhong Kai, 2017, ICML 2017	44	124	125	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400057
C	Malach, E; Shalev-Shwartz, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Malach, Eran; Shalev-Shwartz, Shai			Decoupling "when to update" from "how to update"	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NOISE	Deep learning requires data. A useful approach to obtain data is to be creative and mine data from various sources, that were created for different purposes. Unfortunately, this approach often leads to noisy labels. In this paper, we propose a meta algorithm for tackling the noisy labels problem. The key idea is to decouple "when to update" from "how to update". We demonstrate the effectiveness of our algorithm by mining data for gender classification by combining the Labeled Faces in the Wild (LFW) face recognition dataset with a textual genderizing service, which leads to a noisy dataset. While our approach is very simple to implement, it leads to state-of-the-art results. We analyze some convergence properties of the proposed algorithm.	[Malach, Eran; Shalev-Shwartz, Shai] Hebrew Univ Jerusalem, Sch Comp Sci, Jerusalem, Israel	Hebrew University of Jerusalem	Malach, E (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci, Jerusalem, Israel.	eran.malach@mail.huji.ac.il; shais@cs.huji.ac.il			European Research Council (TheoryDL project)	European Research Council (TheoryDL project)	This research is supported by the European Research Council (TheoryDL project).	Ando R.K., 2007, P 24 INT C MACH LEAR, P25; Atlas L. E., 1990, ADV NEURAL INFORM PR, P566; Awasthi P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P449, DOI 10.1145/2591796.2591839; Barandela R, 2000, LECT NOTES COMPUT SC, V1876, P621; Bekker AJ, 2016, INT CONF ACOUST SPEE, P2682, DOI 10.1109/ICASSP.2016.7472164; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bootkrajang Jakramate, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P143, DOI 10.1007/978-3-642-33460-3_15; Bootkrajang J., 2013, ARXIV13096818; Brodersen Kay H., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3121, DOI 10.1109/ICPR.2010.764; Brodley CE, 1999, J ARTIF INTELL RES, V11, P131, DOI 10.1613/jair.606; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Flatow David, 2017, ROBUSTNESS CONVNENTS; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534; Goldberger J., 2017, ICLR; Grandvalet Y., 2006, SEMISUPERVISED LEARN, P1, DOI [10.7551/mitpress/9780262033589.001.0001, DOI 10.7551/MITPRESS/9780262033589.001.0001]; Grandvalet Yves, 2004, NIPS, P529; Huang G.B., 2008, WORKSHOP FACESREAL L; Ipeirotis Panagiotis G., 2010, P ACM SIGKDD WORKSH, DOI [10.1145/1837885.1837906, DOI 10.1145/1837885.1837906]; Kakar Pravin, 2015, MULT EXP ICME 2015 I, P1; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231; Larsen J, 1998, INT CONF ACOUST SPEE, P1205, DOI 10.1109/ICASSP.1998.675487; Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352; Masek Philip, 2015, THESIS; McDonald RA, 2003, LECT NOTES COMPUT SC, V2709, P35; Menon A., 2016, ARXIV160500751; Mnih V., 2012, P 29 INT C MACHINE L, P567, DOI DOI 10.5555/3042573.3042603; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Nigam K., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P86, DOI 10.1145/354756.354805; Patrini G., 2016, ARXIV160903683; Patrini G., 2016, ARXIV160202450; Rabinovich S. E, 2014, P 3 INT C LEARN REPR; Settles Burr, 2010, ACTIVE LEARNING LIT, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X; Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417; Sukhbaatar Sainbayar, 2014, ICLR; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Zhu Xiaojin, 2005, 1530 TR; Zhuang B., 2016, ARXIV161109960	39	124	124	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401001
C	Margaritis, D; Thrun, S		Solla, SA; Leen, TK; Muller, KR		Margaritis, D; Thrun, S			Bayesian network induction via local neighborhoods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In recent years, Bayesian networks have become highly successful tool for diagnosis, analysis, and decision making in real-world domains. We present an efficient algorithm for learning Bayes networks from data. Our approach constructs Bayesian networks by first identifying each node's Markov blankets, then connecting nodes in a maximally consistent way. In contrast to the majority of work, which typically uses hill-climbing approaches that may produce dense and causally incorrect nets, our approach yields much more compact causal networks by heeding independencies in the data. Compact causal networks facilitate fast inference and are also easier to understand. We prove that under mild assumptions, our approach requires time polynomial in the size of the data and the number of nodes. A randomized variant, also presented here, yields comparable results at much higher speeds.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Margaritis, D (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.							AGOSTA JM, UAI 88; CHENG J, 1997, AI STAT; CHOW CK, 1968, IEEE T INFORMATION T, V14; HERSKOVITZ EH, UAI 90; JUNGER M, 1985, POLYHEDRAL COMBINATO; MARGARITIS D, CMUCS99134 TR; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; REBANE G, UAI 87; Spirtes P., 2000, CAUSATION PREDICTION; VERMA TS, UAI 90	10	124	125	0	10	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						505	511						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700072
C	Hou, RB; Chang, H; Ma, BP; Shan, SG; Chen, XL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hou, Ruibing; Chang, Hong; Ma, Bingpeng; Shan, Shiguang; Chen, Xilin			Cross Attention Network for Few-shot Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Few-shot classification aims to recognize unlabeled samples from unseen classes given only few labeled samples. The unseen classes and low-data problem make few-shot classification very challenging. Many existing approaches extracted features from labeled and unlabeled samples independently, as a result, the features are not discriminative enough. In this work, we propose a novel Cross Attention Network to address the challenging problems in few-shot classification. Firstly, Cross Attention Module is introduced to deal with the problem of unseen classes. The module generates cross attention maps for each pair of class feature and query sample feature so as to highlight the target object regions, making the extracted feature more discriminative. Secondly, a transductive inference algorithm is proposed to alleviate the low-data problem, which iteratively utilizes the unlabeled query set to augment the support set, thereby making the class features more representative. Extensive experiments on two benchmarks show our method is a simple, effective and computationally efficient framework and outperforms the state-of-the-arts.	[Hou, Ruibing; Chang, Hong; Shan, Shiguang; Chen, Xilin] Chinese Acad Sci, Chinese Acad Sci CAS, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing, Peoples R China; [Hou, Ruibing; Chang, Hong; Ma, Bingpeng; Shan, Shiguang; Chen, Xilin] Univ Chinese Acad Sci, Beijing, Peoples R China; [Shan, Shiguang] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences	Hou, RB (corresponding author), Chinese Acad Sci, Chinese Acad Sci CAS, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing, Peoples R China.; Hou, RB (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.	ruibing.hou@vipl.ict.ac.cn; changhong@ict.ac.cn; bpma@ucas.ac.cn; sgshan@ict.ac.cn; xlchen@ict.ac.cn		Shan, Shiguang/0000-0002-8348-392X	National Key R&D Program of China [2017YFA0700800]; Natural Science Foundation of China (NSFC) [61876171]; Beijing Natural Science Foundation [L182054]	National Key R&D Program of China; Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work is partially supported by National Key R&D Program of China (No.2017YFA0700800), Natural Science Foundation of China (NSFC): 61876171 and Beijing Natural Science Foundation under Grant L182054.	ANDRYCHOWICZ M, 2018, NEURIPS; [Anonymous], 2019, CVPR; [Anonymous], 2018, ICLR; Bertinetto Luca, 2016, NIPS; Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429; Chen L., 2017, P IEEE C COMP VIS PA, P5659; Dong-Hyun L., 2013, INT C MACH LEARN WOR; Finn C, 2017, PR MACH LEARN RES, V70; Gao P., 2018, ARXIV180802632; Gidaris S., 2019, CVPR; Gidaris Spyros, 2018, CVPR; Hou R., 2019, P IEEE C COMP VIS PA, P9317; Hou R., 2019, P IEEE C COMP VIS PA, P7183; Hu J., 2017, CORR; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lee K., 2019, CVPR; Li Zhenguo, 2017, METASGD LEARNING LEA; Liu Y., 2019, ARXIV190408479; Mishra N., 2018, INT C LEARN REPR; Munkhdalai  T., 2018, ICML; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Naik D. K., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P437, DOI 10.1109/IJCNN.1992.287172; Nair V., 2010, ICML; Nichol Alex, 2018, ABS180302999 ARXIV; OLIVER A., 2018, NEURIPS; Oreshkin Boris N, 2018, ADV NEURAL INFORM PR; Park Jongchan, 2018, BMVC; Paszke Adam, 2017, NIPS WORKSH; Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140; Ravi Sachin, 2017, ICLR; Ren Mengye, 2018, ARXIV18030067; Rusu Andrei A, 2019, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sun Q., 2019, CVPR; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Thrun S, 1998, LEARNING TO LEARN, P181; Thrun S, 1998, LEARNING TO LEARN, P3; Vinyals Oriol, 2016, ARXIV160604080, P3630; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K., 2015, SHOW ATTEND TELL NEU; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu D., 2017, P IEEE C COMP VIS PA, P4709; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhong Zhun, 2017, ARXIV170804896; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou DY, 2004, ADV NEUR IN, V16, P321	49	123	127	1	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304005
C	Li, YY; Pirk, S; Su, H; Qi, CR; Guibas, LJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Yangyan; Pirk, Soren; Su, Hao; Qi, Charles R.; Guibas, Leonidas J.			FPNN: Field Probing Neural Networks for 3D Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DESCRIPTORS	Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points - sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space "intelligently", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets.	[Li, Yangyan; Pirk, Soren; Su, Hao; Qi, Charles R.; Guibas, Leonidas J.] Stanford Univ, Stanford, CA 94305 USA; [Li, Yangyan] Shandong Univ, Jinan, Shandong, Peoples R China	Stanford University; Shandong University	Li, YY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.; Li, YY (corresponding author), Shandong Univ, Jinan, Shandong, Peoples R China.				NSF [DMS-1546206, IIS-1528025]; UCB MURI grant [N00014-13-1-0341]; Chinese National 973 Program [2015CB352501]; Stanford AI Lab-Toyota Center for Artificial Intelligence Research; Max Planck Center for Visual Computing and Communication; Google Focused Research award	NSF(National Science Foundation (NSF)); UCB MURI grant; Chinese National 973 Program(National Basic Research Program of China); Stanford AI Lab-Toyota Center for Artificial Intelligence Research; Max Planck Center for Visual Computing and Communication; Google Focused Research award(Google Incorporated)	We would first like to thank all the reviewers for their valuable comments and suggestions. Yangyan thanks Daniel Cohen-Or and Zhenhua Wang for their insightful proofreading. The work was supported in part by NSF grants DMS-1546206 and IIS-1528025, UCB MURI grant N00014-13-1-0341, Chinese National 973 Program (2015CB352501), the Stanford AI Lab-Toyota Center for Artificial Intelligence Research, the Max Planck Center for Visual Computing and Communication, and a Google Focused Research award.	Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844; Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693; BRONSTEIN AM, 2011, TOG, V30, P1, DOI DOI 10.1145/1899404.1899405; Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446; Glorot X., 2010, PROC MACH LEARN RES, P249; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Jia Y., 2014, P 22 ACM INT C MULT, P675; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kazhdan M., 2003, Symposium on Geometry Processing, P156; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148; Masci Jonathan, 2015, ICCV WORKSH 3D REPR; Maturana Daniel, 2015, IROS; Nair V, 2010, P 27 INT C MACHINE L, P807; Nieaner M., 2013, T GRAPHICS, V32; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Socher R., 2012, ADV NEURAL INFORM PR, V1, P656; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801	31	123	129	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703052
C	Livni, R; Shalev-Shwartz, S; Shamir, O		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Livni, Roi; Shalev-Shwartz, Shai; Shamir, Ohad			On the Computational Efficiency of Training Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.	[Livni, Roi; Shalev-Shwartz, Shai] Hebrew Univ Jerusalem, Jerusalem, Israel; [Shamir, Ohad] Weizmann Inst Sci, Rehovot, Israel	Hebrew University of Jerusalem; Weizmann Institute of Science	Livni, R (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.	roi.livni@mail.huji.ac.il; shais@cs.huji.ac.il; ohad.shamir@weizmann.ac.il			Intel (ICRI-CI); ISF grant [425/13]; Marie-Curie Career Integration Grant; MOS center of Knowledge for AI and ML [3-9243]; Google Europe Fellowship in Learning Theory; Google Fellowship	Intel (ICRI-CI); ISF grant; Marie-Curie Career Integration Grant; MOS center of Knowledge for AI and ML; Google Europe Fellowship in Learning Theory(Google Incorporated); Google Fellowship(Google Incorporated)	This research is supported by Intel (ICRI-CI). OS was also supported by an ISF grant (No. 425/13), and a Marie-Curie Career Integration Grant. SSS and RL were also supported by the MOS center of Knowledge for AI and ML (No. 3-9243). RL is a recipient of the Google Europe Fellowship in Learning Theory, and this research is supported in part by this Google Fellowship. We thank Itay Safran for spotting a mistake in a previous version of Sec. 2 and to James Martens for helpful discussions.	Andoni A., 2014, ICML; Andoni A., 2014, SODA; [Anonymous], 2012, ICML; Anthony M, 2002, NEURAL NETWORK LEARN; Arora Sanjeev, 2013, PROVABLE BOUNDS LEAR; Auer P., 1996, NIPS; Bartlett PL, 2002, THEOR COMPUT SCI, V284, P53, DOI 10.1016/S0304-3975(01)00057-3; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Blais E, 2010, MACH LEARN, V80, P273, DOI 10.1007/s10994-010-5179-6; BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3; Dahl G. E., 2013, ICASSP; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Daniely A., 2014, FOCS; Kalai A. T., 2009, FOCS; Kalai AT, 2008, SIAM J COMPUT, V37, P1777, DOI 10.1137/060649057; Klivans A. R., 2006, FOCS; Krizhevsky A., 2012, ADV NEURAL INF PROCE; PIPPENGER N, 1979, J ACM, V26, P361, DOI 10.1145/322123.322138; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz Shai, 2011, ICML; Sipser M., 2006, INTRO THEORY COMPUTA, V2nd; Sutskever I., 2013, P 30 INT C MACH LEAR; Zeiler M. D., 2014, EUR C COMP VIS, P818	24	123	124	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100046
C	Li, YJ; Yu, MC; Li, SZ; Avestimehr, S; Kim, NS; Schwing, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Youjie; Yu, Mingchao; Li, Songze; Avestimehr, Salman; Kim, Nam Sung; Schwing, Alexander			Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Distributed training of deep nets is an important technique to address some of the present day computing challenges like memory consumption and computational demands. Classical distributed approaches, synchronous or asynchronous, are based on the parameter server architecture, i.e., worker nodes compute gradients which are communicated to the parameter server while updated parameters are returned. Recently, distributed training with AllReduce operations gained popularity as well. While many of those operations seem appealing, little is reported about wall-clock training time improvements. In this paper, we carefully analyze the AllReduce based setup, propose timing models which include network latency, bandwidth, cluster size and compute time, and demonstrate that a pipelined training with a width of two combines the best of both synchronous and asynchronous training Specifically, for a setup consisting of a four-node GPU cluster we show wall-clock time training improvements of up to 5.4 x compared to conventional approaches.	[Li, Youjie; Kim, Nam Sung; Schwing, Alexander] Univ Illinois, Urbana, IL 61801 USA; [Yu, Mingchao; Li, Songze; Avestimehr, Salman] Univ Southern Calif, Los Angeles, CA USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Southern California	Li, YJ (corresponding author), Univ Illinois, Urbana, IL 61801 USA.		Li, Songze/AAU-6876-2021	Li, Songze/0000-0003-4282-3307	NSF [IIS 17-18221, CNS 17-05047, CNS 15-57244, CCF-1763673, CCF-1703575]; 3M; IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR); Defense Advanced Research Projects Agency (DARPA) [HR001117C0053]	NSF(National Science Foundation (NSF)); 3M(3M); IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR)(International Business Machines (IBM)); Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work is supported in part by grants from NSF (IIS 17-18221, CNS 17-05047, CNS 15-57244, CCF-1763673 and CCF-1703575). This work is also supported by 3M and the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR). Besides, this material is based in part upon work supported by Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0053. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Alistarh D, 2017, ADV NEUR IN, V30; Bagade S., 2016, P 2016 49 ANN IEEEAC, P1; Bengio Y., 2013, PAMI; Chen C.-Y., 2018, AAAI; Chen Liang-Chich, 2015, ABS14127062 CORR; CHILIMBI TM, 2014, P OSDI, V14, P571; Cui HG, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901323; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Dryden N., 2018, TECHNICAL REPORT; Dryden Nikoli, 2016, WORKSH MACH LEARN HP; Goyal P., 2017, LARGE MINIBATCH SGD; Harlap A., 2018, ARXIV PREPRINT ARXIV; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223; Iandola FN, 2016, PROC CVPR IEEE, P2592, DOI 10.1109/CVPR.2016.284; Intel Corporation, 2018, INT MATH KERN LIB; Isard M., 2007, Operating Systems Review, V41, P59, DOI 10.1145/1272998.1273005; Kim H., 2016, ARXIV160208191CS; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langford J., 2009, NIPS; Larsson G., 2016, FRACTALNET ULTRADEEP; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; LI Y, 2018, MICRO; Lian XR, 2018, PR MACH LEARN RES, V80; Lian XR, 2017, ADV NEUR IN, V30; Liao Renjie, 2016, ADV NEURAL INFORM PR, P2; Lin YL, 2018, INT CONF SYST SCI EN; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moritz P, 2016, INT C LEARNING REPRE; Murray DG, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P439, DOI 10.1145/2517349.2522738; Nvidia, 2015, GPU BAS DEEP LEARN I; NVIDIA Corporation, 2017, TITAN XP; NVIDIA Corporation, 2010, NVIDIA CUDA C PROGR; OpenMPI Community, 2017, OPENMPI HIGH PERF ME; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Seide F, 2014, INT CONF ACOUST SPEE; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Strom N., 2015, INTERSPEECH, V7, P10; Thakur R, 2005, INT J HIGH PERFORM C, V19, P49, DOI 10.1177/1094342005051521; Wang Q., 2016, ISCAS; Wang Q., 2017, NEUROCOMPUTING; Wen W., 2017, P NIPS, P1509; Zaharia M, 2010, HOTCLOUD, P10, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?; Zhang C, 2015, ACMSIGDA INT S FIELD, P161, DOI [DOI 10.1145/2684746.2689060, 10.1145/2684746.2689060]; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	52	122	121	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002058
C	Mei, HY; Eisner, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mei, Hongyuan; Eisner, Jason			The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Many events occur in the world. Some event types are stochastically excited or inhibited-in the sense of having their probabilities elevated or decreased-by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.	[Mei, Hongyuan; Eisner, Jason] Johns Hopkins Univ, Dept Comp Sci, 3400 N Charles St, Baltimore, MD 21218 USA	Johns Hopkins University	Mei, HY (corresponding author), Johns Hopkins Univ, Dept Comp Sci, 3400 N Charles St, Baltimore, MD 21218 USA.	hmei@cs.jhu.edu; jason@cs.jhu.edu	Jeong, Yongwook/N-7413-2016					Chelba Ciprian, 2013, ARXIV13123005 COMP R; Choi E, 2015, IEEE DATA MINING, P721, DOI 10.1109/ICDM.2015.144; Du N., 2015, ADV NEURAL INFORM PR, P3492; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Du N, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P219, DOI 10.1145/2783258.2783411; Etesami J., 2016, P 32 C UNC ART INT, P162; Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742; Graves A, 2012, STUD COMPUT INTELL, V385, P37; Guo FJ, 2015, JMLR WORKSH CONF PRO, V38, P315; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; He XR, 2015, PR MACH LEARN RES, V37, P871; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Karpathy A., 2015, ARXIV150602078; Kingma D.P, P 3 INT C LEARNING R; Lee Y, 2016, PR MACH LEARN RES, V48; LEWIS PAW, 1979, NAV RES LOG, V26, P403, DOI 10.1002/nav.3800260304; Liniger Thomas Josef, 2009, THESIS EIDGENOSSISCH; Lukasik M, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P393; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Palm C., 1943, ERICSSON TECHNICS; Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057; Rodriguez Manuel Gomez, 2013, P 6 ACM INT C WEB SE, P23, DOI DOI 10.1145/2433396.2433402; Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194; Wang Yichen, 2016, P INT C MACH LEARN I; Xiao S., 2017, OINT MODELING EVENT; Xiao S, 2017, ADV NEUR IN, V30; Xu Hongteng, 2016, P INT C MACH LEARN I; Yang Shuang-Hong, 2013, P 30 INT C MACH LEAR, V28; Zaidan Omar, 2008, P EMNLP; Zhao QY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1513, DOI 10.1145/2783258.2783401; Zhou K, 2013, ICML	31	122	123	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406079
C	Joulin, A; Mikolov, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Joulin, Armand; Mikolov, Tomas			Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NETWORKS	Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.	[Joulin, Armand; Mikolov, Tomas] Facebook AI Res, 770 Broadway, New York, NY 10003 USA	Facebook Inc	Joulin, A (corresponding author), Facebook AI Res, 770 Broadway, New York, NY 10003 USA.	ajoulin@fb.com; tmikolov@fb.com	Mikolov, Tomas/AAM-1274-2020					Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1, DOI [DOI 10.1038/NATURE14539, 10.1016/j.asoc.2014.05.028, DOI 10.1016/J.ASOC.2014.05.028]; Bishop CM, 2006, PATTERN RECOGNITION; Boden M., 2000, CONNECTION SCI; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Bridle John S, 1990, NEUROCOMPUTING, P227, DOI DOI 10.1007/978-3-642-76153-9_28; Christiansen MH, 1999, COGNITIVE SCI, V23, P157, DOI 10.1016/S0364-0213(99)00003-8; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Ciresan D., 2011, HIGH PERFORMANCE NEU; Crocker M. W., 1996, MECH SENTENCE PROCES; Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090; Das S., 1992, ACCSS; Das S., 1993, NIPS; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Fanty M., 1994, PARALLEL NATURAL LAN; Gers FA, 2001, IEEE T NEURAL NETWOR, V12, P1333, DOI 10.1109/72.963769; Graves A., 2014, ARXIV14105401; Grunwald P., 1996, ACCSS; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Holldobler S., 1997, ADV ARTIFICIAL INTEL; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mikolov T., 2014, LEARNING LONGER MEMO; Mikolov T., 2012, GOOGLE; Minsky M., 1969, PERCEPTRONS; Mozer M. C., 1993, NIPS; POLLACK JB, 1991, MACH LEARN, V7, P227, DOI 10.1007/BF00114845; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Rodriguez P., 1999, CONNECTION SCI; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Tabor W., 2000, EXPERT SYSTEMS; WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X; Weston J., 2015, ICLR; Wiles J., 1995, ACCSS; Williams R.J., 1995, BACKPROPAGATION THEO, V1, P433; Zaremba W., 2014, LEARNING TO EXECUTE; ZENG Z, 1994, IEEE T NEURAL NETWOR, V5, P320, DOI 10.1109/72.279194	37	122	124	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102006
C	Li, Q; Sun, ZA; He, R; Tan, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Qi; Sun, Zhenan; He, Ran; Tan, Tieniu			Deep Supervised Discrete Hashing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.	[Li, Qi; Sun, Zhenan; He, Ran; Tan, Tieniu] Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Ctr Res Intelligent Percept & Comp,Natl Lab Patte, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS	Li, Q (corresponding author), Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Ctr Res Intelligent Percept & Comp,Natl Lab Patte, Beijing, Peoples R China.	qli@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn; rhe@nlpr.ia.ac.cn; tnt@nlpr.ia.ac.cn			National Key Research and Development Program of China [2016YFB1001000]; Natural Science Foundation of China [61622310]	National Key Research and Development Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was partially supported by the National Key Research and Development Program of China (Grant No. 2016YFB1001000) and the Natural Science Foundation of China (Grant No. 61622310).	Cao Y, 2016, AAAI CONF ARTIF INTE, P3457; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Ji Jianqiu, 2012, ADV NEURAL INFORM PR, P108; Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947; Li W., 2016, INT JOINT C ARTIFICI, P1711; Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253; Lin K, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301269; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Mu YD, 2010, AAAI CONF ARTIF INTE, P539; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Wang J., 2010, ICML, P1127; Wang Jianfeng, 2013, P 21 ACM INT C MULTI, P133; Wang X., 2016, P AS C COMP VIS, P70; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Yang H. F., 2017, IEEE T PATTERN ANAL, P1; Yao T., 2016, P INT JOINT C ART IN, P3931; Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhu H, 2016, AAAI CONF ARTIF INTE, P2415	27	121	132	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402052
C	Greydanus, S; Dzamba, M; Yosinski, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Greydanus, Sam; Dzamba, Misko; Yosinski, Jason			Hamiltonian Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CHEMISTRY	Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time. [GRAPHICS]	[Greydanus, Sam] Google Brain, Mountain View, CA 94043 USA; [Dzamba, Misko] PetCube, San Francisco, CA USA; [Yosinski, Jason] Uber AI Labs, San Francisco, CA USA	Google Incorporated	Greydanus, S (corresponding author), Google Brain, Mountain View, CA 94043 USA.	sgrey@google.com; mouse9911@gmail.com; yosinski@uber.com		Greydanus, Sam/0000-0001-7039-6790; Yosinski, Jason/0000-0002-4701-0199				Battaglia Peter W, 2016, ARXIV161200222; Behler J, 2011, PHYS CHEM CHEM PHYS, V13, P17930, DOI 10.1039/c1cp21668f; Brockman G., 2016, OPENAI GYM; Chang Michael B, 2016, ARXIV161200341; Chen T. Q., 2018, NEURAL ORDINARY DIFF, P6571; Chmiela S, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1603015; Cohen-Tannoudji C, 1997, PHOTONS ATOMS INTRO, P486; Gastegger M, 2015, J CHEM THEORY COMPUT, V11, P2187, DOI 10.1021/acs.jctc.5b00211; Girvin S. M., 2019, MODERN CONDENSED MAT, DOI 10.1017/9781316480649; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Grzeszczuk R., NEUROANIMATOR FAST N; Ha D., 2018, ADV NEURAL INFORM PR, P2450; Hafner D., 2018, ARXIV181104551; Hamrick J.B., 2018, ARXIV180601203; Iten R., 2018, ARXIV180710300; Jacobsen J.H., 2018, ICLR; Kingma D., 2014, ADAM METHOD STOCHAST; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lamacraft A., 2019, ARXIV190604645; Levine S., 2018, INT J ROBOT RES, V37, P421; Liu R, 2018, ADV NEURAL INFORM PR, P9605; Lutter M., 2019, INT C LEARN REPR; MacKay M., 2018, ADV NEURAL INFORM PR, V31, P9029; Mnih V., 2013, ARXIV E PRINTS; NOETHER E, 1971, TRANSPORT THEOR STAT, V1, P186, DOI 10.1080/00411457108231446; Reichl L.E, 1999, MODERN COURSE STAT P; Runge C., 1895, MATH ANN, V46, P167, DOI DOI 10.1007/BF01446807; Rupp M., 2012, PHYS REV LETT, V108; Sakurai JJ., 1995, MODERN QUANTUM MECH, VRevised; SALMON R, 1988, ANNU REV FLUID MECH, V20, P225, DOI 10.1146/annurev.fl.20.010188.001301; Santoro A., 2017, ADV NEURAL INFORM PR, P4967; Schmidt M, 2009, SCIENCE, V324, P81, DOI 10.1126/science.1165893; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Smith JS, 2017, CHEM SCI, V8, P3192, DOI 10.1039/c6sc05720a; Taylor J.R., 2005, CLASSICAL MECH; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Wang J, 2018, ACS CENTRAL SCI; Watters N., 2017, ADV NEURAL INFORM PR, P4539; Yosinski J, 2011, P 20 EUROPEAN C ARTI, P890	45	120	119	6	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907008
C	Kohl, SAA; Romera-Paredes, B; Meyer, C; De Fauw, J; Ledsam, JR; Maier-Hein, KH; Eslami, SMA; Rezende, DJ; Ronneberger, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kohl, Simon A. A.; Romera-Paredes, Bernardino; Meyer, Clemens; De Fauw, Jeffrey; Ledsam, Joseph R.; Maier-Hein, Klaus H.; Eslami, S. M. Ali; Rezende, Danilo Jimenez; Ronneberger, Olaf			A Probabilistic U-Net for Segmentation of Ambiguous Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.	[Kohl, Simon A. A.; Romera-Paredes, Bernardino; Meyer, Clemens; De Fauw, Jeffrey; Ledsam, Joseph R.; Eslami, S. M. Ali; Rezende, Danilo Jimenez; Ronneberger, Olaf] DeepMind, London, England; [Kohl, Simon A. A.; Maier-Hein, Klaus H.] German Canc Res Ctr, Div Med Image Comp, Heidelberg, Germany	Helmholtz Association; German Cancer Research Center (DKFZ)	Kohl, SAA (corresponding author), DeepMind, London, England.; Kohl, SAA (corresponding author), German Canc Res Ctr, Div Med Image Comp, Heidelberg, Germany.	simon.kohl@dkfz.de; brp@google.com; meyerc@google.com; defauw@google.com; jledsam@google.com; k.maier-hein@dkfz.de; aeslami@google.com; danilor@google.com; olafr@google.com	Maier-Hein, Klaus Hermann/AAF-8487-2020	Maier-Hein, Klaus Hermann/0000-0002-6626-2463				Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204; Ba J., 2017, P 3 INT C LEARN REPR; Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1; Bellemare MG, 2017, ARXIV; Bouchacourt D, 2016, ADV NEURAL INFORM PR, P352; Chen C., 2013, P 16 INT C ART INT S, P161; Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Esser P., 2018, ARXIV180404694; Goodfellow I., 2016, ARXIV; Higgins I., 2017, ICLR, P1; Ilg E, 2018, ARXIV180207095; Isola P., 2017, IMAGE TO IMAGE TRANS; Kendall A, 2015, P BRIT MACH VIS C 20; Kendall A., 2017, WHAT UNCERTAINTIES W, V3, P4; Kingma D. P., 2014, NEURAL INFORM PROCES; Kingma DP, 2013, P 2 INT C LEARN REPR; Kirillov A., 2016, ADV NEURAL INFORM PR, P334; Kirillov A, 2015, IEEE I CONF COMP VIS, P1814, DOI 10.1109/ICCV.2015.211; Kirillov Alexander, 2015, ADV NEURAL INFORM PR, P613; Klebanov L., 2005, N DISTANCES THEIR AP; Kohli P, 2012, P ADV NEUR INF PROC, P1799; Kosub S., 2016, ARXIV161202696; Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6405; Lee S., 2016, ADV NEURAL INFORM PR, V29, P2119; Lee Stefan, 2015, ARXIV151106314, P2; Lipkus AH, 1999, J MATH CHEM, V26, P263, DOI 10.1023/A:1019154432472; Radford A., 2018, ARXIV180305573; RAO CR, 1982, THEOR POPUL BIOL, V21, P24, DOI 10.1016/0040-5809(82)90004-1; Rezende D. J., 2014, P 31 INT C MACH LEAR; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rupprecht C., 2017, INT C COMP VIS ICCV; Salimans T., 2016, ADV NEUR IN, P2234; Sohn Kihyuk, 2015, NEURAL INFORM PROCES; Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018	36	120	122	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001050
C	Rogez, G; Schmid, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rogez, Gregory; Schmid, Cordelia			MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper addresses the problem of 3D human pose estimation in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D poses. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images with 2D human pose annotations using 3D Motion Capture (MoCap) data. Given a candidate 3D pose our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a K-way classification problem. Such an approach is viable only with large training sets such as ours. Our method outperforms the state of the art in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for in-the-wild images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images.	[Rogez, Gregory; Schmid, Cordelia] Inria Grenoble Rhone Alpes, Lab Jean Kuntzmann, Grenoble, France	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria	Rogez, G (corresponding author), Inria Grenoble Rhone Alpes, Lab Jean Kuntzmann, Grenoble, France.				European Commission under FP7 Marie Curie IOF grant [PIOF-GA-2012-328288]; ERC advanced grant Allegro; NVIDIA	European Commission under FP7 Marie Curie IOF grant; ERC advanced grant Allegro; NVIDIA	This work was supported by the European Commission under FP7 Marie Curie IOF grant (PIOF-GA-2012-328288) and partially supported by ERC advanced grant Allegro. We acknowledge the support of NVIDIA with the donation of the GPUs used for this research. We thank P. Weinzaepfel for his help and the anonymous reviewers for their comments and suggestions.	Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21; Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bissacco A., 2006, NIPS; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Enzweiler M, 2008, PROC CVPR IEEE, P1944; Fan XC, 2014, LECT NOTES COMPUT SC, V8689, P174, DOI 10.1007/978-3-319-10590-1_12; Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006; Hornung A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186645; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318; Johnson Sam, 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12]; Kostrikov I., 2014, PROC BRIT MACH VIS C; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Okada R, 2008, LECT NOTES COMPUT SC, V5303, P434, DOI 10.1007/978-3-540-88688-4_32; Park Dennis, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P58, DOI 10.1109/CVPRW.2015.7301337; Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052; Rogez G, 2015, PROC CVPR IEEE, P4325, DOI 10.1109/CVPR.2015.7299061; Rogez G, 2012, INT J COMPUT VISION, V99, P25, DOI 10.1007/s11263-012-0516-9; Romero J, 2010, IEEE INT CONF ROBOT, P458, DOI 10.1109/ROBOT.2010.5509753; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Simo-Serra E, 2013, PROC CVPR IEEE, P3634, DOI 10.1109/CVPR.2013.466; Simo-Serra E., 2012, CVPR; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Wang CY, 2014, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2014.303; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Zhou F, 2014, LECT NOTES COMPUT SC, V8694, P62, DOI 10.1007/978-3-319-10599-4_5; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zuffi S, 2015, PROC CVPR IEEE, P3537, DOI 10.1109/CVPR.2015.7298976	45	120	124	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704087
C	Dou, Q; Castro, DC; Kamnitsas, K; Glocker, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dou, Qi; Castro, Daniel C.; Kamnitsas, Konstantinos; Glocker, Ben			Domain Generalization via Model-Agnostic Learning of Semantic Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Generalization capability to unseen domains is crucial for machine learning models when deploying to real-world conditions. We investigate the challenging problem of domain generalization, i.e., training a model on multi-domain source data such that it can directly generalize to target domains with unknown statistics. We adopt a model-agnostic learning paradigm with gradient-based meta-train and meta-test procedures to expose the optimization to domain shift. Further, we introduce two complementary losses which explicitly regularize the semantic structure of the feature space. Globally, we align a derived soft confusion matrix to preserve general knowledge about inter-class relationships. Locally, we promote domain-independent class-specific cohesion and separation of sample features with a metric-learning component. The effectiveness of our method is demonstrated with new state-of-the-art results on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.	[Dou, Qi; Castro, Daniel C.; Kamnitsas, Konstantinos; Glocker, Ben] Imperial Coll London, Biomed Image Anal Grp, London, England	Imperial College London	Dou, Q (corresponding author), Imperial Coll London, Biomed Image Anal Grp, London, England.	qi.dou@imperial.ac.uk; dc315@imperial.ac.uk; kk2412@imperial.ac.uk; b.glocker@imperial.ac.uk		Coelho de Castro, Daniel/0000-0002-6829-7045	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [757173]; EPSRC Impact Acceleration Award [EP/R511547/1]; CAPES, Ministry of Education, Brazil [BEX 1500/2015-05]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); EPSRC Impact Acceleration Award(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); CAPES, Ministry of Education, Brazil(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant No 757173, project MIRA, ERC2017-STG) and is supported by an EPSRC Impact Acceleration Award (EP/R511547/1). DCC is also partly supported by CAPES, Ministry of Education, Brazil (BEX 1500/2015-05).	Balaji Y, 2018, ADV NEUR IN, V31; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Carlucci Fabio M, 2019, CVPR; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fang C, 2013, IEEE I CONF COMP VIS, P1657, DOI 10.1109/ICCV.2013.208; Finn C, 2017, PR MACH LEARN RES, V70; Ganin Y., 2016, JMLR, V17, P2096; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, ADV NEURAL INF PROCE; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton Geoffrey, 2014, NEURIPS 2014 DEEP LE; Hoffman J, 2018, PR MACH LEARN RES, V80; Hsu Yen-Chang, 2018, ICLR; Kamnitsas K, 2018, PR MACH LEARN RES, V80; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar A., 2010, ADV NEURAL INFORM PR, V23, P478; Li D, 2018, AAAI CONF ARTIF INTE, P3490; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li Da, 2019, ARXIV190200113; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li Ke, 2017, ICLR; Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38; Li YJ, 2019, PR MACH LEARN RES, V97; Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136; Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Muandet Krikamol, 2013, ICML; Nichol Alex, 2018, ARXIV180302999; Ravi S., 2017, INT C LEARN REPR, P12; Robinson R., 2019, MACHINE LEARNING MUL; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; Schmidhuber J, 1987, THESIS; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Shankar Shiv, 2018, P INT C LEARN REPR I; Thrun S, 1998, LEARNING TO LEARN, P181; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Volpi Riccardo, 2018, ARXIV180512018	53	119	119	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306045
C	Lim, S; Kim, I; Kim, T; Kim, C; Kim, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lim, Sungbin; Kim, Ildoo; Kim, Taesup; Kim, Chiheon; Kim, Sungwoong			Fast AutoAugment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, AutoAugment [5] has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of GPU hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet. Our code is open to the public by the official GitHub(3) of Kakao Brain.	[Lim, Sungbin] UNIST, Ulsan, South Korea; [Kim, Ildoo; Kim, Chiheon; Kim, Sungwoong] Kakao Brain, Seongnam, South Korea; [Kim, Taesup] Univ Montreal, MILA, Montreal, PQ, Canada	Ulsan National Institute of Science & Technology (UNIST); Universite de Montreal	Lim, S (corresponding author), UNIST, Ulsan, South Korea.	sungbin@unist.ac.kr; ildoo.kim@kakaobrain.com; taesup.kim@umontreal.ca; chiheon.kim@kakaobrain.com; swkim@kakaobrain.com	Lim, Sungbin/ABG-4115-2021; Lim, Sungbin/GPS-4633-2022	Lim, Sungbin/0000-0003-2684-2022; 				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Brain K., 2019, AUTOCLINT AUTOMATIC; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Esfahani MS, 2014, BIOINFORMATICS, V30, P242, DOI 10.1093/bioinformatics/btt662; Gastaldi Xavier, 2017, ARXIV170507485; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hernandez-Garcia A., 2018, ARXIV180603852; Ho D, 2019, PR MACH LEARN RES, V97; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jaderberg Max, 2017, ABS171109846 CORR; Kim S., 2018, SCALABLE NEURAL ARCH; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lemley J, 2017, IEEE ACCESS, V5, P5858, DOI 10.1109/ACCESS.2017.2696121; Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241; Moritz P, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P561; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Vo N, 2017, IEEE INT CONF BIG DA, P797; Paschali M., 2019, ARXIV PREPRINT ARXIV, DOI [10.1007/978-3-030-20351-1_40, DOI 10.1007/978-3-030-20351-1_40]; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sato Ikuro, 2015, ARXIV150503229; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; TERRELL GR, 1992, ANN STAT, V20, P1236, DOI 10.1214/aos/1176348768; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Yamada Y., 2018, ABS180202375 CORR; You Yang, 2017, ARXIV170803888, V6, P12; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	41	119	122	4	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306064
C	Alvarez-Melis, D; Jaakkola, TS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Alvarez-Melis, David; Jaakkola, Tommi S.			Towards Robust Interpretability with Self-Explaining Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general - explicitness, faithfulness, and stability - and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.	[Alvarez-Melis, David; Jaakkola, Tommi S.] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Alvarez-Melis, D (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	dalvmel@mit.edu; tommi@csail.mit.edu	Alvarez-Melis, David/AAV-1099-2021	/0000-0002-9591-8986	MIT-IBM grant on deep rationalization; Hewlett Packard; CONACYT	MIT-IBM grant on deep rationalization(International Business Machines (IBM)); Hewlett Packard; CONACYT(Consejo Nacional de Ciencia y Tecnologia (CONACyT))	The authors would like to thank the anonymous reviewers and Been Kim for helpful comments. The work was partially supported by an MIT-IBM grant on deep rationalization and by Graduate Fellowships from Hewlett Packard and CONACYT.	Al-Shedivat M., 2017, ARXIV170510301; Alvarez-Melis David, 2017, P 2017 C EMP METH NA, P412, DOI DOI 10.18653/V1/D17-1042; Alvarez-Melis David, 2018, ARXIV180608049; Arras L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181142; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Grgic-Hlaca N, 2018, AAAI CONF ARTIF INTE, P51; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kim B, 2017, ARXIV PREPRINT ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li O, 2018, AAAI CONF ARTIF INTE, P3530; Lichman M, 2013, UCI MACHINE LEARNING; Lundberg SM, 2017, ADV NEUR IN, V30; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Sundararajan M, 2017, PR MACH LEARN RES, V70; Yosinski J., 2015, ICML DEEP LEARN WORK; Zafar MB, 2017, ADV NEURAL INFORM PR, V30, P229; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	26	119	120	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002033
C	Dabkowski, P; Gal, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dabkowski, Piotr; Gal, Yarin			Real Time Image Saliency for Black Box Classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.	[Dabkowski, Piotr; Gal, Yarin] Univ Cambridge, Cambridge, England; [Gal, Yarin] Alan Turing Inst, London, England	University of Cambridge	Dabkowski, P (corresponding author), Univ Cambridge, Cambridge, England.	pd437@cam.ac.uk; yarin.gal@eng.cam.ac.uk	Jeong, Yongwook/N-7413-2016					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Romero Adriana, 2014, CORR; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Springenberg J.T., 2014, ARXIV14126806; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang Jianming, 2016, TOP DOWN NEURAL ATTE; Zhou B., 2015, CVPR; Zhou B., 2014, CORR, V1412, P6856	18	119	122	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407006
C	Snelson, E; Rasmussen, CE; Ghahramani, Z		Thrun, S; Saul, K; Scholkopf, B		Snelson, E; Rasmussen, CE; Ghahramani, Z			Warped Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.	Univ Coll London, Gatsby Comp Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Snelson, E (corresponding author), Univ Coll London, Gatsby Comp Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.			Rasmussen, Carl Edward/0000-0001-8899-7850				CAMACHO R, 2000, THESIS U PORTO; Cole D, 2000, SCI TECHNOL WELD JOI, V5, P81, DOI 10.1179/136217100101538065; DIGGLE PJ, 1998, APPL STAT; Gibbs M., 1997, THESIS; GOLDBERG PW, 1998, ADV NEURAL INFORMATI, V10; MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133; Neal R. M, 1997, 9702 U TOR; Newman C. B. D., 1998, UCI REPOSITORY MACHI; OHAGAN A, 2000, 49800 U SHEFF; Rasmussen C.E., 1996, THESIS U TORONTO; WILLIAMS CKI, 1996, ADV NEURAL INFORMATI, V8	11	119	123	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						337	344						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500043
C	Ryan, J; Lin, MJ; Miikkulainen, R		Jordan, MI; Kearns, MJ; Solla, SA		Ryan, J; Lin, MJ; Miikkulainen, R			Intrusion detection with neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					With the rapid expansion of computer networks during the past few years, security has become a crucial issue for modern computer systems. A good way to detect illegitimate use is through monitoring unusual user activity. Methods of intrusion detection based on hand-coded rule sets or predicting commands on-line are laborous to build or not very reliable. This paper proposes a new way of applying neural networks to detect intrusions. We believe that a user leaves a 'print' when using the system; a neural network can be used to learn this print and identify each user much like detectives use thumbprints to place people at crime scenes. If a user's behavior does not match his/her print, the system administrator can be alerted of a possible security breech. A backpropagation neural network called NNID (Neural Network Intrusion Detector) was trained in the identification task and tested experimentally on a system of 10 users. The system was 96% accurate in detecting unusual activity, with 7% false alarm rate. These results suggest that learning user profiles is an effective way for detecting intrusions.	MCI Commun Corp, Austin, TX 78753 USA		Ryan, J (corresponding author), MCI Commun Corp, 9001 N IH 35, Austin, TX 78753 USA.								0	119	124	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						943	949						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700133
C	Zhang, RX; Che, T; Ghahramani, Z; Bengio, Y; Song, YQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Ruixiang; Che, Tong; Ghahramani, Zoubin; Bengio, Yoshua; Song, Yangqiu			MetaGAN: An Adversarial Approach to Few-Shot Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we propose a conceptually simple and general framework called MetaGAN for few-shot learning problems. Most state-of-the-art few-shot classification models can be integrated with MetaGAN in a principled and straightforward way. By introducing an adversarial generator conditioned on tasks, we augment vanilla few-shot classification models with the ability to discriminate between real and fake data. We argue that this GAN-based approach can help few-shot classifiers to learn sharper decision boundary, which could generalize better. We show that with our MetaGAN framework, we can extend supervised few-shot learning models to naturally cope with unlabeled data. Different from previous work in semi-supervised few-shot learning, our algorithms can deal with semi-supervision at both sample-level and task-level. We give theoretical justifications of the strength of MetaGAN, and validate the effectiveness of MetaGAN on challenging few-shot image classification benchmarks.	[Zhang, Ruixiang; Che, Tong] Univ Montreal, MILA, Montreal, PQ, Canada; [Ghahramani, Zoubin] Univ Cambridge, Cambridge, England; [Bengio, Yoshua] Univ Montreal, MILA, CIFAR, Montreal, PQ, Canada; [Song, Yangqiu] HKUST, Hong Kong, Peoples R China	Universite de Montreal; University of Cambridge; Canadian Institute for Advanced Research (CIFAR); Universite de Montreal; Hong Kong University of Science & Technology	Zhang, RX (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.	sodabeta7@gmail.com; tongcheprivate@gmail.com; zoubin@cam.ac.uk; yoshua.bengio@mila.quebec; yqsong@cse.ust.hk	Zhang, Rui/GXN-3801-2022		Intel Corporation	Intel Corporation(Intel Corporation)	We thank Intel Corporation for supporting our deep learning related research.	[Anonymous], INT C LEARN REPR; Che  Tong, 2017, INT C LEARN REPR; Chen X, 2016, ADV NEUR IN, V29; Dai Z., 2017, P ADV NEURAL INFORM, P6510; Edwards  Harrison, 2017, 5 INT C LEARN REPR I; Finn C, 2017, PR MACH LEARN RES, V70; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Kingma D.P, P 3 INT C LEARNING R; Mishra Nikhil, 2018, P ICLR; Munkhdalai T, 2017, ABS171209926 CORR; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Ravi S., 2017, P INT C LEARN REPR, P1; Salimans T., 2016, ADV NEUR IN, P2234; Santoro A, 2016, PR MACH LEARN RES, V48; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F., 2018, P IEEE C COMP VIS PA; Thrun S, 1998, LEARNING TO LEARN, P181; Vinyals Oriol, 2016, ARXIV160604080, P3630	22	118	120	11	37	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302038
C	Roth, V; Steinhage, V		Solla, SA; Leen, TK; Muller, KR		Roth, V; Steinhage, V			Nonlinear discriminant analysis using kernel functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Fishers linear discriminant analysis (LDA) is a classical multivariate technique both for dimension reduction and classification. The data vectors are transformed into a low dimensional subspace such that the class centroids are spread out as much as possible. In this subspace LDA works as a simple prototype classifier with linear decision boundaries. However, in many applications the linear boundaries do not adequately separate the classes, We present a nonlinear generalization of discriminant analysis that uses the kernel trick of representing dot products by kernel functions. The presented algorithm allows a simple formulation of the EM-algorithm in terms of kernel functions which leads to a unique concept for unsupervised mixture analysis, supervised discriminant analysis and semi-supervised discriminant analysis with partially unlabelled observations in feature spaces.	Univ Bonn, Inst Comp Sci III, D-53117 Bonn, Germany	University of Bonn	Roth, V (corresponding author), Univ Bonn, Inst Comp Sci III, Romerstr 164, D-53117 Bonn, Germany.		Roth, Volker/Q-4025-2017	Roth, Volker/0000-0003-0991-0273				Duda R.O., 1973, J ROYAL STAT SOC SER; Flury B., 1997, 1 COURSE MULTIVARIAT; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; HASTIE T, 1996, J R STAT SOC B, V58, P158; JOACHIMS T, 1999, ADV KERNAL METHODS S; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Olshen R., 1984, CLASSIFICATION REGRE; ROTH V, 1999, IAITR997 BONN U DEP; ROTH V, 1999, MUSTERERKENNUNG 1999, P120; SAUNDERS S, 1998, RIDGE REGRESSION LEA; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SCHOLKOPF B, 1997, THESIS MUNICH; Vapnik V.N, 1998, STAT LEARNING THEORY	14	118	127	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						568	574						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700081
C	Lan, X; Zhu, XT; Gong, SG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lan, Xu; Zhu, Xiatian; Gong, Shaogang			Knowledge Distillation by On-the-Fly Native Ensemble	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Knowledge distillation is effective to train the small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a high-capacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) learning strategy for one-stage online distillation. Specifically, ONE only trains a single multi-branch network while simultaneously establishing a strong teacher on-the-fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance of a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.	[Lan, Xu; Gong, Shaogang] Queen Mary Univ London, London, England; [Zhu, Xiatian] Vis Semant Ltd, London, England	University of London; Queen Mary University London	Lan, X (corresponding author), Queen Mary Univ London, London, England.		Zhu, Xiatian/Y-1601-2019	Zhu, Xiatian/0000-0002-9284-2955	China Scholarship Council; Royal Society Newton Advanced Fellowship Programme [NA150459]; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety [98111-571149]; Vision Semantics Limited	China Scholarship Council(China Scholarship Council); Royal Society Newton Advanced Fellowship Programme; Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety(UK Research & Innovation (UKRI)Innovate UK); Vision Semantics Limited	This work was partly supported by the China Scholarship Council, Vision Semantics Limited, the Royal Society Newton Advanced Fellowship Programme (NA150459), and Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety (98111-571149).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bucilua Cristian, 2006, P 12 ACM SIGKDD; Chaudhari Pratik, 2016, ENTROPY SGD BIASING; Girshick R., 2015, ICCV; Guo YQ, 2017, IEEE INT C INTELL TR; Han S., 2016, P 4 INT C LEARN REPR, P1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Gao, 2017, ARXIV PREPRINT ARXIV; Huang  Gao, 2017, ARXIV171109224; Jin XX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901353; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lan X., 2018, P AS C COMP VIS, P284; Lan X, 2018, LECT NOTES COMPUT SC, V11205, P553, DOI 10.1007/978-3-030-01246-5_33; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Li H., 2017, P INT C LEARN REPR I, P1; Li Shen, 2017, SQUEEZE AND EXCITATI; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Romero Adriana, 2014, ARXIV14126550; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Y., 2017, ARXIV170500384	33	117	123	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002010
C	Rasmussen, CE; Ghahramani, Z		Leen, TK; Dietterich, TG; Tresp, V		Rasmussen, CE; Ghahramani, Z			Occam's Razor	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The Bayesian Paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.	Tech Univ Denmark, Dept Math Modelling, DK-2800 Lyngby, Denmark	Technical University of Denmark	Rasmussen, CE (corresponding author), Tech Univ Denmark, Dept Math Modelling, Bldg 321, DK-2800 Lyngby, Denmark.			Rasmussen, Carl Edward/0000-0001-8899-7850				JEFFERYS WH, 1992, AM SCI, V80, P64; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Neal RM, 1996, LECT NOTES STAT, V118; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; SMITH AFM, 1980, J ROY STAT SOC B MET, V42, P213	5	117	117	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						294	300						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800042
C	Williams, CKI		Solla, SA; Leen, TK; Muller, KR		Williams, CKI			A MCMC approach to hierarchical mixture modelling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					There are many hierarchical clustering algorithms available, but these lack a firm statistical basis. Here we set up a hierarchical probabilistic mixture model, where data is generated in a hierarchical tree-structured manner. Markov chain Monte Carlo (MCMC) methods are demonstrated which can be used to sample from the posterior distribution over trees containing variable numbers of hidden units.	Univ Edinburgh, Inst Adapt & Neural Comp, Div Informat, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Williams, CKI (corresponding author), Univ Edinburgh, Inst Adapt & Neural Comp, Div Informat, 5 Forrest Hill, Edinburgh EH1 2QL, Midlothian, Scotland.							ADAMS NJ, 1999, UNPUB ICPR 2000; AMBROSINGERSON J, 1990, SCIENCE, V247, P1344, DOI 10.1126/science.2315702; Durbin R., 1998, BIOL SEQUENCE ANAL P; EDWARDS AWF, 1970, J ROY STAT SOC B, V32, P155; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; HANSON R, 1991, IJCAI 91 P 12 INT JO; HOFMANN T, 1995, P ICANN 95; LUETTGEN MR, 1995, IEEE T IMAGE PROCESS, V4, P194, DOI 10.1109/83.342185; MADIGAN D, 1995, INT STAT REV, V63, P215, DOI 10.2307/1403615; MOZER M, 1991, ADV NEURAL INFORMATI, V3; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Ripley BD., 1996; Vasconcelos N, 1999, ADV NEUR IN, V11, P606; WILLIAMS CKI, 1999, ADV NEURAL INFORMATI, V11; XU L, 1989, UNCERTAINTY ARTIFICI, V3	15	117	123	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						680	686						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700097
C	Minnen, D; Balle, J; Toderici, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Minnen, David; Balle, Johannes; Toderici, George			Joint Autoregressive and Hierarchical Priors for Learned Image Compression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate-distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.	[Minnen, David; Balle, Johannes; Toderici, George] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Minnen, D (corresponding author), Google Res, Mountain View, CA 94043 USA.	dminnen@google.com; jballe@google.com; gtoderici@google.com						Agustsson E, 2017, ADV NEUR IN, V30; [Anonymous], 2000, INF TECHN JPEG 2000; [Anonymous], 2013, 230082 ISOIEC; Baig MH, 2017, ADV NEUR IN, V30; Balle  J., 2018, 6 INT C LEARN REPR; Balle  J., 2016, 4 INT C LEARN REPR; Balle Johannes, 2017, ICLR 2017; Bellard Fabrice, BPG IMAGE FORMAT; Chen  X., 2017, 5 INT C LEARN REPR; Cottrell G. W., 1987, MODELS COGNITION REV, V1, P461; David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; Google, WEBP COMPR TECHNQ; Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P9, DOI 10.1109/79.952802; Gulrajani  I., 2017, 5 INT C LEARN REPR; Higgins  I., 2017, 5 INT C LEARN REPR; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Jiang J, 1999, SIGNAL PROCESS-IMAGE, V14, P737, DOI 10.1016/S0923-5965(98)00041-1; Johnston  N., 2018, 2018 IEEE C COMP VIS; Kingma D.P., 2014, 2 INT C LEARN REP IC; Klopp J. P., 2018, BRIT MACH VIS C BMVC; Kodak  E., PHOTOCD PCD0992; Li M., 2017, CORR ABS17031; Luttrell SP, 1988, IGARSS; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Martin  G., 1979, VID DAT REC C JUL; Mentzer F., 2018, 2018 IEEE C COMP VIS; Minnen D., 2017, INT C IM PROC; Minnen D., 2018, INT C IM PROC; Nakanishi K., 2018, ARXIV180506386; Oord A. v. d., 2017, ARXIV171110433; Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009; Reed  S., 2017, INT C MACH LEARN ICM; Rippel O, 2017, PR MACH LEARN RES, V70; RISSANEN J, 1981, IEEE T INFORM THEORY, V27, P12, DOI 10.1109/TIT.1981.1056282; Theis Lucas, 2017, 5 INT C LEARN REPR; Toderici  G., 2016, 4 INT C LEARN REPR; Toderici  G., 2017, 2017 IEEE C COMP VIS, DOI [10.1109/CVPR.2017.577, DOI 10.1109/CVPR.2017.577.ARXIV:1608.05148]; VANLEEUWEN J, 1976, P 3 INT C AUT LANG P, P382; Wainwright MJ, 2000, ADV NEUR IN, V12, P855; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Watkins Bruce  E., 1991, DATA COMPRESSION USI	41	116	116	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005036
C	Paccanaro, A; Hinton, GE		Dietterich, TG; Becker, S; Ghahramani, Z		Paccanaro, A; Hinton, GE			Learning hierarchical structures with linear relational embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts. Its final goal is to be able to generalize, i.e. infer new instances of these relations among the concepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to find compact distributed representations for variable-sized recursive data structures, such as trees and lists.	UCL, Gatsby Computat Neurosci Unit, London, England	University of London; University College London	Paccanaro, A (corresponding author), UCL, Gatsby Computat Neurosci Unit, 17 Queen Sq, London, England.	alberto@gatsby.ucl.ac.uk; hinton@gatsby.ucl.ac.uk	Paccanaro, Alberto/AAW-5625-2020					Hinton G, 1986, P 8 ANN C COGN SCI S, V1, DOI DOI 10.1016/J.NEUCOM.2013.03.009; Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P77; O'Reilly R.C., 1996, THESIS CARNEGIE MELL; Paccanaro A, 2000, ICML 2000, P711; PACCANARO A, 2002, THESIS U TORONTO; POLLACK JB, 1990, ARTIF INTELL, V46, P77, DOI 10.1016/0004-3702(90)90005-K; QUINLAN JR, 1990, MACH LEARN, V5, P239, DOI 10.1007/BF00117105	7	116	121	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						857	864						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100107
C	Drucker, H; Cortes, C		Touretzky, DS; Mozer, MC; Hasselmo, ME		Drucker, H; Cortes, C			Boosting decision trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						AT&T BELL LABS,HOLMDEL,NJ 07733	AT&T; Nokia Corporation; Nokia Bell Labs									0	116	122	0	6	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						479	485						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00068
C	Newell, A; Deng, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Newell, Alejandro; Deng, Jia			Pixels to Graphs by Associative Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph definition. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and demonstrate state-of-the-art performance on the challenging task of scene graph generation.	[Newell, Alejandro; Deng, Jia] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Newell, A (corresponding author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.	alnewell@umich.edu; jiadeng@umich.edu	Jeong, Yongwook/N-7413-2016		King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR) [OSR-2015-CRG4-2639]	King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR)	This publication is based upon work supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-2015-CRG4-2639.	Abadi M, 2015, P 12 USENIX S OPERAT; Atzmon Y., 2016, ARXIV160807639; Chao Y. -W., 2017, ARXIV170205448; Dai B., 2017, ARXIV170403114; Frome A, 2007, IEEE I CONF COMP VIS, P94; Frome Andrea, 2013, NEURIPS; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; He K., 2017, ARXIV170306870, P2980, DOI [10.1109/ICCV.2017.322, DOI 10.1109/ICCV.2017.322]; Hu Ronghang, 2016, ARXIV161109978; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Krishna R., 2016, VISUAL GENOME CONNEC; Li Yi, 2017, P IEEE C COMP VIS PA; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Liao Wentong, 2016, ARXIV160905834; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu Cewu, 2016, ARXIV161207310; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Newell Alejandro, 2016, ARXIV161105424; Plummer B. A., 2016, ARXIV161106641; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Santoro A, 2017, ARXIV PREPRINT ARXIV; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25; Zhang Hanwang, 2017, PROC CVPR IEEE, P5532, DOI [DOI 10.1109/CVPR.2017.331, DOI 10.1109/CVPR.2018.00611]; Zhuang Bohan, 2017, ARXIV170306246	28	115	117	1	13	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402022
C	Guestrin, C; Koller, D; Parr, R		Dietterich, TG; Becker, S; Ghahramani, Z		Guestrin, C; Koller, D; Parr, R			Multiagent planning with factored MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a principled and efficient planning algorithm for cooperative multiagent dynamic systems. A striking feature of our method is that the coordination and communication between the agents is not imposed, but derived directly from the system dynamics and function approximation architecture. We view the entire multiagent system as a single, large Markov decision process (MDP), which we assume can be represented in a factored way using a dynamic Bayesian network (DBN). The action space of the resulting MDP is the joint action space of the entire set of agents. Our approach is based on the use of factored linear value functions as an approximation to the joint value function. This factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme. We provide a simple and efficient method for computing such an approximate value function by solving a single linear program, whose size is determined by the interaction between the value function structure and the DBN. We thereby avoid the exponential blowup in the state and action space. We show that our approach compares favorably with approaches based on reward sharing. We also show that our algorithm is an efficient alternative to more complicated algorithms even in the single agent case.	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Guestrin, C (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.							Bertele U., 1972, NONSERIAL DYNAMIC PR; Boutilier C, 1999, J ARTIF INTELL RES, V11, P1; Dean T., 1989, Computational Intelligence, V5, P142, DOI 10.1111/j.1467-8640.1989.tb00324.x; Dechter R, 1999, ARTIF INTELL, V113, P41, DOI 10.1016/S0004-3702(99)00059-4; DEFARIAS DP, 2001, UNPUB IEEE T AUTOMAT; GUESTRIN C, 2001, P 17 IJCAI; Jensen F., 1994, Uncertainty in Artificial Intelligence. Proceedings of the Tenth Conference (1994), P367; KOLLER D, 1999, P 16 INT JOINT C ART; KOLLER D, 2000, P 16 UAI; PESHKIN L, 2000, P 16 UAI; SCHNEIDER J, 1999, P 16 ICML; SCHWEITZER PJ, 1985, J MATH ANAL APPL, V110, P568, DOI 10.1016/0022-247X(85)90317-8; WOLPERT D, 1999, P 3 AG C	13	114	114	1	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1523	1530						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100189
C	Chapelle, O; Vapnik, V		Solla, SA; Leen, TK; Muller, KR		Chapelle, O; Vapnik, V			Model selection for support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					New functionals for parameter (model) selection of Support Vector Machines are introduced based on the concepts of the span of support vectors and rescaling of the feature space. It is shown that using these functionals, one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter.	AT&T Labs Res, Red Bank, NJ 07701 USA	AT&T	Chapelle, O (corresponding author), AT&T Labs Res, 100 Schultz Dr,POB7033, Red Bank, NJ 07701 USA.							Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Jaakkola T, 1999, P 1999 C AI STAT; OPPER M, 1999, IN PRESS ADV LARGE M; SCHOLKOPF B, 1997, SPRINGER LECT NOTES, V1327, P583; SCHOLKOPF B, 9 INT C ART NEUR NET, P304; VAPNIK V, 1999, NEURAL COMPUTATION; Vapnik V.N, 1998, STAT LEARNING THEORY	7	114	121	0	8	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						230	236						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700033
C	Chami, I; Ying, R; Re, C; Leskovec, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chami, Ines; Ying, Rex; Re, Christopher; Leskovec, Jure			Hyperbolic Graph Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations, such as feature transformation and aggregation, in hyperbolic space. Furthermore, since input features are often Euclidean, it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs. We derive GCNs operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure, and leads to improved performance when compared to Euclidean analogs, even with very low dimensional embeddings: compared to state-of-the-art GCNs, HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification, also improving state-of-the art on the Pubmed dataset.	[Ying, Rex; Re, Christopher; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Chami, Ines] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA	Stanford University; Stanford University	Chami, I (corresponding author), Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.	chami@cs.stanford.edu; rexying@cs.stanford.edu; chrismre@cs.stanford.edu; jure@cs.stanford.edu			DARPA [FA87501720095, FA86501827865, FA86501827882, FA865018C7880]; NIH [U54EB020405]; IARPA [2017-17071900005]; NSF [CCF1763315, CCF1563078, OAC-1835598]; Stanford Data Science Initiative; ARO under MURI; Chan Zuckerberg Biohub, JD.com; Amazon; Boeing; Docomo; Huawei; Hitachi; Observe; Siemens; UST Global; Moore Foundation; NXP; Xilinx; LETI-CEA; Intel; IBM; Microsoft; NEC; Toshiba; TSMC; ARM; BASF; Accenture; Ericsson; Qualcomm; Analog Devices; Okawa Foundation; American Family Insurance; Google Cloud; Swiss Re; TOTAL	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); IARPA; NSF(National Science Foundation (NSF)); Stanford Data Science Initiative; ARO under MURI(MURI); Chan Zuckerberg Biohub, JD.com; Amazon; Boeing; Docomo; Huawei(Huawei Technologies); Hitachi; Observe; Siemens(Siemens AG); UST Global; Moore Foundation(Gordon and Betty Moore Foundation); NXP; Xilinx; LETI-CEA; Intel(Intel Corporation); IBM(International Business Machines (IBM)); Microsoft(Microsoft); NEC; Toshiba; TSMC; ARM; BASF(BASF); Accenture; Ericsson(Ericsson); Qualcomm; Analog Devices; Okawa Foundation; American Family Insurance; Google Cloud(Google Incorporated); Swiss Re; TOTAL(Total SA)	Jure Leskovec is a Chan Zuckerberg Biohub investigator. This research has been supported in part by DARPA under FA865018C7880 (ASED), (MSC); NIH under No. U54EB020405 (Mobilize); ARO under MURI; IARPA under No. 2017-17071900005 (HFC), NSF under No. OAC-1835598 (CINES); Stanford Data Science Initiative, Chan Zuckerberg Biohub, JD.com, Amazon, Boeing, Docomo, Huawei, Hitachi, Observe, Siemens, and UST Global. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M), FA86501827865 (SDH), and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, TOTAL, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.	Adcock AB, 2013, IEEE DATA MINING, P1, DOI 10.1109/ICDM.2013.77; ANDERSON R M, 1991; Belkin M, 2002, ADV NEUR IN, V14, P585; Bonnabel S., 2013, IEEE T AUTOMATIC CON; Chamberlain BP, 2017, CORR MLG WORKSH 2017; Chen W, 2013, INTERNET MATH, V9, P434; Clauset A, 2008, NATURE, V453, P98, DOI 10.1038/nature06830; Dhingra Bhuwan, 2018, NAACL HLT; Frechet Maurice, 1948, ANN I H POINCARE; Ganea Octavian, 2018, ADV NEURAL INFORM PR, P5345; Ganea OE, 2018, PR MACH LEARN RES, V80; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Gu A., 2019, ICLR; Gulcehre Caglar, 2019, INT C LEARN REPR; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Jonckheere Edmond, 2008, J GRAPH THEORY; Khrulkov Valentin, 2019, IEEE C COMP VIS PATT; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Kleinberg Robert, 2007, IEEE INT C COMP COMM; Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Li Yujia, 2016, P INT C LEARN REPR I, P2; Liu Qi, 2019, NEURIPS, V1, P2; Namata Galileo, 2012, QUERY DRIVEN ACTIVE; Narayan O, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066108; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2017, ADV NEUR IN, V30; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Ravasz E, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.026112; Robbin Joel W, 2011, ETH LECT NOTES PRELI; Sala F, 2018, PR MACH LEARN RES, V80; Sarkar R., 2011, INT S GRAPH DRAW; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Szklarczyk D., 2016, NUCL ACIDS RES; Tay Y, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P583, DOI 10.1145/3159652.3159664; Tifrea Alexandru, 2019, ICLR; vandeLeemput Joyce, 2014, NEURON; Velickovic P., 2018, P INT C LEARN REPR; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wilson RC, 2014, IEEE T PATTERN ANAL, V36, P2255, DOI 10.1109/TPAMI.2014.2316836; Wu F, 2019, PR MACH LEARN RES, V97; Xu K, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337923; Yang Z, 2016, PR MACH LEARN RES, V48; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592; Zhang MH, 2018, ADV NEUR IN, V31; Zitnik M, 2019, P NATL ACAD SCI USA, V116, P4426, DOI 10.1073/pnas.1818013116	50	113	116	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID	32256024				2022-12-19	WOS:000534424304083
C	Piech, C; Bassen, J; Huang, J; Ganguli, S; Sahami, M; Guibas, L; Sohl-Dickstein, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Piech, Chris; Bassen, Jonathan; Huang, Jonathan; Ganguli, Surya; Sahami, Mehran; Guibas, Leonidas; Sohl-Dickstein, Jascha			Deep Knowledge Tracing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				TUTORS	Knowledge tracing-where a machine models the knowledge of a student as they interact with coursework-is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.	[Piech, Chris; Bassen, Jonathan; Huang, Jonathan; Ganguli, Surya; Sahami, Mehran; Guibas, Leonidas; Sohl-Dickstein, Jascha] Stanford Univ, Stanford, CA 94305 USA; [Sohl-Dickstein, Jascha] Khan Acad, Mountain View, CA USA; [Huang, Jonathan] Google, Mountain View, CA USA	Stanford University; Google Incorporated	Piech, C (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	piech@cs.stanford.edu; jbassen@cs.stanford.edu; jascha@stanford.edu			NSF-GRFP [DGE-114747]	NSF-GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD))	Many thanks to John Mitchell for his guidance and Khan Academy for its support. Chris Piech is supported by NSF-GRFP grant number DGE-114747.	Baker RSJD, 2008, LECT NOTES COMPUT SC, V5091, P406; Baker RSJD, 2011, LECT NOTES COMPUT SC, V6787, P13, DOI 10.1007/978-3-642-22362-4_2; Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718; Cen H, 2006, LECT NOTES COMPUT SC, V4053, P164; Cohen GL, 2008, CURR DIR PSYCHOL SCI, V17, P365, DOI 10.1111/j.1467-8721.2008.00607.x; Corbett A, 2001, LECT NOTES ARTIF INT, V2109, P137; CORBETT AT, 1994, USER MODEL USER-ADAP, V4, P253, DOI 10.1007/BF01099821; Drasgow F., 1990, HDB IND ORG PSYCHOL, P577; Elliot A.J., 2013, HDB COMPETENCE MOTIV; Feng MY, 2009, USER MODEL USER-ADAP, V19, P243, DOI 10.1007/s11257-009-9063-7; Fitch WT, 2005, COGNITION, V97, P179, DOI 10.1016/j.cognition.2005.02.005; GENTNER D., COGNITIVE SCI, V7, P2; GONG Y., INTELLIGENT TUTORING; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; HOCHREITER S., NEURAL COMPUTATION, V9, P8; Karpathy A., 2014, ARXIV14122306; Khajah M., 2014, P 7 INT C ED DAT MIN, DOI 10.1.1.661.8695; KHAJAH M. M., 2014, P 4 INT WORKSH PERS; Linnenbrink PR, 2004, MOTIVATION EMOTION C, P57; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Pardos ZA, 2011, LECT NOTES COMPUT SC, V6787, P243, DOI 10.1007/978-3-642-22362-4_21; Pavlik P.I., 2009, ONLINE SUBMISSION; PIECH C., 2015, ABS150505969 CORR; PIECH C., P 43 ACM S COMP SCI; Piech Chris, 2015, P 2015 2 ACM C LEARN, P195, DOI [10.1145/2724660.2724668, DOI 10.1145/2724660.2724668]; Polson M. C., 2013, FDN INTELLIGENT TUTO; Rafferty AN, 2011, LECT NOTES ARTIF INT, V6738, P280, DOI 10.1007/978-3-642-21869-9_37; Rohrer D, 2009, J RES MATH EDUC, V40, P4; Schraagen J.M., 2000, COGNITIVE TASK ANAL; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Yudelson Michael V., 2013, Artificial Intelligence in Education. Proceedings of 16th International Conference (AIED 2013): LNCS 7926, P171, DOI 10.1007/978-3-642-39112-5_18	32	112	112	11	19	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100022
C	Korbar, B; Tran, D; Torresani, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Korbar, Bruno; Du Tran; Torresani, Lorenzo			Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9% in action recognition accuracy on UCF101 and a boost of +17.7% on HMDB51.	[Korbar, Bruno; Torresani, Lorenzo] Dartmouth Coll, Hanover, NH 03755 USA; [Du Tran] Facebook Res, Menlo Pk, CA USA	Dartmouth College; Facebook Inc	Korbar, B (corresponding author), Dartmouth Coll, Hanover, NH 03755 USA.	bruno.18@dartmouth.edu; trandu@fb.com; LT@dartmouth.edu	Tran, Du/AAB-5973-2021		NSF [CNS-120552]	NSF(National Science Foundation (NSF))	This work was funded in part by NSF award CNS-120552. We gratefully acknowledge NVIDIA and Facebook for the donation of GPUs used for portions of this work. We would like to thank Relja Arandjelovic for discussions and for sharing informations regarding L<SUP>3</SUP>-Net, and members of the Visual Learning Group at Dartmouth for their feedback.	Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27; Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chung Joon Son, 2016, P AS C COMP VIS, P251, DOI [DOI 10.1007/978-3-319-54427-4_19, 10.1007/978-3-319-54427-4{_}19, DOI 10.1007/978-3-319-54427-4{_}19]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Feichtenhofer Christoph, 2016, NIPS; Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607; Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261; Gu Chunhui, 2017, ABS170508421 CORR; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Hang Zhao, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11205), P587, DOI 10.1007/978-3-030-01246-5_35; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kay W., 2017, ARXIV PREPRINT ARXIV; Koch G., 2015, ICML DEEP LEARNING W; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41; Le Q, 2011, P 28 INT C MACH LEAR, P265; Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496; Lee H, 2016, ADV NEURAL INFORM PR, V19; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48; Piczak KJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1015, DOI 10.1145/2733373.2806390; Ranzato Marc'Aurelio, 2007, 2007 IEEE COMP SOC C; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sailor HB, 2017, INTERSPEECH, P3107, DOI 10.21437/Interspeech.2017-831; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Soomro K., 2012, ARXIV; Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang XB, 2015, IEEE I CONF COMP VIS, P1787, DOI 10.1109/ICCV.2015.208; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhao Hang, 2017, ABS171209374 CORR	46	111	111	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002032
C	Frey, BJ; MacKay, DJC		Jordan, MI; Kearns, MJ; Solla, SA		Frey, BJ; MacKay, DJC			A revolution: Belief propagation in graphs with cycles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Until recently, artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief networks that have cycles. The probability propagation algorithm is only exact in networks that are cycle-free. However, it has recently been discovered that the two best error-correcting decoding algorithms are actually performing probability propagation in belief networks with cycles.	Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	University of Toronto	Frey, BJ (corresponding author), Univ Toronto, Dept Comp Sci, 100 Coll St, Toronto, ON, Canada.								0	111	115	0	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						479	485						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700068
C	Chen, CF; Li, O; Tao, CF; Barnett, AJ; Su, J; Rudin, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Chaofan; Li, Oscar; Tao, Chaofan; Barnett, Alina Jade; Su, Jonathan; Rudin, Cynthia			This Looks Like That: Deep Learning for Interpretable Image Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture - prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.	[Chen, Chaofan; Li, Oscar; Tao, Chaofan; Barnett, Alina Jade; Rudin, Cynthia] Duke Univ, Durham, NC 27706 USA; [Su, Jonathan] MIT, Lincoln Lab, Cambridge, MA 02139 USA	Duke University; Lincoln Laboratory; Massachusetts Institute of Technology (MIT)	Chen, CF (corresponding author), Duke Univ, Durham, NC 27706 USA.	cfchen@cs.duke.edu; oscarli@alumni.duke.edu; chaofan.tao@duke.edu; abarnett@cs.duke.edu; su@ll.mit.edu; cynthia@cs.duke.edu	Chen, Chaofan/AED-3261-2022	Chen, Chaofan/0000-0002-9250-5887	MIT Lincoln Laboratory	MIT Lincoln Laboratory	This work was sponsored in part by a grant from MIT Lincoln Laboratory to C. Rudin.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2016, ADV NEUR IN, V29; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495; Branson S, 2014, P BRIT MACH VIS C, DOI 10.5244/C.28.87; Chang J, 2018, WOODH PUBL SER BIOM, P119, DOI 10.1016/B978-0-08-100936-9.00008-3; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476; Ghiasi-Shirazi K., 2019, NEURAL PROCESSING LE; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Holt A, 2005, KNOWL ENG REV, V20, P289, DOI 10.1017/S0269888906000622; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jiang Y G, 2007, P 6 ACM INT C IM VID, P494, DOI DOI 10.1145/1282280.1282352; Kim B., 2014, ADV NEURAL INFORM PR, P1952; Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Li O, 2018, AAAI CONF ARTIF INTE, P3530; Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Ming Y, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P903, DOI 10.1145/3292500.3330908; Nalaie K, 2017, PROCEEDINGS OF THE 2017 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE), P211; Nister David, 2006, CVPR, P2161, DOI DOI 10.1109/CVPR.2006.264; Papernot N, 2018, ARXIV180304765; Priebe CE, 2003, J CLASSIF, V20, P3, DOI 10.1007/s00357-003-0003-7; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Salakhutdinov Ruslan, 2007, J MACHINE LEARNING R, P412, DOI DOI 10.1109/ICCV.2017.74; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136; Simonyan Karen, 2014, WORKSH 2 INT C LEARN, P2; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Smilkov D, 2017, ARXIV; Sundararajan M, 2017, PR MACH LEARN RES, V70; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; van den Oord A, 2016, PR MACH LEARN RES, V48; Wah C., 2011, TECH REP; Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wu C., 2017, ARXIV170108916; Xiao LX, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511399; Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685; Yosinski J., 2015, DEEP LEARN WORKSH 32; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920; Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319	57	110	110	4	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900051
C	Malinin, A; Gales, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Malinin, Andrey; Gales, Mark			Predictive Uncertainty Estimation via Prior Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models distributional uncertainty. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST and CIFAR-10 datasets, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.	[Malinin, Andrey; Gales, Mark] Univ Cambridge, Dept Engn, Cambridge, England	University of Cambridge	Malinin, A (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	am969@cam.ac.uk; mjfg@eng.cam.ac.uk	Malinin, Andrey/AAL-5041-2020		Cambridge Assessment, University of Cambridge; DTA EPSRC; Google Research award	Cambridge Assessment, University of Cambridge; DTA EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google Research award(Google Incorporated)	This paper reports on research partly supported by Cambridge Assessment, University of Cambridge. This work also partly funded by a DTA EPSRC away and a Google Research award. We would also like to thank members of the CUED Machine Learning group, especially Dr. Richard Turner, for fruitful discussions.	Abadi M, 2015, P 12 USENIX S OPERAT; Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300; Amodei D., 2016, CONCRETE PROBLEMS AI; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, CS231N TIN IMAGENET; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blundell C., 2015, P INT C MACH LEARN I; Buscema M, 1998, SUBST USE MISUSE, V33, P439, DOI 10.3109/10826089809115875; Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613; Depeweg S., 2017, ARXIV171007283; Dozat T., 2016, ICLR WORKSHOP; Gal Y., 2016, THESIS, V1, P3; Gal Y, 2016, PR MACH LEARN RES, V48; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I. J., 2013, ARXIV13126082; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hannun A.Y., 2014, ARXIV14125567, P1; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Hinton G., 2015, ARXIV150302531; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Kendall Alex, 2017, P C NEUR INF PROC SY; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lakshminarayanan B., 2017, P C NEUR INF PROC SY; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee Kimin, 2018, INT C LEARN REPR; Liang Shiyu, 2018, P INT C LEARN REPR, V1; Louizos C, 2016, PR MACH LEARN RES, V48; Mackay D.J.C., 1992, THESIS CALIFORNIA I; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Mikolov T., 2013, ARXIV; Mikolov T., 2013, P 2013 C N AM CHAPTE, P746, DOI DOI 10.3109/10826089109058901; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Murphy K P, 2012, MACHINE LEARNING; Neal Radford M, 1996, BAYESIAN LEARNING NE, V118, DOI [10.1007/978-1-4612-0745-0, DOI 10.1007/978-1-4612-0745-0]; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Villegas R, 2017, PR MACH LEARN RES, V70; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7	42	110	110	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001058
C	Schulman, J; Heess, N; Weber, T; Abbeel, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Schulman, John; Heess, Nicolas; Weber, Theophane; Abbeel, Pieter			Gradient Estimation Using Stochastic Computation Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				POLICY-GRADIENT	In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs-directed acyclic graphs that include both deterministic functions and conditional probability distributions-and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.	[Schulman, John; Heess, Nicolas; Weber, Theophane] Google DeepMind, London, England; [Schulman, John; Abbeel, Pieter] Univ Calif Berkeley, EECS Dept, Berkeley, CA 94720 USA	Google Incorporated; University of California System; University of California Berkeley	Schulman, J (corresponding author), Google DeepMind, London, England.	joschu@eecs.berkeley.edu; heess@google.com; theophane@google.com; pabbeel@eecs.berkeley.edu						Bengio Yoshua, 2013, ARXIV13083432; Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4; Glasserman P., 2003, MONTE CARLO METHODS, V53; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Gregor K., 2013, ARXIV PREPRINT ARXIV, V2; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Diederik P, 2014, INT C MACH LEARN ICM; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Martens J., 2010, P 27 INT C MACH LEAR, P735; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mnih V, 2014, ADV NEUR IN, V27; Munos R, 2006, J MACH LEARN RES, V7, P771; Neal R. M., 1990, LEARNING STOCHASTIC, V64, P1577; Pearl Judea, 2014, PROBABILISTIC REASON; Ranganath R., 2013, ARXIV14010118; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Silver D, 2014, PR MACH LEARN RES, V32; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Vlassis N, 2009, AUTON ROBOT, V27, P123, DOI 10.1007/s10514-009-9132-0; Wierstra D, 2010, LOG J IGPL, V18, P620, DOI 10.1093/jigpal/jzp049; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wingate D., 2013, ARXIV13011299; Wright S., 1999, NUMERICAL OPTIMIZATI, V2; Zaremba Wojciech, 2015, ARXIV150500521	29	110	111	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102048
C	Bach, FR; Jordan, MI		Thrun, S; Saul, K; Scholkopf, B		Bach, FR; Jordan, MI			Learning spectral clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors.	Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Bach, FR (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					[Anonymous], [No title captured]; [Anonymous], [No title captured]; BACH FR, LEARNING SPECTRAL CL; BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016; CHAN PK, 1994, IEEE T COMPUT AID D, V13, P1088, DOI 10.1109/43.310898; CRISTIANINI N, 2002, NIPS, V14; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gu M., 2001, SPECTRAL RELAXATION; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Kamvar S. D., 2003, IJCAI; Martin D. R., 2001, ICCV; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Wagstaff K., 2001, ICML; Xing E. P., 2003, NIPS, V15; YU SX, 2002, NIPS, V14; ZHA H, 2002, NIPS, V14	16	110	112	1	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						305	312						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500039
C	Siarohin, A; Lathuiliere, S; Tulyakov, S; Ricci, E; Sebe, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Siarohin, Aliaksandr; Lathuiliere, Stephane; Tulyakov, Sergey; Ricci, Elisa; Sebe, Nicu			First Order Motion Model for Image Animation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly available(1).	[Siarohin, Aliaksandr; Lathuiliere, Stephane; Ricci, Elisa; Sebe, Nicu] Univ Trento, DISI, Trento, Italy; [Lathuiliere, Stephane] Inst Polytech Paris, Telecom Paris, LTCI, Palaiseau, France; [Tulyakov, Sergey] Snap Inc, Santa Monica, CA USA; [Ricci, Elisa] Fdn Bruno Kessler, Povo, Italy; [Sebe, Nicu] Huawei Technol Ireland, Dublin, Ireland	University of Trento; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Fondazione Bruno Kessler; Huawei Technologies	Siarohin, A (corresponding author), Univ Trento, DISI, Trento, Italy.	aliaksandr.siarohin@unitn.it; stephane.lathuilire@telecom-paris.fr; stulyakov@snap.com; e.ricci@unitn.it; niculae.sebe@unitn.it		Sebe, Niculae/0000-0002-6597-7248; Ricci, Elisa/0000-0002-0228-1147				Amersfoort J.V., 2017, ABS170108435 CORR; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2016, NIPS; [Anonymous], 2017, ICCV; [Anonymous], 2014, ICLR; Babaeizadeh Mohammad, 2017, ICLR; Balakrishnan Guha, 2018, CVPR; Bansal A., 2018, EUR C COMP VIS ECCV; Blanz V., 1999, SIGGRAPH; Bulat A., 2017, ICCV; Cao C., 2014, TOG; Cao Z., 2017, P IEEE C COMP VIS PA; Chan Caroline, 2018, ECCV; Dibeklioglu Hamdi, 2012, ECCV; Ebert F., 2017, CORL; Esser P., 2018, CVPR; Finn Chelsea, 2016, NIPS; Geng Z., 2019, CVPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grigorev Artur, 2019, CVPR; Isola P., 2017, CVPR; Jakab T., 2018, NIPS; Johnson J, 2016, ECCV; Liu Yahui, 2019, ACM MM; Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950; Neumann U., 2018, CVPR; Oh J., 2015, NIPS; Robinson J. P., 2019, ICCV; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Shysheya Aliaksandra, 2019, CVPR; Siarohin A., 2018, CVPR; Siarohin Aliaksandr, 2019, CVPR, P2; Srivastava Nitish, 2015, ICML; Tang Hao, 2018, ACCV; Tang Hao, 2018, ACM MM; Thies J., 2016, CVPR; Tulyakov S., 2018, CVPR; Wang Ting-Chun, 2018, NIPS; Wang Z., 2003, ACSSC; Wiles O., 2018, ECCV; Zablotskaia Polina, 2019, ARXIV191009139; Zakharov Egor, 2019, ICCV; Zhang Y., 2018, CVPR; Zhao Long, 2018, ECCV; Zollhofer Michael, 2018, COMPUTER GRAPHICS FO	45	109	111	3	22	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307018
C	Lanctot, M; Zambaldi, V; Gruslys, A; Lazaridou, A; Tuyls, K; Perolat, J; Silver, D; Graepel, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lanctot, Marc; Zambaldi, Vinicius; Gruslys, Audrunas; Lazaridou, Angeliki; Tuyls, Karl; Perolat, Julien; Silver, David; Graepel, Thore			A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LEARNERS; BEHAVIOR; GO	To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.	[Lanctot, Marc; Zambaldi, Vinicius; Gruslys, Audrunas; Lazaridou, Angeliki; Tuyls, Karl; Perolat, Julien; Silver, David; Graepel, Thore] DeepMind, London, England		Lanctot, M (corresponding author), DeepMind, London, England.	lanctot@google.com; vzambaldi@google.com; audrunas@google.com; angeliki@google.com; karltuyls@google.com; perolat@google.com; davidsilver@google.com; thore@google.com	Jeong, Yongwook/N-7413-2016; Tuyls, Karl P/Q-7328-2018	Tuyls, Karl P/0000-0001-7929-1944				Al-Shedivat M., 2017, ARXIV PREPRINT ARXIV; Amato C, 2015, AAAI CONF ARTIF INTE, P1995; [Anonymous], 2003, ICML; [Anonymous], 2011, IJCAI, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-054; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bansal T, 2017, CORR; Barreto Andre, 2017, P 31 ANN C NEUR INF; Bloembergen D, 2015, J ARTIF INTELL RES, V53, P659, DOI 10.1613/jair.4818; Blum A, 2007, ALGORITHMIC GAME THEORY, P79; Bosansky B, 2016, ARTIF INTELL, V237, P1, DOI 10.1016/j.artint.2016.03.005; Bosansky Branislav, 2013, P 23 INT JOINT C ART; Bowling M, 2002, ARTIF INTELL, V136, P215, DOI 10.1016/S0004-3702(02)00121-2; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Brown N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P7; Brown Noam, 2017, CORR; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Burch N., 2007, P 27 ART INT AAAI 07; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Camerer Colin F., 2004, Q J EC; Costa-Gomes MA, 2006, AM ECON REV, V96, P1737, DOI 10.1257/aer.96.5.1737; Finn C, 2017, PR MACH LEARN RES, V70; Foerster J, 2017, PR MACH LEARN RES, V70; Ganzfried S., 2015, ACM T EC COMPUTATION, V3, P1; Gatti N, 2013, P 27 AAAI C ART INT, P335; Gibson Richard, 2013, CORR; Gilpin A., 2009, THESIS; Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579; Greenwald Amy, 2017, OUTSTANDING CONTRIBU, V11, P185; Gruslys Audrunas, 2017, CORR; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hart Sergiu, 2001, EC ESSAYS FESTSCHRIF; Hartford Jason S., 2016, P 30 C NEUR INF PROC; Hausknecht M. J., 2016, P INT C LEARNING REP, P1; Hausknecht M. J., 2016, THESIS; He H, 2016, PR MACH LEARN RES, V48; Heinrich J, 2015, PR MACH LEARN RES, V37, P805; Heinrich Johannes, 2016, CORR; Hoang T. N., 2013, P 23 INT JOINT C ART, P2298; Hofbauer J, 2002, ECONOMETRICA, V70, P2265, DOI 10.1111/j.1468-0262.2002.00440.x; Johanson M., 2008, NEURAL INFORM PROCES, V20, P1128; Johanson M., 2013, P 12 INT C AUT AG MU; Johanson M. B., 2016, THESIS; Kaisers M., 2010, P 9 INT C AUTONOMOUS, V1, P309; Kingma D.P, P 3 INT C LEARNING R; Kleiman-Weiner M., 2016, P 38 ANN C COGNITIVE; Koller D., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P750, DOI 10.1145/195058.195451; Kouvaris Je Kostas, 2017, PLOS COMPUTATIONAL B, P1; Kuhn HW., 1953, CONTRIBUTIONS THEORY, P193; Lanctot M, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1257; Lanctot Marc, 2017, CORR; Lauer M., 2004, P AAMAS 04 NEW YORK; Laurent GJ, 2011, INT J KNOWL-BASED IN, V15, P55, DOI 10.3233/KES-2010-0206; Lazaridou A., 2017, P INT C LEARN REPR I; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Leibo J.Z., 2017, P 16 INT C AUT AG MU; Leslie DS, 2006, GAME ECON BEHAV, V56, P285, DOI 10.1016/j.geb.2005.08.005; Littman ML, 2015, NATURE, V521, P445, DOI 10.1038/nature14540; Littman ML, 1994, ICML 1994, P157; Long J, 2010, AAAI CONF ARTIF INTE, P134; Lowe R., 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295385; Marecki Janusz, 2008, P 7 INT JOINT C AUT; Marivate Vukosi N., 2015, THESIS; Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057; McMahan H. B., 2003, P 20 INT C MACH LEAR; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moravcik M., 2017, SCIENCE, V358; Nair R., 2004, THESIS; Oliehoek F.A., 2016, CONCISE INTRO DECENT; Oliehoek F. A., 2006, P GEN EV COMP C GECC; Omidshafiei S, 2017, PR MACH LEARN RES, V70; Panait L, 2008, J MACH LEARN RES, V9, P423; Parkes DC, 2015, SCIENCE, V349, P267, DOI 10.1126/science.aaa8403; Ponsen Marc, 2009, ENTERTAINMENT COMPUT; Sailer F., 2007, IEEE S COMP INT GAM, P37; Samothrakis Spyridon, 2013, IEEE T EVOLUTIONARY; Schmid M, 2014, AAAI CONF ARTIF INTE, P784; Schvartzman L. Julian, 2009, P 8 INT C AUT AG MUL, P249; Shang WL, 2016, PR MACH LEARN RES, V48; Shoham Y., 2009, MULTIAGENT SYSTEMS A; Shoham Y, 2007, ARTIF INTELL, V171, P365, DOI 10.1016/j.artint.2006.02.006; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sukhbaatar S., 2016, 30 C NEUR INF PROC S; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395; Tavares Anderson, 2016, 12 AAAI C ART INT IN; TAYLOR PD, 1978, MATH BIOSCI, V40, P145, DOI 10.1016/0025-5564(78)90077-9; Tenenbaum, 2011, P 32 ANN C COGN SCI, V33, P2469, DOI DOI 10.1093/BRAIN/AWW287; Tuyls K., 2008, AGENTS SIMULATION AP, P218; Tuyls K, 2012, AI MAG, V33, P41, DOI 10.1609/aimag.v33i3.2426; Walsh W. E., 2002, AAAI 02 WORKSH GAM T; Wellman Michael P., 2006, P NAT C ART INT AAAI; Whiteson S, 2016, 30 C NEUR INF PROC S; Whiteson S., 2011, 2011 IEEE S AD DYN P, P120, DOI DOI 10.1109/ADPRL.2011.5967363; Wright JR, 2010, AAAI CONF ARTIF INTE, P901; Wright Mason, 2016, CORR; Yakovenko Nikolai, 2016, P 30 AAAI C ART INT; Yoshida W, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000254; Zinkevich M., 2008, ADV NEURAL INFORM PR, P1729	107	109	113	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404026
C	Vincent, P; Bengio, Y		Dietterich, TG; Becker, S; Ghahramani, Z		Vincent, P; Bengio, Y			K-Local hyperplane and convex distance nearest neighbor algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classification tasks. We then propose modified K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classification tasks suggest that the modified KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs.	Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Vincent, P (corresponding author), Univ Montreal, Dept IRO, CP 6128, Montreal, PQ H3C 3J7, Canada.	vincentp@iro.umontreal.ca; bengioy@iro.umontreal.ca						ATKESON C, 1996, ARTIFICIAL INTELLIGE; BOSER B, 1992, P 5 ANN WORKSH COMP, V5, P144; BOTTOU L, 1992, NEURAL COMPUT, V4, P888, DOI 10.1162/neco.1992.4.6.888; Chapelle O, 2001, ADV NEUR IN, V13, P416; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Friedman JH, 1994, TECH REP; Hastie T, 1996, ADV NEUR IN, V8, P409; Li SZ, 1999, IEEE T NEURAL NETWOR, V10, P439, DOI 10.1109/72.750575; MYLES JP, 1990, PATTERN RECOGN, V23, P1291, DOI 10.1016/0031-3203(90)90123-3; ORMONEIT D, 2000, ADV NEURAL INFORMATI, V12; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; SHORT RD, 1981, IEEE T INFORM THEORY, V27, P622, DOI 10.1109/TIT.1981.1056403; Simard P. Y., 1998, LECT NOTES COMPUTER, V1524; Tong S, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P658; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; ZHANG B, 2001, HPL200189	16	109	114	0	5	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						985	992						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100123
C	Liu, ZJ; Tang, HT; Lin, YJ; Han, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Zhijian; Tang, Haotian; Lin, Yujun; Han, Song			Point-Voxel CNN for Efficient 3D Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80% of the time is wasted on structuring the sparse data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular, sparse data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves a much higher accuracy than the voxel-based baseline with 10X GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7X measured speedup on average. Remarkably, the narrower version of PVCNN achieves 2X speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by up to 2.4% mAP with 1.8X measured speedup and 1.4X GPU memory reduction.	[Liu, Zhijian; Lin, Yujun; Han, Song] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Tang, Haotian] Shanghai Jiao Tong Univ, Shanghai, Peoples R China	Massachusetts Institute of Technology (MIT); Shanghai Jiao Tong University	Liu, ZJ (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.		Han, Song/AAR-9464-2020; Liu, Zhijian/ABF-4061-2020; Lin, Yujun/AAR-9588-2020	Han, Song/0000-0002-4186-7618; Liu, Zhijian/0000-0003-3632-9986; Lin, Yujun/0000-0001-6314-1722	MIT Quest for Intelligence; MIT-IBM Watson AI Lab; Samsung; Facebook; SONY	MIT Quest for Intelligence; MIT-IBM Watson AI Lab(International Business Machines (IBM)); Samsung(Samsung); Facebook(Facebook Inc); SONY	We thank MIT Quest for Intelligence, MIT-IBM Watson AI Lab, Samsung, Facebook and SONY for supporting this research. We also thank AWS Machine Learning Research Awards for providing the computation resource and NVIDIA for donating the Jetson AGX Xavier.	Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170; Chang Angel X., 2015, ARXIV151203012CSGR P; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; He Yihui, 2018, ECCV; Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323; Howard A.G., 2017, MOBILENETS EFFICIENT; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Lan S., 2019, CVPR; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298; Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Lin D.D., 2016, ICLR; Ma Ningning, 2018, P EUR C COMP VIS ECC; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Maturana Daniel, 2015, IROS; Mutlu Onur, DDR ACCESS ILLUSTRAT; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qi Charles R, 2017, ARXIV170602413; Qi Charles Ruizhongtai, 2018, CVPR; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ronneberger O, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Savarese S., 2017, JOINT 2D 3D SEMANTIC; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067; Wang K, 2019, PROC CVPR IEEE, P8604, DOI 10.1109/CVPR.2019.00881; Wang PS, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275050; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang Y, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317024; Wang Zongji, 2019, TVCG; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhou Aojun, 2017, ICLR; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	51	108	110	6	21	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301001
C	Golan, I; El-Yaniv, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Golan, Izhak; El-Yaniv, Ran			Deep Anomaly Detection Using Geometric Transformations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of anomaly detection in images, and present a new detection technique. Given a sample of images, all known to belong to a "normal" class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects). The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images. The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images. We present extensive experiments using the proposed detector, which indicate that our technique consistently improves all known algorithms by a wide margin.	[Golan, Izhak; El-Yaniv, Ran] Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel	Technion Israel Institute of Technology	Golan, I (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel.	izikgo@cs.technion.ac.il; rani@cs.technion.ac.il			Israel Science Foundation [710/18]	Israel Science Foundation(Israel Science Foundation)	This research was partially supported by the Israel Science Foundation (grant No. 710/18).	An J., 2015, SPEC LECT, V2, P1, DOI DOI 10.1007/BF00758335; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Burnaev E, 2015, PROC SPIE, V9875, DOI 10.1117/12.2228523; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Davis J., 2006, P 23 INT C MACH LEAR, V148, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]; Deecke Lucas, 2018, ANOMALY DETECTION GE; El-Yaniv R., 2010, ABS10104466 CORR; El-Yaniv R, 2007, LECT NOTES COMPUT SC, V4539, P157, DOI 10.1007/978-3-540-72927-3_13; El-Yaniv Ran, 2007, P ADV NEURAL INF PRO, V19, P377, DOI 10.7551/mitpress/7503.003.0052; Elson J, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P366; Geifman Y., 2017, ARXIV171100941; Geifman Y., 2018, CORR; Geifman Y., 2018, ABS181107579 CORR; Geifman Y, 2017, ADV NEUR IN, V30; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Iwata T, 2016, ADV NEUR IN, V29; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Kim J, 2012, J MACH LEARN RES, V13, P2529; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; LeCun Y., 2010, MNIST HANDWRITTEN DI; Liang Shiyu, 2018, INT C LEARN REPR; Minka TP, 2000, ESTIMATING DIRICHLET; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ruff L, 2018, PR MACH LEARN RES, V80; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Scholkopf B, 2000, ADV NEUR IN, V12, P582; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Taylor A, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P130, DOI 10.1109/DSAA.2016.20; Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142; Wang SQ, 2018, PATTERN RECOGN, V74, P198, DOI 10.1016/j.patcog.2017.09.012; Wicker N, 2008, COMPUT STAT DATA AN, V52, P1315, DOI 10.1016/j.csda.2007.07.011; Wiener Y., 2012, ADV NEURAL INFORM PR, P2051; Wiener Y., 2011, ADV NEURAL INFORM PR, P1665; Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177; Xiao H., 2017, FASHION MNIST NOVEL; Yosinski J., 2015, ICML DEEP LEARN WORK; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhai SF, 2016, PR MACH LEARN RES, V48; Zimek Arthur, 2012, Statistical Analysis and Data Mining, V5, P363, DOI 10.1002/sam.11161; Zong B., INT C LEARNING REPRE	43	108	110	4	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004033
C	Zhao, H; Zhang, SH; Wu, GH; Costeira, JP; Moura, JMF; Gordon, GJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhao, Han; Zhang, Shanghang; Wu, Guanhang; Costeira, Joao P.; Moura, Jose M. F.; Gordon, Geoffrey J.			Adversarial Multiple Source Domain Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					While domain adaptation has been actively researched, most algorithms focus on the single-source-single-target adaptation setting. In this paper we propose new generalization bounds and algorithms under both classification and regression settings for unsupervised multiple source domain adaptation. Our theoretical analysis naturally leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose multisource domain adversarial networks (MDAN) that approach domain adaptation by optimizing task-adaptive generalization bounds. To demonstrate the effectiveness of MDAN, we conduct extensive experiments showing superior adaptation performance on both classification and regression problems: sentiment analysis, digit classification, and vehicle counting.	[Zhao, Han; Zhang, Shanghang; Wu, Guanhang; Moura, Jose M. F.; Gordon, Geoffrey J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zhang, Shanghang; Costeira, Joao P.] Univ Lisbon, IST, Lisbon, Portugal	Carnegie Mellon University; Universidade de Lisboa; Instituto Superior Tecnico	Zhao, H (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	hzhao1@andrew.cmu.edu; shanghaz@andrew.cmu.edu; guanhanw@andrew.cmu.edu; jpc@isr.ist.utl.pt; moura@andrew.cmu.edu; ggordon@andrew.cmu.edu	Zhang, Lisa/AAW-9795-2021; Costeira, Joao P/D-8157-2013	Costeira, Joao P/0000-0001-6769-2935	ONR [N000141512365]; Fundacao para a Ciencia e a Tecnologia (project FCT) [SFRH/BD/113729/2015]; Fundacao para a Ciencia e a Tecnologia (Carnegie Mellon-Portugal program)	ONR(Office of Naval Research); Fundacao para a Ciencia e a Tecnologia (project FCT)(Portuguese Foundation for Science and Technology); Fundacao para a Ciencia e a Tecnologia (Carnegie Mellon-Portugal program)(Portuguese Foundation for Science and Technology)	HZ and GG gratefully acknowledge support from ONR, award number N000141512365. This research was supported in part by Fundacao para a Ciencia e a Tecnologia (project FCT [SFRH/BD/113729/2015] and a grant from the Carnegie Mellon-Portugal program). HZ would also like to thank Remi Tachet for his valuable comments and suggestions.	Adel T, 2017, AAAI CONF ARTIF INTE, P1691; Ajakan H., 2014, ABS14124446 CORR; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; Becker Carlos J, 2013, ADV NEURAL INFORM PR, V1, P485; Ben-David S., 2007, ADV NEURAL INFORM PR, V19, P137; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blitzer J., 2007, ADV NEURAL INFORM PR, V20, P129; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Chen M., 2012, ARXIV12064683; Cortes C, 2008, LECT NOTES ARTIF INT, V5254, P38, DOI 10.1007/978-3-540-87987-9_8; Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027; Donahue J, 2014, PR MACH LEARN RES, V32; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17; Ganin Y., 2016, JMLR, V17, P2096; Germain P., 2013, INT C MACH LEARN, P738; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Gopalan R, 2014, IEEE T PATTERN ANAL, V36, P2288, DOI 10.1109/TPAMI.2013.249; Hoffman, 2017, ARXIV171103213; Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50; Hoffman Judy, 2017, ARXIV171105037; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Kifer D., 2004, P 30 INT C VER LARG, V30, P180; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Long MS, 2015, PR MACH LEARN RES, V37, P97; Louizos C., 2015, ARXIV PREPRINT ARXIV; Mansour Y., 2009, P 25 C UNC ART INT, P367; Mansour Yishay, 2009, COLT 2009 22 C LEARN; Mansour Yishay, 2012, ISAIM; Mohri M., 2018, FDN MACHINE LEARNING; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pei Zhongyi, 2018, MULTIADVERSARIAL DOM; Rostamizadeh A., 2009, ADV NEURAL INFORM PR, P1041; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sha, 2013, P INT C MACH LEARN; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Shu Rui, 2018, P 6 INT C LEARN REPR, P2; Sun Qian, 2011, ADV NEURAL INFORM PR, P505; Tsuboi Y., 2009, J INF PROCESS, V17, P138, DOI DOI 10.2197/IPSJJIP.17.138; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang K, 2015, AAAI CONF ARTIF INTE, P3150; Zhang SH, 2017, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2017.454	52	108	109	4	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003015
C	Vigario, R; Jousmaki, V; Hamalainen, M; Hari, R; Oja, E		Jordan, MI; Kearns, MJ; Solla, SA		Vigario, R; Jousmaki, V; Hamalainen, M; Hari, R; Oja, E			Independent component analysis for identification of artifacts in magnetoencephalographic recordings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We have studied the application of an independent component analysis (ICA) approach to the identification and possible removal of artifacts from a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude distributions over time, thus distinguishing between strictly periodical signals, and regularly and irregularly occurring signals. Many artifacts belong to the last category. In order to assess the effectiveness of the method, controlled artifacts were produced, which included saccadic eye movements and blinks, increased muscular tension due to biting and the presence of a digital watch inside the magnetically shielded room. The results demonstrate the capability of the method to identify and clearly isolate the produced artifacts.	Helsinki Univ Technol, Lab Comp & Info Sci, FIN-02015 HUT, Finland	Aalto University	Vigario, R (corresponding author), Helsinki Univ Technol, Lab Comp & Info Sci, POB 2200, FIN-02015 HUT, Finland.		Vigário, Ricardo/AAX-9822-2021; Hari, Riitta K/J-1880-2012; Jousmäki, Veikko/H-6008-2012; Vigário, Ricardo/C-4641-2018	Vigário, Ricardo/0000-0003-0950-6035; Hari, Riitta K/0000-0002-3142-2703; Jousmäki, Veikko/0000-0003-1963-5834; Hamalainen, Matti/0000-0001-6841-112X					0	108	113	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						229	235						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700033
C	Ghahramani, Z; Roweis, ST		Kearns, MS; Solla, SA; Cohn, DA		Ghahramani, Z; Roweis, ST			Learning nonlinear dynamical systems using an EM algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					The Expectation-Maximization (EM) algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables [2]. It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9]. We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems. The "expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the "maximization" step re-estimates the parameters using these uncertain state estimates. In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states. However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.	Univ Coll London, Gatsby Comp Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Ghahramani, Z (corresponding author), Univ Coll London, Gatsby Comp Neurosci Unit, London WC1N 3AR, England.							DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Jordan M. I., 1999, MACHINE LEARNING; Kalman R.E., 1961, J BASIC ENG-T ASME, V83, P95, DOI [10.1115/1.3658902, DOI 10.1115/1.3658902]; LJUNG L, 1983, THEORY PRACTISE RECU; Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281; Neal R.M., 1993, PROBABILISTIC INFERE; RAUCH HE, 1963, IEEE T AUTOMAT CONTR, VAC 8, P371, DOI 10.1109/TAC.1963.1105600; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x	8	107	107	1	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						431	437						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700061
C	Wang, TC; Liu, MY; Zhu, JY; Liu, GL; Tao, A; Kautz, J; Catanzaro, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Ting-Chun; Liu, Ming-Yu; Zhu, Jun-Yan; Liu, Guilin; Tao, Andrew; Kautz, Jan; Catanzaro, Bryan			Video-to-Video Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website.	[Wang, Ting-Chun; Liu, Ming-Yu; Liu, Guilin; Tao, Andrew; Kautz, Jan; Catanzaro, Bryan] NVIDIA, Santa Clara, CA 95051 USA; [Zhu, Jun-Yan] MIT, CSAIL, Cambridge, MA 02139 USA	Nvidia Corporation; Massachusetts Institute of Technology (MIT)	Wang, TC (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.	tingchunw@nvidia.com; mingyul@nvidia.com; junyanz@mit.edu; guilinl@nvidia.com; atao@nvidia.com; jkautz@nvidia.com; bcatanzaro@nvidia.com	Wang, Ting-Chun/AAZ-2408-2020; Wang, Ting-Chun/AAD-4410-2021	Wang, Ting-Chun/0000-0002-1522-2381; 				Aberman Kfir, 2018, ARXIV180806847; Arulkumaran K, 2017, IEEE SIGNAL PROC MAG, V34, P26, DOI 10.1109/MSP.2017.2743240; Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376; Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964; Bitouk D., 2008, ACM SIGGRAPH; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chan C., 2018, EUR C COMP VIS ECCV; Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen T, 2013, IEEE T IMAGE PROCESS, V22, P2532, DOI 10.1109/TIP.2013.2251642; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Denton Emily L, 2015, NEURIPS, V2, P4; Dosovitskiy Alexey, 2016, NEURIPS; Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923; Ghosh A, 2018, PROC CVPR IEEE, P8513, DOI 10.1109/CVPR.2018.00888; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Gulrajani I, 2017, P NIPS 2017; Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745; Huang XY, 2018, IEEE COMPUT SOC CONF, P1067, DOI 10.1109/CVPRW.2018.00141; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Huang Xun, 2018, ECCV; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kajiya J. T., 1986, ACM SIGGRAPH; Kalchbrenner N, 2016, ARXIV161000527; King DE, 2009, J MACH LEARN RES, V10, P1755; Kingma D.P, P 3 INT C LEARNING R; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lee Alex X, 2018, ARXIV180401523; Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Ma LQ, 2017, ADV NEUR IN, V30; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mathieu Michael, 2016, ICLR; Odena A, 2017, PR MACH LEARN RES, V70; Ohnishi K, 2018, AAAI CONF ARTIF INTE, P2387; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Rossler Andreas, 2018, FACEFORENSICS LARGE, V2, P19; Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Schodl A., 2000, ACM T GRAPHICS TOG; Shechtman E, 2005, IEEE T PATTERN ANAL, V27, P531, DOI 10.1109/TPAMI.2005.85; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319; Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361; Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wexler Y., 2004, IEEE C COMP VIS PATT; Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Yang CY, 2018, LECT NOTES COMPUT SC, V11214, P204, DOI 10.1007/978-3-030-01249-6_13; Yang S., 2016, ARXIV160801250; Zhang H., 2017, ICCV; Zheng Zhaoheng, 2017, [Computational Visual Media, 计算可视媒体], V3, P337; Zhu Jun-Yan, 2017, ICCV	81	106	106	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301016
C	Kiryo, R; Niu, G; du Plessis, MC; Sugiyama, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kiryo, Ryuichi; Niu, Gang; du Plessis, Marthinus C.; Sugiyama, Masashi			Positive-Unlabeled Learning with Non-Negative Risk Estimator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning, in which the state of the art is unbiased PU learning. However, if its model is very flexible, empirical risks on training data will go negative, and we will suffer from serious overfitting. In this paper, we propose a non-negative risk estimator for PU learning: when getting minimized, it is more robust against overfitting, and thus we are able to use very flexible models (such as deep neural networks) given limited P data. Moreover, we analyze the bias, consistency, and mean-squared-error reduction of the proposed risk estimator, and bound the estimation error of the resulting empirical risk minimizer. Experiments demonstrate that our risk estimator fixes the overfitting problem of its unbiased counterparts.	[Kiryo, Ryuichi; Niu, Gang; Sugiyama, Masashi] Univ Tokyo, 7-3-1 Hongo, Tokyo 1130033, Japan; [Kiryo, Ryuichi; Niu, Gang; Sugiyama, Masashi] RIKEN, 1-4-1 Nihonbashi, Tokyo 1030027, Japan	University of Tokyo; RIKEN	Kiryo, R (corresponding author), Univ Tokyo, 7-3-1 Hongo, Tokyo 1130033, Japan.; Kiryo, R (corresponding author), RIKEN, 1-4-1 Nihonbashi, Tokyo 1030027, Japan.	kiryo@ms.k.u-tokyo.ac.jp; gang@ms.k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp	Jeong, Yongwook/N-7413-2016; Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	JST CREST [JPMJCR1403]; Microsoft Research Asia	JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); Microsoft Research Asia(Microsoft)	GN and MS were supported by JST CREST JPMJCR1403 and GN was also partially supported by Microsoft Research Asia.	[Anonymous], 2003, IJCAI; [Anonymous], NIPS; [Anonymous], 2002, ICML; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Chung K. L., 1968, COURSE PROBABILITY T; DeComite F., 1999, ALT; Denis F., 1998, ALT; du Plessis M. C., 2015, ICML; du Plessis MC, 2017, MACH LEARN, V106, P463, DOI 10.1007/s10994-016-5604-6; Duchi J, 2011, J MACH LEARN RES, V12, P2121; ELKAN C, 2008, KDD; Glorot X., 2010, P 13 INT C ART INT S, VVolume 9, P249; Hsieh C. - J., 2015, ICML; Ioffe S., 2015, P 32 INT C INT C MAC, V37, P448; Jain S., 2016, NIPS; James W., 1961, P 4 BERK S MATH STAT; Kingma D.P., 2015, INT C LEARN REPR, P1; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lang K., 1995, ICML; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Lee W.S., 2003, ICML; Letouzey F., 2000, ALT; Li X., 2009, SDM; Liu B., 2003, ICDM; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Menon A., 2015, ICML; Mohri M., 2018, FDN MACHINE LEARNING; Nair V., 2010, ICML; Natarajan N., 2013, NIPS; Nguyen M. N., 2011, IJCAI; Niu  G., 2016, NIPS; Patrini G., 2016, ICML; Pennington Jeffrey, 2014, EMNLP; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Ramaswamy H., 2016, ICML; Sakai T., 2017, ICML; Sansone E., 2016, ARXIV160806807; Scott  C., 2009, AISTATS; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Springenberg J.T., 2015, P ICLR WORKSH TRACK, P1; Stein C., 1956, CONTRIBUTIONS THEORY, V1; Tokui S., 2015, MACH LEARN SYST WORK; Vapnik V.N, 1998, STAT LEARNING THEORY; Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x; Yuan GX, 2012, J MACH LEARN RES, V13, P1999; Yuille A. L., 2001, NIPS	50	106	111	2	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401069
C	Lee, TW; Bell, AJ; Lambert, RH		Mozer, MC; Jordan, MI; Petsche, T		Lee, TW; Bell, AJ; Lambert, RH			Blind separation of delayed and convolved sources.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We address the difficult problem of separating multiple speakers with multiple microphones in a real room. We combine the work of Torkkola and Amari, Cichocki and Yang, to give Natural Gradient information maximisation rules for recurrent (IIR) networks, blindly adjusting delays, separating and deconvolving mixed signals. While they work well on simulated data, these rules fail in real rooms which usually involve non-mini mum phase transfer functions, not-invertible using stable lm filters. An approach that sidesteps this problem is to perform infomax on a feedforward architecture in the frequency domain (Lambert 1996). We demonstrate real-room separation of two natural signals using this approach.			Lee, TW (corresponding author), CARNEGIE MELLON UNIV,INTERACT SYST GRP,PITTSBURGH,PA 15213, USA.								0	105	115	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						758	764						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00107
C	Chen, JX; Yang, L; Zhang, YZ; Alber, M; Chen, DZ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Jianxu; Yang, Lin; Zhang, Yizhe; Alber, Mark; Chen, Danny Z.			Combining Fully Convolutional and Recurrent Neural Networks for 3D Biomedical Image Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Segmentation of 3D images is a fundamental problem in biomedical image analysis. Deep learning (DL) approaches have achieved state-of-the-art segmentation performance. To exploit the 3D contexts using neural networks, known DL segmentation methods, including 3D convolution, 2D convolution on planes orthogonal to 2D image slices, and LSTM in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3D biomedical images. In this paper, we propose a new DL framework for 3D image segmentation, based on a combination of a fully convolutional network (FCN) and a recurrent neural network (RNN), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively. To our best knowledge, this is the first DL framework for 3D image segmentation that explicitly leverages 3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal Structure Segmentation Challenge and in-house image stacks for 3D fungus segmentation, our approach achieves promising results comparing to the known DL-based 3D segmentation approaches.	[Chen, Jianxu; Yang, Lin; Zhang, Yizhe; Alber, Mark; Chen, Danny Z.] Univ Notre Dame, Notre Dame, IN 46556 USA	University of Notre Dame	Chen, JX (corresponding author), Univ Notre Dame, Notre Dame, IN 46556 USA.	jchen16@nd.edu; lyang5@nd.edu; yzhang29@nd.edu; malber@nd.edu; dchen@nd.edu	ray, jay/GYV-3810-2022; Alber, Mark/AAR-1280-2020	Chen, Jianxu/0000-0002-8500-1357	NSF [CCF-1217906, CCF-1617735]; NIH [R01-GM095959, U01-HL116330]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research was support in part by NSF Grants CCF-1217906 and CCF-1617735 and NIH Grants R01-GM095959 and U01-HL116330. Also, we would like to thank Dr. Viorica Patraucean at University of Cambridge (UK) for discussion of BDC-LSTM, and Prof. David P. Hughes and Dr. Maridel Fredericksen at Pennsylvania State University (US) for providing the 3D fungus datasets.	[Anonymous], ARXIV150502000; Cardona A, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000502; Chen H, 2016, PROCEEDINGS OF THE ASME 35TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING , 2016, VOL 2; Chen H, 2016, PROC CVPR IEEE, P2487, DOI 10.1109/CVPR.2016.273; Ciresan Dan, 2012, ADV NEURAL INFORM PR, P2843, DOI DOI 10.5555/2999325.2999452; Collobert R., 2011, NIPS; Dauphin Y. N., 2015, ARXIV150204390, V28, P1504; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Lee K., 2015, NIPS, P3559; Leonard N., 2015, ARXIV151107889; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Patraucean Viorica, 2015, ARXIV151106309; Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Stollenga M. F., 2015, ADV NEURAL INFORM PR, P2998; Sun C., 2015, ARXIV151103776; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6	19	104	108	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703084
C	van den Oord, A; Kalchbrenner, N; Vinyals, O; Espeholt, L; Graves, A; Kavukcuoglu, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		van den Oord, Aaron; Kalchbrenner, Nal; Vinyals, Oriol; Espeholt, Lasse; Graves, Alex; Kavukcuoglu, Koray			Conditional Image Generation with PixelCNN Decoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.	[van den Oord, Aaron; Kalchbrenner, Nal; Vinyals, Oriol; Espeholt, Lasse; Graves, Alex; Kavukcuoglu, Koray] Google DeepMind, London, England	Google Incorporated	van den Oord, A (corresponding author), Google DeepMind, London, England.	avdnoord@google.com; nalk@google.com; vinyals@google.com; espeholt@google.com; gravesa@google.com; korayk@google.com						Abadi M., TENSORFLOW LARGE SCA; [Anonymous], 2016, ARXIV160305106; [Anonymous], 2009, ADV NEURAL INFORM PR; Ba J. L, 2015, ARXIV151102793; Bellemare M. G., 2016, ARXIV160601868; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Dinh L., 2014, ARXIV; Gatys LeonA., 2015, ARXIV, DOI 10.1167/16.12.326; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2015, P 32 INT C MACH LEAR; Gregor K., 2014, P 31 INT C MACH LEAR; Gregor  Karol, 2016, ARXIV160106759; He  Kaiming, 2015, ARXIV1501203385; Kaiser L., 2015, COMPUTER SCI; Kalchbrenner N., 2015, COMPUTER SCI; Larochelle  Hugo, 2011, J MACHINE LEARNING R; Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7; Olah  Christopher, 2015, INCEPTIONISM GOING D; Reed S., 2016, ARXIV160505396; Rezende D. J., 2014, P 31 INT C MACH LEAR; Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sohl-Dickstein  Jascha, 2015, P 32 INT C MACH LEAR; Srivastava Rupesh K, 2015, TRAINING VERY DEEP N, V28, P2377; Theis Lucas, 2015, ARXIV151101844; Theis  Lucas, 2015, ADV NEURAL INFORM PR; Uria B., 2016, ARXIV160502226; van den Oord A., 2016, ARXIV160106759; van den Oord  Aaron, 2014, ADV NEURAL INFORM PR; van den Oord Aaron<spacing diaeresis>, 2014, J MACHINE LEARNING R, p2014b; Van Dijk TA, 2015, INT C MACH LEARN ICM, P1, DOI DOI 10.1002/9781118541555.WBIEPC058	32	104	106	1	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704051
C	Littman, ML; Sutton, RS; Singh, S		Dietterich, TG; Becker, S; Ghahramani, Z		Littman, ML; Sutton, RS; Singh, S			Predictive representations of state	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations. State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations. Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models. Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls). We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model.	AT&T Labs Res, Florham Pk, NJ 07932 USA	AT&T	Littman, ML (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.							BAUM LE, 1970, ANN MATH STAT, V41, P164, DOI 10.1214/aoms/1177697196; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Jaeger H, 2000, NEURAL COMPUT, V12, P1371, DOI 10.1162/089976600300015411; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Lovejoy WS, 1991, ANN OPER RES, V28, P47, DOI 10.1007/BF02055574; McCallum AK., 1995, THESIS U ROCHESTER; RIVEST RL, 1994, J ACM, V41, P555, DOI 10.1145/176584.176589; SHATKAY H, 1997, P 15 INT JOINT C ART, P920; Singh S, 1998, ADV NEUR IN, V10, P1057; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; [No title captured]	12	104	105	1	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1555	1561						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100193
C	Xu, L; Ren, JSJ; Liu, C; Jia, JY		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Xu, Li; Ren, Jimmy S. J.; Liu, Ce; Jia, Jiaya			Deep Convolutional Neural Network for Image Deconvolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Many fundamental image-related problems involve deconvolution operators. Real blur degradation seldom complies with an ideal linear convolution model due to camera noise, saturation, image compression, to name a few. Instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation. We note directly applying existing deep neural networks does not produce reasonable results. Our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in a supervised manner with proper initialization. They yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.	[Xu, Li; Ren, Jimmy S. J.] Lenovo Res & Technol, Beijing, Peoples R China; [Liu, Ce] Microsoft Res, Redmond, WA USA; [Jia, Jiaya] Chinese Univ Hong Kong, Shatin, Peoples R China	Legend Holdings; Lenovo; Microsoft; Chinese University of Hong Kong	Xu, L (corresponding author), Lenovo Res & Technol, Beijing, Peoples R China.	xulihk@lenovo.com; jimmy.sj.ren@gmail.com; celiu@microsoft.com; leojia@cse.cuhk.edu.hk	Jia, Jiaya/I-3251-2012		Research Grants Council of the Hong Kong Special Administrative Region [413113]	Research Grants Council of the Hong Kong Special Administrative Region(Hong Kong Research Grants Council)	Project webpage: http://www.lxu.me/projects/dcnn/. The paper is partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region (Project No. 413113).	Agostinelli F., 2013, NIPS; Agrawal A. K., 2007, CVPR; Burger H.C., 2012, CVPR; Cho S., 2011, ICCV; Dabov K, 2008, PROC SPIE, V6812, DOI 10.1117/12.766355; Eigen D., 2013, ICCV; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Jain V., 2008, NIPS; Kenig T, 2010, IEEE T PATTERN ANAL, V32, P2191, DOI 10.1109/TPAMI.2010.45; Krishnan D., 2009, NIPS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Levin A., 2009, CVPR; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Michaeli Tomer, 2013, ICCV; PERONA P, 1995, IEEE T PATTERN ANAL, V17, P488, DOI 10.1109/34.391394; Richardson W. H., 1972, J OPTICAL SOC AM, V62; Roth S., 2009, INT J COMPUTER VISIO, V82; Schmidt U., 2013, CVPR; Schuler Christian J, 2013, CVPR; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Whyte O., 2011, ICCV WORKSH; Wiener N., 1949, J AM STAT ASS, V47; Xie Junyuan, 2012, ADV NEURAL INFORM PR, V25; Xu L., 2014, ECCV; Yuan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360673; Zeiler Matthew D., 2010, CVPR; Zoran Daniel, 2011, ICCV	27	103	104	3	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102046
C	Bottou, U; Le Cun, Y		Thrun, S; Saul, K; Scholkopf, B		Bottou, U; Le Cun, Y			Large scale online learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.	NEC Corp Ltd, Labs Amer, Princeton, NJ 08540 USA	NEC Corporation	Bottou, U (corresponding author), NEC Corp Ltd, Labs Amer, Princeton, NJ 08540 USA.							Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], 2002, HDB BRAIN THEORY NEU; Bottou L, 1998, LECT NOTES COMPUTER, V1524; BOTTOU L, 2003, IN PRESS APPL STOCHA; Bottou Leon, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Chambers JM., 1992, STAT MODELS S; DENNIS JR, 1983, NUMERICAL METHODS UN; Murata N, 1999, SIGNAL PROCESS, V74, P3, DOI 10.1016/S0165-1684(98)00206-0; Tsypkin Y.Z, 1973, FDN THEORY LEARNING; Vapnik V., 1974, THEORY PATTERN RECOG	11	103	104	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						217	224						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500028
C	Swaminathan, A; Joachims, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Swaminathan, Adith; Joachims, Thorsten			The Self-Normalized Estimator for Counterfactual Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem. In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy. This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs. The Counterfactual Risk Minimization (CRM) principle [1] offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator. We show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces. We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem. This naturally gives rise to a new learning algorithm - Normalized Policy Optimizer for Exponential Models (Norm-POEM) - for structured output prediction using linear rules. We evaluate the empirical effectiveness of Norm-POEM on several multi-label classification problems, finding that it consistently outperforms the conventional estimator.	[Swaminathan, Adith; Joachims, Thorsten] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Swaminathan, A (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	adith@cs.cornell.edu; tj@cs.cornell.edu		Joachims, Thorsten/0000-0003-3654-3683	NSF [IIS-1247637, IIS-1217686, IIS-1513692]; JTCII Cornell-Technion Research Fund	NSF(National Science Foundation (NSF)); JTCII Cornell-Technion Research Fund	This research was funded in part through NSF Awards IIS-1247637, IIS-1217686, IIS-1513692, the JTCII Cornell-Technion Research Fund, and a gift from Bloomberg.	Beygelzimer A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Boyle P, 1997, J ECON DYN CONTROL, V21, P1267, DOI 10.1016/S0165-1889(97)00028-6; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; HESTERBERG T, 1995, TECHNOMETRICS, V37, P185, DOI 10.2307/1269620; Ionides EL, 2008, J COMPUT GRAPH STAT, V17, P295, DOI 10.1198/106186008X320456; Kong A., 1992, NOTE IMPORTANCE SAMP; Langford J., 2008, P 25 INT C MACHINE L, P528; Lewis AS, 2013, MATH PROGRAM, V141, P135, DOI 10.1007/s10107-012-0514-2; Li LH, 2015, JMLR WORKSH CONF PRO, V38, P608; Maurer Andreas, 2009, ARXIV09073740; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; Rubinstein RY, 2008, SIMULATION MONTE CAR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Swaminathan A, 2015, PR MACH LEARN RES, V37, P814; Thomas PS, 2015, AAAI CONF ARTIF INTE, P3000; Trotter H.F., 1956, S MONTE CARLO METHOD, P64; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Yu J, 2010, J MACH LEARN RES, V11, P1145; Zadrozny B, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P435, DOI 10.1109/icdm.2003.1250950	27	102	102	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101008
C	Ghahramani, Z; Beal, MJ		Leen, TK; Dietterich, TG; Tresp, V		Ghahramani, Z; Beal, MJ			Propagation algorithms for variational Bayesian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Ghahramani, Z (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							ATTIAS H, 2000, ADV NEURAL INFORMATI, V12; BEAL MJ, 2000, VARIATIONAL KALMAN S; BISHOP CM, 1999, P 9 ICANN; FRUWIRTHSCHNATT.S, 1995, J ROYAL STAT SOC B, V57, P237; GHAHRAMANI Z, 2000, ADV NEUR INF PROC SY, V12; Hinton G.E., 1993, 6 ACM C COMP LEARN T; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; MacKay D.J.C., 1997, BOOK ENSEMBLE LEARNI; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x; WATERHOUSE S, 1995, ADV NEUR INF PROC SY, V7	11	102	104	1	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						507	513						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800072
C	Zhang, XS; Wan, F; Liu, C; Ji, RR; Ye, QX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Xiaosong; Wan, Fang; Liu, Chang; Ji, Rongrong; Ye, Qixiang			FreeAnchor: Learning to Match Anchors for Visual Object Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modern CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In this study, we propose a learning-to-match approach to break IoU restriction, allowing objects to match anchors in a flexible manner. Our approach, referred to as FreeAnchor, updates hand-crafted anchor assignment to "free" anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization. FreeAnchor is implemented by optimizing detection customized likelihood and can be fused with CNN-based detectors in a plug-and-play manner. Experiments on COCO demonstrate that FreeAnchor consistently outperforms the counterparts with significant margins(1).	[Zhang, Xiaosong; Wan, Fang; Liu, Chang; Ye, Qixiang] Univ Chinese Acad Sci, Beijing, Peoples R China; [Ji, Rongrong] Xiamen Univ, Xiamen, Peoples R China; [Ye, Qixiang] Peng Cheng Lab, Shenzhen, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Xiamen University; Peng Cheng Laboratory	Ye, QX (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Ye, QX (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	zhangxiaosong18@mails.ucas.ac.cn; qxye@ucas.ac.cn			NSFC [61836012, 61671427, 61771447]; Post Doctoral Innovative Talent Support Program [119103S304]	NSFC(National Natural Science Foundation of China (NSFC)); Post Doctoral Innovative Talent Support Program	This work was supported in part by the NSFC under Grant 61836012, 61671427, and 61771447 and Post Doctoral Innovative Talent Support Program under Grant 119103S304.	Duan Kaiwen, 2019, IEEE CVPR; Fisher R.A., 1922, PHILOS T R SOC LON A, V222, P309, DOI [DOI 10.1098/RSTA.1922.0009, 10.1098/rsta.1922.0009]; Fu C. -Y., 2017, ARXIV170106659; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48; Kong T., 2019, ARXIV190403797; Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maron O, 1998, ADV NEUR IN, V10, P570; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang T, 2018, ADV NEUR IN, V31; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283; Zhu C, 2019, IEEE ENER CONV, P5084, DOI 10.1109/ECCE.2019.8912878	23	101	105	4	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300014
C	Cao, W; Wang, D; Li, J; Zhou, H; Li, YT; Li, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cao, Wei; Wang, Dong; Li, Jian; Zhou, Hao; Li, Yitan; Li, Lei			BRITS: Bidirectional Recurrent Imputation for Time Series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Time series are ubiquitous in many classification/regression applications. However, the time series data in real applications may contain many missing values. Hence, given multiple (possibly correlated) time series data, it is important to fill in missing values and at the same time to predict their class labels. Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose a novel method, called BRITS, based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data. We evaluate our model on three real-world datasets, including an air quality dataset, a healthcare dataset, and a localization dataset for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression.	[Cao, Wei; Li, Jian] Tsinghua Univ, Beijing, Peoples R China; [Cao, Wei; Zhou, Hao; Li, Yitan; Li, Lei] Bytedance AI Lab, Beijing, Peoples R China; [Wang, Dong] Duke Univ, Durham, NC 27706 USA	Tsinghua University; Duke University	Cao, W (corresponding author), Tsinghua Univ, Beijing, Peoples R China.; Cao, W (corresponding author), Bytedance AI Lab, Beijing, Peoples R China.	cao-13@tsinghua.org.cn; dong.wang363@duke.edu; lijian83@mail.tsinghua.edu.cn; haozhou0806@gmail.com; liyitan@bytedance.com; lileilab@bytedance.com		Li, Lei/0000-0003-3095-9776; Wang, Dong/0000-0002-2169-2937	National Basic Research Program of China [2015CB358700]; National Natural Science Foundation of China [61822203, 61772297, 61632016, 61761146003]; Microsoft Research Asia	National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Microsoft Research Asia(Microsoft)	Wei Cao and Jian Li are supported in part by the National Basic Research Program of China Grant 2015CB358700, the National Natural Science Foundation of China Grant 61822203, 61772297, 61632016, 61761146003, and a grant from Microsoft Research Asia.	Ansley C. F., 1984, TIME SERIES ANAL IRR, P9, DOI DOI 10.1007/978-1-4684-9403-7_2; Azur MJ, 2011, INT J METH PSYCH RES, V20, P40, DOI 10.1002/mpr.329; Basharat A, 2009, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2009.5459429; Batres-Estrada B., 2015, DEEP LEARNING MULTIV; Bauer S, 2016, PR MACH LEARN RES, V48; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bengio Y., 2014, ARXIV14061078; Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2; Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9; Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301; Fung DSC, 2006, METHODS ESTIMATION M; HARVEY AC, 1990, FORECASTING STRUCTUR; Hastie T, 2009, ELEMENTS STAT LEARNI; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Honkala M., 2015, ADV NEURAL INFORM PR, P856, DOI DOI 10.1021/ACS.JCIM.9B00943; Ian G, 2016, DEEP LEARNING; Kaluza B, 2010, LECT NOTES COMPUT SC, V6439, P177, DOI 10.1007/978-3-642-16917-5_18; Kreindler David M, 2012, NONLINEAR DYNAMICAL, P135; Li L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507; Lipton Zachary C, 2016, MACHINE LEARNING HEA, P253; Liu Zitao, 2016, Proc SIAM Int Conf Data Min, V2016, P810; Moritz S, 2017, R J, V9, P207; Ozaki T., 1985, HDB STATISTICS, V5, P25; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Rani S., 2012, INT J COMPUTER APPL, V52; Silva I, 2012, COMPUT CARDIOL CONF, V39, P245; Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409; Wang D, 2017, PROC INT CONF DATA, P243, DOI 10.1109/ICDE.2017.83; Wang J., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P501, DOI 10.1145/1148170.1148257; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Yi Xiuwen, 2016, P 25 INT JOINT C ART; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847; Zhang Junbo, 2017, P AAAI C ART INT, V31	35	101	101	4	15	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001033
C	Ratner, AJ; Ehrenberg, HR; Hussain, Z; Dunnmon, J; Re, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ratner, Alexander J.; Ehrenberg, Henry R.; Hussain, Zeshan; Dunnmon, Jared; Re, Christopher			Learning to Compose Domain-Specific Transformations for Data Augmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				TEXT CATEGORIZATION	Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.	[Ratner, Alexander J.; Ehrenberg, Henry R.; Hussain, Zeshan; Dunnmon, Jared; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Ratner, AJ (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ajratner@cs.stanford.edu; henryre@cs.stanford.edu; zeshanmh@cs.stanford.edu; jdunnmon@cs.stanford.edu; chrismre@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Defense Advanced Research Projects Agency (DARPA) SIMPLEX program [N66001-15-C-4043]; DARPA D3M program [FA8750-17-2-0095]; DARPA programs [FA8750-12-2-0335, FA8750-13-2-0039]; National Institute of Health (NIH) [U54EB020405]; Office of Naval Research (ONR) [N000141210041, N000141310129]; Moore Foundation; Okawa Research Grant; American Family Insurance; Intel; Stanford DAWN project: Intel; Microsoft; VMware; Teradata; DOE [108845]; DARPA [FA8750-17-2-0095]; Accenture; Toshiba	Defense Advanced Research Projects Agency (DARPA) SIMPLEX program(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA D3M program; DARPA programs; National Institute of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Office of Naval Research (ONR)(Office of Naval Research); Moore Foundation(Gordon and Betty Moore Foundation); Okawa Research Grant; American Family Insurance; Intel(Intel Corporation); Stanford DAWN project: Intel; Microsoft(Microsoft); VMware; Teradata; DOE(United States Department of Energy (DOE)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Accenture; Toshiba	We would like to thank Daniel Selsam, Ioannis Mitliagkas, Christopher De Sa, William Hamilton, and Daniel Rubin for valuable feedback and conversations. We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) SIMPLEX program under No. N66001-15-C-4043, the DARPA D3M program under No. FA8750-17-2-0095, DARPA programs No. FA8750-12-2-0335 and FA8750-13-2-0039, DOE 108845, National Institute of Health (NIH) U54EB020405, the Office of Naval Research (ONR) under awards No. N000141210041 and No. N000141310129, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, and Intel. This research was also supported in part by affiliate members and other supporters of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware. This material is based on research sponsored by DARPA under agreement number FA8750-17-2-0095. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, AFRL, NSF, NIH, ONR, or the U.S. Government.	Baluja S., 2017, ARXIV170309387; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Ciresan D. C., 2010, DEEP BIG SIMPLE NEUR, P80; Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7; DeVries T., 2017, ARXIV PREPRINT ARXIV; Doddington G. R., 2004, C LANG RES EV, P837; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fawzi A, 2016, IEEE IMAGE PROC, P3688, DOI 10.1109/ICIP.2016.7533048; Globerson A., 2008, ADV NEURAL INFORM PR, P1489; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Graham B, 2014, ARXIV14126071; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Hauberg S, 2016, JMLR WORKSH CONF PRO, V51, P342; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heath M, 2001, IWDM 2000: 5TH INTERNATIONAL WORKSHOP ON DIGITAL MAMMOGRAPHY, P212; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lu XH, 2006, J AM MED INFORM ASSN, V13, P526, DOI 10.1197/jamia.M2051; Mirza M., 2014, ARXIV; Miyato T, 2015, ARXIV150700677; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rubin, 2016, CANC IMAGING ARCHIVE; Sajjadi Mehdi, 2016, CORR; Salimans T, 2016, ADV NEUR IN, V29; Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870; Sixt L., 2016, ARXIV161101331; Springenberg Jost Tobias, 2015, ARXIV151106390; Uhlich S, 2017, INT CONF ACOUST SPEE, P261, DOI 10.1109/ICASSP.2017.7952158; Wierstra D, 2010, LOG J IGPL, V18, P620, DOI 10.1093/jigpal/jzp049	32	101	104	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST	29375240				2022-12-19	WOS:000452649403030
C	Zhou, BL; Lapedriza, A; Xiao, JX; Totralba, A; Oliva, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhou, Bolei; Lapedriza, Agata; Xiao, Jianxiong; Totralba, Antonio; Oliva, Aude			Learning Deep Features for Scene Recognition using Places Database	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.	[Zhou, Bolei; Lapedriza, Agata; Totralba, Antonio; Oliva, Aude] MIT, Cambridge, MA 02139 USA; [Xiao, Jianxiong] Princeton Univ, Princeton, NJ 08544 USA; [Lapedriza, Agata] Univ Oberta Catalunya, Barcelona, Spain	Massachusetts Institute of Technology (MIT); Princeton University; UOC Universitat Oberta de Catalunya	Zhou, BL (corresponding author), MIT, Cambridge, MA 02139 USA.				National Science Foundation [1016862]; ONR MURI [N000141010933]; MIT Big Data Initiative at CSAIL; Google; Xerox Award; Intel; Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory [FA8650-12-C-7211];  [TIN2012-38187-C03-02]	National Science Foundation(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); MIT Big Data Initiative at CSAIL; Google(Google Incorporated); Xerox Award; Intel(Intel Corporation); Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory; 	Thanks to Aditya Khosla for valuable discussions. This work is supported by the National Science Foundation under Grant No. 1016862 to A.O, ONR MURI N000141010933 to A.T, as well as MIT Big Data Initiative at CSAIL, Google and Xerox Awards, a hardware donation from NVIDIA Corporation, to A.O and A.T., Intel and Google awards to J.X, and grant TIN2012-38187-C03-02 to A.L. This work is also supported by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory, contract FA8650-12-C-7211 to A.T. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL, or the U.S. Government.	Agrawal P., 2014, P ECCV; [Anonymous], 2009, P CVPR; Bengio Y., FDN TRENDS MACHINE L; Doersch C., 2013, NEURIPS; Donahue J., 2014, DECAF DEEP CONVOLUTI; Fan R.E., 2008, LIBLINEAR LIB LARGE; Fei-Fei L., 2007, COMPUTER VISION IMAG; Griffin G., 2007, 120 CAL I TECHN; Heip C., 1998, OCEANIS; Jia Y., 2013, CAFFE OPEN SOURCE CO; Konkle T., 2010, PSYCH SCI; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Lazebnik S., 2006, P IEEE C COMP VIS PA, P1; LeCun Y., 1989, NEURAL COMPUTATION; Li L.J., 2007, P ICCV; Oliva A., 2013, NEW VISUAL NEUROSCIE; Oliva A., 2001, INT J COMPUTER VISIO; Patterson G., 2012, P CVPR; Quattoni A., 2009, P CVPR; Razavian A- S., 2014, CORR, V1403, P6382; Sanchez J., 2013, INT J COMPUTER VISIO; Simpson E.H., 1949, NATURE; Torralba A., 2011, P CVPR; Xiao J., 2010, P CVPR; Yao B., 2011, P ICCV	25	101	101	2	19	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101020
C	de Vries, H; Strub, F; Mary, J; Larochelle, H; Pietquin, O; Courville, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		de Vries, Harm; Strub, Florian; Mary, Jeremie; Larochelle, Hugo; Pietquin, Olivier; Courville, Aaron			Modulating early visual processing by language	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VISION	It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic inputs are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the entire visual processing by a linguistic input. Specifically, we introduce Conditional Batch Normalization (CBN) as an efficient mechanism to modulate convolutional feature maps by a linguistic embedding. We apply CBN to a pre-trained Residual Network (ResNet), leading to the MODulatEd ResNet (MODERN) architecture, and show that this significantly improves strong baselines on two visual question answering tasks. Our ablation study confirms that modulating from the early stages of the visual processing is beneficial.	[de Vries, Harm; Courville, Aaron] Univ Montreal, Montreal, PQ, Canada; [Strub, Florian; Mary, Jeremie] Univ Lille, CNRS, Cent Lille, Inria,UMR 9189 CRIStAL, Lille, France; [Larochelle, Hugo] Google Brain, Mountain View, CA USA; [Pietquin, Olivier] DeepMind, London, England	Universite de Montreal; Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Lille - ISITE; Centrale Lille; Universite de Lille; Google Incorporated	de Vries, H (corresponding author), Univ Montreal, Montreal, PQ, Canada.	mail@harmdevries.com; florian.strub@inria.fr; jeremie.mary@univ-lille3.fr; hugolarochelle@google.com; pietquin@google.com; aaron.courville@gmail.com	Jeong, Yongwook/N-7413-2016		CHISTERA IGLU; CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies; NSERC; Calcul Quebec; Compute Canada; Canada Research Chairs; CIFAR	CHISTERA IGLU; CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies(Region Hauts-de-France); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Calcul Quebec; Compute Canada; Canada Research Chairs(Canada Research ChairsCGIAR); CIFAR(Canadian Institute for Advanced Research (CIFAR))	The authors would like to acknowledge the stimulating research environment of the SequeL lab. We thank Vincent Dumoulin for helpful discussions about conditional batch normalization. We acknowledge the following agencies for research funding and computing support: CHISTERA IGLU and CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR. We thank NVIDIA for providing access to a DGX-1 machine used in this work.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ben-younes Hedi, 2017, ARXIV170506676; Bengio Y., 2014, ARXIV14061078; Boutonnet B, 2015, J NEUROSCI, V35, P9329, DOI 10.1523/JNEUROSCI.5111-14.2015; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; de Vries H, 2017, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2017.475; Dumoulin Vincent, 2017, ICLR; Ferreira F, 2007, J MEM LANG, V57, P455, DOI 10.1016/j.jml.2007.08.002; Fukui Akira, 2016, ARXIV160601847; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jiasen J., 2016, P NIPS; Kaiming K., 2016, P CVPR; Kim J., 2016, P NIPS; Kim J., 2017, P ICLR; Kok P, 2014, J COGNITIVE NEUROSCI, V26, P1546, DOI 10.1162/jocn_a_00562; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Malinowski M., 2016, ARXIV160502697; Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Ren MY, 2015, ADV NEUR IN, V28; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thierry G, 2009, P NATL ACAD SCI USA, V106, P4567, DOI 10.1073/pnas.0811155106; Van der Maaten L., 2008, J MACH LEARN RES, V9, P2579; Xu H., 2015, P ECCV; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yashand G., 2017, P CVPR	28	100	100	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406064
C	Maddox, WJ; Garipov, T; Izmailov, P; Vetrov, D; Wilson, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Maddox, Wesley J.; Garipov, Timur; Izmailov, Pavel; Vetrov, Dmitry; Wilson, Andrew Gordon			A Simple Baseline for Bayesian Uncertainty in Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.	[Maddox, Wesley J.; Izmailov, Pavel; Wilson, Andrew Gordon] NYU, New York, NY 10003 USA; [Garipov, Timur; Vetrov, Dmitry] Samsung AI Ctr Moscow, Moscow, Russia; [Vetrov, Dmitry] Natl Res Univ Higher Sch Econ, Samsung HSE Lab, Moscow, Russia	New York University; HSE University (National Research University Higher School of Economics)	Maddox, WJ (corresponding author), NYU, New York, NY 10003 USA.				Amazon Research Award; Facebook Research; NSF [IIS-1563887, IIS-1910266]; NSF Graduate Research Fellowship [DGE-1650441]; Russian Science Foundation [19-71-30020]	Amazon Research Award; Facebook Research(Facebook Inc); NSF(National Science Foundation (NSF)); NSF Graduate Research Fellowship(National Science Foundation (NSF)); Russian Science Foundation(Russian Science Foundation (RSF))	WM, PI, and AGW were supported by an Amazon Research Award, Facebook Research, NSF IIS-1563887, and NSF IIS-1910266. WM was additionally supported by an NSF Graduate Research Fellowship under Grant No. DGE-1650441. DV was supported by the Russian Science Foundation grant no. 19-71-30020. We would like to thank Jacob Gardner and Polina Kirichenko for helpful discussions.	Athiwaratkun B., 2019, INT C LEARN REPR; Blier L., 2018, ADV NEURAL INFORM PR, P11; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Bui TD, 2016, PR MACH LEARN RES, V48; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Chen X., 2016, ARXIV161008637; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Dasgupta S, 2007, LECT NOTES COMPUT SC, V4539, P278, DOI 10.1007/978-3-540-72927-3_21; Gal Y., 2017, ADV NEURALINFORMATIO; Gal Y, 2016, PR MACH LEARN RES, V48; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Gur-Ari G., 2019, GRADIENT DESCENT HAP; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hernfindez-Lobato J. M., 2015, ADV NEURAL INFORM PR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Izmailov P, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P876; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Keskar N.S., 2017, ICLR; Khan ME, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2015, ARXIV150602557CSSTAT; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kuleshov V, 2018, PR MACH LEARN RES, V80; Lakshminarayanan B., 2017, ADV NEURALINFORMATIO; Loshchilov Ilya, 2019, INT C LEARN REPR; Louizos C, 2017, PR MACH LEARN RES, V70; lzmailov P., 2019, ARXIV190707504; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Mandt Stephan, 2017, ARXIV170404289; Merity Stephen, 2017, ICLR; Molchanov D, 2017, PR MACH LEARN RES, V70; Mukhoti J., 2018, EVALUATING BAYESIAN; Neal Radford M, 1996, BAYESIAN LEARNING NE, V118, DOI [10.1007/978-1-4612-0745-0, DOI 10.1007/978-1-4612-0745-0]; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Nikishin E., 2018, IMPROVING STABILITY; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Ritter Hippolyt, 2018, ADV NEURAL INFORM PR, P1; Ritter Hippolyt, 2018, ICLR; Ruppert D., 1988, 781 CORN U SCH OP; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wu A., 2019, ICLR; Yang G, 2019, INT CONF KNOWL SMART, P170, DOI 10.1109/KST.2019.8687775; Yazici Y, 2019, INT C LEARN REPR ICL; Zhang G., 2017, ARXIV171202390CSSTAT	49	99	99	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904075
C	Balaji, Y; Sankaranarayanan, S; Chellappa, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Balaji, Yogesh; Sankaranarayanan, Swami; Chellappa, Rama			MetaReg: Towards Domain Generalization using Meta-Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, we encode this notion of domain generalization using a novel regularization function. We pose the problem of finding such a regularization function in a Learning to Learn (or) meta-learning framework. The objective of domain generalization is explicitly modeled by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization.	[Balaji, Yogesh] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Sankaranarayanan, Swami] Butterfly Network Inc, New York, NY USA; [Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Balaji, Y (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	yogesh@cs.umd.edu; swamiviv@butterflynetinc.com; rama@umiacs.umd.edu	Chellappa, Rama/AAJ-1504-2020; Sankaranarayanan, Swami/AFP-9228-2022; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012		MURI from the Army Research Office [W911NF-17-1-0304]; US DOD; UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative	MURI from the Army Research Office; US DOD(United States Department of Defense); UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This reseach was supported by MURI from the Army Research Office under the Grant No. W911NF-17-1-0304. This is part of the collaboration between US DOD, UK MOD and UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative.	Andrychowicz M, 2016, ADV NEUR IN, V29; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Chen Minmin, 2012, ICML, DOI DOI 10.1109/ISGT-ASIA.2012.6303206; Chen Minmin, 2011, ADV NEURAL INFORM PR, V24; Daume H, 2007, P 45 ANN M ASS COMP, V45, P256; Finn C, 2017, PR MACH LEARN RES, V70; Ganin Y., 2016, JMLR, V17, P2096; Ghifary Muhammad, 2015, 2015 IEEE INT C COMP; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R., 2011, P 2011 INT C COMP VI; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hospedales Timothy M., 2017, CORR; Khosla Aditya, 2012, P 12 EUR C COMP VIS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; KROGH A, 1992, ADV NEUR IN, V4, P950; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li Ke, 2017, CORR; Long MS, 2015, PR MACH LEARN RES, V37, P97; Muandet Krikamol, 2013, ICML; Ni J, 2013, PROC CVPR IEEE, P692, DOI 10.1109/CVPR.2013.95; Ravi S., 2017, INT C LEARN REPR, P12; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Schmidhuber Jurgen, 1995, TECHNICAL REPORT; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Teney Damien, 2017, CORR; Thrun S, 1998, LEARNING TO LEARN, P181; Wan L., 2013, P INT C MACHINE LEAR, P1058; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhou Fengwei, 2018, CORR	32	99	100	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301003
C	Wang, Y; Tao, X; Qi, XJ; Shen, XY; Jia, JY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Yi; Tao, Xin; Qi, Xiaojuan; Shen, Xiaoyong; Jia, Jiaya			Image Inpainting via Generative Multi-column Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPLETION	In this paper, we propose a generative multi-column network for image inpainting. This network synthesizes different image components in a parallel manner within one stage. To better characterize global structures, we design a confidence-driven reconstruction loss while an implicit diversified MRF regularization is adopted to enhance local details. The multi-column network combined with the reconstruction and MRF loss propagates local and global information derived from context to the target inpainting regions. Extensive experiments on challenging street view, face, natural objects and scenes manifest that our method produces visual compelling results even without previously common post-processing.	[Wang, Yi; Tao, Xin; Qi, Xiaojuan; Jia, Jiaya] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Tao, Xin; Shen, Xiaoyong; Jia, Jiaya] Tencent, YouTu Lab, Shenzhen, Peoples R China	Chinese University of Hong Kong; Tencent	Wang, Y (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	yiwang@cse.cuhk.edu.hk; xtao@cse.cuhk.edu.hk; xjqi@cse.cuhk.edu.hk; goodshenxy@gmail.com; leojia@cse.cuhk.edu.hk	Jia, Jiaya/I-3251-2012					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Gulrajani I, 2017, P NIPS 2017; He KM, 2014, IEEE T PATTERN ANAL, V36, P2423, DOI 10.1109/TPAMI.2014.2330611; He KM, 2012, LECT NOTES COMPUT SC, V7573, P16, DOI 10.1007/978-3-642-33709-3_2; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Jia JY, 2004, IEEE T PATTERN ANAL, V26, P771, DOI 10.1109/TPAMI.2004.10; Jia JY, 2003, PROC CVPR IEEE, P643; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274; Talmi I, 2017, PROC CVPR IEEE, P1311, DOI 10.1109/CVPR.2017.144; Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1; Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Zelnik-Manor L., 2018, ARXIV PREPRINT ARXIV; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009	28	99	104	5	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300031
C	Klein, D; Manning, CD		Dietterich, TG; Becker, S; Ghahramani, Z		Klein, D; Manning, CD			Natural language grammar induction using a constituent-context model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper presents a novel approach to the unsupervised learning of syntactic analyses of natural language text. Most previous work has focused on maximizing likelihood according to generative PCFG models. In contrast, we employ a simpler probabilistic model over trees based directly on constituent identity and linear context, and use an Em-like iterative procedure to induce structure. This method produces much higher quality analyses, giving the best published results on the ATIS dataset.	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Klein, D (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	klein@cs.stanford.edu; manning@cs.stanford.edu	Manning, Christopher/AAM-9535-2020	Manning, Christopher/0000-0001-6155-649X				Baker J. K., 1979, 97 M AC SOC AM, P547; Carroll G, 1992, WORKSH STAT BAS NLP, P1; Charniak E, 2000, 6TH APPLIED NATURAL LANGUAGE PROCESSING CONFERENCE/1ST MEETING OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE AND PROCEEDINGS OF THE ANLP-NAACL 2000 STUDENT RESEARCH WORKSHOP, pA132; Chomsky N., 1968, SOUND PATTERN ENGLIS; Chomsky N., 1986, KNOWLEDGE LANGUAGE; CLARK A, 2001, 5 C NAT LANG LEARN; Collins M, 1997, 35TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 8TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P16; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FINCH S, 1994, PROCEEDINGS OF THE SIXTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P301; KLEIN D, 2001, 5 C NAT LANG LEARN; Lari K., 1990, Computer Speech and Language, V4, P35, DOI 10.1016/0885-2308(90)90022-X; Pereira F.C.N., 1992, ACL 30, P128; Radford Andrew, 1988, TRANSFORMATIONAL GRA; Schutze H, 1995, SEVENTH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P141; STOLCKE A, 1994, GRAMMATICAL INFERENC; VANZAANEN M, 2001, 200105 U LEEDS; WOLFF JG, 1988, CATEGORIES PROCESSES, P179; Zellig S, 1951, METHODS STRUCTURAL L	18	99	101	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						35	42						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100005
C	Geirhos, R; Temme, CRM; Rauber, J; Schutt, HH; Bethge, M; Wichmann, FA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Geirhos, Robert; Temme, Carlos R. Medina; Rauber, Jonas; Schuett, Heiko H.; Bethge, Matthias; Wichmann, Felix A.			Generalisation in humans and deep neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISCRIMINATION; CONTRAST	We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.	[Geirhos, Robert; Temme, Carlos R. Medina; Schuett, Heiko H.; Wichmann, Felix A.] Univ Tubingen, Neural Informat Proc Grp, Tubingen, Germany; [Geirhos, Robert; Rauber, Jonas; Bethge, Matthias; Wichmann, Felix A.] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany; [Geirhos, Robert; Rauber, Jonas] Int Max Planck Res Sch Intelligent Syst, Tubingen, Germany; [Schuett, Heiko H.] Univ Tubingen, Grad Sch Neural & Behav Sci, Tubingen, Germany; [Schuett, Heiko H.] Univ Potsdam, Dept Psychol, Potsdam, Germany; [Bethge, Matthias; Wichmann, Felix A.] Bernstein Ctr Computat Neurosci Tubingen, Tubingen, Germany; [Bethge, Matthias] Max Planck Inst Biol Cybernet, Tubingen, Germany; [Wichmann, Felix A.] Max Planck Inst Intelligent Syst, Stuttgart, Germany	Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; University of Potsdam; Max Planck Society; Max Planck Society	Geirhos, R (corresponding author), Univ Tubingen, Neural Informat Proc Grp, Tubingen, Germany.; Geirhos, R (corresponding author), Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.; Geirhos, R (corresponding author), Int Max Planck Res Sch Intelligent Syst, Tubingen, Germany.	robert.geirhos@bethgelab.org	Schütt, Heiko/AAV-3817-2021	Wichmann, Felix/0000-0002-2592-634X	German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tubingen [FKZ: 01GQ1002]; German Research Foundation (DFG) [Sachbeihilfe Wi 2103/4-1, SFB 1233]; International Max Planck Research School for Intelligent Systems (IMPRS-IS); Bosch Forschungsstiftung (Stifterverband) [T113/30057/17]; Centre for Integrative Neuroscience Tubingen [EXC 307]; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]	German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tubingen(Federal Ministry of Education & Research (BMBF)); German Research Foundation (DFG)(German Research Foundation (DFG)); International Max Planck Research School for Intelligent Systems (IMPRS-IS); Bosch Forschungsstiftung (Stifterverband); Centre for Integrative Neuroscience Tubingen; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)	This work has been funded, in part, by the German Federal Ministry of Education and Research (BMBF) through the Bernstein Computational Neuroscience Program Tubingen (FKZ: 01GQ1002) as well as the German Research Foundation (DFG; Sachbeihilfe Wi 2103/4-1 and SFB 1233 on "Robust Vision"). The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting R.G. and J.R.; J.R. acknowledges support by the Bosch Forschungsstiftung (Stifterverband, T113/30057/17); M.B. acknowledges support by the Centre for Integrative Neuroscience Tubingen (EXC 307) and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003.	Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Berardino Alexander, 2017, ADV NEURAL INFORM PR, P3533; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BOX GEP, 1976, J AM STAT ASSOC, V71, P791, DOI 10.2307/2286841; Brainard David H, 1996, HUMAN COLOR VISION C; Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Carandini M, 1997, J NEUROSCI, V17, P8621; Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136; Chen  Z., 2016, LIFELONG MACHINE LEA; Cichy RM, 2016, NEUROIMAGE; Dekel Ron, 2017, ARXIV170104674; Delorme A, 2000, VISION RES, V40, P2187, DOI 10.1016/S0042-6989(00)00083-3; DERRINGTON AM, 1984, J PHYSIOL-LONDON, V357, P241, DOI 10.1113/jphysiol.1984.sp015499; DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010; Dodge S., 2016, 2016 8 INT C QUALITY, P1, DOI DOI 10.1109/QOMEX.2016.7498955; Dodge S, 2017, 2017 26TH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND NETWORKS (ICCCN 2017); Dodge Samuel, 2017, ARXI171004744; Donahue J, 2014, PR MACH LEARN RES, V32; DOUGLAS RJ, 1991, TRENDS NEUROSCI, V14, P286, DOI 10.1016/0166-2236(91)90139-L; Flachot A, 2018, J OPT SOC AM A, V35, pB334, DOI 10.1364/JOSAA.35.00B334; Gal Y, 2016, PR MACH LEARN RES, V48; Geirhos R., 2017, ARXIV170606969; Geirhos R., 2018, IMAGENET TRAINED CNN, DOI DOI 10.48550/ARXIV.1811.12231; Gerstner Wulfram, 2005, 23 PROBLEMS SYSTEMS, P135; GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8; Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Henning GB, 2002, J OPT SOC AM A, V19, P1259, DOI 10.1364/JOSAA.19.001259; Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Jozwik KM, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01726; Karimi-Rouzbahani H, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13756-8; Kawaguchi K., 2017, ARXIV171005468V4STAT, DOI DOI 10.2196/jmir.5870; Kheradpisheh S. R., 2016, ARXIV150803929; Kietzmann Tim C, 2017, BIORXIV; Kleiner M, 2007, PERCEPTION, V36, P14; Koenderink J, 2017, J VISION, V17, DOI 10.1167/17.2.7; Kriegeskorte N, 2015, ANNU REV VIS SCI, V1, P417, DOI 10.1146/annurev-vision-082114-035447; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kubilius J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004896; Kummerer Matthias, 2016, ARXIV161001563; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lamme VAF, 1998, CURR OPIN NEUROBIOL, V8, P529, DOI 10.1016/S0959-4388(98)80042-1; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; NACHMIAS J, 1974, VISION RES, V14, P1039, DOI 10.1016/0042-6989(74)90175-8; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Pelli DG, 1999, J OPT SOC AM A, V16, P647, DOI 10.1364/JOSAA.16.000647; POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509; Pramod RT, 2016, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2016.177; R Development Core Team, 2018, R LANG ENV STAT COMP; Rosch E., 1999, CONCEPTS CORE READIN, P189, DOI [DOI 10.1016/B978-1-4832-1446-7.50028-5, 10.1016/b978-l-4832-1446-7.50028-5]; Rosenfeld A, 2018, ARXIV180301485; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schutt HH, 2017, J VISION, V17, DOI 10.1167/17.12.12; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sporns O, 2004, NEUROINFORMATICS, V2, P145, DOI 10.1385/NI:2:2:145; Stockman A, 2000, VISION RES, V40, P1711, DOI 10.1016/S0042-6989(00)00021-3; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453; Vasiljevic I., 2016, ARXIV161105760; Wallis TSA, 2017, J VISION, V17, DOI 10.1167/17.12.5; Wichmann F.A., 2017, ELECT IMAGING, V14, P36, DOI 10.2352/ISSN.2470-1173.2017.14.HVEI-113; Wichmann F. A., 1999, THESIS; Wichmann FA, 2006, VISION RES, V46, P1520, DOI 10.1016/j.visres.2005.11.008; Wichmann FA, 2010, J VISION, V10, DOI 10.1167/10.4.6; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zemel R.S., 2016, P 4 INT C LEARN REPR; Zhang Chiyuan, 2016, ARXIV161103530; Zhou YR, 2017, INT CONF ACOUST SPEE, P1213, DOI 10.1109/ICASSP.2017.7952349	79	98	99	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002012
C	Blanchard, P; El Mhamdi, EM; Guerraoui, R; Stainer, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Blanchard, Peva; El Mhamdi, El Mandi; Guerraoui, Rachid; Stainer, Julien			Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.	[Blanchard, Peva; El Mhamdi, El Mandi; Guerraoui, Rachid; Stainer, Julien] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	El Mhamdi, EM (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	peva.blanchard@epfl.ch; elmandi.elmhamdi@epfl.ch; rachid.guerraoui@epfl.ch; julien.stainer@epfl.ch	Jeong, Yongwook/N-7413-2016		European ERC [339539 - AOC]; Swiss National Science Foundation [200021_169588]	European ERC(European Research Council (ERC)); Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	The authors would like to thank Georgios Damaskinos and Rhicheek Patra from the Distributed Computing group at EPFL for kindly providing their distributed machine learning framework, on top of which we could test our algorithm, Krum, and its variants described in this work. Further implementation details and additional experiments will be posted in the lab's Github repository [20]. The authors would also like to thank Saad Benjelloun, Le Nguyen Hoang and Sebastien Rouault for fruitful comments. This work has been supported in part by the European ERC (Grant 339539 - AOC) and by the Swiss National Science Foundation (Grant 200021_169588 TARBDA). A preliminary version of this work appeared as a brief announcement during the 36st ACM Symposium on Principles of Distributed Computing [2].	Abadi Martin, 2016, 12 USENIX S OP SYST, P265, DOI DOI 10.5555/3026877.3026899; [Anonymous], 1983, FESTSCHRIFT ERICH L; Blanchard P, 2017, PROCEEDINGS OF THE ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING (PODC'17), P455, DOI 10.1145/3087801.3087861; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Cohen MB, 2016, ACM S THEORY COMPUT, P9, DOI 10.1145/2897518.2897647; Dean J., 2012, NIPS 12, V1, P1223; El Mhamdi E, 2017, SYM REL DIST SYST, P84, DOI 10.1109/SRDS.2017.21; El Mhamdi E, 2017, INT PARALL DISTRIB P, P1028, DOI 10.1109/IPDPS.2017.66; Fawzi A., 2016, ADV NEURAL INFORM PR, P1632; Feng J., 2017, ARXIV170100251; Haykin S., 2009, NEURAL NETWORKS LEAR, V3; Herlihy M, 2014, LECT NOTES COMPUT SC, V8392, P214; Konecny J., 2016, ARXIV161005492; LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176; Lichman M., 2013, UCI MACHINE LEARNING; LPD-EPFL, IMPL IS PART LARG DI; Lynch N. A., 1996, DISTRIBUTED ALGORITH; Markoff J., 2012, NY TIMES; McMahan Brendan, 2015, ARXIV151103575; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Mendes H, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P391; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Saad D, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; SCHNEIDER FB, 1990, COMPUT SURV, V22, P299, DOI 10.1145/98163.98167; Srivastava R.K., 2015, P 28 INT C NEUR INF, P2377, DOI [10.5555/2969442.2969505, DOI 10.5555/2969442.2969505]; Su LL, 2016, PROCEEDINGS OF THE 2016 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING (PODC'16), P425, DOI 10.1145/2933057.2933105; Trask A, 2015, PR MACH LEARN RES, V37, P2266; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Wang B, 2016, ARXIV161200334; Zhang S., 2015, NEURAL INFORM PROCES, P685; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332	34	98	98	3	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400012
C	Wang, YC; Ye, XJ; Zha, HY; Song, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Yichen; Ye, Xiaojing; Zha, Hongyuan; Song, Le			Predicting User Activity Level In Point Processes With Mass Transport Equation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an efficient estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to the state of the art.	[Wang, Yichen; Zha, Hongyuan; Song, Le] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Ye, Xiaojing] Georgia Inst Technol, Sch Math, Atlanta, GA 30332 USA; [Song, Le] Ant Financial, Xihu, Peoples R China	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Wang, YC (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	yichen.wang@gatech.edu; xye@gsu.edu; zha@cc.gatech.edu; lsong@cc.gatech.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1218749, IIS-1639792 EAGER, CNS-1704701]; NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR [N00014-15-1-2340, DMS-1620342, CMMI-1745382, IIS-1639792, IIS-1717916]; NVIDIA; Intel ISTC; Amazon AWS	NSF(National Science Foundation (NSF)); NIH BIGDATA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); NVIDIA; Intel ISTC; Amazon AWS	This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, DMS-1620342, CMMI-1745382, IIS-1639792, IIS-1717916, NVIDIA, Intel ISTC and Amazon AWS.	Aalen OO, 2008, STAT BIOL HEALTH, P1; Antoniades D., 2013, ARXIV13096001; BLACKWELL D, 1947, ANN MATH STAT, V18, P105, DOI 10.1214/aoms/1177730497; BREMAUD P, 1981, POINT PROCESSES QUEU; Da Fonseca J, 2014, J FUTURES MARKETS, V34, P548, DOI 10.1002/fut.21644; Dai H., 2016, ARXIV160903675; Dormand J.R., 1980, J COMPUT APPL MATH, V6, P19, DOI [10.1016/0771-050X(80)90013-3, DOI 10.1016/0771-050X(80)90013-3]; Du N., 2015, ADV NEURAL INFORM PR, P3492; Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147; Du Nan, 2012, NIPS, P2780; Farajtabar M, 2015, ADV NEUR IN, V28; Gao S, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P107, DOI 10.1145/2684822.2685303; Gelfand I. M., 2000, CALCULUS VARIATIONS; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; HE N, 2016, ARXIV160801264; He XR, 2015, PR MACH LEARN RES, V37, P871; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Lian WZ, 2015, PR MACH LEARN RES, V37, P2030; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Pan JW, 2016, PR MACH LEARN RES, V48; Pastor-Satorras R, 2015, REV MOD PHYS, V87, P925, DOI 10.1103/RevModPhys.87.925; Tan Xi, 2016, UAI, P726; Trivedi R, 2017, PR MACH LEARN RES, V70; Wang Y., 2016, ADV NEURAL INFORM PR, P4547; Wang YC, 2016, PR MACH LEARN RES, V48; Wang YC, 2017, ADV NEUR IN, V30; Wang YC, 2017, PR MACH LEARN RES, V70; Wang Yichen, 2016, ARXIV160309021; Yang Shuang-Hong, 2013, P 30 INT C MACH LEAR, V28; Yu Linyun, 2015, ICDM; Zhao Qingyuan, 2015, KDD, P1513, DOI DOI 10.1145/2783258.2783401; Zhou K., 2013, ARTIF INTELL, P641	33	98	102	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401066
C	Wang, YX; Ramanan, D; Hebert, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Yu-Xiong; Ramanan, Deva; Hebert, Martial			Learning to Model the Tail	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We describe an approach to learning from long-tailed, imbalanced datasets that are prevalent in real-world settings. Here, the challenge is to learn accurate "few-shot" models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows. First, we propose to transfer meta-knowledge about learning-to-learn from the head classes. This knowledge is encoded with a meta-network that operates on the space of model parameters, that is trained to predict many-shot model parameters from few-shot model parameters. Second, we transfer this meta-knowledge in a progressive manner, from classes in the head to the "body", and from the "body" to the tail. That is, we transfer knowledge in a gradual fashion, regularizing meta-networks for few-shot regression with those trained with more training data. This allows our final network to capture a notion of model dynamics, that predicts how model parameters are likely to change as more training data is gradually added. We demonstrate results on image classification datasets (SUN, Places, and ImageNet) tuned for the long-tailed setting, that significantly outperform common heuristics, such as data resampling or reweighting.	[Wang, Yu-Xiong; Ramanan, Deva; Hebert, Martial] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, YX (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	yuxiongw@cs.cmu.edu; dramanan@cs.cmu.edu; hebert@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		ONR MURI [N000141612007]; U.S. Army Research Laboratory (ARL) under the Collaborative Technology Alliance Program [W911NF-10-2-0016]; National Science Foundation (NSF) [IIS-1618903]; Google; Facebook	ONR MURI(MURIOffice of Naval Research); U.S. Army Research Laboratory (ARL) under the Collaborative Technology Alliance Program; National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Google(Google Incorporated); Facebook(Facebook Inc)	We thank Liangyan Gui, Olga Russakovsky, Yao-Hung Hubert Tsai, and Ruslan Salakhutdinov for valuable and insightful discussions. This work was supported in part by ONR MURI N000141612007 and U.S. Army Research Laboratory (ARL) under the Collaborative Technology Alliance Program, Cooperative Agreement W911NF-10-2-0016. DR was supported in part by the National Science Foundation (NSF) under grant number IIS-1618903, Google, and Facebook. We also thank NVIDIA for donating GPUs and AWS Cloud Credits for Research program.	Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andrychowicz M, 2016, ADV NEUR IN, V29; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Bengio S., 2015, ICMI; Bertinetto Luca, 2016, NIPS; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Donahue J, 2014, PR MACH LEARN RES, V32; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Finn C, 2017, PR MACH LEARN RES, V70; Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441; George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612; Ha David, 2017, ICLR; Han B., 2016, P CVPR; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Huh Minyoung, 2016, ARXIV160808614; Jia Y., 2014, P 22 ACM INT C MULT, P675; Koch G., 2015, ICML DEEP LEARNING W; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li Ke, 2017, ICLR; Li Z, 2016, LECT NOTES COMPUT SC, V9906, P541, DOI 10.1007/978-3-319-46475-6_34; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Ravi S., 2017, INT C LEARN REPR, P12; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J, 1997, MACH LEARN, V28, P105, DOI 10.1023/A:1007383707642; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Schmidhuber J, 1987, THESIS; Schmidhuber J., 1993, IEEE INT C NEUR NETW; Shen Li, 2016, ECCV; Sinha Abhishek, 2017, ICLR; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Thrun S., 2012, LEARNING LEARN; Triantafillou E, 2017, NIPS; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Horn Grant, 2017, ARXIV170901450; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang Y.-X., 2016, AAAI; Wang Y.X., 2015, CVPR; Wang YC, 2016, ADV NEUR IN, V29; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187; Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhong Q, 2016, CVPR WORKSH; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhu XX, 2016, INT J COMPUT VISION, V119, P76, DOI 10.1007/s11263-015-0812-2; Zhu XX, 2014, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2014.122	63	98	102	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407012
C	Rajeswaran, A; Finn, C; Kakade, SM; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rajeswaran, Aravind; Finn, Chelsea; Kakade, Sham M.; Levine, Sergey			Meta-Learning with Implicit Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.	[Rajeswaran, Aravind; Kakade, Sham M.] Univ Washington, Seattle, WA 98195 USA; [Finn, Chelsea; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of Washington; University of Washington Seattle; University of California System; University of California Berkeley	Rajeswaran, A (corresponding author), Univ Washington, Seattle, WA 98195 USA.	aravraj@cs.washington.edu; cbfinn@cs.stanford.edu; sham@cs.washington.edu; svlevine@eecs.berkeley.edu			Washington Research Foundation for innovation in Data-intensive Discovery; ONR [N00014-18-1-2247]; NSF [CCF 1740551, CCF-1703574]	Washington Research Foundation for innovation in Data-intensive Discovery; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	Aravind Rajeswaran thanks Emo Todorov for valuable discussions about implicit gradients and potential application domains; Aravind Rajeswaran also thanks Igor Mordatch and Rahul Kidambi for helpful discussions and feedback. Sham Kakade acknowledges funding from the Washington Research Foundation for innovation in Data-intensive Discovery; Sham Kakade also graciously acknowledges support from ONR award N00014-18-1-2247, NSF Award CCF-1703574, and NSF CCF 1740551 award.	Al-Shedivat Maruan, 2017, CORR; Alet F., 2018, ARXIV180610166; Allen K. R., 2019, ARXIV190204552; Andrychowicz M, 2016, ADV NEUR IN, V29; BAUR W, 1983, THEOR COMPUT SCI, V22, P317, DOI 10.1016/0304-3975(83)90110-X; Baydin Atilim Gunes, 2015, CORR; Bertinetto Luca, 2018, INT C LEARN REPR; Bubeck Sebastien, 2015, FDN TRENDS MACHINE L; Do C. B., 2007, NIPS; Domke J., 2012, AISTATS; Duan Y., 2016, RL2 FAST REINFORCEME; Finn C., 2018, ADV NEURAL INFORM PR, P9516; Finn C., 2017, INT C MACH LEARN ICM; Finn C., 2017, ARXIV171011622; Finn Chelsea, 2017, C ROB LEARN PMLR, P357; Finn Chelsea, 2018, THESIS, P1; Finn Chelsea, 2019, INT C MACH LEARN ICM; Franceschi L, 2017, PR MACH LEARN RES, V70; Grant E., 2018, INT C LEARN REPR ICL; Griewank Andreas, 1993, SOME BOUNDS COMPLEXI; Harrison James, 2018, ARXIV180708912; Hascoet Laurent, 2006, CORR; Hochreiter S., 2001, INT C ART NEUR NETW; Jin C., 2017, ICML; Kim Jaehong, 2018, ARXIV180606927; Kingma DP, 2015, INT C LEARN REPR ICL; Koch G., 2015, ICML DEEP LEARN WORK; Kolter J. Z, 2017, P MACHINE LEARNING R, P136; Lake B.M., 2011, C COGN SCI SOC COGSC; Landry B., 2019, ARXIV190203319; Lee Kwonjoon, 2019, ARXIV190403758; Li Ke, 2016, ARXIV160601885; Li Zhenguo, 2017, METASGD LEARNING LEA; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Martens J., 2010, ICML; Mi Fei, 2019, ARXIV190505644; Mishra Nikhil, 2017, ARXIV170703141; Mordatch Igor, 2018, CORR; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Naik D, 1992, INT JOINT C NEUR NET; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nichol Alex, 2018, ABS180302999 ARXIV; Nocedal J., 2000, SPRINGER SERIES OPER; Oreshkin B., 2018, TADAM TASK DEPENDENT, V31, P721; Pedregosa F., 2016, ARXIV160202355; Rajeswaran A., 2017, NIPS; Ravi Sachin, 2017, INT C LEARN REPR, V2, P5; Rusu Andrei A, 2018, ARXIV180705960; Santoro Adam, 2016, INT C MACH LEARN ICM; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Schulman J., 2015, INT C MACH LEARN ICM; Shaban Amirreza, 2018, CORR; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; Triantafillou Eleni, 2019, ARXIV190303096; Vinyals Oriol, 2016, NEURAL INFORM PROCES; Wang J.X., 2016, ARXIV161105763; Zhou F, 2018, ADV NEURAL INFORM PR; Zintgraf Luisa M, 2018, ARXIV181003642	60	97	97	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300011
C	Jin, C; Allen-Zhu, Z; Bubeck, S; Jordan, MI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jin, Chi; Allen-Zhu, Zeyuan; Bubeck, Sebastien; Jordan, Michael I.			Is Q-learning Provably Efficient?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BOUNDS	Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [7, 22]. The theoretical question of "whether model-free algorithms can be made sample efficient" is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret (O) over tilde(root H(3)SAT), where S and A are the numbers of states and actions, H is the number of steps per episode, and T is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single root H factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes root T regret without requiring access to a "simulator."	[Jin, Chi; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Allen-Zhu, Zeyuan; Bubeck, Sebastien] Microsoft Res, Redmond, WA USA	University of California System; University of California Berkeley; Microsoft	Jin, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	chijin@cs.berkeley.edu; zeyuan@csail.mit.edu; sebubeck@microsoft.com; jordan@cs.berkeley.edu	Jordan, Michael I/C-5253-2013	Jordan, Michael/0000-0001-8935-817X; Jin, Chi/0000-0002-2865-5610	DARPA program on Lifelong Learning Machines; Microsoft Research Gratis Traveler program	DARPA program on Lifelong Learning Machines; Microsoft Research Gratis Traveler program(Microsoft)	We thank Nan Jiang, Sham M. Kakade, Greg Yang and Chicheng Zhang for valuable discussions. This work was supported in part by the DARPA program on Lifelong Learning Machines, and Microsoft Research Gratis Traveler program.	AGRAWAL S, 2017, ADV NEURAL INFORM PR, P1184; [Anonymous], 2017, ARXIV170802596; [Anonymous], 2018, ARXIV180105039; Azar Mohammad, 2012, P 29 INT C MACH LEAR; Azar Mohammad, 2011, C NEUR INF PROC SYST, P2411; Azar MG, 2017, PR MACH LEARN RES, V70; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jiang Nan, 2016, ARXIV161009512; Kakade Sham, 2018, ABS180209184 ARXIV; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; KOENIG S, 1993, PROCEEDINGS OF THE ELEVENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P99; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V., 2013, ARXIV PREPRINT ARXIV; Osband I., 2014, GEN EXPLORATION VIA; Osband Ian, 2016, ABS160802732 ARXIV; Pong V., 2018, ARXIV180209081; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sidford A, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P770; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Watkins CJCH., 1989, THESIS	27	97	98	4	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304084
C	Dong, X; Chen, SY; Pan, SJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dong, Xin; Chen, Shangyu; Pan, Sinno Jialin			Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					How to develop slim and accurate deep neural networks has become crucial for real-world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. By controlling layer-wise errors properly, one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods. Codes of our work are released at: https://github.com/csyhhu/L-OBS.	[Dong, Xin; Chen, Shangyu; Pan, Sinno Jialin] Nanyang Technol Univ, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Dong, X (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	n1503521a@e.ntu.edu.sg; schen025@e.ntu.edu.sg; sinnopan@ntu.edu.sg	PAN, Sinno Jialin/P-6696-2014; Jeong, Yongwook/N-7413-2016	PAN, Sinno Jialin/0000-0001-6565-3836; 	NTU Singapore Nanyang Assistant Professorship (NAP) grant [M4081532.020]; Singapore MOE AcRF Tier-2 grant [MOE2016-T2-2-060]; Singapore MOE AcRF Tier-1 grant [2016-T1-001-159]	NTU Singapore Nanyang Assistant Professorship (NAP) grant(Nanyang Technological University); Singapore MOE AcRF Tier-2 grant(Ministry of Education, Singapore); Singapore MOE AcRF Tier-1 grant(Ministry of Education, Singapore)	This work is supported by NTU Singapore Nanyang Assistant Professorship (NAP) grant M4081532.020, Singapore MOE AcRF Tier-2 grant MOE2016-T2-2-060, and Singapore MOE AcRF Tier-1 grant 2016-T1-001-159.	Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; de Vivo L, 2017, SCIENCE, V355, P507, DOI 10.1126/science.aah5982; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Glorot X., 2011, P 14 INT C ART INT S, P315; Gong Yunchao, 2014, ARXIV14126115; Guo YW, 2016, ADV NEUR IN, V29; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu H., 2016, ARXIV PREPRINT ARXIV; Jin Xiaojie, 2016, ARXIV160705423; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Nguyen N., 2016, J MACHINE LEARNING R; Rockafellar R., 1997, CONVEX ANAL PRINCETO; Sanjoy D., 2013, INT C MACH LEARN, P1319, DOI DOI 10.5555/3042817.3043084; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Sun Y, 2016, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR.2016.525; Tai C., 4 INT C LEARN REPR I; Wolfe N., 2017, ARXIV170104465	27	97	102	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404090
C	Mejjati, YA; Richardt, C; Tompkin, J; Cosker, D; Kim, KI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mejjati, Youssef A.; Richardt, Christian; Tompkin, James; Cosker, Darren; Kim, Kwang In			Unsupervised Attention-guided Image-to-Image Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Current unsupervised image-to-image translation techniques struggle to focus their attention on individual objects without altering the background or the way multiple objects interact within a scene. Motivated by the important role of attention in human perception, we tackle this limitation by introducing unsupervised attention mechanisms that are jointly adversarially trained with the generators and discriminators. We demonstrate qualitatively and quantitatively that our approach attends to relevant regions in the image without requiring supervision, which creates more realistic mappings when compared to those of recent approaches.	[Mejjati, Youssef A.; Richardt, Christian; Cosker, Darren; Kim, Kwang In] Univ Bath, Bath, Avon, England; [Tompkin, James] Brown Univ, Providence, RI 02912 USA	University of Bath; Brown University	Mejjati, YA (corresponding author), Univ Bath, Bath, Avon, England.	yam28@bath.ac.uk; christian@richardt.name; james_tompkin@brown.edu; D.P.Cosker@bath.ac.uk; k.kim@bath.ac.uk		Cosker, Darren/0000-0001-5177-4741; Richardt, Christian/0000-0001-6716-9845	Marie Sklodowska-Curie grant [665992]; UK's EPSRC Centre for Doctoral Training in Digital Entertainment (CDE) [EP/L016540/1]; RCUK [EP/M023281/1]	Marie Sklodowska-Curie grant; UK's EPSRC Centre for Doctoral Training in Digital Entertainment (CDE)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); RCUK(UK Research & Innovation (UKRI))	Youssef A. Mejjati thanks the Marie Sklodowska-Curie grant agreement No 665992, and the UK's EPSRC Centre for Doctoral Training in Digital Entertainment (CDE), EP/L016540/1. Kwang In Kim, Christian Richardt, and Darren Cosker thank RCUK EP/M023281/1.	[Anonymous], 2016, NIPS; [Anonymous], 2018, CVPR; [Anonymous], 2017, ICCV; Bottou L., 2017, ICML; Cao Y., 2017, ECML PKDD; Chen Xinyuan, 2018, ECCV; Dougal J., 2018, ICLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heusel M., 2017, NIPS; Huang Xun, 2018, ECCV; Isola P., 2017, CVPR; Jetley S., 2018, ICLR; Kim T., 2017, JMLR; Kuen Jason, 2016, CVPR; Lampert C. H., 2009, CVPR; Ledig C., 2017, CVPR; Li C., 2016, ECCV; Liu M.Y., 2016, NIPS; Liu M. -Y., 2017, NIPS; Liu Wei, 2015, CVPR; Ma S., 2018, ARXIV180206454; Mao X., 2017, ICCV; Mariani G., 2018, BAGAN DATA AUGMENTAT, P1, DOI 10.48550/ARXIV.1803.09655; Mnih V., 2014, NIPS; Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667; Szegedy C., 2016, CVPR; Wang F., 2017, CVPR; Wu B., 2017, ARXIV171205927; Yang Jianwei, 2017, INT C LEARN REPR; Yi Z., 2017, ICCV; Zhang H., 2018, ARXIV180508318	31	96	97	3	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303067
C	Singh, S; Bertsekas, D		Mozer, MC; Jordan, MI; Petsche, T		Singh, S; Bertsekas, D			Reinforcement learning for dynamic channel allocation in cellular telephone systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In cellular telephone systems, an important problem is to dynamically allocate the communication resource (channels) so as to maximize service in a stochastic caller environment. This problem is naturally formulated as a dynamic programming problem and we use a reinforcement learning (RL) method to find dynamic channel allocation policies that are better than previous heuristic solutions. The policies obtained perform well for a broad variety of call traffic patterns. We present results on a large cellular system with approximately 49(49) states.			Singh, S (corresponding author), UNIV COLORADO,DEPT COMP SCI,BOULDER,CO 80309, USA.								0	96	96	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						974	980						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00137
C	Arora, S; Du, SS; Hu, W; Li, ZY; Salakhutdinov, R; Wang, RS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arora, Sanjeev; Du, Simon S.; Hu, Wei; Li, Zhiyuan; Salakhutdinov, Ruslan; Wang, Ruosong			On Exact Computation with an Infinitely Wide Neural Net	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its "width"-namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers - is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width. The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for performance of a pure kernel-based method on CIFAR-10, being 10% higher than the methods reported in [Novak et al., 2019], and only 6% lower than the performance of the corresponding finite deep net architecture (once batch normalization etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.	[Arora, Sanjeev; Hu, Wei; Li, Zhiyuan] Princeton Univ, Princeton, NJ 08544 USA; [Arora, Sanjeev; Du, Simon S.] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA; [Salakhutdinov, Ruslan; Wang, Ruosong] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Princeton University; Institute for Advanced Study - USA; Carnegie Mellon University	Arora, S (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.; Arora, S (corresponding author), Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.	arora@cs.princeton.edu; ssdu@ias.edu; huwei@cs.princeton.edu; zhiyuanli@cs.princeton.edu; rsalakhu@cs.cmu.edu; ruosongw@andrew.cmu.edu	Hu, Wei/AHE-7065-2022; li, zhiyuan/HGD-9581-2022		NSF [IIS-1763562]; ONR; Simons Foundation; Schmidt Foundation; Mozilla Research; Amazon Research; DARPA; SRC; Office of Naval Research [N000141812861]; Nvidia NVAIL award	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Simons Foundation; Schmidt Foundation; Mozilla Research; Amazon Research; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); SRC; Office of Naval Research(Office of Naval Research); Nvidia NVAIL award	We thank Jason D. Lee, Haochuan Li and Xiyu Zhai for useful discussions. S. Arora, W. Hu and Z. Li are supported by NSF, ONR, Simons Foundation, Schmidt Foundation, Mozilla Research, Amazon Research, DARPA and SRC. R. Salakhutdinov and R. Wang are supported in part by NSF IIS-1763562, Office of Naval Research grant N000141812861, and Nvidia NVAIL award. We thank Amazon Web Services for providing compute time for the experiments in this paper.	Allen-Zhu Zeyuan, 2018, ARXIV181104918; [Anonymous], ARXIV181103962; [Anonymous], 2019, ARXIV190405526; [Anonymous], 2018, ARXIV180801204; Arora Sanjeev, 2019, ARXIV190108584; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; CAO Y., 2019, ARXIV190201384; Chizat L., 2018, NOTE LAZY TRAINING S; Cho Y., 2009, NIPS, P342; Daniely Amit, 2017, ARXIV PREPRINT ARXIV; Du S.S., 2019, ARXIV191003016; Du SS, 2019, PR MACH LEARN RES, V97; Du Simon S, 2018, GRADIENT DESCENT FIN; Du Simon S, 2018, ADV NEURAL INFORM PR, V31, P382; Garriga-Alonso Adria, 2019, INT C LEARN REPR; Hazan T., 2015, ARXIV150805133; Jacot A., 2018, ARXIV180607572; Lee J., 2019, ARXIV190206720; Lee J, 2018, ROUTL STUD INT BUS W, P1; Mairal J, 2014, ADV NEURAL INFORM PR, V27, P2627; van der Wilk M., 2017, ADV NEURAL INFORM PR, P2849; Yang G., 2019, ARXIV190204760; Yang Greg, 2019, INT C LEARN REPR; Zhang C., 2017, P INT C LEARN REPR I; Zou D, 2018, ARXIV181108888	30	95	95	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308019
C	Teh, YW; Bapst, V; Czarnecki, WM; Quan, J; Kirkpatrick, J; Hadsell, R; Heess, N; Pascanu, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Teh, Yee Whye; Bapst, Victor; Czarnecki, Wojciech Marian; Quan, John; Kirkpatrick, James; Hadsell, Raia; Heess, Nicolas; Pascanu, Razvan			Distral: Robust Multitask Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust to hyperparameter settings and more stable-attributes that are critical in deep reinforcement learning.	[Teh, Yee Whye; Bapst, Victor; Czarnecki, Wojciech Marian; Quan, John; Kirkpatrick, James; Hadsell, Raia; Heess, Nicolas; Pascanu, Razvan] DeepMind, London, England		Teh, YW (corresponding author), DeepMind, London, England.		Jeong, Yongwook/N-7413-2016					Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bengio Y., 2012, P ICML WORKSH UNS TR, P17, DOI DOI 10.1109/83.902291; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Bucila Cristian, 2006, P INT C KNOWL DISC D; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fox R., 2016, UNCERTAINTY ARTIFICI; Fox Roy, 2016, EUR WORKSH REINF LEA; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Haarnoja T, 2017, PR MACH LEARN RES, V70; Hinton G., 2015, NIPS WORKSH, P1; Kappen HJ, 2012, MACH LEARN, V87, P159, DOI 10.1007/s10994-012-5278-7; Lample G., 2017, PLAYING FPS GAMES DE; Levine S, 2016, J MACH LEARN RES, V17; Levine S, 2014, ADV NEUR IN, V27; Levine S, 2014, PR MACH LEARN RES, V32, P829; Levine Sergey, 2013, P 26 INT C NEUR INF, P207; Mirowski Piotr, 2016, INT C LEARN REPR ICL; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Parisotto Emilio, 2016, 4 INT C LEARN REPR I; Pascanu R., 2014, ICLR; Rawlik Konrad, 2012, ROBOTICS SCI SYSTEMS; RUSU A A, 2016, 4 INT C LEARN REPR I, P1; Schaul T, 2015, P INT C LEARN REPR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2017, EQUIVALENCE POLICY; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Taylor ME, 2011, AI MAG, V32, P15, DOI 10.1609/aimag.v32i1.2329; Toussaint M., 2006, EDIINFRR00934; Van Hasselt Hado, 2016, P AAAI C ART INT, V30; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang S., 2015, NEURAL INFORM PROCES, P685	35	95	98	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404055
C	Musco, C; Musco, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Musco, Cameron; Musco, Christopher			Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				LANCZOS METHOD; CONVERGENCE; EIGENVALUE; ALGORITHMS	Since being analyzed by Rokhlin, Szlam, and Tygert [1] and popularized by Halko, Martinsson, and Tropp [2], randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms, yet still converges quickly for any matrix, independently of singular value gaps. After (O) over tilde (1/epsilon) iterations, it gives a low-rank approximation within (1 + epsilon) of optimal for spectral norm error. We give the first provable runtime improvement on Simultaneous Iteration: a randomized block Krylov method, closely related to the classic Block Lanczos algorithm, gives the same guarantees in just (O) over tilde (1/root epsilon) iterations and performs substantially better experimentally. Our analysis is the first of a Krylov subspace method that does not depend on singular value gaps, which are unreliable in practice. Furthermore, while it is a simple accuracy benchmark, even (1 + epsilon) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods.	[Musco, Cameron; Musco, Christopher] MIT, EECS, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Musco, C (corresponding author), MIT, EECS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	cnmusco@mit.edu; cpmusco@mit.edu						Bauer Friedrich L., 1957, Z ANGEW MATH PHYS, V8, P214, DOI DOI 10.1007/BF01600502; Boutsidis C, 2014, SIAM J COMPUT, V43, P687, DOI 10.1137/12086755X; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen Michael B., 2015, P 47 ANN ACM S THEOR; Cullum J., 1974, 1974 IEEE C DEC CONT, P505; Davis TA, 2011, ACM T MATH SOFTWARE, V38, DOI [10.1145/2049662.2049670, 10.1145/2049662.2049663]; Golub G, 1977, MATH SOFTWARE, P361, DOI DOI 10.1016/B978-0-12-587260-7.50018-2; Golub G. H., 2012, MATRIX COMPUTATIONS; GOLUB GH, 1981, ACM T MATH SOFTWARE, V7, P149, DOI 10.1145/355945.355946; Gu Ming, 2014, ARXIV14082208; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Halko N, 2011, SIAM J SCI COMPUT, V33, P2580, DOI 10.1137/100804139; Halko NP, 2012, THESIS; Hall David, 2009, SCALANLP BREEZE; IBM Reseach Division Skylark Team, 2014, LIBSK SKETCH BAS DIS; Karnin Zohar, 2015, P 28 ANN C COMP LEAR, P505; KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066; Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Li RC, 2015, NUMER MATH, V131, P83, DOI 10.1007/s00211-014-0681-6; Liutkus Antoine, 2014, MATLAB CENTRAL FILE; Martinsson Per-Gunnar, 2010, NIPS WORKSH LOW RANK; Martinsson Per-Gunnar, 2006, 1361 YAL U; Musco C, 2015, ADV NEUR IN, V28; Okanohara Daisuke, 2010, REDSVD RANDOMIZED SV; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rennie Jason, 2015, 20 NEWSGROUPS; Rokhlin V, 2009, SIAM J MATRIX ANAL A, V31, P1100, DOI 10.1137/080736417; SAAD Y, 1980, SIAM J NUMER ANAL, V17, P687, DOI 10.1137/0717059; Saad Y, 2011, CLASS APPL MATH, V66, P1, DOI 10.1137/1.9781611970739; Sarlos T, 2006, P 47 ANN IEEE S FDN; Sou Kin Cheong, 2010, P 19 INT S MATH THEO; Szlam A., 2014, ARXIV14123510; Tulloch Andrew, 2014, FAST RANDOMIZED SING; Witten Rafi, 2014, ALGORITHMICA, V31, P1; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	36	95	96	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100103
C	Moreno, PJ; Ho, PP; Vasconcelos, N		Thrun, S; Saul, K; Scholkopf, B		Moreno, PJ; Ho, PP; Vasconcelos, N			A Kullback-Leibler divergence based kernel for SVM classification in multimedia applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Over the last years significant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classifiers such as SVM's. In this paper we suggest an alternative procedure to the Fisher kernel for systematically finding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identification/verification and image classification tasks and show that these new kernels have the best performance in speaker verification and mostly outperform the Fisher kernel based SVM's and the generative classifiers in speaker identification and image classification.	Hewlett Packard Corp, Cambridge Res Lab, Cambridge, MA 02142 USA	Hewlett-Packard	Moreno, PJ (corresponding author), Hewlett Packard Corp, Cambridge Res Lab, Cambridge, MA 02142 USA.			Vasconcelos, Nuno/0000-0002-9024-4302				BIMBOT F, 1995, SPEECH COMMUN, V17, P177, DOI 10.1016/0167-6393(95)00013-E; Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646; CHEN K, 2003, PATTRN RECOGNITION, P329; Jaakkola T., 1999, P INT C INT SYST MOL; MORENO PJ, 2000, ICASSP; SMITH N, 2001, CUEDFINFENGTR387; STERN RM, 1997, DARPA SPEECH REC WOR; Vapnik V.N, 1998, STAT LEARNING THEORY; VASCONCELOS N, 2000, IEEE INT C PATT REC; WAN V, 2000, IEEE P	10	95	97	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1385	1392						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500172
C	Dhurandhar, A; Chen, PY; Luss, R; Tu, CC; Ting, PS; Shanmugam, K; Das, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dhurandhar, Amit; Chen, Pin-Yu; Luss, Ronny; Tu, Chun-Chen; Ting, Paishun; Shanmugam, Karthikeyan; Das, Payel			Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily absent (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically absent is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.	[Dhurandhar, Amit; Chen, Pin-Yu; Luss, Ronny; Shanmugam, Karthikeyan; Das, Payel] IBM Res, Yorktown Hts, NY 10598 USA; [Tu, Chun-Chen; Ting, Paishun] Univ Michigan, Ann Arbor, MI 48109 USA	International Business Machines (IBM); University of Michigan System; University of Michigan	Dhurandhar, A (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	adhuran@us.ibm.com; pin-yu.chen@ibm.com; rluss@us.ibm.com; timtu@umich.edu; paishun@umich.edu; karthikeyan.shanmugam2@ibm.com; daspa@us.ibm.com	Chen, Pin-Yu/AAA-1059-2020					A. M. System, 2013, EXCL PART LIST; Nguyen A, 2016, ADV NEUR IN, V29; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Craddock RC, 2012, HUM BRAIN MAPP, V33, P1914, DOI 10.1002/hbm.21333; Dhurandhar A., 2017, ARXIV PREPRINT ARXIV; Dhurandhar  A., 2015, ACM SIGKDD C KNOWL D; Di Martino A, 2014, Mol Psychiatry, V19, P659, DOI 10.1038/mp.2013.78; Doshi-Velez F, 2017, ARXIV171101134; Gurumoorthy K.S., 2017, ARXIV170701212; Heinsfeld AS, 2018, NEUROIMAGE-CLIN, V17, P16, DOI 10.1016/j.nicl.2017.08.017; Herman  A., 2016, MED DAILY; Hull JV, 2017, FRONT PSYCHIATRY, V7, DOI 10.3389/fpsyt.2016.00205; Ide T, 2017, KNOWL INF SYST, V51, P235, DOI 10.1007/s10115-016-0976-2; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kim B, 2016, ADV NEUR IN, V29; Kindermans  P.-J., 2018, INT C LEARN REPR ICL; Lapuschkin S, 2016, J MACH LEARN RES, V17; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liska  A., 2017, ISMRM HON; Lundberg SM, 2017, ADV NEUR IN, V30; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Mousavi A, 2017, ANN ALLERTON CONF, P744; Nguyen A, 2016, ARXIV160203616, V29, P1; Nielsen JA, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00599; Oramas J., 2017, ARXIV171206302; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Ribeiro Marco Tulio, 2018, AAAI; Samek W., 2017, IEEE T NEURAL NETWOR; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Su  G., 2016, INTERPRETABLE 2 LEVE; Tejwani R., 2017, ARXIV171208041; Wang  F., 2015, IN AISTATS; Yannella P. N., 2018, ANAL ARTICLE 29 WORK; Yeo BTT, 2011, J NEUROPHYSIOL, V106, P1125, DOI 10.1152/jn.00338.2011; Zhang X., 2018, INTERPRETING NEURAL; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	41	94	94	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300055
C	Huang, WB; Zhang, T; Rong, Y; Huang, JZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huang, Wenbing; Zhang, Tong; Rong, Yu; Huang, Junzhou			Adaptive Sampling Towards Fast Graph Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ROBUST	Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.	[Huang, Wenbing; Rong, Yu; Huang, Junzhou] Tencent AI Lab, Bellevue, WA 98004 USA; [Zhang, Tong] Australian Natl Univ, Canberra, ACT, Australia	Australian National University	Huang, WB (corresponding author), Tencent AI Lab, Bellevue, WA 98004 USA.	hwenbing@126.com; tong.zhang@anu.edu.au; yu.rong@hotmail.com; joehhuang@tencent.com	Huang, Wenbing/AHB-1846-2022; Rong, Yu/HGU-8599-2022; Huang, Wenbing/AAI-7943-2021	Huang, Wenbing/0000-0002-2566-4159; Rong, Yu/0000-0001-7387-302X; Huang, Wenbing/0000-0002-2566-4159				Abadi M, 2015, P 12 USENIX S OPERAT; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Bruna J, 2013, PROC INT C LEARN REP; Chen JB, 2018, PR MACH LEARN RES, V80; Cheng JH, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188467; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Duvenaud David K, 2015, P NIPS; Fouss F, 2006, IEEE DATA MINING, P863; Fout A, 2017, ADV NEUR IN, V30; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Henaff M, 2015, ARXIV150605163; Kipf TN, 2016, P INT C LEARN REPR; Liu W, 2012, P IEEE, V100, P2624, DOI 10.1109/JPROC.2012.2197809; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Niepert M, 2016, PR MACH LEARN RES, V48; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Such FP, 2017, IEEE J-STSP, V11, P884, DOI 10.1109/JSTSP.2017.2726981; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Velickovi Petar, 2017, ARXIV171010903	25	94	96	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304056
C	ElHihi, S; Bengio, Y		Touretzky, DS; Mozer, MC; Hasselmo, ME		ElHihi, S; Bengio, Y			Hierarchical recurrent neural networks for long-term dependencies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MONTREAL,DEPT INFORMAT & RECH OPERAT,MONTREAL,PQ H3C 3J7,CANADA	Universite de Montreal									0	94	94	0	3	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						493	499						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00070
C	Singh, G; Gehr, T; Mirman, M; Puschel, M; Vechev, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Singh, Gagandeep; Gehr, Timon; Mirman, Matthew; Puschel, Markus; Vechev, Martin			Fast and Effective Robustness Certification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a new method and system, called DeepZ, for certifying neural network robustness based on abstract interpretation. Compared to state-of-the-art automated verifiers for neural networks, DeepZ: (i) handles ReLU, Tanh and Sigmoid activation functions, (ii) supports feedforward, convolutional, and residual architectures, (iii) is significantly more scalable and precise, and (iv) and is sound with respect to floating point arithmetic. These benefits are due to carefully designed approximations tailored to the setting of neural networks. As an example, DeepZ achieves a verification accuracy of 97% on a large network with 88; 500 hidden units under L-infinity attack with epsilon = 0:1 with an average runtime of 133 seconds.	[Singh, Gagandeep; Gehr, Timon; Mirman, Matthew; Puschel, Markus; Vechev, Martin] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Singh, G (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	gsingh@inf.ethz.ch; timon.gehr@inf.ethz.ch; matthew.mirman@inf.ethz.ch; pueschel@inf.ethz.ch; martin.vechev@inf.ethz.ch		Singh, Gagandeep/0000-0002-9299-2961				Athalye A., 2018, P INT C MACH LEARN I; Carlini N., 2017, ABS170910207 CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cousot P., 1977, S PRINC PROGR LANG P, P238, DOI [DOI 10.1145/512950.512973, 10.1145/512950.512973]; Dong Yinpeng, 2018, IEEE C COMP VIS PATT; Dvijotham K., 2018, P UNC ART INT UAI, P162; Ehlers R, 2017, LECT NOTES COMPUT SC, V10482, P269, DOI 10.1007/978-3-319-68167-2_19; Evtimov Ivan, 2017, P 2018 IEEE CVF C CO; Gehr T., 2018, 2018 IEEE S SEC PRIV, P948; Ghorbal K, 2009, LECT NOTES COMPUT SC, V5643, P627, DOI 10.1007/978-3-642-02658-4_47; Goodfellow I., 2015, P INT C LEARN REPR I; Gu S., 2014, ARXIV14125068; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kurakin A., 2016, ABS160702533 CORR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Madry Aleksander, 2018, P INT C LEARN REPR I; Mine A, 2004, LECT NOTES COMPUT SC, V2986, P3; Mirman Matthew, 2018, P INT C MACH LEARN I; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Raghunathan Aditi, 2018, P INT C MACH LEARN I P INT C MACH LEARN I; Singh G, 2017, ACM SIGPLAN NOTICES, V52, P46, DOI 10.1145/3093333.3009885; Szegedy C., 2013, ABS13126199 CORR; Tjeng V., 2017, ABS171107356 CORR; Wang SQ, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1599; Weng TW, 2018, PR MACH LEARN RES, V80; Wong  Eric, 2018, ABS180512514 CORR; Wong Eric, 2018, P INT C MACH LEARN I	29	93	96	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005039
C	Rocktaschel, T; Riedel, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rocktaschel, Tim; Riedel, Sebastian			End-to-End Differentiable Proving	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORKS	We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.	[Rocktaschel, Tim] Univ Oxford, Oxford, England; [Riedel, Sebastian] UCL, London, England; [Riedel, Sebastian] Bloomsbury AI, London, England	University of Oxford; University of London; University College London	Rocktaschel, T (corresponding author), Univ Oxford, Oxford, England.	tim.rocktaschel@cs.ox.ac.uk; s.riedel@cs.ucl.ac.uk	Jeong, Yongwook/N-7413-2016		Google PhD Fellowship in Natural Language Processing; Allen Distinguished Investigator Award; Marie Curie Career Integration Award	Google PhD Fellowship in Natural Language Processing(Google Incorporated); Allen Distinguished Investigator Award; Marie Curie Career Integration Award	We thank Pasquale Minervini, Tim Dettmers, Matko Bosnjak, Johannes Welbl, Naoya Inoue, Kai Arulkumaran, and the anonymous reviewers for very helpful comments on drafts of this paper. This work has been supported by a Google PhD Fellowship in Natural Language Processing, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award.	Abadi Martin, 2016, arXiv; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Andrychowicz M, 2016, ADV NEUR IN, V29; Artur S., 2012, NEURAL SYMBOLIC LEAR; Beltagy Islam, 2017, COMPUTATIONAL LINGUI; Bordes A., 2013, ADV NEURAL INFORM PR; Bosnjak M, 2017, PR MACH LEARN RES, V70; Bouchard Guillaume, 2015, P 2015 AAAI SPRING S; Broomhead D. S., 1988, Complex Systems, V2, P321; Chang Kai-Wei, 2014, P 2014 C EMP METH NA, P1568, DOI [DOI 10.3115/V1/D14-1165, 10.3115/v1/D14-1165]; Cohen W.W., 2016, CORR; Das Rajarshi, 2017, C EUR CHAPT ASS COMP; Demeester T., 2016, EMNLP, P1389, DOI 10.18653/v1/d16-1146; DING LY, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOLS 1-5, P3603, DOI 10.1109/ICSMC.1995.538347; Franca MVM, 2014, MACH LEARN, V94, P81, DOI 10.1007/s10994-013-5392-1; Gallaire H., 1978, ADV DATA BASE THEORY; Garcez ASA, 1999, APPL INTELL, V11, P59, DOI 10.1023/A:1008328630915; Gardner M., 2014, P 2014 C EMPIRICAL M, P397, DOI DOI 10.3115/V1/D14-1044; Gardner Matt, 2013, P 2013 C EMP METH NA, P833; Getoor Lise, 2007, INTRO STAT RELATIONA; Glorot X., 2010, PROC MACH LEARN RES, P249; Graves A., 2014, ARXIV14105401; Grefenstette E, 2015, ADV NEUR IN, V28; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Holldobler S., 1990, AAAI-90 Proceedings. Eighth National Conference on Artificial Intelligence, P587; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; Joulin A, 2015, ADV NEUR IN, V28; Kaliszyk Cezary, 2017, INT C LEARN REPR ICL; Kingma D.P, P 3 INT C LEARNING R; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Kok S., 2007, P 24 INT C MACH LEAR, P433; Komendantskaya E, 2011, LOG J IGPL, V19, P821, DOI 10.1093/jigpal/jzq012; Lao Ni, 2011, P C EMP METH NAT LAN, P529, DOI DOI 10.5555/2145432.2145494; Lao Ni, 2012, P 2012 JOINT C EMP M, P1017; Loos S. M., 2017, 21 INT C LOGIC PROGR, P85; Muggleton S., 1991, New Generation Computing, V8, P295, DOI 10.1007/BF03037089; Muggleton SH, 2015, MACH LEARN, V100, P49, DOI 10.1007/s10994-014-5471-y; Neelakantan A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P156; Neelakantan Arvind, 2016, INT C LEARN REPR ICL; Nickel M., 2012, P 21 INT C WORLD WID, P271; Nickel M, 2016, AAAI CONF ARTIF INTE, P1955; Peng B., 2015, CORR; QUINLAN JR, 1990, MACH LEARN, V5, P239, DOI 10.1007/BF00117105; Reed S, 2016, PR MACH LEARN RES, V48; Riedel Sebastian, 2013, P 2013 C N AM CHAPT, P74, DOI DOI 10.4218/ETRIJ.2018-0553; Rocktaschel T., 2014, ACL WORKSH SEM PARS; Rocktaschel Tim, 2015, P 2015 C N AM CHAPT, P1119, DOI [10.3115/v1/N15-1118, DOI 10.3115/V1/N15-1118]; Russell S., 2010, ARTIF INTELL, DOI DOI 10.1136/gutjnl-2018-317500; Schoenmackers S., 2010, P 2010 C EMPIRICAL M, P1088; Segler M. H. S., 2017, CORR; Serafini Luciano, 2016, ARXIV160604422; Shastri Lokendra, 1992, P 14 ANN C COGN SCI, V14, P159; Shavlik J. W., 1989, Connection Science, V1, P231, DOI 10.1080/09540098908915640; Shen Yelong, 2016, P WORKSH COGN COMP L; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Socher R., 2013, ADV NEURAL INFORM PR, V26, P1; Sourek G., 2015, P NIPS WORKSH COGN C; STICKEL ME, 1984, NEW GENERAT COMPUT, V2, P371, DOI 10.1007/BF03037328; Toutanova Kristina, 2015, P 2015 C EMP METH NA, P1499, DOI DOI 10.18653/V1/D15-1174; TOWELL GG, 1994, ARTIF INTELL, V70, P119, DOI 10.1016/0004-3702(94)90105-8; Trouillon T, 2016, PR MACH LEARN RES, V48; VANGELDER A, 1987, J LOGIC PROGRAM, V4, P23, DOI 10.1016/0743-1066(87)90020-3; Vendrov I., 2016, P INT C LEARN REPR; Wang WY, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P355; Weissenborn Dirk, 2016, CORR; Weston J., 2014, ARXIV14103916; Yang Bishan, 2015, 3 INT C LEARN REPR I; Yang Fan, 2017, CORR	69	93	94	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403083
C	Heess, N; Wayne, G; Silver, D; Lillicrap, T; Tassa, Y; Erez, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Heess, Nicolas; Wayne, Greg; Silver, David; Lillicrap, Timothy; Tassa, Yuval; Erez, Tom			Learning Continuous Control Policies by Stochastic Value Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment instead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.	[Heess, Nicolas; Wayne, Greg; Silver, David; Lillicrap, Timothy; Tassa, Yuval; Erez, Tom] Google DeepMind, London, England	Google Incorporated	Heess, N (corresponding author), Google DeepMind, London, England.	heess@google.com; gregwayne@google.com; davidsilver@google.com; countzero@google.com; tassa@google.com; etom@google.com						Abbeel P., 2006, ICML; Atkeson C. G., 2012, ACC; Baird L., 1995, P 12 INT C MACH LEAR, P30, DOI DOI 10.1016/B978-1-55860-377-6.50013-X; Balduzzi D., 2015, ARXIV150903005; Coulom R., 2002, THESIS; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Fairbank M., 2012, IJCNN; Fairbank M., 2014, THESIS; Grondman I., 2015, THESIS; Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC; JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1207/s15516709cog1603_1; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Munos R, 2006, J MACH LEARN RES, V7, P771; Narendra K S, 1990, IEEE Trans Neural Netw, V1, P4, DOI 10.1109/72.80202; Nguyen D. H., 1990, IEEE Control Systems Magazine, V10, P18, DOI 10.1109/37.55119; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317, DOI 10.1007/11564096_32; Schulman J., 2015, CORR; Silver D, 2014, PR MACH LEARN RES, V32; Singh S. P., 1994, ICML; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tassa Y., 2008, NIPS; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wawrzynski P, 2009, LECT NOTES COMPUT SC, V5495, P380, DOI 10.1007/978-3-642-04921-7_39; Wawrzynski P, 2009, NEURAL NETWORKS, V22, P1484, DOI 10.1016/j.neunet.2009.05.011; Werbos P. J., 1990, NEURAL NETWORKS CONT, P67; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	31	93	102	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101056
C	Thrun, S		Solla, SA; Leen, TK; Muller, KR		Thrun, S			Monte Carlo POMDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We present a Monte Carlo algorithm for learning to act in partially observable Markov decision processes (POMDPs) with real-valued state and action spaces. Our approach uses importance sampling for representing beliefs, and Monte Carlo approximation for belief propagation. A reinforcement learning algorithm, value iteration, is employed to learn value functions over belief states. Finally, a sample-based version of nearest neighbor is used to generalize across states. Initial empirical results suggest that our approach works well in practical applications.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Thrun, S (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.							Bellman RE, 1957, DYNAMIC PROGRAMMING; DAYAN P, 1993, TD LAMBDA CONVERGES, V1; FOX D, AAAI 99; Isard M., 1998, INT J COMPUTER VISIO; KAELBLING LP, 1996, REINFORCEMENT LEARNI, P4; KAELBLING LP, 1997, UNPUB PLANNING ACTIN; KANAZAWA K, UAI 95; LIN LJ, 1992, MACH LEARN, P8; LITTMAN ML, ICML 95; MOORE AW, 1997, AI REV, P11; ORMONEIT D, 1999, 19998 TR STANF U; Pitt M.K., 1999, J AM STAT ASS; SONDIK EJ, 1971, THESIS STANFORD; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tanner M. A, 1993, TOOLS STAT INFERENCE; Watkins C.J.C.H., 1989, THESIS CAMBRIDGE U; 1998, AAAI FALL S POMDPS	17	93	92	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1064	1070						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700150
C	Bjorck, J; Gomes, C; Selman, B; Weinberger, KQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bjorck, Johan; Gomes, Carla; Selman, Bart; Weinberger, Kilian Q.			Understanding Batch Normalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.	[Bjorck, Johan; Gomes, Carla; Selman, Bart; Weinberger, Kilian Q.] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Bjorck, J (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	njb225@cornell.edu; gomes@cornell.edu; selman@cornell.edu; kqw4@cornell.edu			NSF [CCF-1522054]; AFOSR [FA9550-18-1-0136, FA9550-17-1-0292]; National Science Foundation [III-1618134, III-1526012, IIS-1149882, IIS-1724282, TRIPODS-1740822]; Bill and Melinda Gates Foundation; Office of Naval Research; SAP America Inc.	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Science Foundation(National Science Foundation (NSF)); Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Office of Naval Research(Office of Naval Research); SAP America Inc.	We would like to thank Yexiang Xue, Guillaume Perez, Rich Bernstein, Zdzislaw Burda, Liam McAllister, Yang Yuan, Vilja Jarvi, Marlene Berke and Damek Davis for help and inspiration. This research is supported by NSF Expedition CCF-1522054 and Awards FA9550-18-1-0136 and FA9550-17-1-0292 from AFOSR. KQW was supported in part by the III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822 grants from the National Science Foundation, and generous support from the Bill and Melinda Gates Foundation, the Office of Naval Research, and SAP America Inc.	Arjovsky M, 2016, PR MACH LEARN RES, V48; Bertsekas D., 2015, CONVEX OPTIMIZATION; Bjorck J, 2018, ADV NEUR IN, V31; Chaudhari P, 2017, ARXIV171011029; Chaudhari P, 2016, ARXIV161101838; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Cooijmans T., 2016, ARXIV160309025; de Sa Chris, 2017, ADV MACHINE LEARNING; Gilmer Justin, 2016, INT C LEARN REPR; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goyal Priya, 2017, ARXIV170602677; Hardt M., 2016, ARXIV161104231; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ioffe Sergey, 2017, NEURIPS; Jastrzkebski Stanislaw, 2017, ARXIV171104623; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Laurent C, 2016, INT CONF ACOUST SPEE, P2657, DOI 10.1109/ICASSP.2016.7472159; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Liu DZ, 2016, ANN I H POINCARE-PR, V52, P1734, DOI 10.1214/15-AIHP696; Louart C., 2017, ARXIV170205419; Lu H., 2017, ARXIV PREPRINT ARXIV; Masters D., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.07612; Mishkin Dmytro, 2015, ICLR; Molina C. R. R., 2017, ARXIV171202609; Pennington J, 2017, PR MACH LEARN RES, V70; Pennington Jeffrey, NONLINEAR RANDOM MAT; RENEGAR J, 1995, SIAM J OPTIMIZ, V5, P506, DOI 10.1137/0805026; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; SAARINEN S, 1993, SIAM J SCI COMPUT, V14, P693, DOI 10.1137/0914044; Schilling F, 2016, EFFECT BATCH NORMALI; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Smith L. N., 2018, ARXIV180309820; Smith LN., 2017, ARTIF INTELL MACH LE; Smith S.L., 2018, P ICLR; Smith Samuel L, 2017, ARXIV171100489; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Ulyanov D., 2016, ARXIV160708022; van der Smagt P, 1998, LECT NOTES COMPUT SC, V1524, P193; Wu Shuang, 2018, ARXIV180209769; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xiao LC, 2018, PR MACH LEARN RES, V80; YuN C., 2017, ARXIV170702444; Zhang Chiyuan, 2016, ARXIV161103530; Zolezzi T, 2003, SIAM J OPTIMIZ, V14, P507, DOI 10.1137/S1052623402411885	58	92	93	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002026
C	Tran, B; Li, J; Madry, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tran, Brandon; Li, Jerry; Madry, Aleksander			Spectral Signatures in Backdoor Attacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A recent line of work has uncovered a new form of data poisoning: so-called backdoor attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by a perturbation planted by an adversary. In this paper, we identify a new property of all known backdoor attacks, which we call spectral signatures. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards designing ML systems secure against such backdoor attacks.	[Tran, Brandon; Madry, Aleksander] MIT, EECS, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Li, Jerry] Simons Inst, Berkeley, CA 94709 USA	Massachusetts Institute of Technology (MIT)	Tran, B (corresponding author), MIT, EECS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	btran@mit.edu; jerryzli@berkeley.edu; madry@mit.edu			NSF [CCF-1453261, CCF-1565235, CCF-1553428, CNS-1815221]; Google Faculty Research Award; NSF Graduate Research Fellowship; Alfred P. Sloan Research Fellowship; Google Research Award	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); NSF Graduate Research Fellowship(National Science Foundation (NSF)); Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); Google Research Award(Google Incorporated)	J.L. was supported by NSF Award CCF-1453261 (CAREER), CCF-1565235, and a Google Faculty Research Award. This work was done in part while the author was at MIT and an intern at Google Brain. B.T. was supported by an NSF Graduate Research Fellowship. A.M. was supported in part by an Alfred P. Sloan Research Fellowship, a Google Research Award, and the NSF grants CCF-1553428 and CNS-1815221.	Adi Y., 2018, ARXIV180204633; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Balakrishnan S., 2017, C LEARN THEOR PMLR, P169; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Carlini N, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P513; Chen Xinyun, 2017, ARXIV171205526; Collobert R., 2008, P 25 ICML, V25, P160, DOI DOI 10.1145/1390156.1390177; Diakonikolas I, 2018, ARXIV180302815; Diakonikolas I, 2017, PR MACH LEARN RES, V70; Donahue e. a., 2014, ICML; Evtimov Ivan, 2017, P 2018 IEEE CVF C CO; Faghri Fartash, 2016, ARXIV PREPRINT ARXIV; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Gu T., 2017, ARXIV170806733; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Klivans A., 2018, C LEARNING THEORY, P1420; Koh PW, 2017, PR MACH LEARN RES, V70; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Lai KA, 2016, ANN IEEE SYMP FOUND, P665, DOI 10.1109/FOCS.2016.76; Liu K, 2018, LECT NOTES COMPUT SC, V11050, P273, DOI 10.1007/978-3-030-00470-5_13; Madry Aleksander, 2017, ARXIV; Mei S., 2015, ARTIFICIAL INTELLIGE; Prasad A., 2018, ARXIV180206485; Shafahi Ali, 2018, ADV NEURAL INFORM PR, P3; Sharif M., 2016, ACM SIGSAC C COMP CO; Steinhardt Jacob, 2017, NEURIPS; Tramer F., 2017, ARXIV; Vershynin R, 2018, CAMBRIDGE SERIES STA, DOI DOI 10.1017/9781108231596	34	92	92	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002054
C	Lee, SW; Kim, JH; Jun, J; Ha, JW; Zhang, BT		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lee, Sang-Woo; Kim, Jin-Hwa; Jun, Jaehyun; Ha, Jung-Woo; Zhang, Byoung-Tak			Overcoming Catastrophic Forgetting by Incremental Moment Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSDBirds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.	[Lee, Sang-Woo; Kim, Jin-Hwa; Jun, Jaehyun; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea; [Ha, Jung-Woo] NAVER Corp, Clova AI Res, Seongnam, South Korea; [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea	Seoul National University (SNU)	Lee, SW (corresponding author), Seoul Natl Univ, Seoul, South Korea.	slee@bi.snu.ac.kr; jhkim@bi.snu.ac.kr; jhjun@bi.snu.ac.kr; jungwoo.ha@navercorp.com; btzhang@bi.snu.ac.kr	Ha, Jung-Woo/ABI-5223-2020	Ha, Jung-Woo/0000-0002-7400-7681; Kim, Jin-Hwa/0000-0002-0423-0415	Naver Corp.; Korean government [IITP-R0126-16-1072-SW. StarLab, IITP-2017-0-01772-VTT, KEIT-10044009-HRI. MESSI, KEIT-10060086-RISF]	Naver Corp.; Korean government(Korean Government)	The authors would like to thank Jiseob Kim, Min-Oh Heo, Donghyun Kwak, Insu Jeon, Christina Baek, and Heidi Tessmer for helpful comments and editing. This work was supported by the Naver Corp. and partly by the Korean government (IITP-R0126-16-1072-SW. StarLab, IITP-2017-0-01772-VTT, KEIT-10044009-HRI. MESSI, KEIT-10060086-RISF). Byoung-Tak Zhang is the corresponding author.	Amendola Carlos, 2017, ARXIV170205066; Baldi P., 2013, ADV NEURAL INFORM PR, V26, P2814, DOI DOI 10.17744/MEHC.25.2.XHYREGGXDCD0Q4NY; Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Ghahramani Z., 2000, NIPS WORKSH ONL LEAR; Goldberger J., 2005, ADV NEURAL INFORM PR, V17, P505; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Goodfellow IJ, 2014, 3 INT C LEARN REPR I; Huang Z, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3625; Huang Zhen, 2014, 15 ANN C INT SPEECH; Kienzle W., 2006, P 23 INT C MACH LEAR, P457, DOI DOI 10.1145/1143844.1143902; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee Sang- Woo, 2017, NEURAL NETWORKS; Lee Sang-Woo, 2016, IJCAI, P1669; Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37; Louizos C, 2016, PR MACH LEARN RES, V48; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Pascanu Razvan, 2013, ARXIV13013584; Pathak M. A., 2010, ADV NEURAL INFORM PR, V2, P1876; Rashwan A, 2016, JMLR WORKSH CONF PRO, V51, P1469; Ray S, 2005, ANN STAT, V33, P2042, DOI 10.1214/009053605000000417; Ray S, 2012, J MULTIVARIATE ANAL, V108, P41, DOI 10.1016/j.jmva.2012.02.006; Rusu A. A., 2016, ARXIV160604671; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310; Wah C., 2011, TECH REP; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang K, 2010, IEEE T NEURAL NETWOR, V21, P644, DOI 10.1109/TNN.2010.2040835	34	92	94	4	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404070
C	Lin, K; Li, DQ; He, XD; Zhang, ZY; Sun, MT		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lin, Kevin; Li, Dianqi; He, Xiaodong; Zhang, Zhengyou; Sun, Ming-Ting			Adversarial Ranking for Language Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.	[Lin, Kevin; Li, Dianqi; Sun, Ming-Ting] Univ Washington, Seattle, WA 98195 USA; [He, Xiaodong; Zhang, Zhengyou] Microsoft Res, Redmond, WA USA	University of Washington; University of Washington Seattle; Microsoft	Lin, K (corresponding author), Univ Washington, Seattle, WA 98195 USA.	kvlin@uw.edu; dianqili@uw.edu; xiaohe@microsoft.com; zhang@microsoft.com; mts@uw.edu	Lin, Kevin/AAL-5205-2020; zhang, zheng/HCH-9684-2022; Jeong, Yongwook/N-7413-2016	Lin, Kevin/0000-0001-8944-1336; 				Banerjee S., 2005, P ACL WORKSH INTR EX, P65; Bo Dai, 2017, ARXIV170306029; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Denton Emily L, 2015, NEURIPS, V2, P4; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Gehring J., 2017, P ICML; Goodfellow I. J., 2014, ARXIV14126515; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, ARXIV13080850; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333; Huszar Ferenc, 2015, ABS151105101 CORR; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Kusner Matt J, 2016, ARXIV161104051; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li Jiwei, 2017, P EMNLP; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S., IMPROVED IMAGE CAPTI; Liu T-Y., 2009, FOUND TRENDS INF RET, V3, P225, DOI DOI 10.1561/1500000016; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed Scott, 2016, P NIPS; Reschke Kevin, 2013, ACL; Salimans T, 2016, ADV NEUR IN, V29; Shakespeare William, 2014, COMPLETE WORKS W SHA; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Yang Z., 2017, ARXIV; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zhang X., 2015, ARXIV150201710; Zhang Xingxing, 2014, P EMNLP	37	92	96	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403022
C	Nagarajan, V; Kolter, JZ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nagarajan, Vaishnavh; Kolter, J. Zico			Gradient descent GAN optimization is locally stable	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the "gradient descent" form of GAN optimization, i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which is able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.	[Nagarajan, Vaishnavh; Kolter, J. Zico] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Nagarajan, V (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.	vaishnavh@cs.cmu.edu; zkolter@cs.cmu.edu	Jeong, Yongwook/N-7413-2016					Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Che Tong, 5 INT C LEARN REPR I; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3, P991, DOI 10.1109/72.165600; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Im D., 2016, GENERATING IMAGES RE; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Khalil HK., 2002, NONLINEAR SYSTEMS, Vthird; Kushner H. J., 2003, STOCHASTIC MODELLING, V35; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Magnus J. R., 1995, MATRIX DIFFERENTIAL; Mathieu Michael, 2016, ICLR; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Metz Luke, 2017, ICLR; Poole B., 2016, ARXIV161202780; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29	23	92	92	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405065
C	Kingma, DP; Salimans, T; Welling, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kingma, Diederik P.; Salimans, Tim; Welling, Max			Variational Dropout and the Local Reparameterization Trick	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.	[Kingma, Diederik P.; Welling, Max] Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands; [Salimans, Tim] Algoritmica, Amsterdam, Netherlands; [Welling, Max] Univ Calif Irvine, Irvine, CA USA; [Welling, Max] Canadian Inst Adv Res CIFAR, Toronto, ON, Canada	University of Amsterdam; University of California System; University of California Irvine; Canadian Institute for Advanced Research (CIFAR)	Kingma, DP (corresponding author), Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands.	D.P.Kingma@uva.nl; salimans.tim@gmail.com; M.Welling@uva.nl			Google European Fellowship in Deep Learning; NWO project in Natural AI [NAI.14.108]; Google; Facebook	Google European Fellowship in Deep Learning(Google Incorporated); NWO project in Natural AI; Google(Google Incorporated); Facebook(Facebook Inc)	We thank the reviewers and Yarin Gal for valuable feedback. Diederik Kingma is supported by the Google European Fellowship in Deep Learning, Max Welling is supported by research grants from Google and Facebook, and the NWO project in Natural AI (NAI.14.108).	Ahn S., 2012, ARXIV12066380; [Anonymous], [No title captured]; [Anonymous], ARXIV150205336; [Anonymous], 1993, P 6 ANN C COMPUTATIO, DOI DOI 10.1145/168304.168306; [Anonymous], 2010, P PYTH SCI COMP C SC; Ba J., 2013, ADV NEURAL INFORM PR, P3084; Bayer J, 2015, ARXIV150705331; Blundell C., 2015, ARXIV PREPRINT ARXIV; Gal Y., 2015, ARXIV150602142; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hinton GE, 2012, IMPROVING NEURAL NET; Kingma D. P., 2015, 3 INT C LEARN REPR I; Kingma D. P., 2013, ARXIV13060733; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Maeda S.-i., 2014, ARXIV PREPRINT ARXIV; Neal R. M., 2012, BAYESIAN LEARNING NE; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wan L., 2013, P INT C MACHINE LEAR, P1058; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	23	92	92	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100034
C	Bell-Kligler, S; Shocher, A; Irani, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bell-Kligler, Sefi; Shocher, Assaf; Irani, Michal			Blind Super-Resolution Kernel Estimation using an Internal-GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Super resolution (SR) methods typically assume that the low-resolution (LR) image was downscaled from the unknown high-resolution (HR) image by a fixed 'ideal' downscaling kernel (e.g. Bicubic downscaling). However, this is rarely the case in real LR images, in contrast to synthetically generated SR datasets. When the assumed downscaling kernel deviates from the true one, the performance of SR methods significantly deteriorates. This gave rise to Blind-SR - namely, SR when the downscaling kernel ("SR-kernel") is unknown. It was further shown that the true SR-kernel is the one that maximizes the recurrence of patches across scales of the LR image. In this paper we show how this powerful cross-scale recurrence property can be realized using Deep Internal Learning. We introduce "KernelGAN", an image-specific Internal-GAN [29], which trains solely on the LR test image at test time, and learns its internal distribution of patches. Its Generator is trained to produce a downscaled version of the LR test image, such that its Discriminator cannot distinguish between the patch distribution of the downscaled image, and the patch distribution of the original LR image. The Generator, once trained, constitutes the downscaling operation with the correct image-specific SR-kernel. KernelGAN is fully unsupervised, requires no training data other than the input image itself, and leads to state-of-the-art results in Blind-SR when plugged into existing SR algorithms.(1)	[Bell-Kligler, Sefi; Shocher, Assaf; Irani, Michal] Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel	Weizmann Institute of Science	Bell-Kligler, S (corresponding author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel.				European Research Council (ERC) under the Horizon 2020 research & innovation program [788535]	European Research Council (ERC) under the Horizon 2020 research & innovation program(European Research Council (ERC))	Project funded by the European Research Council (ERC) under the Horizon 2020 research & innovation program (grant No. 788535)	Agustsson Eirikur, 2017, IEEE C COMP VIS PATT; [Anonymous], 2013, ICCV; Arora S, 2018, PR MACH LEARN RES, V80; Begin Isabelle, 2004, INT C PATT REC ICPR; Cai Jianrui, 2019, REAL WORLD SINGLE IM; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713; He K., 2014, EUR C COMP VIS ECCV; He Y, 2009, IMAGE VISION COMPUT, V27, P364, DOI 10.1016/j.imavis.2008.05.010; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Joshi N, 2008, PROC CVPR IEEE, P3823; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kim J, 2016, IEEE CONF COMPUT; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Luo WJ, 2016, ADV NEUR IN, V29; Mao Xudong, 2017, COMP VIS ICCV IEEE I; Michaeli T., 2013, INT C COMP VIS ICCV; Riegler G, 2015, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2015.67; Saxe Andrew M, 2013, ARXIV13126120; Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329; Shocher Assaf, 2019, ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Timofte R, 2018, IEEE COMPUT SOC CONF, P965, DOI 10.1109/CVPRW.2018.00130; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Wang Q, 2005, IEEE I CONF COMP VIS, P709; Wang Xintao, 2018, DEEP POLYDENSE NETWO; Yifan W., 2018, CVPR WORKSH JUN; Yu Jiahui, 2018, IEEE C COMP VIS PATT; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zontak M, 2011, PROC CVPR IEEE, P977, DOI 10.1109/CVPR.2011.5995401	39	91	92	1	13	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300026
C	Tung, HYF; Tung, HW; Yumer, E; Fragkiadaki, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tung, Hsiao-Yu Fish; Tung, Hsiao-Wei; Yumer, Ersin; Fragkiadaki, Katerina			Self-supervised Learning of Motion Capture	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.	[Tung, Hsiao-Yu Fish; Fragkiadaki, Katerina] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Tung, Hsiao-Wei] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA; [Yumer, Ersin] Adobe Res, San Jose, CA USA	Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Adobe Systems Inc.	Tung, HYF (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	htung@cs.cmu.edu; hst11@pitt.edu; yumer@adobe.com; katef@cs.cmu.edu	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Alldieck T., 2017, CORR; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Balan Alexandru O., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383340; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bogo F., 2016, ECCV 2016; Brox T., 2006, ECCV; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carreira J., 2015, ARXIV150706550; Chen C., 2016, CORR; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Choo K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P321, DOI 10.1109/ICCV.2001.937643; Cortelazzo G.M., 2008, 3DPVT; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fleet D., 2001, P IEEE C COMP VIS PA; Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Handa A, 2016, LECT NOTES COMPUT SC, V9915, P67, DOI 10.1007/978-3-319-49409-8_9; He K., 2017, CORR; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Patraucean V., 2015, CORR; Pavlakos G., 2016, ARXIV161107828; Ramakrishna V, 2012, LECT NOTES COMPUT SC, V7575, P573, DOI 10.1007/978-3-642-33765-9_41; Rogez G, 2016, ADV NEUR IN, V29; Sfmnet F. K., 2017, SFM NET LEARNING STR; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Tung HYF, 2017, IEEE I CONF COMP VIS, P4364, DOI 10.1109/ICCV.2017.467; Urtasun R., 2006, P IEEE COMP SOC C CO; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Vicente S, 2014, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2014.13; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283	41	91	92	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405031
C	Roweis, S; Saul, LK; Hinton, GE		Dietterich, TG; Becker, S; Ghahramani, Z		Roweis, S; Saul, LK; Hinton, GE			Global coordination of local linear models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold-arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difficult problem. Our local linear models are represented by a mixture of factor analyzers, and the "global coordination" of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model's parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold-even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones.	Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	University of Toronto	Roweis, S (corresponding author), Univ Toronto, Dept Comp Sci, 100 Coll St, Toronto, ON, Canada.							BEYMER D, 1996, SCIENCE, V272; BISHOP CM, 1998, NEURAL COMPUTATION, V10; BREGLER C, 1995, ADV NEURAL INFORMATI, V7; DEMERS D, 1993, ADV NEURAL INFORMATI, V5; Ghahramani Zoubin, 1996, CRGTR961 U TOR; HINTON G, 1997, IEEE T NEURAL NETWOR, V8; Jordan M. I., 1999, MACHINE LEARNING, V37; KAMBHATLA N, 1997, NEURAL COMPUTATION, V9; ROWEIS ST, 2000, SCIENCE, V290; TENENBAUM JB, 2000, SCIENCE, V290	10	91	98	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						889	896						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100111
C	Dai, HJ; Khalil, EB; Zhang, YY; Dilkina, B; Song, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dai, Hanjun; Khalil, Elias B.; Zhang, Yuyu; Dilkina, Bistra; Song, Le			Learning Combinatorial Optimization Algorithms over Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.	[Dai, Hanjun; Khalil, Elias B.; Zhang, Yuyu; Dilkina, Bistra; Song, Le] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China	University System of Georgia; Georgia Institute of Technology	Dai, HJ (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	hanjun.dai@cc.gatech.edu; elias.khalil@cc.gatech.edu; yuyu.zhang@cc.gatech.edu; bdilkina@cc.gatech.edu; lsong@cc.gatech.edu	Jeong, Yongwook/N-7413-2016; Dai, Hanjun/AAQ-8943-2021		NSF [IIS-1218749, IIS-1639792 EAGER, CNS-1704701, CCF-1522054]; NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; ExxonMobil	NSF(National Science Foundation (NSF)); NIH BIGDATA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); Intel ISTC; NVIDIA; Amazon AWS; ExxonMobil(Exxon Mobil Corporation)	This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS. Dilkina is supported by NSF grant CCF-1522054 and ExxonMobil.	Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47; Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2010, PROC 16 ACM SIGKDD I, DOI DOI 10.1145/1835804.1835933; [Anonymous], 2013, ABS13125602 CORR; Applegate D., 2006, CONCORDE TSP SOLVER; Applegate D.L., 2011, TRAVELING SALESMAN P; Ba J., 2017, P 3 INT C LEARN REPR; BALAS E, 1980, COMBINATORIAL OPTIMI, V12, P37; Bello I., 2016, ARXIV PREPRINT ARXIV; Boyan Justin, 2000, J MACHINE LEARNING R, V1, P77; Chen Y., 2016, ARXIV161103824; Dai H., 2016, ICML; Du Nan, 2013, NIPS; ERDOS P, 1960, B INT STATIST INST, V38, P343; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Gu Shixiang, 2016, ARXIV161102247; He H., 2014, ADV NEURAL INFORM PR, P3293; IBM, 2014, CPLEX US MAN VERS 12; Johnson D.S., 2007, TRAVELING SALESMAN P, P369, DOI [10.1007/0-306-48213-49, DOI 10.1007/0-306-48213-49]; Karp RM., 1972, COMPLEXITY COMPUTER, P85; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Khalil E. B., 2017, 26 INT JOINT C ART I; Khalil Elias B., 2014, KNOWLEDGE DISCOVERY; Kleinberg J., 2006, ALGORITHM DESIGN; Lagoudakis M. G., 2001, ELECT NOTES DISCRETE, V9, P344, DOI DOI 10.1016/S1571-0653(04)00332-4; Li Ke, 2016, ARXIV160601885; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI; Peleg D., 1993, Proceedings of the 2nd Israel Symposium on Theory and Computing Systems (Cat. No.93TH0520-7), P69, DOI 10.1109/ISTCS.1993.253482; Reinelt G., 1991, ORSA Journal on Computing, V3, P376, DOI 10.1287/ijoc.3.4.376; Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317, DOI 10.1007/11564096_32; Samulowitz Horst, 2007, AAAI; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Zhang W., 2000, J ARTIF INTELL RES, V1, P1	38	90	90	17	43	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406041
C	Hsu, WN; Zhang, Y; Glass, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hsu, Wei-Ning; Zhang, Yu; Glass, James			Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.	[Hsu, Wei-Ning; Zhang, Yu; Glass, James] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Hsu, WN (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	wnhsu@csail.mit.edu; yzhang87@csail.mit.edu; glass@csail.mit.edu	Jeong, Yongwook/N-7413-2016					Ba J, 2014, ADV NEURAL INFORM PR; Bengio, 2016, ARXIV160901704; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307; Dehak N, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P1527; Dumoulin Vincent, 2016, ARXIV E PRINTS; Edwards Harrison, 2016, ICLR; Garofolo J. S., 1993, SPACE TERR INTEGR NE, V93, P27; Glass J., 2017, AUT SPEECH REC UND A; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742; Hermansky H., 2004, SPEECH PROCESSING AU, P309, DOI [10.1007/0-387-21575-1_6, DOI 10.1007/0-387-21575-1_6]; Higgins I, 2016, BETA VAE LEARNING BA; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsu WN, 2017, INTERSPEECH, P1273, DOI 10.21437/Interspeech.2017-349; Hu ZT, 2017, PR MACH LEARN RES, V70; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kinnunen T., 2017, ICASSP; Kulkarni TD, 2015, ADV NEUR IN, V28; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Li H., 2013, CHINASIP; Makhzani A., 2015, ARXIV151105644; Nakashika T, 2016, IEEE-ACM T AUDIO SPE, V24, P2032, DOI 10.1109/TASLP.2016.2593263; Oord A.V.D., 2016, SSW; PAUL DB, 1992, SPEECH AND NATURAL LANGUAGE, P357; Pearce D., 2002, TECH REP; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Sak H, 2014, INTERSPEECH, P338; Seide F, 2013, ARXIV PREPRINT ARXIV; Serban IV, 2017, AAAI CONF ARTIF INTE, P3288; Serdyuk Dmitriy, 2016, CORR; Shinohara Y, 2016, INTERSPEECH, P2369, DOI 10.21437/Interspeech.2016-879; van Amersfoort J. R., 2014, ARXIV14126581; van den Oord A, 2016, PR MACH LEARN RES, V48; Zhang Y, 2016, INT CONF ACOUST SPEE, P5755, DOI 10.1109/ICASSP.2016.7472780	42	90	90	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401088
C	Garipov, T; Izmailov, P; Podoprikhin, D; Vetrov, D; Wilson, AG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Garipov, Timur; Izmailov, Pavel; Podoprikhin, Dmitrii; Vetrov, Dmitry; Wilson, Andrew Gordon			Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.	[Garipov, Timur] Samsung AI Ctr Moscow, Moscow, Russia; [Garipov, Timur] Skolkovo Inst Sci & Technol, Moscow, Russia; [Izmailov, Pavel; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA; [Podoprikhin, Dmitrii] Natl Res Univ, Higher Sch Econ, Samsung HSE Lab, Moscow, Russia; [Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia	Skolkovo Institute of Science & Technology; Cornell University; HSE University (National Research University Higher School of Economics); HSE University (National Research University Higher School of Economics)	Garipov, T (corresponding author), Samsung AI Ctr Moscow, Moscow, Russia.				Ministry of Education and Science of the Russian Federation [14.756.31.0001]; Samsung Research, Samsung Electronics; NSF [IIS-1563887]; Facebook Research	Ministry of Education and Science of the Russian Federation(Ministry of Education and Science, Russian Federation); Samsung Research, Samsung Electronics(Samsung); NSF(National Science Foundation (NSF)); Facebook Research(Facebook Inc)	Timur Garipov was supported by Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001). Timur Garipov and Dmitrii Podoprikhin were supported by Samsung Research, Samsung Electronics. Andrew Gordon Wilson and Pavel Izmailov were supported by Facebook Research and NSF IIS-1563887.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Auer P, 1996, ADV NEUR IN, V8, P316; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Dinh L, 2017, PR MACH LEARN RES, V70; Draxler F, 2018, PR MACH LEARN RES, V80; Freeman C. D., 2017, INT C LEARN REPR; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Gotmare A, 2018, ARXIV180606977; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang Gao, 2017, ARXIV PREPRINT ARXIV; JONSSON H, 1998, CLASSICAL QUANTUM DY, DOI [10.1142/9789812839664_0016, DOI 10.1142/9789812839664_0016]; Keskar N.S., 2017, ICLR; Lee J. D., 2016, C LEARN THEOR, P1246; Lee S., 2016, ADV NEURAL INFORM PR, V29, P2119; Li Hao, 2017, ARXIV171209913; Loshchilov I., 2017, P INT C LEARNING REP; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Smith L. N., 2017, ARXIV170204283; Xie J., 2013, ARXIV PREPRINT ARXIV; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	22	89	89	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003035
C	Cohn, D; Hofmann, T		Leen, TK; Dietterich, TG; Tresp, V		Cohn, D; Hofmann, T			The missing link - A probabilistic model of document content and hypertext connectivity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.	Burning Glass Technol, Pittsburgh, PA 15213 USA		Cohn, D (corresponding author), Burning Glass Technol, 201 S Craig St,Suite 2W, Pittsburgh, PA 15213 USA.	david.cohn@burning-glass.com; th@cs.brown.edu						BHARAT K, 1998, P 21 ANN INT ACM SIG; BRIN S, 1998, ANATOMY LARGE SCALE; Cohn D., 2000, P 17 INT C MACH LEAR; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Getoor L, 2001, RELATIONAL DATA MINING, P307; HEARST M, 1994, P ACL JUN; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Kleinberg J., 1998, P 9 ACM SIAM S DISCR; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988	10	89	95	0	9	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						430	436						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800061
C	Graepel, T; Herbrich, R; Bollmann-Sdorra, P; Obermayer, K		Kearns, MS; Solla, SA; Cohn, DA		Graepel, T; Herbrich, R; Bollmann-Sdorra, P; Obermayer, K			Classification on pairwise proximity data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We investigate the problem of learning a classification task on data represented in terms of their pairwise proximities. This representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using Euclidean feature vectors, from which pairwise proximities can always be calculated. Our first approach is based on a combined linear embedding and classification procedure resulting in art extension of the Optimal Hyperplane algorithm to pseudo-Euclidean data. As an alternative we present another approach based on a linear threshold model in the proximity values themselves, which is optimized using Structural Risk Minimization. We show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics w.r.t. their generalization. Finally, the algorithms are successfully applied to protein structure data and to data from the cat's cerebral cortex. They show better performance than It-nearest-neighbor classification.	Tech Univ Berlin, Stat Res Grp, Sekr FR 6 9, D-10587 Berlin, Germany	Technical University of Berlin	Graepel, T (corresponding author), Tech Univ Berlin, Stat Res Grp, Sekr FR 6 9, Franklin Str 28-29, D-10587 Berlin, Germany.							BORG I, 1987, SPRINGER SERIES STAT, V13; BOSER B, 1992, P 5 ANN WORKSH COMP, V5, P144; Goldfarb L., 1985, PROGR PATTERN RECOGN, V2, P241; GRAEPEL T, 1998, IN PRESS NEURAL COMP; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; KLOCK H, 1997, ENERGY MINIMIZATION, V1223, P246; SCANNELL JW, 1995, J NEUROSCI, V15, P1463; Torgerson W.S., 1958, THEORY METHODS SCALI; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; WEINSHALL D, 1999, IN PRESS ADV NEURAL, V11	10	89	89	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						438	444						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700062
C	Janner, M; Fu, J; Zhang, M; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Janner, Michael; Fu, Justin; Zhang, Marvin; Levine, Sergey			When to Trust Your Model: Model-Based Policy Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.	[Janner, Michael; Fu, Justin; Zhang, Marvin; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Janner, M (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	janner@eecs.berkeley.edu; justinjfu@eecs.berkeley.edu; marvin@eecs.berkeley.edu; svlevine@eecs.berkeley.edu			NSF [IIS-1651843, IIS-1700697, IIS-1700696]; Office of Naval Research [ARL DCIST CRA W911NF-17-2-0181]; National Science Foundation; Open Philanthropy Project; NDSEG fellowship	NSF(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); National Science Foundation(National Science Foundation (NSF)); Open Philanthropy Project; NDSEG fellowship	We thank Anusha Nagabandi, Michael Chang, Chelsea Finn, Pulkit Agrawal, and Jacob Steinhardt for insightful discussions; Vitchyr Pong, Alex Lee, Kyle Hsu, and Aviral Kumar for feedback on an early draft of the paper; and Kristian Hartikainen for help with the SAC baseline. This research was partly supported by the NSF via IIS-1651843, IIS-1700697, and IIS-1700696, the Office of Naval Research, ARL DCIST CRA W911NF-17-2-0181, and computational resource donations from Google. M.J. is supported by fellowships from the National Science Foundation and the Open Philanthropy Project. M.Z. is supported by an NDSEG fellowship.	Asadi K, 2018, PR MACH LEARN RES, V80; Atkeson C. G., 1997, INT C ROB AUT; Clavera I., 2018, C ROB LEARN; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Depeweg S., 2016, INT C LEARN REPR; Draeger A., 1995, IEEE CONTROL SYSTEMS; Du Y., 2019, INT C MACH LEARN; Ebert E, 2018, ARXIV181200568; Farahmand A.-M., 2017, INT C ART INT STAT; Feinberg V., 2018, INT C MACH LEARN; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gu SX, 2016, PR MACH LEARN RES, V48; Haarnoja T, 2018, PR MACH LEARN RES, V80; Holland G. Z., 2018, ARXIV1806018252018; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Kaiser L., 2019, ARXIV190300374; Kalweit G., 2017, C ROB LEARN; Kumar V., 2016, INT C ROB AUT; Kurutach T., 2018, P INT C LEARN REPR; Levine Sergey, 2013, ICML; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Ma T., 2019, INT C LEARN REPR, P1; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nagabandi Anusha, 2018, INT C ROB AUT; Oh J, 2017, ADV NEUR IN, V30; Piche A., 2019, INT C LEARN REPR; Racanibre S., 2017, ADV NEURAL INFORM PR; Rajeswaran A., 2017, 5 INT C LEARN REPR; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Silver D, 2017, PR MACH LEARN RES, V70; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Szepesvari C., 2010, INT C MACH LEARN; Talvitie E., 2014, C UNC ART INT; Talvitie E., 2016, AAAI C ART INT; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109	45	88	88	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904020
C	Ren, J; Liu, PJ; Fertig, E; Snoek, J; Poplin, R; DePristo, MA; Dillon, JV; Lakshminarayanan, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ren, Jie; Liu, Peter J.; Fertig, Emily; Snoek, Jasper; Poplin, Ryan; DePristo, Mark A.; Dillon, Joshua V.; Lakshminarayanan, Balaji			Likelihood Ratios for Out-of-Distribution Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEQUENCE; CLASSIFICATION; DNA	Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.	[Ren, Jie; Liu, Peter J.; Fertig, Emily; Snoek, Jasper; Poplin, Ryan; DePristo, Mark A.; Dillon, Joshua V.] Google Res, Mountain View, CA 94043 USA; [Lakshminarayanan, Balaji] DeepMind, London, England	Google Incorporated	Ren, J (corresponding author), Google Res, Mountain View, CA 94043 USA.; Lakshminarayanan, B (corresponding author), DeepMind, London, England.	jjren@google.com; peterjliu@google.com; emilyaf@google.com; jsnoek@google.com; rpoplin@google.com; mdepristo@google.com; jvdillon@google.com; balajiln@google.com						Ahlgren NA, 2017, NUCLEIC ACIDS RES, V45, P39, DOI 10.1093/nar/gkw1002; Alemi A. A., 2018, UNCERTAINTY VARIATIO; Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300; Amodei D., 2016, CONCRETE PROBLEMS AI; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Bailey T L, 1995, Proc Int Conf Intell Syst Mol Biol, V3, P21; Bernard G, 2016, SCI REP-UK, V6, DOI 10.1038/srep28970; Bishop C. M., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P141; BISHOP CM, 1994, IEE P-VIS IMAGE SIGN, V141, P217, DOI 10.1049/ip-vis:19941330; BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108; Blauwkamp TA, 2019, NAT MICROBIOL, V4, P663, DOI 10.1038/s41564-018-0349-6; Brady A, 2009, NAT METHODS, V6, P673, DOI [10.1038/nmeth.1358, 10.1038/NMETH.1358]; Busia A., 2018, BIORXIV; Chan CX, 2014, SCI REP-UK, V4, DOI 10.1038/srep06504; Choi Hyunsun, 2018, WAIC WHY GENERATIVE; Eckburg PB, 2005, SCIENCE, V308, P1635, DOI 10.1126/science.1110591; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Gupta A, 2018, MOL INFORM, V37, DOI 10.1002/minf.201700111; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Hendrycks D., 2018, ARXIV181204606; Hildebrand F, 2010, PLOS GENET, V6, DOI 10.1371/journal.pgen.1001107; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jozefowicz Rafal, 2016, ARXIV160202410; Killoran N., 2017, GENERATING DESIGNING; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee K., 2017, ARXIV171109325; Lu YY, 2017, NUCLEIC ACIDS RES, V45, pW554, DOI 10.1093/nar/gkx351; LUHN HP, 1960, AM DOC, V11, P288, DOI 10.1002/asi.5090110403; Nalisnick E., 2019, ICML; Nalisnick Eric, 2018, INT C LEARN REPR; Nayfach S., 2019, NATURE, P1; Olivecrona M, 2017, J CHEMINFORMATICS, V9, DOI 10.1186/s13321-017-0235-x; Ovadia Y, 2019, ADV NEUR IN, V32; Patil KR, 2011, NAT METHODS, V8, P191, DOI 10.1038/nmeth0311-191; Ponsero AJ, 2019, FRONT MICROBIOL, V10, DOI 10.3389/fmicb.2019.00806; Reinert G, 2009, J COMPUT BIOL, V16, P1615, DOI 10.1089/cmb.2009.0198; Ren J., 2018, ARXIV180607810; Ren J, 2018, ANNU REV BIOMED DA S, V1, P93, DOI 10.1146/annurev-biodatasci-080917-013431; Rosen GL, 2011, BIOINFORMATICS, V27, P127, DOI 10.1093/bioinformatics/btq619; Salimans Tim, 2017, ARXIV170105517; Shafaei A., 2018, ARXIV180904729; SUEOKA N, 1962, P NATL ACAD SCI USA, V48, P582, DOI 10.1073/pnas.48.4.582; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Wagstaff K. L., 2012, ICML; Yarza P, 2008, SYST APPL MICROBIOL, V31, P241, DOI 10.1016/j.syapm.2008.07.001; Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/nmeth.3547, 10.1038/NMETH.3547]; Zhu Z., 2018, BIORXIV; Zou J, 2019, NAT GENET, V51, P12, DOI 10.1038/s41588-018-0295-5	56	88	88	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906037
C	Alistarh, D; Hoefler, T; Johansson, M; Khirirat, S; Konstantinov, N; Renggli, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Alistarh, Dan; Hoefler, Torsten; Johansson, Mikael; Khirirat, Sarit; Konstantinov, Nikola; Renggli, Cedric			The Convergence of Sparsified Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DESCENT	Stochastic Gradient Descent (SGD) has become the standard tool for distributed training of massive machine learning models, in particular deep neural networks. Several families of communication-reduction methods, such as quantization, large-batch methods, and gradient sparsification, have been proposed to reduce the overheads of distribution. To date, gradient sparsification methods-where each node sorts gradients by magnitude, and only communicates a subset of the components, accumulating the rest locally-are known to yield some of the largest practical gains. Such methods can reduce the amount of communication per step by up to three orders of magnitude, while preserving model accuracy. Yet, this family of methods currently has no theoretical justification. This is the question we address in this paper. We prove that, under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel SGD. The main insight is that sparsification methods implicitly maintain bounds on the maximum impact of stale updates, thanks to selection by magnitude. Our analysis also reveals that these methods do require analytical conditions to converge well, justifying and complementing existing heuristics.	[Alistarh, Dan; Konstantinov, Nikola] IST Austria, Klosterneuburg, Austria; [Hoefler, Torsten; Renggli, Cedric] Swiss Fed Inst Technol, Zurich, Switzerland; [Johansson, Mikael; Khirirat, Sarit] KTH, Stockholm, Sweden	Institute of Science & Technology - Austria; Swiss Federal Institutes of Technology Domain; ETH Zurich; Royal Institute of Technology	Alistarh, D (corresponding author), IST Austria, Klosterneuburg, Austria.	dan.alistarh@ist.ac.at; htor@inf.ethz.ch; mikaelj@kth.se; sarit@kth.se; nikola.konstantinov@ist.ac.at; cedric.renggli@inf.ethz.ch	Hoefler, Torsten/AAB-7478-2022	Alistarh, Dan/0000-0003-3650-940X	European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant [665385]	European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant	This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant Agreement No. 665385.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Aji A. F., 2017, P 2017 C EMP METH NA, P440; Alistarh  Dan, 2017, P NIPS 2017; Alistarh  Dan, 2018, ARXIV180308841; [Anonymous], 2017, ARXIV170606197; [Anonymous], 2018, ARXIV180208021; [Anonymous], 2015, ARXIV151201274; [Anonymous], [No title captured]; CHILIMBI TM, 2014, P OSDI, V14, P571; De Sa Ch., 2015, NIPS; Dryden N, 2016, PROCEEDINGS OF 2016 2ND WORKSHOP ON MACHINE LEARNING IN HPC ENVIRONMENTS (MLHPC), P1, DOI [10.1109/MLHPC.2016.4, 10.1109/MLHPC.2016.004]; Duchi J. C., 2015, ARXIV150800882; Grubic D., 2018, P 21 INT C EXTENDING, P145; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kyrola A., 2017, ABS170602677 ARXIV; Lin Y., 2017, ARXIV171201887; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Rastegari M., 2016, EUR C COMP VIS; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Seide F, 2014, INTERSPEECH, P1058; Seide F, 2014, INT CONF ACOUST SPEE; Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687; Strom N., 2015, 16 ANN C INT SPEECH; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Wangni J., 2017, ARXIV171009854; Wen W., 2017, NIPS 17, V30, P1509; Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014; You Yang, 2017, ARXIV170803888; Yu Dong, 2014, MSRTR2014112; Zhang  Jian, 2017, ARXIV170603471	33	88	89	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000047
C	Luo, YH; Cai, XR; Zhang, Y; Xu, J; Yuan, XJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Luo, Yonghong; Cai, Xiangrui; Zhang, Ying; Xu, Jun; Yuan, Xiaojie			Multivariate Time Series Imputation with Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MISSING DATA	Multivariate time series usually contain a large number of missing values, which hinders the application of advanced analysis methods on multivariate time series data. Conventional approaches to addressing the challenge of missing values, including mean/zero imputation, case deletion, and matrix factorization-based imputation, are all incapable of modeling the temporal dependencies and the nature of complex distribution in multivariate time series. In this paper, we treat the problem of missing value imputation as data generation. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we propose to learn the overall distribution of a multivariate time series dataset with GAN, which is further used to generate the missing values for each sample. Different from the image data, the time series data are usually incomplete due to the nature of data recording process. A modified Gate Recurrent Unit is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that the proposed model outperformed the baselines in terms of accuracy of imputation. Experimental results also showed that a simple model on the imputed data can achieve state-of-the-art results on the prediction tasks, demonstrating the benefits of our model in downstream applications.	[Luo, Yonghong; Cai, Xiangrui; Zhang, Ying; Yuan, Xiaojie] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China; [Xu, Jun] Renmin Univ China, Sch Informat, Beijing, Peoples R China	Nankai University; Renmin University of China	Zhang, Y (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.	luoyonghong@dbis.nankai.edu.cn; caixiangrui@dbis.nankai.edu.cn; yingzhang@nankai.edu.cn; junxu@ruc.edu.cn; yuanxj@nankai.edu.cn			National Natural Science Foundation of China [61772289, 61872338]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We thank the reviewers for their constructive comments. We also thank Zhicheng Dou for his helpful suggestions. This research is supported by National Natural Science Foundation of China (No. 61772289 and No. 61872338).	Acuna E, 2004, ST CLASS DAT ANAL, P639; Amiri M, 2016, NEUROCOMPUTING, V205, P152, DOI 10.1016/j.neucom.2016.04.015; [Anonymous], 2013, MATLAB REL 2013A, P488; [Anonymous], 2017, ABS170207983 CORR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Batista GEAPA, 2003, APPL ARTIF INTELL, V17, P519, DOI [10.1080/713827181, 10.1080/08839510390219309]; Bengio Y., 2014, ARXIV14061078; Bora Ashish, 2018, INT C LEARN REPR; Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9; Cheema JR, 2014, REV EDUC RES, V84, P487, DOI 10.3102/0034654314532697; Donders ART, 2006, J CLIN EPIDEMIOL, V59, P1087, DOI 10.1016/j.jclinepi.2006.01.014; Fedus William, 2018, INT C LEARN REPR; Garcia-Laencina PJ, 2015, COMPUT BIOL MED, V59, P125, DOI 10.1016/j.compbiomed.2015.02.006; Garcia-Laencina PJ, 2010, NEURAL COMPUT APPL, V19, P263, DOI 10.1007/s00521-009-0295-6; Gheyas IA, 2010, NEUROCOMPUTING, V73, P3039, DOI 10.1016/j.neucom.2010.06.021; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graham JW, 2009, ANNU REV PSYCHOL, V60, P549, DOI 10.1146/annurev.psych.58.110405.085530; Hastie T, 2015, J MACH LEARN RES, V16, P3367; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsieh TJ, 2011, APPL SOFT COMPUT, V11, P2510, DOI 10.1016/j.asoc.2010.09.007; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Johnson AEW, 2014, COMPUT CARDIOL CONF, V41, P157; Kaiser J., 2014, J SYST INTEGR, P42, DOI DOI 10.20470/JSI.V5I1.178; Kantardzic, 2011, DATA MINING CONCEPTS; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee DH, 2017, AAAI CONF ARTIF INTE, P4953; Liu P., 2017, ARXIV171109345; Liu S, 2017, ADV NEUR IN, V30; Lu Z., 2017, ARXIV170604717; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; McKnight P. E., 2007, MISSING DATA GENTLE; Mirza M., 2014, ARXIV; Nagarajan V, 2017, ADV NEUR IN, V30; Nelwamondo FV, 2007, CURR SCI INDIA, V93, P1514; Rajeswar Sai, 2017, ARXIV170510929; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Silva I, 2012, COMPUT CARDIOL CONF, V39, P245; Silva LO, 2014, INTELL DATA ANAL, V18, P1177, DOI 10.3233/IDA-140690; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wothke W, 2000, MODELING LONGITUDINAL AND MULTILEVEL DATA, P219; Yoon J, 2018, PR MACH LEARN RES, V80; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zheng KP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2171, DOI 10.1145/3097983.3098149	47	88	89	5	39	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301057
C	Sener, O; Koltun, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sener, Ozan; Koltun, Vladlen			Multi-Task Learning as Multi-Objective Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MINIMUM NORM POINT; ALGORITHM	In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.	[Sener, Ozan; Koltun, Vladlen] Intel Labs, Santa Clara, CA 95054 USA	Intel Corporation	Sener, O (corresponding author), Intel Labs, Santa Clara, CA 95054 USA.		Sener, Ozan/ABF-9436-2020					Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Bagherjeiran  A., 2005, INT C TOOLS ART INT; Bakker B, 2004, J MACH LEARN RES, V4, P83, DOI 10.1162/153244304322765658; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Bilen H, 2016, ADV NEUR IN, V29; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen Z, 2018, PR MACH LEARN RES, V80; Collobert R., 2008, P 25 ICML, V25, P160, DOI DOI 10.1145/1390156.1390177; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Desideri JA, 2012, CR MATH, V350, P313, DOI 10.1016/j.crma.2012.03.014; Dong DX, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1723; Ehrgott M., 2005, MULTICRITERIA OPTIMI; Fliege J, 2000, MATH METHOD OPER RES, V51, P479, DOI 10.1007/s001860000043; Ghosh  S., 2013, AAAI SPRING S LIF MA; Hashimoto Kazuma, 2017, P 2017 C EMP METH NA, P1923, DOI DOI 10.18653/V1/D17-1206; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hernandez-Lobato D, 2016, PR MACH LEARN RES, V48; Hospedales TM, 2016, ARXIV160604038; Huang JT, 2013, INT CONF ACOUST SPEE, P7304, DOI 10.1109/ICASSP.2013.6639081; Huang Z., 2015, INTERSPEECH; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Kaiser Lukasz, 2017, ARXIV170605137; Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781; Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579; Kuhn H., 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li  C., 2014, ARXIV14043190; Liu X., 2015, NAACL HLT; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long Mingsheng, 2015, ARXIV150602117; Luong Minh-Thang, 2015, ARXIV151106114; MAKIMOTO N, 1994, OPER RES LETT, V16, P33, DOI 10.1016/0167-6377(94)90019-1; Miettinen K., 1999, NONLINEAR MULTIOBJEC; Misra I, 2016, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2016.320; Parisi  S., 2014, IJCNN; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Peitz  S., 2018, NEO; Pirotta M, 2016, AAAI; Poirion F, 2017, COMPUT OPTIM APPL, V68, P317, DOI 10.1007/s10589-017-9921-x; Prudencio R. B. C., 2012, INT C SYST MAN CYB; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Roijers DM, 2013, J ARTIF INTELL RES, V48, P67, DOI 10.1613/jair.3987; Rosenbaum Clemens, 2017, ARXIV171101239; Rudd EM, 2016, LECT NOTES COMPUT SC, V9909, P19, DOI 10.1007/978-3-319-46454-1_2; Ruder Sebastian, 2017, ARXIV170605098; Sabour Sara, 2017, PROC 31 INT C NEURAL; Schaffler S, 2002, J OPTIMIZ THEORY APP, V114, P209, DOI 10.1023/A:1015472306888; SEKITANI K, 1993, MATH PROGRAM, V61, P233, DOI 10.1007/BF01582149; Seltzer ML, 2013, INT CONF ACOUST SPEE, P7398, DOI 10.1109/ICASSP.2013.6639100; Shah  A., 2016, ICML; Stein C., 1956, P BERK S MATH STAT P, V1, P197, DOI DOI 10.1525/9780520313880-018; WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381; Xue Y, 2007, J MACH LEARN RES, V8, P35; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhang Y, 2010, PROCEEDINGS OF THE ASME 29TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2010, VOL 6, P733; Zhou  D., 2017, IEEE ACCESS; Zhou J., 2011, MALSAR MULTITASK LEA; Zhou Jiayu, 2011, Adv Neural Inf Process Syst, V2011, P702	60	88	89	7	19	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300049
C	Blondel, M; Fujino, A; Ueda, N; Ishihata, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Blondel, Mathieu; Fujino, Akinori; Ueda, Naonori; Ishihata, Masakazu			Higher-Order Factorization Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks.	[Blondel, Mathieu; Fujino, Akinori; Ueda, Naonori] NTT Commun Sci Labs, Kyoto, Japan; [Ishihata, Masakazu] Hokkaido Univ, Sapporo, Hokkaido, Japan	Nippon Telegraph & Telephone Corporation; Hokkaido University	Blondel, M (corresponding author), NTT Commun Sci Labs, Kyoto, Japan.							Abadi M, 2015, P 12 USENIX S OPERAT; Baydin Atilim Gunes, 2015, ABS150205767 CORR; Blondel M, 2016, PR MACH LEARN RES, V48; Blondel M, 2015, LECT NOTES ARTIF INT, V9285, P19, DOI 10.1007/978-3-319-23525-7_2; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Li M., 2016, P INT C WEB SEARCH D; Livni R, 2014, ADV NEUR IN, V27; Menon AK, 2011, LECT NOTES ARTIF INT, V6912, P437, DOI 10.1007/978-3-642-23783-6_28; Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269; Pan V. Y., 2001, STRUCTURED MATRICES, P117, DOI [10.1007/978-1-4612-0129-8, DOI 10.1007/978-1-4612-0129-8]; Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771; Rendle S, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P635; Rendle Steffen, 2009, UAI; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1990, SPLINE MODELS OBSERV, V59, DOI [10.1137/1.9781611970128, DOI 10.1137/1.9781611970128]; Yamanishi Y, 2005, BIOINFORMATICS, V21, pI468, DOI 10.1093/bioinformatics/bti1012; Yang J., 2015, ARXIV150401697	20	88	93	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701001
C	Tresp, V		Leen, TK; Dietterich, TG; Tresp, V		Tresp, V			Mixtures of Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes - in particular in form of Gaussian process classification, the support vector machine and the MGP model - can be used for quantifying the dependencies in graphical models.	Siemens AG, Corp Technol, Dept Neural Computat, D-81730 Munich, Germany	Siemens AG; Siemens Germany	Tresp, V (corresponding author), Siemens AG, Corp Technol, Dept Neural Computat, Otto Hahn Ring 6, D-81730 Munich, Germany.							Fahrmeir L., 1994, MULTIVARIATE STAT MO, V425; FRIEDMAN N, 2000, P 16 C UNC ART INT U; Heckerman D., 2000, J MACHINE LEARNING R, V1; HOFMANN R, 1998, ADV NEURAL INFORMATI, V10; Hofmann R., 2000, THESIS; Jacobs R. A., 1991, NEURAL COMPUTATION, V3; SOLLICH P, 2000, ADV NEURAL INFORMATI, V12; TRESP V, 2000, P 6 ACM SIGKDD INT C; WILLIAMS CKI, 1998, IEEE T PATTERN ANAL, V20	9	88	93	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						654	660						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800093
C	Kearns, M; Singh, S		Kearns, MS; Solla, SA; Cohn, DA		Kearns, M; Singh, S			Finite-sample convergence rates for Q-learning and indirect algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In this paper, we address two issues of long-standing interest in the reinforcement learning literature. First, what kinds of performance guarantees can be made for Q-learning after only a finite number of actions? Second, what quantitative comparisons can be made between Q-learning and model-based (indirect) approaches, which use experience to estimate next-state distributions for off-line value iteration? We first show that both Q-learning and the indirect approach enjoy lather rapid convergence to the optimal policy as a function of the number of state transitions observed. In particular, on the order of only (N log(1/epsilon)/epsilon(2))(log(N) + log log(1/epsilon)) transitions are sufficient for both algorithms to come within epsilon of the optimal policy, in an idealized model that assumes the observed transitions are "well-mixed" throughout an N-state MDP. Thus, the two approaches have roughly the same sample complexity. Perhaps surprisingly, this sample complexity is far less than what is required for the model-based approach to actually construct a good approximation to the next-state distribution. The result also shows that the amount of memory required by the model-based approach is closer to N than to N-2. For either approach, to remove the assumption that the observed transitions are well-mixed, we consider a model in which the transitions are determined by a fixed, arbitrary exploration policy. Bounds on the number of transitions required in order to achieve a desired level of performance are then related to the stationary distribution and mixing time of this policy.	AT&T Labs Res, Florham Park, NJ 07932 USA	AT&T	Kearns, M (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Park, NJ 07932 USA.							MAHADEVAN S, 1992, MACHINE LEARNING; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Watkins C.J.C.H., 1989, LEARNING DELAYED REW	4	88	88	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						996	1002						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700140
C	Yoon, J; Jarrett, D; van der Schaar, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yoon, Jinsung; Jarrett, Daniel; van der Schaar, Mihaela			Time-series Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks (GANs) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction-which allow finer control over network dynamics-are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the flexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives, we encourage the network to adhere to the dynamics of the training data during sampling. Empirically, we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively, we find that the proposed framework consistently and significantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.	[Yoon, Jinsung; van der Schaar, Mihaela] Univ Calif Los Angeles, Los Angeles, CA 90095 USA; [Jarrett, Daniel; van der Schaar, Mihaela] Univ Cambridge, Cambridge, England; [van der Schaar, Mihaela] Alan Turing Inst, London, England	University of California System; University of California Los Angeles; University of Cambridge	Yoon, J (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.	jsyoon0823@g.ucla.edu; daniel.jarrett@maths.cam.ac.uk; mv472@cam.ac.uk			National Science Foundation (NSF) [1407712, 1462245, 1533983]; US Office of Naval Research (ONR)	National Science Foundation (NSF)(National Science Foundation (NSF)); US Office of Naval Research (ONR)(Office of Naval Research)	The authors would like to thank the reviewers for their helpful comments. This work was supported by the National Science Foundation (NSF grants 1407712, 1462245 and 1533983), and the US Office of Naval Research (ONR).	Aggarwal CC, 2017, PROC INT CONF DATA, P187, DOI 10.1109/ICDE.2017.71; Alzantot Moustafa, 2017, 2017 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), P188, DOI 10.1109/PERCOMW.2017.7917555; [Anonymous], 2013, PREPRINT ARXIV 1308; Bahdanau D., 2015, P 3 INT C LEARNING R; Bahdanau D., 2016, ARXIV160707086; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bryant F. B., 1995, PRINCIPAL COMPONENTS; Chen YZ, 2018, IEEE T POWER SYST, V33, P3265, DOI 10.1109/TPWRS.2018.2794541; Dai Andrew M., 2015, ADV NEURAL INFORM PR; Donahue C, 2018, ARXIV PREPRINT ARXIV; Dumoulin V., 2016, ARXIV160600704; Esteban C, 2017, REAL VALUED MEDICAL; Ganin Y., 2016, JMLR, V17, P2096; Harada S, 2018, IEEE ENG MED BIO, P368, DOI 10.1109/EMBC.2018.8512396; Janssen R., 2018, ARXIV181108295; Kim Yoon, 2017, ARXIV170604223; Konda VR, 2000, ADV NEUR IN, V12, P1008; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Li Yingzhen, 2018, ARXIV180302991; LYU X., 2018, ARXIV181200490; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Mirza M., 2014, ARXIV PREPRINT ARXIV; Mogren Olof, 2016, ARXIV161109904; Simonetto L, 2018, GENERATING SPIKING T; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; van Amersfoort J. R., 2014, ARXIV14126581; Van Den Oord A., 2016, SSW, V125; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Yoon J, 2019, INT C LEARN REPR INT C LEARN REPR; Zhang C, 2018, COMPLEXITY, V2018, P1, DOI DOI 10.1109/I2MTC.2018.8409794; Zhang Y., 2016, NIPS WORKSH ADV TRAI, V21	35	87	87	4	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305049
C	Fox, D		Dietterich, TG; Becker, S; Ghahramani, Z		Fox, D			KLD-sampling: Adaptive particle filters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Over the last years, particle filters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efficiency of particle filters by adapting the size of sample sets on-the-fly. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle filter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle filters with fixed sample set sizes and over a previously introduced adaptation technique.	Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Fox, D (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	fox@cs.washington.edu						[Anonymous], 1994, CONTINUOUS UNIVARIAT; Cox IJ, 1990, AUTONOMOUS ROBOT VEH; DELMORAL P, 2000, LECT NOTES MATH, V1729; Doucet A., 2001, SEQUENTIAL MONTE CAR; DOUCET A, 2000, STAT COMPUTING, V10; FOX D, 1999, P NAT C ART INT 1999; FOX D, 2001, SEQUENTIAL MONTE CAR; KOLLER D, 1998, P INT C MACH LEARN; MOORE AW, 1997, P INT C MACH LEARN I; Pelikan M., 2000, P GEN EV COMP C GECC; PITT MK, 1999, J AM STAT ASS, V94; Rice J. A., 1995, MATH STAT DATA ANAL	12	87	90	0	8	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						713	720						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100089
C	Scholkopf, B; Bartlett, P; Smola, A; Williamson, R		Kearns, MS; Solla, SA; Cohn, DA		Scholkopf, B; Bartlett, P; Smola, A; Williamson, R			Shrinking the tube: A new support vector regression algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				KERNELS	A new algorithm for Support Vector regression is described. For a priori chosen v, it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction v of the data points lie outside. Moreover, it is shown how to use parametric tube shapes with non-constant radius. The algorithm is analysed theoretically and experimentally.	GMD FIRST, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Scholkopf, B (corresponding author), GMD FIRST, Rudower Chaussee 5, D-12489 Berlin, Germany.	bs@first.gmd.de; Peter.Bartlett@anu.edu.au; smola@first.gmd.de; Bob.Williamson@anu.edu.au	Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925; Bartlett, Peter/0000-0002-8760-3140				Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Osuna EE, 1999, ADVANCES IN KERNEL METHODS, P271; SCHOLKOPF A, 1998, 1998031 TR; Scholkopf B, 1998, ADV NEUR IN, V10, P640; Scholkopf B, 1997, IEEE T SIGNAL PROCES, V45, P2758, DOI 10.1109/78.650102; SCHOLKOPF B, 1998, P 8 INT C ART NEUR N, P111; Scholkopf B., 1995, KDD; SMOLA AJ, 1998, P 8 INT C ART NEUR N, P105; Stitson MO, 1999, ADVANCES IN KERNEL METHODS, P285; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	10	87	88	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						330	336						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700047
C	Yang, ZL; Dai, ZH; Yang, YM; Carbonell, J; Salakhutdinov, R; Le, QV		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V.			XLNet: Generalized Autoregressive Pretraining for Language Understanding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.	[Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Dai, Zihang; Le, Quoc V.] Google AI Brain Team, Mountain View, CA USA	Carnegie Mellon University	Yang, ZL (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zhiliny@cs.cmu.edu; dzihang@cs.cmu.edu; yiming@cs.cmu.edu; jgc@cs.cmu.edu; rsalakhu@cs.cmu.edu; qvl@google.com			Office of Naval Research [N000141812861]; National Science Foundation (NSF) [IIS1763562]; Nvidia fellowship; Siebel scholarship; NSF [IIS-1546329]; DOE-Office of Science under the grant ASCR [KJ040201]	Office of Naval Research(Office of Naval Research); National Science Foundation (NSF)(National Science Foundation (NSF)); Nvidia fellowship; Siebel scholarship; NSF(National Science Foundation (NSF)); DOE-Office of Science under the grant ASCR	The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the project, Jamie Callan for providing the ClueWeb dataset, Youlong Cheng, Yanping Huang and Shibo Wang for providing ideas to improve our TPU implementation, Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and RS were supported by the Office of Naval Research grant N000141812861, the National Science Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY were supported in part by NSF under the grant IIS-1546329 and by the DOE-Office of Science under the grant ASCR #KJ040201.	Al-Rfou Rami, 2018, ARXIV180804444; [Anonymous], 2018, BAM BORN AGAIN MULTI; Baevski A., 2018, PROC INT C LEARN REP; Bengio Y, 2000, ADV NEUR IN, V12, P400; Callan J., 2009, CLUEWEB09 DATA SET; Dai Andrew M., 2015, ADV NEURAL INFORM PR; Dai Z., 2019, ARXIV190102860; Dai ZY, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P126, DOI 10.1145/3159652.3159659; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Fedus W, 2018, P INT C LEARNING REP; Germain M, 2015, PR MACH LEARN RES, V37, P881; Guo JF, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P55, DOI 10.1145/2983323.2983769; Howard J, 2018, ARXIV180106146; Johnson R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P562, DOI 10.18653/v1/P17-1052; Kocijan Vid, 2019, ARXIV190506290; Kudo T., EMNLP 2018, P66, DOI 10.18653/v1/D18-2012; Lai G., 2017, RACE LARGE SCALE REA; Lan Z., 2019, INT C LEARNING REPRE; Liu X., 2019, ARXIV190111504; Liu Yinhan, 2019, COMPUTING RES REPOSI; McCann Bryan, 2017, P ADV NEURAL INFORM, P6294; Miyato T., 2016, STAT-US; Pan Xiaoman, 2019, ARXIV190200993; Parker R, 2011, AR GIG; Peters Matthew, 2018, DEEP CONTEXTUALIZED, P2227, DOI [10.18653/v1/N18-1202, DOI 10.18653/V1/N18-1202]; Rajpurkar P., 2018, ARXIV180603822; Rajpurkar Pranav, 2016, P EMNLP; Sachan Devendra Singh, 2018, REVISITING ISTM NETW; Sutskever I., 2018, IMPROVING LANGUAGE U; Uria  B., 2016, J MACH LEARN RES, V17, P7184; van den Oord A., 2016, ARXIV160106759; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Xie Qizhe, 2019, ARXIV190412848, P2; Xiong CY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P55, DOI 10.1145/3077136.3080809; Yang Zhilin, 2017, ARXIV171103953; Zhang Shuailiang, 2019, ARXIV190109381; Zhang X., 2015, ADV NEURAL INFORM PR, P649; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	39	86	86	11	41	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305072
C	McIntosh, LT; Maheswaranathan, N; Nayebi, A; Ganguli, S; Baccus, SA		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		McIntosh, Lane T.; Maheswaranathan, Niru; Nayebi, Aran; Ganguli, Surya; Baccus, Stephen A.			Deep Learning Models of the Retinal Response to Natural Scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ADAPTATION; PREDICTION; SPIKING	A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.	[McIntosh, Lane T.; Maheswaranathan, Niru; Nayebi, Aran] Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA; [Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA; [Ganguli, Surya; Baccus, Stephen A.] Stanford Univ, Neurobiol Dept, Stanford, CA 94305 USA	Stanford University; Stanford University	McIntosh, LT (corresponding author), Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA.	lmcintosh@stanford.edu; nirum@stanford.edu; anayebi@stanford.edu; sganguli@stanford.edu; baccus@stanford.edu		Ganguli, Surya/0000-0002-9264-7551	NSF; NVIDIA Titan X Award; NEI; Burroughs Wellcome; James S. McDonnell Foundations; ONR; Sloan; McKnight; Simons	NSF(National Science Foundation (NSF)); NVIDIA Titan X Award; NEI(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); Burroughs Wellcome(Burroughs Wellcome Fund); James S. McDonnell Foundations; ONR(Office of Naval Research); Sloan(Alfred P. Sloan Foundation); McKnight; Simons	The authors would like to thank Ben Poole and EJ Chichilnisky for helpful discussions related to this work. Thanks also goes to the following institutions for providing funding and hardware grants, LM: NSF, NVIDIA Titan X Award, NM: NSF, AN and SB: NEI grants, SG: Burroughs Wellcome, Sloan, McKnight, Simons, James S. McDonnell Foundations and the ONR.	Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308; Baccus SA, 2002, NEURON, V36, P909, DOI 10.1016/S0896-6273(02)01050-4; Bastien F., 2012, DEEP LEARN UNS FEAT; Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411; Calvert PD, 2002, J GEN PHYSIOL, V119, P129, DOI 10.1085/jgp.119.2.129; Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Fairhall AL, 2006, J NEUROPHYSIOL, V96, P2724, DOI 10.1152/jn.00995.2005; Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639; Gollisch T, 2013, J PHYSIOL-PARIS, V107, P338, DOI 10.1016/j.jphysparis.2012.12.001; Gollisch T, 2010, NEURON, V65, P150, DOI 10.1016/j.neuron.2009.12.009; Heitman A., 2016, TESTING PSEUDO LINEA, DOI [10.1101/045336, DOI 10.1101/045336]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOCHSTEIN S, 1976, J PHYSIOL-LONDON, V262, P265, DOI 10.1113/jphysiol.1976.sp011595; Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689; Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kastner DB, 2011, NAT NEUROSCI, V14, P1317, DOI 10.1038/nn.2906; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Olveczky BP, 2003, NATURE, V423, P401, DOI 10.1038/nature01652; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005; Pitkow X, 2012, NAT NEUROSCI, V15, P628, DOI 10.1038/nn.3064; Poole B., 2014, ARXIV14061831; Roska B, 2001, NATURE, V410, P583, DOI 10.1038/35069068; Roska B, 2003, NAT NEUROSCI, V6, P600, DOI 10.1038/nn1061; Rust NC, 2005, NAT NEUROSCI, V8, P1647, DOI 10.1038/nn1606; Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021; Schwartz G, 2007, NAT NEUROSCI, V10, P552, DOI 10.1038/nn1887; vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805; Yamins D.L., 2013, ADV NEURAL INFORM PR, V26, P3093, DOI [DOI 10.5555/2999792.2999957, 10.5555/2999792.2999957]	32	86	86	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG	28729779				2022-12-19	WOS:000458973703024
C	Louizos, C; Shalit, U; Mooij, J; Sontag, D; Zemel, R; Welling, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Louizos, Christos; Shalit, Uri; Mooij, Joris; Sontag, David; Zemel, Richard; Welling, Max			Causal Effect Inference with Deep Latent-Variable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PROXY VARIABLES; ERRORS	Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.	[Louizos, Christos] Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands; [Shalit, Uri] NYU, CIMS, New York, NY 10003 USA; [Mooij, Joris] Univ Amsterdam, Amsterdam, Netherlands; [Sontag, David] MIT, CSAIL, Cambridge, MA 02139 USA; [Sontag, David] MIT, IMES, Cambridge, MA 02139 USA; [Zemel, Richard] Univ Toronto, CIFAR, Toronto, ON, Canada; [Welling, Max] Univ Amsterdam, CIFAR, Amsterdam, Netherlands	University of Amsterdam; New York University; University of Amsterdam; Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Canadian Institute for Advanced Research (CIFAR); University of Toronto; University of Amsterdam	Louizos, C (corresponding author), Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands.	c.louizos@uva.nl; uas1@nyu.edu; j.m.mooij@uva.nl; dsontag@mit.edu; zemel@cs.toronto.edu; m.welling@uva.nl	Jeong, Yongwook/N-7413-2016		TNO; NWO; Google; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [639466]	TNO; NWO(Netherlands Organization for Scientific Research (NWO)); Google(Google Incorporated); European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	We would like to thank Fredrik D. Johansson for valuable discussions, feedback and for providing the data for IHDP and Jobs. We would also like to thank Maggie Makar for helping with the Twins dataset. Christos Louizos and Max Welling were supported by TNO, NWO and Google. Joris Mooij was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement 639466).	Abadi M, 2015, P 12 USENIX S OPERAT; Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689; Almond D, 2005, Q J ECON, V120, P1031, DOI 10.1162/003355305774268228; Anandkumar A., 2012, C LEARN THEOR, P33; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1; Arora S, 2005, ANN APPL PROBAB, V15, P69, DOI 10.1214/105051604000000512; Arora S., 2016, CORR; Cai Z, 2008, P 24 C UNCERTAINTY A, P62; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Djork-Arn, ICLR 2016; Edwards JK, 2015, INT J EPIDEMIOL, V44, P1452, DOI 10.1093/ije/dyu272; Filmer D, 2001, DEMOGRAPHY, V38, P115, DOI 10.2307/3088292; FROST PA, 1979, REV ECON STAT, V61, P323, DOI 10.2307/1924606; Fuller W. A., 2009, MEASUREMENT ERROR MO; GOODMAN LA, 1974, BIOMETRIKA, V61, P215, DOI 10.2307/2334349; GREENLAND S, 1983, INT J EPIDEMIOL, V12, P93, DOI 10.1093/ije/12.1.93; Greenland S., 2008, MODERN EPIDEMIOLOGY, V3, P345; Gregor K., 2015, ARXIV E PRINTS; GRILICHES Z, 1986, J ECONOMETRICS, V31, P93, DOI 10.1016/0304-4076(86)90058-8; Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Jernite Yacine, 2013, NIPS, P2355; Johansson FD, 2016, PR MACH LEARN RES, V48; Kingma D.P, P 3 INT C LEARNING R; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kolenikov S, 2009, REV INCOME WEALTH, V55, P128, DOI 10.1111/j.1475-4991.2008.00309.x; KRUSKAL JB, 1976, PSYCHOMETRIKA, V41, P281, DOI 10.1007/BF02293554; Kuroki M., 2011, MEASUREMENT BIAS EFF; Kuroki M, 2014, BIOMETRIKA, V101, P423, DOI 10.1093/biomet/ast066; Louizos C., 2016, 4 INT C LEARN REPR I; Maaloe L, 2016, PR MACH LEARN RES, V48; Maddala G.S., 1992, INTRO ECONOMETRICS; Miao W, 2016, ARXIV160908816; Montgomery MR, 2000, DEMOGRAPHY, V37, P155, DOI 10.2307/2648118; Morgan SL, 2015, ANAL METHOD SOC RES, P1; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Pearl J., 2012, ARXIV12033504; Pearl Judea, 2015, SOCIOLOGICAL METHODS; Peysakhovich A., 2016, ARXIV161102385; Ranganath R, 2016, ADV NEUR IN, V29; Rezende D. Jimenez, 2016, ARXIV E PRINTS; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; SELEN J, 1986, J AM STAT ASSOC, V81, P75, DOI 10.2307/2287969; Shalit U., 2016, ARXIV E PRINTS; Smith JA, 2005, J ECONOMETRICS, V125, P305, DOI 10.1016/j.jeconom.2004.04.011; Thiesson B., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P504; Tran D., 2015, ARXIV151106499; Tran Dustin, 2016, ARXIV161009787; Wager S, 2015, ARXIV151004342; WICKENS MR, 1972, ECONOMETRICA, V40, P759, DOI 10.2307/1912971; Wooldridge JM, 2009, ECON LETT, V104, P112, DOI 10.1016/j.econlet.2009.04.026	54	85	87	1	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406050
C	Bousmalis, K; Trigeorgis, G; Silberman, N; Krishnan, D; Erhan, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bousmalis, Konstantinos; Trigeorgis, George; Silberman, Nathan; Krishnan, Dilip; Erhan, Dumitru			Domain Separation Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				KERNEL	The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.	[Bousmalis, Konstantinos; Trigeorgis, George; Erhan, Dumitru] Google Brain, Mountain View, CA 94040 USA; [Trigeorgis, George] Imperial Coll London, London, England; [Silberman, Nathan] Google Res, New York, NY USA; [Krishnan, Dilip] Google Res, Cambridge, MA USA	Google Incorporated; Imperial College London; Google Incorporated; Google Incorporated	Bousmalis, K (corresponding author), Google Brain, Mountain View, CA 94040 USA.	konstantinos@google.com; g.trigeorgis@imperial.ac.uk; nsilberman@google.com; dilipkay@google.com; dumitru@google.com	Trigeorgis, George/Y-8208-2019					Abadi M., TENSORFLOW LARGE SCA; Ajakan H., 2014, PREPRINT; [Anonymous], 2014, ABS14123474 CORR; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Batist J., 2015, CVPR; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Eigen David, 2014, NEURIPS; Ganin Y., 2015, ICML, P513; Ganin Y, 2016, J MACH LEARN RES, V17; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R., 2011, P 2011 INT C COMP VI; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hinterstoisser S., 2012, ACCV; Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2; Jia Y., 2010, NIPS, V10, P982; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long M., 2015, ICML; Mansour Y., 2009, NIPS; Moiseev B, 2013, LECT NOTES COMPUT SC, V8192, P576, DOI 10.1007/978-3-319-02895-8_52; Netzer Y., 2011, P DEEP LEARN UNS FEA; Olson Edwin, 2011, 2011 IEEE International Conference on Robotics and Automation, P3400; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saenko K., 2010, ECCV, P2; Salzmann Mathieu, 2010, P 13 INT C ART INT S, P701; Stallkamp Johannes, 2012, NEURAL NETWORKS; Sun B., 2016, AAAI; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Virtanen S., 2011, P 28 INT C MACH LEAR, P457, DOI [10.5555/3104482.3104540, DOI 10.5555/3104482.3104540]; Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930	31	85	85	2	20	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702001
C	Sensoy, M; Kaplan, L; Kandemir, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sensoy, Murat; Kaplan, Lance; Kandemir, Melih			Evidential Deep Learning to Quantify Classification Uncertainty	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of outof-distribution queries and endurance against adversarial perturbations.	[Sensoy, Murat] Ozyegin Univ, Dept Comp Sci, Istanbul, Turkey; [Kaplan, Lance] US Army, Res Lab, Adelphi, MD 20783 USA; [Kandemir, Melih] Bosch Ctr Artificial Intelligence, Robert Bosch Campus 1, D-71272 Renningen, Germany	Ozyegin University; United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL)	Sensoy, M (corresponding author), Ozyegin Univ, Dept Comp Sci, Istanbul, Turkey.	murat.sensoy@ozyegin.edu.tr; lkaplan@ieee.org; melih.kandemir@bosch.com	Kandemir, Melih/AAT-7435-2021		U.S. Army Research Laboratory [W911NF-16-2-0173]; U.K. Ministry of Defence [W911NF-16-3-0001]	U.S. Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); U.K. Ministry of Defence	This research was sponsored by the U.S. Army Research Laboratory and the U.K. Ministry of Defence under Agreement Number W911NF-16-3-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory, the U.S. Government, the U.K. Ministry of Defence or the U.K. Government. The U.S. and U.K. Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon. Also, Dr. Sensoy thanks to the U.S. Army Research Laboratory for its support under grant W911NF-16-2-0173.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Chen T., 2017, ICML; Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023; Ciresan Dan, 2012, ADV NEURAL INFORM PR, P2843, DOI DOI 10.5555/2999325.2999452; Dempster AP, 2008, STUD FUZZ SOFT COMP, V219, P73; Faghri Fartash, 2016, ARXIV PREPRINT ARXIV; Gal Y., 2016, P 33 INT C MACH LEAR, P1050, DOI DOI 10.5555/3045390.3045502; Gal Y, 2016, PR MACH LEARN RES, V48; Goodfellow IJ, 2014, 3 INT C LEARNING REP; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Houlsby N., 2012, NIPS, V3, P2096; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Josang A, 2016, ARTIF INTELL-FOUND, P1, DOI 10.1007/978-3-319-42337-1; Kandemir M., 2015, ICML; Kendall A., 2017, WHAT UNCERTAINTIES W, V3, P4; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kotz S., 2000, CONTINUOUS MULTIVARI, V2nd ed., DOI 10.1002/0471722065; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6405; LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19; Louizos C, 2017, PR MACH LEARN RES, V70; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; Molchanov D, 2017, PR MACH LEARN RES, V70; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Srivastava N., 2016, J MACHINE LEARNING R, V15, P1929; Tran Dustin, 2016, INT C LEARN REPR, V5; Welling M., 2015, NIPS; Wilson A. G., 2016, AISTATS	31	84	84	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303020
C	Oh, J; Singh, S; Lee, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Oh, Junhyuk; Singh, Satinder; Lee, Honglak			Value Prediction Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.	[Oh, Junhyuk; Singh, Satinder; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA; [Lee, Honglak] Google Brain, Mountain View, CA USA	University of Michigan System; University of Michigan; Google Incorporated	Oh, J (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	junhyuk@umich.edu; baveja@umich.edu; honglak@umich.edu			NSF [IIS-1526059]	NSF(National Science Foundation (NSF))	This work was supported by NSF grant IIS-1526059. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsor.	Abadi M, 2015, P 12 USENIX S OPERAT; [Anonymous], 2000, THESIS; [Anonymous], 2012, ARXIV12074708; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Chiappa S., 2017, ICLR; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Gu SX, 2016, PR MACH LEARN RES, V48; Guo X., 2016, IJCAI; Hausknecht Matthew, 2015, 2015 AAAI FALL S SER; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heess N., 2015, NIPS; Jaderberg Max, 2017, ICLR; Kalchbrenner N, 2016, ARXIV161000527; Kingma D.P, P 3 INT C LEARNING R; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Kulkarni T. D., 2016, ARXIV160602396; Lakshminarayanan A. S., 2017, AAAI; Lenz I, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Mishra N, 2017, PR MACH LEARN RES, V70; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Oh J, 2016, PR MACH LEARN RES, V48; Parisotto Emilio, 2017, ARXIV170208360; Silver D, 2017, PR MACH LEARN RES, V70; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Silver D, 2012, MACH LEARN, V87, P183, DOI 10.1007/s10994-012-5280-0; Stadie B. C., 2015, ARXIV150700814; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton R. S., 2008, UAI; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tamar A, 2016, ADV NEURAL INFORM PR, P2146; Vezhnevets A., 2016, NIPS; Wang ZY, 2016, PR MACH LEARN RES, V48; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Yao H., 2009, NIPS	39	84	84	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406019
C	Murphy, KP		Solla, SA; Leen, TK; Muller, KR		Murphy, KP			Bayesian map learning in dynamic environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We consider the problem of learning a grid-based map using a robot with noisy sensors and actuators. We compare two approaches: online EM, where the map is treated as a fixed parameter, and Bayesian inference, where the map is a (matrix-valued) random variable. We show that even on a very simple example, online EM can get stuck in local minima, which causes the robot to get "lost" and the resulting map to be useless. By contrast, the Bayesian approach, by maintaining multiple hypotheses, is much more robust. We then introduce a method for approximating the Bayesian solution, called Rao-Blackwellised particle filtering. We show that this approximation, when coupled with an active learning strategy, is fast but accurate.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Murphy, KP (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.							Boyen X., 1998, UAI; BOYEN X, 1998, NIPS; CHEN R, 1999, UNPUB MIXTURE KALMAN; DOUCET A, 1999, STAT COMPUTING; FOX D, 1998, ROBOTICS AUTONOMOUS; Fox D., 1999, AAAI; Gat E, 1998, ARTIF INTELL; KOENIG S, 1996, ICRA; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; SHATKAY H, 1997, IJCAI; Thrun S, 1998, MACH LEARN, V31, P29, DOI 10.1023/A:1007436523611	12	84	90	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1015	1021						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700143
C	Attias, H; Schreiner, CE		Mozer, MC; Jordan, MI; Petsche, T		Attias, H; Schreiner, CE			Temporal low-order statistics of natural sounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In order to process incoming sounds efficiently, it is advantageous for the auditory system to be adapted to the statistical structure of natural auditory scenes. As a first step in investigating the relation between the system and its inputs, we study low-order statistical properties in several sound ensembles using a filter bank analysis. Focusing on the amplitude and phase in different frequency bands, we find simple parametric descriptions for their distribution and power spectrum that are valid for very different types of sounds. In particular, the amplitude distribution has an exponential tail and its power spectrum exhibits a modified power-law behavior, which is manifested by self-similarity and long-range temporal correlations. Furthermore, the statistics for different bands within a given ensemble are virtually identical, suggesting translation invariance along the cochlear axis. These results show that natural sounds are highly redundant, and have possible implications to the neural code used by the auditory system.			Attias, H (corresponding author), UNIV CALIF SAN FRANCISCO,SLOAN CTR THEORET NEUROBIOL,SAN FRANCISCO,CA 94143, USA.			Schreiner, Christoph/0000-0002-4571-4328					0	84	86	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						27	33						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00004
C	Herdade, S; Kappeler, A; Boakye, K; Soares, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Herdade, Simao; Kappeler, Armin; Boakye, Kofi; Soares, Joao			Image Captioning: Transforming Objects into Words	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Image captioning models typically follow an encoder-decoder architecture which uses abstract image feature vectors as input to the encoder. One of the most successful algorithms uses feature vectors extracted from the region proposals obtained from an object detector. In this work we introduce the Object Relation Transformer, that builds upon this approach by explicitly incorporating information about the spatial relationship between input detected objects through geometric attention. Quantitative and qualitative results demonstrate the importance of such geometric attention for image captioning, leading to improvements on all common captioning metrics on the MS-COCO dataset.	[Herdade, Simao; Kappeler, Armin; Boakye, Kofi; Soares, Joao] Yahoo Res, San Francisco, CA 94103 USA		Herdade, S (corresponding author), Yahoo Res, San Francisco, CA 94103 USA.	sherdade@verizonmedia.com; akappeler@apple.com; kaboakye@verizonmedia.com; jvbsoares@verizonmedia.com						Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kiros R, 2014, PR MACH LEARN RES, V32, P595; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632; Luo R., 2017, IMAGE CAPTIONING COD; Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094; Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503	30	83	84	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902073
C	Jiang, H; Kim, B; Guan, MY; Gupta, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Heinrich; Kim, Been; Guan, Melody Y.; Gupta, Maya			To Trust Or Not To Trust A Classifier	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REJECT; CONFIDENCE	Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.	[Jiang, Heinrich; Gupta, Maya] Google Res, Mento Pk, CA USA; [Kim, Been] Google Brain, Mountain View, CA USA; [Guan, Melody Y.] Stanford Univ, Stanford, CA 94305 USA	Google Incorporated; Google Incorporated; Stanford University	Jiang, H (corresponding author), Google Res, Mento Pk, CA USA.	heinrichj@google.com; beenkim@google.com; mguan@stanford.edu; mayagupta@google.com						Amodei D., 2016, CORR; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2010, ADV NEURAL INFORM PR; [Anonymous], 2001, SPRINGE SER STAT N; Balakrishnan S., 2013, ADV NEURAL INFORM PR, P2679; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; Bengio Y., 2009, ADV NEURAL INFORM PR, V21; Chazal F., 2013, UPPER BOUND VOLUME G; CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406; Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5; Cortes Corinna, 2017, ARXIV170303478; Dasgupta S., 2014, ADV NEURAL INF PROCE, V27, P2555; DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633; DUBUISSON B, 1993, PATTERN RECOGN, V26, P155, DOI 10.1016/0031-3203(93)90097-G; El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605; Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226; Fumera G, 2002, LECT NOTES COMPUT SC, V2388, P68; Fumera G, 2000, LECT NOTES COMPUT SC, V1876, P863; Gal Y, 2016, PR MACH LEARN RES, V48; Genovese CR, 2012, J MACH LEARN RES, V13, P1263; Goodfellow I. J., 2014, ARXIV14126572; Guo C., 2017, ARXIV170604599; Hartigan J.A., 1975, CLUSTERING ALGORITHM; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Herbei R, 2006, CAN J STAT, V34, P709, DOI 10.1002/cjs.5550340410; Jiang H, 2017, PR MACH LEARN RES, V70; Jiang H, 2017, PR MACH LEARN RES, V70; Kendall A., 2017, WHAT UNCERTAINTIES W, V3, P4; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6405; Landgrebe TCW, 2006, PATTERN RECOGN LETT, V27, P908, DOI 10.1016/j.patrec.2005.10.015; Lee JD, 2004, HUM FACTORS, V46, P50, DOI 10.1518/hfes.46.1.50.30392; Liang Percy S, 2015, NIPS; Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599; Netzer Y., 2011, P NIPS WORKSH DEEP L; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Niyogi P, 2008, DISCRETE COMPUT GEOM, V39, P419, DOI 10.1007/s00454-008-9053-2; Papernot N, 2018, ARXIV180304765; Parrish N, 2013, J MACH LEARN RES, V14, P3561; Platt JC, 2000, ADV NEUR IN, P61; Provost F., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P445; Rigollet P, 2009, BERNOULLI, V15, P1154, DOI 10.3150/09-BEJ184; Rinaldo A, 2010, ANN STAT, V38, P2678, DOI 10.1214/10-AOS797; Santos-Pereira CM, 2005, PATTERN RECOGN LETT, V26, P943, DOI 10.1016/j.patrec.2004.09.042; Simonyan K., 2015, ICLR; Singh A, 2009, ANN STAT, V37, P2760, DOI 10.1214/08-AOS661; Tax DMJ, 2008, PATTERN RECOGN LETT, V29, P1565, DOI 10.1016/j.patrec.2008.03.010; Tortorella F, 2000, LECT NOTES COMPUT SC, V1876, P611; Tsybakov AB, 1997, ANN STAT, V25, P948, DOI 10.1214/aos/1069362732; Varshney KR, 2017, BIG DATA-US, V5, P246, DOI 10.1089/big.2016.0051; Wang J., 2015, ADV NEURAL INFORM PR; Wei Fan, 2002, AAAI; Wiener Y., 2011, ADV NEURAL INFORM PR, V24, P1665; Yuan M, 2010, J MACH LEARN RES, V11, P111	57	83	83	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000008
C	Weiler, M; Geiger, M; Welling, M; Boomsma, W; Cohen, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Weiler, Maurice; Geiger, Mario; Welling, Max; Boomsma, Wouter; Cohen, Taco			3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R-3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.	[Weiler, Maurice] Univ Amsterdam, Amsterdam, Netherlands; [Geiger, Mario] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Welling, Max] Univ Amsterdam, CIFAR, Amsterdam, Netherlands; [Boomsma, Wouter] Univ Copenhagen, Copenhagen, Denmark; [Cohen, Taco] Qualcomm AI Res, Amsterdam, Netherlands	University of Amsterdam; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Amsterdam; University of Copenhagen	Weiler, M (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	m.weiler@uva.nl; mario.geiger@epfl.ch; m.welling@uva.nl; wb@di.ku.dk; taco.cohen@gmail.com	Boomsma, Wouter/K-2903-2014	Boomsma, Wouter/0000-0002-8257-3827				[Anonymous], P BMVC; Azulay A., 2018, WHY DO DEEP CONVOLUT; Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543; Bekkers EJ, 2018, LECT NOTES COMPUT SC, V11070, P440, DOI 10.1007/978-3-030-00928-1_50; Boomsma W., 2017, ADV NEURAL INFORM PR, V30, P3433; Cohen T, 2014, PR MACH LEARN RES, V32, P1755; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen Taco S., 2017, 5 INT C LEARN REPR I, P2; Cohen Taco S, 2018, ARXIV180310743; Cohen Taco S, 2018, ICLR; Dawson NL, 2017, NUCLEIC ACIDS RES, V45, pD289, DOI 10.1093/nar/gkw1098; Dieleman S, 2016, PR MACH LEARN RES, V48; Duits R, 2011, INT J COMPUT VISION, V92, P231, DOI 10.1007/s11263-010-0332-z; Esteves C., 2017, ARXIV171106721; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Hinton Geoffrey E., 2018, INT C LEARN REPR; Hoogeboom E., 2018, INT C LEARN REPR; Janssen MHJ, 2017, LECT NOTES COMPUT SC, V10302, P643, DOI 10.1007/978-3-319-58771-4_51; Jansson-Frojmark M, 2019, J BEHAV MED, V42, P128, DOI 10.1007/s10865-018-9949-0; Kanatani Kenichi, 1990, GROUP THEORETICAL ME, P4; Kanezaki Asako, 2018, ROTATIONNET JOINT OB; Kingma D.P, P 3 INT C LEARNING R; Kondor R., 2018, ARXIV PREPRINT ARXIV; Kondor R, 2018, PR MACH LEARN RES, V80; Kondor Risi, 2018, INT C LEARN REPR ICL; Kondor Risi, 2018, NEURAL INFORM PROCES; Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540; Olah C., 2014, GROUPS GROUP CONVOLU; Ravanbakhsh S, 2017, PR MACH LEARN RES, V70; Reisert M, 2008, PROC CVPR IEEE, P92; Sabour Sara, 2017, PROC 31 INT C NEURAL; Savva M, 2017, PROC EUROGR WORKSHOP, P39; Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163; Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444; Skibbe H., 2013, THESIS ALBERT LUDWIN; Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2; Thomas Nathaniel, 2018, ARXIV180208219; Tolli F., 2009, J MATH SCI, V156, P11; Torng W, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1702-0; Weiler Maurice, 2018, COMPUTER VISION PATT; Winkels M., 2018, ARXIV PREPRINT ARXIV; Worrall D, 2018, LECT NOTES COMPUT SC, V11209, P585, DOI 10.1007/978-3-030-01228-1_35; Worrall Daniel E, 2017, COMPUTER VISION PATT; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391	46	83	83	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004089
C	Yi, KX; Wu, JJ; Gan, C; Torralba, A; Kohli, P; Tenenbaum, JB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yi, Kexin; Wu, Jiajun; Gan, Chuang; Torralba, Antonio; Kohli, Pushmeet; Tenenbaum, Joshua B.			Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.	[Yi, Kexin] Harvard Univ, Cambridge, MA 02138 USA; [Wu, Jiajun; Torralba, Antonio; Tenenbaum, Joshua B.] MIT CSAIL, Cambridge, MA 02139 USA; [Gan, Chuang] MIT IBM Watson Lab, Cambridge, MA USA; [Kohli, Pushmeet] DeepMind, London, England	Harvard University; Massachusetts Institute of Technology (MIT)	Yi, KX (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.		Yi, Kexin/AAL-6116-2020; Wu, JiaJun/GQH-7885-2022		ONR MURI [N00014-16-1-2007]; Center for Brain, Minds, and Machines (CBMM); IBM Research; Facebook	ONR MURI(MURIOffice of Naval Research); Center for Brain, Minds, and Machines (CBMM); IBM Research(International Business Machines (IBM)); Facebook(Facebook Inc)	We thank Jiayuan Mao, Karthik Narasimhan, and Jon Gauthier for helpful discussions and suggestions. We also thank Drew A. Hudson for sharing experimental results for comparison. This work is in part supported by ONR MURI N00014-16-1-2007, the Center for Brain, Minds, and Machines (CBMM), IBM Research, and Facebook.	Aditya S., 2018, P AAAI C ART INT, P32; Andreas J., 2016, ANN C N AM CHAPT ASS; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Balog M., 2017, ICLR; Berant Jonathan, 2013, P 2013 C EMP METH NA, P1533; Bisk Yonatan, 2018, AAAI; Cao Q., 2018, CVPR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Eslami SM, 2016, NEURIPS, V1; Gan C, 2017, IEEE I CONF COMP VIS, P1829, DOI 10.1109/ICCV.2017.201; Goldman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1809; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Guu K, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1051, DOI 10.18653/v1/P17-1097; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; Higgins I., 2018, ICLR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; Hudson Drew A., 2018, ABS180303067 CORR; Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Johnson M., 2016, P 25 INT JOINT C ART, P4246; Kingma D.P, P 3 INT C LEARNING R; Kulkarni TD, 2015, ADV NEUR IN, V28; Liang P, 2013, COMPUT LINGUIST, V39, P389, DOI 10.1162/COLI_a_00127; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lu JS, 2016, ADV NEUR IN, V29; Luong M., 2015, ARXIV150804025; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Mascharka David, 2018, CVPR; Misra Ishan, 2018, CVPR; Neelakantan Arvind, 2016, ICLR; Parisotto Emilio, 2017, ICLR, P2; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Rothe Anselm, 2017, NIPS; Santoro A, 2017, ADV NEUR IN, V30; Suarez Joseph, 2018, ARXIV180311361; Vedantam Ramakrishna, 2018, ICLR; Vinyals Oriol, 2015, NIPS; Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu JJ, 2017, PROC CVPR IEEE, P7035, DOI 10.1109/CVPR.2017.744; Yang Jimei, 2015, NIPS; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zhu C, 2017, IEEE I CONF COMP VIS, P1300, DOI 10.1109/ICCV.2017.145	50	83	83	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301006
C	Li, JQ; Barron, AR		Solla, SA; Leen, TK; Muller, KR		Li, JQ; Barron, AR			Mixture density estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				ESTIMATION BOUNDS; APPROXIMATION	Gaussian mixtures (or so-called radial basis function networks) for density estimation provide a natural counterpart to sigmoidal neural networks for function fitting and approximation. In both cases, it is possible to give simple expressions for the iterative improvement of performance as components of the network are introduced one at a time. In particular, for mixture density estimation we show that a k-component mixture estimated by maximum likelihood (or by an iterative likelihood improvement that we introduce) achieves log-likelihood within order 1/k of the log-likelihood achievable by any convex combination. Consequences for approximation and estimation using Kullback-Leibler risk are also given. A Minimum Description Length principle selects the optimal number of components k that minimizes the risk bound.	Yale Univ, Dept Stat, New Haven, CT 06520 USA	Yale University	Li, JQ (corresponding author), Yale Univ, Dept Stat, POB 208290, New Haven, CT 06520 USA.							BARRON AR, 1994, MACH LEARN, V14, P115, DOI 10.1023/A:1022650905902; BARRON AR, 1991, IEEE T INFORM THEORY, V37, P1034, DOI 10.1109/18.86996; BELL R, 1988, MANAGE SCI, V34, P724, DOI 10.1287/mnsc.34.6.724; Dasgupta S., 1999, P 40 ANN S FDN COMP, P634, DOI [10.5555/795665.796496, DOI 10.5555/795665.796496, DOI 10.1109/SFFCS.1999.814639]; GENOVESE C, 1998, UNPUB RATES CONVERGE; JONES LK, 1992, ANN STAT, V20, P608, DOI 10.1214/aos/1176348546; Lee WS, 1996, IEEE T INFORM THEORY, V42, P2118, DOI 10.1109/18.556601; LI JK, 1997, THESIS YALE U; LI JQ, 1999, THESIS YALE U DEP ST; Zeevi AJ, 1997, NEURAL NETWORKS, V10, P99, DOI 10.1016/S0893-6080(96)00037-8	11	83	94	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						279	285						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700040
C	Mika, S; Ratsch, G; Weston, J; Scholkopf, B; Smola, A; Muller, KR		Solla, SA; Leen, TK; Muller, KR		Mika, S; Ratsch, G; Weston, J; Scholkopf, B; Smola, A; Muller, KR			Invariant feature extraction and classification in kernel spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				DISCRIMINANT	We incorporate prior knowledge to construct nonlinear algorithms for invariant feature extraction and discrimination. Employing a unified framework in terms of a nonlinear variant of the Rayleigh coefficient, we propose non-linear generalizations of Fisher's discriminant and oriented PCA using Support Vector kernel functions. Extensive simulations show the utility of our approach.	GMD FIRST, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Mika, S (corresponding author), GMD FIRST, Kekulestr 7, D-12489 Berlin, Germany.	mika@first.gmd.de; raetsch@first.gmd.de; jasonw@dcs.rhbnc.ac.uk; bsc@microsoft.com; klaus@first.gmd.de	Mueller, Klaus-Robert/Y-3547-2019; Rätsch, Gunnar/B-8182-2009; Rätsch, Gunnar/O-5914-2017; Schölkopf, Bernhard/A-7570-2013	Mueller, Klaus-Robert/0000-0002-3861-7685; Rätsch, Gunnar/0000-0001-5486-8532; Schölkopf, Bernhard/0000-0002-8177-0925				Bishop, 1995, NEURAL NETWORKS PATT; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Diamantaras K.I., 1996, PRINCIPAL COMPONENT; FANG BQ, 1996, CHINESE J APPL PROBA, V12, P401; FREUND Y, 1994, LNCS; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281; RATSCH G, 1998, NCTR1998021 U LOND R; Saitoh S, 1988, THEORY REPRODUCING K; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 1999, ADV KERNEL METHODS S; Scholkopf Bernhard, 1997, SUPPORT VECTOR LEARN; Shashua A, 1999, NEURAL PROCESS LETT, V9, P129, DOI 10.1023/A:1018677409366; TONG S, 1999, UNPUB IJCAI 99 WORKS	16	83	89	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						526	532						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700075
C	Parra, L; Spence, C; Sajda, P; Ziehe, A; Muller, KR		Solla, SA; Leen, TK; Muller, KR		Parra, L; Spence, C; Sajda, P; Ziehe, A; Muller, KR			Unmixing hyperspectral data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In hyperspectral imagery one pixel typically consists of a mixture of the reflectance spectra of several materials, where the mixture coefficients correspond to the abundances of the constituting materials. We assume linear combinations of reflectance spectra with some additive normal sensor noise and derive a probabilistic MAP framework for analyzing hyperspectral data. As the material reflectance characteristics are not know a priori, we face the problem of unsupervised linear unmixing. The incorporation of different prior information (e.g. positivity and normalization of the abundances) naturally leads to a family of interesting algorithms, for example in the noise-free case yielding an algorithm that can be understood as constrained independent component analysis (ICA). Simulations underline the usefulness of our theory.	Sarnoff Corp, Princeton, NJ 08543 USA	Sarnoff Corporation	Parra, L (corresponding author), Sarnoff Corp, CN-5300, Princeton, NJ 08543 USA.		Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685				BAYLISS J, 1997, P SPIE APPL IM PATT, V9; Boardman J.W., 1994, 10 THEM C GEOL REM S, P407; Haykin S., 1991, ADAPTIVE FILTER THEO; Maselli F, 1996, P SOC PHOTO-OPT INS, V2960, P104, DOI 10.1117/12.262456; Pearlmutter BA, 1997, ADV NEUR IN, V9, P613; SETTLE JJ, 1993, INT J REMOTE SENS, V14, P1159, DOI 10.1080/01431169308904402; SMITH MO, 1990, 5TH P AUSTR REM SENS, V1, P331; Szu H, 1997, P SOC PHOTO-OPT INS, V3078, P147, DOI 10.1117/12.271711; *US GEOL SURV, 1993, 93592 US GEOL SURV	9	83	84	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						942	948						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700133
C	Socci, ND; Lee, DD; Seung, HS		Jordan, MI; Kearns, MJ; Solla, SA		Socci, ND; Lee, DD; Seung, HS			The rectified Gaussian distribution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A simple but powerful modification of the standard Gaussian distribution is studied. The variables of the rectified Gaussian are constrained to be nonnegative, enabling the use of nonconvex energy functions. Two multimodal examples, the competitive and cooperative distributions, illustrate the representational power of the rectified Gaussian. Since the cooperative distribution can represent the translations of a pattern, it demonstrates the potential of the rectified Gaussian for modeling pattern manifolds.	AT&T Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T; Nokia Corporation; Nokia Bell Labs	Socci, ND (corresponding author), AT&T Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.		Lee, Daniel D./B-5753-2013	Lee, Daniel/0000-0003-4239-8777					0	83	83	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						350	356						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700050
C	Bai, L; Yao, LN; Li, C; Wang, XZ; Wang, C		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Bai, Lei; Yao, Lina; Li, Can; Wang, Xianzhi; Wang, Can			Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Modeling complex spatial and temporal correlations in the correlated time series data is indispensable for understanding the traffic dynamics and predicting the future status of an evolving traffic system. Recent works focus on designing complicated graph neural network architectures to capture shared patterns with the help of pre-defined graphs. In this paper, we argue that learning node-specific patterns is essential for traffic forecasting while the pre-defined graph is avoidable. To this end, we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks. Our experiments 1 on two real-world traffic datasets show AGCRN outperforms state-of-the-art by a significant margin without pre-defined graphs about spatial connections.	[Bai, Lei; Yao, Lina; Li, Can] UNSW, Sydney, NSW, Australia; [Wang, Xianzhi] Univ Technol Sydney, Sydney, NSW, Australia; [Wang, Can] Griffith Univ, Nathan, Qld, Australia	University of New South Wales Sydney; University of Technology Sydney; Griffith University	Bai, L (corresponding author), UNSW, Sydney, NSW, Australia.	baisanshi@gmail.com; lina.yao@unsw.edu.au; can.li4@student.unsw.edu.au; xianzhi.wang@uts.edu.au; can.wang@griffith.edu.au	WANG, CAN/GWV-0969-2022	Wang, Can/0000-0002-2890-0057				Adhikari B, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P577, DOI 10.1145/3292500.3330917; Bai L, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2293, DOI 10.1145/3357384.3358097; Bai L, 2019, LECT NOTES ARTIF INT, V11440, P29, DOI 10.1007/978-3-030-16145-3_3; Bai Lei, 2019, P 28 INT JOINT C ART, P1981; Bai S., 2018, ARXIV PREPRINT ARXIV; Bengio Y., 2014, ARXIV14061078; Borovykh A., 2017, ARXIV170304691; Cao W, 2018, ADV NEUR IN, V31; Chen CHW, 2001, AIP CONF PROC, V584, P96, DOI 10.1063/1.1405589; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Geng X, 2019, AAAI CONF ARTIF INTE, P3656; Guo SN, 2019, AAAI CONF ARTIF INTE, P922; HIRANO S, 2019, ADV NEURAL INFORM PR, P242; Huang ST, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2129, DOI 10.1145/3357384.3358132; Jenkins P, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1993, DOI 10.1145/3357384.3358001; Kipf TN, 2016, P INT C LEARN REPR; Lai GK, 2018, ACM/SIGIR PROCEEDINGS 2018, P95, DOI 10.1145/3209978.3210006; Li Can, 2020, P 28 ACM INT C INF K; Li C, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1141, DOI 10.1145/3292500.3330983; Li Y., 2018, ARXIV PREPRINT ARXIV; Luo YH, 2018, ADV NEUR IN, V31; Mallick T, 2019, ARXIV190911197; Pan ZY, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1720, DOI 10.1145/3292500.3330884; Qi Y, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P299, DOI 10.1145/3357384.3357883; Rangapuram SS, 2018, ADV NEUR IN, V31; Salinas D, 2020, INT J FORECASTING, V36, P1181, DOI 10.1016/j.ijforecast.2019.07.001; Sen R, 2019, ADV NEUR IN, V32; Song Chao, 2020, P AAAI C ART INT; Tang X., 2019, ARXIV191110273; Velickovi Petar, 2017, ARXIV171010903; Wu ZL, 2020, PREP BIOCHEM BIOTECH, V50, P37, DOI 10.1080/10826068.2019.1658119; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Xu M., 2020, ARXIV200102908; Yao HX, 2018, AAAI CONF ARTIF INTE, P2588; Ye JX, 2022, IEEE T INTELL TRANSP, V23, P3904, DOI 10.1109/TITS.2020.3043250; Yin X., 2020, ARXIV PREPRINT ARXIV; Ying R, 2018, ADV NEUR IN, V31; Yu B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3634; Zhang Junbo, 2017, P AAAI C ART INT, V31; [张军进 Zhang Junjin], 2016, [黄金科学技术, Gold Science and Technology], V24, P1; Zhang YW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4341; Zhang Yunkai, 2019, ARXIV191009620; Zheng C., 2019, ARXIV191108415; Zivot E., 2006, MODELING FINANCIAL T, P385, DOI DOI 10.1007/978-0-387-32348-0_11	45	82	83	9	14	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000037
C	Hou, QB; Jiang, PT; Wei, YC; Cheng, MM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hou, Qibin; Jiang, Peng-Tao; Wei, Yunchao; Cheng, Ming-Ming			Self-Erasing Network for Integral Object Attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recently, adversarial erasing for weakly-supervised object attention has been deeply studied due to its capability in localizing integral object regions. However, such a strategy raises one key problem that attention regions will gradually expand to non-object regions as training iterations continue, which significantly decreases the quality of the produced attention maps. To tackle such an issue as well as promote the quality of object attention, we introduce a simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions from spreading to unexpected background regions. In particular, SeeNet leverages two self-erasing strategies to encourage networks to use reliable object and background cues for learning to attention. In this way, integral object regions can be effectively highlighted without including much more background regions. To test the quality of the generated attention maps, we employ the mined object regions as heuristic cues for learning semantic segmentation models. Experiments on Pascal VOC well demonstrate the superiority of our SeeNet over other state-of-the-art methods.	[Hou, Qibin; Jiang, Peng-Tao; Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China; [Wei, Yunchao] UIUC, Urbana, IL USA	Nankai University; University of Illinois System; University of Illinois Urbana-Champaign	Cheng, MM (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.	andrewhoux@gmail.com; cmm@nankai.edu.cn	Jiang, Peng-Tao/HHN-3328-2022; Cheng, Ming-Ming/A-2527-2009	Cheng, Ming-Ming/0000-0001-5550-8758	NSFC [61620106008, 61572264]; National youth talent support program, Tianjin Natural Science Foundation for Distinguished Young Scholars [17JCJQJC43700]; Huawei Innovation Research Program	NSFC(National Natural Science Foundation of China (NSFC)); National youth talent support program, Tianjin Natural Science Foundation for Distinguished Young Scholars; Huawei Innovation Research Program(Huawei Technologies)	This research was supported by NSFC (NO. 61620106008, 61572264), the national youth talent support program, Tianjin Natural Science Foundation for Distinguished Young Scholars (NO. 17JCJQJC43700), and Huawei Innovation Research Program.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Bin Jin, 2017, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2017.185; Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30; Chaudhry Arslan, 2017, ARXIV170705821; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hong  Seunghoon, 2017, CVPR, P3626; HOU Q, 2018, ARXIV PREPRINT ARXIV; Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688; Hou  Qibin, 2017, EMMCVPR; Jiang  Huaizu, 2018, FRONT COMPUT SCI; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kim D, 2017, IEEE I CONF COMP VIS, P3554, DOI 10.1109/ICCV.2017.382; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Li FF, 2002, P NATL ACAD SCI USA, V99, P9596, DOI 10.1073/pnas.092277599; Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960; Lin Di, 2016, CVPR; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Nair V, 2010, P 27 INT C MACHINE L, P807; Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209; Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780; Qi X., 2016, ECCV; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Roy A, 2017, PROC CVPR IEEE, P7282, DOI 10.1109/CVPR.2017.770; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14; Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3; Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759; Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang Jianming, 2016, ECCV, V5, P6; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319	42	82	84	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300051
C	Courty, N; Flamary, R; Habrard, A; Rakotomamonjy, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Courty, Nicolas; Flamary, Remi; Habrard, Amaury; Rakotomamonjy, Alain			Joint distribution optimal transportation for domain adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function f in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a nonlinear transformation between the joint feature/label space distributions of the two domain P-s and P-t that can be estimated with optimal transport. We propose a solution of this problem that allows to recover an estimated target P-t(f) = (X, f (X)) by optimizing simultaneously the optimal coupling and f. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.	[Courty, Nicolas] Univ Bretagne Sud, IRISA, UMR 6074, CNRS, Lorient, France; [Flamary, Remi] Univ Cote dAzur, Lagrange, UMR 7293, CNRS,OCA, Nice, France; [Habrard, Amaury] Univ Lyon, UJM St Etienne, CNRS, Lab Hubert Curien UMR 5516, F-42023 Lyon, France; [Rakotomamonjy, Alain] Normandie Univ, Univ Rouen, LITIS EA 4108, Rouen, France	Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Earth Sciences & Astronomy (INSU); UDICE-French Research Universities; Universite Cote d'Azur; Observatoire de la Cote d'Azur; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite de Rouen Normandie	Courty, N (corresponding author), Univ Bretagne Sud, IRISA, UMR 6074, CNRS, Lorient, France.	courty@univ-ubs.fr; remi.flamary@unice.fr; amaury.habrard@univ-st-etienne.fr; alain.rakoto@insa-rouen.fr	Flamary, Rémi/AAC-1958-2022	Flamary, Rémi/0000-0002-4212-6627	French National Research Agency (ANR) [OATMIL ANR-17-CE23-0012]; Normandie Projet GRR-DAISI, European funding FEDER DAISI; CNRS - Defi Imag'In	French National Research Agency (ANR)(French National Research Agency (ANR)); Normandie Projet GRR-DAISI, European funding FEDER DAISI; CNRS - Defi Imag'In	This work benefited from the support of the project OATMIL ANR-17-CE23-0012 of the French National Research Agency (ANR), the Normandie Projet GRR-DAISI, European funding FEDER DAISI and CNRS funding from the Defi Imag'In. The authors also wish to thank Kai Zhang and Qiaojun Wang for providing the Wifi localization dataset.	[Anonymous], 2012, ICML; [Anonymous], 2011, 28 INT C MACH LEARN; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; Ben-David S., 2012, P ISAIM; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Courty N., 2016, IEEE T PATTERN ANAL; Courty N., 2014, ECML PKDD; Cuturi M., 2013, NEURIPS; Fernando B., 2013, ICCV; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Gong B., 2012, CVPR; Gong MM, 2016, PR MACH LEARN RES, V48; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; Kantorovitch L, 1942, CR ACAD SCI URSS, V37, P199; Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183; Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111; Mansour Y., 2009, P COLT; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Patel V. M., 2015, IEEE SIGNAL PROCESSI, V32; Perrot M., 2016, ADV NEURAL INFORM PR, V29, P4197; Rosasco L, 2004, NEURAL COMPUT, V16, P1063, DOI 10.1162/089976604773135104; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126; Sugiyama M., 2008, NIPS; Thorpe M., 2016, CORR; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Zhang K, 2015, AAAI CONF ARTIF INTE, P3150; Zhang Kun, 2013, ICML; Zhang Ning, 2014, ICML; Zhong E., 2010, ECML PKDD	34	82	84	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403077
C	Liu, MY; Tuzel, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Liu, Ming-Yu; Tuzel, Oncel			Coupled Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.	[Liu, Ming-Yu; Tuzel, Oncel] MERL, Cambridge, MA 02139 USA		Liu, MY (corresponding author), MERL, Cambridge, MA 02139 USA.	mliu@merl.com; oncel@merl.com						[Anonymous], 2014, ABS14123474 CORR; [Anonymous], 2014, ICLR; Bethge M., 2016, 4 INT C LEARN REPR I; Denton E. L., 2015, NIPS; Dosovitskiy A., 2015, CVPR; Fernando B, 2015, PATTERN RECOGN LETT, V65, P60, DOI 10.1016/j.patrec.2015.07.009; Ganin Y., 2016, JMLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2015, ICML; He K., 2015, ICCV; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2014, NIPS; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lai K., 2011, ICRA; LeCun Y., 1998, P IEEE; Li  Yujia, 2016, ICML; Liu Z., 2015, ICCV; Long M., 2013, ICCV; Mirza M., 2014, ARXIV PREPRINT ARXIV; Ngiam J., 2011, ICML; Radford A., 2016, ICLR; Reed S. E., 2015, P ADV NEUR INF PROC, P1; Rezende D.J., 2014, ICML; Richard S. Zemel, 2014, Arxiv, DOI arXiv:1411.2539; Rozantsev A., 2016, ARXIV160306432; Silberman N., 2012, ECCV; Sohl-Dickstein J., 2015, ICML; Srivastava N., 2012, NIPS; Wang S, 2012, CVPR; Yan X., 2015, ARXIV151200570; Yang J., 2010, IEEE TIP; Yim Junho, 2015, CVPR	33	82	82	1	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704068
C	Sculley, D; Holt, G; Golovin, D; Davydov, E; Phillips, T; Ebner, D; Chaudhary, V; Young, M; Crespo, JF; Dennison, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sculley, D.; Holt, Gary; Golovin, Daniel; Davydov, Eugene; Phillips, Todd; Ebner, Dietmar; Chaudhary, Vinay; Young, Michael; Crespo, Jean-Francois; Dennison, Dan			Hidden Technical Debt in Machine Learning Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.	[Sculley, D.; Holt, Gary; Golovin, Daniel; Davydov, Eugene; Phillips, Todd; Ebner, Dietmar; Chaudhary, Vinay; Young, Michael; Crespo, Jean-Francois; Dennison, Dan] Google Inc, Mountain View, CA 94043 USA	Google Incorporated	Sculley, D (corresponding author), Google Inc, Mountain View, CA 94043 USA.	dsculley@google.com; gholt@google.com; dgg@google.com; edavydov@google.com; toddphillips@google.com; ebner@google.com; vchaudhary@google.com; mwyoung@google.com; jfcrespo@google.com; dennison@google.com						Ahmed A., 2014, P 11 USENIX C OP SYS, P583; Ananthanarayanan R., 2013, P ACM SIGMOD INT C M, P577; Anonymous A., SE4ML SOFTW ENG MACH; Bottou Leon, 2013, J MACHINE LEARNING R, V14; Brown WH., 1998, ANTIPATTERNS REFACTO; CHILIMBI TM, 2014, P OSDI, V14, P571; Dalessandro B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1573, DOI 10.1145/2623330.2623349; Fowler M., 2018, REFACTORING IMPROVIN; Langford J., 2008, ADV NEURAL INFORM PR, P817; Lin J., 2013, ACM SIGKDD EXPLORATI, V14, P6, DOI DOI 10.1145/2481244.2481247; McMahan HB, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1222; Morgenthaler J. D., 2012, P 3 INT WORKSH MAN T; Sculley D., 2011, P 17 ACM SIGKDD INT; Securities and E. Commission, 2013, SEC CHARG KNIGHT CAP; Spector Alfred, 2012, COMMUNICATIONS ACM, V55; Zheng A., SE4ML SOFTW ENG MACH	16	82	84	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100024
C	Campbell, C; Bennett, KP		Leen, TK; Dietterich, TG; Tresp, V		Campbell, C; Bennett, KP			A linear programming approach to novelty detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Novelty detection involves modeling the normal behaviour of a system hence enabling detection of any divergence from normality. It has potential applications in many areas such as detection of machine damage or highlighting abnormal features in medical data. One approach is to build a hypothesis estimating the support of the normal data i.e. constructing a function which is positive in the region where the data is located and negative elsewhere. Recently kernel methods have been proposed for estimating the support of a distribution and they have performed well in practice - training involves solution of a quadratic programming problem. In this paper we propose a simpler kernel method for estimating the support based on linear programming. The method is easy to implement and can learn large datasets rapidly. We demonstrate the method on medical and fault detection datasets.	Univ Bristol, Dept Engn Math, Bristol BS8 1TR, Avon, England	University of Bristol	Campbell, C (corresponding author), Univ Bristol, Dept Engn Math, Bristol BS8 1TR, Avon, England.							BENNETT K, UNPUB COLUMN GENERAT; BENNETT KP, 2000, P INT C MACH LEARN S; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; CAMPBELL C, IN PRESS RADIAL BASI; Cox L., 1982, ASA P STAT COMP SECT, P55; MANGASARIAN OL, 1999, 9902 U WISC MAD DAT; SCHOLKOPF B, 2000, IN PRESS NEURAL INFO; SCHOLKOPF B, 2000, MSRTR9987 MICR RES C; SCHOLKOPF B, MSRTR200022 MICR; Tax D. M. J., 1999, P 5 ANN C ADV SCH CO, P398; TAX DMJ, 1999, P EUR S ART NEUR NET, P251; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vapnik V.N, 1998, STAT LEARNING THEORY	13	82	87	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						395	401						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800056
C	Waterhouse, S; MacKay, D; Robinson, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Waterhouse, S; MacKay, D; Robinson, T			Bayesian methods for mixtures of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CAMBRIDGE,DEPT ENGN,CAMBRIDGE CB2 1PZ,ENGLAND	University of Cambridge									0	82	84	0	4	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						351	357						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00050
C	Wang, TC; Liu, MY; Tao, A; Liu, GL; Kautz, J; Catanzaro, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Ting-Chun; Liu, Ming-Yu; Tao, Andrew; Liu, Guilin; Kautz, Jan; Catanzaro, Bryan			Few-shot Video-to-Video Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Video-to-video synthesis (vid2vid) aims at converting an input semantic video, such as videos of human poses or segmentation masks, to an output photorealistic video. While the state-of-the-art of vid2vid has advanced significantly, existing approaches share two major limitations. First, they are data-hungry. Numerous images of a target human subject or a scene are required for training. Second, a learned model has limited generalization capability. A pose-to-human vid2vid model can only synthesize poses of the single person in the training set. It does not generalize to other humans that are not in the training set. To address the limitations, we propose a few-shot vid2vid framework, which learns to synthesize videos of previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines using several large-scale video datasets including human-dancing videos, talking-head videos, and street-scene videos. The experimental results verify the effectiveness of the proposed framework in addressing the two limitations of existing vid2vid approaches.	[Wang, Ting-Chun; Liu, Ming-Yu; Tao, Andrew; Liu, Guilin; Kautz, Jan; Catanzaro, Bryan] NVIDIA Corp, Santa Clara, CA 95051 USA	Nvidia Corporation	Wang, TC (corresponding author), NVIDIA Corp, Santa Clara, CA 95051 USA.	tingchunw@nvidia.com; mingyul@nvidia.com; atao@nvidia.com; guilinl@nvidia.com; jkautz@nvidia.com; bcatanzaro@nvidia.com	Wang, Ting-Chun/AAD-4410-2021; Wang, Ting-Chun/AAZ-2408-2020	Wang, Ting-Chun/0000-0002-1522-2381				Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964; Benaim S, 2018, ADV NEUR IN, V31; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Brock A., 2019, INT C LEARNING REPRE; Brostow Gabriel J., 2008, ECCV, P44, DOI [10.1007/978-3-540-88682-2_5, DOI 10.1007/978-3-540-88682-2_5]; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chan, 2018, ARXIV180807371; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; De Brabandere B, 2016, ADV NEUR IN, V29; Gafni O., 2019, ARXIV190408379; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Ha D., 2016, INT C LEARN REPR ICL; Hao ZK, 2018, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2018.00819; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hu Qiyang, 2018, ARXIV181201874; Huang XY, 2018, IEEE COMPUT SOC CONF, P1067, DOI 10.1109/CVPRW.2018.00141; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Huang Xun, 2018, EUR C COMP VIS ECCV; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kalchbrenner N, 2016, ARXIV161000527; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D.P, P 3 INT C LEARNING R; Lee Alex X, 2018, ARXIV180401523; Li YJ, 2018, LECT NOTES COMPUT SC, V11213, P609, DOI 10.1007/978-3-030-01240-3_37; Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194; Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Mathieu Michael, 2016, ICLR; Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8; Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37; Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244; Odena A, 2017, PR MACH LEARN RES, V70; Pan JT, 2019, PROC CVPR IEEE, P3728, DOI 10.1109/CVPR.2019.00385; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Rossler Andreas, 2018, FACEFORENSICS LARGE, V2, P19; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361; Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang TC, 2018, ADV NEUR IN, V31; Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40; Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhang C, 2019, INT C LEARN REPR; Zhang H., 2017, ICCV; Zhang H, 2019, PR MACH LEARN RES, V97; Zhou YP, 2019, IEEE INT CONF COMP V, P1208, DOI 10.1109/ICCVW.2019.00153; Zhu Jun-Yan, 2017, ICCV	69	81	82	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305006
C	Li, ZW; Chen, QF; Koltun, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Zhuwen; Chen, Qifeng; Koltun, Vladlen			Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GAME; GO	We present a learning-based approach to computing solutions for certain NP-hard problems. Our approach combines deep learning techniques with useful algorithmic elements from classic heuristics. The central component is a graph convolutional network that is trained to estimate the likelihood, for each vertex in a graph, of whether this vertex is part of the optimal solution. The network is designed and trained to synthesize a diverse set of solutions, which enables rapid exploration of the solution space via tree search. The presented approach is evaluated on four canonical NP-hard problems and five datasets, which include benchmark satisfiability problems and real social network graphs with up to a hundred thousand nodes. Experimental results demonstrate that the presented approach substantially outperforms recent deep learning work, and performs on par with highly optimized state-of-the-art heuristic solvers for some NP-hard problems. Experiments indicate that our approach generalizes across datasets, and scales to graphs that are orders of magnitude larger than those used during training.	[Li, Zhuwen; Koltun, Vladlen] Intel Labs, Santa Clara, CA 95054 USA; [Chen, Qifeng] HKUST, Hong Kong, Peoples R China	Intel Corporation; Hong Kong University of Science & Technology	Li, ZW (corresponding author), Intel Labs, Santa Clara, CA 95054 USA.							Akiba Takuya, 2015, ALENEX; Andrade DV, 2012, J HEURISTICS, V18, P525, DOI 10.1007/s10732-012-9196-4; [Anonymous], 2017, ICCV; Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090; Balyo Tomas, SAT COMPETITION 2017; Battiti R, 2001, ALGORITHMICA, V29, P610, DOI 10.1007/s004530010074; Bello I., 2016, ARXIV PREPRINT ARXIV; Bronstein Michael M., 2017, IEEE SIGNAL PROCESSI, V34; Christofides N., 1976, WORST CASE ANAL NEW; Dai Hanjun, 2017, NIPS; de Moura L., 2008, TACAS; Defferrard M., 2016, NIPS; Feo Thomas A., 1994, OPERATIONS RES, V42; Gilmer J., 2017, ICML; Gonzalez T.F, 2007, HDB APPROXIMATION AL; Grosso A, 2008, J HEURISTICS, V14, P587, DOI 10.1007/s10732-007-9055-x; Gurobi Optimization Inc, 2018, GUR OPT REF MAN VERS; Guzman-Rivera Abner, 2012, NIPS; He H., 2014, NIPS; Hochbaum D. S., 1997, APPROXIMATION ALGORI; Hoos Holger H., 2000, SAT; Karp RM, 1972, COMPLEXITY COMPUTER; Ke Xu, 2007, ARTIFICIAL INTELLIGE, V171; Kingma D.P., 2015, INT C LEARN REPR, P1; Kipf Thomas N., 2017, INT C LEARNING REPRE; Kool W., 2018, ARXIV PREPRINT ARXIV; Lamm S, 2017, J HEURISTICS, V23, P207, DOI 10.1007/s10732-017-9337-x; Leskovec J, 2014, SNAP DATASETS STANFO; Nair V., 2010, ICML; Nowak A., 2017, ARXIV170607450; Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI; Selsam D., 2018, ARXIV180203685; Sen Prithviraj, 2008, AI MAGAZINE, V29; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Vazirani V., 2004, APPROXIMATION ALGORI; Vinyals O., 2015, NIPS; Wang C., 2018, CVPR; Williamson D. P., 2011, DESIGN APPROXIMATION, DOI DOI 10.1017/CBO9780511921735	39	81	84	5	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300050
C	Su, YC; Grauman, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Su, Yu-Chuan; Grauman, Kristen			Learning Spherical Convolution for Fast Features from 360 degrees Imagery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					While 360 degrees cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield "flat" filters, yet 360 degrees images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360 degrees imagery directly in its equirec-tangular projection. Our approach learns to reproduce the flat filter outputs on 360 degrees data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360 degrees images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art "flat" object detector to 360 degrees data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution.	[Su, Yu-Chuan; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Su, YC (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.							Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; ARISOTTO E, 2016, 4 INT C LEARN REPR I, P1; Barre A., 1987, CURVILINEAR PERSPECT; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Cohen T., 2017, ARXIV170904893; Courtney PG, 2015, IEEE COMP SEMICON; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Furnari A, 2017, IEEE T IMAGE PROCESS, V26, P696, DOI 10.1109/TIP.2016.2627816; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Hansen P., 2007, IROS; Hansen P., 2007, ICCV; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hinton G., 2015, ARXIV150302531; Hu H., 2017, CVPR; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Jia Y., 2014, P 22 ACM INT C MULT, P675; Khasanova R., 2017, INT C COMP VIS WORKS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai W.-S., 2017, IEEE T VISUALIZATION, P1; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Romero Adriana, 2015, ICLR; Su Y.-C., 2017, CVPR; Su Y.-C., 2016, ACCV; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991; Yu F., 2016, P ICLR 2016; Zelnik-Manor L., 2005, ICCV; Zhang Y, 2014, APPL MECH MATER, V518, P3, DOI 10.4028/www.scientific.net/AMM.518.3; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	39	81	81	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400051
C	Rasmussen, CE; Kuss, M		Thrun, S; Saul, K; Scholkopf, B		Rasmussen, CE; Kuss, M			Gaussian processes in reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Rasmussen, CE (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.							Attias H., 2003, INT WORKSHOP ARTIFIC, P9; DEARDEN RN, 1998, 15 NAT C ART INT AAA; Dietterich TG, 2002, ADV NEUR IN, V14, P1491; GIRARD A, 2002, ADV NEURAL INFORMATI, V15; Moore AW, 1995, MACH LEARN, V21, P199, DOI 10.1007/BF00993591; QUINONEROCANDEL.J, 2003, P 2003 IEEE C AC SPE; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; WILLIAMS CKI, 1996, ADV NEURAL INFORMATI, V8	8	81	81	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						751	758						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500094
C	Williams, CKI		Mozer, MC; Jordan, MI; Petsche, T		Williams, CKI			Computing with infinite networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				For neural networks with a wide class of weight-priors, it can be shown that in the limit of an infinite number of hidden units the prior over functions tends to a Gaussian process. In this paper analytic forms are derived for the covariance function of the Gaussian processes corresponding to networks with sigmoidal and Gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units, and shows that, somewhat paradoxically, it may be easier to compute with infinite networks than finite ones.			Williams, CKI (corresponding author), ASTON UNIV,DEPT COMP SCI & APPL MATH,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.								0	81	81	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						295	301						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00042
C	Metzler, CA; Mousavi, A; Baraniuk, RG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Metzler, Christopher A.; Mousavi, Ali; Baraniuk, Richard G.			Learned D-AMP: Principled Neural Network Based Compressive Image Recovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS; MODEL	Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix. It was recently demonstrated that iterative sparse-signal-recovery algorithms can be "unrolled" to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network Learned D-AMP (LDAMP). The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over 50x faster than BM3D-AMP and hundreds of times faster than NLR-CS.	[Metzler, Christopher A.; Mousavi, Ali; Baraniuk, Richard G.] Rice Univ, Houston, TX 77251 USA	Rice University	Metzler, CA (corresponding author), Rice Univ, Houston, TX 77251 USA.	chris.metzler@rice.edu; ali.mousavi@rice.edu; richb@rice.edu	Jeong, Yongwook/N-7413-2016; Baraniuk, Richard/ABA-1743-2020		DARPA REVEAL [HR0011-16-C-0028]; DARPA OMNI-SCIENT [G001534-7500]; ONR [N00014-17-1-2551, N00014-15-1-2735]; ARO [W911NF-15-1-0316]; NSF [CCF-1527501]; NSF GRFP	DARPA REVEAL; DARPA OMNI-SCIENT; ONR(Office of Naval Research); ARO; NSF(National Science Foundation (NSF)); NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported in part by DARPA REVEAL grant HR0011-16-C-0028, DARPA OMNI-SCIENT grant G001534-7500, ONR grant N00014-15-1-2735, ARO grant W911NF-15-1-0316, ONR grant N00014-17-1-2551, and NSF grant CCF-1527501. In addition, C. Metzler was supported in part by the NSF GRFP.	Alain G, 2014, J MACH LEARN RES, V15, P3563; [Anonymous], 2013, ISMIR; [Anonymous], 2016, ARXIV161003082; [Anonymous], 2010, P INT C MACH LEARN; Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718; Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Beygi S., 2017, ARXIV170401992; Borgerding M., 2016, ARXIV161201183; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Daubechies I., 2004, COMMUNICATIONS PURE, V75, P1412; Dave A., 2016, ARXIV161204229; Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449; Gulrajani I., 2017, P ADV NEUR INF PROC, V2017-Decem, P5768; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hershey J. R., 2014, ARXIV14092574; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55; Li ChunLong, 2009, China Vegetables, P46; Maleki A., 2010, THESIS; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683; MEZARD M, 2008, INFORM PHYS COMPUTAT; Mousavi A, 2017, INT CONF ACOUST SPEE, P2272, DOI 10.1109/ICASSP.2017.7952561; Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Ramani S, 2008, IEEE T IMAGE PROCESS, V17, P1540, DOI 10.1109/TIP.2008.2001404; Schniter P., 2016, ARXIV161101376; SMIEJA FJ, 1993, CIRC SYST SIGNAL PR, V12, P331, DOI 10.1007/BF01189880; Sonderby C. K., 2017, P INT C LEARN REPR I; Theis L, 2015, ADV NEURAL INFORM PR, P1927; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Tramel EW, 2016, 2016 IEEE INFORMATION THEORY WORKSHOP (ITW); Tramel EW, 2016, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2016/07/073401; Tramel Eric W., 2017, ARXIV170203260; Vedaldi A., 2015, P ACM INT C MULT; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Yang Y, 2016, ADV NEUR IN, V29; Yao H., 2017, ARXIV170205743	49	80	83	3	15	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401078
C	Gregor, K; Besse, F; Rezende, DJ; Danihelka, I; Wierstra, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gregor, Karol; Besse, Frederic; Rezende, Danilo Jimenez; Danihelka, Ivo; Wierstra, Daan			Towards Conceptual Compression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce convolutional DRAW, a homogeneous deep generative model achieving state-of-the-art performance in latent variable image modeling. The algorithm naturally stratifies information into higher and lower level details, creating abstract features and as such addressing one of the fundamentally desired properties of representation learning. Furthermore, the hierarchical ordering of its latents creates the opportunity to selectively store global information about an image, yielding a high quality 'conceptual compression' framework.	[Gregor, Karol; Besse, Frederic; Rezende, Danilo Jimenez; Danihelka, Ivo; Wierstra, Daan] Google DeepMind, London, England	Google Incorporated	Gregor, K (corresponding author), Google DeepMind, London, England.	karolg@google.com; fbesse@google.com; danilor@google.com; danihelka@google.com; wierstra@google.com						Carlson T, 2013, J VISION, V13, DOI 10.1167/13.10.1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K, 2014, PR MACH LEARN RES, V32, P1242; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Gregor Karol, 2011, ARXIV PREPRINT ARXIV; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Larochelle H., 2011, INT C ART INT STAT; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; van den Oord A, 2016, PR MACH LEARN RES, V48; WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771	15	80	81	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704066
C	Howe, NR; Leventon, ME; Freeman, WT		Solla, SA; Leen, TK; Muller, KR		Howe, NR; Leventon, ME; Freeman, WT			Bayesian reconstruction of 3D human motion from single-camera video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The three-dimensional motion of humans is underdetermined when the observation is limited to a single camera, due to the inherent 3D ambiguity of 2D video. We present a system that reconstructs the 3D motion of human subjects from single-camera video, relying on prior knowledge about human motion, learned from training data, to resolve those ambiguities. After initialization in 2D, the tracking and 3D reconstruction is automatic; we show results for several video sequences. The results show the power of treating 3D body tracking as an inference problem.	Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA	Cornell University	Howe, NR (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA.	nihowe@cs.cornell.edu; leventon@ai.mit.edu; freeman@merl.com						BERGEN JR, 1992, EUR C COMP VIS, P237; Bishop, 1995, NEURAL NETWORKS PATT; Brand M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1237, DOI 10.1109/ICCV.1999.790422; BREGLER C, 1998, IEEE COMP SOC C COMP; GAVRILA DM, 1996, IEEE COMP SOC C COMP; Ghahramani Z., 1997, EM ALGORITHM MIXTURE; GONCALVES L, 1995, P 3 INT C COMP VIS; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; JU SX, 1996, 2 INT C AUT FAC GEST; LEVENTON ME, 1998, TR9806 MITS EL RES L; MORRIS DD, 1998, IEEE COMP SOC C COMP; WACHTER S, 1997, NONR ART MOT WORKSH	12	80	93	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						820	826						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700116
C	Yang, B; Bender, G; Le, QV; Ngiam, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Brandon; Bender, Gabriel; Le, Quoc V.; Ngiam, Jiquan			CondConv: Conditionally Parameterized Convolutions for Efficient Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.	[Yang, Brandon; Bender, Gabriel; Le, Quoc V.; Ngiam, Jiquan] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Yang, B (corresponding author), Google Brain, Mountain View, CA 94043 USA.	bcyang@google.com; gbender@google.com; qvl@google.com; jngiam@google.com						Arel Itamar, 2013, ARXIV13124461; Bengio Emmanuel, 2015, CORR; Bengio Yoshua, 2013, ARXIV13083432; Chen Z., 2018, ARXIV181111205; Cho Kyunghyun, 2014, ARXIV14067362; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; De Brabandere B, 2016, ADV NEUR IN, V29; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Gastaldi Xavier, 2017, ARXIV170507485; Goyal Priya, 2017, ARXIV170602677; Gross S., 2017, P IEEE C COMP VIS PA, P6865; Ha David, 2017, ICLR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Keskin Cem, 2018, ADV NEURAL PROCESSIN; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 1989, ADV NEURAL INFORM PR, V2; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu LQ, 2018, AAAI CONF ARTIF INTE, P8109; Loshchilov Ilya, 2016, INT C LEARN REPR; Luong Minh-Thang, 2015, EMPERICAL METHODS NA; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; McGill M, 2017, PR MACH LEARN RES, V70; Mullapudi RT, 2018, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR.2018.00843; Platanios Emmanouil Antonios, 2018, P 2018 C EMP METH NA, P425, DOI DOI 10.18653/V1/D18-1039; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Rosenbaum C., 2018, INT C LEARN REPR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Silberman N., 2016, TENSORFLOW SLIM IMAG; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Tan Mingxing, 2019, BRIT MACH VIS C BMVC; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang X, 2018, LECT NOTES COMPUT SC, V11217, P420, DOI 10.1007/978-3-030-01261-8_25; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu Felix, 2019, INT C LEARN REPR; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31	49	79	82	5	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301031
C	Jiang, JC; Lu, ZQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Jiechuan; Lu, Zongqing			Learning Attentional Communication for Multi-Agent Cooperation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely helps, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in a variety of cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods.	[Jiang, Jiechuan; Lu, Zongqing] Peking Univ, Beijing, Peoples R China	Peking University	Lu, ZQ (corresponding author), Peking Univ, Beijing, Peoples R China.	jiechuan.jiang@pku.edu.cn; zongqing.lu@pku.edu.cn			Peng Cheng Laboratory; NSFC [61872009]	Peng Cheng Laboratory; NSFC(National Natural Science Foundation of China (NSFC))	This work was supported in part by Peng Cheng Laboratory and NSFC under grant 61872009.	Cao YC, 2013, IEEE T IND INFORM, V9, P427, DOI 10.1109/TII.2012.2219061; Cheney DL, 2005, LINGUIST REV, V22, P135, DOI 10.1515/tlir.2005.22.2-4.135; Foerster JN, 2016, ADV NEUR IN, V29; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Gu S., 2017, 2017 IEEE INT C ROBO, P3389, DOI DOI 10.1109/ICRA.2017.7989385; Havrylov S, 2017, ADV NEUR IN, V30; Kong X., 2017, ARXIV171207305; Lample G, 2017, AAAI CONF ARTIF INTE, P2140; Lanctot M, 2017, ADV NEUR IN, V30; Le HM, 2017, PR MACH LEARN RES, V70; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Lowe R., 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295385; Matignon L., 2012, 26 AAAI C ART INT; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mordatch  I., 2018, P AAAI C ART INT; Peng P., 2017, ARXIV170310069; Pipattanasomporn M., 2009, IEEEPES POWER SYSTEM, P1, DOI DOI 10.1109/PSCE.2009.4840087; Silver D, 2014, PR MACH LEARN RES, V32; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Yang Yaodong, 2018, ARXIV PREPRINT ARXIV, P5571, DOI DOI 10.1115/FEDSM2018-83038	24	79	82	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001078
C	Liu, SF; De Mello, S; Gu, JW; Zhong, GY; Yang, MH; Kautz, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Sifei; De Mello, Shalini; Gu, Jinwei; Zhong, Guangyu; Yang, Ming-Hsuan; Kautz, Jan			Learning Affinity via Spatial Propagation Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be outputs from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, such as image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of deep CNNs. We validate the framework on the task of refinement of image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.	[Liu, Sifei; Yang, Ming-Hsuan] UC Merced, Merced, CA 95343 USA; [Liu, Sifei; De Mello, Shalini; Gu, Jinwei; Yang, Ming-Hsuan; Kautz, Jan] NVIDIA, Santa Clara, CA 95051 USA; [Zhong, Guangyu] Dalian Univ Technol, Dalian, Peoples R China	University of California System; University of California Merced; Nvidia Corporation; Dalian University of Technology	Liu, SF (corresponding author), UC Merced, Merced, CA 95343 USA.; Liu, SF (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.		Jeong, Yongwook/N-7413-2016; Yang, Ming-Hsuan/AAE-7350-2019; Liu, Sifei/AGE-1968-2022; Yang, Ming-Hsuan/T-9533-2019	Liu, Sifei/0000-0002-6011-3686; Yang, Ming-Hsuan/0000-0003-4848-2304	NSF CAREER [1149783]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work is supported in part by the NSF CAREER Grant #1149783, gifts from Adobe and NVIDIA.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arnab A, 2016, LECT NOTES COMPUT SC, V9906, P524, DOI 10.1007/978-3-319-46475-6_33; Bertasius G., 2016, ARXIV160507681; Byeon W., 2015, P IEEE C COMP VIS PA; Chen L.-C, 2015, P INT C LEARN REPR S; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fujiyoshi, 2015, IPSJ T COMPUTER VISI, V7, P99, DOI DOI 10.2197/ipsjtcva.7.99; Gersgorin S., 1931, B ACAD SCI URSS SM; Graves A, 2007, LECT NOTES COMPUT SC, V4668, P549; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kalchbrenner N., 2015, COMPUTER SCI; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Lin G., 2015, ARXIV150602108; Liu RS, 2016, IEEE T PATTERN ANAL, V38, P2457, DOI 10.1109/TPAMI.2016.2522415; Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34; Liu SF, 2015, PROC CVPR IEEE, P3451, DOI 10.1109/CVPR.2015.7298967; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maire M., 2015, CORR; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schwing A. G., 2015, ARXIV PREPRINT ARXIV; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Smith Brandon, 2013, CVPR; Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; van den Oord A, 2016, PR MACH LEARN RES, V48; Visin F., 2015, ARXIV150500393; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zilly J.G., 2016, ARXIV PREPRINT ARXIV	37	79	79	3	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401054
C	Salimbeni, H; Deisenroth, MP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Salimbeni, Hugh; Deisenroth, Marc Peter			Doubly Stochastic Variational Inference for Deep Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to overfitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalizations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm that does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.	[Salimbeni, Hugh; Deisenroth, Marc Peter] Imperial Coll London, London, England; [Salimbeni, Hugh; Deisenroth, Marc Peter] PROWLER Io, Cambridge, England	Imperial College London	Salimbeni, H (corresponding author), Imperial Coll London, London, England.; Salimbeni, H (corresponding author), PROWLER Io, Cambridge, England.	hrs13@ic.ac.uk; m.deisenroth@imperial.ac.uk	Jeong, Yongwook/N-7413-2016		Google Faculty Research Award; Microsoft Azure Scholarship	Google Faculty Research Award(Google Incorporated); Microsoft Azure Scholarship(Microsoft)	We have greatly appreciated valuable discussions with James Hensman and Steindor Saemundsson in the preparation of this work. We thank Vincent Dutordoir and anonymous reviewers for helpful feedback on the manuscript. We are grateful for a Microsoft Azure Scholarship and support through a Google Faculty Research Award to Marc Deisenroth.	Abadi M, 2015, P 12 USENIX S OPERAT; Baldi P., 2014, NATURE COMMUNICATION; Bengio Y., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556; Bonilla E. V., 2016, 160900577 ARXIV; Briol F.-X., 2015, 151200933 ARXIV; Bui TD, 2016, PR MACH LEARN RES, V48; Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626; Cutajar K, 2017, PR MACH LEARN RES, V70; Cutler M., 2015, IEEE INT C ROB AUT; Dai Z., 2016, INT C LEARN REPR; Damianou A. C., 2013, INT C ART INT STAT; Damianou Andreas C, 2011, ADV NEURAL INFORM PR; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Diggle PJ, 2007, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-48536-2; Duvenaud D., 2013, ARXIV13024922V4; Duvenaud D., 2014, ARTIFICIAL INTELLIGE; Gal Y., 2015, INT C MACH LEARN; Garnett R., 2009, P 26 ANN INT C MACH, P345; Guestrin C., 2005, P 22 INT C MACH LEAR, V22, P265; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hensman J., 2014, 14121370 ARXIV; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Ko J., 2008, IEEE INTELLIGENT ROB; Krauth K., 2016, 161005392 ARXIV; Lawrence N. D., 2007, P 24 INT C MACH LEAR, P481; Lazaro-Gredilla M., 2012, ADV NEURAL INFORM PR; Mackay D. J. C., 1999, NEURAL COMPUTATION; Matthews AGD, 2016, JMLR WORKSH CONF PRO, V51, P231; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Mattos C. L. C, 2016, INT C LEARN REPR; Peng H., 2017, 170406735 ARXIV; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Titsias M. K., 2013, ADV NEURAL INFORM PR; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Turner R., 2011, BAYESIAN TIME SERIES; Vafa K., 2016, ADV APPROXIMATE BAYE; Wang Y., 2016, ARTIFICIAL INTELLIGE; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370	44	79	79	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404064
C	Jie, ZQ; Liang, XD; Feng, JS; Jin, XJ; Lu, WF; Yan, SC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jie, Zequn; Liang, Xiaodan; Feng, Jiashi; Jin, Xiaojie; Lu, Wen Feng; Yan, Shuicheng			Tree-Structured Reinforcement Learning for Sequential Object Localization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-RL) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-RL approach learns multiple searching policies through maximizing the long-term reward that reflects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-RL approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-RL offers more diversity in search paths and is able to find multiple objects with a single feed-forward pass. Therefore, Tree-RL can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on PASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.	[Jie, Zequn; Feng, Jiashi; Jin, Xiaojie; Lu, Wen Feng; Yan, Shuicheng] Natl Univ Singapore, Singapore, Singapore; [Liang, Xiaodan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	National University of Singapore; Carnegie Mellon University	Jie, ZQ (corresponding author), Natl Univ Singapore, Singapore, Singapore.		Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022		National University of Singapore startup grant [R-263-000-C08-133]; Ministry of Education of Singapore AcRF Tier One grant [R-263-000-C21-112]	National University of Singapore startup grant(National University of Singapore); Ministry of Education of Singapore AcRF Tier One grant(Ministry of Education, Singapore)	The work of Jiashi Feng was partially supported by National University of Singapore startup grant R-263-000-C08-133 and Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112.	Alexe B., 2012, NIPS; Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba J., 2014, ARXIV; Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Collobert R., 2011, NIPS WORKSHOP; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gonzalez-Garcia Abel, 2015, CVPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908; Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu Yongxi, 2015, ARXIV151207711; Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316; Mathe Stefan, 2014, ARXIV14120100; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2014, ADV NEUR IN, V27; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	28	79	79	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704056
C	Mika, S; Ratsch, G; Muller, KR		Leen, TK; Dietterich, TG; Tresp, V		Mika, S; Ratsch, G; Muller, KR			A mathematical programming approach to the Kernel Fisher algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				DISCRIMINANT-ANALYSIS	We investigate a new kernel-based classifier: the Kernel Fisher Discriminant (KFD). A mathematical programming formulation based on the observation that KFD maximizes the average margin permits an interesting modification of the original KFD algorithm yielding the sparse KID. We find that both, KID and the proposed sparse KFD, can be understood in an unifying probabilistic context. Furthermore, we show connections to Support Vector Machines and Relevance Vector Machines. From this understanding, we are able to outline an interesting kernel-regression technique based upon the KFD algorithm. Simulations support the usefulness of our approach.	GMD FIRST IDA, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Mika, S (corresponding author), GMD FIRST IDA, Kekulestr 7, D-12489 Berlin, Germany.		Mueller, Klaus-Robert/Y-3547-2019; Rätsch, Gunnar/O-5914-2017; Rätsch, Gunnar/B-8182-2009	Mueller, Klaus-Robert/0000-0002-3861-7685; Rätsch, Gunnar/0000-0001-5486-8532; 				Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980; BOSER B, 1992, P 5 ANN WORKSH COMP, V5, P144; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Mika S, 2000, ADV NEUR IN, V12, P526; MIKA S, 2001, IN PRESS P AISTATS 2; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; Roth V, 2000, ADV NEUR IN, V12, P568; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SMOLA AJ, 1998, THESIS TU BERLIN; Tipping ME, 2000, ADV NEUR IN, V12, P652; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	12	79	88	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						591	597						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800084
C	Vapnik, VN; Mukherjee, S		Solla, SA; Leen, TK; Muller, KR		Vapnik, VN; Mukherjee, S			Support vector method for multivariate density estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					A new method for multivariate density estimation is developed based on the Support Vector Method (SVM) solution of inverse ill-posed problems. The solution has the form of a mixture of densities. This method with Gaussian kernels compared favorably to both Parzen's method and the Gaussian Mixture Model method. For synthetic data we achieve more accurate estimates for densities of 2, 6, 12, and 40 dimensions.	Royal Halloway Coll & AT&T Labs, Red Bank, NJ 07701 USA	AT&T	Vapnik, VN (corresponding author), Royal Halloway Coll & AT&T Labs, 100 Schultz Dr, Red Bank, NJ 07701 USA.			Mukherjee, Sayan/0000-0002-6715-3920				BASU S, 1998, NONLINEAR MODELING A; Morozov VA., 1984, METHODS SOLVING INCO; MUKHERJEE S, 1999, 1653 AI MIT; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; PHILLIPS DL, 1962, J ACM, V9, P84, DOI 10.1145/321105.321114; REYNOLDS D, 1995, IEEE T SPEECH AUDIO, V3, P1; Vapnik V.N, 1998, STAT LEARNING THEORY; Vapnik V. N., 1978, AVTOMAT TELEMEKH, P38; Vasin V., 1970, MATH NOTES ACAD SCI, V7, P161, DOI [10.1007/bf01093105, DOI 10.1007/BF01093105]	10	79	85	0	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						659	665						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700094
C	Kosiorek, AR; Sabour, S; Teh, YW; Hinton, GE		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kosiorek, Adam R.; Sabour, Sara; Teh, Yee Whye; Hinton, Geoffrey E.			Stacked Capsule Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%).	[Kosiorek, Adam R.] Univ Oxford, Oxford Robot Inst, Appl AI Lab, Oxford, England; [Kosiorek, Adam R.] Univ Oxford, Dept Stat, Oxford, England; [Kosiorek, Adam R.; Sabour, Sara; Hinton, Geoffrey E.] Google Brain, Toronto, ON, Canada; [Teh, Yee Whye] DeepMind, London, England	University of Oxford; University of Oxford	Kosiorek, AR (corresponding author), Univ Oxford, Oxford Robot Inst, Appl AI Lab, Oxford, England.	adamk@robots.ox.ac.uk						Ba J., 2016, ABS160706450 CORR; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Burgess Christopher P, 2019, ARXIV190111390; Cohen T., 2017, INT C REPR LEARN; Cohen TS, 2016, PR MACH LEARN RES, V48; Dieleman S., 2016, CORR; Engelcke M., 2019, CORR; Eslami SM, 2016, NEURIPS, V1; Greff K., 2019, CORR; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Haeusser P., 2018, GERM C PATT REC; Hinton G. E., 1979, COGNITIVE SCI, V3; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hinton Geoffrey E., 2018, INT C LEARN REPR; Hjelm R. D., 2019, CORR; Hu WH, 2017, PR MACH LEARN RES, V70; Jacobsen J.-H., 2017, CORR; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jaiswal A, 2018, P EUR C COMP VIS EC; Ji X., 2018, CORR; Kosiorek AR, 2018, ADV NEUR IN, V31; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lee J, 2019, PR MACH LEARN RES, V97; Lenssen J. E., 2018, ADV NEURAL INFORM PR; Li HY, 2018, LECT NOTES COMPUT SC, V11215, P266, DOI 10.1007/978-3-030-01252-6_16; Maddison Chris J, 2017, ICLR; Oyallon E., 2015, IEEE C COMP VIS PATT; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rawlinson David, 2018, ARXIV180406094; Rock I., 1973, ORIENTATION AND FORM; Sabour Sara, 2017, PROC 31 INT C NEURAL; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tieleman T., 2014, OPTIMIZING NEURAL NE; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang D, 2018, OPTIMIZATION VIEW DY; Zhang S., 2018, INT S ART INT ROB	37	78	81	5	14	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907020
C	Fried, D; Hu, RH; Cirik, V; Rohrbach, A; Andreas, J; Morency, LP; Berg-Kirkpatrick, T; Saenko, K; Klein, D; Darrell, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fried, Daniel; Hu, Ronghang; Cirik, Volkan; Rohrbach, Anna; Andreas, Jacob; Morency, Louis-Philippe; Berg-Kirkpatrick, Taylor; Saenko, Kate; Klein, Dan; Darrell, Trevor			Speaker-Follower Models for Vision-and-Language Navigation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach-speaker-driven data augmentation, pragmatic reasoning and panoramic action space-dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.	[Fried, Daniel; Hu, Ronghang; Rohrbach, Anna; Andreas, Jacob; Klein, Dan; Darrell, Trevor] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Cirik, Volkan; Morency, Louis-Philippe; Berg-Kirkpatrick, Taylor] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Saenko, Kate] Boston Univ, Boston, MA 02215 USA	University of California System; University of California Berkeley; Carnegie Mellon University; Boston University	Fried, D (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.		Hu, Ronghang/Q-8559-2019	Saenko, Kate/0000-0002-7564-7218	US DoD; DARPA XAI; D3M; NSF [IIS-1833355]; Oculus VR; Berkeley Artificial Intelligence Research (BAIR) Lab; Huawei / Berkeley AI fellowship	US DoD(United States Department of Defense); DARPA XAI; D3M; NSF(National Science Foundation (NSF)); Oculus VR(Swedish Research Council); Berkeley Artificial Intelligence Research (BAIR) Lab; Huawei / Berkeley AI fellowship	This work was partially supported by US DoD and DARPA XAI and D3M, NSF awards IIS-1833355, Oculus VR, and the Berkeley Artificial Intelligence Research (BAIR) Lab. DF was supported by a Huawei / Berkeley AI fellowship. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors, and no official endorsement should be inferred.	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Andreas Jacob, 2015, P C EMP METH NAT LAN, P1165; Andreas Jacob, 2016, P C EMP METH NAT LAN; Artzi Y, 2013, T ASSOC COMPUT LING, V1, P49, DOI DOI 10.1162/TACL_A_00209; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Branavan S.R.K., 2009, P JOINT C 47 ANN M A, P82, DOI DOI 10.3115/1687878.1687892; Chang Angel, 2017, INT C 3D VIS 3DV; Chen D, 2012, P 50 ANN M ASS COMP, V1, P430; Chen K, 2017, IEEE I CONF COMP VIS, P824, DOI 10.1109/ICCV.2017.95; Cirik V., 2018, 32 AAAI C ART INT AA; Cohn- Gordon R., 2018, P C N AM CHAPT ASS C; Das A., 2018, P IEEE C COMP VIS PA; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Frank M. C., 2009, P ANN C COGN SCI SOC; Frank MC, 2012, SCIENCE, V336, P998, DOI 10.1126/science.1218633; Fried Daniel, 2018, P C N AM CHAPT ASS C; Goodman ND, 2013, TOP COGN SCI, V5, P173, DOI 10.1111/tops.12007; Grice H.P., 1975, SYNTAX SEMANTICS VOL, P41, DOI DOI 10.1163/9789004368811_003; Gulcehre Caglar, 2015, USING MONOLINGUAL CO; Guu K, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1051, DOI 10.18653/v1/P17-1097; He, 2017, ARXIV171204440; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hermann K. M., 2017, CORR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu R., 2016, P EUR C COMP VIS ECC; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Liu Chenxi, 2017, P IEEE INT C COMP VI, P1271; Long Reginald, 2016, P ANN M ASS COMP LIN; Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; McClosky David, 2006, P MAIN C HUM LANG TE, P3, DOI DOI 10.3115/1220835.1220855; Mei H., 2016, P C ART INT AAAI; Melis Edward, 2016, P 2016 C EMP METH NA, P1078; Misra Dipendra, 2017, P C EMPIRICAL METHOD, P1004; Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48; Pathak D, 2018, IEEE COMPUT SOC CONF, P2131, DOI 10.1109/CVPRW.2018.00278; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Potts C., 2017, T ASS COMPUTATIONAL, DOI [10.1162/tacl_a_00064, DOI 10.1162/TACL_A_00064]; Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; SCUDDER HJ, 1965, IEEE T INFORM THEORY, V11, P363, DOI 10.1109/tit.1965.1053799; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P86; Silver David, 2017, CORR; Smith N. J., 2013, ADV NEURAL INFORM PR, P3039; Sukhbaatar S, 2017, ARXIVABS170305407 CO; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tellex S., 2011, P NAT C ART INT; Vasudevan A. B., 2018, P IEEE WINT C APPL C; Vedantam R., 2017, P IEEE C COMP VIS PA, V3; Wang MZ, 2016, LECT NOTES COMPUT SC, V9912, P696, DOI 10.1007/978-3-319-46484-8_42; Wang X, 2018, LECT NOTES COMPUT SC, V11220, P38, DOI 10.1007/978-3-030-01270-0_3; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375	58	78	79	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303032
C	Geifman, Y; El-Yaniv, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Geifman, Yonatan; El-Yaniv, Ran			Selective Classification for Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				REJECT	Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.	[Geifman, Yonatan; El-Yaniv, Ran] Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel	Technion Israel Institute of Technology	Geifman, Y (corresponding author), Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel.	yonatan.g@cs.technion.ac.il; rani@cs.technion.ac.il	Jeong, Yongwook/N-7413-2016		Israel Science Foundation [1890/14]	Israel Science Foundation(Israel Science Foundation)	This research was supported by The Israel Science Foundation (grant No. 1890/14)	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; De Stefano C, 2000, IEEE T SYST MAN CY C, V30, P84, DOI 10.1109/5326.827457; El-Yaniv R, 2012, J MACH LEARN RES, V13, P255; El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605; Freund Y, 2004, ANN STAT, V32, P1698, DOI 10.1214/009053604000000058; Fumera G, 2002, LECT NOTES COMPUT SC, V2388, P68; Gal Y, 2016, PR MACH LEARN RES, V48; GASCUEL O, 1992, PATTERN RECOGN LETT, V13, P757, DOI 10.1016/0167-8655(92)90125-J; Gelbhart R., 2017, ARXIV E PRINTS; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HELLMAN ME, 1970, IEEE T SYST SCI CYB, VSSC6, P179, DOI 10.1109/TSSC.1970.300339; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Liu S, 2015, PATT REC ACPR 2015 3, DOI [10.1109/ACPR.2015.7486599, DOI 10.1109/ACPR.2015.7486599]; Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Varshney KR, 2011, 2011 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P769, DOI 10.1109/SSP.2011.5967817; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]	20	78	79	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404092
C	Wen, W; Xu, C; Yan, F; Wu, CP; Wang, YD; Chen, YR; Li, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wen, Wei; Xu, Cong; Yan, Feng; Wu, Chunpeng; Wang, Yandan; Chen, Yiran; Li, Hai			TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DESCENT	High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1, 0, 1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet doesn't incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available(1).	[Wen, Wei; Wu, Chunpeng; Chen, Yiran; Li, Hai] Duke Univ, Durham, NC 27706 USA; [Xu, Cong] Hewlett Packard Labs, Palo Alto, CA USA; [Yan, Feng] Univ Nevada, Reno, NV 89557 USA; [Wang, Yandan] Univ Pittsburgh, Pittsburgh, PA 15260 USA	Duke University; Hewlett-Packard; Nevada System of Higher Education (NSHE); University of Nevada Reno; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Wen, W (corresponding author), Duke Univ, Durham, NC 27706 USA.	wei.wen@duke.edu; cong.xu@hpe.com; fyan@unr.edu; chunpeng.wu@duke.edu; yaw46@pitt.edu; yiran.chen@duke.edu; hai.li@duke.edu	Wen, Wei/AAO-5266-2020; Yan, Feng/AAO-9020-2021; Li, Hai/L-8558-2017; Jeong, Yongwook/N-7413-2016	Wen, Wei/0000-0003-0027-4821; Li, Hai/0000-0003-3228-6544; 	NSF [CCF-1744082]; DOE [SC0017030]	NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE))	This work was supported in part by NSF CCF-1744082 and DOE SC0017030. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, DOE, or their contractors. Thanks Ali Taylan Cemgil at Bogazici University for valuable suggestions on this work.	Abadi M., TENSORFLOW LARGE SCA; Alistarh D, 2017, ADV NEUR IN, V30; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bradley Joseph K, 2011, ARXIV11055379; Chen T, 2015, 151201274 ARXIV; CHILIMBI TM, 2014, P OSDI, V14, P571; Coates A., 2013, INT C MACHINE LEARNI, P1337; Dean J., 2012, NIPS 12, V1, P1223; Garg R., 2009, P 26 ANN INT C MACHI, P337; Gupta S., 2015, P 32 INT C MACH LEAR, V37, P1737; HAN S., 2015, ARXIV151000149; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heafield, 2017, 170405021 ARXIV; Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223; Hubara I, 2016, ADV NEUR IN, V29; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Keskar Nitish Shirish, 2017, INT C LEARN REPR, DOI [10.48550/arxiv.1609.04836, DOI 10.48550/ARXIV.1609.04836]; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; Li Mu, 2017, THESIS, P5; Lin Z., 2015, ARXIV151003009; Moritz Philipp, 2015, 151106051 ARXIV; Neelakantan Arvind, 2015, 151106807 ARXIV; Ott J., 2016, ARXIV160806902; Pan Xinghao, 2017, 170205800 ARXIV; Park Jongsoo, 2017, INT C LEARN REPR ICL; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Saad D, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Seide F, 2014, INTERSPEECH, P1058; Suresh Ananda Theertha, 2016, ARXIV161100429; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy Christian, 2015, 14094842 ARXIV; Wen W., 2016, ADV NEURAL INFORM PR, P2074; Wen W., 2017, ARXIV170905027; Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014; Yan F, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1355, DOI 10.1145/2783258.2783270; Zhang S., 2015, NEURAL INFORM PROCES, P685; Zhang W., 2016, PROC 25 INT JOINT C, P2350; Zhou Shuchang, 2016, P IEEE C COMP VIS PA; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	44	78	78	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401053
C	Daniely, A; Frostig, R; Singer, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Daniely, Amit; Frostig, Roy; Singer, Yoram			Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BOUNDS; SIZE	We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.	[Daniely, Amit; Frostig, Roy; Singer, Yoram] Google Brain, Mountain View, CA 94043 USA; [Frostig, Roy] Stanford Univ, Stanford, CA 94305 USA	Google Incorporated; Stanford University	Daniely, A (corresponding author), Google Brain, Mountain View, CA 94043 USA.							Andoni A, 2014, PR MACH LEARN RES, V32, P1908; Anselmi F., 2015, ARXIV150801084; Anthony M., 1999, NEURAL NETWORK LEARN, V9; Arora S, 2014, PR MACH LEARN RES, V32; Bach F., 2014, ARXIV14128690; Bach F., 2015, EQUIVALENCE KERNEL Q; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Baum EB, 1989, NEURAL COMPUT, V1, P151, DOI 10.1162/neco.1989.1.1.151; Bo LF, 2011, PROC CVPR IEEE, P1729, DOI 10.1109/CVPR.2011.5995719; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Daniely A., 2016, COLT; Giryes R., 2015, ARXIV150408291; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Hardt M., 2015, PREPRINT; Harris Zellig S, 1954, WORD; Hazan T., 2015, ARXIV150805133; Kar P., 2012, ARXIV12016530; Kearns M., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P433, DOI 10.1145/73007.73049; Klivans A. R., 2006, FOCS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Levy O, 2014, ADV NEUR IN, V27; Livni R, 2014, ADV NEUR IN, V27; Mairal J., 2014, ADV NEURAL INFORM PR, V27, P2627; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Neal RM, 1996, LECT NOTES STAT, V118; Neyshabur B., 2015, ADV NEURAL INFORM PR, P2413; ODonnell Ryan, 2014, ANAL BOOLEAN FUNCTIO; Pennington J., 2015, ADV NEURAL INFORM PR, P1837; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Safran I., 2015, ARXIV151104210; Saitoh S., 1988, THEORY REPROD KERNEL; Schoenberg I., 1942, DUKE MATH J, V9, P96, DOI DOI 10.1215/S0012-7094-42-00908-6; Scholkopf B, 1998, ADV NEUR IN, V10, P640; SEDGHI H., 2014, ARXIV14122693; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Williams C. K. I., 1997, COMPUTATION INFINITE, P295	45	78	78	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703063
C	Wisdom, S; Powers, T; Hershey, JR; Le Roux, J; Atlas, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wisdom, Scott; Powers, Thomas; Hershey, John R.; Le Roux, Jonathan; Atlas, Les			Full-Capacity Unitary Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs.	[Wisdom, Scott; Powers, Thomas; Atlas, Les] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA; [Hershey, John R.; Le Roux, Jonathan] Mitsubishi Elect Res Labs, Cambridge, MA USA	University of Washington; University of Washington Seattle	Wisdom, S (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.	swisdom@uw.edu; tcpowers@uw.edu; hershey@merl.com; leroux@merl.com; atlas@uw.edu	Wisdom, Scott/AHD-8304-2022		U.S. ONR [N00014-12-G-0078]; U.S. ARO [W911NF-15-1-0450]	U.S. ONR(Office of Naval Research); U.S. ARO	We thank an anonymous reviewer for suggesting improvements to our proof in Section 3 and Vamsi Potluru for helpful discussions. Scott Wisdom and Thomas Powers were funded by U.S. ONR contract number N00014-12-G-0078, delivery orders 13 and 24. Les Atlas was funded by U.S. ARO grant W911NF-15-1-0450.	[Anonymous], 2000, PERCEPTUAL EVALUATIO; Arjovsky M, 2016, PR MACH LEARN RES, V48; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Brookes M., 2002, VOICEBOX SPEECH PROC; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Garofolo J. S., 1993, SPACE TERR INTEGR NE, V93, P27; Gilmore R, 2008, LIE GROUPS, PHYSICS, AND GEOMETRY, P1, DOI 10.1017/CBO9780511791390; Halberstadt A. K., 1998, THESIS; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOUSEHOLDER AS, 1958, J ACM, V5, P339, DOI 10.1145/320941.320947; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Le Q.V., 2015, ABS150400941 CORR; Loizou P. C., 2007, SPEECH ENHANCEMENT T, DOI [10.1201/9781420015836, DOI 10.1201/9781420015836]; Mnih V, 2014, ADV NEUR IN, V27; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023; Sard A., 1942, B AM MATH SOC, V48, P883, DOI [10.1090/S0002-9904-1942-07811-6, DOI 10.1090/S0002-9904-1942-07811-6, 10.1090/s0002-9904-1942-07811-6]; Saxe Andrew M, 2013, ARXIV13126120; Taal CH, 2011, IEEE T AUDIO SPEECH, V19, P2125, DOI 10.1109/TASL.2011.2114881; Tagare Hemant D., 2011, TECHNICAL REPORT; Theano Development Team, 2016, ARXIV160502688 THEAN; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26	23	78	79	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702074
C	Bian, JW; Li, ZC; Wang, NY; Zhan, HY; Shen, CH; Cheng, MM; Reid, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bian, Jia-Wang; Li, Zhichao; Wang, Naiyan; Zhan, Huangying; Shen, Chunhua; Cheng, Ming-Ming; Reid, Ian			Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However, the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints, networks output scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works, our framework is much simpler and more efficient. Comprehensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the KITTI dataset. Moreover, we show that our ego-motion network is able to predict a globally scale-consistent camera trajectory for long video sequences, and the resulting visual odometry accuracy is competitive with the recent model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep networks trained using unlabelled monocular videos can predict globally scale-consistent camera trajectories over a long video sequence.	[Bian, Jia-Wang; Zhan, Huangying; Shen, Chunhua; Reid, Ian] Univ Adelaide, Adelaide, SA, Australia; [Bian, Jia-Wang; Zhan, Huangying; Shen, Chunhua; Reid, Ian] Australian Ctr Robot Vis, Brisbane, Qld, Australia; [Li, Zhichao; Wang, Naiyan] TuSimple, Beijing, Peoples R China; [Cheng, Ming-Ming] Nankai Univ, Tianjin, Peoples R China	University of Adelaide; Australian Centre for Robotic Vision; Nankai University	Bian, JW (corresponding author), Univ Adelaide, Adelaide, SA, Australia.; Bian, JW (corresponding author), Australian Ctr Robot Vis, Brisbane, Qld, Australia.		Bian, Jia-Wang/AAP-2274-2020; Bian, Jia-Wang/AAH-4463-2019; Cheng, Ming-Ming/A-2527-2009	Bian, Jia-Wang/0000-0003-2046-3363; Bian, Jia-Wang/0000-0003-2046-3363; Cheng, Ming-Ming/0000-0001-5550-8758; Reid, Ian/0000-0001-7790-6423	Australian Centre for Robotic Vision; Major Project for New Generation of AI [2018AAA0100400]; NSFC [61922046]	Australian Centre for Robotic Vision; Major Project for New Generation of AI; NSFC(National Natural Science Foundation of China (NSFC))	The work was supported by the Australian Centre for Robotic Vision, the Major Project for New Generation of AI (No. 2018AAA0100400), and NSFC (NO. 61922046). Jiawang would also like to thank TuSimple, where he started research in this field.	Baker B, 2004, JOM-US, V56; Bian JW, 2020, INT J COMPUT VISION, V128, P1580, DOI 10.1007/s11263-019-01280-3; Bian Jia-Wang, 2019, BRITISH MACHINE VISI; Cordts Marius, 2016, IEEE CONFERENCE ON C; Eigen David, 2014, NEURIPS; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Hartley R., 2003, MULTIPLE VIEW GEOMET; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jiao JB, 2018, LECT NOTES COMPUT SC, V11219, P55, DOI 10.1007/978-3-030-01267-0_4; Kingma D.P, P 3 INT C LEARNING R; Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238; Liu Fayao, 2016, IEEE T PATTERN RECOG, V38; Liu XM, 2018, RES HANDB FINANC LAW, P128; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594; Mur-Artal Raul, 2015, IEEE T ROBOTICS TRO, V31; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Tang CR, 2020, PHARMACOLOGY, V105, P339, DOI 10.1159/000503865; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang Y, 2019, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2019.00149; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yang Z., 2018, AAAI C ART INT; Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578; Yin Z., 2018, IEEE C COMP VIS PATT; Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620; Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	32	77	77	6	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300004
C	Chen, WZ; Gao, J; Ling, H; Smith, EJ; Lehtinen, J; Jacobson, A; Fidler, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Wenzheng; Gao, Jun; Ling, Huan; Smith, Edward J.; Lehtinen, Jaakko; Jacobson, Alec; Fidler, Sanja			Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-R, a differentiable rendering framework which allows gradients to be analytically computed for all pixels in an image. Key to our approach is to view foreground rasterization as a weighted interpolation of local properties and background rasterization as a distance-based aggregation of global geometry. Our approach allows for accurate optimization over vertex positions, colors, normals, light directions and texture coordinates through a variety of lighting models. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively using 2D supervision.	[Chen, Wenzheng; Gao, Jun; Ling, Huan; Smith, Edward J.; Lehtinen, Jaakko; Fidler, Sanja] NVIDIA, Santa Clara, CA 95051 USA; [Chen, Wenzheng; Gao, Jun; Ling, Huan; Jacobson, Alec; Fidler, Sanja] Univ Toronto, Toronto, ON, Canada; [Chen, Wenzheng; Gao, Jun; Ling, Huan; Fidler, Sanja] Vector Inst, Toronto, ON, Canada; [Smith, Edward J.] McGill Univ, Montreal, PQ, Canada; [Lehtinen, Jaakko] Aalto Univ, Espoo, Finland	Nvidia Corporation; University of Toronto; McGill University; Aalto University	Chen, WZ (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.; Chen, WZ (corresponding author), Univ Toronto, Toronto, ON, Canada.; Chen, WZ (corresponding author), Vector Inst, Toronto, ON, Canada.	wenzchen@nvidia.com; jung@nvidia.com; huling@nvidia.com; esmith@nvidia.com; jlehtinen@nvidia.com; jacobson@cs.toronto.edu; sfidler@nvidia.com	Lehtinen, Jaakko/G-2328-2013		DARPA under the REVEAL program; NSERC	DARPA under the REVEAL program; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	Wenzheng Chen wants to thank the support of DARPA under the REVEAL program and NSERC under the COHESA Strategic Network.	Achlioptas Panos, 2017, ARXIV170702392; Arjovsky M., 2017, ARXIV170107875; Bardzell J, 2019, INTERNET THINGS-TECH, P3, DOI 10.1007/978-3-319-94659-7_1; Chang Angel X., 2015, ARXIV151203012CSGR P; Doll ar P., 2017, P ICCV, DOI DOI 10.1109/ICCV.2017.322; Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greene N., 1993, Computer Graphics Proceedings, P231, DOI 10.1145/166117.166147; Gulrajani I, 2017, P NIPS 2017; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HENDERSON P, 2018, P BRIT MACH VIS C; Insafutdinov E, 2018, ADV NEUR IN, V31; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Kingma D.P, P 3 INT C LEARNING R; Lambert J. H., 1760, PHOTOMETRIA; Li TM, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275109; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu HTD, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275047; Liu Shichen, 2019, ARXIV190105567; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Luna F., 2012, INTRO 3D GAME PROGRA; Mirza M., 2014, ARXIV; Petersen F., 2019, ARXIV190311149; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Smith EJ, 2019, PR MACH LEARN RES, V97; Smith Edward J., 2017, ABS170709557 CORR; Szabo Attila, 2018, ARXIV181110519; Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Welinder P., 2010, CNSTR2010001 CALTECH; Woo Mason, 1999, OPENGL PROGRAMMING G; Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40; Xiang Yu, 2014, IEEE WINT C APPL COM; Yang B, 2017, IEEE INT CONF COMP V, P679, DOI 10.1109/ICCVW.2017.86; Yao Shunyu, 2018, ADV NEURAL INFORM PR	40	77	77	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901026
C	Donini, M; Oneto, L; Ben-David, S; Shawe-Taylor, J; Pontil, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Donini, Michele; Oneto, Luca; Ben-David, Shai; Shawe-Taylor, John; Pontil, Massimiliano			Empirical Risk Minimization Under Fairness Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our methodology. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.	[Donini, Michele; Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy; [Oneto, Luca] Univ Genoa, Genoa, Italy; [Ben-David, Shai] Univ Waterloo, Waterloo, ON, Canada; [Shawe-Taylor, John; Pontil, Massimiliano] UCL, London, England	Istituto Italiano di Tecnologia - IIT; University of Genoa; University of Waterloo; University of London; University College London	Donini, M (corresponding author), Ist Italiano Tecnol, Genoa, Italy.		Oneto, Luca/V-2595-2019; Donini, Michele/AAG-5799-2020	Oneto, Luca/0000-0002-8445-395X; Donini, Michele/0000-0002-9769-3899; Shawe-Taylor, John/0000-0002-2030-0073	SAP SE; EPSRC	SAP SE; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We wish to thank Amon Elders, Theodoros Evgeniou and Andreas Maurer for useful comments. This work was supported in part by SAP SE and the EPSRC.	Adebayo J., 2016, C FAIRN ACC TRANSP M; Agarwal A., 2017, C FAIRN ACC TRANSP M; Agarwal Alekh, 2018, ARXIV180302453; Alabi D., 2018, ARXIV180404503; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bechavod Y., 2018, ARXIV170700044V3; Berk R., 2017, CONVEX FRAMEWORK FAI; Beutel A., 2017, C FAIRN ACC TRANSP M; Calders T., 2009, IEEE INT C DAT MIN; Calmon Flavio, 2017, ADV NEURAL INFORM PR; Chierichetti F., 2017, ADV NEURAL INFORM PR; Donini M., 2017, NIPS WORKSH PRIOR ON; Dwork C, 2018, P 1 C FAIRNESS ACCOU; Feldman Michael, 2015, INT C KNOWL DISC DAT; Hardt M., 2016, ADV NEURAL INFORM PR; Jabbari S., 2016, C FAIRN ACC TRANSP M; Joseph M., 2016, ADV NEURAL INFORM PR; Kamiran F., 2009, INT C COMP CONTR COM; Kamiran F., 2010, MACH LEARN C; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kamishima T., 2011, INT C DAT MIN WORKSH; Kearns M., 2017, ARXIV171105144; Kusner Matt J, 2017, ADV NEURAL INFORM PR; Lum K., 2016, ARXIV161008077; Maurer Andreas, 2004, CS0411099 ARXIV; Menon A. K., 2018, C FAIRN ACC TRANSP; Oneto L., 2019, AAAI ACM C AI ETH SO; Parascandolo G., 2017, ADV NEURAL INFORM PR; Perez-Suay A., 2017, MACHINE LEARNING KNO; Pleiss G., 2017, ADV NEURAL INFORM PR; Quadrianto Novi, 2017, ADV NEURAL INFORM PR; Rockafellar R. T., 1970, CONVEX ANAL; Scholkopf B., 2001, COMPUTATIONAL LEARNI; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Smola A. J., 2001, LEARNING KERNELS; Vapnik V.N, 1998, STAT LEARNING THEORY; Woodworth B., 2017, COMPUTATIONAL LEARNI; Yao S., 2017, ADV NEURAL INFORM PR; Zafar M. B., 2017, ADV NEURAL INFORM PR; Zafar M. B., 2017, INT C ART INT STAT; Zafar M. B., 2017, INT C WORLD WID WEB; Zemel R, 2013, INT C MACH LEARN; Zliobaite I., 2015, 2 WORKSH FAIRN ACC T	44	77	77	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302078
C	Mirowski, P; Grimes, MK; Malinowski, M; Hermann, KM; Anderson, K; Teplyashin, D; Simonyan, K; Kavukcuoglu, K; Zisserman, A; Hadsell, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mirowski, Piotr; Grimes, Matthew Koichi; Malinowski, Mateusz; Hermann, Karl Moritz; Anderson, Keith; Teplyashin, Denis; Simonyan, Karen; Kavukcuoglu, Koray; Zisserman, Andrew; Hadsell, Raia			Learning to Navigate in Cities Without a Map	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation ("I am here") and a representation of the goal ("I am going there"). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. A key contribution of this paper is an interactive navigation environment that uses Google Street View for its photographic content and worldwide coverage. Our baselines demonstrate that deep reinforcement learning agents can learn to navigate in multiple cities and to traverse to target destinations that may be kilometres away. The project webpage http://streetlearn.cc contains a video summarizing our research and showing the trained agent in diverse city environments and on the transfer task, the form to request the StreetLearn dataset and links to further resources. The StreetLearn environment code is available at https://github.com/deepmind/streetlearn.	[Mirowski, Piotr; Grimes, Matthew Koichi; Malinowski, Mateusz; Hermann, Karl Moritz; Anderson, Keith; Teplyashin, Denis; Simonyan, Karen; Kavukcuoglu, Koray; Zisserman, Andrew; Hadsell, Raia] DeepMind, London, England		Mirowski, P (corresponding author), DeepMind, London, England.	piotrmirowski@google.com; mkg@google.com; mateuszm@google.com; kmh@google.com; keithanderson@google.com; teplyashin@google.com; simonyan@google.com; korayk@google.com; zisserman@google.com; raia@google.com	Malinowski, Mateusz/AAI-8855-2020					Anderson Peter, 2017, ARXIV171107280; Banino Andrea, 2018, NATURE, P1; Beattie C., 2016, ARXIV161203801; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Berriel Rodrigo F, 2018, HEADING DIRECTION ES; Brahmbhatt Samarth, 2017, ARXIV170109135; Bruce J., 2017, ARXIV171110137; Brunner Gino, 2017, ARXIV171107479; Chang Angel, 2017, INT C 3D VIS 3DV; Chaplot D.S., 2017, ARXIV170607230; Chaplot D. S., 2018, INT C LEARN REPR; Cueva Christopher J, 2018, ARXIV180307770; Devin Coline, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2169, DOI 10.1109/ICRA.2017.7989250; Dhiman V, 2018, CRITICAL INVESTIGATI; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Dosovitskiy A., 2016, ARXIV161101779; Espeholt L, 2018, PR MACH LEARN RES, V80; Graves A, 2017, PR MACH LEARN RES, V70; Gupta, 2017, ARXIV171208125; Gupta S, 2017, PROC CVPR IEEE, P7272, DOI 10.1109/CVPR.2017.769; Hermann Karl Moritz, 2017, ARXIV170606551; Hill Felix, 2017, ARXIV171009867; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Kempka M, 2016, IEEE CONF COMPU INTE; Khan Arbaaz, 2017, ARXIV170905706; Khosla A, 2014, PROC CVPR IEEE, P3710, DOI 10.1109/CVPR.2014.474; Koltun, 2018, ICLR, P1; Kolve Eric, 2017, ARXIV171205474; Lample G, 2017, AAAI CONF ARTIF INTE, P2140; Li H., 2018, ARXIV180208824; Malinowski M, 2017, INT J COMPUT VISION, V125, P110, DOI 10.1007/s11263-017-1038-2; Milford MJ, 2004, IEEE INT CONF ROBOT, P403, DOI 10.1109/ROBOT.2004.1307183; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V, 2016, PR MACH LEARN RES, V48; Parisotto Emilio, 2017, ARXIV170208360; Shah Shital, 2018, FIELD SERVICE ROBOTI, P621, DOI [10.1007/978-3-319-67361-5_40, DOI 10.1007/978-3-319-67361-5_40]; Tessler Chen, 2017, P 31 AAAI C ART INT; Wayne G., 2018, UNSUPERVISED PREDICT; Weyand T, 2016, LECT NOTES COMPUT SC, V9912, P37, DOI 10.1007/978-3-319-46484-8_3; Wu Y, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351296; Zaremba W, 2014, CORR; Zhang Jingwei, 2017, ARXIV170609520	45	77	80	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302043
C	Pidhorskyi, S; Almohsen, R; Adjeroh, DA; Doretto, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pidhorskyi, Stanislav; Almohsen, Ranya; Adjeroh, Donald A.; Doretto, Gianfranco			Generative Probabilistic Novelty Detection with Adversarial Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ANOMALY DETECTION; OUTLIER; CLASSIFICATION; LOCALIZATION	Novelty detection is the problem of identifying whether a new data point is considered to be an inlier or an outlier. We assume that training data is available to describe only the inlier distribution. Recent approaches primarily leverage deep encoder-decoder network architectures to compute a reconstruction error that is used to either compute a novelty score or to train a one-class classifier. While we too leverage a novel network of that kind, we take a probabilistic approach and effectively compute how likely it is that a sample was generated by the inlier distribution. We achieve this with two main contributions. First, we make the computation of the novelty probability feasible because we linearize the parameterized manifold capturing the underlying structure of the inlier distribution, and show how the probability factorizes and can be computed with respect to local coordinates of the manifold tangent space. Second, we improve the training of the autoencoder network. An extensive set of results show that the approach achieves state-of-the-art performance on several benchmark datasets.	[Pidhorskyi, Stanislav; Almohsen, Ranya; Adjeroh, Donald A.; Doretto, Gianfranco] West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA	West Virginia University	Pidhorskyi, S (corresponding author), West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.	stpidhorskyi@mix.wvu.edu; ralmohse@mix.wvu.edu; daadjeroh@mix.wvu.edu; gidoretto@mix.wvu.edu			National Science Foundation [IIS-1761792]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. IIS-1761792.	Bodesheim P, 2013, PROC CVPR IEEE, P3374, DOI 10.1109/CVPR.2013.433; BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; CONG Y, 2011, CVPR 2011, P3449, DOI DOI 10.1109/CVPR.2011.5995434; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DeVries Terrance, 2018, ARXIV180204865; Eskin Eleazar, 2002, APPL DATA MINING COM, P77, DOI DOI 10.1007/978-1-4615-0953-0_4; Eskin Eleazar, 2000, P INT C MACH LEARN; Ge ZQ, 2013, IND ENG CHEM RES, V52, P3543, DOI 10.1021/ie302069q; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86; Hautamaki V, 2004, INT C PATT RECOG, P430, DOI 10.1109/ICPR.2004.1334558; Hendrycks D., 2017, ICLR; Japkowicz N, 1995, INT JOINT CONF ARTIF, P518; Kadurin A, 2017, MOL PHARMACEUT, V14, P3098, DOI 10.1021/acs.molpharmaceut.7b00346; Kendall A., 2017, WHAT UNCERTAINTIES W, V3, P4; Khan SS, 2014, KNOWL ENG REV, V29, P345, DOI 10.1017/S026988891300043X; Kim J, 2012, J MACH LEARN RES, V13, P2529; Kingma DP, 2 INT C LEARN REPR I, P1; Knorr EM, 2000, VLDB J, V8, P237, DOI 10.1007/s007780050006; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Lerman G, 2015, FOUND COMPUT MATH, V15, P363, DOI 10.1007/s10208-014-9221-0; Lewis T., 1974, OUTLIERS STAT DATA; Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111; Liang Shiyu, 2018, INT C LEARN REPR; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Makhzani A, 2017, ADV NEUR IN, V30; Manevitz L, 2007, NEUROCOMPUTING, V70, P1466, DOI 10.1016/j.neucom.2006.05.013; Moonesinghe HDK, 2008, INT J ARTIF INTELL T, V17, P19, DOI 10.1142/S0218213008003753; Moonesinghe HDK, 2006, PROC INT C TOOLS ART, P532; Nene S. A., 1996, COLUMBIA OBJECT IMAG; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026; Rahmani M, 2017, IEEE T SIGNAL PROCES, V65, P6260, DOI 10.1109/TSP.2017.2749215; Ravanbakhsh Mahdyar, 2017, ARXIV170809644; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sabokrou M, 2016, ELECTRON LETT, V52, P1122, DOI 10.1049/el.2016.0440; Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356; Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780; Sakurada M., 2014, P MLSDA 2014 2 WORKS, P4, DOI [DOI 10.1145/2689746.2689747, 10.1145/2689746.2689747]; Salimans T., 2017, INT C LEARNING REPRE; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Simonyan K., 2015, ICLR; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Song FYYZS, 2015, ARXIV150603365; Tsakiris M., 2015, P IEEE INT C COMP VI, P10; van den Oord A., 2016, ARXIV160106759; Wang HG, 2018, FRONT INFORM TECH EL, V19, P116, DOI 10.1631/FITEE.1700786; Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177; Xiao H., 2017, ARXIV 170807747; Xu D, 2015, BMVC; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Xu Pingmei, 2015, ARXIV150406755; Yamanishi K, 2004, DATA MIN KNOWL DISC, V8, P275, DOI 10.1023/B:DAMI.0000023676.72185.7c; YOU C, 2017, ARXIV170403925	61	77	82	2	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001037
C	Duvenaudt, D; Maclaurin, D; Aguilera-Iparraguirre, J; Gomez-Bombarelli, R; Hirzel, T; Aspuru-Guzik, A; Adams, RP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Duvenaudt, David; Maclaurin, Dougal; Aguilera-Iparraguirre, Jorge; Gomez-Bombarelli, Rafael; Hirzel, Timothy; Aspuru-Guzik, Alan; Adams, Ryan P.			Convolutional Networks on Graphs for Learning Molecular Fingerprints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				AQUEOUS SOLUBILITY; NEURAL-NETWORK	We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.	[Duvenaudt, David; Maclaurin, Dougal; Aguilera-Iparraguirre, Jorge; Gomez-Bombarelli, Rafael; Hirzel, Timothy; Aspuru-Guzik, Alan; Adams, Ryan P.] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Duvenaudt, D (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.				Samsung Advanced Institute of Technology; NSF [IIS-1421780]	Samsung Advanced Institute of Technology(Samsung); NSF(National Science Foundation (NSF))	We thank Edward Pyzer-Knapp, Jennifer Wei, and Samsung Advanced Institute of Technology for their support. This work was partially funded by NSF IIS-1421780.	[Anonymous], 2015, ARXIV150202072; Arbib M.A., 1995, CONVOLUTIONAL NETWOR, V3361; Bastien F., 2012, DEEP LEARN UNS FEAT; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Dahl GE, 2014, ARXIV; Gamo FJ, 2010, NATURE, V465, P305, DOI 10.1038/nature09107; Glen RC, 2006, IDRUGS, V9, P199; Graves A, 2014, NEURAL TURING MACHIN; Grefenstette E., 2014, 52 ANN M ASS COMP LI; Hachmann J, 2011, J PHYS CHEM LETT, V2, P2241, DOI 10.1021/jz200866s; Hershey J. R., 2014, ARXIV14092574; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kingma D.P, P 3 INT C LEARNING R; Li W., 2013, INT C MACH LEARN; Lusci A, 2013, J CHEM INF MODEL, V53, P1563, DOI 10.1021/ci400187y; Micheli A, 2009, IEEE T NEURAL NETWOR, V20, P498, DOI 10.1109/TNN.2008.2010350; MORGAN HL, 1965, J CHEM DOC, V5, P107, DOI 10.1021/c160017a018; Oliphant TE, 2007, COMPUT SCI ENG, V9, P10, DOI 10.1109/MCSE.2007.58; Rogers D, 2010, J CHEM INF MODEL, V50, P742, DOI 10.1021/ci100050t; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Socher R., 2011, ADV NEURAL INF PROCE, V24, P1; Socher Richard, 2011, P C EMP METH NAT LAN, P151; Tai Kai Sheng, 2015, ARXIV150300075; Tox2l Challenge, 2014, TOX21 CHALL; Unterthiner T., 2015, ARXIV150301445; Unterthiner T., 2014, ADV NEURAL INFORM PR; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005	29	77	77	19	65	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102102
C	Chen, YK; Yang, T; Zhang, XY; Meng, GF; Xiao, XY; Sun, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Yukang; Yang, Tong; Zhang, Xiangyu; Meng, Gaofeng; Xiao, Xinyu; Sun, Jian			DetNAS: Backbone Search for Object Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Object detectors are usually equipped with backbone networks designed for image classification. It might be sub-optimal because of the gap between the tasks of image classification and object detection. In this work, we present DetNAS to use Neural Architecture Search (NAS) for the design of better backbones for object detection. It is non-trivial because detection training typically needs ImageNet pre-training while NAS systems require accuracies on the target detection task as supervisory signals. Based on the technique of one-shot supernet, which contains all possible networks in the search space, we propose a framework for backbone search on object detection. We train the supernet under the typical detector training schedule: ImageNet pre-training and detection fine-tuning. Then, the architecture search is performed on the trained supernet, using the detection task as the guidance. This framework makes NAS on backbones very efficient. In experiments, we show the effectiveness of DetNAS on various detectors, for instance, one-stage RetinaNet and the two-stage FPN. We empirically find that networks searched on object detection shows consistent superiority compared to those searched on ImageNet classification. The resulting architecture achieves superior performance than hand-crafted networks on COCO with much less FLOPs complexity. Code and models have been made available at: https://github.com/megvii-model/DetNAS.	[Chen, Yukang; Meng, Gaofeng; Xiao, Xinyu] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Yang, Tong; Zhang, Xiangyu; Sun, Jian] Megvii Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS	Chen, YK (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Yang, T (corresponding author), Megvii Technol, Beijing, Peoples R China.	yukang.chen@nlpr.ia.ac.cn; yangtong@megvii.com; zhangxiangyu@megvii.com; gfmeng@nlpr.ia.ac.cn; xinyu.xiao@nlpr.ia.ac.cn; sunjian@megvii.com		Zhang, Xiangyu/0000-0003-2138-4608	Major Project for New Generation of AI Grant [2018AAA0100402]; National Key R&D Program of China [2017YFA0700800]; National Natural Science Foundation of China [61976208, 91646207, 61573352, 61773377]; Beijing Academy of Artificial Intelligence (BAAI)	Major Project for New Generation of AI Grant; National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI)	This work is supported by Major Project for New Generation of AI Grant (No. 2018AAA0100402), National Key R&D Program of China (No. 2017YFA0700800), and the National Natural Science Foundation of China under Grants 61976208, 91646207, 61573352, and 61773377. This work is also supported by Beijing Academy of Artificial Intelligence (BAAI).	Bender G, 2018, PR MACH LEARN RES, V80; Brock Andrew, 2017, CORR; Cai Han, 2019, INT C LEARN REPR; Chang Jianlong, 2019, ABS190501786; Chao Peng, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6181, DOI 10.1109/CVPR.2018.00647; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Guo Zichao, 2019, ABS190400420; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Kaiming, 2019, RETHINKING IMAGENET; Li Liam, 2019, ABS190207638 CORR; Li Zeming, ECCV, P339; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Nekrasov V., 2018, P BRIT MACH VIS C; Pham H, 2018, PR MACH LEARN RES, V80; Qin Zheng, CORR; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Rezatofighi Hamid, 2019, CORR; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413; Xie S, 2019, INT C LEARNING REPRE; Yang T, 2018, ADV NEUR IN, V31; Zhu Xiaojin, 2018, CORR; Zoph B., 2016, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	32	76	80	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306062
C	Donahue, J; Simonyan, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Donahue, Jeff; Simonyan, Karen			Large Scale Adversarial Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models - including image generators and encoders - are available on TensorFlow Hub(1).	[Donahue, Jeff; Simonyan, Karen] DeepMind, London, England		Donahue, J (corresponding author), DeepMind, London, England.	jeffdonahue@google.com; simonyan@google.com						Boesen Anders, 2016, ICML; Brock A., 2019, INT C LEARNING REPRE; Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue Jeff, 2017, INT C LEARN REPR ICL; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Dugas Charles, 2000, NEURIPS; Dumoulin Vincent, 2017, ICLR; Gidaris Spyros, 2018, ARXIV180307728; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves Alex, 2018, ARXIV180402476; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henaff Olivier J., 2019, DATA EFFICIENT IMAGE, P2; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202; Lim Jae Hyun, 2017, ABS170502894 ARXIV; Luciano M, 2020, CLIN ORAL INVEST, V24, P841, DOI 10.1007/s00784-019-02972-3; Makhzani Alireza, 2016, ICLR WORKSH; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rosca Mihaela, 2017, ARXIV170604987; Salimans T, 2016, ADV NEUR IN, V29; Shang WL, 2016, PR MACH LEARN RES, V48; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tran D., 2017, NEURIPS; Ulyanov Dmitry, 2017, ARXIV170402304; van den Oord A, 2017, ADV NEUR IN, V30; van den Oord Aaron, 2018, ARXIV180703748; Yu H, 2016, DESIGN OF CMOS MILLIMETER-WAVE AND TERAHERTZ INTEGRATED CIRCUITS WITH METAMATERIALS, P1; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang Richard, 2016, CVPR	36	76	79	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902020
C	Pu, S; Song, YB; Ma, C; Zhang, HG; Yang, MH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pu, Shi; Song, Yibing; Ma, Chao; Zhang, Honggang; Yang, Ming-Hsuan			Deep Attentive Tracking via Reciprocative Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.	[Pu, Shi; Zhang, Honggang] Beijing Univ Posts & Telecommun, Beijing, Peoples R China; [Song, Yibing] Tencent AI Lab, Shenzhen, Peoples R China; [Ma, Chao] Shanghai Jiao Tong Univ, Shanghai, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA USA	Beijing University of Posts & Telecommunications; Tencent; Shanghai Jiao Tong University; University of California System; University of California Merced	Zhang, HG (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.	pushi_519200@bupt.edu.cn; dynamicstevenson@gmail.com; chaoma@sjtu.edu.cn; zhhg@bupt.edu.cn; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019; PU, SHI/ABG-1922-2021	Yang, Ming-Hsuan/0000-0003-4848-2304; PU, SHI/0000-0002-8748-8971	Beijing Municipal Science and Technology Commission project [Z181100001918005]; Fundamental Research Funds for the Central Universities [2017RC08]; NSF CAREER Grant [1149783]; China Scholarship Council	Beijing Municipal Science and Technology Commission project; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); NSF CAREER Grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); China Scholarship Council(China Scholarship Council)	The work is supported in part by the Beijing Municipal Science and Technology Commission project under Grant No. Z181100001918005, Fundamental Research Funds for the Central Universities (2017RC08), NSF CAREER Grant No. 1149783, and gifts from NVIDIA. Shi Pu is supported by a scholarship from China Scholarship Council.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Babenko B., 2011, IEEE PAMI; Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bischof H., 2006, BMVC, P47; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P1327, DOI 10.1109/TIP.2016.2520358; Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513; Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468; Chu Q., 2017, CVPR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490; Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84; Du W., 2017, CVPR; Galoogahi H. Kiani, 2017, CVPR; Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974; Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45; Henriques J. F., 2015, IEEE PAMI; Henriques J. F., 2012, EUR C COMP VIS, P702, DOI DOI 10.1007/978-3-642-33765-9_50; Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1; Hu J, 2017, 170901507 ARXIV; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kosiorek A., 2017, NIPS; Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14; Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632; Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18; Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515; Luo W., 2014, 14097618 ARXIV; Luo WH, 2018, PR MACH LEARN RES, V80; Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352; NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465; Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462; Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Smeulders A. W., 2014, IEEE PAMI; Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937; Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279; Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158; Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357; Wang N., 2015, 150104587 ARXIV; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148; Zhang J., 2014, EUR C COMP VIS; Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI 10.1109/CVPR.2017.512; Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108	58	76	80	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301088
C	Yang, F; Yang, ZL; Cohen, WW		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yang, Fan; Yang, Zhilin; Cohen, William W.			Differentiable Learning of Logical Rules for Knowledge Base Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.	[Yang, Fan; Yang, Zhilin; Cohen, William W.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Yang, F (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	fanyang1@cs.cmu.edu; zhiliny@cs.cmu.edu; wcoher@cs.cmu.edu			NSF [IIS1250956]; Google Research	NSF(National Science Foundation (NSF)); Google Research(Google Incorporated)	This work was funded by NSF under IIS1250956 and by Google Research.	Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Bordes A., 2014, P EMNLP, P615, DOI DOI 10.3115/V1/D14-1067; Bordes A., 2013, ADV NEURAL INFORM PR; Cohen William W., 2016, ARXIV160506523; Denham Woodrow W, 1973, THESIS; Evans C., 2008, P 2008 ACM SIGMOD IN, P1247, DOI [DOI 10.1145/1376616.1376746, 10.1145/1376616]; Getoor Lise, 2007, INTRO STAT RELATIONA; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kilgarriff A., 2000, WORDNET ELECT LEXICA; Kingma D.P, P 3 INT C LEARNING R; Kok S., 2007, P 24 INT C MACH LEAR, P433; Lao N, 2010, MACH LEARN, V81, P53, DOI 10.1007/s10994-010-5205-8; Lao Ni, 2011, P C EMP METH NAT LAN, P529, DOI DOI 10.5555/2145432.2145494; Miller A., 2016, ARXIV160603126, P1400; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Muggleton Stephen, 1996, ADV INDUCTIVE LOGIC, V32, P254; Muggleton Stephen, 1992, INDUCTIVE LOGIC PROG, V38; Neelakantan Arvind, 2016, INT C LEARN REPR ICL; Neelakantan Arvind, 2015, ARXIV151104834; Shen Y., 2016, ARXIV161104642; Socher R., 2013, ADV NEURAL INFORM PR, V26, P1; Toutanova K., 2015, P 3 WORKSH CONT VECT, DOI DOI 10.18653/V1/W15-4007; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang WY, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2129, DOI 10.1145/2505515.2505573; Wang William Yang, 2014, CIKM 2014; Weston J., 2014, ARXIV14103916; Yang Bishan, 2015, 3 INT C LEARN REPR I	29	76	76	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402036
C	van Rooyen, B; Menon, AK; Williamson, RC		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		van Rooyen, Brendan; Menon, Aditya Krishna; Williamson, Robert C.			Learning with Symmetric Label Noise: The Importance of Being Unhinged	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CLASSIFICATION; RISK	Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l(2) regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss' SLN-robustness is borne out in practice. So, with apologies to Wilde [1895], while the truth is rarely pure, it can be simple.	[van Rooyen, Brendan; Menon, Aditya Krishna; Williamson, Robert C.] Australian Natl Univ, Canberra, ACT, Australia; [van Rooyen, Brendan; Menon, Aditya Krishna; Williamson, Robert C.] Natl ICT Australia, Sydney, NSW, Australia	Australian National University; NICTA	van Rooyen, B (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.	brendan.vanrooyen@nicta.com.au; aditya.menon@nicta.com.au; bob.williamson@nicta.com.au			Australian Government through the Department of Communications; Australian Research Council through the ICT Centre of Excellence Program	Australian Government through the Department of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence Program(Australian Research Council)	NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. The authors thank Cheng Soon Ong for valuable comments on a draft of this paper.	Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bishop C.M, 2006, PATTERN RECOGN; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Denchev V., 2012, P 29 INT C MACH LEAR, P863; Ding N., 2010, ADV NEURAL INF PROCE, V23, P514; Ferguson T. S., 1967, MATH STAT DECISION T; Ghosh A, 2015, NEUROCOMPUTING, V160, P93, DOI 10.1016/j.neucom.2014.09.081; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Kearns Michael, 1998, J ACM, V5, P392; Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z; Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Reid MD, 2011, J MACH LEARN RES, V12, P731; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Saberian MJ, 2011, PROC CVPR IEEE; Scholkopf Bernhard, 2002, LEARNING KERNELS, V129; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Servedio Rocco A., 1999, C COMP LEARN THEOR C; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Smola A., 2007, ALGORITHMIC LEARNING; Stempfel G, 2009, LECT NOTES COMPUT SC, V5768, P884, DOI 10.1007/978-3-642-04274-4_91; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Wilde O., 1895, IMPORTANCE BEING EAR	27	76	77	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102089
C	Collins, M; Dasgupta, S; Schapire, RE		Dietterich, TG; Becker, S; Ghahramani, Z		Collins, M; Dasgupta, S; Schapire, RE			A generalization of principal component analysis to the exponential family	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.	AT&T Labs Res, Florham Pk, NJ 07932 USA	AT&T	Collins, M (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.	mcollins@research.att.com; dasgupta@research.att.com; schapire@research.att.com	Estrela, Vania Vieira/I-7599-2012	Estrela, Vania Vieira/0000-0002-4465-7691				Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; Csiszar I., 1984, STAT DECISIONS, V1, P205; FORSTER J, IN PRESS J COMPUTER; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; LEE DD, 2001, ADV NEURAL INFORMATI, V13; MCCULLAGH P, 1990, GENERALIZED LINEAR M; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tipping ME, 1999, ADV NEUR IN, V11, P592	10	76	76	0	7	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						617	624						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100077
C	Touvron, H; Vedaldi, A; Douze, M; Jegou, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Touvron, Hugo; Vedaldi, Andrea; Douze, Matthijs; Jegou, Herve			Fixing the train-test resolution discrepancy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained at 224x224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224x224 images and further optimized with our technique for test resolution 320x320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date.	[Touvron, Hugo; Vedaldi, Andrea; Douze, Matthijs; Jegou, Herve] Facebook AI Res, New York, NY 10003 USA	Facebook Inc	Touvron, H (corresponding author), Facebook AI Res, New York, NY 10003 USA.		Touvron, Hugo/AAW-1800-2021					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Berg T., 2014, C COMP VIS PATT REC; Berman M., 2019, ARXIV190205509; Boureau Y.-L., 2010, ICML, P111, DOI DOI 10.5555/3104322.3104338; Cubuk Ekin D., 2018, ARXIV180509501; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kornblith Simon, 2018, ARXIV180508974; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu Chenxi, 2018, INT C COMP VIS; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rippel O, 2017, PR MACH LEARN RES, V70; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Stock Pierre, 2018, EUR C COMP VIS; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan MX, 2019, PR MACH LEARN RES, V97; Tolias Giorgos, 2016, ARXIV151105879, P1; Wah C., 2011, TECH REP; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Yalniz I Zeki, 2019, ARXIV190500546, P2; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31	38	75	76	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308029
C	Kusupati, A; Singh, M; Bhatia, K; Kumar, A; Jain, P; Varma, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kusupati, Aditya; Singh, Manish; Bhatia, Kush; Kumar, Ashish; Jain, Prateek; Varma, Manik			FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the "Hey Cortana" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN's code is available at [30].	[Kusupati, Aditya; Jain, Prateek; Varma, Manik] Microsoft Res India, Bengaluru, India; [Singh, Manish] Indian Inst Technol Delhi, New Delhi, India; [Bhatia, Kush; Kumar, Ashish] Univ Calif Berkeley, Berkeley, CA 94720 USA	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi; University of California System; University of California Berkeley	Kusupati, A (corresponding author), Microsoft Res India, Bengaluru, India.	t-vekusu@microsoft.com; singhmanishiitd@gmail.com; kush@cs.berkeley.edu; ashish_kumar@berkeley.edu; prajain@microsoft.com; manik@microsoft.com	Kusupati, Aditya/AAB-3021-2019	Kusupati, Aditya/0000-0001-8455-1851	NSF [IIS-1619362]; AFOSR [FA9550-17-1-0308]	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	We are grateful to Ankit Anand, Niladri Chatterji, Kunal Dahiya, Don Dennis, Inderjit S. Dhillon, Dinesh Khandelwal, Shishir Patil, Adithya Pratapa, Harsha Vardhan Simhadri and Raghav Somani for helpful discussions and feedback. KB acknowledges the support of the NSF through grant IIS-1619362 and of the AFOSR through grant FA9550-17-1-0308.	Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070; Altun K, 2010, PATTERN RECOGN, V43, P3605, DOI 10.1016/j.patcog.2010.04.019; Anguita D., 2012, P 4 INT C AMBIENT AS, DOI DOI 10.1007/978-3-642-35395-6_30; [Anonymous], 2017, ARXIV171206541; Arjovsky M, 2016, PR MACH LEARN RES, V48; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349; Bhatia K., EXTREME CLASSIFICATI; Bradbury J., 2016, ARXIV161101576; Campos V., 2018, ICLR, P1; Chen G., 2014, 2014 IEEE INT C AC S, P4087, DOI [10.1109/ICASSP.2014.6854370, DOI 10.1109/ICASSP.2014.6854370]; Chen GG, 2015, INT CONF ACOUST SPEE, P5236, DOI 10.1109/ICASSP.2015.7178970; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Collins J., 2016, ARXIV PREPRINT ARXIV; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gupta C, 2017, PR MACH LEARN RES, V70; Han S., 2016, P 4 INT C LEARN REPR, P1; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Inan H., 2016, ARXIV161101462; Jaegera H, 2007, NEURAL NETWORKS, V20, P335, DOI 10.1016/j.neunet.2007.04.016; Jing L., 2017, INT C MACHINE LEARNI, P1733; Jing L, 2017, ARXIV170602761; Jose C, 2018, PR MACH LEARN RES, V80; Kanai S, 2017, ADV NEUR IN, V30; Kepuska Z, 2009, NONLINEAR ANAL-THEOR, V71, pE2772, DOI 10.1016/j.na.2009.06.089; Kingma D.P, P 3 INT C LEARNING R; Kumar A, 2017, PR MACH LEARN RES, V70; Kusupati  A., 2017, EDGEML LIB ML LIB MA; Le Q.V., 2015, ABS150400941 CORR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; McAuley Julian, 2013, P 7 ACM C REC SYST A, DOI DOI 10.1145/2507157.2507163; Melis G., 2017, ARXIV; Merity Stephen, 2017, ICLR; Mhammedi Z, 2017, PR MACH LEARN RES, V70; Mikolov  T., 2012, SLT, V12, P8; Narang Sharan, 2017, ARXIV170405119; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1; Siri Team Apple, 2017, HEY SIR ON DEV DNN; STCI Microsoft, WAK DAT; Susto GA, 2015, IEEE T IND INFORM, V11, P812, DOI 10.1109/TII.2014.2349359; Vorontsov E, 2017, PR MACH LEARN RES, V70; Wang ZS, 2017, IEEE T VLSI SYST, V25, P2763, DOI 10.1109/TVLSI.2017.2717950; Warden P, 2018, ARXIV 180403209; Wisdom S, 2016, ADV NEUR IN, V29; Ye J., 2017, ARXIV171205134; Yelp Inc, 2017, YELP DAT CHALL; Zaremba Wojciech, 2014, ABS14092329 CORR; Zhang J, 2018, PR MACH LEARN RES, V80	54	75	76	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003056
C	Kucukelbir, A; Ranganath, R; Gelman, A; Blei, DM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kucukelbir, Alp; Ranganath, Rajesh; Gelman, Andrew; Blei, David M.			Automatic Variational Inference in Stan	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult for non-experts to use. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI); we implement it in Stan (code available), a probabilistic programming system. In ADVI the user provides a Bayesian model and a dataset, nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.	[Kucukelbir, Alp; Gelman, Andrew; Blei, David M.] Columbia Univ, New York, NY 10027 USA; [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA	Columbia University; Princeton University	Kucukelbir, A (corresponding author), Columbia Univ, New York, NY 10027 USA.	alp@cs.columbia.edu; rajeshr@cs.princeton.edu; gelman@stat.columbia.edu; david.blei@columbia.edu			NSF [IIS-0745520, IIS-1247664, IIS-1009542, SES-1424962]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; NDSEG; Siebel Scholar Foundation; John Templeton Foundation; Sloan [G-2015-13987]; IES [DE R305D140059]; Facebook; Adobe; Amazon	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NDSEG; Siebel Scholar Foundation; John Templeton Foundation; Sloan(Alfred P. Sloan Foundation); IES(US Department of EducationInstitute of Education Sciences (IES)); Facebook(Facebook Inc); Adobe; Amazon	We thank Dustin Tran, Bruno Jacobs, and the reviewers for their comments. This work is supported by NSF IIS-0745520, IIS-1247664, IIS-1009542, SES-1424962, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, N66001-15-C-4032, Sloan G-2015-13987, IES DE R305D140059, NDSEG, Facebook, Adobe, Amazon, and the Siebel Scholar and John Templeton Foundations.	Bishop CM, 2006, PATTERN RECOGNITION; Canny J., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P122, DOI 10.1145/1008992.1009016; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gelman A., 2007, DATA ANAL USING REGR, DOI 10.1017/CBO9780511790942; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Goodman N. D., 2008, P 24 C UNCERTAINTY A, P220; Hardle W. K., 2012, APPL MULTIVARIATE ST; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D. P., 2013, AUTO ENCODING VARIAT; Mansinghka V., 2014, ARXIV14040099; Olive DJ., 2014, STAT THEORY INFERENC, DOI 10.1007/978-3-319-04972-4; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Robert C. P., 1999, MONTE CARLO STAT MET; Salimans T., 2014, ARXIV14011022; Stan Development Team, 2017, STAN MOD LANG US GUI; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Villegas Mauricio, 2013, CLEF EV LABS WORKSH; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wingate D., 2013, ARXIV13011299; Winn J, 2005, J MACH LEARN RES, V6, P661; Wood F, 2014, JMLR WORKSH CONF PRO, V33, P1024	25	75	75	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101018
C	Bakir, GH; Weston, J; Scholkopf, B		Thrun, S; Saul, K; Scholkopf, B		Bakir, GH; Weston, J; Scholkopf, B			Learning to find pre-images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difficult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Bakir, GH (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				ALTUN Y, 2003, 20 INT C MACH LEARN; [Anonymous], 2002, LEARNING KERNELS; BURGES CJC, 1996, P 13 INT C MACH LEAR, P77; HAUSSLER D, 1999, UCSCCRL9910 COMP SCI; Hua SJ, 2001, J MOL BIOL, V308, P397, DOI 10.1006/jmbi.2001.4580; KWOK JT, 2002, NIPS 2002 WORKSH KER; LESLIE C, 2002, P PAC S BIOC; Lodhi H, 2002, J MACH LEARN RES, V2, P419, DOI 10.1162/153244302760200687; Mika S, 1999, ADV NEUR IN, V11, P536; Romdhani S, 1999, P 10 BRIT MACH VIS C, P483, DOI [10.5244/C.13.48, DOI 10.5244/C.13.48]; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; WESTON J, 2002, ADV NEURAL INFORMATI, V15	12	75	76	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						449	456						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500057
C	Xu, L; Skoularidou, M; Cuesta-Infante, A; Veeramachaneni, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Lei; Skoularidou, Maria; Cuesta-Infante, Alfredo; Veeramachaneni, Kalyan			Modeling Tabular Data using Conditional GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design CTGAN, which uses a conditional generator to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. CTGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.	[Xu, Lei; Veeramachaneni, Kalyan] MIT, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Skoularidou, Maria] Univ Cambridge, MRC BSU, Cambridge, England; [Cuesta-Infante, Alfredo] Univ Rey Juan Carlos, Mostoles, Spain	Massachusetts Institute of Technology (MIT); MRC Biostatistics Unit; University of Cambridge; Universidad Rey Juan Carlos	Xu, L (corresponding author), MIT, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	leix@mit.edu; ms2407@cam.ac.uk; alfredo.cuesta@urjc.es; kalyanv@mit.edu	CUESTA-INFANTE, ALFREDO/L-3708-2014	CUESTA-INFANTE, ALFREDO/0000-0002-3328-501X	National Science Foundation [ACI-1443068]; Accenture; Spanish Government [RTI2018-098743-B-I00, Y2018/EMT-5062]	National Science Foundation(National Science Foundation (NSF)); Accenture; Spanish Government(Spanish GovernmentEuropean Commission)	This paper is partially supported by the National Science Foundation Grants ACI-1443068. We (authors from MIT) also acknowledge generous support provided by Accenture for the synthetic data generation project. Dr. Cuesta-Infante is funded by the Spanish Government research fundings RTI2018-098743-B-I00 (MICINN/FEDER) and Y2018/EMT-5062 (Comunidad de Madrid).	Arjovsky M, 2017, PR MACH LEARN RES, V70; Avino Laura, 2018, KDD WORKSH MACH LEAR; Bishop CM, 2006, PATTERN RECOGNITION; Camino Ramiro, 2018, ICML WORKSH THEOR FD; Che Zhengping, 2017, INT C DAT MIN; Choi Edward, 2017, MACHI LEARN HEALTHC; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cormode Graham, 2012, INT C DAT ENG; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Jang E., 2016, ARXIV; Jordon James, 2019, INT C LEARN REPR ICL, P2; Kingma D. P., 2013, AUTO ENCODING VARIAT; LeCun Y., 2010, MNIST HANDWRITTEN DI; Lin ZA, 2018, ADV NEUR IN, V31; Park Noseong, 2018, INT C VER LARG DAT B; Patki Neha, 2016, INT C DAT SCI ADV AN; Reiter Jerome P., 2005, J OFF STAT, V21, P441; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P4; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sun Yi, 2018, AAAI C ART INT; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Theis Lucas, 2016, ICLR; Yahi Alexandre, 2017, NIPS WORKSH MACH LEA; Yu L, 2017, PROCEEDINGS OF THE ASME 36TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2017, VOL 3A; Zhang Jun, 2016, INT C MAN DAT; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	29	74	76	9	21	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307036
C	Yang, B; Wang, JA; Clark, R; Hu, QY; Wang, S; Markham, A; Trigoni, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Bo; Wang, Jianan; Clark, Ronald; Hu, Qingyong; Wang, Sen; Markham, Andrew; Trigoni, Niki			Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel, conceptually simple and general framework for instance segmentation on 3D point clouds. Our method, called 3D-BoNet, follows the simple design philosophy of per-point multilayer perceptrons (MLPs). The framework directly regresses 3D bounding boxes for all instances in a point cloud, while simultaneously predicting a point-level mask for each instance. It consists of a backbone network followed by two parallel network branches for 1) bounding box regression and 2) point mask prediction. 3D-BoNet is single-stage, anchor-free and end-to-end trainable. Moreover, it is remarkably computationally efficient as, unlike existing approaches, it does not require any post-processing steps such as non-maximum suppression, feature sampling, clustering or voting. Extensive experiments show that our approach surpasses existing work on both ScanNet and S3DIS datasets while being approximately 10x more computationally efficient. Comprehensive ablation studies demonstrate the effectiveness of our design.	[Yang, Bo; Hu, Qingyong; Markham, Andrew; Trigoni, Niki] Univ Oxford, Oxford, England; [Wang, Jianan] DeepMind, London, England; [Clark, Ronald] Imperial Coll London, London, England; [Wang, Sen] Heriot Watt Univ, Edinburgh, Midlothian, Scotland	University of Oxford; Imperial College London; Heriot Watt University	Yang, B (corresponding author), Univ Oxford, Oxford, England.	bo.yang@cs.ox.ac.uk; jianan.wang@cs.ox.ac.uk; ronald.clark@cs.ox.ac.uk; qingyong.hu@cs.ox.ac.uk; sen.wang@cs.ox.ac.uk; andrew.markham@cs.ox.ac.uk; niki.trigoni@cs.ox.ac.uk		YANG, Bo/0000-0002-2419-4140				Armeni Iro, 2016, CVPR; Bengio Yoshua, 2013, ARXIV13083432; Chen X., 2017, CVPR; Choy C., 2019, CVPR; Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dai A., 2017, CVPR; Elich C., 2019, GCPR; Engelcke M., 2017, ICRA; Engelmann F., 2017, ICCV WORKSH, p[2, 6, 8]; Graham B., 2018, CVPR; Grover A., 2019, ICLR; He K., 2017, ICCV; Hermosilla P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275110; Hou Ji, 2019, CVPR; Hu Qingyong, 2019, ARXIV191111236; Hua B.-S, 2018, CVPR; Huang Q., 2018, CVPR; Kingma D.P., 2015, INT C LEARN REPR, P1; Klokov R., 2017, ICCV; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Kuhn HW, 1956, NAV RES LOGISTICS Q, V3, P253, DOI [DOI 10.1002/NAV.3800030404, 10.1002/nav.3800030404]; Landrieu Loic, 2018, CVPR; Le T., 2018, CVPR; Li B, 2016, ONLINE PORTFOLIO SELECTION: PRINCIPLES AND ALGORITHMS, P3; Li B, 2017, LECT NOTES COMPUT SC, V10362, P3, DOI 10.1007/978-3-319-63312-1_1; Li Jiaxin, 2018, CVPR; Li Y., 2018, NEURLPS; Liang ZX, 2019, WOODHEAD PUBL MATER, P3, DOI 10.1016/B978-0-08-102260-3.00001-9; Lin T., 2017, ICCV; LIU CX, 2019, P 27 ACM INT C MULT, P3, DOI DOI 10.1145/3343031.3350869; Liu S., 2018, CVPR; Mo Kaichun, 2019, CVPR; Narita Gaku, 2019, IROS; Neumann U., 2018, CVPR; Pham Quang-Hieu, 2019, CVPR; Qi C. R., 2018, CVPR; Qi C. R., 2017, CVPR; Qi C. R., 2019, ICCV; Qi Charles Ruizhongtai, 2017, NIPS; Ren S., 2015, CORR ABS150601497; Rethage D., 2018, ECCV; Riegler G., 2017, CVPR; Rusu R. B., 2009, ICRA; Shen Y., 2018, CVPR; Shi Shaoshuai, 2019, CVPR; Su Hang, 2018, CVPR; Tchapmi L. P., 2017, 3DV; Thomas Hugues, 2019, ICCV; Vaquero V., 2017, ECMR; Wang Cheng, 2018, ECCV; Wang Xun, 2019, CVPR; Xu DX, 2018, SACMAT'18: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON ACCESS CONTROL MODELS & TECHNOLOGIES, P3, DOI 10.1145/3205977.3205979; Xu Yifan, 2018, ECCV; Yang Guorun, 2018, ECCV; Yao W, 2018, DESTECH TRANS ENVIR; Yaramasu V, 2017, MODEL PREDICTIVE CONTROL OF WIND ENERGY CONVERSION SYSTEMS, P3; Ye X., 2018, ECCV; Yi Li, 2019, CVPR; Zeng YM, 2018, IEEE ROBOT AUTOM LET, V3, P3434, DOI 10.1109/LRA.2018.2852843; Zhou Y., 2018, CVPR	61	74	75	2	15	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306071
C	Schwartz, E; Karlinsky, L; Shtok, J; Harary, S; Marder, M; Kumar, A; Feris, R; Giryes, R; Bronstein, AM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Schwartz, Eli; Karlinsky, Leonid; Shtok, Joseph; Harary, Sivan; Marder, Mattias; Kumar, Abhishek; Feris, Rogerio; Giryes, Raja; Bronstein, Alex M.			Delta-encoder: an effective sample synthesis method for few-shot object recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning to classify new categories based on just one or a few examples is a long-standing challenge in modern computer vision. In this work, we propose a simple yet effective method for few-shot (and one-shot) object recognition. Our approach is based on a modified auto-encoder, denoted Delta-encoder, that learns to synthesize new samples for an unseen category just by seeing few examples from it. The synthesized samples are then used to train a classifier. The proposed approach learns to both extract transferable intra-class deformations, or "deltas", between same-class pairs of training examples, and to apply those deltas to the few provided examples of a novel class (unseen during training) in order to efficiently synthesize samples from that new class. The proposed method improves the state-of-the-art of one-shot object-recognition and performs comparably in the few-shot case.	[Schwartz, Eli; Karlinsky, Leonid; Shtok, Joseph; Harary, Sivan; Marder, Mattias; Kumar, Abhishek; Feris, Rogerio] IBM Res AI, San Jose, CA 95120 USA; [Schwartz, Eli; Giryes, Raja] Tel Aviv Univ, Sch Elect Engn, Tel Aviv, Israel; [Bronstein, Alex M.] Technion, Dept Comp Sci, Haifa, Israel	Tel Aviv University; Technion Israel Institute of Technology	Karlinsky, L (corresponding author), IBM Res AI, San Jose, CA 95120 USA.	leonidka@il.ibm.com		Harary, Sivan/0000-0003-1183-1144	ERC-StG SPADE grant; IARPA via DOI/IBC [D17PC00341]; ERC StG RAPID	ERC-StG SPADE grant; IARPA via DOI/IBC; ERC StG RAPID	Part of this research was partially supported by the ERC-StG SPADE grant. Rogerio Feris is partly supported by IARPA via DOI/IBC contract number D17PC00341. Alex Bronstein is supported by ERC StG RAPID. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government)	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antoniou Antreas, 2017, ARXIV171104340; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Bucher  M., 2017, ARXIV170806975, V8; Chen  Z., 2018, ARXIV180405298V2; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong X, 2017, P ADV NEUR INF PROC, P1; Dosovitskiy A, 2017, IEEE T PATTERN ANAL, V39, P692, DOI 10.1109/TPAMI.2016.2567384; Durugkar Ishan, 2017, ICLR; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Finn C, 2017, PR MACH LEARN RES, V70; Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Garcia VM, 2017, AIRFIELD AND HIGHWAY PAVEMENTS 2017: TESTING AND CHARACTERIZATION OF BOUND AND UNBOUND PAVEMENT MATERIALS, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Griffin G., 2007, CALTECH UNPUB, V11, P20; Guu Kelvin, 2017, ARXIV170908878; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hilliard N, 2018, FEW SHOT LEARNING ME; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li Z., 2016, EUR C COMP VIS ECCV, P1; Li Zhenguo, 2017, METASGD LEARNING LEA; Lim J. J., 2012, ADV NEURAL INF PROCE, V26, P1; Lin Tsung-Yi, 2017, ARXIV170802002, P2980, DOI [DOI 10.1109/ICCV.2017.324, DOI 10.1109/TPAMI.2018.2858826]; Mao  X., 2016, IMAGE RESTORATION US, P1; Mehrotra Akshay, 2017, ARXIV170308033; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Park Dennis, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P58, DOI 10.1109/CVPRW.2015.7301337; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ravi S., 2017, INT C LEARN REPR, P12; Reed  S., 2016, ARXIV171010304, P1; Rippel O, 2015, ARXIV PREPRINT ARXIV, P1; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sung Flood, 2017, ARXIV171106025; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Wang YX, 2016, ADV NEUR IN, V29; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Welinder P., 2010, CNSTR2010001 CALTECH; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yu A, 2017, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2017.594; Zhou F, 2018, ADV NEURAL INFORM PR; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	54	74	78	7	20	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302083
C	Zaheer, M; Kottur, S; Ravanbhakhsh, S; Poczos, B; Salakhutdinov, R; Smola, AJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zaheer, Manzil; Kottur, Satwik; Ravanbhakhsh, Siamak; Poczos, Barnabas; Salakhutdinov, Ruslan; Smola, Alexander J.			Deep Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics [1], to anomaly detection in piezometer data of embankment dams [2], to cosmology [3, 4]. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.	[Zaheer, Manzil; Kottur, Satwik; Ravanbhakhsh, Siamak; Poczos, Barnabas; Salakhutdinov, Ruslan; Smola, Alexander J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zaheer, Manzil; Smola, Alexander J.] Amazon Web Serv, Seattle, WA 98108 USA	Carnegie Mellon University; Amazon.com	Zaheer, M (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.; Zaheer, M (corresponding author), Amazon Web Serv, Seattle, WA 98108 USA.	manzilz@cs.cmu.edu; skottur@cs.cmu.edu; mravanba@cs.cmu.edu; bapoczos@cs.cmu.edu; rsalakhu@cs.cmu.edu; smola@cs.cmu.edu	Zaheer, Manzil/ABG-6249-2021; Jeong, Yongwook/N-7413-2016					Anandkumar A., 2012, ARXIV12107559, P3; Binney J., 1998, PR S ASTROP; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bourbaki Nicolas, 1990, ELEMENTS MATH THEORI, V1, P15; Brock A., 2016, ARXIV160804236; Chang Michael B, 2016, ARXIV161200341, P3; Chen Minmin, 2013, P 30 INT C MACH LEAR, P1274; Chen X., 2014, ADV NEURAL INFORM PR, P1709; Clevert Djork-Arne, 2015, ARXIV151107289, P27; Connolly AJ, 1995, ASTROPH9508100 ARXIV, p[5, 25]; Curgus B, 2006, EXPO MATH, V24, P81, DOI 10.1016/j.exmath.2005.07.001; Faber FA, 2016, PHYS REV LETT, V117, DOI 10.1103/PhysRevLett.117.135502; Feng SL, 2004, PROC CVPR IEEE, P1002; Gens R., 2014, NIPS; Ghahramani Zoubin, 2005, NIPS, V2, p[22, 6, 7, 20, 21, 22]; Grubinger Michael, 2007, THESIS, P23; Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266; Guttenberg Nicholas, 2016, ARXIV161204530, P3; Hanrahan Pat, 2015, ARXIV151203012CSGR, P5; Hartford J.S., 2016, ADV NEURAL INFORM PR, P2424; Jung I., 2015, ADV ENG INFORM, P1; Khesin Boris A, 2014, ARNOLD SWIMMING TIDE, V86, P15; King DR, 2014, ADV MERGERS ACQUIS, V13, P25, DOI 10.1108/S1479-361X20140000013000; Lin HW, 2004, COMPUT AIDED DESIGN, V36, P1, DOI 10.1016/S0010-4485(03)00064-2; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Ziwei, 2015, P INT C COMP VIS ICC, P8; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Lopez-Paz David, 2016, ARXIV160508179, P3; Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24; Marsden Jerrold E, 1993, ELEMENTARY CLASSICAL, P15; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; MICCHELLI CA, 1986, CONSTR APPROX, V2, P11, DOI 10.1007/BF01893414; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Muandet K., 2013, P 30 INT C MACH LEAR; Muandet K., 2012, P 26 ANN C NEUR INF, p[1, 3]; Ntampaka M., 2016, ASTROPHYS J, P1; Oliva J, 2013, INFECTIOUS FOREST DISEASES, P1, DOI 10.1079/9781780640402.0001; Poczos B., 2013, JMLR WORKSHOP C P, P1; Poczos B., 2012, SUPPORT DISTRIBUTION, p[3, 4]; Pritchard JK, 2000, GENETICS, V155, P945; Ravanbakhsh M., 2016, INT C MACH LEARN ICM, P1; Ravanbakhsh Siamak, 2017, ARXIV170208389, P3; Ravanbakhsh Siamak, 2016, P 33 INT C MACH LEAR, P5; Rozo E, 2014, ASTROPHYS J, V783, DOI 10.1088/0004-637X/783/2/.0; Rusu R. B., 2011, IEEE INT C ROB AUT I; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Szabo Z., 2016, J MACHINE LEARNING R; Taskar B, 2004, ADV NEUR IN, V16, P25; Vinyals Oriol, 2015, ARXIV151106391, P3; Von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733; Welling M., 2016, ARXIV160207576; Wu Jiajun, 2016, ARXIV161007584, p[5, 26]; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801	55	74	74	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403045
C	Li, HF; Jiang, T; Zhang, KH		Thrun, S; Saul, K; Scholkopf, B		Li, HF; Jiang, T; Zhang, KH			Efficient and robust feature extraction by maximum margin criterion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix S-w. Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efficient and stable.	Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA	University of California System; University of California Riverside	Li, HF (corresponding author), Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA.		jiang, tao/GWC-7108-2022; Jiang, Tao/AGX-8391-2022	Jiang, Tao/0000-0003-3833-4498				Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; HONG ZQ, 1991, PATTERN RECOGN, V24, P317, DOI 10.1016/0031-3203(91)90074-F; LIU K, 1992, PATTERN RECOGN, V25, P731, DOI 10.1016/0031-3203(92)90136-7; Loog M, 2001, IEEE T PATTERN ANAL, V23, P762, DOI 10.1109/34.935849; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Samaria F., 1994, P 2 IEEE WORKSH APPL; Stewart G., 1973, INTRO MATRIX COMPUTA; TIAN Q, 1986, OPT ENG, V25, P834, DOI 10.1117/12.7973916; Vapnik V.N, 1998, STAT LEARNING THEORY	11	74	75	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						97	104						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500013
C	Crisp, DJ; Burges, CJC		Solla, SA; Leen, TK; Muller, KR		Crisp, DJ; Burges, CJC			A geometric interpretation of nu-SVM classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SUPPORT	We show that the recently proposed variant of the Support Vector machine (SVM) algorithm, known as nu -SVM, can be interpreted as a maximal separation between subsets of the convex hulls of the data, which we call soft convex hulls. The soft convex hulls are controlled by choice of the parameter nu. If the intersection of the convex hulls is empty, the hyperplane is positioned halfway between them such that the distance between convex hulls, measured along the normal, is maximized; and if it is not, the hyperplane's normal is similarly determined by the soft convex hulls, but its position (perpendicular distance from the origin) is adjusted to minimize the error sum. The proposed geometric interpretation of nu -SVM also leads to necessary and sufficient conditions for the existence of a choice of nu for which the nu -SVM solution is nontrivial.	Univ Adelaide, Dept Elect Engn, Ctr Sensor Signal & Informat Proc, Adelaide, SA, Australia	University of Adelaide	Crisp, DJ (corresponding author), Univ Adelaide, Dept Elect Engn, Ctr Sensor Signal & Informat Proc, Adelaide, SA, Australia.							Burges CJC, 1997, ADV NEUR IN, V9, P375; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; SCHOLKOPF B, 1998, NC2TR1998031 NEUROCO	3	74	77	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						244	250						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700035
C	Roy, N; Thrun, S		Solla, SA; Leen, TK; Muller, KR		Roy, N; Thrun, S			Coastal navigation with mobile robots	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The problem that we address in this paper is how a mobile robot can plan in order to arrive at its goal with minimum uncertainty. Traditional motion planning algorithms often assume that a mobile robot can track its position reliably, however, in real world situations, reliable localization may not always be feasible. Partially Observable Markov Decision Processes (POMDPs) provide one way to maximize the certainty of reaching the goal state, but at the cost of computational intractability for large state spaces. The method we propose explicitly models the uncertainty of the robot's position as a state variable, and generates trajectories through the augmented pose-uncertainty space. By minimizing the positional uncertainty at the goal, the robot reduces the likelihood it becomes lost. We demonstrate experimentally that coastal navigation reduces the uncertainty at the goal, especially with degraded localization.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Roy, N (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.							Bellman RE, 1957, DYNAMIC PROGRAMMING; BURGARD W, 1996, AAAI; FOX D, 1998, ROBOTICS AUTONOMOUS, P25; Howard RonaldA., 1960, DYNAMIC PROGRAMMING; KAELBLING L, 1996, IROS; Koenig S, 1996, MACH LEARN, V22, P227, DOI 10.1007/BF00114729; Latombe J.-C, 2012, ROBOT MOTION PLANNIN, V124; MAHADEVAN S, 1999, ROBUST MODILE ROBOT; Moravec H., 1985, ICRA; SIM R, 1998, IROS; TAKEDA H, 1994, IEEE T PATTERN ANAL, P16; THRUN S, 1998, MACH LEARN, P431	12	73	73	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1043	1049						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700147
C	Hooker, S; Erhan, D; Kindermans, PJ; Kim, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hooker, Sara; Erhan, Dumitru; Kindermans, Pieter-Jan; Kim, Been			A Benchmark for Interpretability Methods in Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches-VarGrad and SmoothGrad-Squared-outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.	[Hooker, Sara; Erhan, Dumitru; Kindermans, Pieter-Jan; Kim, Been] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Hooker, S (corresponding author), Google Brain, Mountain View, CA 94043 USA.	shooker@google.com; dumitru@google.com; pikinder@google.com; beenkim@google.com						Adebayo Julius, 2019, NEURIPS; Ancona M, 2017, BETTER UNDERSTANDING; [Anonymous], 2015, ICLR; [Anonymous], 2016, TENSORFLOW LARGE SCA; [Anonymous], 2017, P 34 INT C MACH LEAR; Bach Sebastian, 2015, PLOS ONE, V10; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259; Bossard Lukas, 2014, EUR C COMP VIS; Dabkowski P., 2017, REAL TIME IMAGE SALI; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Frosst N., 2017, CEX AI IA, V2071; Goyal P., 2017, ACCURATE LARGE MINIB; Gross S., 2017, TRAINING INVESTIGATI; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; He K, 2016, 2016 IEEE C COMP VIS, DOI [10.1109/cvpr.2016.90, 10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Jost Tobias SpringenBerg, 2015, ICLR; Kim B., 2018, P 35 INT C MACH LEAR, P2668; Kindermans P, 2017, UNRELIABILITY SALIEN; Kindermans Pieter-Jan, 2018, INT C LEARN REPR; Kornblith Simon, 2018, ARXIV180508974; Lage Isaac Slavin, 2018, ARXIV180511571; Li K., 2009, CVPR09; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; Morcos AS, 2018, IMPORTANCE SINGLE DI; Olah Chris, 2017, DISTILL, P4, DOI DOI 10.23915/DISTILL.00007; Petsiuk Vitali, 2018, ABS180607421 CORR; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Ross Andrew Slavin, 2017, ABS171109404 CORR; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Selvaraju Ramprasaath R., 2017, IEEE INT C COMP VIS; Shrikumar Avanti, 2016, ARXIV160501713; Smilkov Daniel, 2017, ARXIV170603825; Sobel Irwin, 2014, STANF A I PROJ 1968; Sundararajan Mukund, 2017, ARXIV170301365; Wu M., 2017, SPARSITY TREE REGULA; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang C, 2016, UNDERSTANDING DEEP L; Zhou B., 2014, OBJECT DETECTORS EME; Zhou Bolei, 2018, ABS180602891 CORR; Zintgraf Luisa M, 2017, ICLR	43	72	76	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901037
C	Insafutdinov, E; Dosovitskiy, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Insafutdinov, Eldar; Dosovitskiy, Alexey			Unsupervised Learning of Shape and Pose with Differentiable Point Clouds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We address the problem of learning accurate 3D shape and camera pose from a collection of unlabeled category-specific images. We train a convolutional network to predict both the shape and the pose from a single image by minimizing the reprojection error: given several views of an object, the projections of the predicted shapes to the predicted camera poses should match the provided views. To deal with pose ambiguity, we introduce an ensemble of pose predictors which we then distill to a single "student" model. To allow for efficient learning of high-fidelity shapes, we represent the shapes by point clouds and devise a formulation allowing for differentiable projection of these. Our experiments show that the distilled ensemble of pose predictors learns to estimate the pose accurately, while the point cloud representation allows to predict detailed shape models.	[Insafutdinov, Eldar] Max Planck Inst Informat, Saarbrucken, Germany; [Dosovitskiy, Alexey] Intel Labs, Munich, Germany	Max Planck Society; Intel Corporation	Insafutdinov, E (corresponding author), Max Planck Inst Informat, Saarbrucken, Germany.	eldar@mpi-inf.mpg.de; adosovitskiy@gmail.com						Abadi M., 2016, P USENIX OSDI; [Anonymous], 2017, ICCV; Cashman T. J., 2011, PAMI, V35; Chang A., 2015, TECHNICAL REPORT; Choy C.B., 2016, ECCV; Fan H., 2017, CVPR; Gupta S., 2018, CVPR; Guzman-Rivera Abner, 2012, NIPS; Heess N, 2016, NIPS; Kato H., 2018, IEEE C COMP 6 VIS PA; Kingma D.P., 2015, INT C LEARN REPR, P1; Li Jun, 2017, SIGGRAPH; Lin C.-H., 2018, AAAI; Loper M. M., 2014, ECCV; Rhodin Helge, 2015, ICCV; Soltani A. A., 2017, COMPUTER VISION PATT; Sun Xingyuan, 2018, P IEEE C COMP VIS PA; Tatarchenko M., 2017, ICCV; Tulsiani S., 2017, IEEE C COMP VIS PATT; Tulsiani S., 2017, PAMI, V39; Vicente S., 2014, CVPR; WU J., 2016, NIPS; Wu J., 2018, IJCV; Yan X., 2016, NIPS; Yang Yaoqing, 2018, CVPR	25	72	73	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302079
C	Li, YZ; Liang, YY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Yuanzhi; Liang, Yingyu			Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.	[Li, Yuanzhi] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Liang, Yingyu] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA	Stanford University; University of Wisconsin System; University of Wisconsin Madison	Li, YZ (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	yuanzhil@stanford.edu; yliang@cs.wisc.edu	Li, Yuan/GXV-1310-2022		NSF [CCF-1527371, DMS-1317308]; Simons Investigator Award; Simons Collaboration Grant; Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin Madison; Wisconsin Alumni Research Foundation;  [FA9550-18-1-0166];  [ONR-N00014-16-1-2329]	NSF(National Science Foundation (NSF)); Simons Investigator Award; Simons Collaboration Grant; Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin Madison; Wisconsin Alumni Research Foundation; ; 	We would like to thank the anonymous reviewers of NIPS' 18 and Jason Lee for helpful comments. This work was supported in part by FA9550-18-1-0166, NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329. Yingyu Liang would also like to acknowledge that support for this research was provided by the Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin Madison with funding from the Wisconsin Alumni Research Foundation.	Adams RP., 2018, INT C LEARN REPR; Arora Sanjeev, 2018, ARXIV180205296; Arpit D, 2017, ARXIV170605394; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Baykal C., 2018, ARXIV180405345; Boob D., 2017, ARXIV171011241; Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; Dinh L., 2017, ARXIV170304933; Dziugaite Gintare Karolina, 2017, ARXIV170311008; Ge R., 2017, ARXIV PREPRINT ARXIV; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Hardt M., 2016, ARXIV161104231; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Keskar N.S., 2016, ABS160904836; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y., 2017, ADV NEURAL INFORM PR, P597; Li Yuanzhi, 2017, ARXIV171209203; Livni R., 2014, NIPS, V1, P855; Ma C., 2017, ARXIV171110467; Martens J., 2010, P 27 INT C MACH LEAR, P735; Moustapha C, 2017, ARXIV170408847; Nagarajan Vaishnavh, 2017, NIPS WORKSH DEEP LEA; Neyshabur Behnam, 2014, ARXIV14126614; Neyshabur Behnam, 2017, ARXIV170709564; Soltanolkotabi M., 2017, ARXIV170704926; Soudry D., 2016, ARXIV PREPRINT ARXIV; Srebro N., 2017, ARXIV171010345; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tian Yuandong, 2017, ARXIV170300560; Xie B., 2016, ARXIV161103131; Zhang Chiyuan, 2016, ARXIV161103530; Zhong  Kai, 2017, ARXIV170603175	33	72	72	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002068
C	Liu, Q; Allamanis, M; Brockschmidt, M; Gaunt, AL		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Qi; Allamanis, Miltiadis; Brockschmidt, Marc; Gaunt, Alexander L.			Constrained Graph Variational Autoencoders for Molecule Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.	[Liu, Qi] Singapore Univ Technol & Design, Singapore, Singapore; [Allamanis, Miltiadis; Brockschmidt, Marc; Gaunt, Alexander L.] Microsoft Res, Cambridge, England	Singapore University of Technology & Design; Microsoft	Liu, Q (corresponding author), Singapore Univ Technol & Design, Singapore, Singapore.	qiliu@u.nus.ed; miallama@microsoft.com; mabrocks@microsoft.com; algaunt@microsoft.com						Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47; Allamanis M., 2018, INT C LEARNING REPRE; Bredt J, 1902, BER DTSCH CHEM GES, V35, P1286, DOI 10.1002/cber.19020350215; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Gilmer J, 2017, PR MACH LEARN RES, V70; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Hachmann J, 2011, J PHYS CHEM LETT, V2, P2241, DOI 10.1021/jz200866s; Irwin JJ, 2012, J CHEM INF MODEL, V52, P1757, DOI 10.1021/ci3001277; Jin W., 2018, P 36 INT C MACH LEAR; Johnson D. D., 2017, INT C LEARN REPR; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kipf T, 2018, PR MACH LEARN RES, V80; Kusner M. J., 2017, CORR; Landrum G., 2014, RDKIT OPEN SOURCE CH; Leskovec J, 2010, J MACH LEARN RES, V11, P985; Li Yujia, 2016, P INT C LEARN REPR I, P2; Li YL, 2018, PROC CVPR IEEE, P5997, DOI 10.1109/CVPR.2018.00628; Neil D., 2018, ICLR WORKSH; Olivecrona M, 2017, J CHEMINFORMATICS, V9, DOI 10.1186/s13321-017-0235-x; Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; Ruddigkeit L, 2012, J CHEM INF MODEL, V52, P2864, DOI 10.1021/ci300415d; Samanta B., 2018, CORR; Segler M. H., 2017, ACS CENTRAL SCI; Simonovsky M., 2018, ICLR WORKSH TRACK; Snijders TAB, 1997, J CLASSIF, V14, P75, DOI 10.1007/s003579900004; Vinyals Oriol, 2016, 4 INT C LEARN REPR I, DOI DOI 10.48550/ARXIV.1511.01844; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; Yeung S., 2017, ICML WORK PRINC APPR; You JX, 2018, PR MACH LEARN RES, V80	34	72	72	4	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002035
C	Mathieu, M; Zhao, JB; Sprechmann, P; Ramesh, A; Lecun, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mathieu, Michael; Zhao, Junbo; Sprechmann, Pablo; Ramesh, Aditya; Lecun, Yann			Disentangling factors of variation in deep representations using adversarial training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentaglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.	[Mathieu, Michael; Zhao, Junbo; Sprechmann, Pablo; Ramesh, Aditya; Lecun, Yann] 719 Broadway,12th Floor, New York, NY 10003 USA		Mathieu, M (corresponding author), 719 Broadway,12th Floor, New York, NY 10003 USA.	mathieu@cs.nyu.edu; junbo.zhao@cs.nyu.edu; pablo@cs.nyu.edu; ar2922@cs.nyu.edu; yann@cs.nyu.edu						Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Cheung Brian, 2014, CORR; Collobert R., 2011, NIPS; Denton Emily L, 2015, NEURIPS, V2, P4; Dosovitskiy A., 2014, CORR, V1406; Dumoulin Vincent, 2016, ARXIV E PRINTS; Edwards Harrison, 2016, INT C LEARN REPR ICL, P3; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; GEORGHIADES AS, 2001, SYSTEMS MAN CYBERN A, V23, P643; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni Tejas D, 2015, ADV NEURAL INFORM PR, P2530; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; LeCun Y, 2004, PROC CVPR IEEE, P97; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Louizos C., 2016, 4 INT C LEARN REPR; Makhzani A., 2015, ARXIV151105644; Mathieu Michael, 2015, ARXIV151105440; Mirza M., 2014, ARXIV; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranzato M.A., 2007, IEEE C COMP VIS PATT, P1; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Theis Lucas, 2015, ARXIV151101844; Zhao Junbo, 2016, ICLR WORKSH SUBM	29	72	73	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700016
C	Wilson, AG; Hu, ZT; Salakhutdinov, R; Xing, EP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wilson, Andrew Gordon; Hu, Zhiting; Salakhutdinov, Ruslan; Xing, Eric P.			Stochastic Variational Deep Kernel Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.	[Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA; [Hu, Zhiting; Salakhutdinov, Ruslan; Xing, Eric P.] CMU, Pittsburgh, PA USA	Cornell University; Carnegie Mellon University	Wilson, AG (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.				NSF [IIS-1563887]; ONR [N000141410684, N000141310721, N000141512791]; ADeLAIDE [FA8750-16C-0130-001]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ADeLAIDE	We thank NSF IIS-1563887, ONR N000141410684, N000141310721, N000141512791, and ADeLAIDE FA8750-16C-0130-001 grants.	Aggarwal CC, 2001, SURPRISING BEHAV DIS; Bui T. D., 2015, BLACK BOX LEARN INF; Calandra R, 2014, ARXIV14025876; Dai Z, 2015, ARXIV PREPRINT ARXIV; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Dezfouli A., 2015, ADV NEURAL INFORM PR, P1414; Durrande N., 2011, ARXIV11034023; Gal Yarin, 2014, ADV NEURAL INF PROCE, V2, P3257; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le Quoc V., 2013, INT C MACH LEARN, P244; Lloyd J. R., 2014, ASS ADV ARTIFICIAL I; Nickson T., 2015, ARXIV151007965; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rasmussen CE, 2001, ADV NEUR IN, V13, P294; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Wilson A.G., 2014, THESIS U CAMBRIDGE C; Wilson A. G., 2013, INT C MACH LEARN ICM; Wilson A.G., 2015, ARXIV151101870; Wilson A. G., 2012, INT C MACH LEARN ICM; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Yang Z., 2015, ARTIFICIAL INTELLIGE	31	72	72	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703062
C	Richardson, M; Domingos, P		Dietterich, TG; Becker, S; Ghahramani, Z		Richardson, M; Domingos, P			The intelligent surfer: Probabilistic combination of link and content information in PageRank	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today's large search engines.	Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Richardson, M (corresponding author), Univ Washington, Dept Comp Sci & Engn, Box 352350, Seattle, WA 98195 USA.							BHARAT K, 1998, P 21 ANN INT ACM SIG; Brin S., 1998, P PAP PRES 7 INT WOR; CHAKRABARTI S, 1998, P 7 INT WORLD WID WE; COHN D, 2001, ADV NEURAL INFORMATI, V13; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Haveliwala T., 1999, EFFICIENT COMPUTATIO; HIRAI J, 1999, P 9 WORLD WID WEB C; KLEINBERG J, 1998, P 9 ANN ACM SIAM S D; Page L., 1999, TECHNICAL REPORT 199; Rafiei D., 2000, P 9 INT WORLD WID WE; SALTON G, 1983, INTRO MODERN INFORMA; Zipf G. K., 1949, HUMAN BEHAVIOUR PRIN	12	72	81	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1441	1448						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100179
C	Durkan, C; Bekasov, A; Murray, I; Papamakarios, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Durkan, Conor; Bekasov, Artur; Murray, Iain; Papamakarios, George			Neural Spline Flows	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INTERPOLATION	A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.	[Durkan, Conor; Bekasov, Artur; Murray, Iain; Papamakarios, George] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Edinburgh	Durkan, C (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	conor.durkan@ed.ac.uk; artur.bekasov@ed.ac.uk; i.murray@ed.ac.uk; g.papamakarios@ed.ac.uk			EPSRC Centre for Doctoral Training in Data Science - UK Engineering and Physical Sciences Research Council [EP/L016427/1]; University of Edinburgh; Microsoft Research through its PhD Scholarship Programme	EPSRC Centre for Doctoral Training in Data Science - UK Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); University of Edinburgh; Microsoft Research through its PhD Scholarship Programme(Microsoft)	This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh. George Papamakarios was also supported by Microsoft Research through its PhD Scholarship Programme.	Blinn JF, 2007, IEEE COMPUT GRAPH, V27, P78, DOI 10.1109/MCG.2007.60; Burda Yuri, 2016, 4 INT C LEARN REPR I; Chen S. S., 2001, ADV NEURAL INFORM PR; Chen T.Q., 2018, NEURIPS, P2610; Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217; De Cao N., 2019, C UNC ART INT; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Dinh L, 2017, 5 INT C LEARN REPR I; Dinh Laurent, 2015, ICLR WORKSH; Durkan C., 2019, INT C MACH LEARN; Elidan Gal, 2013, COPULAE MATH QUANTIT, P39; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl Will, 2018, ICLR; GREGORY JA, 1982, IMA J NUMER ANAL, V2, P123, DOI 10.1093/imanum/2.2.123; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Ho Jonathan, 2019, ICML; Hoogeboom E, 2019, PR MACH LEARN RES, V97; Huang CW, 2018, PR MACH LEARN RES, V80; HUTCHINSON MF, 1990, COMMUN STAT SIMULAT, V19, P433, DOI 10.1080/03610919008812866; Jaini P, 2019, PR MACH LEARN RES, V97; Kim Sungwon, 2018, ARXIV181102155; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kumar M., 2019, ARXIV190301434; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Loaiza-Ganem G., 2017, INT C LEARN REPR; Loshchilov I., 2017, P INT C LEARNING REP; Louizos C, 2017, PR MACH LEARN RES, V70; MacKay M., 2018, ADV NEURAL INFORM PR; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Muller T., 2018, ARXIV180803856; Nash C, 2019, PR MACH LEARN RES, V97; Oliva JB, 2018, PR MACH LEARN RES, V80; Oord A.V.D., 2016, SSW; Papamakarios G, 2019, PR MACH LEARN RES, V89, P837; Papamakarios George, 2017, ARXIV170507057; Prenger R., 2018, ARXIV181100002; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rezende Danilo Jimenez, 2018, ARXIV181000597; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans Tim, 2017, ARXIV170105517; Salman H., 2018, ARXIV181003256; Snelson E., 2004, ADV NEURAL INFORM PR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; STEFFEN M, 1990, ASTRON ASTROPHYS, V239, P443; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tomczak Jakub M, 2016, ARXIV161109630; Uria Benigno, 2013, P 26 INT C NEURAL IN; van den Berg R., 2018, C UNC ART INT; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Ziegler ZM, 2019, PR MACH LEARN RES, V97	57	71	71	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307052
C	Choi, E; Xiao, C; Stewart, WF; Sun, JM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Choi, Edward; Xiao, Cao; Stewart, Walter F.; Sun, Jimeng			MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems. External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology. To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes. We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels. We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings. In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15% relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.	[Choi, Edward] Google Brain, Mountain View, CA USA; [Xiao, Cao] IBM Res, Cambridge, MA USA; [Stewart, Walter F.] HINT Consultants, Orinda, CA USA; [Choi, Edward; Sun, Jimeng] Georgia Inst Technol, Atlanta, GA 30332 USA; [Stewart, Walter F.] Sutter Hlth, Sacramento, CA USA	Google Incorporated; International Business Machines (IBM); University System of Georgia; Georgia Institute of Technology	Choi, E (corresponding author), Google Brain, Mountain View, CA USA.	edwardchoi@google.com; cxiao@us.ibm.com; wfs502000@yahoo.com; jsun@cc.gatech.edu	Choi, Edward/AAC-8825-2020	Choi, Edward/0000-0002-5958-3509; Sun, Jimeng/0000-0003-1512-6426	National Science Foundation, award IIS [1418511]; National Science Foundation, award CCF [1533768]; National Institute of Health [1R01MD011682-01, R56HL138415]; Samsung Scholarship	National Science Foundation, award IIS(National Science Foundation (NSF)); National Science Foundation, award CCF; National Institute of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Samsung Scholarship(Samsung)	This work was supported by the National Science Foundation, award IIS-#1418511 and CCF-#1533768, the National Institute of Health award 1R01MD011682-01 and R56HL138415, and Samsung Scholarship. We would also like to thank Sherry Yan for her helpful comments on the original manuscript.	[Anonymous], 2016, SIGKDD; [Anonymous], 2015, SIGKDD; Bajor J.M., 2017, ICLR; Baytas IM, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P65, DOI 10.1145/3097983.3097997; Benton A, 2017, P EACL; Caruana R, 1996, ADV NEUR IN, V8, P959; Che ZP, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507, DOI 10.1145/2783258.2783365; Cho K., 2014, P 2014 C EMP METH NA, P1724; Choi E., 2016, J AM MED INFORM ASS; Choi E, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P787, DOI 10.1145/3097983.3098126; Choi E, 2016, ADV NEUR IN, V29; Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301; Choi Youngduck, 2016, AMIA Jt Summits Transl Sci Proc, V2016, P41; Davis J., 2006, P 23 INT C MACH LEAR, V148, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Esteban C, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI), P93, DOI 10.1109/ICHI.2016.16; Farhan W, 2016, JMIR MED INF, V4, P39, DOI [10.2196/medinform.5977, DOI 10.2196/MEDINFORM.5977]; Fukui Akira, 2016, ARXIV160601847; Futoma J, 2015, J BIOMED INFORM, V56, P229, DOI 10.1016/j.jbi.2015.05.016; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Harutyunyan H., 2017, ARXIV170307771; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Kim J.-H., 2017, ICLR; Kingma D.P, P 3 INT C LEARNING R; Lipton Z.C., 2015, ICLR, P1; Ma FL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1903, DOI 10.1145/3097983.3098088; Miotto R, 2016, SCI REP-UK, V6, DOI 10.1038/srep26094; Ngufor C, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI 2015), P76, DOI 10.1109/ICHI.2015.16; Nori N, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2783258.2783308; Nguyen P, 2018, IEEE IJCNN; ROGER VL, 2004, JAMA-J AM MED ASSOC, V292, P344, DOI [DOI 10.1001/JAMA.292.3.344, 10.1001/jama.292.3.344]; Saito T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118432; Suresh H., 2017, CLIN INTERVENTION PR, P322; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Tensorflow Team, 2016, OSDI; Tran T., 2015, J BIOMEDICAL INFORM; Pham T, 2017, J BIOMED INFORM, V69, P218, DOI 10.1016/j.jbi.2017.04.001; Vijayakrishnan R, 2014, J CARD FAIL, V20, P459, DOI 10.1016/j.cardfail.2014.03.008; Wang F., 2017, SDM, DOI DOI 10.1137/1.9781611974973.23	39	71	72	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304055
C	Chang, SY; Zhang, Y; Han, W; Yu, M; Guo, XX; Tan, W; Cui, XD; Witbrock, M; Hasegawa-Johnson, M; Huang, TS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chang, Shiyu; Zhang, Yang; Han, Wei; Yu, Mo; Guo, Xiaoxiao; Tan, Wei; Cui, Xiaodong; Witbrock, Michael; Hasegawa-Johnson, Mark; Huang, Thomas S.			Dilated Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task. There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DILATEDRNN, which simultaneously tackles all of these challenges. The proposed architecture is characterized by multi-resolution dilated recurrent skip connections, and can be combined flexibly with diverse RNN cells. Moreover, the DILATEDRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies. To provide a theory-based quantification of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures. We rigorously prove the advantages of the DILATEDRNN over other recurrent neural architectures. The code for our method is publicly available(1).	[Chang, Shiyu; Zhang, Yang; Yu, Mo; Guo, Xiaoxiao; Tan, Wei; Cui, Xiaodong; Witbrock, Michael] IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Han, Wei; Hasegawa-Johnson, Mark; Huang, Thomas S.] Univ Illinois, Urbana, IL 61801 USA	International Business Machines (IBM); University of Illinois System; University of Illinois Urbana-Champaign	Chang, SY (corresponding author), IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.	shiyu.chang@ibm.com; yang.zhang2@ibm.com; weihan3@illinois.edu; yum@us.ibm.com; xiaoxiao.guo@ibm.com; wtan@us.ibm.com; cuix@us.ibm.com; witbrock@us.ibm.com; jhasegaw@illinois.edu; t-huang1@illinois.edu	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Arjovsky M, 2016, PR MACH LEARN RES, V48; Bengio, 2016, ARXIV160901704; CAIANIELLO ER, 1982, INT J GEN SYST, V8, P81, DOI 10.1080/03081078208934843; Chung J., 2014, ARXIV14123555; Cooijmans T., 2016, ARXIV160309025; El Hihi Salah, 1995, NIPS, V409; Ha David, 2016, ARXIV160909106; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaeger H., 2001, SHORT TERM MEMORY EC, V5; Koutnik Jan, 2014, ARXIV14023511; Le Q.V., 2015, ABS150400941 CORR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Maharaj T., 2016, ARXIV160601305; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Neil Daniel, 2016, ARXIV161009513; Oord A.V.D., 2016, SSW; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1; Semeniuta S., 2016, P COLING 2016 26 INT, P1757; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Vezhnevets Alexander Sasha, 2017, ARXIV170301161; Wisdom S, 2016, ADV NEUR IN, V29; Xing Z., 2010, ACM SIGKDD EXPLOR NE, V12, P40, DOI [10.1145/1882471.1882478, DOI 10.1145/1882471.1882478]; Yamagishi Junichi, 2012, ENGLISH MULTI SPEAKE; Yu AW, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1880, DOI 10.18653/v1/P17-1172; Yu F., 2016, P ICLR 2016; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75; Zhang SZ, 2016, ADV NEUR IN, V29	30	71	73	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400008
C	Fraccaro, M; Kamronn, S; Paquet, U; Winther, O		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fraccaro, Marco; Kamronn, Simon; Paquet, Ulrich; Winther, Ole			A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.	[Fraccaro, Marco; Kamronn, Simon; Winther, Ole] Tech Univ Denmark, Lyngby, Denmark; [Paquet, Ulrich] DeepMind, London, England	Technical University of Denmark	Fraccaro, M (corresponding author), Tech Univ Denmark, Lyngby, Denmark.		Jeong, Yongwook/N-7413-2016	Winther, Ole/0000-0002-1966-3205	Microsoft Research	Microsoft Research(Microsoft)	We would like to thank Lars Kai Hansen for helpful discussions on the model design. Marco Fraccaro is supported by Microsoft Research through its PhD Scholarship Programme. We thank NVIDIA Corporation for the donation of TITAN X GPUs.	Abadi M, 2015, P 12 USENIX S OPERAT; Archer E., 2015, ARXIV PREPRINT ARXIV; Battaglia Peter W, 2016, ARXIV161200222; Chang MB., 2017, 5 INT C LEARN REPR I; Chiappa S., 2017, ICLR; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Fragkiadaki K., 2016, ICLR; Gao Y., 2016, NIPS; Haarnoja T., 2016, NIPS; Higgins I., 2017, ICLR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jang E., 2016, ARXIV; Karl Maximilian, 2017, ICLR; Kingma D.P, P 3 INT C LEARNING R; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Linderman SW, 2017, PR MACH LEARN RES, V54, P914; Maddison Chris J, 2017, ICLR; Murphy K. P, 1998, TECH REP; Patraucean Viorica, 2015, ARXIV151106309; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Seeger M., 2016, NIPS; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sun W, 2016, PR MACH LEARN RES, V48; Ungerleider Leslie G., 1994, Current Opinion in Neurobiology, V4, P157, DOI 10.1016/0959-4388(94)90066-3; Wahlstrom N., 2015, DEEP LEARN WORKSH 32; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Wu J., 2015, ADV NEURAL INF PROCE, V28, P1, DOI DOI 10.1007/978-3-319-26532-2_15	33	71	71	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403065
C	Li, YT; Murias, M; Major, S; Dawson, G; Dzirasa, K; Carin, L; Carlson, DE		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Yitong; Murias, Michael; Major, Samantha; Dawson, Geraldine; Dzirasa, Kafui; Carin, Lawrence; Carlson, David E.			Targeting EEG/LFP Synchrony with Neural Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the analysis of Electroencephalography (EEG) and Local Field Potential (LFP) datasets, which are "big" in terms of the size of recorded data but rarely have sufficient labels required to train complex models (e. g., conventional deep learning methods). Furthermore, in many scientific applications, the goal is to be able to understand the underlying features related to the classification, which prohibits the blind application of deep networks. This motivates the development of a new model based on parameterized convolutional filters guided by previous neuroscience research; the filters learn relevant frequency bands while targeting synchrony, which are frequency-specific power and phase correlations between electrodes. This results in a highly expressive convolutional neural network with only a few hundred parameters, applicable to smaller datasets. The proposed approach is demonstrated to yield competitive (often state-of-the-art) predictive performance during our empirical tests while yielding interpretable features. Furthermore, a Gaussian process adapter is developed to combine analysis over distinct electrode layouts, allowing the joint processing of multiple datasets to address overfitting and improve generalizability. Finally, it is demonstrated that the proposed framework effectively tracks neural dynamics on children in a clinical trial on Autism Spectrum Disorder.	[Li, Yitong; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Murias, Michael; Major, Samantha; Dawson, Geraldine; Dzirasa, Kafui] Duke Univ, Dept Psychiat, Durham, NC 27706 USA; [Murias, Michael; Major, Samantha; Dawson, Geraldine; Dzirasa, Kafui] Duke Univ, Dept Behav Sci, Durham, NC 27706 USA; [Carlson, David E.] Duke Univ, Dept Civil & Environm Engn, Durham, NC 27706 USA; [Carlson, David E.] Duke Univ, Dept Biostat & Bioinformat, Durham, NC 27706 USA	Duke University; Duke University; Duke University; Duke University; Duke University	Li, YT (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	yitong.li@duke.edu; michael.murias@duke.edu; samantha.major@duke.edu; geraldine.dawson@duke.edu; kafui.dzirasa@duke.edu; lcarin@duke.edu; david.carlson@duke.edu	Dzirasa, Kafui/GQB-1424-2022	Carlson, David/0000-0003-1005-6385; Dawson, Geraldine/0000-0003-1410-2764; Carin, Lawrence/0000-0001-6277-7948	DARPA HIST program; National Institutes of Health [R01MH099192-05S2]; W.M. Keck Foundation; Marcus Foundation; Perkin Elmer; Stylli Translational Neuroscience Award; NICHD [1P50HD093074]	DARPA HIST program; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); W.M. Keck Foundation(W.M. Keck Foundation); Marcus Foundation; Perkin Elmer; Stylli Translational Neuroscience Award; NICHD(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH Eunice Kennedy Shriver National Institute of Child Health & Human Development (NICHD))	In working on this project L.C. received funding from the DARPA HIST program; K.D., L.C., and D.C. received funding from the National Institutes of Health by grant R01MH099192-05S2; K.D received funding from the W.M. Keck Foundation; G.D. received funding from Marcus Foundation, Perkin Elmer, Stylli Translational Neuroscience Award, and NICHD 1P50HD093074.	Aghaei A. S., 2016, IEEE TBME; Bashivan P, 2015, ARXIV151106448; Bastos A.M., 2015, FRONT SYST NEUROSCI, V9; Bosl W, 2011, BMC MED, V9, DOI 10.1186/1741-7015-9-18; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Dawson G, 2017, STEM CELL TRANSL MED, V6, P1332, DOI 10.1002/sctm.16-0474; Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009; Duan RN, 2013, I IEEE EMBS C NEUR E, P81, DOI 10.1109/NER.2013.6695876; Gallagher Neil, 2017, ADV NEURAL INFORM PR, P6842; Hultman R., 2016, NEURON; Jirsa V, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00078; Kingma D.P, P 3 INT C LEARNING R; Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15; Lawhern V. J., 2016, ARXIV161108024; Li, 2016, ADV NEURAL INFORM PR, P1804; Liu W, 2016, LECT NOTES COMPUT SC, V9948, P521, DOI 10.1007/978-3-319-46672-9_58; Lotte F, 2007, J NEURAL ENG, V4, pR1, DOI 10.1088/1741-2560/4/2/R01; Muller KR, 2008, J NEUROSCI METH, V167, P82, DOI 10.1016/j.jneumeth.2007.09.022; Murias M., 2007, BIOL PSYCHIAT; Nurse E, 2016, PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON COMPUTING FRONTIERS (CF'16), P259, DOI 10.1145/2903150.2903159; O'Reilly D, 2012, CLIN NEUROPHYSIOL, V123, P2139, DOI 10.1016/j.clinph.2012.02.087; Oostenveld R, 2011, COMPUT INTEL NEUROSC, V2011, DOI 10.1155/2011/156869; Page A., 2015, IEEE CIRCUITS SYSTEM; Qi Y, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/703816; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Tsinalis Orestis, 2016, arXiv; Ulrich K.R., 2015, ADV NEURAL INFORM PR, P1999; van Putten M. J., 2007, CLIN NEUROPHYSIOL; Williams, 2007, NEURAL INFORM PROCES, P153, DOI DOI 10.5555/2981562.2981582; Yang HJ, 2015, IEEE ENG MED BIO, P2620, DOI 10.1109/EMBC.2015.7318929; Yang Y, 2016, ADV NEUR IN, V29; Zhang N., 2016, INT C NEUR INF PROC; Zheng W.L., 2014, 2014 IEEE INT C MULT, P1, DOI [10.1109/icme.2014.6890166, 10.1109/ICME.2014.6890166, DOI 10.1109/ICME.2014.6890166]; Zheng WL, 2015, IEEE T AUTON MENT DE, V7, P162, DOI 10.1109/TAMD.2015.2431497; Zheng Y, 2014, LECT NOTES COMPUT SC, V8485, P298, DOI 10.1007/978-3-319-08010-9_33	36	71	71	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404067
C	Tran, D; Ranganath, R; Blei, DM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tran, Dustin; Ranganath, Rajesh; Blei, David M.			Hierarchical Implicit Models and Likelihood-Free Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.	[Tran, Dustin; Blei, David M.] Columbia Univ, New York, NY 10027 USA; [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA	Columbia University; Princeton University	Tran, D (corresponding author), Columbia Univ, New York, NY 10027 USA.		Jeong, Yongwook/N-7413-2016		Adobe Research Fellowship; NSF [IIS-0745520, IIS-1247664, IIS-1009542]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; John Templeton Foundation; Google Ph.D. Fellowship in Machine Learning; Facebook; Adobe; Amazon	Adobe Research Fellowship; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); John Templeton Foundation; Google Ph.D. Fellowship in Machine Learning(Google Incorporated); Facebook(Facebook Inc); Adobe; Amazon	We thank Balaji Lakshminarayanan for discussions which helped motivate this work. We also thank Christian Naesseth, Jaan Altosaar, and Adji Dieng for their feedback and comments. DT is supported by a Google Ph.D. Fellowship in Machine Learning and an Adobe Research Fellowship. This work is also supported by NSF IIS-0745520, IIS-1247664, IIS-1009542, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, N66001-15-C-4032, Facebook, Adobe, Amazon, and the John Templeton Foundation.	Anelli G, 2008, J INSTRUM, V3, DOI 10.1088/1748-0221/3/08/S08007; [Anonymous], 2010, BAYESIAN NONPARAMETR; [Anonymous], 1994, THESIS U TORONTO TOR; [Anonymous], 1992, THESIS CALIFORNIA I; [Anonymous], 2008, P 25 INT C MACH LEAR; Bayer J., 2014, LEARNING STOCHASTIC; Beaumont MA, 2010, ANNU REV ECOL EVOL S, V41, P379, DOI 10.1146/annurev-ecolsys-102209-144621; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Chen X., 2016, ARXIV160603657, P2172; Cranmer K., 2015, ARXIV150602169; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Diaconis P, 2007, SIAM REV, V49, P211, DOI 10.1137/S0036144504446436; Dieng A. B., 2017, NEURAL INFORM PROCES; Donahue Jeff, 2017, INT C LEARN REPR ICL; Dumoulin Vincent, 2017, ICLR 2017, P4; Gelman A., 2021, BAYESIAN DATA ANAL, V3rd ed.; Gelman A., 2007, DATA ANAL USING REGR, DOI 10.1017/CBO9780511790942; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gutmann M. U., 2014, ARXIV PREPRINT ARXIV; Hernandez-Lobato J.M., 2016, P 33 INT C MACH LEAR; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Karaletsos T., 2016, ARXIV161205048; KELLER JB, 1986, AM MATH MON, V93, P191, DOI 10.2307/2323340; Kingma D. P, 2014, ARXIV13126114; Kusner Matt J, 2016, ARXIV161104051; Larsen ABL, 2016, PR MACH LEARN RES, V48; Liu Q., 2016, ARXIV161200081; Makhzani A., 2015, ARXIV151105644; Marin JM, 2012, STAT COMPUT, V22, P1167, DOI 10.1007/s11222-011-9288-2; Mescheder L, 2017, PR MACH LEARN RES, V70; Mohamed Shakir, 2016, ARXIV161003483; Papamakarios G, 2016, ADV NEURAL INFORM PR, V29, P1028; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Ranganath R, 2016, ADV NEUR IN, V29; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Sugiyama M, 2012, ANN I STAT MATH, V64, P1009, DOI 10.1007/s10463-011-0343-8; Tran D., 2017, ARXIV171010742; Tran Dustin, 2015, P NIPS 15, P3564; Tran Dustin, 2016, ARXIV161009787; Uehara M., 2016, ARXIV PREPRINT ARXIV; Wilkinson DJ, 2011, STOCHASTIC MODELLING	55	71	72	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405059
C	Shimodaira, H; Noma, K; Nakai, M; Sagayama, S		Dietterich, TG; Becker, S; Ghahramani, Z		Shimodaira, H; Noma, K; Nakai, M; Sagayama, S			Dynamic time-alignment kernel in support vector machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classification algorithms can be employed without further modifications. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs).				sim@jaist.ac.jp; knoma@jaist.ac.jp; mit@jaist.ac.jp; sagayama@hil.t.u-tokyo.ac.jp						Clarkson P, 1999, INT CONF ACOUST SPEE, P585, DOI 10.1109/ICASSP.1999.759734; Collobert R, 2000, SVMTORCH SUPPORT VEC; GANAPATHIRAJU A, 2000, ICSLP2000; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Rabiner L., 1993, FUNDAMENTAL SPEECH R; Scholkopf B., 1998, ADV KERNEL METHODS; SMITH N, 2000, ICSLP 2000, V1, P297; TSUDA K, 1999, EUR S ART NEUR NETW, P183; VAPNIK V, 1998, STAT LEARNING; Watkins C, 2000, ADV NEUR IN, P39	10	71	73	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						921	928						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100115
C	Pearlmutter, BA; Parra, LC		Mozer, MC; Jordan, MI; Petsche, T		Pearlmutter, BA; Parra, LC			Maximum likelihood blind source separation: A context-sensitive generalization of ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result x(i)(t) of mixing n unknown independent sources s(i)(t) through an unknown n x n mixing matrix A(t) of causal linear filters: x(i) = Sigma(j) a(ij) * s(j). We cast the problem as one of maximum likelihood density estimation, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm ''Contextual ICA,'' after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms.			Pearlmutter, BA (corresponding author), UNIV NEW MEXICO,DEPT COMP SCI,FEC 313,ALBUQUERQUE,NM 87131, USA.		Pearlmutter, Barak A/M-8791-2014; Pearlmutter, Barak A./AAL-8999-2020	Pearlmutter, Barak A/0000-0003-0521-4553; 					0	71	74	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						613	619						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00087
C	Dai, AM; Le, QV		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dai, Andrew M.; Le, Quoc V.			Semi-supervised Sequence Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10.	[Dai, Andrew M.; Le, Quoc V.] Google Inc, Mountain View, CA 94043 USA	Google Incorporated	Dai, AM (corresponding author), Google Inc, Mountain View, CA 94043 USA.	adai@google.com; qvl@google.com	Dai, Andrew M./X-3017-2019					Ando RK, 2005, J MACH LEARN RES, V6, P1817; [Anonymous], 2005, ACL; [Anonymous], 2013, EMNLP; Bengio Yoshua, 2003, JMLR; Cardoso-Cachopo A., 2015, DATASETS SINGLE LABE; Dauphin Y., 2013, NIPS; Gers F. A., 2000, NEURAL COMPUTATION; Graves A., 2013, ARXIV; Greff K., 2015, ICML; Hochreiter S, 1997, NEURAL COMPUTATION; Hochreiter S., 2001, FIELD GUIDE DYNAMICA; Jean S., 2014, ICML; Johnson R., 2014, NAACL; Kim Y., 2014, ARXIV PREPRINT ARXIV; Kiros R., 2015, NIPS; Krizhevsky A., 2010, CONVOLUTIONAL DEEP B; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lang K., 1995, ICML; Larochelle H., 2012, JMLR; Le Q., 2014, ICML; Lehmann J., 2014, SEMANTIC WEB; Luong T., 2014, ABS14108206 CORR; Maas A. L., 2011, ACL; McAuley Julian, 2013, P 7 ACM C REC SYST A, DOI DOI 10.1145/2507157.2507163; Mikolov T., 2010, INTERSPEECH; Ng J. Y., 2015, CVPR; Rumelhart D. E., 1986, NATURE; Shang Lifeng, 2015, EMNLP; Socher Richard, 2012, EMNLP; Srivastava Nitish, 2015, ICML; Sutskever I., 2014, NEURIPS; Sutskever I., 2014, ARXIV; Vinyals O., 2015, NIPS; Vinyals O., 2014, CVPR; Vinyals O., 2015, ICML DEEP LEARN WORK; Wang Sida, 2012, ACL; Werbos P., 1975, REGRESSION NEW TOOLS; Zhang X., 2015, NIPS	39	70	72	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102097
C	Zhu, J; Hastie, T		Dietterich, TG; Becker, S; Ghahramani, Z		Zhu, J; Hastie, T			Kernel logistic regression and the import vector machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The support vector machine (SVM) is known for its good performance in binary classification, but its extension to multi-class classification is still an on-going research issue. In this paper, we propose a new approach for classification, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only performs as well as the SVM in binary classification, but also can naturally be generalized to the multi-class case. Furthermore, the lVM provides an estimate of the underlying probability. Similar to the "support points" of the SVM, the lVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a computational advantage over the SVM, especially when the size of the training data set is large.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Zhu, J (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.			Hastie, Trevor/0000-0002-0164-3142				Burges C C, 1998, DATA MINING KNOWLEDG; EVGENIOU T, 1999, ADV LARGE MARGIN CLA; GREEN P, 1985, LECTURE NOTES STATIS, V32, P44; HASTIE T, 2001, IN PRESS ELEMENTS ST; Hastie T.J., 1990, GEN ADDITIVE MODELS, V43; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; Lin X., 1998, 998 U WISC DEP STAT; Smola Alex J, 2000, SPARSE GREEDY MATRIX; Wahba G., 1995, MATH GEN; WAHBA G, 1998, 984RR U WISC DEP STA; WILLIAMS CKI, 2001, ADV NEURAL INFORMATI, V13	11	70	70	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1081	1088						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100135
C	Tipping, ME		Leen, TK; Dietterich, TG; Tresp, V		Tipping, ME			Sparse kernel principal component analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					'Kernel' principal component analysis (PCA) is an elegant nonlinear generalisation of the popular linear data analysis method, where a kernel function implicitly defines a nonlinear transformation into a feature space wherein standard PCA is performed. Unfortunately, the technique is not 'sparse', since the components thus obtained are expressed in terms of kernels associated with every training vector. This paper shows that by approximating the covariance matrix in feature space by a reduced number of example vectors, using a maximum-likelihood approach, we may obtain a highly sparse form of kernel PCA without loss of effectiveness.	Microsoft Res, Cambridge CB2 3NH, England	Microsoft	Tipping, ME (corresponding author), Microsoft Res, St George House,1 Guildhall St, Cambridge CB2 3NH, England.							Ripley BD., 1996; Romdhani S, 1999, P 10 BRIT MACH VIS C, P483, DOI [10.5244/C.13.48, DOI 10.5244/C.13.48]; RUBIN DB, 1982, PSYCHOMETRIKA, V47, P69, DOI 10.1007/BF02293851; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 1996, 44 M PLANCK I BIOL K; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tipping ME, 2000, ADV NEUR IN, V12, P652	7	70	75	0	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						633	639						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800090
C	Maass, W		Mozer, MC; Jordan, MI; Petsche, T		Maass, W			Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We exhibit a novel way of simulating sigmoidal neural nets by networks of noisy spiking neurons in temporal coding. Furthermore it is shown that networks of noisy spiking neurons with temporal coding have a strictly larger computational power than sigmoidal neural nets with the same number of units.			Maass, W (corresponding author), GRAZ TECH UNIV,INST THEORET COMP SCI,KLOSTERWIESGASSE 32-2,A-8010 GRAZ,AUSTRIA.								0	70	76	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						211	217						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00030
C	Volkovs, M; Yu, GW; Poutanen, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Volkovs, Maksims; Yu, Guangwei; Poutanen, Tomi			DropoutNet: Addressing Cold Start in Recommender Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Latent models have become the default choice for recommender systems due to their performance and scalability. However, research in this area has primarily focused on modeling user-item interactions, and few latent models have been developed for cold start. Deep learning has recently achieved remarkable success showing excellent results for diverse input types. Inspired by these results we propose a neural network based latent model called DropoutNet to address the cold start problem in recommender systems. Unlike existing approaches that incorporate additional content-based objective terms, we instead focus on the optimization and show that neural network models can be explicitly trained for cold start through dropout. Our model can be applied on top of any existing latent model effectively providing cold start capabilities, and full power of deep architectures. Empirically we demonstrate state-of-the-art accuracy on publicly available benchmarks.	[Volkovs, Maksims; Yu, Guangwei; Poutanen, Tomi] Layer6 Ai, Toronto, ON, Canada		Volkovs, M (corresponding author), Layer6 Ai, Toronto, ON, Canada.	maks@layer6.ai; guang@layer6.ai; tomi@layer6.ai						Abadi M., TENSORFLOW LARGE SCA; Abel F., 2017, RECSYS CHALLENGE 201; Agarwal D., 2009, C KNOWL DISC DAT MIN; [Anonymous], 2009, ADV ARTIFICIAL INTEL; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Collobert  R., 2011, J MACHINE LEARNING R; Covington P., 2016, ACM RECOMMENDER SYST; Gopalan, 2013, ARXIV13111704; Gopalan P. K., 2014, NEURAL INFORM PROCES; Graves A., 2013, C AC SPEECH SIGN PRO; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2012, IEEE SIGNAL PROCESSI; Hu Y., 2008, INT C DAT ENG; Ioffe S., 2015, P 32 INT C MACH LEAR, P448, DOI DOI 10.1016/J.MOLSTRUC.2016.12.061; Krizhevsky A., 2012, NIPS, V1, P4; Le Q, 2014, INT C MACH LEARN; Maaten L.v.d., 2008, J MACHINE LEARNING R; Srivastava N., 2014, J MACHINE LEARNING R; Van den Oord A., 2013, NEURAL INFORM PROCES; Vincent P., 2010, J MACHINE LEARNING R; Wang C., 2011, C KNOWL DISC DAT MIN; Wang H., 2015, C KNOWL DISC DAT MIN; Wu C.- Y., 2017, C WEB SEARCH DAT MIN	23	69	69	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405004
C	Karpathy, A; Joulin, A; Li, FF		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Karpathy, Andrej; Joulin, Armand; Li Fei-Fei			Deep Fragment Embeddings for Bidirectional Image Sentence Mapping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.	[Karpathy, Andrej; Joulin, Armand; Li Fei-Fei] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Karpathy, A (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	karpathy@cs.stanford.edu; ajoulin@cs.stanford.edu; feifeili@cs.stanford.edu			NVIDIA Corporation; ONR MURI grant; NSF [ISS-1115313]	NVIDIA Corporation; ONR MURI grant; NSF(National Science Foundation (NSF))	We thank Justin Johnson and Jon Krause for helpful comments and discussions. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. This research is supported by an ONR MURI grant, and NSF ISS-1115313.	Andrews S, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P943; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Bengio Y, 2006, STUD FUZZ SOFT COMP, V194, P137; Chen Y., 2006, CVPR, V28; Collobert R., 2008, P 25 ICML, V25, P160, DOI DOI 10.1145/1390156.1390177; De Marneffe M.-C., 2006, P LREC, V6, P449; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Frome Andrea, 2013, NEURIPS; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Huang Eric, 2012, P 50 ANN M ASS COMP, V1, P873; Jia Y, 2011, IEEE INT C COMP VIS; Jia Y., 2013, CAFFE OPEN SOURCE CO; Kiros R, 2014, PR MACH LEARN RES, V32, P595; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li S., 2011, P 15 C COMPUTATIONAL, P220; Matuszek C., 2012, P 2012 INT C MACH LE; Mikolov T., 2013, ARXIV; Mitchell Margaret, 2012, EACL; Mnih A., 2007, INT C MACHINE LEARNI, P641, DOI DOI 10.1145/1273496.1273577; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Ordonez Vicente, 2011, ADV NEURAL INFORM PR, P1143; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.1002/ACP.3140; Russakovsky O., 2013, LARGE SCALE VISUAL R; Socher R., 2011, P 28 INT C INT C MAC, P129; Socher R., 2014, J T ASS COMPUT LINGU, V2, P207, DOI DOI 10.1162/TACL_A_00177; Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112; Srivastava Nitish, 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49; Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384; Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411; Young P., 2014, P TACL, V2, P67, DOI 10.1162/tacl_a_00166; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zitnick C. L., 2013, ICCV	41	69	69	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100060
C	Bakker, B		Dietterich, TG; Becker, S; Ghahramani, Z		Bakker, B			Reinforcement learning with long short-term memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper presents reinforcement learning with a Long Short-Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(lambda) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task.	Leiden Univ, IDSIA, Dept Psychol, NL-2300 RB Leiden, Netherlands	Leiden University; Leiden University - Excl LUMC	Bakker, B (corresponding author), Leiden Univ, IDSIA, Dept Psychol, POB 9555, NL-2300 RB Leiden, Netherlands.	bbakker@fsw.leidenuniv.nl						Bakker B., 2001, REINFORCEMENT LEARNI; CHRISMAN L, 1992, P 10 NAT C AI; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; HARMON ME, 1996, MULTIPLAYER RESIDUAL; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; LIN LJ, 1993, P 2 INT C SIM AD BEH; LOCH J, 1998, P ICML 98; MCCALLUM RA, 1996, P 4 INT C SIM AD BEH; PESHKIN L, 1999, P 16 INT C MACH LEAR; SCHMIDHUBER J, 1990, P DISTR AD NEUR INF; SCHMIDHUBER J, 1991, P INT JOINT C NEUR N, V2, P1458, DOI DOI 10.1109/IJCNN.1991.170605; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	12	69	70	2	10	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1475	1482						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100183
C	Baird, L; Moore, A		Kearns, MS; Solla, SA; Cohn, DA		Baird, L; Moore, A			Gradient descent for general reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					A simple learning rule is derived, the VAPS algorithm, which can be instantiated to generate a wide range of new reinforcement-learning algorithms. These algorithms solve a number of open problems, define several new approaches to reinforcement learning, and unify different approaches to reinforcement learning under a single theory. These algorithms all have guaranteed convergence, and include modifications of several existing algorithms that were known to fail to converge on simple MDPs. These include Q-learning, SARSA, and advantage learning. In addition to these value-based algorithms it also generates pure policy-search reinforcement-learning algorithms, which learn optimal policies without learning a value function. In addition, it allows policy-search and value-based algorithms to be combined, thus unifying two very different approaches to reinforcement learning into a single Value and Policy Search (VAPS) algorithm. And these algorithms converge for POMDPs without requiring a proper belief state. Simulations results are given, and several areas for future research are discussed.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Baird, L (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.							Baird L., 1995, MACHINE LEARNING; GULLAPALLI V, 1992, 9210 U MASS; KAELBLING LP, IN PRESS ARTIFICIAL; Marbach P., 1998, THESIS MIT; MCCALLUM, 1995, REINFORCEMENT LEARNI; NARENDRA KS, 1989, LEARNING AUTOMATA IN; TRESP V, 1995, P NEUR NETW SIGN PRO, V5, P1; Williams R, 1988, THEORY REINFORCEMENT	9	69	73	1	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						968	974						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700136
C	Birattari, M; Bontempi, G; Bersini, H		Kearns, MS; Solla, SA; Cohn, DA		Birattari, M; Bontempi, G; Bersini, H			Lazy learning meets the recursive least squares algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Lazy learning is a memory-based technique that, once a query is received, extracts a prediction interpolating locally the neighboring examples of the query which are considered relevant according to a distance measure. In this paper we propose a data-driven method to select on a query-by-query basis the optimal number of neighbors to be considered for each prediction. As an efficient way to identify and validate local models, the recursive least squares algorithm is introduced in the context of local approximation and lazy learning. Furthermore, beside the winner-takes-all strategy for model selection, a local combination of the most promising models is explored. The method proposed is tested on six different datasets and compared with a state-of-the-art approach.	Free Univ Brussels, IRIDIA, Brussels, Belgium	Universite Libre de Bruxelles; Vrije Universiteit Brussel	Birattari, M (corresponding author), Free Univ Brussels, IRIDIA, Brussels, Belgium.		Bontempi, Gianluca/ABE-4365-2020; Birattari, Mauro/S-3633-2019; Bontempi, Gianluca/J-7121-2012; Birattari, Mauro/D-2597-2009	Bontempi, Gianluca/0000-0001-8621-316X; Birattari, Mauro/0000-0003-3309-2194; Bontempi, Gianluca/0000-0001-8621-316X; Birattari, Mauro/0000-0003-3309-2194				Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; Bierman G.J., 1977, FACTORIZATION METHOD; BONTEMPI G, 1997, UNPUB INT J CONTROL; Friedman J. H., 1994, FLEXIBLE METRIC NEAR; Merz C, 1998, UCI REPOSITORY MACHI; Perrone M., 1993, ARTIFICIAL NEURAL NE, P126; Quinlan J.R., 1993, MACHINE LEARNING P 1, P236, DOI [10.1016/B978-1-55860-307-3.50037-X, DOI 10.1016/B978-1-55860-307-3.50037-X]; Schaal S, 1998, NEURAL COMPUT, V10, P2047, DOI 10.1162/089976698300016963; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; [No title captured]	10	69	70	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						375	381						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700053
C	Alistarh, D; Allen-Zhu, Z; Li, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Alistarh, Dan; Allen-Zhu, Zeyuan; Li, Jerry			Byzantine Stochastic Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper studies the problem of distributed stochastic optimization in an adversarial setting where, out of m machines which allegedly compute stochastic gradients every iteration, an alpha-fraction are Byzantine, and may behave adversarially. Our main result is a variant of stochastic gradient descent (SGD) which finds epsilon-approximate minimizers of convex functions in T = (O) over tilde (1/epsilon(2)m + alpha(2)/epsilon(2) ) iterations. In contrast, traditional mini-batch SGD needs T = O (1/epsilon(2)m) iterations, but cannot tolerate Byzantine failures. Further, we provide a lower bound showing that, up to logarithmic factors, our algorithm is information-theoretically optimal both in terms of sample complexity and time complexity.	[Alistarh, Dan] IST Austria, Klosterneuburg, Austria; [Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA USA; [Li, Jerry] Simons Inst, Berkeley, CA USA	Institute of Science & Technology - Austria	Alistarh, D (corresponding author), IST Austria, Klosterneuburg, Austria.	dan.alistarh@ist.ac.at; zeyuan@csail.mit.edu; jerryzli@berkeley.edu			NSF CAREER Award [CCF-1453261, CCF-1565235]; Google Faculty Research Award; NSF Graduate Research Fellowship	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Faculty Research Award(Google Incorporated); NSF Graduate Research Fellowship(National Science Foundation (NSF))	We would like to thank Yuval Peres for suggesting reference [23]. Jerry Li is supported by NSF CAREER Award CCF-1453261, CCF-1565235, a Google Faculty Research Award, and an NSF Graduate Research Fellowship.	[Anonymous], 2016, NIPS; Balakrishnan S., 2017, C LEARN THEOR PMLR, P169; Ben-Tal A, 2001, LECT MODERN CONVEX O; Bhatia K, 2017, ADV NEUR IN, V30; Chen Y., 2017, P ACM MEAS ANAL COMP, V46, P1, DOI [10.1145/3308809.3308857, DOI 10.1145/3308809.3308857]; Diakonikolas I, 2018, ARXIV180302815; Diakonikolas I, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2683; Feldman P., 1988, Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, P148, DOI 10.1145/62212.62225; Feng J., 2014, ARXIV14095937; Huber PJ, 2009, WILEY SERIES PROBABI; Klivans A., 2018, C LEARNING THEORY, P1420; Konecn J., 2016, ARXIV161005492; Lai KA, 2016, ANN IEEE SYMP FOUND, P665, DOI 10.1109/FOCS.2016.76; LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176; Nasrabadi Nasser M., 2011, ADV NEURAL INFORM PR, P1881; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2017, DOI 10.1109/TIT.2013.2240435; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI [10.1214/aop/1176988477, DOI 10.1214/AOP/1176988477]; Prasad A., 2018, ARXIV180206485; RAKHLIN A., 2012, P INT C MACH LEARN, P1571; Su LL, 2016, PROCEEDINGS OF THE 2016 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING (PODC'16), P425, DOI 10.1145/2933057.2933105; Su Lili, 2016, ISDC; Tukey JW, 1975, P ICM, V2, P523; Xie C., 2018, ARXIV180210116; Yin D, 2018, PR MACH LEARN RES, V80	31	68	68	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304061
C	Li, R; Kahou, SE; Schulz, H; Michalski, V; Charlin, L; Pal, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Raymond; Kahou, Samira Ebrahimi; Schulz, Hannes; Michalski, Vincent; Charlin, Laurent; Pal, Chris			Towards Deep Conversational Recommendations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					There has been growing interest in using neural networks and deep learning techniques to create dialogue systems. Conversational recommendation is an interesting setting for the scientific exploration of dialogue with natural language as the associated discourse involves goal-driven dialogue that often transforms naturally into more free-form chat. This paper provides two contributions. First, until now there has been no publicly available large-scale dataset consisting of real-world dialogues centered around recommendations. To address this issue and to facilitate our exploration here, we have collected REDIAL, a dataset consisting of over 10,000 conversations centered around the theme of providing movie recommendations. We make this data available to the community for further research. Second, we use this dataset to explore multiple facets of conversational recommendations. In particular we explore new neural architectures, mechanisms, and methods suitable for composing conversational recommendation systems. Our dataset allows us to systematically probe model sub-components addressing different parts of the overall problem domain ranging from: sentiment analysis and cold-start recommendation generation to detailed aspects of how natural language is used in this setting in the real world. We combine such sub-components into a full-blown dialogue system and examine its behavior.	[Li, Raymond; Kahou, Samira Ebrahimi; Pal, Chris] Ecole Polytech Montreal, Montreal, PQ, Canada; [Li, Raymond; Pal, Chris] Element AI, Montreal, PQ, Canada; [Kahou, Samira Ebrahimi; Schulz, Hannes] Microsoft Res Montreal, Montreal, PQ, Canada; [Michalski, Vincent] Univ Montreal, Montreal, PQ, Canada; [Michalski, Vincent; Charlin, Laurent; Pal, Chris] Mila, Montreal, PQ, Canada; [Charlin, Laurent] HEC Montreal, Montreal, PQ, Canada	Universite de Montreal; Polytechnique Montreal; Universite de Montreal; Universite de Montreal; HEC Montreal	Li, R (corresponding author), Ecole Polytech Montreal, Montreal, PQ, Canada.; Li, R (corresponding author), Element AI, Montreal, PQ, Canada.							Ba J., 2017, P 3 INT C LEARN REPR; Cho K, 2014, C EMP METH NAT LANG, DOI 10.3115/v1/D14-1179; Christakopoulou K, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P815, DOI 10.1145/2939672.2939746; COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104; Das Abhishek, 2017, C COMP VIS PATT REC, V2; Das Abhishek, 2017, ARXIV170306585CSCV; Dodge Jesse, 2015, ARXIV151106931CSCL; Goker Mehmet H., 2011, ARXIV11070029CSIR; Greco C, 2017, LECT NOTES ARTIF INT, V10640, P372, DOI 10.1007/978-3-319-70169-1_28; Gulcehre C, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P140, DOI 10.18653/v1/p16-1014; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; He J., 2015, ARXIV PREPRINT ARXIV; Jacomy M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098679; Johansson Pontus, 2004, THESIS; Krause Ben, 2017, ARXIV170909816CSCL; Li X, 2017, INT JOINT C NAT LANG; Liu C.W., 2016, ARXIV160308023; Marlin Benjamin M., 2007, P 23 C UNC ART INT, P267; Sedhain S, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P111, DOI 10.1145/2740908.2742726; Serban Iulian Vlad, 2015, ARXIV151205742CSCL; Sordoni Alessandro, 2015, P 24 ACM INT C INF K, P553, DOI DOI 10.1145/2806416.2806493; Subramanian Sandeep, 2018, INT C LEARN REPR; Suglia A, 2017, CLEF; Sun Yueming, 2018, ARXIV180603277; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Warnestal Pontus, 2007, SIGDIAL WORKSH DISC; Widyantoro Dwi H., 2014, 2014 2nd International Conference on Information and Communication Technology (ICoICT), P160, DOI 10.1109/ICoICT.2014.6914058	28	68	68	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004030
C	Sacramento, J; Costa, RP; Bengio, Y; Senn, W		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sacramento, Joao; Costa, Rui Ponte; Bengio, Yoshua; Senn, Walter			Dendritic cortical microcircuits approximate the backpropagation algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPLEMENTARY LEARNING-SYSTEMS; PLASTICITY; MODELS; REPRESENTATIONS; NEURONS; CORTEX; ACTIVATION; PREDICTION; EXCITATION; CIRCUIT	Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances - error backpropagation -appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.	[Sacramento, Joao; Costa, Rui Ponte; Senn, Walter] Univ Bern, Dept Physiol, Bern, Switzerland; [Bengio, Yoshua] Mila, Montreal, PQ, Canada; [Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada; [Sacramento, Joao] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland; [Sacramento, Joao] Swiss Fed Inst Technol, Zurich, Switzerland; [Costa, Rui Ponte] Univ Bristol, Fac Engn, SCEEM, Computat Neurosci Unit,Dept Comp Sci, Bristol, Avon, England	University of Bern; Universite de Montreal; University of Zurich; Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Bristol	Sacramento, J (corresponding author), Univ Bern, Dept Physiol, Bern, Switzerland.	sacramento@pyl.unibe.ch; costa@pyl.unibe.ch; yoshua.bengio@mila.quebec; senn@pyl.unibe.ch		Costa, Rui Ponte/0000-0003-2595-2027	Swiss National Science Foundation [310030L-156863]; European Union's Horizon 2020 Framework Programme for Research and Innovation [785907]; NSERC; CIFAR; Canada Research Chairs	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); European Union's Horizon 2020 Framework Programme for Research and Innovation; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Canada Research Chairs(Canada Research ChairsCGIAR)	This work has been supported by the Swiss National Science Foundation (grant 310030L-156863 of WS), the European Union's Horizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreement No. 785907 (Human Brain Project), NSERC, CIFAR, and Canada Research Chairs.	ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Attinger A, 2017, CELL, V169, P1291, DOI 10.1016/j.cell.2017.05.023; Bengio, 2014, ARXIV14077906; Bono J, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-00740-z; Bottou L., 1998, ONLINE ALGORITHMS ST; Chiu CQ, 2018, NEURON, V97, P368, DOI 10.1016/j.neuron.2017.12.032; Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479; COSTA RP, 2017, ADV NEURAL INFORM PR, P271; CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0; Cun Y., 1988, P 1988 CONN MOD SUMM, P21, DOI DOI 10.3168/JDS.S0022-0302(88)79586-7; Dorrn AL, 2010, NATURE, V465, P932, DOI 10.1038/nature09119; Friedrich J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002092; Friston KJ, 2005, PHILOS T R SOC B, V360, P815, DOI 10.1098/rstb.2005.1622; Froemke RC, 2015, ANNU REV NEUROSCI, V38, P195, DOI 10.1146/annurev-neuro-071714-034002; Fu Y, 2015, ELIFE, V4, DOI 10.7554/eLife.05558; GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x; Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901; Hinton G.E., 1988, NEURAL INFORM PROCES, P358; Kell A. J., 2018, NEURON; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kording KP, 2000, NEURAL NETWORKS, V13, P1, DOI 10.1016/S0893-6080(99)00088-X; Kording KP, 2001, J COMPUT NEUROSCI, V11, P207, DOI 10.1023/A:1013776130161; Kumaran D, 2016, TRENDS COGN SCI, V20, P512, DOI 10.1016/j.tics.2016.05.004; Larkum M, 2013, TRENDS NEUROSCI, V36, P141, DOI 10.1016/j.tins.2012.11.006; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Leinweber M, 2017, NEURON, V95, P1420, DOI 10.1016/j.neuron.2017.08.036; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Lillicrap TP, 2013, NEURON, V77, P168, DOI 10.1016/j.neuron.2012.10.041; Luz Y, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002334; Makino H, 2015, NAT NEUROSCI, V18, P1116, DOI 10.1038/nn.4061; Manita S, 2015, NEURON, V86, P1304, DOI 10.1016/j.neuron.2015.05.006; Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094; Masquelier T., 2007, PLOS COMPUTATIONAL B, V3; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037; Nokland, 2016, ADV NEURAL INFORM PR, V29, P1037; OReilly RC, 1996, NEURAL COMPUT, V8, P895, DOI 10.1162/neco.1996.8.5.895; Paken JMP, 2016, ELIFE, V5, DOI 10.7554/eLife.14985; Petreanu L, 2012, NATURE, V489, P299, DOI 10.1038/nature11321; Petreanu L, 2009, NATURE, V457, P1142, DOI 10.1038/nature07709; Poort J, 2015, NEURON, V86, P1478, DOI 10.1016/j.neuron.2015.05.037; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Roelfsema PR, 2018, NAT REV NEUROSCI, V19, P166, DOI 10.1038/nrn.2018.6; Roelfsema PR, 2005, NEURAL COMPUT, V17, P2176, DOI 10.1162/0899766054615699; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024; Schwiedrzik CM, 2017, NEURON, V96, P89, DOI 10.1016/j.neuron.2017.09.007; Sjostrom PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6; Spicher D., 2018, PREDICTIVE PLA UNPUB; Spruston N, 2008, NAT REV NEUROSCI, V9, P206, DOI 10.1038/nrn2286; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Urban-Ciecko J, 2016, NAT REV NEUROSCI, V17, P401, DOI 10.1038/nrn.2016.53; Urbanczik R, 2014, NEURON, V81, P521, DOI 10.1016/j.neuron.2013.11.030; Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095; Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949; Xie XH, 2003, NEURAL COMPUT, V15, P441, DOI 10.1162/089976603762552988; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zmarz P, 2016, NEURON, V92, P766, DOI 10.1016/j.neuron.2016.09.057	60	68	68	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003029
C	Suwajanakorn, S; Snavely, N; Tompson, J; Norouzi, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Suwajanakorn, Supasorn; Snavely, Noah; Tompson, Jonathan; Norouzi, Mohammad			Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task. We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation. The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet [6] are visualized at keypointnet.github.io.	[Suwajanakorn, Supasorn] Vidyasirimedhi Inst Sci & Technol, Rayong, Thailand; [Snavely, Noah; Tompson, Jonathan; Norouzi, Mohammad] Google AI, Mountain View, CA USA; [Suwajanakorn, Supasorn] Google AI Residency Program, Mountain View, CA 94035 USA	Vidyasirimedhi Institute of Science & Technology	Suwajanakorn, S (corresponding author), Vidyasirimedhi Inst Sci & Technol, Rayong, Thailand.; Suwajanakorn, S (corresponding author), Google AI Residency Program, Mountain View, CA 94035 USA.	supasorn@vistec.ac.th; snavely@google.com; tompson@google.com; mnorouzi@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Arie-Nachimson M., 2009, ICCV; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen Yu, 2017, ARXIV171100253; Choy Christopher Bongsoo, 2016, ADV NEURAL INFORM PR, P2406; Giles M, 2008, EXTENDED COLLECTION; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Guo SJ, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417800; Han K, 2017, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2017.203; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hejrati M., 2012, NIPS; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hinton Geoffrey E., 2018, INT C LEARN REPR; Huang Qi-Xing, 2013, COMPUTER GRAPHICS FO; Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jakab Tomas, 2018, ARXIV180607823; Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Mehta Dushyant, 2017, ARXIV171203453; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Oord A.V.D., 2016, SSW; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Pons-Moll G, 2015, INT J COMPUT VISION, V113, P163, DOI 10.1007/s11263-015-0818-9; Ramakrishna Varun, 2012, ECCV, P573, DOI DOI 10.1007/978-3-642-33765-9; Rhodin Helge, 2018, EUR C COMP VIS ECCV; Sabour Sara, 2017, PROC 31 INT C NEURAL; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Salti Samuele, 2015, ICCV; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Snavely Noah, 2006, ACM T GRAPHICS TOG; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thewlis James, 2017, ADV NEURAL INFORM PR, V3, P8; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Tremblay J, 2018, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW.2018.00143; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758; Tung H.-Y., 2017, ADV NEURAL INFORM PR, P5236; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang Qianqian, 2017, ARXIV171107641; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144; Zhang YT, 2018, PROC CVPR IEEE, P2694, DOI 10.1109/CVPR.2018.00285; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou Xingyi, 2017, ARXIV171205765	67	68	68	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302010
C	Gorham, J; Mackey, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gorham, Jackson; Mackey, Lester			Measuring Sample Quality with Stein's Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MULTIVARIATE NORMAL APPROXIMATION; EXCHANGEABLE PAIRS; METRICS	To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein's method that bounds the discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyper-parameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.	[Gorham, Jackson; Mackey, Lester] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Gorham, J (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.							Ahn S., 2012, P 29 INT C MACH LEAR; Bach F., 2012, P 29 INT C INT C MAC, P1355; Barbour, 1988, J APPL PROBAB A, V25, P175; Bouts Q. W., 2014, P 30 ANN S COMP GEOM; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Caflisch R. E., 1998, Acta Numerica, V7, P1, DOI 10.1017/S0962492900002804; Chatterjee S, 2011, ANN APPL PROBAB, V21, P464, DOI 10.1214/10-AAP712; Chatterjee S, 2008, ALEA-LAT AM J PROBAB, V4, P257; Chen Y., 2010, P 26 UNC ART INT UAI; Chew P., 1986, P 2 ANN S COMP GEOM, P169, DOI DOI 10.1145/10515.10534; Del Barrio E, 1999, ANN PROBAB, V27, P1009, DOI 10.1214/aop/1022677394; Dobler C., 2014, ARXIV14114477; Fan Y., 2006, J COMPUTATIONAL GRAP, V15; GEYER CJ, 1991, COMPUTING SCIENCE AND STATISTICS, P156; Glaeser G., 1958, J ANAL MATH JERUSALE, V6, P1, DOI DOI 10.1007/BF02790231; Gurobi Optimization L, 2020, GUROBI OPTIMIZER REF; Har-Peled S, 2006, SIAM J COMPUT, V35, P1148, DOI 10.1137/S0097539704446281; Korattikara A, 2014, PR MACH LEARN RES, V32; Lubin M, 2015, INFORMS J COMPUT, V27, P238, DOI 10.1287/ijoc.2014.0623; Meckes Elizabeth, 2009, IMS COLL HIGH DIM PR, V5, P153; PELEG D, 1989, J GRAPH THEOR, V13, P99, DOI 10.1002/jgt.3190130114; Reinert G, 2009, ANN PROBAB, V37, P2150, DOI 10.1214/09-AOP467; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Shvartsman P, 2008, T AM MATH SOC, V360, P5529, DOI 10.1090/S0002-9947-08-04469-3; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Stein C., 1972, PROC 6 BERKELEY S MA, VII, p583?602; Vallender SS, 1974, THEOR PROBAB APPL, V18, P784, DOI DOI 10.1137/1118101; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; ZELLNER A, 1995, J AM STAT ASSOC, V90, P921, DOI 10.2307/2291326	31	68	68	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101028
C	Tenenbaum, JB		Kearns, MS; Solla, SA; Cohn, DA		Tenenbaum, JB			Bayesian modeling of human concept learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					I consider the problem of learning concepts from small numbers of positive examples, a feat which humans perform routinely but which computers are rarely capable of. Bridging machine learning and cognitive science perspectives, I present both theoretical analysis and an empirical study with human subjects for the simple task of learning concepts corresponding to axis-aligned rectangles in a multidimensional feature space. Existing learning models, when applied to this task, cannot explain how subjects generalize from only a few examples of the concept. I propose a principled Bayesian model based on the assumption that the examples are a random sample from the concept to be learned. The model gives precise fits to human behavior on this simple task and provides qualitative insights into more complex, realistic cases of concept learning.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Tenenbaum, JB (corresponding author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.							Bruner JS, 1956, STUDY THINKING; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Feldman J, 1997, J MATH PSYCHOL, V41, P145, DOI 10.1006/jmps.1997.1154; HAUSSLER D, 1994, MACH LEARN, V14, P83, DOI 10.1007/BF00993163; JAAKKOLA T, 1996, ADV NEURAL INFORMATI, P8; Japkowicz N., 1995, P 14 INT JOINT C ART; KRUSCHKE JK, 1992, PSYCHOL REV, V99, P22, DOI 10.1037/0033-295X.99.1.22; MITCHELL TOM M., 1997, MACH LEARN, P2; MUGGLETON S, UNPUB MACHINE LEARNI; SHEPARD RN, 1987, SCIENCE, V237, P1317, DOI 10.1126/science.3629243; Tenenbaum J., 1999, THESIS MIT	11	68	69	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						59	65						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700009
C	Niebur, E; Koch, C		Touretzky, DS; Mozer, MC; Hasselmo, ME		Niebur, E; Koch, C			Control of selective visual attention: Modeling the ''where'' pathway	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CALTECH,PASADENA,CA 91125	California Institute of Technology									0	68	84	0	3	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						802	808						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00113
C	Zhuang, JT; Tang, T; Ding, YF; Tatikonda, S; Dvornek, N; Papademetris, X; Duncan, JS		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Zhuang, Juntang; Tang, Tommy; Ding, Yifan; Tatikonda, Sekhar; Dvornek, Nicha; Papademetris, Xenophon; Duncan, James S.			AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability. We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer	[Zhuang, Juntang; Tatikonda, Sekhar; Dvornek, Nicha; Papademetris, Xenophon; Duncan, James S.] Yale Univ, New Haven, CT 06520 USA; [Tang, Tommy] Univ Illinois, Urbana, IL USA; [Ding, Yifan] Univ Cent Florida, Orlando, FL 32816 USA	Yale University; University of Illinois System; University of Illinois Urbana-Champaign; State University System of Florida; University of Central Florida	Zhuang, JT (corresponding author), Yale Univ, New Haven, CT 06520 USA.	j.zhuang@yale.edu; tommymt2@illinois.edu; yf.ding@knights.ucf.edu; sekhar.tatikonda@yale.edu; nicha.dvornek@yale.edu; xenophon.papademetris@yale.edu; james.duncan@yale.edu	Zhuang, Juntang/ABF-9267-2021		NIH [R01NS035193]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research is supported by NIH grant R01NS035193.	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjovsky M., 2017, ARXIV170107875; Balles Lukas, 2017, ARXIV170507774; BATTITI R, 1992, NEURAL COMPUT, V4, P141, DOI 10.1162/neco.1992.4.2.141; BEALE EML, 1955, J ROY STAT SOC B, V17, P173; Bernstein J., 2018, ARXIV180204434, V80, P560; Bernstein J, 2020, ADV NEURAL INFORM PR, V33, P21370; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Chen J., 2018, ARXIV180606763; Chen Xiaoran, 2018, P INT C LEARN REPR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Graves A, 2013, ARXIV13080850; Gulrajani I, 2017, P NIPS 2017; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang H, 2018, ARXIV180507557; Keskar N. S., 2017, ARXIV171207628; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li Wenjie, 2020, ARXIV200409740; LiyuanLiu HaomingJiang, 2019, INT C LEARN REPR; Loshchilov I., 2017, ARXIV171105101; Luo Liangchen, 2019, INT C LEARN REPR; Lyu Kaifeng, 2019, ARXIV190605890; Ma J., 2018, ARXIV181006801, P1; Ma XL, 2015, TRANSPORT RES C-EMER, V54, P187, DOI 10.1016/j.trc.2015.03.014; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Martens J., 2010, P 27 INT C MACH LEAR, P735; Nesterov Y., 1983, SOV MATH DOKL, V27; NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.1090/S0025-5718-1980-0572855-7; Pascanu Razvan, 2013, ARXIV13013584; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reddi SJ, 2016, PR MACH LEARN RES, V48; Reddi Sashank J, 2019, ARXIV190409237, DOI DOI 10.48550/ARXIV.1904.09237; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; ROSENBROCK HH, 1960, COMPUT J, V3, P175, DOI 10.1093/comjnl/3.3.175; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Toussaint M., 2012, LECT NOTES SOME NOTE; Wang Guanghui, 2019, ARXIV190502957; WEDDERBURN RWM, 1974, BIOMETRIKA, V61, P439, DOI 10.1093/biomet/61.3.439; Wilson AC, 2017, ADV NEUR IN, V30; YANG ZZ, 2019, ADV NEURAL INFORM PR, P5937; You Yang, 2017, ARXIV170803888, V6, P12; Zaheer M, 2018, ADV NEUR IN, V31; Zeiler Matthew D, 2012, ARXIV12125701; Zhou D., 2018, CONVERGENCE ADAPTIVE	58	67	67	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000029
C	Banner, R; Nahshan, Y; Soudry, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Banner, Ron; Nahshan, Yury; Soudry, Daniel			Post training 4-bit quantization of convolutional networks for rapid-deployment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Convolutional neural networks require significant memory bandwidth and storage for intermediate computations, apart from substantial computing resources. Neural network quantization has significant benefits in reducing the amount of intermediate results, but it often requires the full datasets and time-consuming fine tuning to recover the accuracy lost after quantization. This paper introduces the first practical 4-bit post training quantization approach: it does not involve training the quantized model (fine-tuning), nor it requires the availability of the full dataset. We target the quantization of both activations and weights and suggest three complementary methods for minimizing quantization error at the tensor level, two of whom obtain a closed-form analytical solution. Combining these methods, our approach achieves accuracy that is just a few percents less the state-of-the-art baseline across a wide range of convolutional models. The source code to replicate all experiments is available on GitHub: https://github.com/submission2019/cnn-quantization.	[Banner, Ron; Nahshan, Yury] Intel Artificial Intelligence Prod Grp AIPG, Santa Clara, CA 95054 USA; [Soudry, Daniel] Technion Israel Inst Technol, Haifa, Israel	Technion Israel Institute of Technology	Banner, R (corresponding author), Intel Artificial Intelligence Prod Grp AIPG, Santa Clara, CA 95054 USA.	ron.banner@intel.com; yury.nahshan@intel.com; daniel.soudry@gmail.com						Choi Jungwook, 2018, P INT C LEARN REPR I; Choukroun Y., 2019, P IEEE INT C COMP VI; Goncharenko Alexander, 2018, ARXIV181207872; HAN S., 2015, ARXIV151000149; Hubara I, 2016, NIPS; Jacob B., 2017, GEMMLOWP SMALL SELF; Lee J.H., 2018, ABS181005488; Lin X., 2017, ADV NEURAL INFORM PR, P345; McKinstry J. L., 2018, ARXIV180904191; Meller E., 2019, ARXIV190201917; Migacz S., 2017, GPU TECHN C; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Wu S., 2018, ARXIV180204680; Zhao Ritchie, 2019, ARXIV190109504; Zhou Shuchang, 2016, P IEEE C COMP VIS PA	17	67	68	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308002
C	Li, XZ; Sun, QR; Liu, YY; Zheng, SB; Zhou, Q; Chua, TS; Schiele, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Xinzhe; Sun, Qianru; Liu, Yaoyao; Zheng, Shibao; Zhou, Qin; Chua, Tat-Seng; Schiele, Bernt			Learning to Self-Train for Semi-Supervised Few-Shot Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Few-shot classification (FSC) is challenging due to the scarcity of labeled training data (e.g. only one labeled data point per class). Meta-learning has shown to achieve promising results by learning to initialize a classification model for FSC. In this paper we propose a novel semi-supervised meta-learning method called learning to self-train (LST) that leverages unlabeled data and specifically metalearns how to cherry-pick and label such unsupervised data to further improve performance. To this end, we train the LST model through a large number of semi-supervised few-shot tasks. On each task, we train a few-shot model to predict pseudo labels for unlabeled data, and then iterate the self-training steps on labeled and pseudo-labeled data with each step followed by fine-tuning. We additionally learn a soft weighting network (SWN) to optimize the self-training weights of pseudo labels so that better ones can contribute more to gradient descent optimization. We evaluate our LST method on two ImageNet benchmarks for semi-supervised few-shot classification and achieve large improvements over the state-of-the-art method.	[Li, Xinzhe; Zheng, Shibao] Shanghai Jiao Tong Univ, Shanghai, Peoples R China; [Sun, Qianru] Singapore Management Univ, Singapore, Singapore; [Liu, Yaoyao] Tianjin Univ, Tianjin, Peoples R China; [Zhou, Qin] Alibaba Grp, Hangzhou, Peoples R China; [Chua, Tat-Seng] Natl Univ Singapore, Singapore, Singapore; [Schiele, Bernt] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany	Shanghai Jiao Tong University; Singapore Management University; Tianjin University; Alibaba Group; National University of Singapore; Max Planck Society	Zheng, SB (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.; Sun, QR (corresponding author), Singapore Management Univ, Singapore, Singapore.	qianrusun@smu.edu.sg; sbzh@sjtu.edu.cn	Liu, Yaoyao/AAV-1380-2021	Liu, Yaoyao/0000-0002-5316-3028	National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative; German Research Foundation (DFG) [CRC 1223]; National Natural Science Foundation of China [61772359, 61671289, 61771301, 61521062]	National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative(National Research Foundation, Singapore); German Research Foundation (DFG)(German Research Foundation (DFG)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This research is part of NExT research which is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative. It is also partially supported by German Research Foundation (DFG CRC 1223), and National Natural Science Foundation of China (61772359, 61671289, 61771301, 61521062).	Antoniou A., 2019, INT C LEARN REPR, P1; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, ADV NEURAL INFORM PR, P9516; Franceschi L, 2018, PR MACH LEARN RES, V80; Grandvalet Yves, 2004, NIPS, P529; Grant Erin, 2018, INT C LEARN REPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Laine Samuli, 2017, P INT C LEARN REPR I, P3; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee D., 2013, INT C MACH LEARN ICM; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Lee Y, 2018, PR MACH LEARN RES, V80; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Liu YH, 2019, INT J PSYCHIAT CLIN, V23, P164, DOI 10.1080/13651501.2019.1569238; Liu Yaoyao, 2019, 190408479 ARXIV; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mehrotra Akshay, 2017, 170308033 ARXIV; Mishra N., 2018, INT C LEARN REPR, P1; Miyato Takeru, 2016, 160507725 ARXIV; Munkhdalai T, 2018, PR MACH LEARN RES, V80; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Oliver A, 2018, ADV NEUR IN, V31; Olivier C, 2006, SEMISUPERVISED LEARN; Oreshkin BN, 2018, ADV NEUR IN, V31; Ravi S., 2017, INT C LEARN REPR, P12; Ren M., 2018, ICLR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu Andrei A, 2019, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; Schwartz E, 2018, ADV NEUR IN, V31; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Tarvainen Antti, 2017, CORR, Vabs/1703; Triguero I, 2015, KNOWL INF SYST, V42, P245, DOI 10.1007/s10115-013-0706-y; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang YL, 2018, PROC CVPR IEEE, P8906, DOI 10.1109/CVPR.2018.00928; Xian YQ, 2019, PROC CVPR IEEE, P8248, DOI 10.1109/CVPR.2019.00845; Yarowsky David, 1995, ACL, P2, DOI [10.3115/981658.981684, DOI 10.3115/981658.981684]; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhang RX, 2018, ADV NEUR IN, V31	42	67	68	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901086
C	Revaud, J; Weinzaepfel, P; De Souza, C; Humenberger, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Revaud, Jerome; Weinzaepfel, Philippe; De Souza, Cesar; Humenberger, Martin			R2D2: Repeatable and Reliable Detector and Descriptor	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Interest point detection and local feature description are fundamental steps in many computer vision applications. Classical approaches are based on a detect-then-describe paradigm where separate handcrafted methods are used to first identify repeatable keypoints and then represent them with a local descriptor. Neural networks trained with metric learning losses have recently caught up with these techniques, focusing on learning repeatable saliency maps for keypoint detection or learning descriptors at the detected keypoint locations. In this work, we argue that repeatable regions are not necessarily discriminative and can therefore lead to select suboptimal keypoints. Furthermore, we claim that descriptors should be learned only in regions for which matching can be performed with high confidence. We thus propose to jointly learn keypoint detection and description together with a predictor of the local descriptor discriminativeness. This allows to avoid ambiguous areas, thus leading to reliable keypoint detection and description. Our detection-and-description-approach simultaneously outputs sparse, repeatable and reliable keypoints that outperforms state-of-the-art detectors and descriptors on the HPatches dataset and on the recent Aachen Day-Night localization benchmark.	[Revaud, Jerome; Weinzaepfel, Philippe; De Souza, Cesar; Humenberger, Martin] NAVER LABS Europe, Meylan, France		Revaud, J (corresponding author), NAVER LABS Europe, Meylan, France.	jerome.revaud@naverlabs.com; philippe.winzaepfel@naverlabs.com; cesar.desouza@naverlabs.com; martin.humenberger@naverlabs.com		Humenberger, Martin/0000-0003-0600-9164				Balntas V., 2016, ARXIV160105030; Balntas V., 2017, CVPR; Balntas V., 2016, P BRIT MACH VIS C YO; Bay H., 2006, ECCV; Cakir Fatih, 2019, CVPR; Calonder M., 2010, ECCV; Csurka G., 2004, ECCV; Csurka G., 2018, ARXIV180710254; DeTone D., 2018, CVPR; Di Febbo P., 2018, CVPR; Dusmanu Mihai, 2019, CVPR; Fathy M. E., 2018, ECCV; Fua P., 2016, ECCV; Gauglitz S., 2011, IJCV; Georgakis Georgios, 2018, CVPR; Gordo A., 2016, ECCV; Grauman K., 2011, SYNTHESIS LECT ARTIF; HARRIS M, 1988, NEW YORK TIMES BK R, P1; Hartmann W, 2014, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2014.9; He Kun, 2018, CVPR; Heinly J., 2015, CVPR; Kundu Jogendra Nath, 2018, ECCV; Laguna A. B., 2019, ARXIV190400889; Leung T., 2015, CVPR; Leutenegger S., 2011, ICCV; Lowe D. G., 2004, IJCV; LUO Z, 2019, C COMP VIS PATT REC; Matas J., 2004, IMAGE VISION COMPUTI; Mikolajczyk K., 2004, IJCV; Mikolajczyk K., 2005, IJCV; Mishchuk A., 2017, NIPS; Mishkin Dmytro, 2018, ECCV, V1, P2; Noh Hyeonwoo, 2017, CVPR; Novotny David, 2018, CVPR; Ono Yuki, 2018, NIPS; Perdoch M., 2009, CVPR; Pollefeys M., 2018, CVPR; Radenovic F, 2016, ECCV; Radenovic F., 2018, CVPR; Revaud J., 2019, ICCV; Revaud J., 2015, CVPR; Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3; Rosten E., 2005, ICCV; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Salahat E., 2017, ICIT, P2; Sattler T., 2018, CVPR; Sattler T., 2017, CVPR; Sattler T., 2012, BMCV; Savinov N., 2017, CVPR; Savinov N., 2017, NIPS; Schdnberger Johannes Lutz, 2016, ECCV; Schonberger Johannes Lutz, 2016, CVPR; Schroff F., 2015, CVPR; Simo-Serra E., 2015, ICCV; Simonyan K., 2014, IEEE T PAMI; Svdirm L., 2016, IEEE T PAMI; Tian Y., 2019, CVPR; Tian Yurun, 2017, CVPR; Trzcinski T., 2012, NIPS; Tuytelaars Tinne, 2008, FDN TRENDS COMPUTER; Ustinova E., 2016, NIPS; Zhang L., 2018, CVPR; Zieba M., 2018, NIPS	63	67	70	4	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904011
C	Duarte, K; Rawat, YS; Shah, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Duarte, Kevin; Rawat, Yogesh S.; Shah, Mubarak			VideoCapsuleNet: A Simplified Network for Action Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue and make the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves state-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive similar to 20% improvement on UCF-101 and similar to 15% improvement on J-HMDB in terms of v-mAP scores.	[Duarte, Kevin; Rawat, Yogesh S.; Shah, Mubarak] Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32816 USA	State University System of Florida; University of Central Florida	Duarte, K (corresponding author), Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32816 USA.	kevin_duarte@knights.ucf.edu; yogesh@crcv.ucf.edu; shah@crcv.ucf.edu		Shah, Mubarak/0000-0001-6172-5572	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) [D17PC00345]	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA)	This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. D17PC00345. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; He J., 2018, WACV; Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010; Hinton G. E., 2018, INT C LEARN REPR; Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Kalogeiton V., 2017, ICCV; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kingma D.P, P 3 INT C LEARNING R; Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472; Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45; Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727; Sabour Sara, 2017, PROC 31 INT C NEURAL; Saha S., 2016, BMVC, P581; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Singh G., 2017, ONLINE REAL TIME MUL; Soomro K., 2012, ARXIV; Yang Z., 2017, BMVC	21	67	69	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002018
C	Woo, S; Kim, D; Cho, D; Kweon, IS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Woo, Sanghyun; Kim, Dahun; Cho, Donghyeon; Kweon, In So			LinkNet: Relational Embedding for Scene Graph	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Objects and their relationships are critical contents for image understanding. A scene graph provides a structured description that captures these properties of an image. However, reasoning about the relationships between objects is very challenging and only a few recent works have attempted to solve the problem of generating a scene graph from an image. In this paper, we present a method that improves scene graph generation by explicitly modeling inter-dependency among the entire object instances. We design a simple and effective relational embedding module that enables our model to jointly represent connections among all related objects, rather than focus on an object in isolation. Our method significantly benefits main part of the scene graph generation task: relationship classification. Using it on top of a basic Faster R-CNN, our model achieves state-of-the-art results on the Visual Genome benchmark. We further push the performance by introducing global context encoding module and geometrical layout encoding module. We validate our final model, LinkNet, through extensive ablation studies, demonstrating its efficacy in scene graph generation.	[Woo, Sanghyun; Kim, Dahun; Cho, Donghyeon; Kweon, In So] Korea Adv Inst Sci & Technol, EE, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Woo, S (corresponding author), Korea Adv Inst Sci & Technol, EE, Daejeon, South Korea.	shwoo93@kaist.ac.kr; mcahny@kaist.ac.kr; cdh12242@gmail.com; iskweon@kaist.ac.kr	Cho, DongHyeon/AAL-9874-2020; Woo, Sanghyun/AAA-2278-2019		Study on Deep Visual Understanding - Samsung Electronics Co., Ltd (Samsung Research)	Study on Deep Visual Understanding - Samsung Electronics Co., Ltd (Samsung Research)(Samsung)	This research is supported by the Study on Deep Visual Understanding funded by the Samsung Electronics Co., Ltd (Samsung Research)	Battaglia Peter, 2016, P NEUR INF PROC SYST; Chang A., 2014, PROC EMNLP, P2028, DOI [DOI 10.3115/V1/D14-1217, 10.3115/v1/D14-1217.]; Chao Y. -W., 2017, ARXIV170205448; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Dai Bo, 2017, P COMP VIS PATT REC; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Gkioxari Georgia, 2018, P COMP VIS PATT REC; Gu JW, 2006, IEEE T IMAGE PROCESS, V15, P1952, DOI 10.1109/TIP.2006.873443; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Henaff M, 2015, ARXIV150605163; HU R, 2017, P COMP VIS PATT REC; Johnson Justin, 2015, P COMP VIS PATT REC; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Li Y., 2017, P COMP VIS PATT REC; Li Yikang, 2017, P INT C COMP VIS ICC; Liang Xiaodan, 2017, P COMP VIS PATT REC; Lim W., 2017, P INT C LEARN REPR; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Newell Alejandro, 2017, P NEUR INF PROC SYST; Nie Wei-Zhi, 2015, P COMP VIS PATT REC; Niepert M, 2016, PR MACH LEARN RES, V48; Plummer Bryan A, 2017, P COMP VIS PATT REC; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Santoro Adam, 2017, P NEUR INF PROC SYST; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Teney Damien, 2016, CORR, V3; Ulhaq A, 2017, INT CONF IMAG VIS; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Watters Nicholas, 2017, P NEUR INF PROC SYST; Xu Danfei, 2017, P COMP VIS PATT REC; Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121; Zellers Rowan, 2018, P COMP VIS PATT REC; Zhang H., 2017, ICCV; Zhang Hang, 2018, P COMP VIS PATT REC; Zhang Hanwang, 2017, P COMP VIS PATT REC; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444; Zhuang Bohan, 2017, P INT C COMP VIS ICC	41	67	68	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300052
C	Wu, MK; Goodman, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Mike; Goodman, Noah			Multimodal Generative Models for Scalable Weakly-Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider two case studies, one of learning image transformations-edge detection, colorization, segmentation-as a set of modalities, followed by one of machine translation between two languages. We find appealing results across this range of tasks.	[Wu, Mike; Goodman, Noah] Stanford Univ, Dept Comp Sci, Stanford, CA 94025 USA; [Goodman, Noah] Stanford Univ, Dept Psychol, Stanford, CA 94025 USA	Stanford University; Stanford University	Wu, MK (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94025 USA.	wumike@stanford.edu; ngoodman@stanford.edu			NSF GRFP; Google Cloud Platform Education grant; DARPA PPAML through the U.S. AFRL [FA8750-14-2-0006]	NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Cloud Platform Education grant(Google Incorporated); DARPA PPAML through the U.S. AFRL	MW is supported by NSF GRFP and the Google Cloud Platform Education grant. NDG is supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006. We thank Robert X. D. Hawkins and Ben Peloquin for helpful discussions.	Artetxe Mikel, 2017, ARXIV171011041; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Bradski G., 2000, OPENCV DR DOBBS J SO; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Cao Y., 2014, ARXIV14107827; Eslami SM, 2016, NEURIPS, V1; Higgins I, 2016, BETA VAE LEARNING BA; Hinton G. E., 2006, TRAINING, V14; King DE, 2009, J MACH LEARN RES, V10, P1755; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lample Guillaume, 2017, INT C LEARN REPR; Larochelle H., 2011, INT C ART INT STAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Pandey G, 2017, IEEE IJCNN, P308, DOI 10.1109/IJCNN.2017.7965870; Ravi S., 2011, 49 ANN M ASS COMPUTA, P12; Sabour Sara, 2017, PROC 31 INT C NEURAL; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Srivastava Nitish, 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49; Suzuki Masahiro, 2016, ARXIV161101891; van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vedantam R., 2017, ARXIV170510762; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang Weiran, 2016, 161003454 ARXIV; Xiao H., 2017, ARXIV 170807747; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yildirim  Ilker, 2014, PERCEPTION CONCEPTIO; Zhao S., 2017, ARXIV170208658	33	67	67	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000011
C	Zhang, H; Weng, TW; Chen, PY; Hsieh, CJ; Daniel, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Huan; Weng, Tsui-Wei; Chen, Pin-Yu; Hsieh, Cho-Jui; Daniel, Luca			Efficient Neural Network Robustness Certification with General Activation Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a nontrivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.	[Zhang, Huan; Hsieh, Cho-Jui] Univ Calif Los Angeles, Los Angeles, CA 90095 USA; [Weng, Tsui-Wei; Daniel, Luca] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Chen, Pin-Yu] IBM Res, MIT IBM Watson AI Lab, Yorktown Hts, NY 10598 USA	University of California System; University of California Los Angeles; Massachusetts Institute of Technology (MIT); International Business Machines (IBM)	Zhang, H (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.; Weng, TW (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	huan@huan-zhang.com; twweng@mit.edu; pin-yu.chen@ibm.com; chohsieh@cs.ucla.edu; dluca@mit.edu	Chen, Pin-Yu/AAA-1059-2020		NSF [IIS-1719097]; Intel faculty award; Google Cloud Credits for Research Program; MIT-IBM Watson AI Lab; MIT-Skoltech program	NSF(National Science Foundation (NSF)); Intel faculty award; Google Cloud Credits for Research Program(Google Incorporated); MIT-IBM Watson AI Lab; MIT-Skoltech program	This work was supported in part by NSF IIS-1719097, Intel faculty award, Google Cloud Credits for Research Program and GPUs donated by NVIDIA. Tsui-Wei Weng and Luca Daniel are partially supported by MIT-IBM Watson AI Lab and MIT-Skoltech program.	Athalye Anish, 2017, ARXIV PREPRINT ARXIV; Biggio B., 2017, ARXIV171203141; Brendel Wieland, 2018, ICLR; Cao XY, 2017, ANN COMPUT SECURITY, P278, DOI 10.1145/3134600.3134606; Carlini N., 2017, ARXIV170910207; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen P.-Y., 2018, AAAI; Dvijotham K., 2018, UAI; Ehlers R, 2017, LECT NOTES COMPUT SC, V10482, P269, DOI 10.1007/978-3-319-68167-2_19; Evtimov I., 2017, ARXIV PREPRINT ARXIV, V2, P4; Fawzi A, 2017, IEEE SIGNAL PROC MAG, V34, P50, DOI 10.1109/MSP.2017.2740965; Fischetti Matteo, 2017, ARXIV171206174; Gehr T., 2018, 2018 IEEE S SEC PRIV, P948; Goodfellow I. J., 2015, ICLR; Hein M., 2017, NIPS; Hsieh CJ, 2017, ARXIV171202051; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Kolter J. Z., 2018, ICML; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Liu Yanpei, 2017, ICLR; Lomuscio A., 2017, CORR; Madry Aleksander, 2018, ICLR; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Peck J., 2017, NIPS; Raghunathan A., 2018, INT C LEARN REPR; Sinha Aman, 2018, ICLR; Szegedy C, 2013, 2 INT C LEARNING REP; Weng Thui-Wei, 2018, INT C LEARN REPR; Weng Tsui-Wei, 2018, ICML	32	67	70	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304091
C	Nemenman, I; Shafee, F; Bialek, W		Dietterich, TG; Becker, S; Ghahramani, Z		Nemenman, I; Shafee, F; Bialek, W			Entropy and inference, revisited	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				PROBABILITY-DISTRIBUTIONS; INFORMATION	We study proper-ties of popular near-uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam-style phase space argument expands the priors into their infinite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Nemenman, I (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.	nemenman@itp.ucsb.edu; fshqfee@princeton.edu; wbialek@princeton.edu						Balasubramanian V, 1997, NEURAL COMPUT, V9, P349, DOI 10.1162/neco.1997.9.2.349; Bialek W, 2001, NEURAL COMPUT, V13, P2409, DOI 10.1162/089976601753195969; Bialek W, 1996, PHYS REV LETT, V77, P4693, DOI 10.1103/PhysRevLett.77.4693; DELAPLACE PS, 1814, ESSAI PHILOS PROBABI; Hardy G.F., 1889, INSURANCE RECORD; JEFFREYS H, 1946, PROC R SOC LON SER-A, V186, P453, DOI 10.1098/rspa.1946.0056; KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331; LIDSTONE GJ, 1920, T FACULTY ACTUARIES, V8, P182; MA S, 1981, J STAT PHYS, V26, P221, DOI 10.1007/BF01013169; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Nemenman I, 2001, ADV NEUR IN, V13, P287; NEMENMAN I, 2000, THESIS PRINCETON, pCH3; Panzeri S, 1996, NETWORK-COMP NEURAL, V7, P87, DOI [10.1088/0954-898X/7/1/006, 10.1080/0954898X.1996.11978656]; Schurmann T, 1996, CHAOS, V6, P414, DOI 10.1063/1.166191; Sjolander K, 1996, COMPUT APPL BIOSCI, V12, P327; SKILLING J, 1989, FUND THEOR, V36, P45; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012; WOLPERT DH, 1995, PHYS REV E, V52, P6841, DOI 10.1103/PhysRevE.52.6841	19	67	67	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						471	478						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100059
C	Seeger, M		Solla, SA; Leen, TK; Muller, KR		Seeger, M			Bayesian model selection for Support Vector machines, Gaussian processes and other kernel classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We present a variational Bayesian method for model selection over families of kernels classifiers like Support Vector machines or Gaussian processes. The algorithm needs no user interaction and is able to adapt a large number of kernel parameters to given data without having to sacrifice training cases for validation. This opens the possibility to use sophisticated families of kernels in situations where the small "standard kernel" classes are clearly inappropriate. We relate the method to other work done on Gaussian processes and clarify the relation between Support Vector machines and certain Gaussian process models.	Univ Edinburgh, Inst Adapt & Neural Comp, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Seeger, M (corresponding author), Univ Edinburgh, Inst Adapt & Neural Comp, 5 Forrest Hill, Edinburgh EH1 2QL, Midlothian, Scotland.	seeger@dai.ed.ac.uk						Barber D, 1998, ADV NEUR IN, V10, P395; GIBBS MN, 1997, THESIS U CAMBRIDGE; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; JAAKKOLA T, 1999, ADV NIPS, V13; JAAKKOLA TS, 1998, ADV NIPS, V11; KWOK JTT, 1999, UNPUB ESANN 99; Neal, 1997, PHYSICS9701026 ARXIV; Opper Manfred, 1999, ADV LARGE MARGIN CLA; SEEGER M, 1999, THESIS U KARLSRUHE G; Skilling J., 1988, MAXIMUM ENTROPY BAYE; SOLLICH P, 1999, ADV NIPS, V13; Vapnik V.N, 1998, STAT LEARNING THEORY; Wahba G., 1990, SPLINE MODELS OBSERV; WAHBA G, 1997, 984 U WISC; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; WILLIAMS CKI, 1997, LEARNING GRAPHICAL M	16	67	68	0	4	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						603	609						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700086
C	Lee, DD; Seung, HS		Mozer, MC; Jordan, MI; Petsche, T		Lee, DD; Seung, HS			Unsupervised learning by convex and conic coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Unsupervised learning algorithms based on convex and conic encoders are proposed. The encoders find the closest convex or conic combination of basis vectors to the input. The learning algorithms produce basis vectors that minimize the reconstruction error of the encoders. The convex algorithm develops locally linear models of the input, while the conic algorithm discovers features. Both algorithms are used to model handwritten digits and compared with vector quantization and principal component analysis. The neural network implementations involve feedback connections that project a reconstruction back to the input layer.			Lee, DD (corresponding author), AT&T BELL LABS,LUCENT TECHNOL,MURRAY HILL,NJ 07974, USA.		Lee, Daniel D./B-5753-2013	Lee, Daniel/0000-0003-4239-8777					0	67	70	1	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						515	521						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00073
C	Zhang, S; Tay, Y; Yao, L; Liu, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Shuai; Tay, Yi; Yao, Lina; Liu, Qi			Quaternion Knowledge Graph Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this work, we move beyond the traditional complex-valued representations, introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. More specifically, quaternion embeddings, hypercomplex-valued embeddings with three imaginary components, are utilized to represent entities. Relations are modelled as rotations in the quaternion space. The advantages of the proposed approach are: (1) Latent inter-dependencies (between all components) are aptly captured with Hamilton product, encouraging a more compact interaction between entities and relations; (2) Quaternions enable expressive rotation in four-dimensional space and have more degree of freedom than rotation in complex plane; (3) The proposed framework is a generalization of ComplEx on hypercomplex space while offering better geometrical interpretations, concurrently satisfying the key desiderata of relational representation learning (i.e., modeling symmetry, anti-symmetry and inversion). Experimental results demonstrate that our method achieves state-of-the-art performance on four well-established knowledge graph completion benchmarks.	[Zhang, Shuai; Yao, Lina] Univ New South Wales, Sydney, NSW, Australia; [Tay, Yi] Nanyang Technol Univ, Singapore, Singapore; [Liu, Qi] Univ Oxford, Oxford, England	University of New South Wales Sydney; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Oxford	Zhang, S (corresponding author), Univ New South Wales, Sydney, NSW, Australia.		Zhang, Shuai/AAI-2337-2019	Zhang, Shuai/0000-0002-7866-4611	ONRG NICOP [N62909-19-1-2009]	ONRG NICOP	This research was partially supported by grant ONRG NICOP N62909-19-1-2009	Bloem P., 2018, P 15 EUR SEM WEB C E, P593, DOI [10.1007/978-3-319-93417-4_38, DOI 10.1007/978-3-319-93417-4_38]; Bordes A., 2013, ADV NEURAL INFORM PR; Dettmers T, 2018, AAAI CONF ARTIF INTE, P1811; Dong XL, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P601, DOI 10.1145/2623330.2623623; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Ebisu T, 2018, AAAI CONF ARTIF INTE, P1819; Gaudet C. J., 2018, P 2018 INT JOINT C N, P1; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hamilton William Rowan, 1844, PHILOS MAG, V25, P489, DOI DOI 10.1080/14786444408645047; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Ji GL, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P687; Kadlec Rudolf, 2017, P 2 WORKSH REPR LEAR, P69, DOI [DOI 10.18653/V1/W17-2609, 10.18653/v1/W17-2609]; Kai Wang, 2018, ARXIV180803752; Kazemi S. M., 2018, NEURAL INFORM PROCES, P4289; Lacroix T, 2018, PR MACH LEARN RES, V80; Lao Ni, 2011, P C EMP METH NAT LAN, P529, DOI DOI 10.5555/2145432.2145494; Lin Y., 2015, 29 AAAI C ART INT; Neelakantan A., 2015, ARXIV150406662; Nguyen D. Q., 2018, P 2018 C N AM ASS CO, V2, P327, DOI [10.18653/v1/N18-2053, DOI 10.18653/V1/N18-2053]; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M, 2016, AAAI CONF ARTIF INTE, P1955; Parcollet T, 2017, INTERSPEECH, P3325, DOI 10.21437/Interspeech.2017-1029; Parcollet T, 2016, IEEE W SP LANG TECH, P362, DOI 10.1109/SLT.2016.7846290; Parcollet Titouan, 2018, INT C LEARN REPR; Parcollet Titouan, 2018, CORR; Parcollet Titouan, ARXIV180607789; Socher R., 2013, ADV NEURAL INFORM PR, P926, DOI DOI 10.1109/ICICIP.2013.6568119; Sun Zhiqing, 2019, 7 INT C LEARN REPR; Toutanova K., 2015, P 3 WORKSH CONT VECT, DOI DOI 10.18653/V1/W15-4007; Trouillon T, 2017, P INT WORKSH STAT RE; Trouillon T, 2016, PR MACH LEARN RES, V48; Yang B., 2015, P INT C LEARN REPR; Zhen W., 2014, 28 AAAI C ART INT	33	66	68	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302070
C	Kim, B; Khanna, R; Koyejo, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kim, Been; Khanna, Rajiv; Koyejo, Oluwasanmi			Examples are not Enough, Learn to Criticize! Criticism for Interpretability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.	[Kim, Been] Allen Inst AI, Seattle, WA 98103 USA; [Khanna, Rajiv] UT Austin, Austin, TX USA; [Koyejo, Oluwasanmi] UIUC, Champaign, IL USA	University of Texas System; University of Texas Austin; University of Illinois System; University of Illinois Urbana-Champaign	Kim, B (corresponding author), Allen Inst AI, Seattle, WA 98103 USA.	beenkim@csail.mit.edu; rajivak@utexas.edu; sanmi@illinois.edu	Khanna, Rajiv/GPK-2566-2022	Khanna, Rajiv/0000-0003-1314-3126				Aamodt Agnar, 1994, AI COMMUNICATIONS; Badanidiyuru A., 2014, KDD; Bichindaritz I., 2006, AI IN MED; Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495; Caruana R., 2015, KDD; Cohen M.S., 1996, HUMAN FACTORS; Deng J., 2009, CVPR; Feige U, 1998, JACM; Gelman A., 2021, BAYESIAN DATA ANAL; Gretton A., 2008, JMLR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hull J. J., 1994, TPAMI; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Kaufman L., 1987, CLUSTERING MEANS MED; Kim B., 2014, NIPS; Koyejo O, 2014, CONSISTENT BINARY CL; Krause A., 2008, JMLR; Kuncheva LI, 1998, IEEE T SYST MAN CY C, V28, P160, DOI 10.1109/5326.661099; Lin H., 2011, ACL; Lloyd James R, 2015, NIPS; Mirzasoleiman B., 2015, NIPS; Nemhauser G. L., 1978, MATH PROGRAMMING; Newell A, 1972, HUMAN PROBLEM SOLVIN; Priebe C. E., 2003, J CLASSIFICATION; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Sharma D., 2015, ICML; Simon I., 2007, ICCV; Varshney K.R., 2016, ARXIV160104126	28	66	66	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702047
C	Norouzi, M; Bengio, S; Chen, ZF; Jaitly, N; Schuster, M; Wu, YH; Schuurmans, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Norouzi, Mohammad; Bengio, Samy; Chen, Zhifeng; Jaitly, Navdeep; Schuster, Mike; Wu, Yonghui; Schuurmans, Dale			Reward Augmented Maximum Likelihood for Neural Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. By establishing a link between the log-likelihood and expected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated scaled rewards. Accordingly, we present a framework to smooth the predictive probability of the outputs using their corresponding rewards. We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards. Experiments on neural sequence to sequence models for speech recognition and machine translation show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.	[Norouzi, Mohammad; Bengio, Samy; Chen, Zhifeng; Jaitly, Navdeep; Schuster, Mike; Wu, Yonghui; Schuurmans, Dale] Google Brain, Mountain View, CA 94039 USA	Google Incorporated	Norouzi, M (corresponding author), Google Brain, Mountain View, CA 94039 USA.	mnorouzi@google.com; bengio@google.com; zhifengc@google.com; ndjaitly@google.com; schuster@google.com; yonghu@google.com; schuurmans@google.com						Andor Daniel, 2016, ARXIV160306042; Banerjee Arindam, 2005, JMLR; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Chorowski J., 2014, P NIPS WORKSH DEEP L; Daume H., 2009, MACH LEARN J; Degris T., 2012, ACC; Domke Justin, 2012, INT C ARTIFICIAL INT; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gimpel Kevin, 2010, HUMAN LANGUAGE TECHN, P733; Hinton G., 2015, ARXIV150302531; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kappen H. J., 2012, MACH LEARN J; Kim B., 2013, P ADV NEUR INF PROC, P2859; Kumar A, 2016, PR MACH LEARN RES, V48; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Levine S., 2013, NIPS; Levine Sergey, 2013, ICML; Lopez-Paz David, 2016, INT C LEARN REPR ICL, DOI DOI 10.1109/PAC.2017.13.ARXIV:1511.0; Luong M., 2015, ARXIV150804025; Luong M.-T., 2015, ACL; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Povey D., 2011, IEEE 2011 WORKSH AUT; Ranzato M, 2016, ICLR; Shen SQ, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1683; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Stoyanov Veselin, 2011, P AISTATS; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Taskar B., 2004, MAX MARGIN MARKOV NE; Todorov E., 2006, LINEARLY SOLVABLE MA; Van Hasselt H., 2015, ARXIV150906461; Vlassis N., 2009, AUTONOMOUS ROBOTS; Volkovs M., 2011, ARXIV11071805V1; Williams R. J., 1992, MACH LEARN J; Wiseman Sam, 2016, ARXIV160602960	39	66	66	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704071
C	Xin, B; Wang, YZ; Gao, W; Wang, BY; Wipf, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xin, Bo; Wang, Yizhou; Gao, Wen; Wang, Baoyuan; Wipf, David			Maximal Sparsity with Deep Networks?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SIGNAL RECONSTRUCTION	The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal l(0)-norm representations in regimes where existing methods fail. The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.	[Xin, Bo; Wang, Yizhou; Gao, Wen] Peking Univ, Beijing, Peoples R China; [Xin, Bo; Wipf, David] Microsoft Res, Beijing, Peoples R China; [Wang, Baoyuan] Microsoft Res, Redmond, WA USA	Peking University; Microsoft; Microsoft	Xin, B (corresponding author), Peking Univ, Beijing, Peoples R China.; Xin, B (corresponding author), Microsoft Res, Beijing, Peoples R China.	boxin@microsoft.com; yizhou.wang@pku.edu.cn; wgao@pku.edu.cn; baoyuanw@microsoft.com; davidwip@microsoft.com			MOEMicrosoft Key Laboratory, Peking University;  [973-2015CB351800];  [NSFC-61231010];  [NSFC-61527804];  [NSFC-61421062];  [NSFC-61210005]	MOEMicrosoft Key Laboratory, Peking University; ; ; ; ; 	This work was done while the first author was an intern at Microsoft Research, Beijing. It is also funded by 973-2015CB351800, NSFC-61231010, NSFC-61527804, NSFC-61421062, NSFC-61210005 and MOEMicrosoft Key Laboratory, Peking University.	Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Blumensath T., 2009, APPL COMPUTATIONAL H, V27; Blumensath T, 2010, IEEE J-STSP, V4, P298, DOI 10.1109/JSTSP.2010.2042411; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Cotter S. F., 2002, IEEE T COMMUNICATION, V50; Figueiredo M. A. T., 2002, NIPS; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hershey J.R., 2014, ARXIV14092574V4; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ikehata S, 2012, PROC CVPR IEEE, P318, DOI 10.1109/CVPR.2012.6247691; Kamilov U., 2015, ARXIV151204754; Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Nair V, 2010, P 27 INT C MACHINE L, P807; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Wang Z., 2015, ARXIV150900153V2; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu L., 2010, P AS C COMP VIS; Xin B, 2016, ADV NEUR IN, V29	26	66	66	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702093
C	Rasmus, A; Valpola, H; Honkala, M; Berglund, M; Raiko, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rasmus, Antti; Valpola, Harri; Honkala, Mikko; Berglund, Mathias; Raiko, Tapani			Semi-Supervised Learning with Ladder Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NEURAL-NETWORKS	We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.	[Rasmus, Antti; Valpola, Harri] Curious AI Co, Helsinki, Finland; [Honkala, Mikko] Nokia Labs, Espoo, Finland; Aalto Univ, Helsinki, Finland	Nokia Corporation; Nokia Finland; Aalto University	Rasmus, A (corresponding author), Curious AI Co, Helsinki, Finland.				Academy of Finland	Academy of Finland(Academy of Finland)	We have received comments and help from a number of colleagues who would all deserve to be mentioned but we wish to thank especially Yann LeCun, Diederik Kingma, Aaron Courville, Ian Goodfellow, Soren Sonderby, Jim Fan and Hugo Larochelle for their helpful comments and suggestions. The software for the simulations for this paper was based on Theano [32] and Blocks [33]. We also acknowledge the computational resources provided by the Aalto Science-IT project. The Academy of Finland has supported Tapani Raiko.	Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y., 2013, P 26 INT C NEUR INF, P899; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Goodfellow I, 2012, P INT C MACH LEARN, P1439; Goodfellow Ian J., 2015, INT C LEARNING REPRE; Goodfellow Ian J., 2013, P ICML 2013; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kingma DP, 2015, INT C LEARN REPR ICL; Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896; MCLACHLAN GJ, 1975, J AM STAT ASSOC, V70, P365; Miyato T., 2015, ARXIV PREPRINT ARXIV; Pitelis Nikolaos, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P565, DOI 10.1007/978-3-662-44851-9_36; Ranzato M., 2008, P 25 INT C MACHINE L, P792, DOI DOI 10.1145/1390156.1390256; Rasmus Antti, 2015, ARXIV150702672; Rasmus Antti, 2015, ARXIV14127210; Rifai S., 2011, P ADV NEUR INF PROC; Springenberg Jost Tobias, 2014, ARXIV14126806; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Suddarth Steven C, 1990, P 1990 EURASIP WORKS, P120, DOI [10.1007/3-540-52255-7_33, DOI 10.1007/3-540-52255-7_33]; Szummer M, 2002, ADV NEUR IN, V14, P945; Titterington D., 1985, STAT ANAL FINITE MIX; Valpola H., 2015, ADV INDEPENDENT COMP, P143, DOI [10.1016/B978-0-12-802806-3.00008-7, DOI 10.1016/B978-0-12-802806-3.00008-7]; van Merrienboer B, 2015, ABS150600619 CORR; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474; Zhao J., 2015, ARXIV150602351	32	66	66	2	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102095
