PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Garg, VK; Pichkhadze, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Garg, Vikas K.; Pichkhadze, Tamar			Online Markov Decoding: Lower Bounds and Near-Optimal Approximation Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS	We resolve the fundamental problem of online decoding with general nth order ergodic Markov chain models. Specifically, we provide deterministic and randomized algorithms whose performance is close to that of the optimal offline algorithm even when latency is small. Our algorithms admit efficient implementation via dynamic programs, and readily extend to (adversarial) non-stationary or time-varying settings. We also establish lower bounds for online methods under latency constraints in both deterministic and randomized settings, and show that no online algorithm can perform significantly better than our algorithms. To our knowledge, our work is the first to analyze general Markov chain decoding under hard constraints on latency. We provide strong empirical evidence to illustrate the potential impact of our work in applications such as gene sequencing.	[Garg, Vikas K.; Pichkhadze, Tamar] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Garg, VK (corresponding author), MIT, Cambridge, MA 02139 USA.	vgarg@csail.mit.edu; tamarp@alum.mit.edu						Altun  Yasemin, 2003, INT C MACH LEARN ICM; Backurs Arturs, 2017, P 34 INT C MACH LEAR, V70, P311; Bengio Samy, 2003, ADV NEURAL INFORM PR, P1237; Bloit J, 2008, INT CONF ACOUST SPEE, P2121, DOI 10.1109/ICASSP.2008.4518061; Bulla J, 2006, COMPUT STAT DATA AN, V51, P2192, DOI 10.1016/j.csda.2006.07.021; Cairo M, 2016, AAAI CONF ARTIF INTE, P1484; Chen BJ, 2011, BIOMETRICAL J, V53, P444, DOI 10.1002/bimj.201000122; Chu W., 2004, INT C MACH LEARN ICM; Churbanov A, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-224; Daume H., 2005, INT C MACHINE LEARNI, P169, DOI [10.1145/1102351.1102373, DOI 10.1145/1102351.1102373]; DeJesus MA, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-303; Felzenszwalb P. F., 2003, P NIPS, P409; Garg V. K., 2018, NEURAL INFORM PROCES; Goh C. Y., 2012, IEEE C INT TRANSP SY; Gotoh O, 2018, BIOINFORMATICS, V34, P3258, DOI 10.1093/bioinformatics/bty353; Gupta C, 2017, PR MACH LEARN RES, V70; Heafield Kenneth, 2013, P 51 ANN M ASS COMP, P690; Jayram T. S., 2001, P 33 ACM S THEOR COM, P540; Kaji N, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P485; Kumar A, 2017, PR MACH LEARN RES, V70; Lifshits Y, 2009, ALGORITHMICA, V54, P379, DOI 10.1007/s00453-007-9128-0; McCallum A., 2000, INT C MACH LEARN ICM; Narasimhan M., 2006, P 23 INT C MACH LEAR, P657, DOI DOI 10.1145/1143844.1143927; Ocana-Riola R, 2005, BIOMETRICAL J, V47, P369, DOI 10.1002/bimj.200310114; Peng J, 2009, 2009 ISECS INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT, VOL I, P141, DOI 10.1109/CCCM.2009.5268129; Perez-Ocon R, 2001, J ROY STAT SOC C-APP, V50, P111, DOI 10.1111/1467-9876.00223; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Sramek R, 2007, LECT N BIOINFORMAT, V4645, P240; Taskar B, 2004, ADV NEUR IN, V16, P25; Thiagarajan A., 2011, NSDI; Tsochantaridis I., 2004, INT C MACH LEARN ICM; VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010; Wang D, 2006, I C WIREL COMM NETW, P1521; Zhu Pengkai, 2019, 22 INT C ART INT STA, P2770	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305065
C	Gautier, G; Bardenet, R; Valko, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gautier, Guillaume; Bardenet, Remi; Valko, Michal			On two ways to use determinantal point processes for Monte Carlo integration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					When approximating an integral by a weighted sum of function evaluations, determinantal point processes (DPPs) provide a way to enforce repulsion between the evaluation points. This negative dependence is encoded by a kernel. Fifteen years before the discovery of DPPs, Ermakov & Zolotukhin (EZ, 1960) had the intuition of sampling a DPP and solving a linear system to compute an unbiased Monte Carlo estimator of the integral. In the absence of DPP machinery to derive an efficient sampler and analyze their estimator, the idea of Monte Carlo integration with DPPs was stored in the cellar of numerical integration. Recently, Bardenet & Hardy (BH, 2019) came up with a more natural estimator with a fast central limit theorem (CLT). In this paper, we first take the EZ estimator out of the cellar, and analyze it using modern arguments. Second, we provide an efficient implementation(1) to sample exactly a particular multidimensional DPP called multivariate Jacobi ensemble. The latter satisfies the assumptions of the aforementioned CLT. Third, our new implementation lets us investigate the behavior of the two unbiased Monte Carlo estimators in yet unexplored regimes. We demonstrate experimentally good properties when the kernel is adapted to basis of functions in which the integrand is sparse or has fast-decaying coefficients. If such a basis and the level of sparsity are known (e.g., we integrate a linear combination of kernel eigenfunctions), the EZ estimator can be the right choice, but otherwise it can display an erratic behavior.	[Gautier, Guillaume; Bardenet, Remi; Valko, Michal] Univ Lille, CNRS, UMR 9189, Cent Lille,CRIStAL, F-59651 Villeneuve Dascq, France; [Gautier, Guillaume; Valko, Michal] Inria Lille Nord Europe, 40 Ave Halley, F-59650 Villeneuve Dascq, France; [Valko, Michal] DeepMind Paris, 14 Rue Londres, F-75009 Paris, France	Centre National de la Recherche Scientifique (CNRS); Universite de Lille - ISITE; Centrale Lille; Universite de Lille	Gautier, G (corresponding author), Univ Lille, CNRS, UMR 9189, Cent Lille,CRIStAL, F-59651 Villeneuve Dascq, France.; Gautier, G (corresponding author), Inria Lille Nord Europe, 40 Ave Halley, F-59650 Villeneuve Dascq, France.	g.gautier@inria.fr; remi.bardenet@gmail.com; valkom@deepmind.com						Bach F., 2012, P 29 INT C INT C MAC, P1355; Bach F., 2017, J MACHINE LEARNING R; Bardenet R., 2019, ANN APPL PROBABILITY; Briol FX, 2015, ADV NEUR IN, V28; Chen Y, 2010, CONFERENCE ON UNCERT; Chow Y, 1994, P AM MATH SOC; Davis P.J., 1984, METHODS NUMERICAL IN; Delyon B., 2016, BERNOULLI; Dick J., 2010, DIGITAL NETS SEQUENC, DOI 10.1017/CBO9780511761188; Ermakov S. M, 1960, THEORY PROBABILITY I; Evans M., 2000, APPROXIMATING INTEGR; Gautier G., 2019, J MACHINE LEARNING R; Gautschi W, 2009, ELECTRON T NUMER ANA; Hough J. B, 2006, PROBAB SURV; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Johansson K, 2006, HOUCHES SUMMER SCH P; Killip R, 2004, INT MATH RES NOTICES; Konig W, 2004, PROBAB SURV; Kulesza A., 2012, ARXIV12076083; Lavancier F, 2012, J ROYAL STAT SOC B; Liu Q., 2017, INT C ART INT STAT A; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mallat S, 2009, WAVELET TOUR OF SIGNAL PROCESSING: THE SPARSE WAY, P1; Mazoyer A, 2019, ARXIV190102099V3; O'Hagan A, 1991, J STAT PLANNING INFE; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Robert C., 2007, BAYESIAN CHOICE DECI; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Simon B, 2011, MB PORTER LECT SERIE; Soshnikov A, 2000, RUSSIAN MATH SURVEYS	31	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307075
C	Ge, SF; Wang, SJ; Teh, YW; Wang, LL; Elliott, LT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ge, Shufei; Wang, Shijia; Teh, Yee Whye; Wang, Liangliang; Elliott, Lloyd T.			Random Tessellation Forests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Space partitioning methods such as random forests and the Mondrian process are powerful machine learning methods for multi-dimensional and relational data, and are based on recursively cutting a domain. The flexibility of these methods is often limited by the requirement that the cuts be axis aligned. The Ostomachion process and the self-consistent binary space partitioning-tree process were recently introduced as generalizations of the Mondrian process for space partitioning with non-axis aligned cuts in the two dimensional plane. Motivated by the need for a multi-dimensional partitioning tree with non-axis aligned cuts, we propose the Random Tessellation Process (RTP), a framework that includes the Mondrian process and the binary space partitioning-tree process as special cases. We derive a sequential Monte Carlo algorithm for inference, and provide random forest methods. Our process is self-consistent and can relax axis-aligned constraints, allowing complex inter-dimensional dependence to be captured. We present a simulation study, and analyse gene expression data of brain tissue, showing improved accuracies over other methods.	[Ge, Shufei; Wang, Shijia; Wang, Liangliang; Elliott, Lloyd T.] Simon Fraser Univ, Dept Stat & Actuarial Sci, Burnaby, BC, Canada; [Wang, Shijia] Nankai Univ, Sch Stat & Data Sci, LPMC, Tianjin, Peoples R China; [Wang, Shijia] Nankai Univ, KLMDASR, Tianjin, Peoples R China; [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England	Simon Fraser University; Nankai University; Nankai University; University of Oxford	Ge, SF (corresponding author), Simon Fraser Univ, Dept Stat & Actuarial Sci, Burnaby, BC, Canada.	shufei_ge@sfu.ca; shijia_wang@sfu.ca; y.w.teh@stats.ox.ac.uk; liangliang_wang@sfu.ca; lloyd_elliott@sfu.ca		Wang, Shijia/0000-0003-0339-1716; Ge, Shufei/0000-0002-9395-3884	European Research Council under the European Union [617071]; NSERC [RGPIN/05484-2019, DGECR/00118-2019, RGPIN/06131-2019]	European Research Council under the European Union(European Research Council (ERC)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	We are grateful to Kevin Sharp, Frauke Harms, Maasa Kawamura, Ruth Van Gurp, Lars Buesing, Tom Loughin and Hugh Chipman for helpful discussion, comments and inspiration. We would also like to thank Fred Popowich and Martin Siegert for help with computational resources at Simon Fraser University. YWT's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071. This research was also funded by NSERC grant numbers RGPIN/05484-2019, DGECR/00118-2019 and RGPIN/06131-2019.	Barnes M. R., 2011, J NEUROSCIENCE RES, V89; Berger M. A., 2012, INTRO PROBABILITY ST; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285; Chiu S.N., 2013, WILEY SERIES PROBABI; Chopin N, 2004, ANN STAT, V32, P2385, DOI 10.1214/009053604000000698; Doucet A., 2000, STAT COMPUT, V10, P3; Fan X., 2018, P 35 INT C ART INT S; Fan X., 2019, 190309348 ARXIV; Fan X., 2016, P 13 C ASS ADV ART I; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Freije W. A., 2004, CANC RES, V64; GEORGE EI, 1987, J APPL PROBAB, V24, P557, DOI 10.2307/3214089; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Godlovitch D., 2011, THESIS; Hajiaghayi M., 2014, P 31 INT C MACH LEAR; Halmos P.R., 1974, MEASURE THEORY; Kemp C., 2006, P 20 C ASS ADV ART I; Lakshminarayanan B., 2015, P 18 INT C ART INT S; Lakshminarayanan B., 2014, P 28 C NEUR INF PROC; Maycox P. R., 2009, MOL PSYCHIAT, V14; Meyer D., 2019, E1071 TU WIEN; Mourtada Jaouad, 2018, 180305784 ARXIV; Nagel W., 2005, ADV APPL PROBABILITY, V37; Olshen R., 1984, CLASSIFICATION REGRE; Rainforth T., 2015, 150705444 ARXIV; Roy D. M., 2008, P 22 C NEUR INF PROC; Tomita T. M., 2015, 150603410 ARXIV; [No title captured]	30	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901023
C	Ghorbani, B; Mei, S; Misiakiewicz, T; Montanari, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ghorbani, Behrooz; Mei, Song; Misiakiewicz, Theodor; Montanari, Andrea			Limitations of Lazy Training of Two-layers Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the supervised learning problem under either of the following two models: (1) Feature vectors xi are d-dimensional Gaussians and responses are y(i) = f(*) (x(i)) for f(*) an unknown quadratic function; (2) Feature vectors xi are distributed as a mixture of two d-dimensional centered Gaussians, and y(i)'s are the corresponding class labels. We use two-layers neural networks with quadratic activations, and compare three different learning regimes: the random features (RF) regime in which we only train the second-layer weights; the neural tangent (NT) regime in which we train a linearization of the neural network around its initialization; the fully trained neural network (NN) regime in which we train all the weights in the network. We prove that, even for the simple quadratic model of point (1), there is a potentially unbounded gap between the prediction risk achieved in these three training regimes, when the number of neurons is smaller than the ambient dimension. When the number of neurons is larger than the number of dimensions, the problem is significantly easier and both NT and NN learning achieve zero risk.	[Ghorbani, Behrooz; Montanari, Andrea] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Mei, Song] Stanford Univ, ICME, Stanford, CA 94305 USA; [Misiakiewicz, Theodor; Montanari, Andrea] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University	Ghorbani, B (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	ghorbani@stanford.edu; songmei@stanford.edu; misiakie@stanford.edu; montanar@stanford.edu	Mei, Song/AFQ-2667-2022		NSF [DMS-1613091, CCF-1714305, IIS-1741162, DMS-1418362, DMS-1407813]; ONR [N00014-18-1-2729]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This work was partially supported by grants NSF DMS-1613091, CCF-1714305, IIS-1741162, and ONR N00014-18-1-2729, NSF DMS-1418362, NSF DMS-1407813.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; [Anonymous], 2018, ARXIV180301206; [Anonymous], ARXIV181103962; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; Bach Francis., 2017, J MACHINE LEARNING R, V18, P629; Bach Francis, 2013, C LEARNING THEORY, P185; Bach Francis R., 2017, J MACHINE LEARNING R, V18, P714; Chizat L., 2018, NOTE LAZY TRAINING S; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Ge R, 2017, PR MACH LEARN RES, V70; Ge Rong, 2017, ARXIV171100501; Ghorbani B., 2019, ARXIV190412191; Haeffele BD, 2014, PR MACH LEARN RES, V32, P2007; Hastie Trevor, 2019, ARXIV190308560; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Klivans A., 2014, APPROXIMATION RANDOM; Lee J., 2019, ARXIV190206720; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3215; Soltanolkotabi M, 2019, IEEE T INFORM THEORY, V65, P742, DOI 10.1109/TIT.2018.2854560; Yehudai Gilad, 2019, ARXIV190400687; Zhong K, 2017, PR MACH LEARN RES, V70; Zou D, 2018, ARXIV181108888	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900067
C	Ghosh, N; Chen, YX; Yue, YS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ghosh, Nikhil; Chen, Yuxin; Yue, Yisong			Landmark Ordinal Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we aim to learn a low-dimensional Euclidean representation from a set of constraints of the form "item j is closer to item i than item k". Existing approaches for this "ordinal embedding" problem require expensive optimization procedures, which cannot scale to handle increasingly larger datasets. To address this issue, we propose a landmark-based strategy, which we call Landmark Ordinal Embedding (LOE). Our approach trades off statistical efficiency for computational efficiency by exploiting the low-dimensionality of the latent embedding. We derive bounds establishing the statistical consistency of LOE under the popular Bradley-Terry-Luce noise model. Through a rigorous analysis of the computational complexity, we show that LOE is significantly more efficient than conventional ordinal embedding approaches as the number of items grows. We validate these characterizations empirically on both synthetic and real datasets. We also present a practical approach that achieves the "best of both worlds", by using LOE to warm-start existing methods that are more statistically efficient but computationally expensive.	[Ghosh, Nikhil] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Chen, Yuxin] UChicago, Chicago, IL USA; [Ghosh, Nikhil; Chen, Yuxin; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA	University of California System; University of California Berkeley; California Institute of Technology	Ghosh, N (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	nikhil_ghosh@berkeley.edu; chenyuxin@uchicago.edu; yyue@caltech.edu		Chen, Yuxin/0000-0003-2133-140X	Caltech Summer Undergraduate Research Fellowship; Swiss NSF Early Mobility Postdoctoral Fellowship	Caltech Summer Undergraduate Research Fellowship; Swiss NSF Early Mobility Postdoctoral Fellowship	Nikhil Ghosh was supported in part by a Caltech Summer Undergraduate Research Fellowship. Yuxin Chen was supported in part by a Swiss NSF Early Mobility Postdoctoral Fellowship. This work was also supported in part by gifts from PIMCO and Bloomberg.	Agarwal Sameer, 2007, J MACHINE LEARNING R, P11; Agichtein E., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P19, DOI 10.1145/1148170.1148177; [Anonymous], 2017, ADV NEURAL INFORM PR; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Cattelan M, 2012, STAT SCI, V27, P412, DOI 10.1214/12-STS396; Chen Y., 2017, ARXIV170709971; De Silva V., 2004, TECHNICAL REPORT; Dryden I.L., 2016, STAT SHAPE ANAL, Vsecond; Jain Lalit, 2016, ADV NEURAL INFORM PR, V29, P2711; Joachims T., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P154, DOI 10.1145/1076034.1076063; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Kleindessner M., 2014, 27 C LEARNING THEORY, V35, P40; Lee JM, 2009, ASIA CONTROL CONF AS, P21; Leunis E., 2012, IEEE INT WORKSH MACH, P1, DOI DOI 10.1109/MLSP.2012.6349720; Luce R, 1959, INDIVIDUAL CHOICE BE; McAuley J, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2783258.2783381; Park D, 2015, PR MACH LEARN RES, V37, P1907; Sireci SG, 2003, J EDUC MEAS, V40, P277, DOI 10.1111/j.1745-3984.2003.tb01108.x; Tamuz Omer, 2011, ARXIV11051033; Terada Y, 2014, PR MACH LEARN RES, V32, P847; Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016; Wilber Michael J, 2014, 2 AAAI C HUM COMP CR; Yue YS, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P75, DOI 10.1145/2566486.2567991; 2010, PREFERENCE LEARNING, P00001	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903017
C	Gonen, A; Hazan, E; Moran, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gonen, Alon; Hazan, Elad; Moran, Shay			Private Learning Implies Online Learning: An Efficient Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the relationship between the notions of differentially private learning and online learning in games. Several recent works have shown that differentially private learning implies online learning, but an open problem of Neel, Roth, and Wu [27] asks whether this implication is efficient. Specifically, does an efficient differentially private learner imply an efficient online learner? In this paper we resolve this open question in the context of pure differential privacy. We derive an efficient black-box reduction from differentially private learning to online learning from expert advice.	[Gonen, Alon] Univ Calif San Diego, La Jolla, CA 92093 USA; [Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA; [Hazan, Elad; Moran, Shay] Google AI Princeton, Princeton, NJ USA	University of California System; University of California San Diego; Princeton University	Gonen, A (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	algonen@cs.ucsd.edu; ehazan@princeton.edu; shaymoran1@gmail.com		Hazan, Elad/0000-0002-1566-3216				ALON N, 2018, CORR, V1806, P949; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Bassily Raef, 2015, P 48 ANN ACM S THEOR, P1046; Beimel Amos, 2013, INNOVATIONS THEORETI, P97, DOI [10.1145/2422436.2422450, DOI 10.1145/2422436.2422450]; Ben-David Shai, 2009, C LEARN THEOR; Beygelzimer A, 2015, PR MACH LEARN RES, V37, P2323; Blum A., 2005, P 24 ACM SIGMOD SIGA, P128, DOI DOI 10.1145/1065167.1065184; Bousquet O., 2019, CORR; Bun Mark, 2015, 2015 IEEE 56 ANN S F; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Daniely A, 2016, ACM S THEORY COMPUT, P105, DOI 10.1145/2897518.2897520; Dinur I., 2003, P 22 ACM SIGMOD SIGA, P202, DOI DOI 10.1145/773153.773173; Dudik Miroslav, 2017, ANN S FDN COMP SCI P; Dwork C., 2015, ADV NEURAL INF PROCE, P2350; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P11, DOI 10.1145/2591796.2591883; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Feldman V., 2018, C LEARNING THEORY, P535; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gonen Alon, 2018, ARXIV181007362; Hardt M, 2014, ANN IEEE SYMP FOUND, P454, DOI 10.1109/FOCS.2014.55; Hazan E, 2016, MATH PROGRAM, V158, P363, DOI 10.1007/s10107-015-0933-y; Littlestone N., 1986, RELATING DATA COMPRE; Nguyen H. H., 2019, CORR; Roth Seth Neel Aaron, 2018, TECHNICAL REPORT; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Vadhan S, 2017, INFORM SEC CRYPT TEX, P347, DOI 10.1007/978-3-319-57048-8_7	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900031
C	Gower, RM; Kovalev, D; Lieder, F; Richtarik, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gower, Robert M.; Kovalev, Dmitry; Lieder, Felix; Richtarik, Peter			RSN: Randomized Subspace Newton	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION	We develop a randomized Newton method capable of solving learning problems with huge dimensional feature spaces, which is a common setting in applications such as medical imaging, genomics and seismology. Our method leverages randomized sketching in a new way, by finding the Newton direction constrained to the space spanned by a random sketch. We develop a simple global linear convergence theory that holds for practically all sketching techniques, which gives the practitioners the freedom to design custom sketching approaches suitable for particular applications. We perform numerical experiments which demonstrate the efficiency of our method as compared to accelerated gradient descent and the full Newton method. Our method can be seen as a refinement and randomized extension of the results of Karimireddy, Stich, and Jaggi [18].	[Gower, Robert M.] Telecom Paristech, IPP, LTCI, Paris, France; [Kovalev, Dmitry; Richtarik, Peter] KAUST, Thuwal, Saudi Arabia; [Lieder, Felix] Heinrich Heine Univ Dusseldorf, Dusseldorf, Germany; [Richtarik, Peter] MIPT, Moscow, Russia	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; King Abdullah University of Science & Technology; Heinrich Heine University Dusseldorf; Moscow Institute of Physics & Technology	Gower, RM (corresponding author), Telecom Paristech, IPP, LTCI, Paris, France.	gowerrobert@gmail.com; dmitry.kovalev@kaust.edu.sa; lieder@opt.uni-duesseldorf.de; peter.richtarik@kaust.edu.sa	Richtarik, Peter/O-5797-2018; Gower, Robert Mansel/Y-8838-2019	Gower, Robert Mansel/0000-0002-2268-9780				Abatzoglou JT, 2018, SCI DATA, V5, DOI 10.1038/sdata.2017.191; Addair T. G., 2014, COMPUTERS GEOSCIENCE, V66; Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Bertsekas D. P, 1996, CONSTRAINED OPTIMIZA, V1st; Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cole JR, 2014, NUCLEIC ACIDS RES, V42, pD633, DOI 10.1093/nar/gkt1244; d'Aspremont A, 2018, SIAM J OPTIMIZ, V28, P2384, DOI 10.1137/17M1116842; DEMBO RS, 1982, SIAM J NUMER ANAL, V19, P400, DOI 10.1137/0719025; Doikov N, 2018, PR MACH LEARN RES, V80; Fountoulakis K, 2018, COMPUT OPTIM APPL, V70, P351, DOI 10.1007/s10589-018-9984-3; Gower RM, 2012, OPTIM METHOD SOFTW, V27, P251, DOI 10.1080/10556788.2011.580098; Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487; Gower Robert M., 2016, P 33 INT C MACH LEAR; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Karimireddy Sai Praneeth, 2018, ARXIV18060041; Lee CH, 2017, KIDNEY RES CLIN PRAC, V36, P3, DOI 10.23876/j.krcp.2017.36.1.3; Lu HH, 2018, SIAM J OPTIMIZ, V28, P333, DOI 10.1137/16M1099546; LUO H, 2016, ADV NEURAL INFORM PR, P902; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 1987, STUDIES APPL MATH; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Overbeek R, 2000, NUCLEIC ACIDS RES, V28, P123, DOI 10.1093/nar/28.1.123; Pilanci M, 2016, J MACH LEARN RES, V17; Pilanci M, 2017, SIAM J OPTIMIZ, V27, P205, DOI 10.1137/15M1021106; Qu Z, 2016, PR MACH LEARN RES, V48; RICHTARIK P, 2017, ARXIV170601108; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Woodbury M.A., 1950, 42 PRINC U; Ypma TJ, 1995, SIAM REV, V37, P531, DOI 10.1137/1037125	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300056
C	Graber, C; Schwing, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Graber, Colin; Schwing, Alexander			Graph Structured Prediction Energy Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					For joint inference over multiple variables, a variety of structured prediction techniques have been developed to model correlations among variables and thereby improve predictions. However, many classical approaches suffer from one of two primary drawbacks: they either lack the ability to model high-order correlations among variables while maintaining computationally tractable inference, or they do not allow to explicitly model known correlations. To address this shortcoming, we introduce `Graph Structured Prediction Energy Networks,' for which we develop inference techniques that allow to both model explicit local and implicit higher-order correlations while maintaining tractability of inference. We apply the proposed method to tasks from the natural language processing and computer vision domain and demonstrate its general utility.	[Graber, Colin; Schwing, Alexander] Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Graber, C (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.	cgraber2@illinois.edu; aschwing@illinois.edu			NSF [1718221]; UIUC; Samsung; 3M; Cisco Systems Inc. [CG 1377144]; Adobe; MRI [1725729]	NSF(National Science Foundation (NSF)); UIUC; Samsung(Samsung); 3M(3M); Cisco Systems Inc.; Adobe; MRI	This work is supported in part by NSF under Grant No. 1718221 and MRI #1725729, UIUC, Samsung, 3M, Cisco Systems Inc. (Gift Award CG 1377144) and Adobe. We thank NVIDIA for providing GPUs used for this work and Cisco for access to the Arcetri cluster.	Akbik Alan, 2018, P COLING; Alvarez J. M., 2012, P ECCV; [Anonymous], 2015, P ICLR; Batra D., 2011, P AISTATS; Beck Amir, 2003, OPER RES LETT; Belanger D., 2016, P ICML; Belanger D., 2017, P ICML; Blondel M., 2018, ARXIV180509717; Boykov Y., 1998, P CVPR; Boykov Y., 2001, PAMI; Chambolle A., 2011, JMIV; Chen L.-C., 2015, P ICML; de Campos T., 2009, P VISAPP; Domke J., 2012, P AISTATS PALM CAN I; Finley T., 2008, P ICML; Frank M., 1956, NRLQ; GLOBERSON A, 2006, P NIPS; Globerson A., 2007, P NIPS; Graber C., 2018, P NEURIPS; Gygli Michael, 2017, ICML; Hazan T., 2010, P NIPS; Hazan T., 2010, T INFORM THEORY; Hazan T., 2008, P UAI; Heskes T., 2006, AI RES; Heskes T., 2002, P NIPS; HESKES T, 2003, P UAI; Heskes T., 2003, P NIPS; Huiskes M. J., 2008, P ICMR; Ihler A. T., 2004, P NIPS; Jaggi Martin, 2013, P ICML; Katakis I., 2008, ECMLPKDD, P1; Komodakis N., 2010, PAMI; Komodakis N., 2011, P CVPR; Krishnan R. G., 2015, P NIPS; Krizhevsky A., 2012, NIPS, P1106, DOI DOI 10.1145/3065386; Kulesza A., 2008, P NIPS; Lacoste-Julien S., 2015, P NIPS; Lacoste-Julien S., 2016, ARXIV PREPRINT ARXIV; Lafferty J., 2001, P ICML; Lample Guillaume, 2016, P NAACL HLT; Lin G., 2015, P NIPS; Ma X., 2016, P ACL; Meltzer T, 2009, P UAI; Meshi O., 2009, P UAI; Meshi O., 2017, P NIPS; Meshi O., 2010, INT C MACH LEARN; Meshi O, 2016, PR MACH LEARN RES, V48; Meshi Ofer, 2015, P NIPS; MURPHY KM, 1999, P UAI; Niculae V., 2018, P ICML; Pennington J., 2014, P 2014 C EMP METH NA; Pletscher P., 2010, P ECML PKDD; Sang E. F. Tjong Kim, 2003, P NAACL HLT; Schlesinger M. I., 1976, KIBERNETIKA; Schwing A., 2011, P CVPR; Schwing A. G., 2014, P ICML; Schwing A. G., 2015, ARXIV150302351 2015A; Schwing A. G., 2012, P NIPS; Shimony S. E., 1994, AI; Sontag D., 2007, P NIPS; Sontag D., 2012, P UAI; Sontag D., 2008, P NIPS; Sontag D., 2009, P AISTATS; Taskar B., 2003, P NIPS; Tompson J., 2014, P NIPS; Tsochantaridis I., 2005, JMLR; Tu Lifu, 2018, P ICLR; Vilnis L., 2015, P UAI; Wainwright M. J., 2008, FTML; Wainwright M. J., 2005, T INFORM THEORY; Wainwright M. J., 2003, T INFORM THEORY; Wainwright M. J., 2003, P C CONTR COMM COMP; WEISS Y, 2007, P UAI; Welling M., 2004, P UAI; Werner T., 2007, PAMI; Xiao L., 2010, JMLR; Yanover C., 2006, JMLR; Yedidia J. S., 2001, P NIPS; Zheng S., 2015, P ICCV	79	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900030
C	Grill, JB; Domingues, OD; Menard, P; Munos, R; Valko, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Grill, Jean-Bastien; Domingues, Omar D.; Menard, Pierre; Munos, Remi; Valko, Michal			Planning in entropy-regularized Markov decision processes and games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose SmoothCruiser, a new planning algorithm for estimating the value function in entropy-regularized Markov decision processes and two-player games, given a generative model of the environment. SmoothCruiser makes use of the smoothness of the Bellman operator promoted by the regularization to achieve problem-independent sample complexity of order (O) over tilde (1/epsilon(4)) for a desired accuracy epsilon, whereas for non-regularized settings there are no known algorithms with guaranteed polynomial sample complexity in the worst case.	[Grill, Jean-Bastien; Munos, Remi; Valko, Michal] DeepMind Paris, Paris, France; [Domingues, Omar D.; Menard, Pierre] Inria Lille, SequeL Team, Villeneuve Dascq, France		Grill, JB (corresponding author), DeepMind Paris, Paris, France.	jbgrill@google.com; omar.darwiche-domingues@inria.fr; pierre.menard@inria.fr; munos@google.com; valkom@deepmind.com			European CHIST-ERA project DELTA; French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council; Inria associated-team north-European project Allocate; Otto-von-Guericke-Universitit Magdeburg associated-team north-European project Allocate; French National Research Agency project BoB [ANR-16-CE23-0003]; FMJH Program PGMO; Criteo	European CHIST-ERA project DELTA; French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council(Region Hauts-de-France); Inria associated-team north-European project Allocate; Otto-von-Guericke-Universitit Magdeburg associated-team north-European project Allocate; French National Research Agency project BoB(French National Research Agency (ANR)); FMJH Program PGMO; Criteo	The research presented was supported by European CHIST-ERA project DELTA, French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, Inria and Otto-von-Guericke-Universitit Magdeburg associated-team north-European project Allocate, and French National Research Agency project BoB (grant n.ANR-16-CE23-0003), FMJH Program PGMO with the support of this program from Criteo.	Bartlett Peter L, 2019, INT C MACH LEARN; Bubeck S, 2010, C LEARN THEOR; Busoniu Lucian, 2012, INT C ART INT STAT; Coquelin P.-A., 2007, BANDIT ALGORIT UNPUB; Feldman Zohar, 2014, J ARTIFICIAL INTELLI; Grill JB, 2016, ADV NEUR IN, V29; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; Hansen Thomas Dueholm, 2013, J ACM, V60; Hren JF, 2008, LECT NOTES ARTIF INT, V5323, P151, DOI 10.1007/978-3-540-89722-4_12; Huang Ruitong, 2017, ALGORITHMIC LEARNING; Kaufmann Emilie, 2017, NEURAL INFORM PROCES; Kearns Michael, 1999, INT C ART INT STAT; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Leurent Edouard, 2019, EUR C MACH LEARN; Mnih V, 2016, PR MACH LEARN RES, V48; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Schulman John, 2017, EQUIVALENCE POLICY; Sutton R.S., 1991, ACM SIGART B, V2, P160, DOI [10.1145/122344.122377, DOI 10.1145/122344.122377]; Sutton Richard S., 2018, REINFORCENEN LEARNIN; Szorenyi Balazs, 2014, NEURAL INFORM PROCES; Walsh TJ, 2010, AAAI CONF ARTIF INTE, P612	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904009
C	Gronlund, A; Kamma, L; Larsen, KG; Mathiasen, A; Nelson, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gronlund, Allan; Kamma, Lior; Larsen, Kasper Green; Mathiasen, Alexander; Nelson, Jelani			Margin-Based Generalization Lower Bounds for Boosted Classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Boosting is one of the most successful ideas in machine learning. The most well-accepted explanations for the low generalization error of boosting algorithms such as AdaBoost stem from margin theory. The study of margins in the context of boosting algorithms was initiated by Schapire, Freund, Bartlett and Lee (1998) and has inspired numerous boosting algorithms and generalization bounds. To date, the strongest known generalization (upper bound) is the kth margin bound of Gao and Zhou (2013). Despite the numerous generalization upper bounds that have been proved over the last two decades, nothing is known about the tightness of these bounds. In this paper, we give the first margin-based lower bounds on the generalization error of boosted classifiers. Our lower bounds nearly match the kth margin bound and thus almost settle the generalization performance of boosted classifiers in terms of margins.	[Gronlund, Allan; Kamma, Lior; Larsen, Kasper Green; Mathiasen, Alexander] Aarhus Univ, Dept Comp Sci, Aarhus, Denmark; [Nelson, Jelani] Univ Calif Berkeley, Dept EECS, Berkeley, CA USA	Aarhus University; University of California System; University of California Berkeley	Gronlund, A (corresponding author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.	jallan@cs.au.dk; lior.kamma@cs.au.dk; larsen@cs.au.dk; alexmath@cs.au.dk; minilek@berkeley.edu			Villum Young Investigator Grant; AUFF Starting Grant; NSF CAREER award [CCF-1350670]; NSF [IIS-1447471]; ONR [N00014-18-1-2562]; ONR DORECG award [N00014-17-1-2127]; Alfred P. Sloan Research Fellowship; Google Faculty Research Award	Villum Young Investigator Grant(Villum Fonden); AUFF Starting Grant; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ONR DORECG award; Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); Google Faculty Research Award(Google Incorporated)	This work was supported by a Villum Young Investigator Grant and an AUFF Starting Grant.; Jelani Nelson is supported by NSF CAREER award CCF-1350670, NSF grant IIS-1447471, ONR grant N00014-18-1-2562, ONR DORECG award N00014-17-1-2127, an Alfred P. Sloan Research Fellowship, and a Google Faculty Research Award	BENNETT KP, 2000, P 17 INT C MACH LEAR, P65; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; EHRENFEUCHT A, 1989, INFORM COMPUT, V82, P247, DOI 10.1016/0890-5401(89)90002-3; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gao W, 2013, ARTIF INTELL, V203, P1, DOI 10.1016/j.artint.2013.07.002; Gronlund A., 2019, P ADV NEUR INF PROC, V32, P1; Grove AJ, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P692; Ke G., 2017, ADV NEURAL INFORM PR, V30, P3146; Khalid O, 2009, THIRD INTERNATIONAL WORKSHOP ON VIRTUALIZATION TECHNOLOGIES IN DISTRIBUTED COMPUTING (VTDC-09), P1; Mathiasen A., 2019, P 36 INT C MACH LEAR, P4392; Ratsch G, 2005, J MACH LEARN RES, V6, P2131; Ratsch G., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P334; Schapire RE, 1998, ANN STAT, V26, P1651; Wang LW, 2011, J MACH LEARN RES, V12, P1835	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903056
C	Guo, TY; Xu, C; Shi, BX; Xu, C; Tao, DC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Guo, Tianyu; Xu, Chang; Shi, Boxin; Xu, Chao; Tao, Dacheng			Learning from Bad Data via Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bad training data would challenge the learning model from understanding the underlying data-generating scheme, which then increases the difficulty in achieving satisfactory performance on unseen test data. We suppose the real data distribution lies in a distribution set supported by the empirical distribution of bad data. A worst-case formulation can be developed over this distribution set, and then be interpreted as a generation task in an adversarial manner. The connections and differences between GANs and our framework have been thoroughly discussed. We further theoretically show the influence of this generation task on learning from bad data and reveal its connection with a data-dependent regularization. Given different distance measures (e.g., Wasserstein distance or JS divergence) of distributions, we can derive different objective functions for the problem. Experimental results on different kinds of bad training data demonstrate the necessity and effectiveness of the proposed method.	[Guo, Tianyu; Xu, Chao] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, CMIC, Beijing, Peoples R China; [Guo, Tianyu; Xu, Chang; Tao, Dacheng] Univ Sydney, Sch Comp Sci, UBTECH Sydney Ctr, Fac Engn, Darlington, NSW 2008, Australia; [Shi, Boxin] Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Shi, Boxin] Peng Cheng Lab, Shenzhen 518040, Peoples R China	Peking University; University of Sydney; Peking University; Peng Cheng Laboratory	Guo, TY (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, CMIC, Beijing, Peoples R China.; Guo, TY; Xu, C (corresponding author), Univ Sydney, Sch Comp Sci, UBTECH Sydney Ctr, Fac Engn, Darlington, NSW 2008, Australia.; Shi, BX (corresponding author), Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.; Shi, BX (corresponding author), Peng Cheng Lab, Shenzhen 518040, Peoples R China.	tianyuguo@pku.edu.cn; chaoxu@cis.pku.edu.cn; shiboxin@pku.edu.cn; c.xu@sydney.edu.au; dacheng.tao@sydney.edu.au		Xu, Chang/0000-0002-4756-0609	National Natural Science Foundation of China [61876007, 61872012]; Australian Research Council [DE-180101438]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Australian Research Council(Australian Research Council)	We thank anonymous area chair and reviewers for their helpful comments. This research was supported in part by National Natural Science Foundation of China under Grant No. 61876007 and 61872012, and Australian Research Council Grant DE-180101438.	Ando S, 2017, LECT NOTES ARTIF INT, V10534, P770, DOI 10.1007/978-3-319-71249-9_46; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Antoniou Antreas, 2017, ARXIV171104340; Arjovsky M., 2017, ARXIV170107875; Brock Andrew, 2018, ARXIV180911096; Cui Yin, 2019, CVPR, P9268; Dodge SF, 2018, IEEE T IMAGE PROCESS, V27, P5553, DOI 10.1109/TIP.2018.2855966; Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205; Drummond C., 2003, WORKSH LEARN IMB DAT, V11, P1; Dumoulin Vincent, 2016, ARXIV E PRINTS; Gan Z, 2017, ADV NEUR IN, V30; Ginodi I., 2011, CORR; Girshick Ross, 2015, P INT C COMP VIS; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; KROGH A, 1992, ADV NEUR IN, V4, P950; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lemley J, 2017, IEEE ACCESS, V5, P5858, DOI 10.1109/ACCESS.2017.2696121; Lin ML, 2013, IEEE T NEUR NET LEAR, V24, P647, DOI 10.1109/TNNLS.2012.2228231; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Ling CX., 2011, ENCY MACH LEARNING; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Ng CJ, 2015, ASIAPAC SIGN INFO PR, P761, DOI 10.1109/APSIPA.2015.7415375; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Vasiljevic I., 2016, ARXIV161105760; Xiao H., 2017, FASHION MNIST NOVEL; Yim J, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING - TECHNIQUES AND APPLICATIONS (DICTA), P345; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhou YR, 2017, INT CONF ACOUST SPEE, P1213, DOI 10.1109/ICASSP.2017.7952349	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306009
C	Hadiji, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hadiji, Hedi			Polynomial Cost of Adaptation for X-Armed Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In the context of stochastic continuum-armed bandits, we present an algorithm that adapts to the unknown smoothness of the objective function. We exhibit and compute a polynomial cost of adaptation to the Holder regularity for regret minimization. To do this, we first reconsider the recent lower bound of Locatelli and Carpentier [21], and define and characterize admissible rate functions. Our new algorithm matches any of these minimal rate functions. We provide a finite-time analysis and a thorough discussion about asymptotic optimality.	[Hadiji, Hedi] Univ Paris Sud, Lab Math Orsay, Orsay, France	UDICE-French Research Universities; Universite Paris Saclay	Hadiji, H (corresponding author), Univ Paris Sud, Lab Math Orsay, Orsay, France.	hedi.hadiji@math.u-psud.fr						AGRAWAL R, 1995, SIAM J CONTROL OPTIM, V33, P1926, DOI 10.1137/S0363012992237273; Audibert Jean-Yves, COLT, P217; Auer Peter, INT C COMP LEARN THE, P454; Bubeck S, 2011, J MACH LEARN RES, V12, P1655; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Bubeck Sebastien, INT C ALG LEARN THEO, P144; Bull AD, 2015, BERNOULLI, V21, P2289, DOI 10.3150/14-BEJ644; Cai TT, 2012, STAT SCI, V27, P31, DOI 10.1214/11-STS355; Cai TT, 2005, ANN STAT, V33, P2311, DOI 10.1214/009053605000000633; Coquelin P.-A., 2007, BANDIT ALGORIT UNPUB; Garivier A, 2019, MATH OPER RES, V44, P377, DOI 10.1287/moor.2017.0928; Garivier Aurelien, KL UCB SWITCH OPTIMA; Grill Jean-Bastien, ADV NEURAL INFORM PR, P667; Kallenberg O., 2006, FDN MODERN PROBABILI; Kleinberg Robert, ABS13121277; Kleinberg Robert, P 17 INT C NEUR INF, P697; Krishnamurthy Akshay, CONTEXTUAL BANDITS C; Lattimore Tor, 2019, BANDIT ALGORITHMS; LEPSKII OV, 1990, THEOR PROBAB APPL+, V35, P454, DOI 10.1137/1135065; Locatelli Andrea, C LEARN THEOR, P1463; Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2; Shang Xuedong, ALGORITHMIC LEARNING, V98; Valko Michal, INT C MACH LEARN	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301007
C	Haimerl, C; Savin, C; Simoncelli, EP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Haimerl, Caroline; Savin, Cristina; Simoncelli, Eero P.			Flexible information routing in neural populations through stochastic comodulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RESPONSES; NEURONS	Humans and animals are capable of flexibly switching between a multitude of tasks, each requiring rapid, sensory-informed decision making. Incoming stimuli are processed by a hierarchy of neural circuits consisting of millions of neurons with diverse feature selectivity. At any given moment, only a small subset of these carry task-relevant information. In principle, downstream processing stages could identify the relevant neurons through supervised learning, but this would require many training trials. Such extensive learning periods are inconsistent with the observed flexibility of humans or animals, both of whom can adjust to changes in task parameters or structure almost immediately. Here, we propose a novel solution based on functionally-targeted stochastic modulation. It has been observed that trialto-trial neural activity is modulated by a shared, low-dimensional, stochastic signal that introduces task-irrelevant noise. Counter-intuitively, this noise appears to be preferentially targeted towards task-informative neurons, corrupting the encoded signal. We hypothesize that this modulation offers a solution to the identification problem, labeling task-informative neurons so as to facilitate decoding. We simulate an encoding population of spiking neurons whose rates are modulated by a shared stochastic signal, and show that a linear decoder with readout weights estimated from neuron-specific modulation strength can achieve near-optimal accuracy. Such a decoder allows fast and flexible task-dependent information routing without relying on hardwired knowledge of the task-informative neurons (as in maximum likelihood) or unrealistically many supervised training trials (as in regression).	[Haimerl, Caroline; Savin, Cristina; Simoncelli, Eero P.] NYU, Ctr Neural Sci, New York, NY 10003 USA; [Simoncelli, Eero P.] NYU, Howard Hughes Med Inst, New York, NY 10003 USA	New York University; Howard Hughes Medical Institute; New York University	Haimerl, C (corresponding author), NYU, Ctr Neural Sci, New York, NY 10003 USA.	ch2880@nyu.edu; csavin@nyu.edu; eero.simoncelli@nyu.edu	Savin, Cristina/ABI-4570-2020	Savin, Cristina/0000-0002-3414-8244; Simoncelli, Eero/0000-0002-1206-527X	Google PhD Fellowship	Google PhD Fellowship(Google Incorporated)	This work was supported by the Google PhD Fellowship (CH).	Akam TE, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002760; Arandia-Romero I, 2016, NEURON, V89, P1305, DOI 10.1016/j.neuron.2016.01.044; Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888; Bondy AG, 2018, NAT NEUROSCI, V21, P598, DOI 10.1038/s41593-018-0089-1; Britten KH, 1996, VISUAL NEUROSCI, V13, P87, DOI 10.1017/S095252380000715X; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Churchland AK, 2011, NEURON, V69, P818, DOI 10.1016/j.neuron.2010.12.037; Cohen MR, 2009, NAT NEUROSCI, V12, P1594, DOI 10.1038/nn.2439; Cohen MR, 2009, J NEUROSCI, V29, P6635, DOI 10.1523/JNEUROSCI.5179-08.2009; Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312; Ganguli D, 2016, NEURAL PERCEPTUAL SI, P1; Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711; Huang C., 2019, NEURON, V101, P1; Jazayeri M, 2007, NATURE, V446, P912, DOI 10.1038/nature05739; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Lin IC, 2015, NEURON, V87, P644, DOI 10.1016/j.neuron.2015.06.035; Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790; MASTERSON M, 2015, ELIFE, P1; Moreno-Bote R, 2014, NAT NEUROSCI, V17, P1410, DOI 10.1038/nn.3807; Ruff DA, 2016, J NEUROSCI, V36, P7523, DOI 10.1523/JNEUROSCI.0610-16.2016; Salinas E, 1994, J Comput Neurosci, V1, P89, DOI 10.1007/BF00962720; SEUNG HS, 1993, P NATL ACAD SCI USA, V90, P10749, DOI 10.1073/pnas.90.22.10749; Shadlen MN, 1996, J NEUROSCI, V16, P1486; Simoncelli, 2009, COGNITIVE NEUROSCIEN, VIV, P525; Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906010
C	Hajiramezanali, E; Hasanzadeh, A; Duffield, N; Narayanan, K; Zhou, MY; Qian, XN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hajiramezanali, Ehsan; Hasanzadeh, Arman; Duffield, Nick; Narayanan, Krishna; Zhou, Mingyuan; Qian, Xiaoning			Variational Graph Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.	[Hajiramezanali, Ehsan; Hasanzadeh, Arman; Duffield, Nick; Narayanan, Krishna; Qian, Xiaoning] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	Texas A&M University System; Texas A&M University College Station; University of Texas System; University of Texas Austin	Hajiramezanali, E (corresponding author), Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.	ehsanr@tamu.edu; armanihm@tamu.edu; duffieldng@tamu.edu; krn@tamu.edu; mingyuan.zhou@mccombs.utexas.edu; xqian@tamu.edu	Zhou, Mingyuan/AAE-8717-2021		National Science Foundation [ENG-1839816, IIS-1848596, CCF-1553281, IIS-1812641, IIS-1812699]	National Science Foundation(National Science Foundation (NSF))	The presented materials are based upon the research supported by the National Science Foundation under Grants ENG-1839816, IIS-1848596, CCF-1553281, IIS-1812641 and IIS-1812699. We also thank Texas A&M High Performance Research Computing and Texas Advanced Computing Center for providing computational resources to perform experiments in this work.	[Anonymous], 2018, ICLR; [Anonymous], ARXIV171105717; Armandpour M, 2019, AAAI CONF ARTIF INTE, P3191; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Donnat C, 2018, INT ACM C KNOWL DISC, V24; Goyal A.G.A.P., 2017, ADV NEURAL INFORM PR, P6713; Goyal P., 2019, KNOWLEDGE BASED SYST; Goyal Palash, 2018, ARXIV180511273; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Kipf T.N., 2016, VARIATIONAL GRAPH AU; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Li JD, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P387, DOI 10.1145/3132847.3132919; Molchanov Dmitry, 2018, ARXIV181002789; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Ribeiro LFR, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P385, DOI 10.1145/3097983.3098061; Seo Y, 2018, LECT NOTES COMPUT SC, V11301, P362, DOI 10.1007/978-3-030-04167-0_33; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Trivedi Rakshit, 2019, ICLR 2019; Yin MZ, 2018, PR MACH LEARN RES, V80; Zhou LK, 2018, AAAI CONF ARTIF INTE, P571	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902034
C	Hamidi, N; Bayati, M; Gupta, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hamidi, Nima; Bayati, Mohsen; Gupta, Kapil			Personalizing Many Decisions with High-Dimensional Covariates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MATRIX COMPLETION	We consider the k-armed stochastic contextual bandit problem with d dimensional features, when both k and d can be large. To the best of our knowledge, all existing algorithms for this problem have regret bounds that scale as polynomials of degree at least two, in k and d. The main contribution of this paper is to introduce and theoretically analyse a new algorithm (REAL-Bandit) with a regret that scales by r(2)(k + d) when r is the rank of the k x d matrix of unknown parameters. REAL-Bandit relies on ideas from low-rank matrix estimation literature and a new row-enhancement subroutine that yields sharper bounds for estimating each row of the parameter matrix that may be of independent interest. We also show via simulations that REAL-Bandit algorithm outperforms existing algorithms that do not leverage the low-rank structure of the problem.	[Hamidi, Nima] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Bayati, Mohsen] Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA; [Gupta, Kapil] Airbnb, San Francisco, CA USA	Stanford University; Stanford University; Airbnb	Hamidi, N (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	hamidi@stanford.edu; bayati@stanford.edu; kapil.gupta@airbnb.com		Bayati, Mohsen/0000-0002-7280-912X	National Science Foundation [CMMI: 1554140]; Stanford Data Science Initiative; Human-Centered AI Initiative	National Science Foundation(National Science Foundation (NSF)); Stanford Data Science Initiative; Human-Centered AI Initiative	The authors gratefully acknowledge support of the National Science Foundation (CAREER award CMMI: 1554140), Stanford Data Science Initiative, and Human-Centered AI Initiative.	Abbasi-Yadkori Y., 2012, THESIS U ALBERTA; Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Agrawal S., 2017, ARXIV E PRINTS; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Bastani H, 2015, ONLINE DECISION MAKI; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen Yuxin, 2019, ARXIV E PRINTS; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Dani V., 2008, STOCHASTIC LINEAR OP; Goldenshluger A., 2013, STOCHASTIC SYSTEMS, V3, P230, DOI [10.1287/11-SSY032, DOI 10.1287/11-SSY032]; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Hamidi Nima, 2019, ARXIV190408576; Hamidi Nima, 2019, PERSONALIZING MANY D; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Johari R, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1517, DOI 10.1145/3097983.3097992; Jun Kwang-Sung, 2019, ARXIV E PRINTS; Kallus N., 2016, ARXIV E PRINTS; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Klopp O, 2014, BERNOULLI, V20, P282, DOI 10.3150/12-BEJ486; Kocak T, 2014, AAAI CONF ARTIF INTE, P1911; Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lale Sahin, 2019, ARXIV E PRINTS; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Scott SL, 2015, APPL STOCH MODEL BUS, V31, P37, DOI 10.1002/asmb.2104; Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Valko M, 2014, PR MACH LEARN RES, V32, P46; Van Roy, 2013, ADV NEURAL INFORM PR, P3003	42	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903014
C	Hao, Y; Orlitsky, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hao, Yi; Orlitsky, Alon			Unified Sample-Optimal Property Estimation in Near-Linear Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NUMBER	We consider the fundamental learning problem of estimating properties of distributions over large domains. Using a novel piecewise-polynomial approximation technique, vve derive the first unified methodology for constructing sample- and time-efficient estimators for all sufficiently smooth, symmetric and non-symmetric, additive properties. This technique yields near-linear-time computable estimators whose approximation values are asymptotically optimal and highly-concentrated, resulting in the first: 1) estimators achieving the O(k/(epsilon(2) log k)) min-max epsilon-error sample complexity for all k-symbol Lipschitz properties; 2) unified near-optimal differentially private estimators for a variety of properties; 3) unified estimator achieving optimal bias and near-optimal variance for five important properties; 4) near-optimal sample-complexity estimators for several important symmetric properties over both domain sizes and confidence levels.	[Hao, Yi; Orlitsky, Alon] Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92103 USA	University of California System; University of California San Diego	Hao, Y (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92103 USA.	yih179@ucsd.edu; alon@ucsd.edu			National Science Foundation (NSF) [CIF-1564355, CIF-1619448]	National Science Foundation (NSF)(National Science Foundation (NSF))	We are grateful to the National Science Foundation (NSF) for supporting this work through grants CIF-1564355 and CIF-1619448.	Acharya J., 2018, ARXIV180300008; Acharya J., 2013, J MACH LEARN RES P T, P764; Acharya J, 2017, PR MACH LEARN RES, V70; CHAO A, 1984, SCAND J STAT, V11, P265; Charikar M, 2019, ACM S THEORY COMPUT, P780, DOI 10.1145/3313276.3316398; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Colwell RK, 2012, J PLANT ECOL, V5, P3, DOI 10.1093/jpe/rtr044; Cover T.M., 2011, KELLY CAPITAL GROWTH, VVolume 3, P181; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Devroye L, 2013, PROBABILISTIC THEORY, V31; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; EFRON B, 1976, BIOMETRIKA, V63, P435, DOI 10.2307/2335721; Gerstner W., 2002, SPIKING NEURON MODEL; Haas P. J., 1995, VLDB '95. Proceedings of the 21st International Conference on Very Large Data Bases, P311; HAN Y, 2018, ARXIV180208405; Hao Y., 2018, P 32 INT C NEURAL IN, P8848; Hao Y., 2019, ARXIV PREPRINT ARXIV; Hao Y, 2019, PR MACH LEARN RES, V97; Hao Yi, 2019, ARXIV190603794; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Jiao Jiantao, 2018, IEEE T INFORM THEORY; Kroes I, 1999, P NATL ACAD SCI USA, V96, P14547, DOI 10.1073/pnas.96.25.14547; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Orlitsky A., 2004, PROC 20 C UNCERTAINT, P426; Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113; Pachon R, 2009, BIT, V49, P721, DOI 10.1007/s10543-009-0240-1; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; THISTED R, 1987, BIOMETRIKA, V74, P445, DOI 10.1093/biomet/74.3.445; Trefethen Lloyd N, 2013, APPROXIMATION THEORY, V128; Valiant G, 2017, J ACM, V64, DOI 10.1145/3125643; Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81; Valiant Gregory, 2011, STOC 11; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; [No title captured]	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902070
C	Harutyunyan, A; Dabney, W; Mesnard, T; Heess, N; Azar, MG; Piot, B; van Hasselt, H; Singh, S; Wayne, G; Precup, D; Munos, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Harutyunyan, Anna; Dabney, Will; Mesnard, Thomas; Heess, Nicolas; Azar, Mohammad G.; Piot, Bilal; van Hasselt, Hado; Singh, Satinder; Wayne, Greg; Precup, Doina; Munos, Remi			Hindsight Credit Assignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of efficient credit assignment in reinforcement learning. In order to efficiently and meaningfully utilize new data, we propose to explicitly assign credit to past decisions based on the likelihood of them having led to the observed outcome. This approach uses new information in hindsight, rather than employing foresight. Somewhat surprisingly, we show that value functions can be rewritten through this lens, yielding a new family of algorithms. We study the properties of these algorithms, and empirically show that they successfully address important credit assignment challenges, through a set of illustrative tasks.	[Harutyunyan, Anna; Dabney, Will; Mesnard, Thomas; Heess, Nicolas; Azar, Mohammad G.; Piot, Bilal; van Hasselt, Hado; Singh, Satinder; Wayne, Greg; Precup, Doina; Munos, Remi] DeepMind, London, England		Harutyunyan, A (corresponding author), DeepMind, London, England.	harutyunyan@google.com; wdabney@google.com; munos@google.com						[Anonymous], 2019, INT C LEARN REPR ICL; Arjona-Medina J. A., 2019, ADV NEURAL INFORM PR, P13544; Buesing Lars, 2018, CORR; Girsanov IV, 1960, THEORY PROBAB APPL, V5, P285, DOI DOI 10.1137/1105027; Gregor Karol, 2016, ARXIV161107507; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Hausman K., 2018, INT C LEARN REPR ICL; Heess N., 2015, NIPS; Henaff M., 2017, ARXIV170507177; Hung C-C, 2018, ARXIV181006721; Mnih Volodymyr, 2016, P MACHINE LEARNING R, V48, P1928; MOORE AW, 1993, MACH LEARN, V13, P103, DOI 10.1007/BF00993104; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rauber Paulo, 2019, INT C LEARN REPR ICL; Rubino G, 2009, RARE EVENT SIMULATIO, V73; Schulman J., 2015, CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Singh Satinder, 1994, INT C MACH LEARN ICM; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 2000, ADV NEUR IN, V12, P1057; van den Oord Aaron, 2019, REPRESENTATION LEARN, P4	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904017
C	Hassani, H; Karbasi, A; Mokhtari, A; Shen, ZB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hassani, Hamed; Karbasi, Amin; Mokhtari, Aryan; Shen, Zebang			Stochastic Continuous Greedy plus plus : When Upper and Lower Bounds Match	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUBMODULAR FUNCTION MAXIMIZATION; MULTILINEAR RELAXATION; FUNCTION SUBJECT	In this paper, we develop Stochastic Continuous Greedy++ (SCG++), the first efficient variant of a conditional gradient method for maximizing a continuous submodular function subject to a convex constraint. Concretely, for a monotone and continuous DR-submodular function, SCG++ achieves a tight [(1 - 1/e)OPT - epsilon] solution while using O(1/epsilon(2)) stochastic gradients and O(1/epsilon) calls to the linear optimization oracle. The best previously known algorithms either achieve a suboptimal [(1/2)OPT - epsilon] solution with O(1/epsilon(2)) stochastic gradients or the tight [(1 - 1/e)OPT - epsilon] solution with suboptimal O(1 / epsilon(3)) stochastic gradients. We further provide an information-theoretic lower bound to showcase the necessity of O(1 / epsilon(2)) stochastic oracle queries in order to achieve [(1 - 1/e)OPT - epsilon] for monotone and DR-submodular functions. This result shows that our proposed SCG++ enjoys optimality in terms of both approximation guarantee, i.e., (1 - 1/e) approximation factor, and stochastic gradient evaluations, i.e., O(1 / epsilon(2)) calls to the stochastic oracle. By using stochastic continuous optimization as an interface, we also show that it is possible to obtain the [(1 - 1/)OPT - epsilon] tight approximation guarantee for maximizing a monotone but stochastic submodular set function subject to a general matroid constraint after at most O(n(2)/epsilon(2)) calls to the stochastic function value, where n is the number of elements in the ground set.	[Hassani, Hamed; Shen, Zebang] Univ Penn, ESE Dept, Philadelphia, PA 19104 USA; [Karbasi, Amin] Yale Univ, ECE Dept, New Haven, CT USA; [Mokhtari, Aryan] Univ Texas Austin, ECE Dept, Austin, TX 78712 USA	University of Pennsylvania; Yale University; University of Texas System; University of Texas Austin	Hassani, H (corresponding author), Univ Penn, ESE Dept, Philadelphia, PA 19104 USA.	hassani@seas.upenn.edu; amin.karbasi@yale.edu; mokhtari@austin.utexas.edu; zebang@seas.upenn.edu			NSF [CPS-1837253, IIS-1845032]; ONR [N00014- 19-1-2406]; AFOSR [FA9550-18-1-0160]; Zhejiang Provincial Natural Science Foundation of China [LZ18F020002]; National Natural Science Foundation of China [61672376, 61751209, 61472347]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Zhejiang Provincial Natural Science Foundation of China(Natural Science Foundation of Zhejiang Province); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The work of H. Hassani was partially supported by NSF CPS-1837253. Karbasi's work is partially supported by NSF (IIS-1845032), ONR (N00014- 19-1-2406) and AFOSR (FA9550-18-1-0160). Shen's work is supported by Zhejiang Provincial Natural Science Foundation of China under Grant No. LZ18F020002, and National Natural Science Foundation of China (Grant No: 61672376, 61751209, 61472347).	Allen-Zhu Zeyuan, 2018, ADV NEURAL INFORM PR, P2680; Bach F, 2015, ARXIV151100394; Badanidiyuru A., 2014, P 25 ANN ACM SIAM S, P1497; Bian AA, 2017, PR MACH LEARN RES, V54, P111; Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Chekuri C, 2014, SIAM J COMPUT, V43, P1831, DOI 10.1137/110839655; Chekuri C, 2011, ACM S THEORY COMPUT, P783; Das A., 2011, ICML; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Djolonga Josip, 2014, NIPS; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46; Gharan SO, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1098; Hassani H, 2017, ADV NEURAL INFORM PR, P5841; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Kulesza A., 2012, ARXIV12076083; Lin H., 2011, P ANN M ASS COMP LIN; Lin Zhouchen, 2018, ADV NEURAL INFORM PR, P687; Mokhtari A., 2018, C ART INT STAT, V84, P1886; Mokhtari A., 2018, ARXIV180409554; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Niazadeh R., 2018, OPTIMAL ALGORITHMS C; Reddi SJ, 2016, PR MACH LEARN RES, V48; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Soma T., 2015, NIPS; Sviridenko M., 2015, PROC 26 ACM SIAM S D, P1134; Vondrak J, 2008, ACM S THEORY COMPUT, P67; Wang C, 2011, J PHYS CHEM LETT, V2, P1701, DOI 10.1021/jz200492d	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904069
C	Hayashi, K; Yamaguchi, T; Sugawara, Y; Maeda, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hayashi, Kohei; Yamaguchi, Taiki; Sugawara, Yohei; Maeda, Shin-ichi			Einconv: Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Tensor decomposition methods are widely used for model compression and fast inference in convolutional neural networks (CNNs). Although many decompositions are conceivable, only CP decomposition and a few others have been applied in practice, and no extensive comparisons have been made between available methods. Previous studies have not determined how many decompositions are available, nor which of them is optimal. In this study, we first characterize a decomposition class specific to CNNs by adopting a flexible graphical notation. The class includes such well-known CNN modules as depthwise separable convolution layers and bottleneck layers, but also previously unknown modules with nonlinear activations. We also experimentally compare the tradeoff between prediction accuracy and time/space complexity for modules found by enumerating all possible decompositions, or by using a neural architecture search. We find some nonlinear decompositions outperform existing ones.	[Hayashi, Kohei; Yamaguchi, Taiki; Sugawara, Yohei; Maeda, Shin-ichi] Preferred Networks, Tokyo, Japan; [Yamaguchi, Taiki] Univ Tokyo, Tokyo, Japan	University of Tokyo	Hayashi, K (corresponding author), Preferred Networks, Tokyo, Japan.	hayasick@preferred.jp; yamaguchi@hep-th.phys.s.u-tokyo.ac.jp; suga@preferred.jp; ichi@preferred.jp						[Anonymous], 2013, COMPUT SCI; [Anonymous], 2016, 3D MNIST DATASET; Bridgeman JC, 2017, J PHYS A-MATH THEOR, V50, DOI 10.1088/1751-8121/aa6dc3; Chollet F., 2016, CORR; Cohen N, 2016, PR MACH LEARN RES, V48; Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017; Dentinel Zarembaw, 2014, NEURIPS, P1269; Goodfellow I., 2016, DEEP LEARNING; Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9; He Kaiming, 2015, ARXIV07061234; He ZB, 2018, IEEE T VEH TECHNOL, V67, P665, DOI 10.1109/TVT.2017.2738018; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; Howard A. G., 2017, MOBILENETS EFFICIENT; Kim YD., 2015, ARXIV PREPRINT ARXIV; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kopuklu O., 2019, ARXIV190402422; Lebedev V., 2015, 3 INT C LEARNING REP; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; Pham H, 2018, 35 INT C MACH LEARN; Real E., 2018, ARXIV180201548; Rigamonti R, 2013, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR.2013.355; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sifre Laurent, 2014, THESIS; Sloane N. J. A., 2019, ON LINE ENCY INTEGER; Smith S. W., 1997, SCI ENG GUIDE DIGITA; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tai C., 4 INT C LEARN REPR I; Tokui S, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2002, DOI 10.1145/3292500.3330756; Tran D., 2014, ARXIV141207671177; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Wang WQ, 2018, PROC CVPR IEEE, P9329, DOI 10.1109/CVPR.2018.00972; Wiebe M., 2011, NUMPY DISCUSSION EIN; Xiao H., 2017, ARXIV170807747; Yang YC, 2017, PR MACH LEARN RES, V70; Ye K., 2018, ARXIV180102662; Zoph B., 2016, ICLR; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305053
C	Heaukulani, C; van der Wilk, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Heaukulani, Creighton; van der Wilk, Mark			Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We implement gradient-based variational inference routines for Wishart and inverse Wishart processes, which we apply as Bayesian models for the dynamic, heteroskedastic covariance matrix of a multivariate time series. The Wishart and inverse Wishart processes are constructed from i.i.d. Gaussian processes, existing variational inference algorithms for which form the basis of our approach. These methods are easy to implement as a black-box and scale favorably with the length of the time series, however, they fail in the case of the Wishart process, an issue we resolve with a simple modification into an additive white noise parameterization of the model. This modification is also key to implementing a factored variant of the construction, allowing inference to additionally scale to high-dimensional covariance matrices. Through experimentation, we demonstrate that some (but not all) model variants outperform multivariate GARCH when forecasting the covariances of returns on financial instruments.	[van der Wilk, Mark] PROWLER Io, Cambridge, England		van der Wilk, M (corresponding author), PROWLER Io, Cambridge, England.	c.k.heaukulani@gmail.com; mark@prowler.io						[Anonymous], 2014, ICLR; Bauer M., 2016, NIPS; Engle R, 2002, J BUS ECON STAT, V20, P339, DOI 10.1198/073500102288618487; ENGLE RF, 1995, ECONOMET THEOR, V11, P122, DOI 10.1017/S0266466600009063; Figurnov M., 2018, NIPS; Fox E., 2011, ARXIV11075239; Fox EB, 2015, J MACH LEARN RES, V16, P2501; Ghalanos A., 2014, RMGARCH MULTIVARIATE; Hensman J., 2015, AISTATS; Hensman J., 2013, UAI; Kaggle Marjanovic B, 2017, HUGE STOCK MARKET DA; Kingma D.P., 2015, INT C LEARN REPR, P1; Knowles D. A., 2015, ARXIV150901631; Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Nugent Cam, 2018, S P 500 STOCK DATA; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rao C. R., 1973, LINEAR STAT INFERENC, V2; Ruiz F. R., 2016, NIPS; Sheppard K., 2014, ARCH; Van der Weide R, 2002, J APPL ECONOMET, V17, P549, DOI 10.1002/jae.688; van der Wilk M., 2014, NIPS 2014 WORKSH ADV; Wilson A.-G., 2010, UAI; Wu Y., 2014, NIPS; Wu Y., 2013, ICML	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304057
C	Herbster, M; Robinson, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Herbster, Mark; Robinson, James			Online Prediction of Switching Graph Labelings with Cluster Specialists	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TRACKING	We address the problem of predicting the labeling of a graph in an online setting when the labeling is changing over time. We present an algorithm based on a specialist [11] approach; we develop the machinery of cluster specialists which probabilistically exploits the cluster structure in the graph. Our algorithm has two variants, one of which surprisingly only requires O(log n) time on any trial t on an n-vertex graph, an exponential speed up over existing methods. We prove switching mistake-bound guarantees for both variants of our algorithm. Furthermore these mistake bounds smoothly vary with the magnitude of the change between successive labelings. We perform experiments on Chicago Divvy Bicycle Sharing data and show that our algorithms significantly outperform an existing algorithm (a kernelized Perceptron) as well as several natural benchmarks.	[Herbster, Mark; Robinson, James] UCL, Dept Comp Sci, London, England	University of London; University College London	Herbster, M (corresponding author), UCL, Dept Comp Sci, London, England.	m.herbster@cs.ucl.ac.uk; j.robinson@cs.ucl.ac.uk						Adamskiy Dmitry, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P290, DOI 10.1007/978-3-642-34106-9_24; Adamskiy D., 2012, P 25 INT C NEUR INF, P135; [Anonymous], 2012, ADV NEURAL INFORM PR; Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e; Blum A., 2001, P INT C MACH LEARN I, P19, DOI DOI 10.1184/R1/6606860.V1; Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654; Cesa-Bianchi N, 2013, J MACH LEARN RES, V14, P1251; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cesa- Bianchi Nicolo, 2009, P C LEARN THEOR, P145; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; Freund Y., 2000, COMMUNICATION; Freund Y., 1997, P 20 9 ANN ACM S THE, P334, DOI [10.1145/258533.258616, DOI 10.1145/258533.258616]; Gyorgy A, 2012, IEEE T INFORM THEORY, V58, P6709, DOI 10.1109/TIT.2012.2209627; Hazan E., 2007, ELECT C COMPUTATIONA, V14; Herbster M, 2001, J MACH LEARN RES, V1, P281, DOI 10.1162/153244301753683726; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Herbster M, 2008, ADV NEURAL INFORM PR, V21, P649; Herbster M, 2005, ACM INT C PROCEEDING, V119, P305; Herbster M., 2009, COLT 2009 22 C LEARN; Herbster M., 2015, P 28 INT C NEUR INF, V2, P2935; Herbster M, 2006, ADV NEURAL INFORM PR, V19, P577; Herbster M, 2015, J MACH LEARN RES, V16, P2003; Kerner BS, 1998, PHYS REV LETT, V81, P3797, DOI 10.1103/PhysRevLett.81.3797; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; KLEIN DJ, 1993, J MATH CHEM, V12, P81, DOI 10.1007/BF01164627; Koolen W., 2008, P 21 ANN C LEARN THE, P275; Lyons R., 2017, PROBABILITY TREES NE; M. Herbster, 1997, TRACKING BEST UNPUB; Padilla OHM, 2018, J MACH LEARN RES, V18; Rakhlin A, 2017, PR MACH LEARN RES, V54, P1403; Veness J, 2013, IEEE DATA COMPR CONF, P321, DOI 10.1109/DCC.2013.40; Vitale Fabio, 2011, NIPS, P1584; Vovk V, 1999, MACH LEARN, V35, P247, DOI 10.1023/A:1007595032382; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371; Wilson D. B., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P296, DOI 10.1145/237814.237880; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu Xiaojin., 2003, P ICLR, P912	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307006
C	Hu, HZ; Langford, J; Caruana, R; Mukherjee, S; Horvitz, E; Dey, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Hanzhang; Langford, John; Caruana, Rich; Mukherjee, Saurajit; Horvitz, Eric; Dey, Debadeepta			Efficient Forward Architecture Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a neural architecture search (NAS) algorithm, Petridish, to iteratively add shortcut connections to existing network layers. The added shortcut connections effectively perform gradient boosting on the augmented layers. The proposed algorithm is motivated by the feature selection algorithm forward stage-wise linear regression, since we consider NAS as a generalization of feature selection for regression, where NAS selects shortcuts among layers instead of selecting features. In order to reduce the number of trials of possible connection combinations, we train jointly all possible connections at each stage of growth while leveraging feature selection techniques to choose a subset of them. We experimentally show this process to be an efficient forward architecture search algorithm that can find competitive models using few GPU days in both the search space of repeatable network modules (cell-search) and the space of general networks (macro-search). Petridish is particularly well-suited for warm-starting from existing models crucial for lifelong-learning scenarios.	[Hu, Hanzhang] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Langford, John; Caruana, Rich; Mukherjee, Saurajit; Horvitz, Eric; Dey, Debadeepta] Microsoft Res, Redmond, WA USA	Carnegie Mellon University; Microsoft	Hu, HZ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	hanzhang@cs.cmu.edu; jcl@microsoft.com; rcaruana@microsoft.com; saurajim@microsoft.com; horvitz@microsoft.com; dedey@microsoft.com						Cai H, 2018, PR MACH LEARN RES, V80; Cai Han, 2019, INT C LEARN REPR; Casale Francesco Paolo, 2019, PROBABILISTIC NEURAL; Colson B, 2007, ANN OPER RES, V153, P235, DOI 10.1007/s10479-007-0176-2; Cortes C, 2017, PR MACH LEARN RES, V70; Das A, 2011, P 28 INT C INT C MAC; DeVries T., 2017, P 2017 COMPUTER VISI; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Elsken T., 2018, J MACH LEARN RES; Elsken Thomas, 2018, EFFICIENT MULTIOBJEC; Fahlman S.E., 1990, ADV NEURAL INFORM PR, P524; Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang F., 2018, PR MACH LEARN RES; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Gao, 2017, ABS171109224 CORR; Kandasamy Kirthevasan, 2018, NIPS, P2016; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Larsson G., 2017, INT C LEARN REPR ICL; Li Liam, 2019, ABS190207638 CORR; Liu C., 2017, ARXIV171200559; Liu Hanxiao, 2019, DARTS DIFFERENTIABLE; Liu Hanxiao, 2018, ICLR; Loshchilov I., 2017, P INT C LEARNING REP; Luo Renqian, 2018, NIPS; Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241; Marcus Mitchell, 1993, M ANN BUILDING LARGE; Negrinho R., 2017, ARXIV170408792; PATI Y, 1993, AS C SIGN SYST COMP; Pham H, 2018, PR MACH LEARN RES, V80; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Real E, 2017, PR MACH LEARN RES, V70; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; SCHAFFER JD, 1990, PHYSICA D, V42, P244, DOI 10.1016/0167-2789(90)90078-4; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wei T, 2016, PR MACH LEARN RES, V48; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xie Sirui, 2019, INT C LEARN REPR; Yang Zhilin, 2018, ICLR; Ying C, 2019, PR MACH LEARN RES, V97; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901072
C	Hu, HY; Yarats, D; Gong, QC; Tian, YD; Lewis, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Hengyuan; Yarats, Denis; Gong, Qucheng; Tian, Yuandong; Lewis, Mike			Hierarchical Decision Making by Generating and Following Natural Language Instructions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We explore using natural language instructions as an expressive and compositional representation of complex actions for hierarchical decision making. Rather than directly selecting micro-actions, our agent first generates a plan in natural language, which is then executed by a separate model. We introduce a challenging real-time strategy game environment in which the actions of a large number of units must be coordinated across long time scales. We gather a dataset of 76 thousand pairs of instructions and executions from human play, and train instructor and executor models. Experiments show that models generate intermediate plans in natural langauge significantly outperform models that directly imitate human actions. The compositional structure of language is conducive to learning generalizable action representations. We also release our code, models and data(23).	[Hu, Hengyuan; Yarats, Denis; Gong, Qucheng; Tian, Yuandong; Lewis, Mike] Facebook AI Res, New York, NY 10011 USA; [Yarats, Denis] NYU, New York, NY 10003 USA	Facebook Inc; New York University	Hu, HY (corresponding author), Facebook AI Res, New York, NY 10011 USA.	hengyuan@fb.com; denisyarats@cs.nyu.edu; qucheng@fb.com; yuandong@fb.com; mikelewis@fb.com						Andreas J, 2017, PR MACH LEARN RES, V70; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2017, ABS170804782 CORR; Artzi Y, 2013, T ASSOC COMPUT LING, V1, P49, DOI DOI 10.1162/TACL_A_00209; Bacon P., 2016, ABS160905140 CORR ABS160905140 CORR; Certicky Michal, 2013, ARXIV13064460; Churchill D., 2012, P 8 AAAI C ART INT I, P112; Churchill D., 2011, INTERACTIVE DIGITAL, P14; Daniele AF, 2017, ACMIEEE INT CONF HUM, P109, DOI 10.1145/2909824.3020241; Fried D., 2018, ARXIV180602724; Fried Daniel, 2017, ARXIV171104987; Hermann Karl Moritz, 2017, ARXIV170606551; Mei Hongyuan, 2016, AAAI, V1, P2; Miller A. H., 2017, ARXIV170506476; Mnih V., 2013, CORR; Oh J., 2017, ICML; Ontanon S, 2013, IEEE T COMP INTEL AI, V5, P293, DOI 10.1109/TCIAIG.2013.2286295; OpenAI, 2018, OP 5; Peng P., 2017, ABS170310069 CORR; Richard Sutton, 1999, ARTIF INTELL, V112; Sukhbaatar Sainbayar, 2015, ABS151107401 CORR; Sun P., 2018, ARXIV180907193, V1809; Synnaeve Gabriel, 2016, ABS161100625 CORR; Tessler Chen, 2016, ABS160407255 CORR; Usunier N, 2017, INT C LEARN REPR; Wu Bin, 2018, ARXIV181207887; Yarats D, 2018, PR MACH LEARN RES, V80; Zambaldi V., 2018, ARXIV180601830; Zettlemoyer Luke S., 2007, P EMNLP CONLL	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901063
C	Hu, WQ; Li, CJ; Lian, XR; Liu, J; Yuan, HZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Wenqing; Li, Chris Junchi; Lian, Xiangru; Liu, Ji; Yuan, Huizhuo			Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Stochastic compositional optimization arises in many important machine learning applications. The objective function is the composition of two expectations of stochastic functions, and is more challenging to optimize than vanilla stochastic optimization problems. In this paper, we investigate the stochastic compositional optimization in the general smooth non-convex setting. We employ a recently developed idea of Stochastic Recursive Gradient Descent to design a novel algorithm named SARAH-Compositional, and prove a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization: ((n + m)(1/2) epsilon(-2)) in the finite-sum case and O(epsilon(-3)) in the online case. Such a complexity is known to be the best one among IFO complexity results for non-convex stochastic compositional optimization. Numerical experiments on riskadverse portfolio management validate the superiority of SARAH-Compositional over a few rival algorithms.	[Hu, Wenqing] Missouri Univ Sci & Techol, Rolla, MO USA; [Li, Chris Junchi; Yuan, Huizhuo] Tencent AI Lab, Bellevue, WA 98004 USA; [Lian, Xiangru; Liu, Ji] Univ Rochester, Rochester, NY 14627 USA; [Liu, Ji] Kwai Inc, Beijing, Peoples R China; [Yuan, Huizhuo] Peking Univ, Beijing, Peoples R China	University of Missouri System; Missouri University of Science & Technology; University of Rochester; Peking University	Li, CJ (corresponding author), Tencent AI Lab, Bellevue, WA 98004 USA.	huwen@mst.edu; junchi.li.duke@gmail.com; admin@mail.xrlian.com; ji.liu.uwisc@gmail.com; hzyuan@pku.edu.cn	LI, chris/HDO-6232-2022	Hu, Wenqing/0000-0002-6116-9104				Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Dentcheva D, 2017, ANN I STAT MATH, V69, P737, DOI 10.1007/s10463-016-0559-8; Fang C., 2018, ADV NEURAL INFORM PR, P689; Foster Dylan J, 2019, ARXIV191202365; Huo Z., 2018, 32 AAAI C ART INT; Lei LH, 2017, ADV NEUR IN, V30; Li H., 2015, PROC 28 INT C NEURAL, P379; LI HS, 2019, ADV NEURAL INFORM PR, P2440, DOI DOI 10.1145/3343031.3350874; Lian XR, 2017, PR MACH LEARN RES, V54, P1159; Lin Tianyi, 2018, ARXIV180600458; Liu L., 2017, ARXIV171104416; Mandal S, 2015, 2015 THIRD INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P73, DOI 10.1109/ICIIP.2015.7414743; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nguyen L. M., 2019, ARXIV190107648; Nguyen LM, 2017, PR MACH LEARN RES, V70; Reddi SJ, 2016, PR MACH LEARN RES, V48; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Wang M., 2017, J MACH LEARN RES, V18, P1, DOI DOI 10.5555/3122009.3176849; Wang MD, 2017, MATH PROGRAM, V161, P419, DOI 10.1007/s10107-016-1017-3; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Yang SG, 2019, SIAM J OPTIMIZ, V29, P616, DOI 10.1137/18M1164846; Zhou D., 2018, ADV NEURAL INFORM PR, P3921	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306088
C	Huang, ZA; Liu, FC; Su, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huang, Zhiao; Liu, Fangchen; Su, Hao			Mapping State Space using Landmarks for Universal Goal Reaching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					An agent that has well understood the environment should be able to apply its skills for any given goals, leading to the fundamental problem of learning the Universal Value Function Approximator (UVFA). A UVFA learns to predict the cumulative rewards between all state-goal pairs. However, empirically, the value function for long-range goals is always hard to estimate and may consequently result in failed policy. This has presented challenges to the learning process and the capability of neural networks. We propose a method to address this issue in large MDPs with sparse rewards, in which exploration and routing across remote states are both extremely challenging. Our method explicitly models the environment in a hierarchical manner, with a high-level dynamic landmark-based map abstracting the visited state space, and a low-level value network to derive precise local decisions. We use farthest point sampling to select landmark states from past experience, which has improved exploration compared with simple uniform sampling. Experimentally we showed that our method enables the agent to reach long-range goals at the early training stage, and achieve better performance than standard RL algorithms for a number of challenging tasks.	[Huang, Zhiao; Liu, Fangchen; Su, Hao] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Huang, ZA (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	z2huang@eng.ucsd.edu; fliu@eng.ucsd.edu; haosu@eng.ucsd.edu						Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Brockman G., 2016, OPENAI GYM; Bullo F., 2004, TEXTS APPL MATH, V49; De Silva V., 2004, TECHNICAL REPORT; Duan Yan, 2016, ABS160406778 CORR; Faust A, 2018, IEEE INT CONF ROBOT, P5113; Goldberg AV, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P156; Hafner D., 2018, ARXIV181104551; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Henaff M., 2017, ARXIV170507177; Ichter B., 2018, ABS180710366 CORR; Kavraki L., 1994, PROBABILISTIC ROADMA, V1994; Ke Nan Rosemary, 2018, MODELING LONG TERM F; Klamt T., 2019, ARXIV190302308; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; LaValle S. M., 1998, RAPIDLY EXPLORING RA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Levy A., 2018, ARXIV180508180; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mao Jiayuan, 2018, UNIVERSAL AGENT DISE; Mnih V., 2013, ARXIV PREPRINT ARXIV; Moore A.W., 1990, TECHNICAL REPORT; Nachum O., 2018, NEURIPS; Oh Junhyuk, 2017, NIPS; Plappert M, 2018, ABS180209464 CORR; Pong Vitchyr, 2018, ABS180209081 CORR; Qureshi Ahmed H., 2018, ABS180910252 CORR; Savinov N., 2018, ARXIV180300653; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srinivas A., 2018, ABS180400645 CORR; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Yu Tianhe, 2019, ABS190205542 CORR; Zhang A., 2018, ARXIV180300512	37	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301088
C	Hwang, G; Kim, S; Bae, HM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hwang, Gunpil; Kim, Seohyeon; Bae, Hyeon-Min			Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPECTROGRAM CORRELATION; TARGET RECOGNITION; SONAR; SENSOR; LOCALIZATION; RANGE; MODEL	In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899, and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.	[Hwang, Gunpil; Kim, Seohyeon; Bae, Hyeon-Min] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Hwang, G (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	gphwang@kaist.ac.kr; dddokman@kaist.ac.kr; hmbae@kaist.ac.kr						AKBARALLY H, 1995, IEEE INT CONF ROBOT, P3003, DOI 10.1109/ROBOT.1995.525710; Altes Richard A, 1970, TECHNICAL REPORT; Bates ME, 2011, SCIENCE, V333, P627, DOI 10.1126/science.1202065; Bin Tahir Muqaddas, 2012, GLOBAL J RES ENG, V12; Boonman A, 2013, FRONT PHYSIOL, V4, DOI 10.3389/fphys.2013.00248; Boufounos P, 2011, INT CONF ACOUST SPEE, P5972; Cahlander David Allen, 1964, TECHNICAL REPORT; Covey Ellen, 1995, Springer Handbook of Auditory Research, V5, P235; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Denny M., 2007, BLIP PING BUZZ MAKIN; Devaud F, 2006, ULTRASON, P1381; Devaud F, 2004, ULTRASON, P2041; DROR IE, 1995, NEURAL NETWORKS, V8, P149, DOI 10.1016/0893-6080(94)00057-S; Griffin D. R., 1958, LISTENING DARK ACOUS; HESON Jr OW, 1987, NATL GEOGR RES, V3, P82; Kaniak G, 2008, IEEE IMTC P, P842, DOI 10.1109/IMTC.2008.4547154; Kingma D.P, P 3 INT C LEARNING R; KOBER R, 1990, J ACOUST SOC AM, V87, P882, DOI 10.1121/1.398898; Kutila M, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P265, DOI 10.1109/ITSC.2016.7795565; McCann M.T., 2017, ARXIV171004011; Miller L.A., 1988, ANIMAL SONAR, P803; Moebus M, 2009, IN-VEHICLE CORPUS AND SIGNAL PROCESSING FOR DRIVER BEHAVIOR, P137, DOI 10.1007/978-0-387-79582-9_11; Nair V, 2010, P 27 INT C MACHINE L, P807; Neuweiler G., 2000, BIOL BATS; Ochoa A, 2009, IEEE T INSTRUM MEAS, V58, P3031, DOI 10.1109/TIM.2009.2016820; Peremans H, 1998, J ACOUST SOC AM, V104, P1101, DOI 10.1121/1.423326; Reijniers J, 2007, IEEE T ROBOT, V23, P1151, DOI 10.1109/TRO.2007.907487; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; SAILLANT PA, 1993, J ACOUST SOC AM, V94, P2691, DOI 10.1121/1.407353; Schnitzler H.-U., 1983, P235; Schnupp J., 2011, AUDITORY NEUROSCIENC; Simmons J.A., 1980, ANIMAL SONAR SYSTEMS, P695; SIMMONS JA, 1989, COGNITION, V33, P155, DOI 10.1016/0010-0277(89)90009-7; SIMMONS JA, 1973, J ACOUST SOC AM, V54, P157, DOI 10.1121/1.1913559; SIMMONS JA, 1989, J ACOUST SOC AM, V86, P1333, DOI 10.1121/1.398694; SIMMONS JA, 1980, J COMP PHYSIOL, V135, P61, DOI 10.1007/BF00660182; Simmons JA, 1995, NEURAL NETWORKS, V8, P1239, DOI 10.1016/0893-6080(95)00059-3; Simmons JA, 2012, CURR OPIN NEUROBIOL, V22, P311, DOI 10.1016/j.conb.2012.02.007; Skolnik M. I., 1970, RADAR HDB; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Steckel J, 2013, IEEE T ROBOT, V29, P161, DOI 10.1109/TRO.2012.2221313; STROTHER GK, 1961, J ACOUST SOC AM, V33, P696, DOI 10.1121/1.1908771; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; WATANABE S, 1992, IEEE T ROBOTIC AUTOM, V8, P240, DOI 10.1109/70.134277; Wohlgemuth MJ, 2016, CURR OPIN NEUROBIOL, V41, P78, DOI 10.1016/j.conb.2016.08.002; Yang Chang-sheng, 2008, Journal of System Simulation, V20, P5324; Yang CS, 2014, 2014 7TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP 2014), P943, DOI 10.1109/CISP.2014.7003914; YOUNG ED, 1992, PHILOS T ROY SOC B, V336, P407, DOI 10.1098/rstb.1992.0076; Zhang L, 2012, EVID-BASED COMPL ALT, V2012, P1, DOI 10.1155/2012/678107; Zhu B, 2018, NATURE, V555, P487, DOI 10.1038/nature25988	50	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303068
C	Igl, M; Ciosek, K; Li, YZ; Tschiatschek, S; Zhang, C; Devlin, S; Hofmann, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Igl, Maximilian; Ciosek, Kamil; Li, Yingzhen; Tschiatschek, Sebastian; Zhang, Cheng; Devlin, Sam; Hofmann, Katja			Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The ability for policies to generalize to new environments is key to the broad application of RL agents. A promising approach to prevent an agent's policy from overfitting to a limited set of training environments is to apply regularization techniques originally developed for supervised learning. However, there are stark differences between supervised learning and RL. We discuss those differences and propose modifications to existing regularization techniques in order to better adapt them to RL. In particular, we focus on regularization techniques relying on the injection of noise into the learned function, a family that includes some of the most widely used approaches such as Dropout and Batch Normalization. To adapt them to RL, we propose Selective Noise Injection (SNI), which maintains the regularizing effect the injected noise has, while mitigating the adverse effects it has on the gradient quality. Furthermore, we demonstrate that the Information Bottleneck (IB) is a particularly well suited regularization technique for RL as it is effective in the low-data regime encountered early on in training RL agents. Combining the IB with SNI, we significantly outperform current state of the art results, including on the recently proposed generalization benchmark Coinrun.	[Igl, Maximilian] Univ Oxford, Oxford, England; [Igl, Maximilian; Ciosek, Kamil; Li, Yingzhen; Tschiatschek, Sebastian; Zhang, Cheng; Devlin, Sam; Hofmann, Katja] Microsoft Res, Cambridge, MD USA	University of Oxford; Microsoft	Igl, M (corresponding author), Univ Oxford, Oxford, England.				UK EPSRC CDT on Autonomous Intelligent Machines and Systems	UK EPSRC CDT on Autonomous Intelligent Machines and Systems(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We would like to thank Shimon Whiteson for his helpful feedback, Sebastian Lee, Luke Harris, Hiske Overweg and Patrick Fernandes for help with experimental evaluations and Adrian O'Grady, Jaroslaw Rzepecki and Andre Kramer for help with the computing infrastructure. M. Igl is supported by the UK EPSRC CDT on Autonomous Intelligent Machines and Systems.	Al- Shedivat Maruan, 2018, INT C LEARN REPR; Alemi Alexander A., 2017, 5 INT C LEARN REPR I; [Anonymous], 2019, INT C LEARN REPR; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2016, CORR; Antonova Rika, 2017, ARXIV170300472; Beeching Edward, 2019, CORR; Belkin M., 2018, ARXIV181211118; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bontrager Philip, 2018, ARXIV180610729; Brutzkus A., 2018, CORR; Chaplot D. S., 2016, NIPS DEEP REINF LEAN; Chevalier-Boisvert M., 2018, GITHUB REPOSITORY; Clavera Ignasi, 2018, CORR; Cobbe K., 2019, P 36 INT C MACH LEAR; DeVries T., 2017, P 2017 COMPUTER VISI; Duan Y., 2016, RL2 FAST REINFORCEME; Goyal A, 2019, INT C LEARN REPR, P1; Henderson P., 2018, 32 AAAI C ART INT; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Johnson M., 2016, P 25 INT JOINT C ART, P4246; Juliani Arthur, 2019, CORR; Kanagawa Yuji, 2019, ARXIV190408129; Kansky K, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2014, P INT C LEARN REPR; Koos S, 2013, IEEE T EVOLUT COMPUT, V17, P122, DOI 10.1109/TEVC.2012.2185849; Levine S, 2018, INT J ROBOT RES, V37, P421, DOI 10.1177/0278364917710318; Mandlekar A, 2017, IEEE INT C INT ROBOT, P3932; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V., 2013, ARXIV PREPRINT ARXIV; Nichol A., 2018, ARXIV180403720; Peng XB, 2018, IEEE INT CONF ROBOT, P3803, DOI 10.1109/ICCABS.2018.8541936; Pinto L, 2017, PR MACH LEARN RES, V70; Poupart P., 2006, ICML, P697; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rajeswaran A., 2017, 5 INT C LEARN REPR I; Rajeswaran A., 2017, ADV NEURAL INFORM PR, P6550; Rakelly K., 2019, P 36 INT C MACH LEAR; Sadeghi F, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J., 2017, ABS170706347 CORR; Schulman J., 2016, 4 INT C LEARN REPR I; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Song D., 2018, ARXIV181012282; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Srouji Mario, 2018, INT C MACH LEARN, P4749; Stulp Freek, 2011, 2011 IEEE International Conference on Robotics and Automation, P5703; Sung F., 2017, ARXIV170609529; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tishby Naftali, 2000, P 37 ANN ALL C COMM, Patent No. [physics/0004057, 0004057]; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Whiteson S., 2011, 2011 IEEE S AD DYN P, P120, DOI DOI 10.1109/ADPRL.2011.5967363; Wydmuch M., 2018, IEEE T GAMES; Yu WH, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII; Zambaldi V., 2019, INT C LEARN REPR; Zeng HQ, 2017, PROC INT CONF RECON; Zhang A., 2018, ARXIV181106032; Zhang A., 2018, ARXIV180607937; Zhang C., 2018, ARXIV180406893; Zhao C., 2019, ARXIV190207015	65	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905061
C	Ismail, AA; Gunady, M; Pessoa, L; Bravo, HC; Feizi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ismail, Aya Abdelsalam; Gunady, Mohamed; Pessoa, Luiz; Bravo, Hector Corrada; Feizi, Soheil			Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent efforts to improve the interpretability of deep neural networks use saliency to characterize the importance of input features to predictions made by models. Work on interpretability using saliency-based methods on Recurrent Neural Networks (RNNs) has mostly targeted language tasks, and their applicability to time series data is less understood. In this work we analyze saliency-based methods for RNNs, both classical and gated cell architectures. We show that RNN saliency vanishes over time, biasing detection of salient features only to later time steps and are, therefore, incapable of reliably detecting important features at arbitrary time intervals. To address this vanishing saliency problem, we propose a novel RNN cell structure (input-cell attention(dagger)), which can extend any RNN cell architecture. At each time step, instead of only looking at the current input vector, input-cell attention uses a fixed-size matrix embedding, each row of the matrix attending to different inputs from current or previous time steps. Using synthetic data, we show that the saliency map produced by the input-cell attention RNN is able to faithfully detect important features regardless of their occurrence in time. We also apply the input-cell attention RNN on a neuroscience task analyzing functional Magnetic Resonance Imaging (fMRI) data for human subjects performing a variety of tasks. In this case, we use saliency to characterize brain regions (input features) for which activity is important to distinguish between tasks. We show that standard RNN architectures are only capable of detecting important brain regions in the last few time steps of the fMRI data, while the input-cell attention model is able to detect important brain region activity across time without latter time step biases.	[Ismail, Aya Abdelsalam; Gunady, Mohamed; Bravo, Hector Corrada; Feizi, Soheil] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Pessoa, Luiz] Univ Maryland, Dept Psychol, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Ismail, AA (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	asalam@cs.umd.edu; mgunady@cs.umd.edu; pessoa@umd.edu; hcorrada@umiacs.umd.edu; sfeizi@cs.umd.edu		Gunady, Mohamed/0000-0002-5027-9279	U.S. National Institutes of Health [R01GM114267]; NSF [CDSE:1854532]	U.S. National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This work was supported by the U.S. National Institutes of Health grant [R01GM114267] and NSF award [CDS&E:1854532]. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.	Ancona Marco, 2018, ICLR, DOI DOI 10.1109/TNSE.2020.2996738; Arras Leila, 2017, WORKSH COMP APPR SUB; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Guo T, 2019, PR MACH LEARN RES, V97; Hasani R., 2019, P 2019 INT JOINT C N, P1; Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOCHREITER S., 1998, INT J UNCERTAINTY FU; Joffe Sergey, 2010, IEEE INT C DAT MIN; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Karpathy A., 2015, ARXIV150602078; Lin Z., 2017, ARXIV PREPRINT ARXIV; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; Samek W., 2016, IEEE T NEURAL NETWOR; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shrikumar A, 2017, PR MACH LEARN RES, V70; Smilkov D, 2017, ARXIV; Springenberg J. T, 2015, ARXIV PREPRINT ARXIV; Strobelt Hendrik, 2017, IEEE T VISUALIZATION; Sundararajan M, 2017, PR MACH LEARN RES, V70; Thomas A.W., 2018, ARXIV181009945; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zintgraf Luisa M., 2017, P ICLR	31	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902044
C	Ito, SJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ito, Shinji			Submodular Function Minimization with Noisy Evaluation Oracle	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVEX-OPTIMIZATION; ALGORITHM; POWER	This paper considers submodular function minimization with noisy evaluation oracles that return the function value of a submodular objective with zero-mean additive noise. For this problem, we provide an algorithm that returns an O(n(3/2)/root T)-additive approximate solution in expectation, where n and T stand for the size of the problem and the number of oracle calls, respectively. There is no room for reducing this error bound by a factor smaller than O(1/root n). Indeed, we show that any algorithm will suffer additive errors of Omega(n/root T) in the worst case. Further, we consider an extended problem setting with multiple-pointfeedback in which we can get the feedback of k function values with each oracle call. Under the additional assumption that each noisy oracle is submodular and that 2 <= k O(1), we provide an algorithm with an O(n/root T)-additive error bound as well as a worst-case analysis including a lower bound of Omega(n/root T), which together imply that the algorithm achieves an optimal error bound up to a constant.	[Ito, Shinji] Univ Tokyo, NEC Corp, Tokyo, Japan	NEC Corporation; University of Tokyo	Ito, SJ (corresponding author), Univ Tokyo, NEC Corp, Tokyo, Japan.	i-shinji@nec.com			JST, ACT-I, Japan [JPMJPR18U5]	JST, ACT-I, Japan	This work was supported by JST, ACT-I, Grant Number JPMJPR18U5, Japan.	Agarwal A., 2010, P COLT, P28; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Axelrod B, 2020, PROCEEDINGS OF THE THIRTY-FIRST ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA'20), P837; Bach F., 2010, ARXIV10104207; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Belloni A., 2015, C LEARN THEOR, P240; Blais E, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2113; Bubeck S, 2017, ACM S THEORY COMPUT, P72, DOI 10.1145/3055399.3055403; Cardei I, 2011, ANN IEEE SYST CONF, P353; Chakrabarty D., 2014, ADV NEURAL INFORM PR, V1, P802, DOI 10.48550/arXiv.1411.0095; Chakrabarty D, 2017, ACM S THEORY COMPUT, P1220, DOI 10.1145/3055399.3055419; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dadush D, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P832; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; FUJISHIGE S, 1980, MATH OPER RES, V5, P186, DOI 10.1287/moor.5.2.186; Fujishige S, 2011, PAC J OPTIM, V7, P3; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Halabi M. E., 2019, ARXIV190512145; Hassani H, 2017, ADV NEURAL INFORM PR, P5841; Hassidim A., 2017, COLT, P1069; Hazan E., 2014, ADV NEURAL INFORM PR, V27, P784; Hazan E, 2012, J MACH LEARN RES, V13, P2903; Ito S., 2016, ADV NEURAL INFORM PR, P3855; Iwata S, 2003, SIAM J COMPUT, V32, P833, DOI 10.1137/S0097539701397813; Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096; Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589; Jegelka S.., 2011, P 28 INT C MACHINE L, P345; Karimi M., 2017, ADV NEURAL INFORM PR, P6853; Kohli P, 2010, STUD COMPUT INTELL, V285, P51; Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Mokhtari A., 2018, C ART INT STAT, V84, P1886; Orlin JB, 2009, MATH PROGRAM, V118, P237, DOI 10.1007/s10107-007-0189-2; Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989; Shamir O., 2013, P C LEARN THEOR, P3; Shamir O, 2017, J MACH LEARN RES, V18; Singla A., 2016, 30 AAAI C ART INT; WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903069
C	Ito, S; Hatano, D; Sumita, H; Takemura, K; Fukunaga, T; Kakimura, N; Kawarabayashi, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ito, Shinji; Hatano, Daisuke; Sumita, Hanna; Takemura, Kei; Fukunaga, Takuro; Kakimura, Naonori; Kawarabayashi, Ken-ichi			Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose computationally efficient algorithms for online linear optimization with bandit feedback, in which a player chooses an action vector from a given (possibly infinite) set A subset of R-d, and then suffers a loss that can be expressed as a linear function in action vectors. Although existing algorithms achieve an optimal regret bound of (O) over tilde(root T) for T rounds (ignoring factors of poly(d, log T)), computationally efficient ways of implementing them have not yet been specified, in particular when vertical bar A vertical bar is not bounded by a polynomial size in d. A standard way to pursue computational efficiency is to assume that we have an efficient algorithm referred to as oracle that solves (offline) linear optimization problems over A. Under this assumption, the computational efficiency of a bandit algorithm can then be measured in terms of oracle complexity, i.e., the number of oracle calls. Our contribution is to propose algorithms that offer optimal regret bounds of (O) over tilde(root T) as well as low oracle complexity for both non -stochastic settings and stochastic settings. Our algorithm for non -stochastic settings has an oracle complexity of (O) over tilde(root T) and is the first algorithm that achieves both a regret bound of (O) over tilde(root T) and an oracle complexity of a (O) over tilde (poly(T)), given only linear optimization oracles. Our algorithm for stochastic settings calls the oracle only O(poly(d, log T)) times, which is smaller than the current best oracle complexity of O(T) if T is sufficiently large.	[Ito, Shinji; Takemura, Kei] NEC Corp Ltd, Tokyo, Japan; [Ito, Shinji] Univ Tokyo, Tokyo, Japan; [Hatano, Daisuke; Fukunaga, Takuro] RIKEN AIP, Tokyo, Japan; [Sumita, Hanna] Tokyo Metropolitan Univ, Tokyo, Japan; [Fukunaga, Takuro] Chuo Univ, Tokyo, Japan; [Fukunaga, Takuro] JST PRESTO, Kawaguchi, Saitama, Japan; [Kakimura, Naonori] Keio Univ, Tokyo, Japan; [Kawarabayashi, Ken-ichi] Natl Inst Informat, Tokyo, Japan	NEC Corporation; University of Tokyo; RIKEN; Tokyo Metropolitan University; Chuo University; Japan Science & Technology Agency (JST); Keio University; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Ito, SJ (corresponding author), NEC Corp Ltd, Tokyo, Japan.; Ito, SJ (corresponding author), Univ Tokyo, Tokyo, Japan.	i-shinji@nec.com; daisuke.hatano@riken.jp; sumita@tmu.ac.jp; kei_takemura@nec.com; fukunaga.07s@g.chuo-u.ac.jp; kakimura@math.keio.ac.jp; k-keniti@nii.ac.jp			JST, ERATO, Japan [JPMJER1201]; JST, ACT-I, Japan [JPMJPR18U5]; JST, PRESTO, Japan [JPMJPR1759]; JSPS, KAKENHI, Japan [JP18H05291]	JST, ERATO, Japan(Japan Science & Technology Agency (JST)); JST, ACT-I, Japan; JST, PRESTO, Japan; JSPS, KAKENHI, Japan(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by JST, ERATO, Grant Number JPMJER1201, Japan.; This work was supported by JST, ACT-I, Grant Number JPMJPR18U5, Japan.; This work was supported by JST, PRESTO, Grant Number JPMJPR1759, Japan.; This work was supported by JSPS, KAKENHI, Grant Number JP18H05291, Japan.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abeille M, 2017, ELECTRON J STAT, V11, P5165, DOI 10.1214/17-EJS1341SI; Abernethy J. D., 2008, C LEARN THEOR; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Bubeck S, 2017, ACM S THEORY COMPUT, P72, DOI 10.1145/3055399.3055403; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Cohen A., 2017, C LEARN THEOR, P629; Combes R., 2015, P 28 INT C NEUR INF, P2116; Cover T.M., 2011, KELLY CAPITAL GROWTH, VVolume 3, P181; Dani V, 2008, P C LEARN THEOR COLT, P355; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; Dani V, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P937, DOI 10.1145/1109557.1109660; Garber Dan, 2017, ADV NEURAL INFORM PR, P627; Hazan E., 2018, ADV NEURAL INFORM PR, P5652; Hazan E., 2016, J MACHINE LEARNING R, V17, P4062; Ito S., 2019, ADV NEURAL INFORM PR; Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Lattimore T., 2019, PREPRINT; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; McMahan HB, 2004, LECT NOTES COMPUT SC, V3120, P109, DOI 10.1007/978-3-540-27819-1_8; Nesterov Y., 1994, INTERIOR POINT POLYN, V13; PREKOPA A, 1971, ACTA SCI MATH, V32, P301; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Schrijver A., 1998, THEORY LINEAR INTEGE; Valko M, 2014, PR MACH LEARN RES, V32, P46; Vovk V. G., 1990, P COMP LEARN THEOR 1	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902024
C	Ito, S; Hatano, D; Sumita, H; Takemura, K; Fukunaga, T; Kakimura, N; Kawarabayashi, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ito, Shinji; Hatano, Daisuke; Sumita, Hanna; Takemura, Kei; Fukunaga, Takuro; Kakimura, Naonori; Kawarabayashi, Ken-ichi			Improved Regret Bounds for Bandit Combinatorial Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bandit combinatorial optimization is a bandit framework in which a player chooses an action within a given finite set A subset of {0, 1}(d) and incurs a loss that is the inner product of the chosen action and an unobservable loss vector in R-d in each round. In this paper, we aim to reveal the property, which makes the bandit combinatorial optimization hard. Recently, Cohen et al. [8] obtained a lower bound Omega(root dk(3)T/logT) of the regret, where k is the maximum l(1)-norm of action vectors, and T is the number of rounds. This lower bound was achieved by considering a continuous strongly-correlated distribution of losses. Our main contribution is that we managed to improve this bound by Omega(root dk(3)T) through applying a factor of root log T, which can be done by means of strongly-correlated losses with binary values. The bound derives better regret bounds for three specific examples of the bandit combinatorial optimization: the multitask bandit, the bandit ranking and the multiple-play bandit. In particular, the bound obtained for the bandit ranking in the present study addresses an open problem raised in [8]. In addition, we demonstrate that the problem becomes easier without considering correlations among entries of loss vectors. In fact, if each entry of loss vectors is an independent random variable, then, one can achieve a regret of (O) over tilde(root dk(2)T), which is root k times smaller than the lower bound shown above. The observed results indicated that correlation among losses is the reason for observing a large regret.	[Ito, Shinji; Takemura, Kei] NEC Corp Ltd, Tokyo, Japan; [Ito, Shinji] Univ Tokyo, Tokyo, Japan; [Hatano, Daisuke; Fukunaga, Takuro] RIKEN AIP, Tokyo, Japan; [Sumita, Hanna] Tokyo Metropolitan Univ, Tokyo, Japan; NEC Corp Ltd, Tokyo, Japan; [Fukunaga, Takuro] Chuo Univ, Hachioji, Tokyo, Japan; [Fukunaga, Takuro] JST PRESTO, Tokyo, Japan; [Kakimura, Naonori] Keio Univ, Tokyo, Japan; [Kawarabayashi, Ken-ichi] Natl Inst Informat, Tokyo, Japan	NEC Corporation; University of Tokyo; RIKEN; Tokyo Metropolitan University; NEC Corporation; Chuo University; Japan Science & Technology Agency (JST); Keio University; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Ito, SJ (corresponding author), NEC Corp Ltd, Tokyo, Japan.; Ito, SJ (corresponding author), Univ Tokyo, Tokyo, Japan.	i-shinji@nec.com; daisuke.hatano@riken.jp; sumita@tmu.ac.jp; kei_takemura@nec.com; fukunaga.07s@g.chuo-u.ac.jp; kakimura@math.keio.ac.jp; k-keniti@nii.ac.jp			JST, ERATO, Japan [JPMJER1201]; JST, ACT-I, Japan [JPMJPR18U5]; JST, PRESTO, Japan [JPMJPR1759]; JSPS, KAKENHI, Japan [JP18H05291]	JST, ERATO, Japan(Japan Science & Technology Agency (JST)); JST, ACT-I, Japan; JST, PRESTO, Japan; JSPS, KAKENHI, Japan(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by JST, ERATO, Grant Number JPMJER1201, Japan.; This work was supported by JST, ACT-I, Grant Number JPMJPR18U5, Japan.; This work was supported by JST, PRESTO, Grant Number JPMJPR1759, Japan.; This work was supported by JSPS, KAKENHI, Grant Number JP18H05291, Japan.	Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P3; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Bubeck Sebastien, 2012, P ANN C LEARN THEORY, V23; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Cohen A., 2017, C LEARN THEOR, P629; Combes R., 2015, P 28 INT C NEUR INF, P2116; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; Gyorgy A, 2007, J MACH LEARN RES, V8, P2369; Hazan E., 2016, J MACHINE LEARNING R, V17, P4062; Helmbold DP, 2009, J MACH LEARN RES, V10, P1705; Ito S., 2019, ADV NEURAL INFORM PR; Komiyama J, 2015, PR MACH LEARN RES, V37, P1152; Kotlowski Wojciech, 2019, C LEARN THEOR COLT 2, P1994; Lagree Paul, 2016, ADV NEURAL INFORM PR, P1597; Lattimore T., 2019, PREPRINT; Lattimore T., 2018, ADV NEURAL INFORM PR; McMahan HB, 2004, LECT NOTES COMPUT SC, V3120, P109, DOI 10.1007/978-3-540-27819-1_8; Sakaue S., 2018, PMLR, P585; Uchiya T, 2010, LECT NOTES ARTIF INT, V6331, P375, DOI 10.1007/978-3-642-16108-7_30; Valko M, 2014, PR MACH LEARN RES, V32, P46	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903064
C	Jaber, A; Zhang, JJ; Bareinboim, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jaber, Amin; Zhang, Jiji; Bareinboim, Elias			Identification of Conditional Causal Effects under Markov Equivalence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Causal identification is the problem of deciding whether a post-interventional distribution is computable from a combination of qualitative knowledge about the data-generating process, which is encoded in a causal diagram, and an observational distribution. A generalization of this problem restricts the qualitative knowledge to a class of Markov equivalent causal diagrams, which, unlike a single, fully-specified causal diagram, can be inferred from the observational distribution. Recent work by (Jaber et al., 2019a) devised a complete algorithm for the identification of unconditional causal effects given a Markov equivalence class of causal diagrams. However, there are identifiable conditional causal effects that cannot be handled by that algorithm. In this work, we derive an algorithm to identify conditional effects, which are particularly useful for evaluating conditional plans or policies.	[Jaber, Amin] Purdue Univ, W Lafayette, IN 47907 USA; [Zhang, Jiji] Lingnan Univ, Hong Kong, Peoples R China; [Bareinboim, Elias] Columbia Univ, New York, NY 10027 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Lingnan University; Columbia University	Jaber, A (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	jaber0@purdue.edu; jijizhang@ln.edu.hk; eb@cs.columbia.edu			NSF [IIS-1704352, IIS-1750807]; IBM Research; Adobe Research; Research Grants Council of Hong Kong under the General Research Fund [LU13602818]	NSF(National Science Foundation (NSF)); IBM Research(International Business Machines (IBM)); Adobe Research; Research Grants Council of Hong Kong under the General Research Fund	Bareinboim and Jaber are supported in parts by grants from NSF IIS-1704352, IIS-1750807 (CAREER), IBM Research, and Adobe Research. Zhang's research was supported in part by the Research Grants Council of Hong Kong under the General Research Fund LU13602818.	Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Hyttinen A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P395; Jaber A., 2018, IJCAI 18, P5024; Jaber A., 2019, R50 COL U COL CAUS A; Jaber A., 2019, P 36 INT C MACH LEAR; Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.1093/biomet/82.4.669; Pearl J., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P444; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Richardson T, 2002, ANN STAT, V30, P962; Shpitser I, 2006, P 22 C UNCERTAINTY A, P437, DOI [10.48550/ARXIV.1206.6876, DOI 10.48550/ARXIV.1206.6876]; Spirtes P., 2000, CAUSATION PREDICTION; Spirtes Peter, 2001, CAUSATION PREDICTION, V81; Tian J, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P567; Tian J., 2004, P 20 C UNC ART INT, V20, P561; Verma T., 1993, R191 UCLA; Zhang J., 2007, P 11 INT C ART INT S, P667; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001; Zhang JJ, 2008, J MACH LEARN RES, V9, P1437	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903018
C	Jain, L; Jamieson, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jain, Lalit; Jamieson, Kevin			A New Perspective on Pool-Based Active Classification and False-Discovery Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BOUNDS	In many scientific settings there is a need for adaptive experimental design to guide the process of identifying regions of the search space that contain as many true positives as possible subject to a low rate of false discoveries (i.e. false alarms). Such regions of the search space could differ drastically from a predicted set that minimizes 0/1 error and accurate identification could require very different sampling strategies. Like active learning for binary classification, this experimental design cannot be optimally chosen a priori, but rather the data must be taken sequentially and adaptively. However, unlike classification with 0/1 error, collecting data adaptively to find a set with high true positive rate and low false discovery rate (FDR) is not as well understood. In this paper we provide the first provably sample efficient adaptive algorithm for this problem. Along the way we highlight connections between classification, combinatorial bandits, and FDR control making contributions to each.	[Jain, Lalit; Jamieson, Kevin] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Jain, L (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.	lalitj@cs.washington.edu; jamieson@cs.washington.edu						Agarwal S, 2005, J MACH LEARN RES, V6, P393; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Bardenet R, 2015, BERNOULLI, V21, P1361, DOI 10.3150/14-BEJ605; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Bennett PN, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P711, DOI 10.1145/3018661.3018730; Beygelzimer Alina, 2008, ARXIV08124952; Boucheron S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bousquet O, 2002, CR MATH, V334, P495, DOI 10.1016/S1631-073X(02)02292-6; Boyd Kendrick, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P451, DOI 10.1007/978-3-642-40994-3_29; Camacho DM, 2018, CELL, V173, P1581, DOI 10.1016/j.cell.2018.05.015; Cao T., 2017, ARXIV171108018; Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189; Castro RM, 2014, BERNOULLI, V20, P2217, DOI 10.3150/13-BEJ555; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING; Chen L., 2017, C LEARN THEOR, V65, P482; Chen LJ, 2017, PR MACH LEARN RES, V54, P101; Chen S., 2014, P ADV NEUR INF PROC, P379; Dasgupta A, 2006, PROCEEDINGS OF THE ASIA-PACIFIC CONFERENCE ON LIBRARY & INFORMATION EDUCATION & PRACTICE 2006, P239; Dasgupta S., 2008, ADV NEURAL INFORM PR, P353; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Gabillon V, 2016, JMLR WORKSH CONF PRO, V51, P1004; Garnett R., 2015, ADV NEURAL INFORM PR, V28, P2755; Hanneke S, 2007, LECT NOTES COMPUT SC, V4539, P66, DOI 10.1007/978-3-540-72927-3_7; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; He C, 2016, 2016 16th International Workshop on Junction Technology (IWJT), P64, DOI 10.1109/IWJT.2016.7486675; Jamieson K., 2014, C LEARN THEOR, P423; Jamieson Kevin, 2018, ADV NEURAL INFORM PR; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Naik AW, 2016, ELIFE, V5, DOI 10.7554/eLife.10047; Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298; Rocklin GJ, 2017, SCIENCE, V357, P168, DOI 10.1126/science.aan0693; Sabharwal A, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P825; Sawade Christoph, 2010, ADV NEURAL INFORM PR, V23, P2083; Settles B., 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI [10.2200/s00429ed1v01y201207aim018, DOI 10.2200/S00429ED1V01Y201207AIM018]; Simchowitz M., 2017, C LEARN THEOR, V65, P1794; Sverchkov Y, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005466; Tallorin L, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07717-6; Yang F, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ELECTRONIC PACKAGING TECHNOLOGY (ICEPT), P1595, DOI 10.1109/ICEPT.2017.8046741; Zhang L, 2017, DRUG DISCOV TODAY, V22, P1680, DOI 10.1016/j.drudis.2017.08.010; Zhang Martin J, 2019, ARXIV190200197	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905064
C	Jalali, S; Nuzman, C; Saniee, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jalali, Shirin; Nuzman, Carl; Saniee, Iraj			Efficient Deep Approximation of GMMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The universal approximation theorem states that any regular function can be approximated closely using a single hidden layer neural network. Some recent work has shown that, for some special functions, the number of nodes in such an approximation could be exponentially reduced with multi-layer neural networks. In this work, we extend this idea to a rich class of functions, namely the discriminant functions that arise in optimal Bayesian classification of Gaussian mixture models (GMMs) in R-n. We show that such functions can be approximated with arbitrary precision using O(n) nodes in a neural network with two hidden layers (deep neural network), while in contrast, a neural network with a single hidden layer (shallow neural network) would require at least O(exp(n)) nodes or exponentially large coefficients. Given the universality of the Gaussian distribution in the feature spaces of data, e.g., in speech, image and text, our results shed light on the observed efficiency of deep neural networks in practical classification problems.	[Jalali, Shirin; Nuzman, Carl; Saniee, Iraj] Nokia, Bell Labs, 600-700 Mt Ave, Murray Hill, NJ 07974 USA	AT&T; Nokia Corporation; Nokia Bell Labs	Jalali, S (corresponding author), Nokia, Bell Labs, 600-700 Mt Ave, Murray Hill, NJ 07974 USA.	shirin.jalali@nokia-bell-labs.com; carl.nuzman@nokia-bell-labs.com; iraj.saniee@nokia-bell-labs.com						Abbe E., 2018, ARXIV181206369; BARRON AR, 1994, MACH LEARN, V14, P115, DOI 10.1023/A:1022650905902; Bengio, 2011, ADV NEURAL INFORM PR, P666, DOI DOI 10.5555/2986459.2986534; Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245, DOI 10.1145/502512.502546; Cohen N., 2015, ARXIV150905009; Cybenko George, 1989, MATH CONTROL SIGNAL, V2, P183, DOI DOI 10.1007/BF02551274; Daniely A., 2017, ARXIV170208489; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Indurkhya N., 2010, HDB NATURAL LANGUAGE, V2; Lin HW, 2017, J STAT PHYS, V168, P1223, DOI 10.1007/s10955-017-1836-5; Martens James, 2014, ARXIV14117717 ARXIV PREPRINT ARXIV; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361; Rolnick D., 2018, INT C LEAR REP; Shpilka A., 2010, ARITHMETIC CIRCUITS; Telgarsky M, 2015, ARXIV150908101V2CSLG; Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304054
C	Janz, D; Hron, J; Mazur, P; Hofmann, K; Hernandez-Lobato, JM; Tschiatschek, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Janz, David; Hron, Jiri; Mazur, Przemyslaw; Hofmann, Katja; Hernandez-Lobato, Jose Miguel; Tschiatschek, Sebastian			Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those.	[Janz, David; Hron, Jiri; Hernandez-Lobato, Jose Miguel] Univ Cambridge, Cambridge, England; [Mazur, Przemyslaw] Wayve Technol, London, England; [Hofmann, Katja; Hernandez-Lobato, Jose Miguel; Tschiatschek, Sebastian] Microsoft Res, Redmond, WA USA; [Hernandez-Lobato, Jose Miguel] Alan Turing Inst, London, England	University of Cambridge; Microsoft	Janz, D (corresponding author), Univ Cambridge, Cambridge, England.	dj343@cam.ac.uk; jh2084@cam.ac.uk		Tschiatschek, Sebastian/0000-0002-2592-0108	Nokia CASE Studentship	Nokia CASE Studentship(Nokia Corporation)	We thank Matej Balog and the anonymous reviewers for their helpful comments and suggestions. Jiri Hron acknowledges support by a Nokia CASE Studentship.	Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Azizzadenesheli K, 2018, ARXIV180204412; Ba J., 2017, P 3 INT C LEARN REPR; Barreto A, 2017, ADV NEURAL INFORM PR; Busoniu L, 2010, AUTOM CONTROL ENG SE, P1, DOI 10.1201/9781439821091-f; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Dearden R, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P761; Ecoffet A, 2019, ARXIV PREPRINT ARXIV; Gal Y., 2016, U CAMBRIDGE; Hessel M., 2018, AAAI C ART INT; Jonker T. M, 2017, DEEP REINFORCEMENT L; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Kulkarni T. D., 2016, ARXIV160602396; Levine N., 2017, ADV NEURAL INFORM PR; Lipton Z. C, 2018, AAAI C ART INT; Machado M., 2018, ARXIV180711622; Machado M. C, 2017, ARXIV171011089; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos R., 2016, ADV NEURAL INFORM PR, V29; O'Donoghue B, 2018, INT C MACH LEARN ICM; Osband I., 2016, ARXIV PREPRINT ARXIV; Osband I., 2016, ADV NEURAL INFORM PR; Osband Ian, 2013, ADV NEURAL INFORM PR; Osband Ian, 2016, INT C MACH LEARN ICM; Plappert M., 2018, INT C LEARN REPR ICL; Precup D., 2000, COMPUTER SCI DEP FAC; Schaul T, 2016, C TRACK P, P1; Strens M, 2000, C MACH LEARN ICML; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Touati A., 2018, ARXIV180602315; Van Hasselt H., 2016, AAAI C ART INT; Wang Z., 2016, INT C MACH LEARN ICM	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304050
C	Jia, S; Navidi, F; Nagarajan, V; Ravi, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jia, Su; Navidi, Fatemeh; Nagarajan, Viswanath; Ravi, R.			Optimal Decision Tree with Noisy Outcomes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GREEDY ALGORITHM	A fundamental task in active learning involves performing a sequence of tests to identify an unknown hypothesis that is drawn from a known distribution. This problem, known as optimal decision tree induction, has been widely studied for decades and the asymptotically best-possible approximation algorithm has been devised for it. We study a generalization where certain test outcomes are noisy, even in the more general case when the noise is persistent, i.e., repeating a test gives the same noisy output, disallowing simple repetition as a way to gain confidence. We design new approximation algorithms for both the non-adaptive setting, where the test sequence must be fixed a-priori, and the adaptive setting where the test sequence depends on the outcomes of prior tests. Previous work in the area assumed at most a logarithmic number of noisy outcomes per hypothesis and provided approximation ratios that depended on parameters such as the minimum probability of a hypothesis. Our new approximation algorithms provide guarantees that are nearly best-possible and work for the general case of a large number of noisy outcomes per test or per hypothesis where the performance degrades smoothly with this number. Our results adapt and generalize methods used for submodular ranking and stochastic set cover. We evaluate the performance of our algorithms on two natural applications with noise: toxic chemical identification and active learning of linear classifiers. Despite our theoretical logarithmic approximation guarantees, our methods give solutions with cost very close to the information theoretic minimum, demonstrating the effectiveness of our methods.	[Jia, Su; Ravi, R.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Navidi, Fatemeh; Nagarajan, Viswanath] Univ Michigan, Ann Arbor, MI 48109 USA	Carnegie Mellon University; University of Michigan System; University of Michigan	Jia, S (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	sjia1@andrew.cmu.edu; navidi@umich.edu; viswa@umich.edu; ravi@andrew.cmu.edu			NSF [CCF-1750127]	NSF(National Science Foundation (NSF))	Research of F. Navidi and V. Nagarajan partly supported by NSF grant CCF-1750127.	Adler M, 2008, LECT NOTES COMPUT SC, V5171, P1; Arkin EM, 1998, INT J COMPUT GEOM AP, V8, P343, DOI 10.1142/S0218195998000175; Azar Y, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1070; Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Bellala G., 2011, P 14 INT C ART INT S, V15, P155; Chakaravarthy VT, 2011, ACM T ALGORITHMS, V7, DOI 10.1145/1921659.1921661; Chakaravarthy VT, 2009, LECT NOTES COMPUT SC, V5555, P210, DOI 10.1007/978-3-642-02927-1_19; Chen YX, 2017, PR MACH LEARN RES, V54, P223; Cicalese F, 2014, PR MACH LEARN RES, V32; Dasgupta S, 2005, ADV NEURAL INFORM PR, P337; Garey M. R., 1974, Acta Informatica, V3, P347, DOI 10.1007/BF00263588; Golovin D., 2017, ABS10033967 CORR; Golovin D, 2010, ARON CULOTTA ADV NEU, P766; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Guillory A, 2011, INT C MACH LEARN; Guillory A, 2009, LECT NOTES ARTIF INT, V5809, P141, DOI 10.1007/978-3-642-04414-4_15; Guillory Andrew, 2010, P 27 INT C INT C MAC, P415; Gupta A, 2017, MATH OPER RES, V42, P876, DOI 10.1287/moor.2016.0831; Hanneke S., 2007, P 24 INT C MACH LEAR, P353, DOI [10.1145/1273496.1273541, DOI 10.1145/1273496.1273541]; Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8; Im S, 2016, ACM T ALGORITHMS, V13, DOI 10.1145/2987751; Javdani S, 2014, JMLR WORKSH CONF PRO, V33, P430; Kambadur P, 2017, LECT NOTES COMPUT SC, V10328, P317, DOI 10.1007/978-3-319-59250-3_26; Kosaraju SR, 1999, LECT NOTES COMPUT SC, V1663, P157; LOVELAND DW, 1985, ACTA INFORM, V22, P101, DOI 10.1007/BF00290148; Moshkov MJ, 2010, FUND INFORM, V104, P285, DOI 10.3233/FI-2010-350; Naghshvar M, 2012, ANN ALLERTON CONF, P1626, DOI 10.1109/Allerton.2012.6483415; Nan F, 2017, IEEE T INFORM THEORY, V63, P7612, DOI 10.1109/TIT.2017.2749505; Nowak R., 2009, ADV NEURAL INFORM PR, P1366; Saettler A, 2017, ALGORITHMICA, V79, P886, DOI 10.1007/s00453-016-0211-2; WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303031
C	Jiang, SL; Moseley, B; Garnett, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jiang, Shali; Moseley, Benjamin; Garnett, Roman			Cost effective active search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study a paradigm of active learning we call cost effective active search, where the goal is to find a given number of positive points from a large unlabeled pool with minimum labeling cost. Most existing methods solve this problem heuristically, and few theoretical results have been established. Here we adopt a principled Bayesian approach for the first time. We first derive the Bayesian optimal policy and establish a strong hardness result: the optimal policy is hard to approximate, with the best-possible approximation ratio bounded below by Omega(n(0.16)). We then propose an efficient and nonmyopic policy, simulating future search progress using the negative Poisson binomial distribution. We propose simple and fast approximations for its expectation, which serves as an essential role in our proposed policy. We conduct comprehensive experiments on drug and materials discovery datasets and demonstrate that our proposed method is superior to a popular (greedy) baseline.	[Jiang, Shali; Garnett, Roman] WUSTL, CSE, St Louis, MO 63130 USA; [Moseley, Benjamin] CMU, Tepper Sch Business, Pittsburgh, PA 15213 USA; [Moseley, Benjamin] Relational AI, Pittsburgh, PA 15213 USA	Washington University (WUSTL); Carnegie Mellon University	Jiang, SL (corresponding author), WUSTL, CSE, St Louis, MO 63130 USA.	jiang.s@wustl.edu; moseleyb@andrew.cmu.edu; garnett@wustl.edu			National Science Foundation (NSF) [IIA-1355406, IIS-1845434, OAC-1940224]; Google Research Award; NSF [CCF-1830711, CCF-1824303, CCF-1733873]	National Science Foundation (NSF)(National Science Foundation (NSF)); Google Research Award(Google Incorporated); NSF(National Science Foundation (NSF))	We would like to thank Mark Bober for providing support regarding computational services. SJ and RG were supported by the National Science Foundation (NSF) under award numbers IIA-1355406, IIS-1845434, and OAC-1940224. BM was supported by a Google Research Award and by the NSF under awards CCF-1830711, CCF-1824303, and CCF-1733873.	Charalambous C., 2014, EVOLUTION PARTICLE F; Chen SX, 1997, STAT SINICA, V7, P875; Chen Y., 2013, INT C MACHINE LEARNI, P160; Garnett R., 2012, P 29 INT C MACH LEAR; Garnett R, 2015, J COMPUT AID MOL DES, V29, P305, DOI 10.1007/s10822-015-9832-9; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Grossman M. R., 2010, RICH JL TECH, V17, P1; Grossman Maura R., 2016, TREC; Hu J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1107, DOI 10.1109/ROBIO.2018.8664854; Jiang S., 2018, MACH LEARN MOL MAT W; Jiang S., 2017, P 34 INT C MACH LEAR P 34 INT C MACH LEAR, P1714; Liebscher S, 2017, INT J PERF ANAL SPOR, V17, P666, DOI 10.1080/24748668.2017.1372162; Renders JM, 2018, LECT NOTES COMPUT SC, V10772, P722, DOI 10.1007/978-3-319-76941-7_68; Vanchinathan HP, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1195, DOI 10.1145/2783258.2783360; Warmuth MK, 2002, ADV NEUR IN, V14, P1449; Warmuth MK, 2003, J CHEM INF COMP SCI, V43, P667, DOI 10.1021/ci025620t; Yu Z, 2018, PROCEEDINGS OF THE 4TH ACM SIGSOFT INTERNATIONAL WORKSHOP ON NLP FOR SOFTWARE ENGINEERING (NL4SE '18), P10, DOI 10.1145/3283812.3283818	17	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304084
C	Jiang, YBY; Xu, QQ; Yang, ZY; Cao, XC; Huang, QM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jiang, Yangbangyan; Xu, Qianqian; Yang, Zhiyong; Cao, Xiaochun; Huang, Qingming			DM2C: Deep Mixed-Modal Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Data exhibited with multiple modalities are ubiquitous in real-world clustering tasks. Most existing methods, however, pose a strong assumption that the pairing information for modalities is available for all instances. In this paper, we consider a more challenging task where each instance is represented in only one modality, which we call mixed-modal data. Without any extra pairing supervision across modalities, it is difficult to find a universal semantic space for all of them. To tackle this problem, we present an adversarial learning framework for clustering with mixed-modal data. Instead of transforming all the samples into a joint modality-independent space, our framework learns the mappings across individual modality spaces by virtue of cycle-consistency. Through these mappings, we could easily unify all the samples into a single modality space and perform the clustering. Evaluations on several real-world mixed-modal datasets could demonstrate the superiority of our proposed framework.	[Jiang, Yangbangyan; Yang, Zhiyong; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China; [Jiang, Yangbangyan; Yang, Zhiyong; Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China; [Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Tech, Beijing, Peoples R China; [Huang, Qingming] Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing, Peoples R China; [Cao, Xiaochun; Huang, Qingming] Peng Cheng Lab, Shenzhen, Peoples R China	Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Peng Cheng Laboratory	Huang, QM (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.; Huang, QM (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Tech, Beijing, Peoples R China.; Huang, QM (corresponding author), Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing, Peoples R China.; Huang, QM (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	jiangyangbangyan@iie.ac.cn; xuqianqian@ict.ac.cn; yangzhiyong@iie.ac.cn; caoxiaochun@iie.ac.cn; qmhuang@ucas.ac.cn			National Key R&D Program of China [2016YFB0800603]; National Natural Science Foundation of China [61620106009, U1636214, 61836002, 61733007, 61672514, 61976202]; National Basic Research Program of China (973 Program) [2015CB351800]; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]; Beijing Natural Science Foundation [KZ201910005007, 4182079]; Peng Cheng Laboratory Project of Guangdong Province [PCL2018KP004]; Youth Innovation Promotion Association CAS; Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-SYS013]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Basic Research Program of China (973 Program)(National Basic Research Program of China); Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Peng Cheng Laboratory Project of Guangdong Province; Youth Innovation Promotion Association CAS; Key Research Program of Frontier Sciences, CAS	This work was supported in part by the National Key R&D Program of China (Grant No. 2016YFB0800603), in part by National Natural Science Foundation of China: 61620106009, U1636214, 61836002, U1636214, 61733007, 61672514 and 61976202, in part by National Basic Research Program of China (973 Program): 2015CB351800, in part by Key Research Program of Frontier Sciences, CAS: QYZDJ-SSW-SYS013, in part by the Strategic Priority Research Program of Chinese Academy of Sciences, Grant No. XDB28000000, in part by Beijing Natural Science Foundation (No. KZ201910005007 and 4182079), in part by Peng Cheng Laboratory Project of Guangdong Province PCL2018KP004, and in part by Youth Innovation Promotion Association CAS.	Almahairi A, 2018, PR MACH LEARN RES, V80; Amigo E, 2009, INFORM RETRIEVAL, V12, P461, DOI 10.1007/s10791-008-9066-8; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bin Gao, 2005, 13th Annual ACM International Conference on Multimedia, P112; Chang XB, 2018, PROC CVPR IEEE, P1488, DOI 10.1109/CVPR.2018.00161; Chaudhuri K., 2009, PROC INT C MACHINE L, P129, DOI DOI 10.1145/1553374.1553391; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142; Fard Maziar Moradi, 2018, ARXIV180610069; Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902; Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482; Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753; Hao WL, 2018, AAAI CONF ARTIF INTE, P6886; Hoffman J, 2018, PR MACH LEARN RES, V80; Hu WH, 2017, PR MACH LEARN RES, V70; Jiang YBY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P718, DOI 10.1145/3240508.3240582; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Li SY, 2014, AAAI CONF ARTIF INTE, P1968; Liu J., 2013, PROC SOC IND APPL MA, P252, DOI [10.1137/1.9781611972832.28, DOI 10.1137/1.9781611972832.28]; Luo SR, 2018, AAAI CONF ARTIF INTE, P3730; Meng L, 2014, IEEE T KNOWL DATA EN, V26, P2293, DOI 10.1109/TKDE.2013.47; Mueller F, 2018, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2018.00013; Paszke A., 2017, AUTOMATIC DIFFERENTI; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Taigman Y., 2017, ICLR; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326; Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970; Xiao Cai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1977, DOI 10.1109/CVPR.2011.5995740; Yang B, 2017, PR MACH LEARN RES, V70; Yang XT, 2018, AAAI CONF ARTIF INTE, P7485; Yang ZY, 2019, IEEE T IMAGE PROCESS, V28, P5147, DOI 10.1109/TIP.2019.2913096; Zhang C, 2019, PROC CVPR IEEE, P9444, DOI 10.1109/CVPR.2019.00968; Zhang XC, 2015, AAAI CONF ARTIF INTE, P3174; Zhang ZZ, 2018, PROC CVPR IEEE, P9242, DOI 10.1109/CVPR.2018.00963; Zhou D., 2007, P 24 INT C MACHINE L, P1159, DOI DOI 10.1145/1273496.1273642; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zong LL, 2018, AAAI CONF ARTIF INTE, P4621; Zong LL, 2017, NEURAL NETWORKS, V88, P74, DOI 10.1016/j.neunet.2017.02.003	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305083
C	Jin, TY; Shi, JM; Xiao, XK; Chen, EH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jin, Tianyuan; Shi, Jieming; Xiao, Xiaokui; Chen, Enhong			ficient Pure Exploration in Adaptive Round model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In the adaptive setting, many multi -armed bandit applications allow the learner to adaptively draw samples and adjust sampling strategy in rounds. In many real applications, not only the query complexity but also the round complexity need to be optimized In this paper, we study both PAC and exact top -k arm identification problems and design efficient algorithms considering both round complexity and query complexity. For PAC problem, we achieve optimal query complexity and use only 0(log1 (n)) rounds, which matches the lower bound of round complexity, while most of existing works need 0(log) rounds. For exact top -k arm identification, we improve the round complexity factor from log n to log*, (n), and achieve near optimal query complexity. In experiments, our algorithms conduct far fewer rounds, and outperform state of the art by orders of magnitude with respect to query cost.	[Jin, Tianyuan; Chen, Enhong] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China; [Shi, Jieming; Xiao, Xiaokui] Natl Univ Singapore, Sch Comp, Singapore, Singapore	Chinese Academy of Sciences; University of Science & Technology of China, CAS; National University of Singapore	Chen, EH (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.	jty123@mail.ustc.edu.cn; shijm@nus.edu.sg; xkxiao@nus.edu.sg; cheneh@ustc.edu.cn	Jin, Tianyuan/AGR-2947-2022		National Natural Science Foundation of China [U1605251]; National University of Singapore under SUG grant [R-252-000-686-133]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National University of Singapore under SUG grant	This research was supported by National Natural Science Foundation of China (No.U1605251) and by the National University of Singapore under SUG grant R-252-000-686-133.	Bertsimas D, 2007, OPER RES, V55, P1120, DOI 10.1287/opre.1070.0427; Cao Wei, 2015, ADV NEURAL INFORM PR, P1036; Chen J., 2017, ICML, P722; Chen L.-C., 2015, CORR; Chen LJ, 2017, PR MACH LEARN RES, V54, P101; Chen Lijie, 2016, C LEARNING THEORY, P647; Chen S., 2014, P ADV NEUR INF PROC, P379; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jamieson K., 2014, C LEARN THEOR, P423; Joulani P., 2013, ICML, P1453; Jun KS, 2016, JMLR WORKSH CONF PRO, V51, P139; Kalyanakrishnan S., 2010, P 27 INT C MACH LEAR, P511; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Mannor S, 2004, J MACH LEARN RES, V5, P623; Melhem SB, 2017, 2017 IEEE 5TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD 2017), P32, DOI 10.1109/FiCloud.2017.37; Perchet V, 2016, ANN STAT, V44, P660, DOI 10.1214/15-AOS1381; Schwartz EM, 2017, MARKET SCI, V36, P500, DOI 10.1287/mksc.2016.1023; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Wang RG, 2017, DESTECH TRANS SOC, P534; Wu Y, 2015, PROC EUR SOLID-STATE, P128, DOI 10.1109/ESSCIRC.2015.7313845; Zhou Y, 2014, PR MACH LEARN RES, V32, P217	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306059
C	Jin, YJ; Sidford, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jin, Yujia; Sidford, Aaron			Principal Component Projection and Regression in Nearly Linear Time through Asymmetric SVRG	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Given a data matrix A is an element of R-nxd, principal component projection (PCP) and principal component regression (PCR), i.e. projection and regression restricted to the top-eigenspace of A, are fundamental problems in machine learning, optimization, and numerical analysis. In this paper we provide the first algorithms that solve these problems in nearly linear time for fixed eigenvalue distribution and large n. This improves upon previous methods which have superlinear running times when both the number of top eigenvalues and inverse gap between eigenspaces is large. We achieve our results by applying rational polynomial approximations to reduce PCP and PCR to solving asymmetric linear systems which we solve by a variant of SVRG. We corroborate these findings with preliminary empirical experiments.	[Jin, Yujia; Sidford, Aaron] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Jin, YJ (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yujiajin@stanford.edu; sidford@stanford.edu			NSF CAREER Award [CCF-1844855]; Stanford Graduate Fellowship	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Stanford Graduate Fellowship(Stanford University)	This research was partially supported by NSF CAREER Award CCF-1844855 and Stanford Graduate Fellowship. We would also like to thank the anonymous reviewers who helped improve the completeness and readability of this paper by providing many helpful comments.	Agarwal Naman, 2017, ARXIV171108426; Allen-Zhu Z., 2017, J MACH LEARN RES, V18, P8194; ALLEN-ZHU Z., 2017, P 34 INT C MACH LEAR, P107; Aremenko A, 2007, J ANAL MATH, V101, P313, DOI 10.1007/s11854-007-0011-3; Bubeck S., 2015, ARXIV150608187MATHOC; Fengxi Song, 2010, Proceedings of the 2010 International Conference on System Science, Engineering Design and Manufacturing Informatization (ICSEM 2010), P27, DOI 10.1109/ICSEM.2010.14; Frostig R, 2016, PR MACH LEARN RES, V48; Frostig R, 2015, PR MACH LEARN RES, V37, P2540; Gallivan K, 1996, NUMER ALGORITHMS, V12, P33, DOI 10.1007/BF02141740; Gonchar AA., 1969, MAT SBORNIK, V78, P640; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Malhi A, 2004, IEEE T INSTRUM MEAS, V53, P1517, DOI 10.1109/TIM.2004.834070; Metsalu T, 2015, NUCLEIC ACIDS RES, V43, pW566, DOI 10.1093/nar/gkv468; Musco C, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1605; Nakatsukasa Y, 2016, SIAM REV, V58, P461, DOI 10.1137/140990334; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182; Nesterov Yurii, 2018, SPRINGER OPTIMIZATIO, V137; Niedoba Tomasz, 2014, PHYSICOCHEMICAL PROB, V50; Olver Frank, 2010, NIST HDB MATH FUNCTI, V1; Palaniappan B., 2016, NEURIPS, P1416; Pan V. Y., 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P507, DOI 10.1145/301250.301389; Pascoal C, 2012, IEEE INFOCOM SER, P1755, DOI 10.1109/INFCOM.2012.6195548; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Stegun I. A., 1965, HDB MATH FUNCTIONS F, V55; Williams VV, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P887; Yin YH, 2015, SENSORS-BASEL, V15, P19443, DOI 10.3390/s150819443; Zolotarev EI., 1877, ZAP IMP AKAD NAUK ST, V30, P1	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303081
C	Jordon, J; Yoon, J; van der Schaar, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jordon, James; Yoon, Jinsung; van der Schaar, Mihaela			Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Differential Privacy is a popular and well-studied notion of privacy. In the era of big data that we are in, privacy concerns are becoming ever more prevalent and thus differential privacy is being turned to as one such solution. A popular method for ensuring differential privacy of a classifier is known as subsample-and-aggregate, in which the dataset is divided into distinct chunks and a model is learned on each chunk, after which it is aggregated. This approach allows for easy analysis of the model on the data and thus differential privacy can be easily applied. In this paper, we extend this approach by dividing the data several times (rather than just once) and learning models on each chunk within each division. The first benefit of this approach is the natural improvement of utility by aggregating models trained on a more diverse range of subsets of the data (as demonstrated by the well-known bagging technique). The second benefit is that, through analysis that we provide in the paper, we can derive tighter differential privacy guarantees when several queries are made to this mechanism. In order to derive these guarantees, we introduce the upwards and downwards moments accountants and derive bounds for these moments accountants in a data-driven fashion. We demonstrate the improvements our model makes over standard subsample-and-aggregate in two datasets (Heart Failure (private) and UCI Adult (public)).	[Jordon, James] Univ Oxford, Oxford, England; [Yoon, Jinsung; van der Schaar, Mihaela] Univ Calif Los Angeles, Los Angeles, CA USA; [van der Schaar, Mihaela] Univ Cambridge, Cambridge, England; [van der Schaar, Mihaela] Alan Turing Inst, London, England	University of Oxford; University of California System; University of California Los Angeles; University of Cambridge	Jordon, J (corresponding author), Univ Oxford, Oxford, England.	james.jordon@wolfson.ox.ac.uk; jsyoonO823@g.ucla.edu; mv472@cam.ac.uk			National Science Foundation (NSF) [1462245, 1533983]; US Office of Naval Research (ONR)	National Science Foundation (NSF)(National Science Foundation (NSF)); US Office of Naval Research (ONR)(Office of Naval Research)	This work was supported by the National Science Foundation (NSF grants 1462245 and 1533983), and the US Office of Naval Research (ONR).	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Bassily Raef, 2018, ARXIV180305101; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2009, ACM S THEORY COMPUT, P371; Jorgensen Z, 2015, PROC INT CONF DATA, P1023, DOI 10.1109/ICDE.2015.7113353; Liu XQ, 2018, APPL SOFT COMPUT, V62, P807, DOI 10.1016/j.asoc.2017.09.010; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Papernot N., 2018, ICLR P, P1; Papernot N., 2017, ICLR; Thakurta Abhradeep Guha, 2013, P 26 ANN C LEARN THE, P819	12	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304034
C	Joulani, P; Gyorgy, A; Szepesvari, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Joulani, Pooria; Gyorgy, Andras; Szepesvari, Csaba			Think out of the "Box": Generically-Constrained Asynchronous Composite Optimization and Hedging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present two new algorithms, ASYNCADA and HEDGEHOG, for asynchronous sparse online and stochastic optimization. ASYNCADA is, to our knowledge, the first asynchronous stochastic optimization algorithm with finite-time data-dependent convergence guarantees for generic convex constraints. In addition, ASYNCADA: (a) allows for proximal (i.e., composite-objective) updates and adaptive step-sizes; (b) enjoys any-time convergence guarantees without requiring an exact global clock; and (c) when the data is sufficiently sparse, its convergence rate for (non-)smooth, (non-)strongly-convex, and even a limited class of non-convex objectives matches the corresponding serial rate, implying a theoretical "linear speed-up". The second algorithm, HEDGEHOG, is an asynchronous parallel version of the Exponentiated Gradient (EG) algorithm for optimization over the probability simplex (a.k.a. Hedge in online learning), and, to our knowledge, the first asynchronous algorithm enjoying linear speed-ups under sparsity with nonSGD-style updates. Unlike previous work, ASYNCADA and HEDGEHOG and their convergence and speed-up analyses are not limited to individual coordinatewise (i.e., "box-shaped") constraints or smooth and strongly-convex objectives. Underlying both results is a generic analysis framework that is of independent interest, and further applicable to distributed and delayed feedback optimization.	[Joulani, Pooria; Gyorgy, Andras; Szepesvari, Csaba] DeepMind, London, England; [Joulani, Pooria] Univ Alberta, Edmonton, AB, Canada	University of Alberta	Joulani, P (corresponding author), DeepMind, London, England.	pjoulani@google.com; agyorgy@google.com; szepi@google.com						[Anonymous], 2016, FDN TRENDS IN OPTIMI; [Anonymous], 2017, J MACHINE LEARNING R; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Cannelli L., 2016, ARXIV160704818; Cannelli L., 2017, ARXIV170104900; Cannelli Loris, 2017, ARXIV160704818; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; DAVIS D, 2016, ADV NEURAL INFORM PR, P226; De Sa Christopher, 2015, ARXIV150606438; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Duchi J., 2013, ADV NEURAL INFORM PR, P2832; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Facchinei F, 2015, IEEE T SIGNAL PROCES, V63, P1874, DOI 10.1109/TSP.2015.2399858; Fan XH, 2016, AAAI CONF ARTIF INTE, P1547; Fercoq O, 2016, SIAM REV, V58, P739, DOI 10.1137/16M1085905; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Joulani P., 2017, INT C ALG LEARN THEO, P681; Leblond R., 2016, ARXIV160604809; Leblond Remi, 2018, ARXIV180103749; Liu J., 2013, ARXIV13111873; Mania H., 2015, ARXIV150706970STATML; McMahan H. B., 2010, P 23 C LEARN THEOR; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nguyen Lam M., 2018, ARXIV180203801; Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8; Pan Xinghao, 2016, ADV NEURAL INFORM PR, P2568; Pedregosa F., 2017, P ADV NEUR INF PROC, P55; Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950; Razaviyayn M., 2014, P NEUR INF PROC NIPS, P1440; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Scutari G., 2016, IEEE T SIGNAL PROCES, V65, P1929; Scutari G., 2016, IEEE T SIGNAL PROCES, V65, P1945; Scutari G, 2018, LECT NOTES MATH, V2224, P141, DOI 10.1007/978-3-319-97142-1_3; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Sun T, 2017, ADV NEURAL INFORM PR, P6182; Xiao L., 2009, ADV NEURAL INFORM PR, Vvol 22, P2116	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903082
C	Jun, KS; Cutkosky, A; Orabona, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jun, Kwang-Sung; Cutkosky, Ashok; Orabona, Francesco			Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION; BOUNDS	In this paper, we consider the nonparametric least square regression in a Reproducing Kernel Hilbert Space (RKHS). We propose a new randomized algorithm that has optimal generalization error bounds with respect to the square loss, closing a long-standing gap between upper and lower bounds. Moreover, we show that our algorithm has faster finite-time and asymptotic rates on problems where the Bayes risk with respect to the square loss is small. We state our results using standard tools from the theory of least square regression in RKHSs, namely, the decay of the eigenvalues of the associated integral operator and the complexity of the optimal predictor measured through the integral operator.	[Jun, Kwang-Sung] Univ Arizona, Tucson, AZ 85721 USA; [Cutkosky, Ashok] Google Res, Mountain View, CA USA; [Orabona, Francesco] Boston Univ, Boston, MA 02215 USA	University of Arizona; Google Incorporated; Boston University	Jun, KS (corresponding author), Univ Arizona, Tucson, AZ 85721 USA.	kjun@cs.arizona.edu; ashok@cutkosky.com; francesco@orabona.com		Jun, Kwang-Sung/0000-0001-5483-3161	National Science Foundation [1908111]	National Science Foundation(National Science Foundation (NSF))	The authors thank Junhong Lin, Lorenzo Rosasco, and Alessandro Rudi for the comments and discussions on this work. This material is based upon work supported by the National Science Foundation under grant no. 1908111 "Collaborative Research: TRIPODS Institute for Optimization and Learning".	Audibert JY, 2011, ANN STAT, V39, P2766, DOI 10.1214/11-AOS918; Cesa-Bianchi N, 2005, SIAM J COMPUT, V34, P640, DOI 10.1137/S0097539703432542; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Fischer Simon, 2017, ARXIV170207254; Hastie T., 2019, ARXIV190308560V2; Hsu D., 2012, JMLR WORKSHOP C P, V23; Liang T, 2018, ARXIV180800387; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; Lin Junhong, 2018, ARXIV180107226; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; ORABONA F, 2014, ADV NEUR IN, V27, pNI568; Pillaud-Vivien Loucas, 2018, ADV NEURAL INFORM PR, P8114; Rosasco L, 2010, J MACH LEARN RES, V11, P905; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Steinwart I., 2009, COLT; Viering T., 2019, P 32 C LEARN THEOR, P3198; Wahba G., 1990, SPLINE MODELS OBSERV; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zeng HQ, 2017, PROC INT CONF RECON; Zhang T, 2003, NEURAL COMPUT, V15, P1397, DOI 10.1162/089976603321780326; Zhang T, 2001, ADV NEUR IN, V13, P357; Zhdanov F, 2013, THEOR COMPUT SCI, V473, P157, DOI 10.1016/j.tcs.2012.10.016	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907006
C	Jung, YH; Tewari, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jung, Young Hun; Tewari, Ambuj			Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MULTIARMED BANDIT; WHITTLE INDEX; OPTIMALITY	Restless bandit problems are instances of non-stationary multi-armed bandits. These problems have been studied well from the optimization perspective, where the goal is to efficiently find a near-optimal policy when system parameters are known. However, very few papers adopt a learning perspective, where the parameters are unknown. In this paper, we analyze the performance of Thompson sampling in episodic restless bandits with unknown parameters. We consider a general policy map to define our competitor and prove an (O) over tilde(root T) Bayesian regret bound. Our competitor is flexible enough to represent various benchmarks including the best fixed action policy, the optimal policy, the Whittle index policy, or the myopic policy. We also present empirical results that support our theoretical findings.	[Jung, Young Hun; Tewari, Ambuj] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Jung, YH (corresponding author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.	yhjung@umich.edu; tewaria@umich.edu			NSF CAREER grant [IIS-1452099]; Sloan Research Fellowship	NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	We acknowledge the support of NSF CAREER grant IIS-1452099. AT was also supported by a Sloan Research Fellowship. AT visited Criteo AI Lab, Paris and had discussions with Criteo researchers - Marc Abeille, Clement Calauzenes, and Jeremie Mary -regarding non-stationarity in bandit problems. These discussions were very helpful in attracting our attention to the regret analysis of restless bandit problems and the need for considering a variety of benchmark competitors when defining regret.	Ahmad SHA, 2009, IEEE T INFORM THEORY, V55, P4040, DOI 10.1109/TIT.2009.2025561; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Dai WH, 2014, IEEE INT CONF SENS, P64, DOI 10.1109/SAHCN.2014.6990328; Dai WH, 2011, INT CONF ACOUST SPEE, P2940; Gittins J. C., 1989, MULTIARMED BANDIT AL, V25; Lattimore T., 2020, BANDIT ALGORITHMS; Liu HY, 2013, IEEE T INFORM THEORY, V59, P1902, DOI 10.1109/TIT.2012.2230215; Liu HY, 2011, INT CONF ACOUST SPEE, P1968; Liu KQ, 2010, IEEE T INFORM THEORY, V56, P5547, DOI 10.1109/TIT.2010.2068950; Meshram R, 2018, IEEE T AUTOMAT CONTR, V63, P3046, DOI 10.1109/TAC.2018.2799521; Meshram R, 2017, INT CONF COMMUN SYST, P206, DOI 10.1109/COMSNETS.2017.7945378; Meshram R, 2016, IEEE DECIS CONTR P, P7210, DOI 10.1109/CDC.2016.7799381; Ortner R, 2012, P IEEE RAS-EMBS INT, P219, DOI 10.1109/BioRob.2012.6290800; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Tekin C, 2012, IEEE T INFORM THEORY, V58, P5588, DOI 10.1109/TIT.2012.2198613; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Whittle P., 1988, J APPL PROBAB, V25, P287, DOI DOI 10.2307/3214163	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900058
C	Kalhan, S; Makarychev, K; Zhou, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kalhan, Sanchit; Makarychev, Konstantin; Zhou, Timothy			Correlation Clustering with Local Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATE	Correlation Clustering is a powerful graph partitioning model that aims to cluster items based on the notion of similarity between items. An instance of the Correlation Clustering problem consists of a graph G (not necessarily complete) whose edges are labeled by a binary classifier as "similar" and "dissimilar". An objective which has received a lot of attention in literature is that of minimizing the number of disagreements: an edge is in disagreement if it is a "similar" edge and is present across clusters or if it is a "dissimilar" edge and is present within a cluster. Define the disagreements vector to be an n dimensional vector indexed by the vertices, where the v-th index is the number of disagreements at vertex v. Recently, Puleo and Milenkovic (ICML '16) initiated the study of the Correlation Clustering framework in which the objectives were more general functions of the disagreements vector. In this paper, we study algorithms for minimizing l(q) norms (q >= 1) of the disagreements vector for both arbitrary and complete graphs. We present the first known algorithm for minimizing the l(q) norm of the disagreements vector on arbitrary graphs and also provide an improved algorithm for minimizing the l(q) norm (q >= 1) of the disagreements vector on complete graphs. We also study an alternate cluster-wise local objective introduced by Ahmadi, Khuller and Saha (IPCO '19), which aims to minimize the maximum number of disagreements associated with a cluster. We also present an improved (2 + epsilon)-approximation algorithm for this objective. Finally, we compliment our algorithmic results for minimizing the l(q) norm of the disagreements vector with some hardness results.					Makarychev, Konstantin/P-6054-2017					Ahmadi S, 2019, LECT NOTES COMPUT SC, V11480, P13, DOI 10.1007/978-3-030-17953-3_2; Ahmadi Saba, 2019, MIN MAX CORRELATION; Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513; Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95; Bansal N, 2011, ANN IEEE SYMP FOUND, P17, DOI 10.1109/FOCS.2011.79; Bartal Y, 1996, AN S FDN CO, P184, DOI 10.1109/SFCS.1996.548477; Bonchi F., 2014, KDD, P1972; Charikar M, 2017, LECT NOTES COMPUT SC, V10328, P136, DOI 10.1007/978-3-319-59250-3_12; Charikar Moses, 2003, IEEE S FDN COMP SCI; Chawla S, 2006, COMPUT COMPLEX, V15, P94, DOI 10.1007/s00037-006-0210-9; Chawla S, 2015, ACM S THEORY COMPUT, P219, DOI 10.1145/2746539.2746604; Cohen W., 2001, ACM SIGIR 2001 WORKS; Cohen William W, 2002, P 8 ACM SIGKDD INT C; Demaine ED, 2006, THEOR COMPUT SCI, V361, P172, DOI 10.1016/j.tcs.2006.05.008; Fakcharoenphol J, 2004, J COMPUT SYST SCI, V69, P485, DOI 10.1016/j.jcss.2004.04.01; Fakcharoenphol Jittat, 2003, 6 INT WORKSH APPR AL; Garg N, 1996, SIAM J COMPUT, V25, P235, DOI 10.1137/S0097539793243016; Gupta A, 2003, ANN IEEE SYMP FOUND, P534, DOI 10.1109/SFCS.2003.1238226; Khot S, 2008, J COMPUT SYST SCI, V74, P335, DOI 10.1016/j.jcss.2007.06.019; Puleo Gregory, 2016, INT C MACH LEARN; Ramachandran A, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P342; Rao Satish, 1999, P 15 ANN S COMP GEOM; Wirth Anthony, 2017, CORRELATION CLUSTERI, DOI [10.1007/978-1-4899-7687-1_176, DOI 10.1007/978-1-4899-7687-1_176]	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901002
C	Kallus, N; Zhou, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kallus, Nathan; Zhou, Angela			Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption by means of sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.	[Kallus, Nathan; Zhou, Angela] Cornell Univ, New York, NY 10021 USA	Cornell University	Kallus, N (corresponding author), Cornell Univ, New York, NY 10021 USA.	kallus@cornell.edu; az434@cornell.edu			National Science Foundation [1846210]; JPMorgan Chase Co	National Science Foundation(National Science Foundation (NSF)); JPMorgan Chase Co	This material is based upon work supported by the National Science Foundation under Grant No. 1846210. This research was funded in part by JPMorgan Chase & Co. Any views or opinions expressed herein are solely those of the authors listed, and may differ from the views and opinions expressed by JPMorgan Chase & Co. or its affiliates. This material is not a product of the Research Department of J.P. Morgan Securities LLC. This material should not be construed as an individual recommendation for any particular client and is not intended as a recommendation of particular securities, financial instruments or strategies for a particular client. This material does not constitute a solicitation or offer in any jurisdiction.	Adler M., WELL BEING FAIR DIST; Angrist J. D., 1996, J AM STAT ASS; Angwin J., 2016, MACHINE BIAS ONL MAY; Athey S., 2017, SCIENCE; Athey S, 2019, ANN STAT, V47, P1148, DOI 10.1214/18-AOS1709; Balke A, 1997, J AM STAT ASSOC, V92, P1171, DOI 10.2307/2965583; Barabas C., 2017, P MACHINE LEARNING R; Barocas S., 2014, CALIFORNIA LAW REV; Barocas Solon, 2018, FAIRNESS MACHINE LEA; Behaghel L., 2014, AM EC J APPL EC; Behncke S., 2007, 3085 IZA; Bennett A., 2019, ADV NEURAL INFORM PR; Beresteanu A, 2012, J ECONOMETRICS, V166, P17, DOI 10.1016/j.jeconom.2011.06.003; Berger M., 2000, ECONOMETRIC EVALUATI, P59; Bhattacharya D., 2012, J ECONOMETRICS; Brown C., 2016, 7915 WORLD BANK GROU; Carneiro P., 2002, TECHNICAL REPORT; Charnes A., 1962, NAV RES LOG, V9, P181, DOI [10.1002/nay.3800090303, DOI 10.1002/NAV.3800090303, 10.1002/nav.3800090303]; Chen JH, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P339, DOI 10.1145/3287560.3287594; Chernozhukov V, 2017, AM ECON REV, V107, P261, DOI 10.1257/aer.p20171038; Chouldechova A., 2016, P FATML; Corbett-Davies Sam, 2018, ARXIV 2018 180800023; Crepon B, 2016, ANNU REV ECON, V8, P521, DOI 10.1146/annurev-economics-080614-115738; Davis JMV, 2017, AM ECON REV, V107, P546, DOI 10.1257/aer.p20171000; Dudik M., 2014, STAT SCI; Eubanks V, 2018, AUTOMATING INEQUALIT; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Heidari H., 2018, NEURAL INFORM PROCES, P1265; Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162; Hu L., 2019, ARXIV190500147; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Jung J., 2018, ALGORITHMIC DECISION; Kallus N, 2019, ARXIV190600285; Kallus N., 2018, ADV NEURAL INFORM PR, P9269; Kallus N., 2017, P 34 INT C MACH LEAR; Kallus N., 2018, ADV NEURAL INFORM PR, P8895; Kallus N., 2019, 22 INT C ART INT STA, P2281; Kallus N., 2019, P INT C MACH LEARN; Kallus N., 2018, ICML; Kallus Nathan, 2019, ADV NEURAL INFORM PR; Kitagawa T., 2015, EMPIRICAL WELFARE MA; Kube A., 2019, P AAAI C ART INT; Kusner MJ, 2017, NIPS; Lakkaraju H., 2017, P KKD2017; Liu L, 2018, DESTECH TRANS ENVIR, P35; Madras D., 2019, ACM C FAIRN ACC TRAN; Manski C., 2005, ECONOMETRIC I LECT; Manski C., 2003, PARTIAL IDENTIFICATI; Manski CF, 1997, ECONOMETRICA, V65, P1311, DOI 10.2307/2171738; McBride L., 2016, 7849 WORLD BANK GROU; Mitchell S., 2018, PREDICTION BASED DEC; Mkandawire T., 2005, SOCIAL POLICY DEV; Rice E., 2013, TAY TRIAGE TOOL TOOL; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Rockafellar R.T., 2015, CONVEX ANAL; Shalit U., 2017, P 34 INT C MACH LEAR; Shroff R, 2017, BIG DATA, V5, P189, DOI 10.1089/big.2016.0052; Wager S., 2017, EFFICIENT POLICY LEA; Wager Stefan, 2017, J AM STAT ASS	63	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303042
C	Kamath, G; Sheffet, O; Singhal, V; Ullman, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kamath, Gautam; Sheffet, Or; Singhal, Vikrant; Ullman, Jonathan			Differentially Private Algorithms for Learning Mixtures of Separated Gaussians	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning the parameters of Gaussian mixture models is a fundamental and widely studied problem with numerous applications. In this work, we give new algorithms for learning the parameters of a high-dimensional, well separated, Gaussian mixture model subject to the strong constraint of differential privacy. In particular, we give a differentially private analogue of the algorithm of Achlioptas and McSherry (COLT 2005). Our algorithm has two key properties not achieved by prior work: (1) The algorithm's sample complexity matches that of the corresponding non-private algorithm up to lower order terms in a wide range of parameters. (2) The algorithm requires very weak a priori bounds on the parameters of the mixture.	[Kamath, Gautam] Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON N2L 3G1, Canada; [Sheffet, Or] Bar Ilan Univ, Dept Comp Sci, Fac Exact Sci, IL-5290002 Ramat Gan, Israel; [Singhal, Vikrant; Ullman, Jonathan] Northeastern Univ, Khoury Coll Comp Sci, 360 Huntington Ave, Boston, MA 02115 USA	University of Waterloo; Bar Ilan University; Northeastern University	Kamath, G (corresponding author), Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON N2L 3G1, Canada.	g@csail.mit.edu; or.sheffet@biu.ac.il; singhal.vi@husky.neu.edu; jullman@ccs.neu.edu			Microsoft Research Fellow, Simons-Berkeley Research Fellowship program; University of Waterloo; Natural Sciences and Engineering Research Council of Canada (NSERC) [2017-06701]; NSF [CCF-1718088, CCF-1750640, CNS-1816028]	Microsoft Research Fellow, Simons-Berkeley Research Fellowship program; University of Waterloo; Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); NSF(National Science Foundation (NSF))	Part of this work was done while the authors were visiting the Simons Institute for Theoretical Computer Science. Parts of this work were done while GK was supported as a Microsoft Research Fellow, as part of the Simons-Berkeley Research Fellowship program, while visiting Microsoft Research, Redmond, and while supported by a University of Waterloo startup grant. This work was done while OS was affiliated with the University of Alberta. OS gratefully acknowledges the Natural Sciences and Engineering Research Council of Canada (NSERC) for its support through grant #2017-06701. JU and VS were supported by NSF grants CCF-1718088, CCF-1750640, and CNS-1816028.	Acharya J., 2018, P 35 INT C MACH LEAR, P30; Acharya Jayadev, 2019, 22 INT C ARTIFICIAL, V89, P1120; Achlioptas D, 2005, LECT NOTES COMPUT SC, V3559, P458, DOI 10.1007/11503415_31; Anderson J., 2014, P MACH LEARN RES, V35, P1135; [Anonymous], 2017, LEARNING PRIVACY SCA; [Anonymous], 2018, ARXIV181108382; [Anonymous], [No title captured]; Ashtiani H., 2018, ADV NEURAL INFORM PR, V31, P3412; Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4; Balcan MF, 2017, PR MACH LEARN RES, V70; Bassily Raef, 2015, P 48 ANN ACM S THEOR, P1046; Belkin M, 2010, ANN IEEE SYMP FOUND, P103, DOI 10.1109/FOCS.2010.16; Bhaskara A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P594, DOI 10.1145/2591796.2591881; Blum A., 2005, P 24 ACM SIGMOD SIGA, P128, DOI DOI 10.1145/1065167.1065184; Bun M, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1, DOI 10.1145/2591796.2591877; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Chan Siu On, 2016, P 46 ANN ACM S THEOR, P604; Chaudhuri K., 2008, COLT, V4, P9; Chaudhuri K, 2013, J MACH LEARN RES, V14, P2905; Dajani Aref N., 2017, SEPT 2017 M CENS SCI, P10; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; Dasgupta S., 2000, P 16 C UNC ART INT, P152; DASKALAKIS C., 2017, C LEARN THEOR, V65, P704; Daskalakis Constantinos, 2014, P 27 C LEARN THEOR C, P1183; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Diakonikolas I., 2015, ADV NEURAL INFORM PR, P2566; Diakonikolas Ilias, 2016, P 50 ANN ACM S THEOR, P1047; Duchi J. C, 2018, ARXIV180605756; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, SCIENCE, V349, P636, DOI 10.1126/science.aaa9375; Dwork Cynthia, 2016, P 46 ANN ACM S THEOR, P11; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Feldman J, 2006, LECT NOTES ARTIF INT, V4005, P20, DOI 10.1007/11776420_5; Feldman J, 2008, SIAM J COMPUT, V37, P1536, DOI 10.1137/060670705; Gaboardi Marco, 2019, P 22 INT C ART INT S, P2545; Ge R, 2015, ACM S THEORY COMPUT, P761, DOI 10.1145/2746539.2746616; Gupta A, 2010, PROC APPL MATH, V135, P1106; Hardt M, 2015, ACM S THEORY COMPUT, P753, DOI 10.1145/2746539.2746579; Hopkins SB, 2018, ACM S THEORY COMPUT, P1021, DOI 10.1145/3188745.3188748; Hsu D., 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439; Huang ZY, 2018, PODS'18: PROCEEDINGS OF THE 37TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P395, DOI 10.1145/3196959.3196977; Kairouz P, 2016, PR MACH LEARN RES, V48; Kalai AT, 2010, ACM S THEORY COMPUT, P553; Kamath Gautam, 2019, DIFFERENTIALLY PRIVA; Kamath Gautam, 2019, P 32 ANN C LEARNING, P1853; Kapralov M, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1395; Karwa V., 2018, P 9 INN THEOR COMP S, V94, p44:1; Kothari PK, 2018, ACM S THEORY COMPUT, P1035, DOI 10.1145/3188745.3188970; Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35; Li J., 2017, J MACH LEARN RES, V65, P1302; Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15; Nissim K., 2018, ALGORITHMIC LEARNING, P619; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Nissim K, 2016, PODS'16: PROCEEDINGS OF THE 35TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P413, DOI 10.1145/2902251.2902296; Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003; Rao Satish, 2008, P 21 ANN C LEARN THE, P21; Regev O, 2017, ANN IEEE SYMP FOUND, P85, DOI 10.1109/FOCS.2017.17; Sanjeev A., 2001, P 33 ANN ACM SIGACT, P247, DOI [DOI 10.1145/380752.380808, 10.1145/380752.380808]; Smith A, 2011, ACM S THEORY COMPUT, P813; Steinke T., 2016, J PRIV CONFID, V7, P3; Suresh A. T., 2014, ADV NEURAL INFORM PR, V27, P1395; Vempala S, 2002, ANN IEEE SYMP FOUND, P113, DOI 10.1109/SFCS.2002.1181888; Wang S., 2016, ARXIV160708025; Xu Ji, 2016, ADV NEURAL INFORM PR, P2676; Ye M, 2018, IEEE T INFORM THEORY, V64, P5662, DOI 10.1109/TIT.2018.2809790	67	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300016
C	Kancharla, P; Channappayya, SS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kancharla, Parimala; Channappayya, Sumohana S.			Quality Aware Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Generative Adversarial Networks (GANs) have become a very popular tool for implicitly learning high-dimensional probability distributions. Several improvements have been made to the original GAN formulation to address some of its shortcomings like mode collapse, convergence issues, entanglement, poor visual quality etc. While a significant effort has been directed towards improving the visual quality of images generated by GANs, it is rather surprising that objective image quality metrics have neither been employed as cost functions nor as regularizers in GAN objective functions. In this work, we show how a distance metric that is a variant of the Structural SIMilarity (SSIM) index (a popular full-reference image quality assessment algorithm), and a novel quality aware discriminator gradient penalty function that is inspired by the Natural Image Quality Evaluator (NIQE, a popular no-reference image quality assessment algorithm) can each be used as excellent regularizers for GAN objective functions. Specifically, we demonstrate state-of-the-art performance using the Wasserstein GAN gradient penalty (WGAN-GP) framework over CIFAR-10, STL10 and CelebA datasets.	[Kancharla, Parimala; Channappayya, Sumohana S.] Indian Inst Technol Hyderabad, Dept Elect Engn, Hyderabad, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Hyderabad	Kancharla, P (corresponding author), Indian Inst Technol Hyderabad, Dept Elect Engn, Hyderabad, India.	ee15m17p100001@iith.ac.in; sumohana@iith.ac.in						Adler J., 2018, ADV NEURAL INFORM PR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Feng BQ, 2017, CHIN CONT DECIS CONF, P1220, DOI 10.1109/CCDC.2017.7978704; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hensel M, 2017, ADV NEUR IN, V30; Hoang Thanh-Tung, 2019, ARXIV190203984; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kim Sangpil, 2018, ARXIV180704812; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mescheder L, 2018, PR MACH LEARN RES, V80; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Moorthy AK, 2010, INT CONF ACOUST SPEE, P962, DOI 10.1109/ICASSP.2010.5495298; Pan J, 2017, PROC IEEE C COMPUT V, P1; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006; Salimans T, 2016, ADV NEUR IN, V29; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302089
C	Karmalkar, S; Klivans, AR; Kothari, PK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Karmalkar, Sushrut; Klivans, Adam R.; Kothari, Pravesh K.			List-decodeable Linear Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LITTLEWOOD-OFFORD PROBLEM; HIGH DIMENSIONS; MIXTURES	We give the first polynomial-time algorithm for robust regression in the list-decodable setting where an adversary can corrupt a greater than 1/2 fraction of examples. For any alpha < 1, our algorithm takes as input a sample {(x(i), y(i))}(i <= n) of n linear equations where alpha n of the equations satisfy y(i) = < x(i), l*> + zeta for some small noise zeta and (1 - alpha)n of the equations are arbitrarily chosen. It outputs a list L of size O(1/alpha) - a fixed constant - that contains an l that is close to l*. Our algorithm succeeds whenever the inliers are chosen from a certifiably anti-concentrated distribution D. As a corollary of our algorithmic result, we obtain a (d/alpha)(O(1/alpha 8)) time algorithm to find a O(1/alpha) size list when the inlier distribution is standard Gaussian. For discrete product distributions that are anti-concentrated only in regular directions, we give an algorithm that achieves similar guarantee under the promise that l* has all coordinates of the same magnitude. To complement our result, we prove that the anti-concentration assumption on the inliers is information-theoretically necessary. To solve the problem we introduce a new framework for list-decodable learning that strengthens the "identifiability to algorithms" paradigm based on the sum-of-squares method.	[Karmalkar, Sushrut; Klivans, Adam R.] Univ Texas Austin, Austin, TX 78712 USA; [Kothari, Pravesh K.] Princeton Univ, Princeton, NJ 08544 USA; [Kothari, Pravesh K.] Inst Adv Study, Princeton, NJ USA	University of Texas System; University of Texas Austin; Princeton University; Institute for Advanced Study - USA	Karmalkar, S (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	sushrutk@cs.utexas.edu; klivans@cs.utexas.edu; kothari@cs.princeton.edu			NSF [CNS-1414023, CCF-1717896]; Schmidt Foundation Fellowship; Avi Wigderson's NSF [CCF-1412958]	NSF(National Science Foundation (NSF)); Schmidt Foundation Fellowship; Avi Wigderson's NSF	Sushrut Karmalkar was supported by NSF Award CNS-1414023. Adam Klivans was supported by NSF Award CCF-1717896. Pravesh Kothari was supported by Schmidt Foundation Fellowship and Avi Wigderson's NSF Award CCF-1412958.	[Anonymous], 2011, ROBUST STAT APPROACH; Awasthi Pranjal, 2013, ABS13078371 CORR; Balakrishnan Sivaraman, 2014, ABS14082156 CORR; Balcan MF, 2008, ACM S THEORY COMPUT, P671; Barak B, 2015, ACM S THEORY COMPUT, P143, DOI 10.1145/2746539.2746605; BERNHOLT T, 2006, TECHNICAL REPORT; Bhatia K, 2017, ADV NEUR IN, V30; Cheng Yu, 2019, P 30 ANN ACM SIAM S, P2755; DEVEAUX RD, 1989, COMPUT STAT DATA AN, V8, P227, DOI 10.1016/0167-9473(89)90043-1; Diakonikolas I., 2017, LEARNING GEOMETRIC C; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1047, DOI 10.1145/3188745.3188758; Diakonikolas I, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2683; Diakonikolas Ilias, 2017, ABS170403866 CORR; Diakonikolas Ilias, 2018, ABS180302815 CORR; Diakonikolas Ilias, 2016, ABS160406443 CORR; ERDOS P, 1945, B AM MATH SOC, V51, P898, DOI 10.1090/S0002-9904-1945-08454-7; Faria S, 2010, J STAT COMPUT SIM, V80, P201, DOI 10.1080/00949650802590261; Hainline John, 2019, 22 INT C ART INT STA, P1042; Huber P. J., 2011, INT ENCY STAT SCI, P1248, DOI [10.1007/978-3-642-04898-2_594, DOI 10.1007/978-3-642-04898-2_594]; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Karmalkar Sushrut, 2019, OASICS, V69; Klivans A., 2018, C LEARNING THEORY, P1420; Kothari Pravesh K., 2017, ABS171111581 CORR; Kothari Pravesh K., 2017, BETTER AGNOSTIC CLUS; Kothari Pravesh K., 2019, LIST DECODABLE UNPUB; Lai KA, 2016, ANN IEEE SYMP FOUND, P665, DOI 10.1109/FOCS.2016.76; Li Jerry., 2017, MIXTURE MODELS ROBUS; Lubinsky Doron S, 2007, MATH0701099 ARXIV; Ma TY, 2016, ANN IEEE SYMP FOUND, P438, DOI 10.1109/FOCS.2016.54; MARONNA R.A., 2006, ROBUST STAT; Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15; Prasad A., 2018, ARXIV180206485; Rudelson M, 2008, ADV MATH, V218, P600, DOI 10.1016/j.aim.2008.01.010; Sedghi H, 2016, JMLR WORKSH CONF PRO, V51, P1223; Tao T, 2012, COMBINATORICA, V32, P363, DOI 10.1007/s00493-012-2716-x; Tukey J., 1975, P INT C MATH, V2; Yi X., 2013, ARXIV13103745; Zhong K., 2016, ADV NEURAL INFORM PR, P2190	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307044
C	Katariya, S; Tripathy, A; Nowak, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Katariya, Sumeet; Tripathy, Ardhendu; Nowak, Robert			MaxGap Bandit: Adaptive Algorithms for Approximate Ranking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MULTIARMED BANDIT	This paper studies the problem of adaptively sampling from K distributions (arms) in order to identify the largest gap between any two adjacent means. We call this the MaxGap-bandit problem. This problem arises naturally in approximate ranking, noisy sorting, outlier detection, and top-arm identification in bandits. The key novelty of the MaxGap bandit problem is that it aims to adaptively determine the natural partitioning of the distributions into a subset with larger means and a subset with smaller means, where the split is determined by the largest gap rather than a pre-specified rank or threshold. Estimating an arm's gap requires sampling its neighboring arms in addition to itself, and this dependence results in a novel hardness parameter that characterizes the sample complexity of the problem. We propose elimination and UCB-style algorithms and show that they are minimax optimal. Our experiments show that the UCB-style algorithms require 6-8x fewer samples than non-adaptive sampling to achieve the same error.	[Katariya, Sumeet; Tripathy, Ardhendu; Nowak, Robert] UW Madison, Madison, WI 53706 USA; [Katariya, Sumeet] Amazon, Seattle, WA 98109 USA	University of Wisconsin System; University of Wisconsin Madison; Amazon.com	Katariya, S (corresponding author), UW Madison, Madison, WI 53706 USA.; Katariya, S (corresponding author), Amazon, Seattle, WA 98109 USA.	sumeetsk@gmail.com; astripathy@wisc.edu; rdnowak@wisc.edu	Tripathy, Ardhendu Shekhar/U-2887-2019	Tripathy, Ardhendu Shekhar/0000-0003-1893-4891	AFOSR/AFRL grants [FA8750-17-2-0262, FA9550-18-1-0166]	AFOSR/AFRL grants	Ardhendu Tripathy would like to thank Ervin Tanczos for helpful discussions. The authors would also like to thank the reviewers for their comments and suggestions. This work was partially supported by AFOSR/AFRL grants FA8750-17-2-0262 and FA9550-18-1-0166.	Braverman M, 2016, ACM S THEORY COMPUT, P851, DOI 10.1145/2897518.2897642; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Chen LJ, 2017, PR MACH LEARN RES, V54, P101; Chen S., 2014, P ADV NEUR INF PROC, P379; Davidson S, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2684066; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; Gabillon V., 2012, ADV NEURAL INFORM PR, P3212; Garivier A., 2016, C LEARN THEOR, P998; Huang WR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2291; Jamieson K., 2014, C LEARN THEOR, P423; Jun KS, 2016, JMLR WORKSH CONF PRO, V51, P139; Kalyanakrishnan S., 2010, P 27 INT C MACH LEAR, P511; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; KATARIYA S, 2018, CHICAGO STREETVIEW D; Katariya S, 2018, PR MACH LEARN RES, V84; Katariya Sumeet, 2019, CODE MAXGAP BANDIT A; Mannor S, 2004, J MACH LEARN RES, V5, P623; Melhem SB, 2017, 2017 IEEE 5TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD 2017), P32, DOI 10.1109/FiCloud.2017.37; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; Rigollet P, 2018, P ALG LEARN THEOR; Soare M., 2014, ARXIV14096110; Zhuang H., 2017, ADV NEURAL INFORM PR, P5204	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902065
C	Kim, T; Ahn, S; Bengio, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kim, Taesup; Ahn, Sungjin; Bengio, Yoshua			Variational Temporal Abstraction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMAGINATION	We introduce a variational approach to learning and inference of temporally hierarchical structure and representation for sequential data. We propose the Variational Temporal Abstraction (VTA), a hierarchical recurrent state space model that can infer the latent temporal structure and thus perform the stochastic state transition hierarchically. We also propose to apply this model to implement the jumpy imagination ability in imagination-augmented agent-learning in order to improve the efficiency of the imagination. In experiments, we demonstrate that our proposed method can model 2D and 3D visual sequence datasets with interpretable temporal structure discovery and that its application to jumpy imagination enables more efficient agent-learning in a 3D navigation task.	[Kim, Taesup; Bengio, Yoshua] Univ Montreal, Mila, Montreal, PQ, Canada; [Kim, Taesup; Ahn, Sungjin] Rutgers State Univ, New Brunswick, NJ 08901 USA; [Kim, Taesup] Kakao Brain, Jeju City, South Korea	Universite de Montreal; Rutgers State University New Brunswick	Kim, T (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.; Ahn, S (corresponding author), Rutgers State Univ, New Brunswick, NJ 08901 USA.; Kim, T (corresponding author), Kakao Brain, Jeju City, South Korea.	taesup.kim@umontreal.ca; sungjin.ahn@rutgers.edu			Kakao Brain; Center for Super Intelligence (CSI); Element AI	Kakao Brain; Center for Super Intelligence (CSI); Element AI	We would like to acknowledge Kakao Brain cloud team for providing computing resources used in this work. TK would also like to thank colleagues at Mila, Kakao Brain, and Rutgers Machine Learning Group. SA is grateful to Kakao Brain, the Center for Super Intelligence (CSI), and Element AI for their support. Mila (TK and YB) would also like to thank NSERC, CIFAR, Google, Samsung, Nuance, IBM, Canada Research Chairs, Canada Graduate Scholarship Program, and Compute Canada.	Auger-Methe M, 2016, SCI REP-UK, V6, DOI 10.1038/srep26677; Bengio, 2016, ARXIV160901704; Bengio Yoshua, 2013, ESTIMATING PROPAGATI, P4; Buckner RL, 2010, ANNU REV PSYCHOL, V61, P27, DOI 10.1146/annurev.psych.60.110707.163508; Chung J., ADV NEURAL INFORM PR, V28, P2980; Dai Hanjun, 2016, RECURRENT HIDDEN SEM; Fraccaro M., 2017, ADV NEURAL INFORM PR, V30, P3601; Ghahramani Z, 2000, NEURAL COMPUT, V12, P831, DOI 10.1162/089976600300015619; Gregor Karol, 2019, INT C LEARN REPR; Hafner D., 2018, ARXIV181104551; Hafner D., 2018, CORR; Hamrick JB, 2019, CURR OPIN BEHAV SCI, V29, P8, DOI 10.1016/j.cobeha.2018.12.011; Hassabis Demis, 2018, ARXIV180203006; Jang E., 2017, ICLR; Jayaraman Dinesh, 2019, INT C LEARN REPR; Kingma D. P., 2014, P INT C LEARN REPR; Kipf T., 2018, CORR; Klinger Tim, 2017, 31 AAAI C ART INT; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Linderman S. W., 2016, ARXIV PREPRINT ARXIV; Maddison Chris J, 2017, ICLR; Mullally SL, 2014, NEUROSCIENTIST, V20, P220, DOI 10.1177/1073858413495091; Neitz A., 2018, ADV NEURAL INFORM PR, P9816; Pertsch K, 2019, CORR; Wei XX, 2015, ELIFE, V4, DOI 10.7554/eLife.08362; Yu SZ, 2010, ARTIF INTELL, V174, P215, DOI 10.1016/j.artint.2009.11.011; Zheng X, 2017, ARXIV171111179CSLG	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903023
C	Kim, W; Lee, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kim, Wonjae; Lee, Yoonho			Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIMPLICITY PRINCIPLE	Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention-based reasoning by modelling it as a continuous dynamical system using neural ordinary differential equations. As a proof of concept, we augment a state-of-the-art visual reasoning model with DAFT. Our experiments reveal that applying DAFT yields similar performance to the original model while using fewer reasoning steps, showing that it implicitly learns to skip unnecessary steps. We also propose a new metric, Total Length of Transition (TLT), which represents the effective reasoning step size by quantifying how much a given model's focus drifts while reasoning about a question. We show that adding DAFT results in lower TLT, demonstrating that our method indeed obeys the human prior towards shorter reasoning paths in addition to producing more interpretable attention maps. Our code is available at https://github.com/kakao/DAFT.	[Kim, Wonjae; Lee, Yoonho] Kakao Corp, Pangyo, South Korea		Kim, W (corresponding author), Kakao Corp, Pangyo, South Korea.	dandelin.kim@kakaocorp.com; eddy.l@kakaocorp.com						Agrawal Aishwarya, 2015, ARXIV150500468; Andreas Jacob, 2015, P IEEE C COMP VIS PA, P39; Blier Leonard, 2018, ADV NEURAL INFORM PR, P2216; Bottou L, 2014, MACH LEARN, V94, P133, DOI 10.1007/s10994-013-5335-x; Chen R. T., 2018, ADV NEURAL INFORM PR, P6571; Dormand J.R., 1980, J COMPUT APPL MATH, V6, P19, DOI [10.1016/0771-050X(80)90013-3, DOI 10.1016/0771-050X(80)90013-3]; Dupont Emilien, 2019, ARXIV190401681; Feldman J, 2016, WIRES COGN SCI, V7, P330, DOI 10.1002/wcs.1406; Feldman J, 2009, PSYCHOL REV, V116, P875, DOI 10.1037/a0017144; Goyal Y, 2017, P IEEE C COMP VIS PA, P6904; Graves A., 2016, ADAPTIVE COMPUTATION; HOCHBERG J, 1953, J EXP PSYCHOL, V46, P361, DOI 10.1037/h0055809; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Hudson D. A., 2019, ARXIV190209506; Hudson Drew A., 2018, ARXIV180303067; Ilyas A, 2019, ADV NEUR IN, V32; Johnson Justin, 2017, P IEEE C COMP VIS PA, P2901; Johnson O, 2017, IEEE INT SYMP INFO, P898, DOI 10.1109/ISIT.2017.8006658; Kim J.-H., 2018, ADV NEURAL INFORM PR, P1564; Kolmogorov A.N., 1963, SANKHYA INDIAN J S A, P369, DOI [DOI 10.1016/S0304-3975(98)00075-9, 10.1016/S0304-3975(98)00075-9]; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Lu JS, 2016, ADV NEUR IN, V29; Lu Y., 2017, FINITE LAYER NEURAL; Mao Jiayuan, 2018, NEURO SYMBOLIC CONCE; Mascharka D, 2018, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2018.00519; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Poursabzi-Sangdeh Forough, 2018, ARXIV180207810; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Ruthotto L., 2018, ARXIV180404272; Santoro A., 2017, ADV NEURAL INFORM PR, P4967; Suarez Joseph, 2018, ARXIV180311361; Vendetti MS, 2014, NEURON, V84, P906, DOI 10.1016/j.neuron.2014.09.035; Xiong CM, 2016, PR MACH LEARN RES, V48; Yi K., 2018, ADV NEURAL INFORM PR, V31, P1031	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306007
C	Kleinberg, R; Leyton-Brown, K; Lucier, B; Graham, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kleinberg, Robert; Leyton-Brown, Kevin; Lucier, Brendan; Graham, Devon			Procrastinating with Confidence: Near-Optimal, Anytime, Adaptive Algorithm Configuration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION	Algorithm configuration methods optimize the performance of a parameterized heuristic algorithm on a given distribution of problem instances. Recent work introduced an algorithm configuration procedure ("Structured Procrastination") that provably achieves near optimal performance with high probability and with nearly minimal runtime in the worst case. It also offers an anytime property: it keeps tightening its optimality guarantees the longer it is run. Unfortunately, Structured Procrastination is not adaptive to characteristics of the parameterized algorithm: it treats every input like the worst case. Follow-up work ("LeapsAndBounds") achieves adaptivity but trades away the anytime property. This paper introduces a new algorithm, "Structured Procrastination with Confidence", that preserves the near-optimality and anytime properties of Structured Procrastination while adding adaptivity. In particular, the new algorithm will perform dramatically faster in settings where many algorithm configurations perform poorly. We show empirically both that such settings arise frequently in practice and that the anytime property is useful for finding good configurations quickly.	[Kleinberg, Robert] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Leyton-Brown, Kevin; Graham, Devon] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada; [Lucier, Brendan] Microsoft Res, Redmond, WA USA	Cornell University; University of British Columbia; Microsoft	Kleinberg, R (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	rdk@cs.cornell.edu; kevinlb@cs.ubc.ca; brlucier@microsoft.com; drgraham@cs.ubc.ca						[Anonymous], 2016, ARXIV160306560; Ansotegui C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P733; Ansotegui C, 2009, LECT NOTES COMPUT SC, V5732, P142, DOI 10.1007/978-3-642-04244-7_14; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30; Balcan M., 2017, C LEARN THEOR, P213; Balcan M.-F., 2018, INT C MACH LEARN; Balcan MF, 2018, ANN IEEE SYMP FOUND, P603, DOI 10.1109/FOCS.2018.00064; Bergstra J. S., 2011, P 24 INT C NEUR INF, P2546, DOI DOI 10.1145/3065386; Birattari M., 2002, P GENETIC EVOLUTIONA, V2, P11; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck S, 2011, J MACH LEARN RES, V12, P1655; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chaudhuri K., 2009, ADV NEURAL INFORM PR; Ganchev K, 2010, COMMUN ACM, V53, P99, DOI 10.1145/1735223.1735247; Guha S, 2007, ACM S THEORY COMPUT, P104, DOI 10.1145/1250790.1250807; Gupta R, 2017, SIAM J COMPUT, V46, P992, DOI 10.1137/15M1050276; Hutter F., 2011, NIPS WORKSH BAYES OP; Hutter F., 2007, P 22 C ARTIFICIAL IN, V7, P1152; Hutter F, 2014, ARTIF INTELL, V206, P79, DOI 10.1016/j.artint.2013.10.003; Hutter F, 2009, J ARTIF INTELL RES, V36, P267, DOI 10.1613/jair.2861; Kandasamy K., 2016, ADV NEURAL INFORM PR, V29, P1777; Kleinberg R., 2017, P 26 INT JOINT C ART; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Kleinberg R, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P928, DOI 10.1145/1109557.1109659; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lopez-Ibanez M., 2011, TECHNICAL REPORT; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Sorensson Niklas, 2005, SAT, V53, P1; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Tran- Thanh L., 2012, AAAI C ART INT; Weisz Gellert, 2018, ICML 2018 AUTOML WOR; Weisz Gellert, 2018, P 35 INT C MACHINE L, P5254; WELLNER JA, 1978, Z WAHRSCHEINLICHKEIT, V45, P73, DOI 10.1007/BF00635964	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900047
C	Kobayashi, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kobayashi, Takumi			Gaussian-Based Pooling for Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Convolutional neural networks (CNNs) contain local pooling to effectively downsize feature maps for increasing computation efficiency as well as robustness to input variations. The local pooling methods are generally formulated in a form of convex combination of local neuron activations for retaining the characteristics of an input feature map in a manner similar to image downscaling. In this paper, to improve performance of CNNs, we propose a novel local pooling method based on the Gaussian-based probabilistic model over local neuron activations for flexibly pooling (extracting) features, in contrast to the previous model restricting the output within the convex hull of local neurons. In the proposed method, the local neuron activations are aggregated into the statistics of mean and standard deviation in a Gaussian distribution, and then on the basis of those statistics, we construct the probabilistic model suitable for the pooling in accordance with the knowledge about local pooling in CNNs. Through the probabilistic model equipped with trainable parameters, the proposed method naturally integrates two schemes of adaptively training the pooling form based on input feature maps and stochastically performing the pooling throughout the end-to-end learning. The experimental results on image classification demonstrate that the proposed method favorably improves performance of various CNNs in comparison with the other pooling methods.	[Kobayashi, Takumi] Natl Inst Adv Ind Sci & Technol, 1-1-1 Umezono, Tsukuba, Ibaraki, Japan	National Institute of Advanced Industrial Science & Technology (AIST)	Kobayashi, T (corresponding author), Natl Inst Adv Ind Sci & Technol, 1-1-1 Umezono, Tsukuba, Ibaraki, Japan.	takumi.kobayashi@aist.go.jp						Anonymous A., 2019, GLOBAL FEATU S UNPUB; Bishop C.M., 1994, MIXTURE DENSITY NETW; Boureau Y.-L., 2010, ICML, P111, DOI DOI 10.5555/3104322.3104338; Crow EL, 1988, LOGNORMAL DISTRIBUTI; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lee CY, 2016, JMLR WORKSH CONF PRO, V51, P464; Li X., 2018, 180105134 ARXIV; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Pewsey A, 2002, COMMUN STAT-THEOR M, V31, P1045, DOI 10.1081/STA-120004901; Riesenhuber M, 1998, ADV NEUR IN, V10, P215; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Saeedan F, 2018, PROC CVPR IEEE, P9108, DOI 10.1109/CVPR.2018.00949; Serre T, 2010, COMMUN ACM, V53, P54, DOI 10.1145/1831407.1831425; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Swietojanski P, 2016, IEEE-ACM T AUDIO SPE, V24, P1773, DOI 10.1109/TASLP.2016.2584700; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Weber N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980239; Williams Travis, 2018, ICLR; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423; Yu DJ, 2014, LECT NOTES ARTIF INT, V8818, P364, DOI 10.1007/978-3-319-11740-9_34; Zeiler MD, 2013, ARXIV13013557, DOI DOI 10.1007/978-3-319-26532-2_6; Zhai S., 2017, CVPR, P770	33	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902080
C	Komanduru, A; Honorio, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Komanduru, Abi; Honorio, Jean			On the Correctness and Sample Complexity of Inverse Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Inverse reinforcement learning (IRL) is the problem of finding a reward function that generates a given optimal policy for a given Markov Decision Process. This paper looks at an algorithmic-independent geometric analysis of the IRL problem with finite states and actions. A L1-regularized Support Vector Machine formulation of the IRL problem motivated by the geometric analysis is then proposed with the basic objective of the inverse reinforcement problem in mind: to find a reward function that generates a specified optimal policy. The paper further analyzes the proposed formulation of inverse reinforcement learning with n states and k actions, and shows a sample complexity of O(d(2) log(nk)) for transition probability matrices with at most d nonzeros per row, for recovering a reward function that generates a policy that satisfies Bellman's optimality condition with respect to the true transition probabilities.	[Komanduru, Abi; Honorio, Jean] Purdue Univ, W Lafayette, IN 47906 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Komanduru, A (corresponding author), Purdue Univ, W Lafayette, IN 47906 USA.	akomandu@purdue.edu; jhonorio@purdue.edu						Abbeel P., 2004, P 21 INT C MACHINE L, P1; Daneshmand H, 2014, PR MACH LEARN RES, V32, P793; Dvijotham Krishnamurthy, 2010, P 27 INT C MACH LEAR, P335, DOI DOI 10.0RG/PAPERS/571.PDF; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Levine S., 2011, C NEURAL INFORM PROC, V24, P19; Neu G., 2007, P 23 C UNC ART INT, P295; Ramachandran Deepak, 2007, URBANA, V51, P1; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Ravikumar P., 2008, ADV NEURAL INFORM PR, V20, P1201; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Zhu J, 2004, ADV NEUR IN, V16, P49; Ziebart B. D., 2008, AAAI 2008	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307016
C	Koutis, I; Le, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Koutis, Ioannis; Le, Huong			Spectral Modification of Graphs for Improved Spectral Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPARSIFICATION	Spectral clustering algorithms provide approximate solutions to hard optimization problems that formulate graph partitioning in terms of the graph conductance. It is well understood that the quality of these approximate solutions is negatively affected by a possibly significant gap between the conductance and the second eigenvalue of the graph. In this paper we show that for any graph G, there exists a 'spectral maximizer' graph H which is cut-similar to G, but has eigenvalues that are near the theoretical limit implied by the cut structure of G. Applying then spectral clustering on H has the potential to produce improved cuts that also exist in G due to the cut similarity. This leads to the second contribution of this work: we describe a practical spectral modification algorithm that raises the eigenvalues of the input graph, while preserving its cuts. Combined with spectral clustering on the modified graph, this yields demonstrably improved cuts.	[Koutis, Ioannis; Le, Huong] New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA	New Jersey Institute of Technology	Koutis, I (corresponding author), New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA.	ikoutis@njit.edu; hy14@njit.edu			 [CCF-1149048];  [CCF-1813374]	; 	This work has been partially supported by grants CCF-1149048, CCF-1813374.	Amini AA, 2013, ANN STAT, V41, P2097, DOI 10.1214/13-AOS1138; Batson J, 2013, COMMUN ACM, V56, P87, DOI 10.1145/2492007.2492029; Bienkowski Marcin, 2003, SPAA 03, DOI [10.1145/777412.777418, DOI 10.1145/777412.777418]; Bojchevski A, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P737, DOI 10.1145/3097983.3098156; Boman EG, 2003, SIAM J MATRIX ANAL A, V25, P694, DOI 10.1137/S0895479801390637; Chung F., 1997, REGIONAL C SERIES MA, V92; Cucuringu M, 2016, JMLR WORKSH CONF PRO, V51, P445; Donath W. E., 1972, IBM Technical Disclosure Bulletin, V15, P938; Durfee D, 2017, ACM S THEORY COMPUT, P730, DOI 10.1145/3055399.3055499; Fan Rong-En, 2008, TECHNICAL REPORT; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Guattery S, 1998, SIAM J MATRIX ANAL A, V19, P701, DOI 10.1137/S0895479896312262; Joseph A, 2016, ANN STAT, V44, P1765, DOI 10.1214/16-AOS1447; Kelner J. A., 2014, P 25 ANN ACM SIAM S, P217; Koutis I, 2012, COMMUN ACM, V55, P99, DOI 10.1145/2347736.2347759; Koutis I, 2011, ANN IEEE SYMP FOUND, P590, DOI 10.1109/FOCS.2011.85; Koutis Ioannis, 2008, S PAR ALG ARCH SPAA; Lee JR, 2014, J ACM, V61, DOI 10.1145/2665063; Peng R., 2016, P 27 ANN ACM SIAM S, P1862; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Qin T., 2013, ADV NEURAL INFORM PR, P3120; Qiu JZ, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P459, DOI 10.1145/3159652.3159706; Racke H, 2002, ANN IEEE SYMP FOUND, P43, DOI 10.1109/SFCS.2002.1181881; Racke H., 2014, P 25 ANN ACM SIAM S, P227, DOI [10.1137/1.9781611973402.17, DOI 10.1137/1.9781611973402.17]; Rohe K, 2018, ADV NEURAL INFORM PR, P10631; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Spielman DA, 2011, SIAM J COMPUT, V40, P981, DOI 10.1137/08074489X; Tolliver D.A., 2006, 2006 IEEE COMP SOC C, V1, P1053; Xie JY, 2016, PR MACH LEARN RES, V48	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304082
C	Krishnamurthy, A; Mazumdar, A; McGregor, A; Pal, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Krishnamurthy, Akshay; Mazumdar, Arya; McGregor, Andrew; Pal, Soumyabrata			Sample Complexity of Learning Mixtures of Sparse Linear Regressions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RESTRICTED ISOMETRY PROPERTY	In the problem of learning mixtures of linear regressions,the goal is to learn a collection of signal vectors from a sequence of (possibly noisy) linear measurements, where each measurement is evaluated on an unknown signal drawn uniformly from this collection. This setting is quite expressive and has been studied both in terms of practical applications and for the sake of establishing theoretical guarantees. In this paper, we consider the case where the signal vectors are sparse;this generalizes the popular compressed sensing paradigm. We improve upon the state-of-the-art results as follows: In the noisy case, we resolve an open question of Yin et al. (IEEE Transactions on Information Theory, 2019) by showing how to handle collections of more than two vectors and present the first robust reconstruction algorithm, i.e., if the signals are not perfectly sparse, we still learn a good sparse approximation of the signals. In the noiseless case, as well as in the noisy case, we show how to circumvent the need for a restrictive assumption required in the previous work. Our techniques are quite different from those in the previous work: for the noiseless case, we rely on a property of sparse polynomials and for the noisy case, we provide new connections to learning Gaussian mixtures and use ideas from the theory of error correcting codes.	[Krishnamurthy, Akshay] NYC, Microsoft Res, New York, NY 10001 USA; [Mazumdar, Arya; McGregor, Andrew; Pal, Soumyabrata] UMass Amherst, Amherst, NY USA	Microsoft	Krishnamurthy, A (corresponding author), NYC, Microsoft Res, New York, NY 10001 USA.	akshay@cs.umass.edu; arya@cs.umass.edu; mcgregor@cs.umass.edu; spal@cs.umass.edu			NSF [CCF 1642658, 1618512, 1909046, 1908849, 1934846]	NSF(National Science Foundation (NSF))	This research is supported in part by NSF Grants CCF 1642658, 1618512, 1909046, 1908849 and 1934846.	Arora Sanjeev, 2001, S THEOR COMP; Baraniuk R., 2006, PREPRINT, V100, P1; Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Borwein P., 1997, INDIANA U MATH J; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Curtiss DR, 1917, ANN MATH, V19, P251; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; DEVEAUX RD, 1989, COMPUT STAT DATA AN, V8, P227, DOI 10.1016/0167-9473(89)90043-1; Devroye L., 2012, COMBINATORIAL METHOD; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Faria S, 2010, J STAT COMPUT SIM, V80, P201, DOI 10.1080/00949650802590261; Gantmakher F. R., 1959, THEORY MATRICES, V131; Hardt Moritz, 2015, S THEOR COMP; Kalai AT, 2012, COMMUN ACM, V55, P113, DOI [10.1145/2076450.2076474, 10.1145/2078450.2076474]; Krishnamurthy A., 2019, 27 ANN EUR S ALG ESA; Kwon Jeongyeol, 2018, ARXIV181005752; Moitra Ankur, 2010, FDN COMPUTER SCI; Nazarov Fedor, 2017, S THEOR COMP; Stadler N, 2010, TEST-SPAIN, V19, P209, DOI 10.1007/s11749-010-0197-z; Titterington DM, 1985, STAT ANAL FINITE MIX; Viele K, 2002, STAT COMPUT, V12, P315, DOI 10.1023/A:1020779827503; Yi XY, 2014, PR MACH LEARN RES, V32, P613; Yi Xinyang, 2016, ARXIV160805749; Yin D, 2019, IEEE T INFORM THEORY, V65, P1430, DOI 10.1109/TIT.2018.2864276	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902019
C	Kuhnle, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kuhnle, Alan			Interlaced Greedy Algorithm for Maximization of Submodular Functions in Nearly Linear Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATIONS	A deterministic approximation algorithm is presented for the maximization of non-monotone submodular functions over a ground set of size n subject to cardinality constraint k; the algorithm is based upon the idea of interlacing two greedy procedures. The algorithm uses interlaced, thresholded greedy procedures to obtain tight ratio 1/4 - epsilon in O(n/epsilon log (k/epsilon)) queries of the objective function, which improves upon both the ratio and the quadratic time complexity of the previously fastest deterministic algorithm for this problem. The algorithm is validated in the context of two applications of non-monotone submodular maximization, on which it outperforms the fastest deterministic and randomized algorithms in prior literature.	[Kuhnle, Alan] Florida State Univ, Dept Comp Sci, Tallahassee, FL 32306 USA	State University System of Florida; Florida State University	Kuhnle, A (corresponding author), Florida State Univ, Dept Comp Sci, Tallahassee, FL 32306 USA.	akuhnle@fsu.edu			Florida State University	Florida State University	The work of A. Kuhnle was partially supported by Florida State University and the Informatics Institute of the University of Florida. Victoria G. Crawford and the anonymous reviewers provided helpful feedback which improved the paper.	Badanidiyuru Ashwinkumar, 2014, ACM SIAM S DISCR ALG; Balkanski Eric, 2018, ADV NEURAL INFORM PR; Buchbinder N., 2015, ACM SIAM S DISCR ALG; Buchbinder N., 2018, HDB APPROXIMATION AL, V2nd; Buchbinder N., 2014, P 25 ANN ACM SIAM S, P1433; Buchbinder N, 2018, ACM T ALGORITHMS, V14, DOI 10.1145/3184990; Buchbinder Niv, 2012, S FDN COMP SCI FOCS; Buchbinder Niv, 2016, ARXIV161103253V1; Chekuri Chandra, 2015, INT C AUT LANG PROGR; Ene Alina, 2019, ARXIV190513272; Ene Alina, 2016, S FDN COMP SCI FOCS; Fahrbach Matthew, 2019, INT C MACH LEARN ICM; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman Moran, 2017, C LEARN THEOR COLT; Feldman Moran, 2018, ADV NEURAL INFORM PR; FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195; Gillenwater J., 2012, ADV NEURAL INFORM PR; Gupta Anupam, 2010, INT WORKSH INT NETW; Kazemi Ehsan, 2019, INT C MACH LEARN ICM; Kempe D, 2003, ACM SIGKDD INT C KNO; Lee J, 2010, SIAM J DISCRETE MATH, V23, P2053, DOI 10.1137/090750020; Leskovec Jure, 2007, ACM SIGKDD INT C KNO; Mirzasoleiman Baharan, 2016, INT C MACH LEARN ICM; Mirzasoleiman Baharan, 2015, AAAI C ART INT AAAI; Mirzasoleiman Baharan, 2018, AAAI C ART INT; Naor Joseph Seffi, 2011, S FDN COMP SCI FOCS; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302038
C	Kumagai, A; Iwata, T; Fujiwara, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumagai, Atsutoshi; Iwata, Tomoharu; Fujiwara, Yasuhiro			Transfer Anomaly Detection by Inferring Latent Domain Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a method to improve the anomaly detection performance on target domains by transferring knowledge on related domains. Although anomaly labels are valuable to learn anomaly detectors, they are difficult to obtain due to their rarity. To alleviate this problem, existing methods use anomalous and normal instances in the related domains as well as target normal instances. These methods require training on each target domain. However, this requirement can be problematic in some situations due to the high computational cost of training. The proposed method can infer the anomaly detectors for target domains without re-training by introducing the concept of latent domain vectors, which are latent representations of the domains and are used for inferring the anomaly detectors. The latent domain vector for each domain is inferred from the set of normal instances in the domain. The anomaly score function for each domain is modeled on the basis of autoencoders, and its domain-specific property is controlled by the latent domain vector. The anomaly score function for each domain is trained so that the scores of normal instances become low and the scores of anomalies become higher than those of the normal instances, while considering the uncertainty of the latent domain vectors. When target normal instances can be used during training, the proposed method can also use them for training in a unified framework. The effectiveness of the proposed method is demonstrated through experiments using one synthetic and four real-world datasets. Especially, the proposed method without re-training outperforms existing methods with target specific training.	[Kumagai, Atsutoshi] NTT Software Innovat Ctr, NTT Secure Platform Labs, Tokyo, Japan; [Iwata, Tomoharu; Fujiwara, Yasuhiro] NTT Commun Sci Labs, Kyoto, Japan	Nippon Telegraph & Telephone Corporation; Nippon Telegraph & Telephone Corporation	Kumagai, A (corresponding author), NTT Software Innovat Ctr, NTT Secure Platform Labs, Tokyo, Japan.	atsutoshi.kumagai.ht@hco.ntt.co.jp; tomoharu.iwata.gy@hco.ntt.co.jp; yasuhiro.fujiwara.kh@hco.ntt.co.jp						Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39; Al-Stouhi S, 2016, KNOWL INF SYST, V48, P201, DOI 10.1007/s10115-015-0870-3; Andrews J. T., 2016, AN DET WORKSH ICML; Babar S, 2010, ICNSA; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chalapathy R., 2019, ARXIV PREPRINT ARXIV; Chalapathy R., 2018, ARXIV PREPRINT ARXIV; Chawla NV, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P875, DOI 10.1007/978-0-387-09823-4_45; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Chen Jinghui, 2017, P 2017 SIAM INT C DA, P90, DOI DOI 10.1137/1.9781611974973.11; Chen JX, 2014, PATTERN RECOGN LETT, V37, P32, DOI 10.1016/j.patrec.2013.07.017; Daume H, 2007, P 45 ANN M ASS COMP, V45, P256; Deshpande D., 2005, INFOSECCD; Dinh Laurent, 2014, ARXIV14108516; Dokas P., 2014, RECENT ADV INTRUSION, V15, P21; Fujita H, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P274, DOI 10.5220/0006613502740283; Ge L, 2014, STAT ANAL DATA MIN, V7, P254, DOI 10.1002/sam.11217; Germain M, 2015, PR MACH LEARN RES, V37, P881; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Guo HX, 2017, EXPERT SYST APPL, V73, P220, DOI 10.1016/j.eswa.2016.12.035; Hendrycks D., 2019, ICLR, P1; Herschtal Alan, 2004, P 21 INT C MACH LEAR, P49; Higgins I, 2016, BETA VAE LEARNING BA; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hoffman J, 2018, PR MACH LEARN RES, V80; Ide T., 2017, ICDM; Ide T, 2018, INT CONF DAT MIN WOR, P120, DOI 10.1109/ICDMW.2018.00024; Iwata T., 2019, SUPERVISED ANOMALY D; Keller F, 2012, PROC INT CONF DATA, P1037, DOI 10.1109/ICDE.2012.88; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kou Y., 2004, ICNSC; Kumagai A., 2018, ZERO SHOT DOMAIN ADA; Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17; Long MS, 2016, ADV NEUR IN, V29; LONG MS, 2018, ADV NEURAL INFORM PR, V31; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Ruff L, 2018, PR MACH LEARN RES, V80; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; SAITO K, 2018, CVPR; Sakurada M., 2014, P MLSDA 2014 2 WORKS, P4, DOI [DOI 10.1145/2689746.2689747, 10.1145/2689746.2689747]; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Song Q., 2018, INT C LEARN REPR; Tokui S., 2015, NEURIPS; Uria  B., 2016, J MACH LEARN RES, V17, P7184; Wang J, 2017, IEEE DATA MINING, P475, DOI 10.1109/ICDM.2017.57; Xiao YS, 2015, KNOWL INF SYST, V44, P407, DOI 10.1007/s10115-014-0765-8; Xue Y, 2007, J MACH LEARN RES, V8, P35; Yamanaka Y., 2019, AUTOENCODING BINARY; Yamanishi K, 2004, DATA MIN KNOWL DISC, V8, P275, DOI 10.1023/B:DAMI.0000023676.72185.7c; Yan L., 2003, ICML; Yuan Z., 2017, CIIS; Zaheer Manzil, 2017, NEURIPS; Zhai SF, 2016, PR MACH LEARN RES, V48; Zhou C., 2017, KDD	57	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302047
C	Kuralenok, I; Ershov, V; Labutin, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kuralenok, Igor; Ershov, Vasily; Labutin, Igor			MonoForest framework for tree ensemble analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this work, we introduce a new decision tree ensemble representation framework: instead of using a graph model we transform each tree into a well-known polynomial form. We apply the new representation to three tasks: theoretical analysis, model reduction, and interpretation. The polynomial form of a tree ensemble allows a straightforward interpretation of the original model. In our experiments, it shows comparable results with state-of-the-art interpretation techniques. Another application of the framework is the ensemble-wise pruning: we can drop monomials from the polynomial, based on train data statistics. This way we reduce the model size up to 3 times without loss of its quality. It is possible to show the equivalence of tree shape classes that share the same polynomial. This fact gives us the ability to train a model in one tree's shape and exploit it in another, which is easier for computation or interpretation. We formulate a problem statement for optimal tree ensemble translation from one form to another and build a greedy solution to this problem.	[Kuralenok, Igor; Ershov, Vasily; Labutin, Igor] Yandex, Moscow, Russia; [Kuralenok, Igor] JetBrains Res, St Petersburg, Russia; [Labutin, Igor] SPb HSE, St Petersburg, Russia		Kuralenok, I (corresponding author), Yandex, Moscow, Russia.; Kuralenok, I (corresponding author), JetBrains Res, St Petersburg, Russia.	solar@yandex-team.ru; noxoomo@yandex-team.ru; Labutin.IgorL@gmail.com		Ershov, Vasily/0000-0002-2444-5237				Aaron Fisher, 2018, ALL MODELS ARE WRONG; Avanti Shrikumar, 2017, CORR; Bach Francis R., 2011, CORR; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Carolin Strobl, 2008, BMC BIOINFORMATICS, P9; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Chen T.Q., 2018, ADV NEURAL INFORM PR; Christino Tamon, 2000, MACHINE LEARNING ECM, P404; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Hans Zantema, 1999, INT J FOUND COMPUT S, V11, P343; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Ishwaran H, 2007, ELECTRON J STAT, V1, P519, DOI 10.1214/07-EJS039; Ke Guolin, 2017, ADV NEURAL INFORM PR, V30, P3146; Kearns M., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P269; Koh PW, 2017, PR MACH LEARN RES, V70; Kohavi R., 1995, IJCAI-95. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, P1071; LAVALLE IH, 1987, THEOR DECIS, V23, P37, DOI 10.1007/BF00127336; Leo B, 1984, CLASSIFICATION REGRE; Lundberg S.M., 2017, ADV NEUR IN, P4765; Lundberg Scott M, 2018, CORR; MargineantuDragos D., 1997, P 14 INT C MACH LEAR, P211; Martinez-Munoz G, 2007, PATTERN RECOGN LETT, V28, P156, DOI 10.1016/j.patrec.2006.06.018; Martinez-Munoz G, 2009, IEEE T PATTERN ANAL, V31, P245, DOI 10.1109/TPAMI.2008.78; Mehta M., 1995, KDD-95 Proceedings. First International Conference on Knowledge Discovery and Data Mining, P216; Mingers J., 1989, Machine Learning, V4, P227, DOI 10.1023/A:1022604100933; Prokhorenkova L., 2018, ADV NEURAL INF PROCE, V2018, P6638; RAMON D, 2006, BMC BIOINFORMATICS; Ribeiro MT, 2018, AAAI CONF ARTIF INTE, P1527; Strumbelj E, 2014, KNOWL INF SYST, V41, P647, DOI 10.1007/s10115-013-0679-x; Trevor H., 2014, ELEMENTS STAT LEARNI, P106; Veronika Dorogush Anna, 2018, CORR	31	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905045
C	Kusmierczyk, T; Sakaya, J; Klami, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kusmierczyk, Tomasz; Sakaya, Joseph; Klami, Arto			Variational Bayesian Decision-making for Continuous Utilities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bayesian decision theory outlines a rigorous framework for making optimal decisions based on maximizing expected utility over a model posterior. However, practitioners often do not have access to the full posterior and resort to approximate inference strategies. In such cases, taking the eventual decision-making task into account while performing the inference allows for calibrating the posterior approximation to maximize the utility. We present an automatic pipeline that co-opts continuous utilities into variational inference algorithms to account for decision-making. We provide practical strategies for approximating and maximizing the gain, and empirically demonstrate consistent improvement when calibrating approximations for specific utilities.	[Kusmierczyk, Tomasz; Sakaya, Joseph; Klami, Arto] Univ Helsinki, Helsinki Inst Informat Technol HIIT, Dept Comp Sci, Helsinki, Finland	Aalto University; University of Helsinki	Kusmierczyk, T (corresponding author), Univ Helsinki, Helsinki Inst Informat Technol HIIT, Dept Comp Sci, Helsinki, Finland.	tomasz.kusmierczyk@helsinki.fi; joseph.sakaya@helsinki.fi; arto.klami@helsinki.fi			Academy of Finland [1266969, 1313125]; Finnish Center for Artificial Intelligence (FCAI), a Flagship of the Academy of Finland	Academy of Finland(Academy of Finland); Finnish Center for Artificial Intelligence (FCAI), a Flagship of the Academy of Finland	The work was supported by Academy of Finland (1266969, 1313125), as well as the Finnish Center for Artificial Intelligence (FCAI), a Flagship of the Academy of Finland. We also thank the Finnish Grid and Cloud Infrastructure (urn:nbn:fi:research-infras-2016072533) for computational resources.	Abbasnejad E., 2015, P 29 AAAI C ART INT; Berger J. O, 1985, STAT DECISION THEORY, P118; Bertin-Mahieux T., 2011, ISMIR; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Chen Liqun, 2018, P 35 INT C MACH LEAR; Cobb A.D., 2018, ARXIV PREPRINT ARXIV; Damoulas T, 2019, ARXIV190402063; Figurnov M, 2018, ADV NEUR IN, V31; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Gelman A., 2017, ARXIV14124869; Guo Fangjian, 2017, ARXIV161105559; Hoffman M. D., 2015, P 18 INT C ART INT S; Kingma Diederik P., 2015, 3 INT C LEARN REPRES, V3; Kusmierczyk Tomasz, 2019, ARXIV190904919; Lacoste-Julien S., 2011, P 14 INT C ART INT S; Locatello F, 2018, ADV NEUR IN, V31; Minka Thomas P., 2001, P 17 C UNC ART INT; Mnih Andriy, 2008, ADV NEURAL INFORM PR, V20; Naesseth Christian, 2017, P 20 INT C ART INT S; Rainforth Tom, 2018, P 35 INT C MACH LEAR; Ranganath Rajesh, 2014, P 17 INT C ART INT S; Rezende Danilo, 2015, P 32 INT C MACH LEAR; Robert C. P., 2007, BAYESIAN CHOICE; Ruiz Francisco J.R., 2016, ADV NEURAL INFORM PR, V29; Schaul Tom, 2013, P 1 INT C LEARN REPR; Teh Y. W., 2007, ADV NEURAL INFORM PR, V19; Titsias Michalis, 2014, P 31 INT C MACH LEAR; Yao Yuling, 2018, P 35 INT C MACH LEAR	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306040
C	Lacotte, J; Pilanci, M; Pavone, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lacotte, Jonathan; Pilanci, Mert; Pavone, Marco			High-Dimensional Optimization in Adaptive Random Subspaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NYSTROM METHOD; MATRIX; ALGORITHMS; RECOVERY; SKETCHES	We propose a new randomized optimization method for high-dimensional problems which can be seen as a generalization of coordinate descent to random subspaces. We show that an adaptive sampling strategy for the random subspace significantly outperforms the oblivious sampling method, which is the common choice in the recent literature. The adaptive subspace can be efficiently generated by a correlated random matrix ensemble whose statistics mimic the input data. We prove that the improvement in the relative error of the solution can be tightly characterized in terms of the spectrum of the data matrix, and provide probabilistic upper-bounds. We then illustrate the consequences of our theory with data matrices of different spectral decay. Extensive experimental results show that the proposed approach offers significant speed ups in machine learning problems including logistic regression, kernel classification with random convolution layers and shallow neural networks with rectified linear units. Our analysis is based on convex analysis and Fenchel duality, and establishes connections to sketching and randomized matrix decomposition.	[Lacotte, Jonathan; Pilanci, Mert] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Pavone, Marco] Stanford Univ, Dept Aeronaut &Astronaut, Stanford, CA 94305 USA	Stanford University; Stanford University	Lacotte, J (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	lacotte@stanford.edu		Pavone, Marco/0000-0002-0206-4337	National Science Foundation [IIS-1838179]; Office of Naval Research, ONR YIP Program [N00014-17-1-2433]	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research, ONR YIP Program(Office of Naval Research)	This work was partially supported by the National Science Foundation under grant IIS-1838179 and Office of Naval Research, ONR YIP Program, under contract N00014-17-1-2433.	Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Bach Francis, 2013, C LEARNING THEORY, P185; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Bollapragada R, 2019, IMA J NUMER ANAL, V39, P545, DOI 10.1093/imanum/dry009; Boutsidis C, 2013, SIAM J MATRIX ANAL A, V34, P1301, DOI 10.1137/120874540; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Friedman J., 2001, ELEMENTS STAT LEARNI, V1; Gittens A, 2016, J MACH LEARN RES, V17; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Kumar S, 2012, J MACH LEARN RES, V13, P981; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Recht B, 2018, ARXIV180600451; Rockafellar R. T., 1970, CONVEX ANAL; Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88; Tu Stephen, 2016, ARXIV160205310; Vempala SS, 2005, RANDOM PROJECTION ME, V65; Vershynin R, 2018, CAMBRIDGE SERIES STA, DOI DOI 10.1017/9781108231596; Vu K., 2017, ARXIV170602730; Williams CKI, 2001, ADV NEUR IN, V13, P682; Witten R, 2015, ALGORITHMICA, V72, P264, DOI 10.1007/s00453-014-9891-7; Yang T., 2012, ADV NEURAL INFORM PR, P476; Yang Y, 2017, ANN STAT, V45, P991, DOI 10.1214/16-AOS1472; Zhang L., 2013, P C LEARN THEOR, P135; Zhang LJ, 2014, IEEE T INFORM THEORY, V60, P7300, DOI 10.1109/TIT.2014.2359204	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902047
C	Lai, GK; Dai, ZH; Yang, YM; Yoo, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lai, Guokun; Dai, Zihang; Yang, Yiming; Yoo, Shinjae			Re-examination of the Role of Latent Variables in Sequence Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					With latent variables, stochastic recurrent models have achieved state-of-the-art performance in modeling sound-wave sequence. However, opposite results are also observed in other domains, where standard recurrent networks often outperform stochastic models. To better understand this discrepancy, we re-examine the roles of latent variables in stochastic recurrent models for speech density estimation. Our analysis reveals that under the restriction of fully factorized output distribution in previous evaluations, the stochastic variants were implicitly leveraging intra-step correlation but the deterministic recurrent baselines were prohibited to do so, resulting in an unfair comparison. To correct the unfairness, we remove such restriction in our re-examination, where all the models can explicitly leverage intra-step correlation with an auto-regressive structure. Over a diverse set of univariate and multivariate sequential data, including human speech, MIDI music, handwriting trajectory and frame-permuted speech, our results show that stochastic recurrent models fail to deliver the performance advantage claimed in previous work. In contrast, standard recurrent models equipped with an auto-regressive output distribution consistently perform better, dramatically advancing the state-of-the-art results on three speech datasets.	[Lai, Guokun; Dai, Zihang; Yang, Yiming] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Yoo, Shinjae] Brookhaven Natl Lab, Upton, NY 11973 USA	Carnegie Mellon University; United States Department of Energy (DOE); Brookhaven National Laboratory	Lai, GK (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	guokun@cs.cmu.edu; dzihang@cs.cmu.edu; yiming@cs.cmu.edu; sjyoo@bnl.gov			National Science Foundation (NSF) [IIS-1546329]; DOE-Office of Science under grant ASCR [KJ040201]	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); DOE-Office of Science under grant ASCR	This work is supported in part by the National Science Foundation (NSF) under grant IIS-1546329 and by DOE-Office of Science under grant ASCR #KJ040201.	Aksan E., 2019, ARXIV190206568; Bayer J, 2014, ARXIV14117610; Boulanger-Lewandowski N, 2012, P 29 INT C MACH LEAR, P1159; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Dai Zihang, 2019, ARXIV190102860, P2; Dinh L, 2016, ARXIV PREPRINT ARXIV; Flunkert V., 2017, ARXIV170404110; Germain M, 2015, PR MACH LEARN RES, V37, P881; Graves A, 2013, ARXIV13080850; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Lai G., 2018, ARXIV180606116; Loshchilov I., 2017, P INT C LEARNING REP; Mishra N., 2017, ARXIV171209763; Parmar Niki, 2018, ARXIV180205751, P2; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Salimans Tim, 2017, ARXIV170105517; Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI [10.1109/MASSP.1986.1165342, 10.1002/0471250953.bia03as18]; Sontag D., 2015, ARXIV151105121; Sutskever Ilya, 2009, ADV NEURAL INFORM PR, P2; Uria  B., 2016, J MACH LEARN RES, V17, P7184; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307079
C	Lawson, D; Tucker, G; Dai, B; Ranganath, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lawson, Dieterich; Tucker, George; Dai, Bo; Ranganath, Rajesh			Energy-Inspired Models: Learning with Sampler-Induced Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Energy-based models (EBMs) are powerful probabilistic models [8, 44], but suffer from intractable sampling and density evaluation due to the partition function. As a result, inference in EBMs relies on approximate sampling algorithms, leading to a mismatch between the model and inference. Motivated by this, we consider the sampler-induced distribution as the model of interest and maximize the likelihood of this model. This yields a class of energy-inspired models (EIMs) that incorporate learned energy functions while still providing exact samples and tractable log-likelihood lower bounds. We describe and evaluate three instantiations of such models based on truncated rejection sampling, self-normalized importance sampling, and Hamiltonian importance sampling. These models outperform or perform comparably to the recently proposed Learned Accept/Reject Sampling algorithm [5] and provide new insights on ranking Noise Contrastive Estimation [34, 46] and Contrastive Predictive Coding [57]. Moreover, EIMs allow us to generalize a recent connection between multi-sample variational lower bounds [9] and auxiliary variable variational inference [1, 63, 59, 47]. We show how recent variational bounds [9, 49, 52, 42, 73, 51, 65] can be unified with EIMs as the variational family.	[Lawson, Dieterich] Stanford Univ, Stanford, CA 94305 USA; [Tucker, George; Dai, Bo] Google Res, Brain Team, Mountain View, CA USA; [Lawson, Dieterich; Ranganath, Rajesh] NYU, New York, NY 10003 USA	Stanford University; Google Incorporated; New York University	Lawson, D (corresponding author), Stanford Univ, Stanford, CA 94305 USA.; Lawson, D (corresponding author), NYU, New York, NY 10003 USA.	jdlawson@stanford.edu; gjt@google.com; bodai@google.com; rajeshr@cims.nyu.edu						Agakov FV, 2004, LECT NOTES COMPUT SC, V3316, P561; Babaeizadeh Mohammad, 2017, ARXIV171011252; Bachman P., 2015, NIPS APPR INF WORKSH; Bauer M., 2018, ARXIV181011428; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Brown L. D., 1986, FUNDAMENTALS STAT EX; Burda Yuri, 2015, ICLR; Burges, 1998, MNIST DATABASE HANDW; Caterini AL, 2018, ADV NEUR IN, V31; Chen Xi, 2016, ARXIV161102731; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Cremer Chris, 2017, ARXIV170402916; Dai B., 2018, ARXIV181102228; Dai Z., 2017, ARXIV170201691; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Denton E, 2018, PR MACH LEARN RES, V80; Dinh L, 2016, ARXIV PREPRINT ARXIV; Domke J, 2019, ADV NEUR IN, V32; Domke J, 2018, ADV NEUR IN, V31; Doucet A, 2001, STAT ENG IN, P3; Du Y., 2019, ADV NEURAL INFORM PR, V6; FREUND Y, 1992, ADV NEUR IN, V4, P912; Gao RQ, 2018, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2018.00954; Goyal A, 2017, ADV NEUR IN, V30; Gulrajani I., 2016, P INT C LEARN REPR; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Jang E., 2016, ARXIV; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Jozefowicz R., 2016, ARXIV PREPRINT ARXIV; Kim T. H., 2016, CORR; Kinderman R., 1980, MARKOV RANDOM FIELDS; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Le T. A., 2017, ARXIV170510306; LeCun Y., 2006, PREDICTING STRUCTURE; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma Zhuang, 2018, P C EMP METH NAT LAN, P3698, DOI DOI 10.18653/V1/D18-1405; Maaloe L, 2016, PR MACH LEARN RES, V48; Maddison Chris J, 2016, ARXIV161100712; Mnih A, 2016, PR MACH LEARN RES, V48; Molchanov D., 2018, ARXIV181002789; Naesseth CA, 2018, PR MACH LEARN RES, V84; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neal Radford M, 2005, BANFF INT RES STAT B; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Nijkamp E, 2019, ADV NEUR IN, V32; Papamakarios George, 2017, ARXIV170507057; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Sobolev Artem, 2018, BAYES DEEP LEARN WOR; Sohl-Dickstein J., 2011, P 28 INT C INT C MAC, P905; Sontag D., 2015, ARXIV151105121; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; van den Oord Aaron, 2018, ARXIV180703748; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xiao H., 2017, FASHION MNIST NOVEL; Xie JW, 2018, PROC CVPR IEEE, P8629, DOI 10.1109/CVPR.2018.00900; Xie JW, 2016, PR MACH LEARN RES, V48; Xie JW, 2017, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2017.209; Yin Mingzhang, 2018, ARXIV180511183; ZELLNER A, 1988, AM STAT, V42, P278, DOI 10.2307/2685143; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420	75	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900013
C	Le Guen, V; Thome, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Le Guen, Vincent; Thome, Nicolas			Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CHANGE-POINT DETECTION; MULTISTEP	This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. To handle this challenging task, we introduce DILATE (DIstortion Loss including shApe and TimE), a new objective function for training deep neural networks. DILATE aims at accurately predicting sudden changes, and explicitly incorporates two terms supporting precise shape and temporal change detection. We introduce a differentiable loss function suitable for training deep neural nets, and provide a custom back-prop implementation for speeding up optimization. We also introduce a variant of DILATE, which provides a smooth generalization of temporally-constrained Dynamic Time Warping (DTW). Experiments carried out on various non-stationary datasets reveal the very good behaviour of DILATE compared to models trained with the standard Mean Squared Error (MSE) loss function, and also to DTW and variants. DILATE is also agnostic to the choice of the model, and we highlight its benefit for training fully connected networks as well as specialized recurrent architectures, showing its capacity to improve over state-of-the-art trajectory forecasting approaches.	[Le Guen, Vincent; Thome, Nicolas] EDF R&D, 6 Quai Watier, F-78401 Chatou, France	Electricite de France (EDF)	Le Guen, V (corresponding author), EDF R&D, 6 Quai Watier, F-78401 Chatou, France.	vincent.le-guen@edf.fr; nicolas.thome@cnam.fr						Abid A., 2018, ADV NEURAL INFORM PR, P10547; [Anonymous], 2018, ADV NEURAL INFORM PR; [Anonymous], 2018, INT C LEARN REPR ICL; Bao YK, 2014, NEUROCOMPUTING, V129, P482, DOI 10.1016/j.neucom.2013.09.010; Ben Taieb S, 2016, IEEE T NEUR NET LEAR, V27, P62, DOI 10.1109/TNNLS.2015.2411629; Borovykh A, 2017, STAT-US; Box G.E.P., 2015, TIME SERIES ANAL FOR; Chandra R, 2017, NEUROCOMPUTING, V243, P21, DOI 10.1016/j.neucom.2017.02.065; Chang Wei-Cheng, 2019, INT C LEARN REPR ICL; Chaudhry SA, 2017, PEER PEER NETW APPL, V10, P1, DOI 10.1007/s12083-015-0400-9; Chen Y, 2015, UCR TIME SERIES CLAS; Cho K., 2014, P 2014 C EMP METH NA, P1724; Choi E, 2016, ADV NEUR IN, V29; Cuturi M, 2017, PR MACH LEARN RES, V70; Ding Xiao, 2015, INT JOINT C ART INT; Durand T, 2019, IEEE T PATTERN ANAL, V41, P337, DOI 10.1109/TPAMI.2017.2788435; Durand T, 2015, IEEE I CONF COMP VIS, P2713, DOI 10.1109/ICCV.2015.311; Durbin J., 2012, OXFORD STATIST SCI S, Vsecond; Florita A, 2013, IEEE GREEN TECHNOL, P147, DOI 10.1109/GreenTech.2013.30; Fox I, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1387, DOI 10.1145/3219819.3220102; Frias-Paredes L, 2017, ENERG CONVERS MANAGE, V142, P533, DOI 10.1016/j.enconman.2017.03.056; Gal Y, 2016, PR MACH LEARN RES, V48; Garreau D, 2018, ELECTRON J STAT, V12, P4440, DOI 10.1214/18-EJS1513; Ghaderi A, 2017, ICML TIM SER WORKSH; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hussein S, 2016, IEEE C EVOL COMPUTAT, P3084, DOI 10.1109/CEC.2016.7744179; Hyndman RJ, 2008, SPRINGER SER STAT, P3; Jeong YS, 2011, PATTERN RECOGN, V44, P2231, DOI 10.1016/j.patcog.2010.09.022; Kuznetsov Vitaly, 2019, INT C ART INT STAT A; Lai GK, 2018, ACM/SIGIR PROCEEDINGS 2018, P95, DOI 10.1145/3209978.3210006; Laptev Nikolay, 2017, INT C MACH LEARN, P1; Li S., 2015, ADV NEURAL INFORM PR, P3366; Li S., 2019, NEURIPS; Lv YS, 2015, IEEE T INTELL TRANSP, V16, P865, DOI 10.1109/TITS.2014.2345663; Masum S, 2018, LECT NOTES ARTIF INT, V10841, P148, DOI 10.1007/978-3-319-91253-0_15; Mensch Arthur, 2018, INT C MACH LEARN ICM; An NH, 2015, 2015 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTING AND APPLICATIONS (ACOMP), P142, DOI 10.1109/ACOMP.2015.24; Nowozin S, 2014, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2014.77; Oord A. v. d., 2016, ARXIV160903499; Qin Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2627; Rivest Franqois, 2019, IEEE T NEURAL NETWOR; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Salinas David, 2017, INT C MACH LEARN ICM; Seeger MW., 2016, ADV NEURAL INFORM PR, P4646; Sen R, 2019, ADV NEURAL INFORM PR; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tao Y., 2018, ARXIV180600685; Thome N., 2018, P EUR C COMP VIS ECC, P153; Truong C, 2019, INT CONF ACOUST SPEE, P3147, DOI 10.1109/ICASSP.2019.8683471; Valiance L, 2017, SOL ENERGY, V150, P408, DOI 10.1016/j.solener.2017.04.064; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Venkatraman A., 2015, 29 AAAI C ART INT; Wang Y, 2019, INT C MACH LEARN, P6607; Wen R., 2017, MULTIHORIZON QUANTIL; Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847; Yu Jiaqian, 2018, IEEE T PATTERN ANAL; Yu R., 2017, ICML WORKSH DEEP STR, V17; Yu R., 2017, LONG TERM FORECASTIN; Zheng J, 2017, 2017 51 ANN C INFORM, P1, DOI DOI 10.1109/CISS.2017.7926112; [No title captured]	63	0	0	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304022
C	Lee, H; Mangoubi, O; Vishnoi, NK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, Holden; Mangoubi, Oren; Vishnoi, Nisheeth K.			Online Sampling from Log-Concave Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BINARY	Given a sequence of convex functions integral(0), integral(1),...,integral(T) we study the problem of sampling from the Gibbs distribution pi(t) proportional to e(-) Sigma(t)(k=0) f(k) for each epoch t in an online manner. Interest in this problem derives from applications in machine learning, Bayesian statistics, and optimization where, rather than obtaining all the observations at once, one constantly acquires new data, and must continuously update the distribution. Our main result is an algorithm that generates roughly independent samples from pi(t) for every epoch t and, under mild assumptions, makes polylog(T) gradient evaluations per epoch. All previous results imply a bound on the number of gradient or function evaluations which is at least linear in T. Motivated by real-world applications, we assume that functions are smooth, their associated distributions have a bounded second moment, and their minimizer drifts in a bounded manner, but do not assume they are strongly convex. In particular, our assumptions hold for online Bayesian logistic regression, when the data satisfy natural regularity properties, giving a sampling algorithm with updates that are poly-logarithmic in T. In simulations, our algorithm achieves accuracy comparable to an algorithm specialized to logistic regression. Key to our algorithm is a novel stochastic gradient Langevin dynamics Markov chain with a carefully designed variance reduction step and constant batch size. Technically, lack of strong convexity is a significant barrier to analysis and, here, our main contribution is a martingale exit time argument that shows our Markov chain remains in a ball of radius roughly poly-logarithmic in T for enough time to reach within epsilon of pi(t).	[Lee, Holden] Duke Univ, Durham, NC 27706 USA; [Mangoubi, Oren] Worcester Polytech Inst, Worcester, MA 01609 USA; [Vishnoi, Nisheeth K.] Yale Univ, New Haven, CT 06520 USA	Duke University; Worcester Polytechnic Institute; Yale University	Lee, H (corresponding author), Duke Univ, Durham, NC 27706 USA.				NSF [CCF-1908347]; SNSF [200021_182527]	NSF(National Science Foundation (NSF)); SNSF(Swiss National Science Foundation (SNSF))	This research was partially supported by NSF CCF-1908347 and SNSF 200021_182527 grants.	Agarwal A, 2009, IMMUNE INFERTILITY, P155, DOI 10.1007/978-3-642-01379-9_3.2; ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; [Anonymous], 2016, FDN TRENDS IN OPTIMI; [Anonymous], 2017, ARXIV170602692; Barber RF., 2016, STAT ANAL HIGH DIMEN, P15, DOI [10.1007/978-3-319-27099-9_2, DOI 10.1007/978-3-319-27099-9_2]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Campbell T, 2018, PR MACH LEARN RES, V80; Campbell T, 2019, J MACH LEARN RES, V20; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chatterji Niladri, 2018, P MACHINE LEARNING R, V80, P764; Chopin N, 2017, STAT SCI, V32, P64, DOI 10.1214/16-STS581; DE SA C., 2018, P 35 INTER80 NATL C, P1165; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154; Dumitrascu Bianca, 2018, ADV NEURAL INFORM PR; Durmus A., 2019, J MACHINE LEARNING R, V20, P1; Durmus A., 2017, ARXIV170500166; Dwivedi Raaz, 2018, P 2018 C LEARN THEOR; Faes C, 2011, J AM STAT ASSOC, V106, P959, DOI 10.1198/jasa.2011.tm10301; Filippone M, 2015, PR MACH LEARN RES, V37, P1015; Foster D. J., 2018, P MACH LEARN RES, V75, P1; Ge Rong, 2018, ARXIV181200793; Giraud F, 2017, BERNOULLI, V23, P670, DOI 10.3150/14-BEJ680; Hazan E., 2014, C LEARNING THEORY, P197; Huggins Jonathan, 2016, P ADV NEUR INF PROC, P4080; Koltchinskii V, 2015, INT MATH RES NOTICES, V2015, P12991, DOI 10.1093/imrn/rnv096; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Mendelson S, 2014, GOD AND NATURE IN THE THOUGHT OF MARGARET CAVENDISH, P27; Narayanan H, 2017, J MACH LEARN RES, V18; Nickl Richard, 2012, STAT THEORY; Russo DJ, 2018, FOUND TRENDS MACH LE, V11, P1, DOI 10.1561/2200000070; Wang L, 2011, ACTA POLYM SIN, P752, DOI 10.3724/SP.J.1105.2011.10220; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301025
C	Lee, T; Ndirango, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, Tyler; Ndirango, Anthony			Generalization in multitask deep neural classifiers: a statistical physics approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A proper understanding of the striking generalization abilities of deep neural networks presents an enduring puzzle. Recently, there has been a growing body of numerically-grounded theoretical work that has contributed important insights to the theory of learning in deep neural nets. There has also been a recent interest in extending these analyses to understanding how multitask learning can further improve the generalization capacity of deep neural nets. These studies deal almost exclusively with regression tasks which are amenable to existing analytical techniques. We develop an analytic theory of the nonlinear dynamics of generalization of deep neural networks trained to solve classification tasks using softmax outputs and cross-entropy loss, addressing both single task and multitask settings. We do so by adapting techniques from the statistical physics of disordered systems, accounting for both finite size datasets and correlated outputs induced by the training dynamics. We discuss the validity of our theoretical results in comparison to a comprehensive suite of numerical experiments. Our analysis provides theoretical support for the intuition that the performance of multitask learning is determined by the noisiness of the tasks and how well their input features align with each other. Highly related, clean tasks benefit each other, whereas unrelated, clean tasks can be detrimental to individual task performance.	[Lee, Tyler; Ndirango, Anthony] Intel AI Lab, Santa Clara, CA 95054 USA		Lee, T (corresponding author), Intel AI Lab, Santa Clara, CA 95054 USA.	tyler.p.lee@intel.com; anthony.ndirango@intel.com						Advani M. S., 2017, CORR; Arora Sanjeev, 2019, CORR; Baity-Jesi Marco, 2018, P 35 INT C MACH LEAR, P324; Bilen H., 2017, CORR; BOS S, 1993, PHYS REV E, V47, P1384, DOI 10.1103/PhysRevE.47.1384; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chaudhari Pratik, 2017, 5 INT C LEARN REPR I; DERRIDA B, 1981, PHYS REV B, V24, P2613, DOI 10.1103/PhysRevB.24.2613; Doersch Carl, 2017, CORR; Draxler F., 2018, P 35 INT C MACH LEAR, V80, P1308; Du Yunshu, 2018, ARXIV E PRINTS; Goldt Sebastian, 2019, CORR; Howard J., 2018, ARXIV PREPRINT ARXIV; Kaiser Lukasz, 2017, CORR; Kendall A., 2017, CORR, V1703.04309; Kim S, 2017, INT CONF ACOUST SPEE, P4835, DOI 10.1109/ICASSP.2017.7953075; Lampinen Andrew Kyle, 2018, CORR; Lee Junghwan, 2019, CORR; Lin Henry W., 2016, CORR; LITTLEWOOD JE, 1934, INEQUALITIES; Liu Xiaodong, 2019, CORR; Luong M.-T., 2015, ARXIV E PRINTS; Mallat S., 2016, CORR; Meyerson Elliot, 2017, CORR; Misra I., 2016, CORR; Qian YM, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P310, DOI 10.1109/ASRU.2015.7404810; Saxe A.M., 2014, 2 INT C LEARN REPR I; Sener Ozan, 2018, CORR	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907051
C	Lei, Q; Zhuo, JC; Caramanis, C; Dhillon, IS; Dimakis, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lei, Qi; Zhuo, Jiacheng; Caramanis, Constantine; Dhillon, Inderjit S.; Dimakis, Alexandros G.			Primal-Dual Block Generalized Frank-Wolfe	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a generalized variant of Frank-Wolfe algorithm for solving a class of sparse/low-rank optimization problems. Our formulation includes Elastic Net, regularized SVMs and phase retrieval as special cases. The proposed Primal-Dual Block Generalized Frank-Wolfe algorithm reduces the per-iteration cost while maintaining linear convergence rate. The per iteration cost of our method depends on the structural complexity of the solution (i.e. sparsity/low-rank) instead of the ambient dimension. We empirically show that our algorithm outperforms the state-of-the-art methods on (multi-class) classification tasks.	[Lei, Qi; Zhuo, Jiacheng; Caramanis, Constantine; Dhillon, Inderjit S.; Dimakis, Alexandros G.] UT Austin, Austin, TX 78712 USA; [Dhillon, Inderjit S.] Amazon, Seattle, WA USA	University of Texas System; University of Texas Austin; Amazon.com	Lei, Q (corresponding author), UT Austin, Austin, TX 78712 USA.	leiqi@oden.utexas.edu; jzhuo@utexas.edu; constantine@utexas.edu; inderjit@cs.utexas.edu; dimakis@austin.utexas.edu			NSF [1618689, EECS-1609279, CCF-1302435, CNS-1704778, IIS-1546452, CCF-1564000, DMS 1723052, CCF 1763702, AF 1901292]	NSF(National Science Foundation (NSF))	This work is supported by NSF Grants 1618689, EECS-1609279, CCF-1302435, CNS-1704778, IIS-1546452, CCF-1564000, DMS 1723052, CCF 1763702, AF 1901292 and research gifts by Google, Western Digital and NVIDIA.	Allen-Zhu Zeyuan, 2017, ADV NEURAL INFORM PR; [Anonymous], 2013, INTRO LECT CONVEX OP; Argyriou A., 2008, MACHINE LEARNING; Candes Emmanuel J, 2015, SIAM REV; Chang C.-C., 2011, ACM T INTELLIGENT SY; Chang Yin-Wen, 2010, J MACHINE LEARNING R; Dhillon I. S., 2011, ADV NEURAL INFORM PR; Du D.-Z., 2013, MINIMAX APPL; Du S. S., 2018, ARXIV180201504; Dudik Miroslav, 2012, ARTIFICIAL INTELLIGE; Garber D., 2015, ICML; Goldfarb D., 2017, ARXIV170307269; Hazan E., 2016, INT C MACH LEARN; Jaggi Martin., 2013, ICML; Johoson R., 2013, ADV NEURAL INFORM PR; Kakade Sham, DUALITY STRONG CONVE; Karimireddy Sai Praneeth, 2018, ARXIV181006999; Kerdreux Thomas, 2019, INT C MACH LEARN; Khot Subhash, 2005, J ACM JACM; Kumar Piyush, 2011, INFORMS J COMPUTING; Lacoste-Julien S., 2015, ADV NEURAL INFORM PR; Lacoste-Julien S., 2012, ARXIV12074747; Lan Guanghui, 2016, SIAM J OPTIMIZATION; Lei Qi, 2016, ADV NEURAL INFORM PR; Lei Qi, 2017, P 34 INT C MACH LEAR, V70; Meyer Gerard GL, 1974, SIAM J CONTROL; Nanculef R., 2014, INFORM SCI; Nesterov Yurii, 2012, SIAM J OPTIMIZATION, V2012; Nutini Julie, 2015, INT C MACH LEARN; Ortiz Carmen, 1985, TRANSPORTATION RES B; Pong Ting Kei, 2010, SIAM J OPTIMIZATION; Rahimi A, 2007, NEURAL INFORM PROCES; Shai Shalev-Shwartz, 2013, J MACHINE LEARNING R; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR; Wang Jialei, 2015, P 34 INT C MACH LEAR, V70; Xiao Lin, 2019, J MACHINE LEARNING R; Zhang Yuchen, 2014, ARXIV14093257; Zou H., 2005, J ROYAL STAT SOC B	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905053
C	Lengerich, B; Aragam, B; Xing, EP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lengerich, Benjamin; Aragam, Bryon; Xing, Eric P.			Learning Sample-Specific Models with Low-Rank Personalized Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CELL	Modern applications of machine learning (ML) deal with increasingly heterogeneous datasets comprised of data collected from overlapping latent subpopulations. As a result, traditional models trained over large datasets may fail to recognize highly predictive localized effects in favour of weakly predictive global patterns. This is a problem because localized effects are critical to developing individualized policies and treatment plans in applications ranging from precision medicine to advertising. To address this challenge, we propose to estimate sample-specific models that tailor inference and prediction at the individual level. In contrast to classical ML models that estimate a single, complex model (or only a few complex models), our approach produces a model personalized to each sample. These sample-specific models can be studied to understand subgroup dynamics that go beyond coarse-grained class labels. Crucially, our approach does not assume that relationships between samples (e.g. a similarity network) are known a priori. Instead, we use unmodeled covariates to learn a latent distance metric over the samples. We apply this approach to financial, biomedical, and electoral data as well as simulated data and show that sample-specific models provide fine-grained interpretations of complicated phenomena without sacrificing predictive accuracy compared to state-of-the-art models such as deep neural networks.	[Lengerich, Benjamin; Xing, Eric P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Aragam, Bryon] Univ Chicago, Chicago, IL 60637 USA	Carnegie Mellon University; University of Chicago	Lengerich, B (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	blengeri@cs.cmu.edu; bryon@chicagobooth.edu; epxing@cs.cmu.edu			NIH [R01GM114311]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This material is based upon work supported by NIH R01GM114311. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Institutes of Health.	Ageenko I. I., 2010, US Patent App., Patent No. [12/316,967, 12316967]; Al-Shedivat M., 2018, ARXIV180109810; Al-Shedivat M., 2017, ARXIV170510301; Buettner F, 2015, NAT BIOTECHNOL, V33, P155, DOI 10.1038/nbt.3102; Cleveland WS., 1991, STAT MODELS S, P309; Ding X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2327; Dressman HK, 2007, J CLIN ONCOL, V25, P517, DOI 10.1200/JCO.2006.06.3743; Fan JQ, 1999, ANN STAT, V27, P1491, DOI 10.1214/aos/1017939139; Fisher AJ, 2018, P NATL ACAD SCI USA, V115, pE6106, DOI 10.1073/pnas.1711978115; Gormley IC, 2008, ANN APPL STAT, V2, P1452, DOI 10.1214/08-AOAS178; Hallac D, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P387, DOI 10.1145/2783258.2783313; Hart SA, 2016, MIND BRAIN EDUC, V10, P209, DOI 10.1111/mbe.12109; HASTIE T, 1993, J ROY STAT SOC B MET, V55, P757; Jabbari Fattaneh, 2018, Proc Mach Learn Res, V72, P169; Jiang J, 2007, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-47946-0; Kuijjer ML, 2019, ISCIENCE, V14, P226, DOI 10.1016/j.isci.2019.03.021; Lengerich BJ, 2018, BIOINFORMATICS, V34, P178, DOI 10.1093/bioinformatics/bty250; Li X., 2018, FRONTIERS GENETICS, V9; Liu C, 2011, CELL, V146, P209, DOI 10.1016/j.cell.2011.06.014; Liu XP, 2016, NUCLEIC ACIDS RES, V44, DOI 10.1093/nar/gkw772; Ma SY, 2018, GENOME BIOL, V19, DOI 10.1186/s13059-018-1511-4; Margulis K, 2018, P NATL ACAD SCI USA, V115, P6347, DOI 10.1073/pnas.1803733115; Ng Kenney, 2015, AMIA Jt Summits Transl Sci Proc, V2015, P132; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Platanios Emmanouil Antonios, 2018, P 2018 C EMP METH NA, P425, DOI DOI 10.18653/V1/D18-1039; Puig X, 2014, J APPL STAT, V41, P73, DOI 10.1080/02664763.2013.830088; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042; Sollich P, 1996, ADV NEUR IN, V8, P190; Tibshirani R., 2017, ARXIV171200484; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; Vinayak RK, 2019, PR MACH LEARN RES, V97; Visweswaran S, 2010, J MACH LEARN RES, V11, P3333; Wang B, 2014, NAT METHODS, V11, P333, DOI [10.1038/NMETH.2810, 10.1038/nmeth.2810]; Xu J., 2015, P 2015 INT C DAT MIN; Yamada M., 2016, STAT, V1050, P20; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	39	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303055
C	Lesmana, NS; Zhang, X; Bei, XH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lesmana, Nixie S.; Zhang, Xuan; Bei, Xiaohui			Balancing Efficiency and Fairness in On-Demand Ridesourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				A-RIDE PROBLEM; TAXI	We investigate the problem of assigning trip requests to available vehicles in on-demand ridesourcing. Much of the literature has focused on maximizing the total value of served requests, achieving efficiency on the passengers' side. However, such solutions may result in some drivers being assigned to insufficient or undesired trips, therefore losing fairness from the drivers' perspective. In this paper, we focus on both the system efficiency and the fairness among drivers and quantitatively analyze the tradeoffs between these two objectives. In particular, we give an explicit answer to the question of whether there always exists an assignment that achieves any target efficiency and fairness. We also propose a simple reassignment algorithm that can achieve any selected tradeoff. Finally, we demonstrate the effectiveness of the algorithms through extensive experiments on real-world datasets.	[Lesmana, Nixie S.; Bei, Xiaohui] Nanyang Technol Univ, Dept Math Sci, Singapore 637371, Singapore; [Zhang, Xuan] Univ Illinois, Dept Ind & Enterprise Syst Engn, Champaign, IL 61801 USA	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Illinois System; University of Illinois Urbana-Champaign	Lesmana, NS (corresponding author), Nanyang Technol Univ, Dept Math Sci, Singapore 637371, Singapore.	nixiesap001@e.ntu.edu.sg; xuan6@illinois.edu; xhbei@ntu.edu.sg						Aslam Javed, 2002, CS0205008 ARXIV; Baldacci R, 2012, EUR J OPER RES, V218, P1, DOI 10.1016/j.ejor.2011.07.037; Bei Xiaohui, 2012, 26 AAAI C ART INT; Bertsimas D, 2012, MANAGE SCI, V58, P2234, DOI 10.1287/mnsc.1120.1549; Bertsimas D, 2011, OPER RES, V59, P17, DOI 10.1287/opre.1100.0865; Caragiannis I, 2012, THEOR COMPUT SYST, V50, P589, DOI 10.1007/s00224-011-9359-y; Chen ZL, 2006, TRANSPORT SCI, V40, P74, DOI 10.1287/trsc.1050.0133; Cho CH, 2006, PROCEEDINGS OF THE 3RD ASIAN CONFERENCE ON REFRIGERATION AND AIR-CONDITIONING VOLS I AND II, P197; Cohler Yuga J., 2011, 25 AAAI C ART INT; Cordeau JF, 2007, ANN OPER RES, V153, P29, DOI 10.1007/s10479-007-0170-8; Cordeau JF, 2006, OPER RES, V54, P573, DOI 10.1287/opre.1060.0283; Desaulniers G, 2016, OPER RES, V64, P1388, DOI 10.1287/opre.2016.1535; Donovan Dan, 2016, BRIAN; Faye A, 2016, STATIC DIAL RIDE PRO; Goel A, 2001, SIAM PROC S, P384; Goel A, 2001, J COMPUT SYST SCI, V63, P62, DOI 10.1006/jcss.2001.1755; Iancu DA, 2014, OPER RES, V62, P1283, DOI 10.1287/opre.2014.1310; Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.1038/sj.jors.2600523; Kim T. S., 2011, THESIS; Kleinberg J., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P568, DOI 10.1109/SFFCS.1999.814631; Lee DH, 2004, TRANSPORT RES REC, P193, DOI 10.3141/1882-23; Li YW, 2018, ENERGIES, V11, DOI 10.3390/en11051277; Lin K., 2018, ARXIV180206444; Ma S, 2013, PROC INT CONF DATA, P410, DOI 10.1109/ICDE.2013.6544843; Megiddo N., 1974, MATH PROGRAM, V7, P97, DOI 10.1007/BF01585506; Miao F, 2016, IEEE T AUTOM SCI ENG, V13, P463, DOI 10.1109/TASE.2016.2529580; Nedregard Ida, 2015, THESIS; Rawls J., 2009, THEORY JUSTICE; Santi P, 2014, P NATL ACAD SCI USA, V111, P13290, DOI 10.1073/pnas.1403657111; Santos DO, 2015, EXPERT SYST APPL, V42, P6728, DOI 10.1016/j.eswa.2015.04.060; Seow KT, 2010, IEEE T AUTOM SCI ENG, V7, P607, DOI 10.1109/TASE.2009.2028577; Xu Z, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P905, DOI 10.1145/3219819.3219824; Young H.P., 1995, EQUITY THEORY PRACTI	35	0	0	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305032
C	Lheritier, A; Cazals, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lheritier, Alix; Cazals, Frederic			Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TREE WEIGHTING METHOD; SEQUENTIAL PREDICTION; COMPRESSION	We propose a novel nonparametric online predictor for discrete labels conditioned on multivariate continuous features. The predictor is based on a feature space discretization induced by a full-fledged k-d tree with randomly picked directions and a recursive Bayesian distribution, which allows to automatically learn the most relevant feature scales characterizing the conditional distribution. We prove its pointwise universality, i.e., it achieves a normalized log loss performance asymptotically as good as the true conditional entropy of the labels given the features. The time complexity to process the n-th sample point is O (log n) in probability with respect to the distribution generating the data points, whereas other exact nonparametric methods require to process all past observations. Experiments on challenging datasets show the computational and statistical efficiency of our algorithm in comparison to standard and state-of-the-art methods.	[Lheritier, Alix] Amadeus SAS, F-06902 Sophia Antipolis, France; [Cazals, Frederic] Univ Cote Azur, INRIA, F-06902 Sophia Antipolis, France	Amadeus; Inria; UDICE-French Research Universities; Universite Cote d'Azur	Lheritier, A (corresponding author), Amadeus SAS, F-06902 Sophia Antipolis, France.	alix.lheritier@amadeus.com; frederic.cazals@inria.fr						ALGOET P, 1992, ANN PROBAB, V20, P901, DOI 10.1214/aop/1176989811; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Barron A. R., 1998, BAYESIAN STAT, V6, P27; Begleiter R, 2006, J MACH LEARN RES, V7, P379; Cai HX, 2005, 2005 IEEE International Symposium on Information Theory (ISIT), Vols 1 and 2, P2340; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Devroye L., 1996, PROBABILISTIC THEORY, V31, DOI 10.1007/978-1-4612-0711-5; Gretton A, 2012, ADV NEURAL INF PROCE; Grunwald P D., 2007, MINIMUM DESCRIPTION; Gyorfi L, 1999, IEEE T INFORM THEORY, V45, P2642, DOI 10.1109/18.796420; HALL P, 1988, BIOMETRIKA, V75, P705; JEFFREYS H, 1946, PROC R SOC LON SER-A, V186, P453, DOI 10.1098/rspa.1946.0056; Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181; Kozat SS, 2007, IEEE T SIGNAL PROCES, V55, P3730, DOI 10.1109/TSP.2007.894235; KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Lheritier A, 2018, IEEE T INFORM THEORY, V64, P3361, DOI 10.1109/TIT.2018.2800658; Lichman M, 2013, UCI MACHINE LEARNING; Lugosi G., 2005, MODELING UNCERTAINTY, P225; Mahmoud HM, 1992, EVOLUTION RANDOM SEA; Merhav N, 1998, IEEE T INFORM THEORY, V44, P2124, DOI 10.1109/18.720534; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; RISSANEN J, 1992, IEEE T INFORM THEORY, V38, P315, DOI 10.1109/18.119689; Shtarkov YM, 1987, PROBL INFORM TRANSM, V23, P3; Silva J.M.V.D., 2008, THESIS; Tziortziotis N, 2014, J MACH LEARN RES, V15, P2313; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; van Erven T, 2012, J R STAT SOC B, V74, P361, DOI 10.1111/j.1467-9868.2011.01025.x; Veness J., 2017, ARXIV171201897; Veness J, 2012, IEEE DATA COMPR CONF, P327, DOI 10.1109/DCC.2012.39; Willems FMJ, 1998, IEEE T INFORM THEORY, V44, P792, DOI 10.1109/18.661523; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012; YU B, 1992, PROBAB THEORY REL, V92, P195, DOI 10.1007/BF01194921	36	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906026
C	Li, HH; Cabeli, V; Sella, N; Isambert, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Honghao; Cabeli, Vincent; Sella, Nadir; Isambert, Herve			Constraint-based Causal Structure Learning with Consistent Separating Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LATENT	We consider constraint-based methods for causal structure learning, such as the PC algorithm or any PC-derived algorithms whose first step consists in pruning a complete graph to obtain an undirected graph skeleton, which is subsequently oriented. All constraint-based methods perform this first step of removing dispensable edges, iteratively, whenever a separating set and corresponding conditional independence can be found. Yet, constraint-based methods lack robustness over sampling noise and are prone to uncover spurious conditional independences in finite datasets. In particular, there is no guarantee that the separating sets identified during the iterative pruning step remain consistent with the final graph. In this paper, we propose a simple modification of PC and PC-derived algorithms so as to ensure that all separating sets identified to remove dispensable edges are consistent with the final graph, thus enhancing the explainability of constraint-based methods. It is achieved by repeating the constraint-based causal structure learning scheme, iteratively, while searching for separating sets that are consistent with the graph obtained at the previous iteration. Ensuring the consistency of separating sets can be done at a limited complexity cost, through the use of block-cut tree decomposition of graph skeletons, and is found to increase their validity in terms of actual d-separation. It also significantly improves the sensitivity of constraint-based methods while retaining good overall structure learning performance. Finally and foremost, ensuring sepset consistency improves the interpretability of constraint-based models for real-life applications.	[Li, Honghao; Cabeli, Vincent; Sella, Nadir; Isambert, Herve] PSL Res Univ, CNRS, UMR168, Inst Curie, Paris, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute of Chemistry (INC); UDICE-French Research Universities; PSL Research University Paris; Sorbonne Universite; UNICANCER; Institut Curie; Universite Paris Cite	Isambert, H (corresponding author), PSL Res Univ, CNRS, UMR168, Inst Curie, Paris, France.	honghao.li@curie.fr; vincent.cabeli@curie.fr; nadir.sella@curie.fr; herve.isambert@curie.fr	li, honghao/HGC-1028-2022	Cabeli, Vincent/0000-0003-0818-2518	French Ministry of Higher Education and Research; PSL Research University; Sorbonne University	French Ministry of Higher Education and Research; PSL Research University; Sorbonne University	The authors acknowledge financial support from the French Ministry of Higher Education and Research, PSL Research University and Sorbonne University.	Affeldt S, 2016, BMC BIOINFORMATICS, V17; Affeldt S, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P42; Colombo D, 2014, J MACH LEARN RES, V15, P3741; Colombo D, 2012, ANN STAT, V40, P294, DOI 10.1214/11-AOS940; Hyttinen A., 2013, P 20 9 C UNC ART INT, P301; Kalisch M, 2012, J STAT SOFTW, V47, P1; Kalisch M, 2008, J COMPUT GRAPH STAT, V17, P773, DOI 10.1198/106186008X381927; Koller D., 2009, PROBABILISTIC GRAPHI; PEARL J, 1991, KNOWLEDGE REPRESENTA, P441; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Richardson T, 2002, ANN STAT, V30, P962; Scheines R, 1998, MULTIVAR BEHAV RES, V33, P65, DOI 10.1207/s15327906mbr3301_3; Scutari M, 2010, J STAT SOFTW, V35, P1, DOI 10.18637/jss.v035.i03; Sella N, 2018, BIOINFORMATICS, V34, P2311, DOI 10.1093/bioinformatics/btx844; Spirtes P., 1991, Social Science Computer Review, V9, P62, DOI 10.1177/089443939100900106; Spirtes P, 1999, COMPUTATION, CAUSATION, AND DISCOVERY, P211; Spirtes P., 2000, CAUSATION PREDICTION; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Verny L, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005662; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905088
C	Li, LG; Pluta, D; Shahbaba, B; Fortin, N; Ombao, H; Baldi, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Lingge; Pluta, Dustin; Shahbaba, Babak; Fortin, Norbert; Ombao, Hernando; Baldi, Pierre			Modeling Dynamic Functional Connectivity with Latent Factor Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POSTERIOR DISTRIBUTIONS; RATES; FMRI	Dynamic functional connectivity, as measured by the time-varying covariance of neurological signals, is believed to play an important role in many aspects of cognition. While many methods have been proposed, reliably establishing the presence and characteristics of brain connectivity is challenging due to the high dimensionality and noisiness of neuroimaging data. We present a latent factor Gaussian process model which addresses these challenges by learning a parsimonious representation of connectivity dynamics. The proposed model naturally allows for inference and visualization of connectivity dynamics. As an illustration of the scientific utility of the model, application to a data set of rat local field potential activity recorded during a complex non-spatial memory task provides evidence of stimuli differentiation.	[Li, Lingge; Pluta, Dustin; Shahbaba, Babak; Fortin, Norbert; Baldi, Pierre] UC Irvine, Irvine, CA 92697 USA; [Ombao, Hernando] KAUST, Thuwal, Saudi Arabia	University of California System; University of California Irvine; King Abdullah University of Science & Technology	Li, LG (corresponding author), UC Irvine, Irvine, CA 92697 USA.	linggel@uci.edu; dpluta@uci.edu; babaks@uci.edu; norbert.fortin@uci.edu; hernando.ombao@kaust.edu.sa; pfbaldi@ics.uci.edu		Shahbaba, Babak/0000-0002-8102-1609	NIH [R01-MH115697]; NSF [DMS1622490, BSC-1439267]; Whitehall Foundation [2010-05-84]; NSF CAREER award [IOS1150292]; KAUST research fund	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); Whitehall Foundation; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); KAUST research fund	This work was supported by NIH award R01-MH115697 (B.S., H.O., N.J.F), NSF award DMS1622490 (B.S.), Whitehall Foundation Award 2010-05-84 (N.J.F.), NSF CAREER award IOS1150292 (N.J.F.), NSF award BSC-1439267 (N.J.F.), and KAUST research fund (H.O.). We would like to thank Michele Guindani (UC-Irvine), Weining Shen (UC-Irvine), and Moo Chung (Univ. of Wisconsin) for their helpful comments regarding this work.	Aguilar O., 1998, BAYESIAN STAT, V6, P1; Allen EA, 2014, CEREB CORTEX, V24, P663, DOI 10.1093/cercor/bhs352; Allen TA, 2016, J NEUROSCI, V36, P1547, DOI 10.1523/JNEUROSCI.2874-15.2016; Arsigny V, 2005, LECT NOTES COMPUT SC, V3749, P115; Bhattacharya A, 2011, BIOMETRIKA, V98, P291, DOI 10.1093/biomet/asr013; Bilmes J. A, 1998, GENTLE TUTORIAL EM A; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Carvalho C. M., 2009, J MACHINE LEARNING R, V5, P73; Chiu TYM, 1996, J AM STAT ASSOC, V91, P198, DOI 10.2307/2291396; Demertzi A, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aat7603; Fiecas M, 2016, J AM STAT ASSOC, V111, P1440, DOI 10.1080/01621459.2016.1165683; Fox EB, 2015, J MACH LEARN RES, V16, P2501; Ghosal S, 2007, ANN STAT, V35, P192, DOI 10.1214/009053606000001172; Handwerker DA, 2012, NEUROIMAGE, V63, P1712, DOI 10.1016/j.neuroimage.2012.06.078; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Holbrook A, 2017, STAT-US, V6, P53, DOI 10.1002/sta4.137; Kastner G, 2017, J COMPUT GRAPH STAT, V26, P905, DOI 10.1080/10618600.2017.1322091; Khosla Meenakshi, 2018, ARXIV181211477; Lan Shiwei, 2017, FLEXIBLE BAYESIAN DY; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Leonardi N, 2015, NEUROIMAGE, V104, P430, DOI 10.1016/j.neuroimage.2014.09.007; Lindquist MA, 2014, NEUROIMAGE, V101, P531, DOI 10.1016/j.neuroimage.2014.06.052; Lopes HF, 2004, STAT SINICA, V14, P41; Motta G, 2012, BIOMETRICS, V68, P825, DOI 10.1111/j.1541-0420.2012.01744.x; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Nielsen Soren FV, 2016, ARXIV160100496; Ombao H, 2018, NEUROIMAGE, V180, P609, DOI 10.1016/j.neuroimage.2017.11.061; Prado R., 2010, TIME SERIES MODELING; Preti MG, 2017, NEUROIMAGE, V160, P41, DOI 10.1016/j.neuroimage.2016.12.061; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Samdin SB, 2019, I S BIOMED IMAGING, P1483, DOI 10.1109/ISBI.2019.8759405; Ting CM, 2018, IEEE T MED IMAGING, V37, P1011, DOI 10.1109/TMI.2017.2780185; van der Vaart A.W., 2008, PUSHING LIMITS CONT, P200; Van Der Wart AW, 2008, ANN STAT, V36, P1435, DOI 10.1214/009053607000000613; Wang YX, 2016, IEEE J-STSP, V10, P1315, DOI 10.1109/JSTSP.2016.2600023; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Zhu HT, 2009, J AM STAT ASSOC, V104, P1203, DOI 10.1198/jasa.2009.tm08096	38	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID	33041607				2022-12-19	WOS:000534424308030
C	Li, MN; Wu, LS; Ammar, HB; Wang, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Minne; Wu, Lisheng; Ammar, Haitham Bou; Wang, Jun			Multi-View Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper is concerned with multi-view reinforcement learning (MVRL), which allows for decision making when agents share common dynamics but adhere to different observation models. We define the MVRL framework by extending partially observable Markov decision processes (POMDPs) to support more than one observation model and propose two solution methods through observation augmentation and cross-view policy transfer. We empirically evaluate our method and demonstrate its effectiveness in a variety of environments. Specifically, we show reductions in sample complexities and computational time for acquiring policies that handle multi-view environments.	[Li, Minne; Wu, Lisheng; Ammar, Haitham Bou; Wang, Jun] UCL, London, England	University of London; University College London	Li, MN (corresponding author), UCL, London, England.	minne.li@cs.ucl.ac.uk; lisheng.wu.17@ucl.ac.uk; haitham.bouammar71@googlemail.com; junwang@cs.ucl.ac.uk						Al-Shedivat M., 2017, ARXIV171003641; Ammar HB, 2014, PR MACH LEARN RES, V32, P1206; Ammar HB, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3345; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barati E, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2002; Chen J, 2017, IEEE INT CONF COMP V, P1050, DOI 10.1109/ICCVW.2017.128; Clavera I., 2018, ARXIV180311347; Coumans Erwin, 2016, PYBULLET PYTHON MODU; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Duan Y., 2016, RL2 FAST REINFORCEME; Finn C, 2017, ARXIV170303400; Frans Kevin, 2017, P INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu SX, 2016, PR MACH LEARN RES, V48; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Leibfried F., 2016, ARXIV161107078; Leurent E., 2018, ENV AUTONOMOUS DRIVI; Levine S, 2014, ADV NEUR IN, V27; Li MN, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P983, DOI 10.1145/3308558.3313433; Li Y, 2018, IEEE T KNOWLEDGE DAT; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Parisotto Emilio, 2015, ARXIV151106342; Schmidhuber J., 2018, ADV NEURAL INFORM PR, P2455; Schmidhuber J., 2015, ARXIV151109249; Schulman J., 2017, ABS170706347 CORR; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Teh Y., 2017, ADV NEURAL INFORM PR, P4496; Tian Z., 2019, P 28 INT JOINT C ART, P602; Wahlstrom N., 2015, ARXIV150202251; Wang J.X., 2016, ARXIV161105763; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746; Wen Yeming, 2019, INT C LEARN REPR; Xu C., 2013, ARXIV; Yang Y., 2018, P 35 ICML JUL, V80, P5571; Zhao J, 2017, INFORM FUSION, V38, P43, DOI 10.1016/j.inffus.2017.02.007; Zoph B., 2016, ICLR	41	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301041
C	Li, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Rui			Multivariate Sparse Coding of Nonstationary Covariances with Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE	This paper studies statistical characteristics of multivariate observations with irregular changes in their covariance structures across input space. We propose a unified nonstationary modeling framework to jointly encode the observation correlations to generate a piece-wise representation with a hyper-level Gaussian process (GP) governing the overall contour of the pieces. In particular, we couple the encoding process with automatic relevance determination (ARD) to promote sparsity to account for the inherent redundancy. The hyper GP enables us to share statistical strength among the observation variables over a collection of GPs defined within the observation pieces to characterize the variables' respective local smoothness. Experiments conducted across domains show superior performances over the state-of-the-art methods.	[Li, Rui] Rochester Inst Technol, Golisano Coll Comp & Informat Sci, Rochester, NY 14623 USA	Rochester Institute of Technology	Li, R (corresponding author), Rochester Inst Technol, Golisano Coll Comp & Informat Sci, Rochester, NY 14623 USA.	rxlics@rit.edu			National Science Foundation [NSF-1850492]	National Science Foundation(National Science Foundation (NSF))	This work is funded in part by National Science Foundation (NSF-1850492).	Aldor-Noiman S, 2016, STAT SINICA, V26, P1587, DOI 10.5705/ss.2014.217t; Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675; Faul S, 2007, IEEE T BIO-MED ENG, V54, P2151, DOI 10.1109/TBME.2007.895745; Fox E.B., 2012, P 25 INT C NEURAL IN, V25, P737, DOI DOI 10.5555/2999134.2999217; FRIGESSI A, 1993, J R STAT SOC B, V55, P205; Gramacy RB, 2008, J AM STAT ASSOC, V103, P1119, DOI 10.1198/016214508000000689; Heinonen M, 2016, JMLR WORKSH CONF PRO, V51, P732; Holmes CC, 2006, BAYESIAN ANAL, V1, P145, DOI 10.1214/06-BA105; Kim HM, 2005, J AM STAT ASSOC, V100, P653, DOI 10.1198/016214504000002014; Liu S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/2/026026; McDowall D, 2012, J QUANT CRIMINOL, V28, P389, DOI 10.1007/s10940-011-9145-7; Meeds E., 2005, ADV NEURAL INFORM PR, P883; Paciorek C.J., 2003, ADV NEURAL INFORM PR, P273; Park S, 2010, JMLR WORKSH CONF PRO, V13, P95; Rasmussen CE, 2002, ADV NEUR IN, V14, P881; Snelson Edward, 2007, P 11 INT C ARTIFICIA, P524; Taddy MA, 2010, J AM STAT ASSOC, V105, P1403, DOI 10.1198/jasa.2010.ap09655; Varatharahah Yogatheesan, 2017, NIPS, P5377; Warren CP, 2010, J NEUROPHYSIOL, V104, P3530, DOI 10.1152/jn.00368.2010; Wetjen NM, 2009, J NEUROSURG, V110, P1147, DOI 10.3171/2008.8.JNS17643; Worrell GA, 2008, BRAIN, V131, P928, DOI 10.1093/brain/awn006; Wulsin DF, 2014, ARTIF INTELL, V216, P55, DOI 10.1016/j.artint.2014.05.006	22	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301058
C	Li, SY; Jin, XY; Xuan, Y; Zhou, XY; Chen, WH; Wang, YX; Yan, XF		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Shiyang; Jin, Xiaoyong; Xuan, Yao; Zhou, Xiyou; Chen, Wenhu; Wang, Yu-Xiang; Yan, Xifeng			Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NETWORKS	Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer [1]. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dot-product self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only O(L(log L)(2)) memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and real-world datasets show that it compares favorably to the state-of-the-art.	[Li, Shiyang; Jin, Xiaoyong; Xuan, Yao; Zhou, Xiyou; Chen, Wenhu; Wang, Yu-Xiang; Yan, Xifeng] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA	University of California System; University of California Santa Barbara	Li, SY (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.	shiyangli@ucsb.edu; x_jin@ucsb.edu; yxuan@ucsb.edu; xiyou@ucsb.edu; wenhuchen@ucsb.edu; yuxiangw@cs.ucsb.edu; xyan@cs.ucsb.edu	Zhou, Xiyou/GPG-0516-2022					[Anonymous], 2013, PREPRINT ARXIV 1308; [Anonymous], 2018, ADV NEURAL INFORM PR; Borovykh A, 2017, STAT-US; Box G.E.P., 2015, TIME SERIES ANAL FOR; BOX GEP, 1968, ROY STAT SOC C-APP, V17, P91; Chapados Nicolas, 2014, ARXIV14053738; Child R., 2019, ARXIV190410509; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cooijmans T., 2016, P 5 INT C LEARN REPR; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Durbin J., 2012, OXFORD STATIST SCI S, Vsecond; Flunkert V., 2017, ARXIV170404110; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Gray Scott, 2017, ARXIV171109224; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Huang Cheng-Zhi Anna, 2018, ARXIV180904281, P2; Jin X., 2019, MULTISTEP DEEP AUTOR; Khandelwal U., 2018, ARXIV180504623; Kingma D.P, P 3 INT C LEARNING R; Lai GK, 2018, ACM/SIGIR PROCEEDINGS 2018, P95, DOI 10.1145/3209978.3210006; Laptev Nikolay, 2017, INT C MACH LEARN, P1; Liu Peter J., 2018, GENERATING WIKIPEDIA, P2; Maddix D.C., 2018, ARXIV181200098; Makridakis S, 2018, INT J FORECASTING, V34, P802, DOI 10.1016/j.ijforecast.2018.06.001; Parmar Niki, 2018, ARXIV180205751; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Poggio T, 2017, INT J AUTOM COMPUT, V14, P503, DOI 10.1007/s11633-017-1054-2; Povey D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5874; Radford Alec, 2018, IMPROVING LANGUAGE S; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wen R., 2017, TIME SERIES WORKSHOP; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847; Yu R., 2017, P 2017 SIAM INT C DA, DOI [10.1137/1.9781611974973.87, DOI 10.1137/1.9781611974973.87]; Yu R., 2017, LONG TERM FORECASTIN; Zhang GQ, 1998, INT J FORECASTING, V14, P35, DOI 10.1016/S0169-2070(97)00044-7	39	0	0	17	42	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305026
C	Li, S; Tang, GG; Wakin, MB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Shuang; Tang, Gongguo; Wakin, Michael B.			The Landscape of Non-convex Empirical Risk with Degenerate Population Risk	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RANK MATRIX RECOVERY; OPTIMIZATION	The landscape of empirical risk has been widely studied in a series of machine learning problems, including low-rank matrix factorization, matrix sensing, matrix completion, and phase retrieval. In this work, we focus on the situation where the corresponding population risk is a degenerate non-convex loss function, namely, the Hessian of the population risk can have zero eigenvalues. Instead of analyzing the non-convex empirical risk directly, we first study the landscape of the corresponding population risk, which is usually easier to characterize, and then build a connection between the landscape of the empirical risk and its population risk. In particular, we establish a correspondence between the critical points of the empirical risk and its population risk without the strongly Morse assumption, which is required in existing literature but not satisfied in degenerate scenarios. We also apply the theory to matrix sensing and phase retrieval to demonstrate how to infer the landscape of empirical risk from that of the corresponding population risk.	[Li, Shuang; Tang, Gongguo; Wakin, Michael B.] Colorado Sch Mines, Dept Elect Engn, Golden, CO 80401 USA	Colorado School of Mines	Li, S (corresponding author), Colorado Sch Mines, Dept Elect Engn, Golden, CO 80401 USA.	shuangli@mines.edu; gtang@mines.edu; mwakin@mines.edu	Wakin, Michael/G-1582-2012	Wakin, Michael/0000-0002-2165-4586	NSF [CCF-1704204]; DARPA Lagrange Program under ONR/SPAWAR [N660011824020]	NSF(National Science Foundation (NSF)); DARPA Lagrange Program under ONR/SPAWAR	SL would like to thank Qiuwei Li at Colorado School of Mines for many helpful discussions on the analysis of matrix sensing and phase retrieval. The authors would also like to thank the anonymous reviewers for their constructive comments and suggestions which greatly improved the quality of this paper. This work was supported by NSF grant CCF-1704204, and the DARPA Lagrange Program under ONR/SPAWAR contract N660011824020.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; [Anonymous], 2017, ADV NEURAL INF PROCE; [Anonymous], 2017, ARXIV171103247; [Anonymous], 2016, ADV NEURAL INFORM PR; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100; Ge R., 2017, ARXIV PREPRINT ARXIV; Ge R, 2017, PR MACH LEARN RES, V70; Guo P, 2018, INT POW ELEC APPLICA, P113; Jiang XM, 2017, CHIN CONTR CONF, P4894, DOI 10.23919/ChiCC.2017.8028127; Jin C, 2017, PR MACH LEARN RES, V70; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Li Q., 2017, AS C SIGN SYST COMP; Li QW, 2019, INT CONF ACOUST SPEE, P7928, DOI 10.1109/ICASSP.2019.8682568; Li XG, 2019, IEEE T INFORM THEORY, V65, P3489, DOI 10.1109/TIT.2019.2898663; Mei S., 2016, ARXIV160706534; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; VAPNIK V, 1992, ADV NEUR IN, V4, P831; Zhu Z, 2017, ARXIV170301256; Zhu Z., 2018, ARXIV181103129; ZOU ZW, 2018, INFORM INFERENCE J I, V8, P51	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303049
C	Li, TB; Ke, YP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Tianbo; Ke, Yiping			Thinning for Accelerating the Learning of Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE; SIMULATION	This paper discusses one of the most fundamental issues about point processes that what is the best sampling method for point processes. We propose thinning as a downsampling method for accelerating the learning of point processes. We find that the thinning operation preserves the structure of intensity, and is able to estimate parameters with less time and without much loss of accuracy. Theoretical results including intensity, parameter and gradient estimation on a thinned history are presented for point processes with decouplable intensities. A stochastic optimization algorithm based on the thinned gradient is proposed. Experimental results on synthetic and real-world datasets validate the effectiveness of thinning in the tasks of parameter and gradient estimation, as well as stochastic optimization.	[Li, Tianbo; Ke, Yiping] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Li, TB (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.	tianbo001@e.ntu.edu.sg; ypke@ntu.edu.sg		Ke, Yiping/0000-0001-9473-3202	Data Science and Artificial Intelligence Research Centre (DSAIR); School of Computer Science and Engineering at Nanyang Technological University	Data Science and Artificial Intelligence Research Centre (DSAIR); School of Computer Science and Engineering at Nanyang Technological University(Nanyang Technological University)	This work is partially supported by the Data Science and Artificial Intelligence Research Centre (DSAIR) and the School of Computer Science and Engineering at Nanyang Technological University.	Achab M, 2017, PR MACH LEARN RES, V70; Andersen PK., 2012, STAT MODELS BASED CO; [Anonymous], 1991, POINT PROCESSES THEI; [Anonymous], 2014, ICML; Bacry E., 2015, ARXIV150100725; Blundell C, 2012, ADV NEURAL INFORM PR, P2600; BOKER F, 1986, STOCH PROC APPL, V23, P143, DOI 10.1016/0304-4149(86)90021-9; Bowsher CG, 2007, J ECONOMETRICS, V141, P876, DOI 10.1016/j.jeconom.2006.11.007; Bremaud Pierre, 1981, POINTP ROCESSES QUEU, V50; BRILLINGER DR, 1975, ANN PROBAB, V3, P909, DOI 10.1214/aop/1176996218; Daley D. J., 2007, INTRO THEORY POINT P, V2; Daley DJ., 2002, INTRO THEORY POINT P, VI; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Du N, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P219, DOI 10.1145/2783258.2783411; Guo C., P 2013 23 INT C FIEL, P1; KALLENBERG O, 1975, J APPL PROBAB, V12, P269, DOI 10.2307/3212440; Kingma D.P, P 3 INT C LEARNING R; Lemonnier R, 2017, AAAI CONF ARTIF INTE, P2168; LEWIS PAW, 1979, NAV RES LOG, V26, P403, DOI 10.1002/nav.3800260304; LI S, 2018, ADV NEURAL INFORM PR, P10781; Li TB, 2018, IEEE DATA MINING, P1116, DOI 10.1109/ICDM.2018.00145; Liu B, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1043; Luo Dixin, 2015, 24 INT JOINT C ART I; Mavroforakis C, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1421, DOI 10.1145/3038912.3052669; Mei H., 2017, ADV NEURAL INFORM PR, P6754; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; OGATA Y, 1988, J AM STAT ASSOC, V83, P9, DOI 10.1080/01621459.1988.10478560; SERFOZO R, 1984, MATH OPER RES, V9, P522, DOI 10.1287/moor.9.4.522; Tran J, 2015, NEW APPROACH RELIG, P91; Wang YC, 2016, GEOTECH SP, P226; Xu H, 2016, INT C MACH LEARN, P1717; Xu Hongteng, 2018, ARXIV180204725; Yang Y., 2017, NEURAL INFORM PROCES; Zha H, 2014, 28 AAAI C ART INT; Zhou K, 2013, ICML; Zhou K., 2013, ARTIF INTELL, P641	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304013
C	Li, X; Yang, WH; Zhang, ZH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Xiang; Yang, Wenhao; Zhang, Zhihua			A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose and study a general framework for regularized Markov decision processes (MDPs) where the goal is to find an optimal policy that maximizes the expected discounted total reward plus a policy regularization term. The extant entropy-regularized MDPs can be cast into our framework. Moreover, under our framework, many regularization terms can bring multi-modality and sparsity, which are potentially useful in reinforcement learning. In particular, we present sufficient and necessary conditions that induce a sparse optimal policy. We also conduct a full mathematical analysis of the proposed regularized MDPs, including the optimality condition, performance error, and sparseness control. We provide a generic method to devise regularization forms and propose off-policy actor critic algorithms in complex environment settings. We empirically analyze the numerical properties of optimal policies and compare the performance of different sparse regularization forms in discrete and continuous environments.	[Li, Xiang] Peking Univ, Sch Math Sci, Beijing, Peoples R China; [Yang, Wenhao] Peking Univ, Ctr Data Sci, Beijing, Peoples R China; [Zhang, Zhihua] Peking Univ, Sch Math Sci, Natl Engn Lab Big Data Anal & Applicat, Beijing, Peoples R China	Peking University; Peking University; Peking University	Li, X (corresponding author), Peking Univ, Sch Math Sci, Beijing, Peoples R China.	lx10077@pku.edu.cn; yangwenhaosms@pku.edu.cn; zhzhang@math.pku.edu.cn	李, 翔/HDO-6262-2022; zhang, zh/GWV-4677-2022; Yang, wenhao/GXG-5965-2022		Key Project of MOST of China [2018AAA0101000]; Beijing Municipal Commission of Science and Technology [181100008918005]; Beijing Academy of Artificial Intelligence (BAAI)	Key Project of MOST of China; Beijing Municipal Commission of Science and Technology; Beijing Academy of Artificial Intelligence (BAAI)	This work is sponsored by the Key Project of MOST of China (No. 2018AAA0101000), by Beijing Municipal Commission of Science and Technology under Grant No. 181100008918005, and by Beijing Academy of Artificial Intelligence (BAAI).	[Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; [Anonymous], 2016, ARXIV PREPRINT ARXIV; Asadi K., 2017, ICML, P243; Bellmann R., 1957, DYNAMIC PROGRAMMING; Belousov B., 2017, ARXIV180100056; Bertsekas D. P., 2008, ENCY OPTIMIZATION, P2555; Brockman G., 2016, OPENAI GYM; Chow Y., 2018, INT C MACH LEARN, P979; Farahmand AM, 2009, P AMER CONTR CONF, P725, DOI 10.1109/ACC.2009.5160611; Fox R., 2015, ARXIV PREPRINT ARXIV; Fujimoto S., 2018, ARXIV180209477; Geist M., 2019, ARXIV190111275; Grau-Moya Jordi, 2018, SOFT Q LEARNING MUTU; Haarnoja T., 2018, P 35 INT C MACH LEAR; Haarnoja T., 2017, ARXIV170208165; Haarnoja Tuomas, 2018, ARXIV181205905; Johns J., 2010, ADV NEURAL INFORM PR, P1009; Kingma D.P, P 3 INT C LEARNING R; Kolter JZ, 2009, P 26 ANN INT C MACH, P521, DOI DOI 10.1145/1553374.1553442; Lee K., 2018, IEEE ROBOT AUTOM LET, V3, P1466, DOI [10.1109/LRA.2018.2800085, DOI 10.1109/LRA.2018.2800085]; Liu Y., 2017, ARXIV170402399; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nachum O., 2017, ARXIV170701891; Nachum O, 2017, NIPS, P2775; Oh S, 2019, ARXIV190200137; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Puterman M.L., 2014, MARKOV DECISION PROC; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2017, EQUIVALENCE POLICY; Smart D.R., 1980, FIXED POINT THEOREMS, V66; Todorov E., 2007, ADV NEURAL INFORM PR, V19, DOI 10.7551/mitpress/7503.003.0176; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; TSALLIS C, 1988, J STAT PHYS, V52, P479, DOI 10.1007/BF01016429; Vamplew P, 2017, NEUROCOMPUTING, V263, P74, DOI 10.1016/j.neucom.2016.09.141; Van Hasselt H, 2010, ADV NEURAL INFORM PR, P2613; Vandenberghe L., 2010, CVXOPT LINEAR QUADRA	41	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305088
C	Li, XY; Li, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Xiaoyun; Li, Ping			Generalization Error Analysis of Quantized Compressive Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RANDOM PROJECTIONS; NEIGHBOR	Compressive(1) learning is an effective method to deal with very high dimensional datasets by applying learning algorithms in a randomly projected lower dimensional space. In this paper, we consider the learning problem where the projected data is further compressed by scalar quantization, which is called quantized compressive learning. Generalization error bounds are derived for three models: nearest neighbor (NN) classifier, linear classifier and least squares regression. Besides studying finite sample setting, our asymptotic analysis shows that the inner product estimators have deep connection with NN and linear classification problem through the variance of their debiased counterparts. By analyzing the extra error term brought by quantization, our results provide useful implications to the choice of quantizers in applications involving different learning tasks. Empirical study is also conducted to validate our theoretical findings.	[Li, Xiaoyun] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA; [Li, Ping] Baidu Res, Cognit Comp Lab, Bellevue, WA 98004 USA	Rutgers State University New Brunswick; Baidu	Li, XY (corresponding author), Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA.	xiaoyun.li@rutgers.edu; liping11@baidu.com						Boufounos PT, 2008, 2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3, P16, DOI 10.1109/CISS.2008.4558487; Buhler J, 2001, BIOINFORMATICS, V17, P419, DOI 10.1093/bioinformatics/17.5.419; Calderbank R, 2009, COMPRESSED LEARNING; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965; Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437; Chen J, 2010, PROCEEDINGS OF THE ASME SUMMER BIOENGINEERING CONFERENCE, 2010, P281; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Dahl GE, 2013, INT CONF ACOUST SPEE, P3422, DOI 10.1109/ICASSP.2013.6638293; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Durrant RJ, 2013, ICML, P693; Fagin R., 2003, P 2003 ACM SIGMOD IN, P301, DOI [10.1145/872757.872795, DOI 10.1145/872757.872795]; FRITZ J, 1975, IEEE T INFORM THEORY, V21, P552, DOI 10.1109/TIT.1975.1055443; Garg A., 2002, ICML, P171; Gottlieb Lee-Ad, 2014, ADV NEURAL INFORM PR, P370; GYORFI L, 1978, IEEE T INFORM THEORY, V24, P512, DOI 10.1109/TIT.1978.1055900; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Kaban A, 2014, JMLR WORKSH CONF PRO, V33, P448; Kabdn Ata, 2015, ACML15, P65; Klartag B, 2005, J FUNCT ANAL, V225, P229, DOI 10.1016/j.jfa.2004.10.009; LI P, 2017, ADV NEURAL INFORM PR, P456; Li P, 2017, PR MACH LEARN RES, V54, P1430; Li P, 2016, JMLR WORKSH CONF PRO, V51, P1515; Li Ping, 2016, ADV NEURAL INFORM PR, P2748; Li Ping, 2014, P 31 INT C MACH LEAR; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Lu HY, 2019, MOL INFORM, V38, DOI 10.1002/minf.201900030; Maillard O., 2009, ADV NEURAL INFORM PR, P1213; MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Slawski M, 2017, PR MACH LEARN RES, V54, P1207; Thanei GA, 2017, CONTRIB STAT, P51, DOI 10.1007/978-3-319-41573-4_3; Vempala Santosh S., 2004, RANDOM PROJECTION ME; WAGNER TJ, 1971, IEEE T INFORM THEORY, V17, P566, DOI 10.1109/TIT.1971.1054698; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906077
C	Li, XY; Li, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Xiaoyun; Li, Ping			Random Projections with Asymmetric Quantization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				JOHNSON	The method of random projection has been a popular tool for data compression, similarity search, and machine learning. In many practical scenarios, applying quantization on randomly projected data could be very helpful to further reduce storage cost and facilitate more efficient retrievals, while only suffering from little loss in accuracy. In real-world applications, however, data collected from different sources may be quantized under different schemes, which calls for a need to study the asymmetric quantization problem. In this paper, we investigate the cosine similarity estimators derived in such setting under the Lloyd-Max (LM) quantization scheme. We thoroughly analyze the biases and variances of a series of estimators including the basic simple estimators, their normalized versions, and their debiased versions. Furthermore, by studying the monotonicity, we show that the expectation of proposed estimators increases with the true cosine similarity, on a broader family of stair-shaped quantizers. Experiments on nearest neighbor search justify the theory and illustrate the effectiveness of our proposed estimators.	[Li, Xiaoyun] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA; [Li, Ping] Baidu Res USA, Cognit Comp Lab, Bellevue, WA 98004 USA	Rutgers State University New Brunswick; Baidu	Li, XY (corresponding author), Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA.	xiaoyun.li@rutgers.edu; liping11@baidu.com						Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Anderson Theodore W., 2003, INTRO MULTIVARIATE S; BERGER JM, 1961, INFORM CONTROL, V4, P68, DOI 10.1016/S0019-9958(61)80037-5; Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245, DOI 10.1145/502512.502546; Boufounos PT, 2008, 2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3, P16, DOI 10.1109/CISS.2008.4558487; Buhler J, 2002, J COMPUT BIOL, V9, P225, DOI 10.1089/10665270252935430; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Cao R, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON ELECTRIC POWER EQUIPMENT-SWITCHING TECHNOLOGY (ICEPE-ST), P456; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Dahl GE, 2013, INT CONF ACOUST SPEE, P3422, DOI 10.1109/ICASSP.2013.6638293; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dasgupta S., 2000, UNCERTAINTY ARTIFICI, P143; Dasgupta S, 2008, ACM S THEORY COMPUT, P537; Datar M., 2004, P ACM 20 ANN S COMP, P253; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Dua D., 2017, UCI MACHINE LEARNING; Fagin R., 2003, P 2003 ACM SIGMOD IN, P301, DOI DOI 10.1145/872757.872795; Fern XZ, 2003, P INT C MACH LEARN, V20, P186; Freund Y., 2007, ADV NEURAL INFORM PR, P473; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jacques L, 2013, IEEE T INFORM THEORY, V59, P2082, DOI 10.1109/TIT.2012.2234823; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Li P, 2006, LECT NOTES ARTIF INT, V4005, P635, DOI 10.1007/11776420_46; Li P, 2019, AAAI CONF ARTIF INTE, P4205; Li P, 2017, PR MACH LEARN RES, V54, P1430; Li P, 2016, JMLR WORKSH CONF PRO, V51, P1515; Li X, 2008, WIRELESS AD HOC AND SENSOR NETWORKS: THEORY AND APPLICATIONS, P1, DOI 10.1017/CBO9780511754722; Li Xiaoyun, 2019, ADV NEURAL INFORM PR; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548; PRADHAN SS, 2000, P C INF SCI SYST CIS; Vempala Santosh S., 2004, RANDOM PROJECTION ME; Younis O., 2004, P 23 ANN JOINT C IEE; Zhong ZC, 2008, INT CONF NANO MICRO, P120; Zymnis A, 2010, IEEE SIGNAL PROC LET, V17, P149, DOI 10.1109/LSP.2009.2035667	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902048
C	Li, Z; De Sa, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Zheng; De Sa, Christopher			Dimension-Free Bounds for Low-Precision Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models. Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates. These bounds tend to depend on the dimension of the model d in that the number of bits needed to achieve a particular error bound increases as d increases. In this paper, we derive new bounds for low-precision training algorithms that do not contain the dimension d, which lets us better understand what affects the convergence of these algorithms as parameters scale. Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization.	[Li, Zheng] Tsinghua Univ, IIIS, Beijing, Peoples R China; [De Sa, Christopher] Cornell Univ, Ithaca, NY 14853 USA	Tsinghua University; Cornell University	Li, Z (corresponding author), Tsinghua Univ, IIIS, Beijing, Peoples R China.	lzlz19971997@gmail.com; cdesa@cs.comell.edu						Alistarh D, 2017, ADV NEUR IN, V30; Burger D., 2017, MICROSOFT RES MICROS, V22; Caulfield AM, 2017, IEEE MICRO, V37, P52, DOI 10.1109/MM.2017.51; Courbariaux M., 2015, ADV NEUR IN, P3123; Courbariaux M., 2014, ABS14127024 CORR; Das D., 2018, CORR; De Sa C., 2015, NIPS, P2674; De Sa C., 2018, ARXIV180303383; De Sa C, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P561, DOI 10.1145/3079856.3080248; Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906; Gupta S., 2015, P 32 INT C MACH LEAR, V37, P1737; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Hubara I, 2016, ADV NEUR IN, V29; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Koster U., 2017, ADV NEURAL INFORM PR, P1742; Lee EH, 2017, INT CONF ACOUST SPEE, P5900, DOI 10.1109/ICASSP.2017.7953288; LI H., 2017, ADV NEURAL INFORM PR, P5813; Li JL, 2017, INT CONF MACH LEARN, P35; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Wu S., 2018, ARXIV180204680; Zhou Shuchang, 2016, P IEEE C COMP VIS PA; Zhu Chenzhuo, 2016, ARXIV161201064	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													29	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903037
C	Li, ZZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Zhize			SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle Points	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INEQUALITIES	We analyze stochastic gradient algorithms for optimizing nonconvex problems. In particular, our goal is to find local minima (second-order stationary points) instead of just finding first-order stationary points which may be some bad unstable saddle points. We show that a simple perturbed version of stochastic recursive gradient descent algorithm (called SSRGD) can find an (epsilon, delta)-second-order stationary point with (O) over tilde (root n/c(2) + root n/delta(4) + n/delta(3)) stochastic gradient complexity for nonconvex finite-sum problems. As a by-product, SSRGD finds an epsilon-first-order stationary point with O(n + root n/epsilon(2)) stochastic gradients. These results are almost optimal since Fang et al. [11] provided a lower bound Omega(root n/epsilon(2)) for finding even just an epsilon-first-order stationary point. We emphasize that SSRGD algorithm for finding second-order stationary points is as simple as for finding first-order stationary points just by adding a uniform perturbation sometimes, while all other algorithms for finding second-order stationary points with similar gradient complexity need to combine with a negative-curvature search subroutine (e.g., Neon2 [4]). Moreover, the simple SSRGD algorithm gets a simpler analysis. Besides, we also extend our results from nonconvex finite-sum problems to nonconvex online (expectation) problems, and prove the corresponding convergence results.	[Li, Zhize] Tsinghua Univ, Beijing, Peoples R China; [Li, Zhize] KAUST, Thuwal, Saudi Arabia	Tsinghua University; King Abdullah University of Science & Technology	Li, ZZ (corresponding author), Tsinghua Univ, Beijing, Peoples R China.; Li, ZZ (corresponding author), KAUST, Thuwal, Saudi Arabia.	zhizeli.thu@gmail.com			Office of Sponsored Research of KAUST	Office of Sponsored Research of KAUST	This work was supported by Office of Sponsored Research of KAUST, through the Baseline Research Fund of Prof. Peter Richtarik. The author would like to thank Rong Ge (Duke), Jian Li (Tsinghua) and the anonymous reviewers for their useful discussions/suggestions.	AGARWAL N, 2016, ARXIV161101146; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Zeyuan, 2018, ADV NEURAL INFORM PR, P2680; [Anonymous], 2016, ADV NEURAL INFORM PR; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Carmon Yair, 2016, ARXIV161100756; Daneshmand Hadi, 2018, ARXIV180305999; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Du S.S., 2017, ADV NEURAL INFORM PR, P1067; Chung F, 2006, INTERNET MATH, V3, P79, DOI 10.1080/15427951.2006.10129115; Ge R., 2017, ARXIV PREPRINT ARXIV; Ge R., 2019, C LEARN THEOR; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Jin C, 2017, PR MACH LEARN RES, V70; Lan Guanghui, 2019, ADV NEURAL INFORM PR; Lei LH, 2017, ADV NEUR IN, V30; Li Z., 2018, ADV NEURAL INFORM PR, P5569; Lin Zhouchen, 2018, ADV NEURAL INFORM PR, P687; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nguyen LM, 2017, PR MACH LEARN RES, V70; Pham N. H., 2019, ARXIV190205679; Reddi SJ, 2016, PR MACH LEARN RES, V48; Tao T, 2015, ANN PROBAB, V43, P782, DOI 10.1214/13-AOP876; TROPP JA, 2012, FOUND COMPUT MATH, V12, P389, DOI DOI 10.1007/s10208-011-9099-z; Tropp Joel A, 2011, TECHNICAL REPORT; Wang Zhe, 2018, ARXIV181010690; Xu Yi, 2018, ADV NEURAL INFORM PR, P5535; Zeyuan Allen-Zhu, 2018, ADV NEURAL INFORM PR, P3720; Zhou D., 2018, ARXIV180608782; Zhou D., 2018, ADV NEURAL INFORM PR	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301050
C	Li, ZH; Fresacher, M; Scarlett, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Zihan; Fresacher, Matthias; Scarlett, Jonathan			Learning Erdos-Renyi Random Graphs via Edge Detecting Queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we consider the problem of learning an unknown graph via queries on groups of nodes, with the result indicating whether or not at least one edge is present among those nodes. While learning arbitrary graphs with n nodes and k edges is known to be hard in the sense of requiring Omega(min{k(2) log n, n(2)}) tests (even when a small probability of error is allowed), we show that learning an Erdos-Renyi random graph with an average of (k) over bar edges is much easier; namely, one can attain asymptotically vanishing error probability with only O((k) over bar log n) tests. We establish such bounds for a variety of algorithms inspired by the group testing problem, with explicit constant factors indicating a near-optimal number of tests, and in some cases asymptotic optimality including constant factors. In addition, we present an alternative design that permits a near-optimal sublinear decoding time of O((k) over bar log(2) (k) over bar + (k) over bar log n).	[Li, Zihan; Scarlett, Jonathan] Natl Univ Singapore, Singapore, Singapore; [Fresacher, Matthias] Univ Adelaide, Adelaide, SA, Australia	National University of Singapore; University of Adelaide	Li, ZH (corresponding author), Natl Univ Singapore, Singapore, Singapore.	lizihan@u.nus.edu; matthias.fresacher@adelaide.edu.au; scarlett@comp.nus.edu.sg	Scarlett, Jonathan/AGK-0892-2022	Fresacher, Matthias/0000-0003-0677-3701	NUS Early Career Research Award	NUS Early Career Research Award	This work was supported by an NUS Early Career Research Award.	Abasi H., 2018, LEARNING GRAPHS EDGE; Abasi H, 2018, THEOR COMPUT SCI, V716, P15, DOI 10.1016/j.tcs.2017.11.019; AIGNER M, 1988, J GRAPH THEOR, V12, P45, DOI 10.1002/jgt.3190120106; AIGNER M, 1986, DISCRETE APPL MATH, V14, P215, DOI 10.1016/0166-218X(86)90026-0; Aldridge M., 2019, GROUP TESTING INFORM; Aldridge M, 2019, IEEE T INFORM THEORY, V65, P2058, DOI 10.1109/TIT.2018.2873136; Aldridge M, 2017, IEEE T INFORM THEORY, V63, P7142, DOI 10.1109/TIT.2017.2748564; Aldridge M, 2014, IEEE T INFORM THEORY, V60, P3671, DOI 10.1109/TIT.2014.2314472; Alon N, 2005, SIAM J DISCRETE MATH, V18, P697, DOI 10.1137/S0895480103431071; Alon N, 2004, SIAM J COMPUT, V33, P487, DOI 10.1137/S0097539702420139; Angluin D, 2008, J COMPUT SYST SCI, V74, P546, DOI 10.1016/j.jcss.2007.06.006; Angluin D, 2006, J MACH LEARN RES, V7, P2215; Baldassini L, 2013, IEEE INT SYMP INFO, P2676, DOI 10.1109/ISIT.2013.6620712; Bollobas Bla, 2001, RANDOM GRAPHS, P215; Bondorf S., 2019, SUBLINEAR TIME NONAD; Bouvel M, 2005, LECT NOTES COMPUT SC, V3787, P16; Bshouty NH, 2015, LECT NOTES COMPUT SC, V9079, P74, DOI 10.1007/978-3-319-18173-8_5; Cai S, 2017, IEEE T INFORM THEORY, V63, P2113, DOI 10.1109/TIT.2017.2659619; Chun Lam Chan, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1832; Coja-Oghlan A., 2019, INT C AUT LANG PROG; Cover TM, 2006, ELEMENTS INFORM THEO; D'yachkov AG, 2016, IEEE INT SYMP INFO, P1178, DOI 10.1109/ISIT.2016.7541485; deCaen D, 1997, DISCRETE MATH, V169, P217, DOI 10.1016/S0012-365X(96)00107-0; Du D., 2000, SER APPL M, V12; Galambos J., 1996, BONFERRONI TYPE INEQ; GILBERT EN, 1959, ANN MATH STAT, V30, P1141, DOI 10.1214/aoms/1177706098; Grebinski V, 1998, DISCRETE APPL MATH, V88, P147, DOI 10.1016/S0166-218X(98)00070-5; Hwang F. K., 2006, POOLING DESIGNS NONA, V18; Inan H. A., 2019, IEEE T INF THEORY; Johann P, 2002, DISCRETE APPL MATH, V117, P99, DOI 10.1016/S0166-218X(01)00181-0; Johnson O, 2019, IEEE T INFORM THEORY, V65, P707, DOI 10.1109/TIT.2018.2861772; Kameli H, 2018, DISCRETE MATH THEOR, V20; Lee K., 2015, SAFFRON FAST EFFICIE; Malioutov D, 2012, INT CONF ACOUST SPEE, P3305, DOI 10.1109/ICASSP.2012.6288622; Malyutov M. B., 1980, MAT ZAMETKI, V29, P109; Motwani R., 2010, RANDOMIZED ALGORITHM; Scarlett J., 2018, NOISY NONADAPTIVE GR; Scarlett J., 2016, P ACM SIAM S DISC AL; Scarlett J, 2018, IEEE J-STSP, V12, P902, DOI 10.1109/JSTSP.2018.2844818; Scarlett Jonathan, 2019, INTRO GUIDE FANOS IN; Shanmugam K., 2014, ADV NEUR INF P SYS N	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300037
C	Liang, KJ; Wang, GY; Li, YT; Henao, R; Carin, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liang, Kevin J.; Wang, Guoyin; Li, Yitong; Henao, Ricardo; Carin, Lawrence			Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We investigate time-dependent data analysis from the perspective of recurrent kernel machines, from which models with hidden units and gated memory cells arise naturally. By considering dynamic gating of the memory cell, a model closely related to the long short-term memory (LSTM) recurrent neural network is derived. Extending this setup to n-gram filters, the convolutional neural network (CNN), Gated CNN, and recurrent additive network (RAN) are also recovered as special cases. Our analysis provides a new perspective on the LSTM, while also extending it to n-gram convolutional filters. Experiments(1) are performed on natural language processing tasks and on analysis of local field potentials (neuroscience). We demonstrate that the variants we derive from kernels perform on par or even better than traditional neural methods. For the neuroscience application, the new models demonstrate significant improvements relative to the prior state of the art.	[Liang, Kevin J.; Wang, Guoyin; Li, Yitong; Henao, Ricardo; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA	Duke University	Liang, KJ (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	kevin.liang@duke.edu; guoyin.wang@duke.edu; yitong.li@duke.edu; ricardo.henao@duke.edu; lcarin@duke.edu		Liang, Kevin/0000-0002-0221-9108	DARPA; DOE; NIH; NSF; ONR	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	The research reported here was supported in part by DARPA, DOE, NIH, NSF and ONR.	[Anonymous], 2015, INT C MACH LEARN; [Anonymous], 2016, ARXIV160903499; [Anonymous], 2002, LEARNING KERNELS; [Anonymous], 2018, CHI C HUM FACT COMP; [Anonymous], 2014, EMPIRICAL METHODS NA; Anselmi F., 2015, ARXIV150801084; Ba L.J, 2016, P C WORKSH NEUR INF; Balduzzi David, 2016, INT C MACH LEARN; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Bietti Alberto, 2017, NEURAL INFORM PROCES; Bradbury J., 2017, P INT C LEARN REPR I, P1; Cheng Jianpeng, 2016, NEURAL SUMMARIZATION; Cho K., 2014, EMPIRICAL METHODS NA; Dauphin Y. N., 2017, INT C MACH LEARN; Genton M. G, 2001, J MACHINE LEARNING R; Golub David, 2016, EMPIRICAL METHODS NA; Greff K., 2017, T NEURAL NETWORKS LE; Hermans Michiel, 2012, NEURAL COMPUTATION; Hochreiter S, 1997, NEURAL COMPUTATION; LeCun Y., 1998, P IEEE; LeCun Y., 1995, HDB BRAIN THEORY NEU; Lee Kenton, 2017, ARXIV170507393V2; Lei Tao, 2017, INT C MACH LEARN; Li Yunzhu, 2017, NEURAL INFORM PROCES; Mairal Julien, 2016, NEURAL INFORM PROCES; Marcus Mitchell S. B., 1993, COMPUT LINGUIS; Merity S., 2017, INT C LEARN REPR; Merity Stephen, 2018, INT C LEARN REPR; Mia Xu Chen, 2018, ARXIV180409849V2; Pennington Jeffrey, 2014, P 2014 EMNLP; Roth Christopher, 2019, INT C LEARN REPR; Tallec C., 2018, ARXIV; van Enkhuizen J, 2013, BEHAV BRAIN RES, V249, P44, DOI 10.1016/j.bbr.2013.04.023; Vaswani A., 2017, PROC ADV; Wang Jin, 2016, DIMENSIONAL SENTIMEN; Wilson Andrew Gordon, 2016, INT C ART INT STAT; Zhang Xiang, 2015, NEURAL INFORM PROCES; Zhou C, 2015, COMPUT ENCE	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303039
C	Liang, LY; Jin, LW; Xu, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liang, Lingyu; Jin, Lianwen; Xu, Yong			Adaptive GNN for Image Analysis and Editing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ENHANCEMENT; REGULARIZATION; DIFFUSION	Graph neural network (GNN) has powerful representation ability, but optimal configurations of GNN are non-trivial to obtain due to diversity of graph structure and cascaded nonlinearities. This paper aims to understand some properties of GNN from a computer vision (CV) perspective. In mathematical analysis, we propose an adaptive GNN model by recursive definition, and derive its relation with two basic operations in CV: filtering and propagation operations. The proposed GNN model is formulated as a label propagation system with guided map, graph Laplacian and node weight. It reveals that 1) the guided map and node weight determine whether a GNN leads to filtering or propagation diffusion, and 2) the kernel of graph Laplacian controls diffusion pattern. In practical verification, we design a new regularization structure with guided feature to produce GNN-based filtering and propagation diffusion to tackle the ill-posed inverse problems of quotient image analysis (QIA), which recovers the reflectance ratio as a signature for image analysis or adjustment. A flexible QIA-GNN framework is constructed to achieve various image-based editing tasks, like face illumination synthesis and low-light image enhancement. Experiments show the effectiveness of the QIA-GNN, and provide new insights of GNN for image analysis and editing.	[Liang, Lingyu; Jin, Lianwen] South China Univ Tech, Guangzhou, Peoples R China; [Xu, Yong] South China Univ Tech, Peng Cheng Lab, Guangzhou, Peoples R China	South China University of Technology; South China University of Technology	Jin, LW (corresponding author), South China Univ Tech, Guangzhou, Peoples R China.; Xu, Y (corresponding author), South China Univ Tech, Peng Cheng Lab, Guangzhou, Peoples R China.	lianglysky@gmail.com; lianwen.jin@gmail.com; yxu@scut.edu.cn			Natural Science Foundation of Guangdong Province [2017A030312006, 2019A1515011045, 2016A030308013]; National Key Research and Development Program of China [2016YFB1001405]; NSFC [61673182, 61771199, 61502176]; GDSTP [2017A010101027]; GZSTP [201704020134]; Fundamental Research Funds for the Central Universities [2019MS023]; National Nature Science Foundation of China [61672241, U1611461]; Science and Technology Program of Guangzhou [201802010055]	Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); GDSTP; GZSTP; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Program of Guangzhou	Lingyu Liang and Lianwen Jin are supported by Natural Science Foundation of Guangdong Province (No. 2017A030312006, 2019A1515011045), the National Key Research and Development Program of China (No. 2016YFB1001405), NSFC (Grant No.: 61673182, 61771199, 61502176), GDSTP (No. 2017A010101027), GZSTP (No. 201704020134) and Fundamental Research Funds for the Central Universities (No. 2019MS023); Yong Xu is supported by National Nature Science Foundation of China (61672241, U1611461), Natural Science Foundation of Guangdong Province (2016A030308013), and Science and Technology Program of Guangzhou (201802010055).	Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32; An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639; [Anonymous], 2005, THEORETICAL NUMERICA; Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513; Chapelle O., 2006, IEEE T NEURAL NETW, V20, P542; Chen XW, 2016, IEEE T IMAGE PROCESS, V25, P1688, DOI 10.1109/TIP.2016.2523429; Chen XW, 2013, IEEE T IMAGE PROCESS, V22, P4249, DOI 10.1109/TIP.2013.2271548; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168; Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666; Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304; Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025; Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592; Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732; Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Jia XX, 2018, NEUROCOMPUTING, V322, P216, DOI 10.1016/j.neucom.2018.09.064; Kemelmacher-Shlizerman I, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925871; Kipf TN, 2017, P 5 INT C LEARN REPR; Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397; Krasula L, 2017, IEEE T IMAGE PROCESS, V26, P1496, DOI 10.1109/TIP.2017.2651374; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Li C., 2018, IEEE T PAMI; Li Q, 2010, VISUAL COMPUT, V26, P41, DOI 10.1007/s00371-009-0375-8; Liang LY, 2017, IEEE T CIRC SYST VID, V27, P125, DOI 10.1109/TCSVT.2016.2602812; Liu RS, 2016, IEEE T PATTERN ANAL, V38, P2457, DOI 10.1109/TPAMI.2016.2522415; Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008; Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329; Nestmeyer Thomas, 2017, P CVPR, V2, P4; Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024; NORDSTROM KN, 1990, IMAGE VISION COMPUT, V8, P318, DOI 10.1016/0262-8856(90)80008-H; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; Saleem A, 2017, NEUROCOMPUTING, V226, P161, DOI 10.1016/j.neucom.2016.11.044; Santoro A., 2018, INT C MACH LEARN, P4477; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Shashua A, 2001, IEEE T PATTERN ANAL, V23, P129, DOI 10.1109/34.908964; Tao Xin, 2018, ARXIV180201770; Weisfeiler B., 1968, NAUCHNOTECHNICHESKAY, V2, P12; Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37; Xiao B, 2018, NEUROCOMPUTING, V275, P2798, DOI 10.1016/j.neucom.2017.11.057; Xu HT, 2017, PROC CVPR IEEE, P3825, DOI 10.1109/CVPR.2017.407; Xu K., 2019, P ICLR; Xu L, 2015, PR MACH LEARN RES, V37, P1669; Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423; Ye W, 2018, IEEE T IMAGE PROCESS, V27, P4465, DOI 10.1109/TIP.2018.2838660; Yue HJ, 2017, IEEE T IMAGE PROCESS, V26, P3981, DOI 10.1109/TIP.2017.2703078; Zhang Kai, 2018, IEEE T IMAGE PROCESS; Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438; Zhang R., 2017, ARXIV170502999; Zhang S, 2017, NEUROCOMPUTING, V245, P1, DOI 10.1016/j.neucom.2017.03.029; Zhang Z., 2018, ARXIV181204202; Zhou J., 2018, ARXIV181208434; Zhu X., 2002, TECH REP; Zhu Xiaojin., 2003, P ICLR, P912	57	0	0	3	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303061
C	Ligett, K; Shenfeld, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ligett, Katrina; Shenfeld, Moshe			A necessary and sufficient stability notion for adaptive generalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a new notion of the stability of computations, which holds under post-processing and adaptive composition. We show that the notion is both necessary and sufficient to ensure generalization in the face of adaptivity, for any computations that respond to bounded-sensitivity linear queries while providing accuracy with respect to the data sample set. The stability notion is based on quantifying the effect of observing a computation's outputs on the posterior over the data sample elements. We show a separation between this stability notion and previously studied notion and observe that all differentially private algorithms also satisfy this notion.	[Ligett, Katrina; Shenfeld, Moshe] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Ligett, K (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.	katrina@cs.huji.ac.il; moshe.shenfeld@cs.huji.ac.il			Israel Science Foundation (ISF) [1044/16]; United States Air Force [FA8750-16-C-0022]; DARPA [FA8750-16-C-0022]; Federmann Cyber Security Center; Israel national cyber directorate	Israel Science Foundation (ISF)(Israel Science Foundation); United States Air Force(United States Department of Defense); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Federmann Cyber Security Center; Israel national cyber directorate	This work was supported in part by Israel Science Foundation (ISF) grant 1044/16, the United States Air Force and DARPA under contract FA8750-16-C-0022, and the Federmann Cyber Security Center in conjunction with the Israel national cyber directorate. Part of this work was done while the authors were visiting the Simons Institute for the Theory of Computing. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Air Force and DARPA.	Alabdulmohsin I, 2017, PR MACH LEARN RES, V54, P92; [Anonymous], 2016, ARXIV160301508; [Anonymous], 2018, ARXIV180707878; [Anonymous], 2017, ADV NEURAL INFORM PR; Bassily R., 2016, ARXIV160403336; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Bassily Raef, 2017, ARXIV171005233; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cummings R., 2016, P 29 C LEARN THEOR C, P772; De A, 2012, LECT NOTES COMPUT SC, V7194, P321, DOI 10.1007/978-3-642-28914-9_18; Dwork C., 2015, ADV NEURAL INF PROCE, P2350; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, ACM S THEORY COMPUT, P117, DOI 10.1145/2746539.2746580; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Esposito A. R., 2019, ARXIV190301777; Feldman V., 2017, ARXIV171207196; Gelman A, 2014, AM SCI, V102, P460, DOI 10.1511/2014.111.460; Ioannidis JPA, 2005, PLOS MED, V2, P696, DOI 10.1371/journal.pmed.0020124; Kasiviswanathan S. P., 2014, J PRIVACY CONFIDENTI, V6; LITTLESTONE N, 1986, RELATING DATA COMPRE; Nissim K., 2018, ADV NEURAL INFORM PR, P6402; Raginsky Maxim, 2016, 2016 IEEE Information Theory Workshop (ITW), P26, DOI 10.1109/ITW.2016.7606789; Rogers R, 2016, ANN IEEE SYMP FOUND, P487, DOI 10.1109/FOCS.2016.59; Russo D, 2016, JMLR WORKSH CONF PRO, V51, P1232; Shalev-Shwartz Shai, 2014, UNDERSTANDINGMACHINE; Tao T, 2015, ANN PROBAB, V43, P782, DOI 10.1214/13-AOP876; Zrnic Tijana, 2019, ARXIV190111143	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903015
C	Liu, CD; Yu, GW; Chang, C; Rai, HMS; Ma, JW; Gorti, SK; Volkovs, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Chundi; Yu, Guangwei; Chang, Cheng; Rai, Himanshu; Ma, Junwei; Gorti, Satya Krishna; Volkovs, Maksims			Guided Similarity Separation for Image Retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite recent progress in computer vision, image retrieval remains a challenging open problem. Numerous variations such as view angle, lighting and occlusion make it difficult to design models that are both robust and efficient. Many leading methods traverse the nearest neighbor graph to exploit higher order neighbor information and uncover the highly complex underlying manifold. In this work we propose a different approach where we leverage graph convolutional networks to directly encode neighbor information into image descriptors. We further leverage ideas from clustering and manifold learning, and introduce an unsupervised loss based on pairwise separation of image similarities. Empirically, we demonstrate that our model is able to successfully learn a new descriptor space that significantly improves retrieval accuracy, while still allowing efficient inner product inference. Experiments on five public benchmarks show highly competitive performance with up to 24% relative improvement in mAP over leading baselines. Full code for this work is available here: https://github.com/layer6ai-labs/GSS.	[Liu, Chundi; Yu, Guangwei; Chang, Cheng; Rai, Himanshu; Ma, Junwei; Gorti, Satya Krishna; Volkovs, Maksims] Layer6 AI, Toronto, ON, Canada		Liu, CD (corresponding author), Layer6 AI, Toronto, ON, Canada.	chundi@layer6.ai; guang@layer6.ai; jason@layer6.ai; himanshu@layer6.ai; jeremy@layer6.ai; satya@layer6.ai; maks@layer6.ai						Abadi M., TENSORFLOW LARGE SCA; Arandjelovic R, 2012, CVPR; Bai S., 2017, AAAI; Chang C., 2019, CVPR; Chapelle O., 2003, NIPS; Chapelle O., 2005, P AISTATS, P1; Chum O., 2007, ICCV; Chum O, 2010, IEEE T PATTERN ANAL, V32, P371, DOI 10.1109/TPAMI.2009.166; Donoser Michael, 2013, CVPR; Fischer Bernd, 2004, NIPS; Fischler M. A., 1981, COMMUNICATIONS ACM; Gordo A., 2016, ECCV; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Heinly J., 2015, CVPR; Hinton Geoffrey E, 2003, NIPS; Huang J., 2015, ICCV; Iscen A., 2017, CVPR; Iscen A., 2018, CVPR; Iscen Ahmet, 2018, ACCV; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Kalpathy-Cramer Jayashree, 2015, COMPUTERIZED MED IMA; Kingma D.P, P 3 INT C LEARNING R; Kipf Thomas N., 2017, INT C LEARNING REPRE; Liu Z., 2016, P IEEE C COMP VIS PA; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mishkin Dmytro, 2018, ECCV, V1, P2; Noh Hyeonwoo, 2017, ICCV; Philbin J., 2007, CVPR; Philbin J., 2008, CVPR; Qin D, 2011, CVPR; Radenovic F., 2018, CVPR; Radenovic F., 2018, IEEE T PATTERN ANAL; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Shi Z., 2015, CVPR; Simeoni Oriane, 2019, CVPR; Sivic J., 2003, ICCV; Tang J., 2015, WWW; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tolias Giorgos, 2016, INT J COMPUTER VISIO; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaquero D. A., 2009, WACV; Wang Shuang, 2015, ACM T MULTIMEDIA COM; Weyand T., 2011, ICCV; Xie J., 2016, ICML; Xu Jian, 2018, T MULTIMEDIA; Yang Xingwei, 2009, CVPR WORKSH; Zhou D., 2003, NIPS	47	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301053
C	Liu, D; Zhang, HC; Xiong, ZW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Dong; Zhang, Haochen; Xiong, Zhiwei			On The Classification-Distortion-Perception Tradeoff	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMAGE QUALITY ASSESSMENT	Signal degradation is ubiquitous, and computational restoration of degraded signal has been investigated for many years. Recently, it is reported that the capability of signal restoration is fundamentally limited by the so-called perception-distortion tradeoff, i.e. the distortion and the perceptual difference between the restored signal and the ideal "original" signal cannot be made both minimal simultaneously. Distortion corresponds to signal fidelity and perceptual difference corresponds to perceptual naturalness, both of which are important metrics in practice. Besides, there is another dimension worthy of consideration-the semantic quality of the restored signal, i.e. the utility of the signal for recognition purpose. In this paper, we extend the previous perception-distortion tradeoff to the case of classification-distortion-perception (CDP) tradeoff, where we introduced the classification error rate of the restored signal in addition to distortion and perceptual difference. In particular, we consider the classification error rate achieved on the restored signal using a predefined classifier as a representative metric for semantic quality. We rigorously prove the existence of the CDP tradeoff, i.e. the distortion, perceptual difference, and classification error rate cannot be made all minimal simultaneously. We also provide both simulation and experimental results to showcase the CDP tradeoff. Our findings can be useful especially for computer vision research where some low-level vision tasks (signal restoration) serve for high-level vision tasks (visual understanding). Our code and models have been published.	[Liu, Dong; Zhang, Haochen; Xiong, Zhiwei] Univ Sci & Technol China, Hefei 230027, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS	Liu, D (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.	dongeliu@ustc.edu.cn			Natural Science Foundation of China [61772483]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the Natural Science Foundation of China under Grant 61772483.	[Anonymous], 2018, P EUR C COMPUT VIS; Arjovsky M., 2017, ARXIV170107875; Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kuang HL, 2017, IEEE T INTELL TRANSP, V18, P927, DOI 10.1109/TITS.2016.2598192; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu D, 2016, SENS IMAGING, V18, DOI 10.1007/s11220-016-0152-5; Lu Q, 2016, IEEE T IMAGE PROCESS, V25, P2311, DOI 10.1109/TIP.2016.2535375; Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050; Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563; Shermeyer Jacob, 2019, CVPR WORKSH, P1; Vu T, 2018, GEOTECH SP, P1; van Erven T, 2014, IEEE T INFORM THEORY, V60, P3797, DOI 10.1109/TIT.2014.2320500; VidalMata R. G., 2019, ARXIV190109482; VU T, 2018, ECCV, P1; Wang J., 2017, P IEEE C COMP VIS PA, V2017, P1279; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301023
C	Liu, JB; Wang, B; Qi, ZQ; Tian, YJ; Shi, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Jiabin; Wang, Bo; Qi, Zhiquan; Tian, Yingjie; Shi, Yong			Learning from Label Proportions with Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we leverage generative adversarial networks (GANs) to derive an effective algorithm LLP-GAN for learning from label proportions (LLP), where only the bag-level proportional information in labels is available. Endowed with end-to-end structure, LLP-GAN performs approximation in the light of an adversarial learning mechanism, without imposing restricted assumptions on distribution. Accordingly, we can directly induce the final instance-level classifier upon the discriminator. Under mild assumptions, we give the explicit generative representation and prove the global optimality for LLP-GAN. Additionally, compared with existing methods, our work empowers LLP solver with capable scalability inheriting from deep models. Several experiments on benchmark datasets demonstrate vivid advantages of the proposed approach.	[Liu, Jiabin] Samsung Res China Beijing, Beijing 100028, Peoples R China; [Wang, Bo] Univ Int Business & Econ, Beijing 100029, Peoples R China; [Qi, Zhiquan; Tian, Yingjie; Shi, Yong] Univ Chinese Acad Sci, Beijing 100190, Peoples R China	University of International Business & Economics; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Liu, JB (corresponding author), Samsung Res China Beijing, Beijing 100028, Peoples R China.	liujiabin008@126.com; wangbo@uibe.edu.cn; qizhiquan@foxmail.com; tyj@ucas.ac.cn; yshi@ucas.ac.cn		Tian, Yingjie/0000-0002-4675-0398	National Natural Science Foundation of China [61702099, 71731009, 61472390, 71932008, 91546201, 71331005]; Science and Technology Service Network Program of Chinese Academy of Sciences (STS Program) [KFJ-STS-ZDTP-060]; Fundamental Research Funds for the Central Universities in UIBE [CXTD10-05]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Service Network Program of Chinese Academy of Sciences (STS Program); Fundamental Research Funds for the Central Universities in UIBE	This work is supported by grants from: National Natural Science Foundation of China (No.61702099, 71731009, 61472390, 71932008, 91546201, and 71331005), Science and Technology Service Network Program of Chinese Academy of Sciences (STS Program, No.KFJ-STS-ZDTP-060), and the Fundamental Research Funds for the Central Universities in UIBE (No.CXTD10-05). Bo Wang would like to acknowledge that this research was conducted during his visit at Texas A&M University and thank Dr. Xia Hu for his hosting and insightful discussions.	Ardehaly EM, 2017, INT CONF DAT MIN WOR, P1017, DOI 10.1109/ICDMW.2017.144; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arjovsky Martin, 2016, INT C LEARN REPR; Battaglia Peter W, 2018, ARXIV180601261; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bishop CM, 2006, PATTERN RECOGNITION; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Dai Z., 2017, ADV NEURAL INFORM PR, P6510; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Dulac-Arnold G., 2019, ARXIV190512909; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grandvalet Y., 2005, CAP, P529; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Maron O, 1998, ADV NEUR IN, V10, P570; Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975; Patrini Giorgio, 2014, ADV NEURAL INFORM PR, P190; Qi ZQ, 2017, IEEE T CYBERNETICS, V47, P3293, DOI 10.1109/TCYB.2016.2598749; Quadrianto N, 2009, J MACH LEARN RES, V10, P2349; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ruping S., 2010, P 27 INT C MACH LEAR, P911; Salimans T, 2016, ADV NEUR IN, V29; Springenberg Jost Tobias, 2015, ARXIV151106390; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Wang ZL, 2013, NEUROCOMPUTING, V119, P273, DOI 10.1016/j.neucom.2013.03.031; Warde-Farley D, 2016, NEURAL INF PROCESS S, P311; Yu FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P977, DOI 10.1145/2647868.2654993; Yu Felix X., 2013, P 30 INT C MACH LEAR, P504	34	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307021
C	Liu, JB; Rigollet, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Jingbo; Rigollet, Philippe			Power analysis of knockoff filters for correlated designs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FALSE DISCOVERY RATE	The knockoff filter introduced by Barber and Candes 2016 is an elegant framework for controlling the false discovery rate in variable selection. While empirical results indicate that this methodology is not too conservative, there is no conclusive theoretical result on its power. When the predictors are i.i.d. Gaussian, it is known that as the signal to noise ratio tend to infinity, the knockoff filter is consistent in the sense that one can make FDR go to 0 and power go to 1 simultaneously. In this work we study the case where the predictors have a general covariance matrix Sigma. We introduce a simple functional called effective signal deficiency (ESD) of the covariance matrix of the predictors that predicts consistency of various variable selection methods. In particular, ESD reveals that the structure of the precision matrix plays a central role in consistency and therefore, so does the conditional independence structure of the predictors. To leverage this connection, we introduce Conditional Independence knockoff, a simple procedure that is able to compete with the more sophisticated knockoff filters and that is defined when the predictors obey a Gaussian tree graphical models (or when the graph is sufficiently sparse). Our theoretical results are supported by numerical evidence on synthetic data.	[Liu, Jingbo] MIT, Inst Data Syst & Soc, Cambridge, MA 02139 USA; [Rigollet, Philippe] MIT, Dept Math, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Liu, JB (corresponding author), MIT, Inst Data Syst & Soc, Cambridge, MA 02139 USA.	jingbo@mit.edu; rigollet@math.mit.edu			IDSS Wiener Fellowship; NSF [IIS-BIGDATA-1838071, DMS-1712596, CCF-TRIPODS- 1740751]; ONR [N00014-17-1-2147]	IDSS Wiener Fellowship; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	JL was supported by the IDSS Wiener Fellowship. PR was supported by NSF awards IIS-BIGDATA-1838071, DMS-1712596 and CCF-TRIPODS- 1740751; ONR grant N00014-17-1-2147.	Barber RF, 2015, ANN STAT, V43, P2055, DOI 10.1214/15-AOS1337; Bates Stephen, 2019, ARXIV190300434; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Candes E, 2018, J R STAT SOC B, V80, P551, DOI 10.1111/rssb.12265; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Fan Yi, 2019, J Anat, V234, P709, DOI 10.1111/joa.12949; Javanmard A, 2014, IEEE T INFORM THEORY, V60, P6522, DOI 10.1109/TIT.2014.2343629; Katsevich E, 2019, ANN APPL STAT, V13, P1, DOI 10.1214/18-AOAS1185; Romano Y, 2020, J AM STAT ASSOC, V115, P1861, DOI 10.1080/01621459.2019.1660174; Sesia Matteo, 2019, BIORXIV; Storey JD, 2004, J R STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x; Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221; Weinstein A., 2017, ARXIV171206465; Zhang CH, 2014, J R STAT SOC B, V76, P217, DOI 10.1111/rssb.12026	15	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907014
C	Liu, LQ; Prasad, A; Ravikumar, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu Leqi; Prasad, Adarsh; Ravikumar, Pradeep			On Human-Aligned Risk Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PROSPECT-THEORY	The statistical decision theoretic foundations of modem machine learning have largely focused on the minimization of the expectation of some loss function for a given task. However, seminal results in behavioral economics have shown that human decision-making is based on different risk measures than the expectation of any given loss function. In this paper, we pose the following simple question: in contrast to minimizing expected loss, could we minimize a better human-aligned risk measure? While this might not seem natural at first glance, we analyze the properties of such a revised risk measure, and surprisingly show that it might also better align with additional desiderata like fairness that have attracted considerable recent attention. We focus in particular on a class of human-aligned risk measures inspired by cumulative prospect theory. We empirically study these risk measures, and demonstrate their improved performance on desiderata such as fairness, in contrast to the traditional workhorse of expected loss minimization.	[Liu Leqi; Prasad, Adarsh; Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Liu, LQ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	leqil@cs.cmu.edu; adarshp@cs.cmu.edu; pradeepr@cs.cmu.edu	Liu, Leqi/HGA-0678-2022		ONR [N000141812861]	ONR(Office of Naval Research)	We thank the reviewers for providing thoughtful and constructive feedback for the paper. We thank Hongseok Namkoong for providing the method to optimize conditional value-at-risk. We acknowledge the support of ONR via N000141812861.	Adebayo J. A., 2016, FAIRML TOOLBOX DIAGN; Bellamy Rachel K. E., 2018, AL FAIRNESS 360 EXTE; CAMERER C, 1992, J RISK UNCERTAINTY, V5, P325, DOI 10.1007/BF00122575; Camerer CF, 2004, ROUNDTABLE SER BEHAV, P3; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Chow Y., 2015, P 28 INT C NEUR INF, P1522; de Oliveira Whio M, 2016, ARXIV160106412; Diecidue E, 2001, J RISK UNCERTAINTY, V23, P281, DOI 10.1023/A:1011877808366; Duchi John C, DISTRIBUTIONALLY ROB; Durrett R., 2019, PROBABILITY THEORY E, DOI [10.1017/9781108591034, DOI 10.1017/9781108591034]; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Gonzalez R, 1999, COGNITIVE PSYCHOL, V38, P129, DOI 10.1006/cogp.1998.0710; Gopalan A., 2017, 31 AAAI C ART INT; He Xue Dong, 2018, MATH CONTROL RELATED; HOWARD RA, 1972, MANAGE SCI, V18, P356, DOI 10.1287/mnsc.18.7.356; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Low C, 2012, MATH FINANC, V22, P379, DOI 10.1111/j.1467-9965.2010.00463.x; Osogami, 2012, ADV NEURAL INFORM PR, V1, P233; Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; Prashanth LA, 2016, PR MACH LEARN RES, V48; Prelec D, 1998, ECONOMETRICA, V66, P497, DOI 10.2307/2998573; Rieger MO, 2006, ECON THEOR, V28, P665, DOI 10.1007/s00199-005-0641-6; Rockafellar R., 2000, J RISK, V2, P21, DOI 10.21314/JOR.2000.038; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; TVERSKY A, 1992, J RISK UNCERTAINTY, V5, P297, DOI 10.1007/BF00122574; Williamson R.C., 2019, ARXIV190108665; Wu G, 1996, MANAGE SCI, V42, P1676, DOI 10.1287/mnsc.42.12.1676	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906068
C	Liu, Q; Wu, LM; Wang, DL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Qiang; Wu, Lemeng; Wang, Dilin			Splitting Steepest Descent for Growing Neural Architectures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We develop a progressive training approach for neural networks which adaptively grows the network structure by splitting existing neurons to multiple off-springs. By leveraging a functional steepest descent idea, we derive a simple criterion for deciding the best subset of neurons to split and a splitting gradientfor optimally updating the off-springs. Theoretically, our splitting strategy is a second-order functional steepest descent for escaping saddle points in an infinity-Wasserstein metric space, on which the standard parametric gradient descent is a first-order steepest descent. Our method provides a new practical approach for optimizing neural network structures, especially for learning lightweight neural architectures in resource-constrained settings.	[Liu, Qiang; Wu, Lemeng; Wang, Dilin] UT Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Liu, Q (corresponding author), UT Austin, Austin, TX 78712 USA.	lqiang@cs.utexas.edu; lmwu@cs.utexas.edu; dilin@cs.utexas.edu			NSF CRII [1830161]; NSF CAREER [1846421]; Google Cloud and Amazon Web Services (AWS)	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Cloud and Amazon Web Services (AWS)(Google Incorporated)	This work is supported in part by NSF CRII 1830161 and NSF CAREER 1846421. We would like to acknowledge Google Cloud and Amazon Web Services (AWS) for their support.	[Anonymous], 2017, P ICLR; BACH F, 2017, J MACHINE LEARNING R, V18; Bengio Y., 2006, ADV NEURAL INFORM PR, P123; Cai Han, 2018, INT C LEARN REPR; Chandra V., 2017, ARXIV171107128; Chen Tianqi, 2016, INT C LEARN REPR; Chen Yutian, 2010, C UNC ART INT UAI; Chizat Lenaic, 2018, ADV NEURAL INFORM PR, P3036; Darrell T., 2019, INT C LEARN REPR; Gretton A, 2012, J MACH LEARN RES, V13, P723; Han S., 2016, IEEE ICC, DOI DOI 10.1109/ICC.2016.7511104; Howard A.G., 2017, ARXIV170404861; Jin C, 2017, PR MACH LEARN RES, V70; Kim SJ, 2016, PORTL INT CONF MANAG, P2460, DOI 10.1109/PICMET.2016.7806794; Li O, 2018, AAAI CONF ARTIF INTE, P3530; Liu Hanxiao, 2019, INT C LEARNING REPRE; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Mei Song, 2018, P NATL ACAD SCI US; Pham H, 2018, PR MACH LEARN RES, V80; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Real Esteban, 2018, ICML AUTOML WORKSH; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Warden P, 2018, ARXIV 180403209; Xie S., 2018, INT C LEARN REPR; Zoph Barret, 2017, P 5 INT C LEARNING R	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902030
C	Liu, SC; Demirel, MF; Liang, YY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Shengchao; Demirel, Mehmet Furkan; Liang, Yingyu			N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GENERATION	Machine learning techniques have recently been adopted in various applications in medicine, biology, chemistry, and material engineering. An important task is to predict the properties of molecules, which serves as the main subroutine in many downstream applications such as virtual screening and drug design. Despite the increasing interest, the key challenge is to construct proper representations of molecules for learning algorithms. This paper introduces N-gram graph, a simple unsupervised representation for molecules. The method first embeds the vertices in the molecule graph. It then constructs a compact representation for the graph by assembling the vertex embeddings in short walks in the graph, which we show is equivalent to a simple graph neural network that needs no training. The representations can thus be efficiently computed and then used with supervised learning methods for prediction. Experiments on 60 tasks from 10 benchmark datasets demonstrate its advantages over both popular graph neural networks and traditional representation methods. This is complemented by theoretical analysis showing its strong representation and prediction power.	[Liu, Shengchao; Demirel, Mehmet Furkan; Liang, Yingyu] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Liu, SC (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.	shengchao@cs.wisc.edu; demirel@cs.wisc.edu; yliang@cs.wisc.edu			University of Wisconsin-Madison Office of the Vice Chancellor for Research and Graduate Education; Wisconsin Alumni Research Foundation;  [FA9550-18-1-0166]	University of Wisconsin-Madison Office of the Vice Chancellor for Research and Graduate Education; Wisconsin Alumni Research Foundation; 	This work was supported in part by FA9550-18-1-0166. The authors would also like to acknowledge computing resources from the University of Wisconsin-Madison Center for High Throughput Computing and support provided by the University of Wisconsin-Madison Office of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation.	Altae-Tran H, 2017, ACS CENTRAL SCI, V3, P283, DOI 10.1021/acscentsci.6b00367; [Anonymous], 2017, IEEE DATA ENG B; [Anonymous], 2018, CELL; Arora S., 2016, T ASS COMPUTATIONAL, V4, P385, DOI DOI 10.1162/TACL_A_00106; Arora S., 2016, INT C LEARN REPR; Arora Sanjeev, 2018, ICLR; Arora Sanjeev., 2018, T ASSOC COMPUT LING, V6, P483, DOI DOI 10.1162/TACL_A_00034; Artemov A. V., 2016, BIORXIV; Butler KT, 2018, NATURE, V559, P547, DOI 10.1038/s41586-018-0337-2; Calderbank R, 2009, COMPRESSED LEARNING; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Chen H., 2018, DRUG DISCOVERY TODAY; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Ching T, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2017.0387; Dahl George, 2012, DEEP LEARNING I DID; Duvenaud David K, 2015, C NEUR INF PROC SYST, P2224; Faber FA, 2017, J CHEM THEORY COMPUT, V13, P5255, DOI 10.1021/acs.jctc.7b00577; Fey M., 2019, ICLR WORKSH REPR LEA; Foucart S., 2017, B AM MATH SOC, V54, P151; Gamo FJ, 2010, NATURE, V465, P305, DOI 10.1038/nature09107; Gayvert KM, 2016, CELL CHEM BIOL, V23, P1294, DOI 10.1016/j.chembiol.2016.07.023; Gilmer J, 2017, PR MACH LEARN RES, V70; Gomez-Bombarelli Rafael, 2016, ACS CENTRAL SCI; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hachmann J, 2011, J PHYS CHEM LETT, V2, P2241, DOI 10.1021/jz200866s; Jastrzebski Stanislaw, 2016, ARXIV160206289; Kasiviswanathan Shiva Prasad, 2019, ARXIV190405510; Kusner M. J., 2017, P INT C MACHINE LEAR; Landrum G., 2016, RDKIT OPEN SOURCE CH; Liu S., 2018, BIORXIV; Ma JS, 2015, J CHEM INF MODEL, V55, P263, DOI 10.1021/ci500747n; Matlock MK, 2018, ACS CENTRAL SCI, V4, P52, DOI 10.1021/acscentsci.7b00405; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; MORGAN HL, 1965, J CHEM DOC, V5, P107, DOI 10.1021/c160017a018; Ramsundar B., 2019, DEEP LEARNING LIFE S; Ren X., 2018, ARXIV180608804; Rohrer SG, 2009, J CHEM INF MODEL, V49, P169, DOI 10.1021/ci8002649; Ruddigkeit L, 2012, J CHEM INF MODEL, V52, P2864, DOI 10.1021/ci300415d; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Todeschini R., 2009, MOL DESCRIPTORS CHEM, V41; Unterthiner T., 2014, ADV NEURAL INFORM PR, V27; Wang B, 2012, ADV ENG FORUM, V2-3, P900, DOI 10.4028/www.scientific.net/AEF.2-3.900; Wang S., 2012, P 50 ANN M ASS COMP, V2, P90, DOI DOI 10.5555/2390665.2390688; WEININGER D, 1989, J CHEM INF COMP SCI, V29, P97, DOI 10.1021/ci00062a008; Wieting J., 2016, P 4 INT C LEARN REPR; Wu Z., 2019, 190100596 ARXIV; Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a; Xu K., 2018, INT C LEARN REPR; Yin Z, 2018, P ADV NEURAL INFORM, V31, P895; Zhou J., 2018, ARXIV181208434	58	0	0	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900010
C	Liu, SQ; Hauskrecht, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Siqi; Hauskrecht, Milos			Nonparametric Regressive Point Processes Based on Conditional Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Real-world event sequences consist of complex mixtures of different types of events occurring in time. An event may depend on past events of the same type, as well as, the other types. Point processes define a general class of models for event sequences. "Regressive point processes" refer to point processes that directly model the dependency between an event and any past event, an example of which is a Hawkes process. In this work, we propose and develop a new nonparametric regressive point process model based on Gaussian processes. We show that our model can represent better many commonly observed real-world event sequences and capture the dependencies between events that are difficult to model using existing nonparametric Hawkes process variants. We demonstrate the improved predictive performance of our model against state-of-the-art baselines on multiple synthetic and real-world datasets.	[Liu, Siqi; Hauskrecht, Milos] Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15213 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Liu, SQ (corresponding author), Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15213 USA.	siqiliu@cs.pitt.edu; milos@pitt.edu			NIH [R01-GM088224]; Department of Computer Science, University of Pittsburgh	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Department of Computer Science, University of Pittsburgh	This work was supported by NIH grant R01-GM088224. Siqi Liu was also supported by CS50 Merit Pre-doctoral Fellowship from the Department of Computer Science, University of Pittsburgh. The content of this paper is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Bacry Emmanuel, 2014, ARXIV14010903PHYSICS; Daley D. J., 2007, INTRO THEORY POINT P, V2; Ding H., 2018, INT C ART INT STAT, P1108; Donnet Sophie, 2018, ARXIV180205975MATHST; Eichler M, 2017, J TIME SER ANAL, V38, P225, DOI 10.1111/jtsa.12213; Gunter T, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P310; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35; Kim Minyoung, 2018, INT C MACH LEARN, P2640; Lasko TA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P469; Lloyd C, 2016, JMLR WORKSH CONF PRO, V51, P389; Lloyd C, 2015, PR MACH LEARN RES, V37, P1814; Luo DX, 2014, IEEE T BROADCAST, V60, P61, DOI 10.1109/TBC.2013.2295894; Mei H., 2017, ADV NEURAL INFORM PR, P6754; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rao V. A., 2011, ADV NEURAL INFORM PR, P2474; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Vere-Jones, 2003, INTRO THEORY POINT P; Wang YC, 2016, GEOTECH SP, P226; Xu H, 2016, INT C MACH LEARN, P1717; Xu Hongteng, 2017, ARXIV170809252CSSTAT; Young R, 2016, PROCEEDINGS OF CIE 2016 LIGHTING QUALITY AND ENERGY EFFICIENCY, P79; Zhang Rui, 2018, ARXIV181003730CSSTAT; Zhou K, 2013, ICML; Zhou K., 2013, ARTIF INTELL, P641	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301010
C	Liu, S; Kanamori, T; Jitkrittum, W; Chen, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Song; Kanamori, Takafumi; Jitkrittum, Wittawat; Chen, Yu			Fisher Efficient Inference of Intractable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Maximum Likelihood Estimators (MLE) has many good properties. For example, the asymptotic variance of MLE solution attains equality of the asymptotic CramerRao lower bound (efficiency bound), which is the minimum possible variance for an unbiased estimator. However, obtaining such MLE solution requires calculating the likelihood function which may not be tractable due to the normalization term of the density model. In this paper, we derive a Discriminative Likelihood Estimator (DLE) from the Kullback-Leibler divergence minimization criterion implemented via density ratio estimation and a Stein operator. We study the problem of model inference using DLE. We prove its consistency and show that the asymptotic variance of its solution can attain the equality of the efficiency bound under mild regularity conditions. We also propose a dual formulation of DLE which can be easily optimized. Numerical studies validate our asymptotic theorems and we give an example where DLE successfully estimates an intractable model constructed using a pre-trained deep neural network.	[Liu, Song; Chen, Yu] Univ Bristol, Bristol BS8 1TH, Avon, England; [Liu, Song] Alan Turing Inst, London, England; [Kanamori, Takafumi] Tokyo Inst Technol, Tokyo, Japan; [Kanamori, Takafumi] RIKEN, Tokyo, Japan; [Jitkrittum, Wittawat] Max Planck Inst Intelligent Syst, Stuttgart, Germany	University of Bristol; Tokyo Institute of Technology; RIKEN; Max Planck Society	Liu, S (corresponding author), Univ Bristol, Bristol BS8 1TH, Avon, England.; Liu, S (corresponding author), Alan Turing Inst, London, England.	song.liu@bristol.ac.uk; kanamori@c.titech.ac.jp; wittawat@tuebingen.mpg.de; yc14600@bristol.ac.uk			Alan Turing Institute under the EPSRC [EP/N510129/1]; JSPS KAKENHI [15H01678, 15H03636, 16K00044, 19H04071]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. TK was partially supported by JSPS KAKENHI Grant Number 15H01678, 15H03636, 16K00044, and 19H04071. Authors would like to thank Dr. Carl Henrik Ek and three anonymous reviewers for their insightful comments.	AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Barp A., 2019, ARXIV190608283; Chen WY, 2018, P 35 INT C MACH LEAR, P844; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Cram?r H., 1946, MATH METHODS STAT; Ding J, 2007, APPL MATH LETT, V20, P1223, DOI 10.1016/j.aml.2006.11.016; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gorham J, 2015, ADV NEUR IN, V28; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Hayakawa J, 2016, COMMUN STAT-THEOR M, V45, P6860, DOI 10.1080/03610926.2014.968735; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003; Jitkrittum W., 2017, ADV NEURAL INFORM PR, P262; Li Y., 2018, INT C LEARN REPR; Lin LN, 2016, ELECTRON J STAT, V10, P806, DOI 10.1214/16-EJS1126; Liu Q., 2016, NEURIPS; Liu Q, 2016, PR MACH LEARN RES, V48; Lyu S., 2009, P 25 C UNC ART INT, P359; Merikoski Jorma K, 2004, APPL MATH E-NOTES, V4, P150; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Rao C. R, 1945, B CALCUTTA MATH SOC, V37, P81, DOI DOI 10.1007/978-1-4612-0919-5_16; Robert C. P., 2005, MONTE CARLO STAT MET; Sanchez-Moreno P., 2012, J PHYS A, V45; Shi J., 2018, P MACHINE LEARNING R, P4644; Sriperumbudur Bharath, 2017, J MACHINE LEARNING R, V18, P1830; Stein C., 1972, PROC 6 BERKELEY S MA, VII, p583?602; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Sugiyama M., 2008, NIPS, P1433; Vaart A. W., 1998, ASYMPTOTIC STAT; Wang D., 2016, ARXIV161101722; Yanai H, 2011, STAT SOC BEHAV SC, P1, DOI 10.1007/978-1-4419-9887-3; Yu S., 2018, ARXIV181210551	35	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900039
C	Liu, TY; Chen, MS; Zhou, M; Du, SS; Zhou, EL; Zhao, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Tianyi; Chen, Minshuo; Zhou, Mo; Du, Simon S.; Zhou, Enlu; Zhao, Tuo			Towards Understanding the Importance of Shortcut Connections in Residual Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Residual Network (ResNet) is undoubtedly a milestone in deep learning. ResNet is equipped with shortcut connections between layers, and exhibits efficient training using simple first order algorithms. Despite of the great empirical success, the reason behind is far from being well understood. In this paper, we study a two-layer non-overlapping convolutional ResNet. Training such a network requires solving a non-convex optimization problem with a spurious local optimum. We show, however, that gradient descent combined with proper normalization, avoids being trapped by the spurious local optimum, and converges to a global optimum in polynomial time, when the weight of the first layer is initialized at 0, and that of the second layer is initialized arbitrarily in a ball. Numerical experiments are provided to support our theory.	[Liu, Tianyi; Chen, Minshuo; Zhou, Enlu; Zhao, Tuo] Georgia Tech, Atlanta, GA 30332 USA; [Zhou, Mo] Duke Univ, Durham, NC 27706 USA; [Du, Simon S.] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA	University System of Georgia; Georgia Institute of Technology; Duke University; Institute for Advanced Study - USA	Liu, TY (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.		Liu, Tianyi/GLU-4923-2022					BALDUZZI D, 2017, P 34 INT C MACH LEAR, V70; BARRERA G, 2015, ARXIV151009207; BARTLETT P. L, 2018, ARXIV180405012; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal Priya, 2017, ARXIV170602677; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Hardt M., 2016, ARXIV161104231; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Li H, 2018, ADV NEUR IN, V31; Li S., 2016, DEMYSTIFYING RESNET; Li YT, 2017, ADV NEUR IN, V30; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; SINGH B, 2015, 2015 IEEE 14 INT C M; Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58; Smith Leslie N., 2018, SUPER CONVERGENCE VE, P6; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Veit A, 2016, ADV NEUR IN, V29; Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738; Yu Xin, 2018, P IEEE C COMP VIS PA; Zhang Han, 2019, ICLR; Zhou M, 2019, PR MACH LEARN RES, V97; Zhou Z, 2017, ADV NEURAL INFORM PR	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307086
C	Liu, WW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Weiwei			Copula Multi-label Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEMIPARAMETRIC ESTIMATION	A formidable challenge in multi-label learning is to model the interdependencies between labels and features. Unfortunately, the statistical properties of existing multi-label dependency modelings are still not well understood. Copulas are a powerful tool for modeling dependence of multivariate data, and achieve great success in a wide range of applications, such as finance, econometrics and systems neuroscience. This inspires us to develop a novel copula multi-label learning paradigm for modeling label and feature dependencies. The copula based paradigm enables to reveal new statistical insights in multi-label learning. In particular, the paper first leverages the kernel trick to construct continuous distribution in the output space, and then estimates our proposed model semiparametrically where the copula is modeled parametrically, while the marginal distributions are modeled nonparametrically. Theoretically, we show that our estimator is an unbiased and consistent estimator and follows asymptotically a normal distribution. Moreover, we bound the mean squared error of estimator. The experimental results from various domains validate the superiority of our proposed approach.	[Liu, Weiwei] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China	Wuhan University	Liu, WW (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.	liuweiwei863@gmail.com			National Natural Science Foundation of China [61976161]; Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)	This work is supported by the National Natural Science Foundation of China under Grants 61976161 and the Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies).	[Anonymous], NIPS; Berkes P, 2008, P 22 ANN C NEUR INF, P129; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Chen Y.N., 2012, P ADV NEUR INF PROC, P1529; Cherubini U., 2004, COPULA METHODS FINAN; Elidan G., 2010, PROC INT C NEURAL IN, P559; Frees Edward W., 1998, N AM ACTUAR J, V2, P1; GENEST C, 1995, BIOMETRIKA, V82, P543, DOI 10.2307/2337532; Kalaitzis A., 2013, ADV NEURAL INFORM PR, P2517; Liu H, 2009, J MACH LEARN RES, V10, P2295; Liu WW, 2015, ADV NEUR IN, V28; Liu WW, 2017, J MACH LEARN RES, V18; Liu WW, 2019, IEEE T PATTERN ANAL, V41, P408, DOI 10.1109/TPAMI.2018.2794976; Liu WW, 2015, AAAI CONF ARTIF INTE, P2800; Liu WW, 2017, J MACH LEARN RES, V18; Lopez-Paz D., 2012, SEMISUPERVISED DOMAI, V25, P674; Nelsen RB., 2006, INTRO COPULAS, V2nd; OAKES D, 1989, J AM STAT ASSOC, V84, P487, DOI 10.2307/2289934; Patton AJ, 2006, INT ECON REV, V47, P527, DOI 10.1111/j.1468-2354.2006.00387.x; Poczos B, 2012, P 29 INT C MACH LEAR, P775; Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5; Serfling R., 1980, APPROXIMATION THEORE; Shen XB, 2018, AAAI CONF ARTIF INTE, P4066; Shen XB, 2018, IEEE T NEUR NET LEAR, V29, P4324, DOI 10.1109/TNNLS.2017.2763967; Slark A., 1996, DISTRIBUTIONS FIXED, DOI [DOI 10.1214/1NMS/1215452606, 10.1214/lnms/1215452606, DOI 10.1214/LNMS/1215452606]; Tsoumakas G, 2012, MACH LEARN, V88, P1, DOI 10.1007/s10994-012-5292-9; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Tsukahara H, 2005, CAN J STAT, V33, P357, DOI 10.1002/cjs.5540330304; Xiang R, 2013, P 6 ACM INT C WEB SE, P647; Yen IEH, 2016, PR MACH LEARN RES, V48; Yu N, 2011, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON NUCLEAR ENGINEERING 2010, VOL 4 PTS A AND B, P873; Zhou J. T., 2019, IEEE T PATTERN ANAL; Zhou JTY, 2019, J MACH LEARN RES, V20; Zhou JT, 2019, MACH LEARN, V108, P809, DOI 10.1007/s10994-019-05786-2; ZHOU JT, 2019, ACL, P3461	35	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306035
C	Liu, XY; Oh, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Xiyang; Oh, Sewoong			Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POLYNOMIALS; ENTROPY	Differential privacy has become a widely accepted notion of privacy, leading to the introduction and deployment of numerous privatization mechanisms. However, ensuring the privacy guarantee is an error-prone process, both in designing mechanisms and in implementing those mechanisms. Both types of errors will be greatly reduced, if we have a data-driven approach to verify privacy guarantees, from a black-box access to a mechanism. We pose it as a property estimation problem, and study the fundamental trade-offs involved in the accuracy in estimated privacy guarantees and the number of samples required. We introduce a novel estimator that uses polynomial approximation of a carefully chosen degree to optimally trade-off bias and variance. With n samples, we show that this estimator achieves performance of a straightforward plug-in estimator with n ln n samples, a phenomenon known as sample size amplification. The minimax optimality of the estimator is proved by comparing it to a matching fundamental lower bound.	[Liu, Xiyang; Oh, Sewoong] Univ Washington, Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Liu, XY (corresponding author), Univ Washington, Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.	xiyangl@cs.washington.edu; sewoong@cs.washington.edu			NSF [CNS-1527754, CNS-1705007, CCF-1927712, RI1929955]	NSF(National Science Foundation (NSF))	This work is partially supported by NSF awards CNS-1527754, CNS-1705007, CCF-1927712, RI1929955 and generous gift from Google.	Abowd John, 2018, KDD INVITED TALK; Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435; Apple Differential Privacy Team, 2017, APPLE MACHINE LEARNI; Barthe G, 2012, ACM SIGPLAN NOTICES, V47, P97, DOI 10.1145/2103621.2103670; Ben Stoddard, 2014, ARXIV14115428; Bernstein S, 1914, ACTA MATH-DJURSHOLM, V37, P1, DOI 10.1007/BF02401828; Cai TT, 2011, ANN STAT, V39, P1012, DOI 10.1214/10-AOS849; Chen R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129, DOI 10.1145/2783258.2783379; Chen Yan, 2015, ARXIV150807306; Choi JP, 2014, I C INF COMM TECH CO, P930, DOI 10.1109/ICTC.2014.6983336; Daskalakis C, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2747; DeVore R. A., 1993, CONSTRUCTIVE APPROXI, V303; Ding ZY, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P475, DOI 10.1145/3243734.3243818; Ditzian Z., 1987, SPRINGER SERIES COMP, V9; Dixit K, 2013, LECT NOTES COMPUT SC, V7785, P418, DOI 10.1007/978-3-642-36594-2_24; Driscoll Tobin A., 2014, CHEBFUN GUIDE; Dwork C., 2016, ARXIV PREPRINT ARXIV; Dwork C., 2006, P 33 INT C AUT LANG, P1, DOI DOI 10.1007/11787006_1; Dwork C., 2011, ENCY CRYPTOGRAPHY SE, P338, DOI DOI 10.1007/978-1-4419-5906-5_752; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Fanti Giulia, 2016, Proceedings on Privacy Enhancing Technologies, V2016, P41, DOI 10.1515/popets-2016-0015; Gao WH, 2017, ADV NEUR IN, V30; Gao Weihao, 2016, P ADV NEUR INF PROC, P2460; Ghosh A, 2012, SIAM J COMPUT, V41, P1673, DOI 10.1137/09076828X; Gilbert AC, 2018, ANN ALLERTON CONF, P249, DOI 10.1109/ALLERTON.2018.8636068; Han YJ, 2015, IEEE T INFORM THEORY, V61, P6343, DOI 10.1109/TIT.2015.2478816; Han Yanjun, 2016, ARXIV160509124; Huang C., 2018, ABS180705306 CORR; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Jiao Jiantao, 2018, IEEE T INFORM THEORY; Kairouz P., 2015, ADV NEURAL INFORM PR, V28, P2008; Kairouz P., 2014, ADV NEURAL INFORM PR, V27, P2879; Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Kifer D, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2514689; Lin Z., 2018, NIPS, P1498; Liu Xiyang, 2019, ARXIV190510335; Lyu M, 2017, PROC VLDB ENDOW, V10, P637, DOI 10.14778/3055330.3055331; Mhaskar HN, 2013, B MATH SCI, V3, P485, DOI 10.1007/s13373-013-0043-1; Mitzenmacher M., 2005, PROBABILITY COMPUTIN; Sajjadi MS, 2018, NEURIPS, V2018, P5228; Song S, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1291, DOI 10.1145/3035918.3064025; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Valiant G, 2017, J ACM, V64, DOI 10.1145/3125643; Wang Yuxin, 2019, ARXIV190312254; Wu YH, 2019, ANN STAT, V47, P857, DOI 10.1214/17-AOS1665; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; Yekhanin Sergey, 2017, NIPS, P3571; Zhang YC, 2018, INT C ELECTR MACH SY, P1562, DOI 10.23919/ICEMS.2018.8549231	51	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302042
C	Livni, R; Mansour, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Livni, Roi; Mansour, Yishay			Graph-based Discriminators: Sample Complexity and Expressiveness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DISTRIBUTIONS	A basic question in learning theory is to identify if two distributions are identical when we have access only to examples sampled from the distributions. This basic task is considered, for example, in the context of Generative Adversarial Networks (GANs), where a discriminator is trained to distinguish between a real life distribution and a synthetic distribution. Classically, we use a hypothesis class H and claim that the two distributions are distinct if for some h E H the expected value on the two distributions is (significantly) different. Our starting point is the following fundamental problem: "is having the hypothesis dependent on more than a single random example beneficial". To address this challenge we define k-ary based discriminators, which have a family of Boolean k-ary functions g. Each function g E g naturally defines a hyper-graph, indicating whether a given hyper-edge exists. A function g E I distinguishes between two distributions, if the expected value of g, on a k-tuple of i.i.d examples, on the two distributions is (significantly) different. We study the expressiveness of families of k-ary functions, compared to the classical hypothesis class H, which is k = 1. We show a separation in expressiveness of k + 1-ary versus k-ary functions. This demonstrate the great benefit of having k > 2 as distinguishers. For k > 2 we introduce a notion similar to the VC -dimension, and show that it controls the sample complexity. We proceed and provide upper and lower bounds as a function of our extended notion of VC -dimension.	[Livni, Roi; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Mansour, Yishay] Google, Menlo Pk, CA USA	Tel Aviv University; Google Incorporated	Livni, R (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.	rlivni@tauex.tau.ac.il; mansour.yishay@gmail.com			ISF	ISF(Israel Science Foundation)	Y.M is supported in part by a grant from the ISF	Arora S., 2017, ARXIV170608224; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Ben-David Shai, 2015, ARXIV150705307; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Bousquet O., 2019, ARXIV190203468; Chan S., 2014, P 25 ANN ACM SIAM S, P1193, DOI [10.1137/1.9781611973402.88, DOI 10.1137/1.9781611973402.88]; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Clemencon Stephan, 2016, J MACHINE LEARNING R, V17, P2682; Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163; Goldreich O., 2011, STUDIES COMPLEXITY C, V6650, P68, DOI [10.1007/978-3-642-22670-0_9, DOI 10.1007/978-3-642-22670-0_9, 10.1007/978-3-642-22670-09, DOI 10.1007/978-3-642-22670-09]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Impagliazzo R, 1995, AN S FDN CO, P538, DOI 10.1109/SFCS.1995.492584; Kothari Pravesh K, 2018, 9 INN THEOR COMP SCI; Lin Z., 2018, NIPS, P1498; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987; Richardson E., 2018, PROC C ADV NEURAL IN, P5847; Vadhan S., 2017, P 2017 C LEARNING TH, V65, P1835; VAPNIK VN, 1968, DOKL AKAD NAUK SSSR+, V181, P781	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306067
C	Lu, YD; Squillante, MS; Wu, CW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lu, Yingdong; Squillante, Mark S.; Wu, Chai Wah			A Family of Robust Stochastic Operators for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider a new family of stochastic operators for reinforcement learning that seeks to alleviate negative effects and become more robust to approximation or estimation errors. Theoretical results are established, showing that our family of operators preserve optimality and increase the action gap in a stochastic sense. Empirical results illustrate the strong benefits of our robust stochastic operators, significantly outperforming the classical Bellman and recently proposed operators.	[Lu, Yingdong; Squillante, Mark S.; Wu, Chai Wah] IBM Res, Math Sci, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Lu, YD (corresponding author), IBM Res, Math Sci, Yorktown Hts, NY 10598 USA.	yingdong@us.ibm.com; mss@us.ibm.com; cwwu@us.ibm.com	Wu, Chai Wah/GXG-7451-2022	Wu, Chai Wah/0000-0002-0657-0683				Alzantot M., 2017, SOLUTION MOUNTAINCAR; [Anonymous], [No title captured]; Azar M. G., 2011, ADV NEURAL INFORM PR, V24; Baird L, 1999, THESIS; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Bellemare MG, 2016, AAAI CONF ARTIF INTE, P1476; Bertsekas D., 2012, MATH OPERATIONS RES, V37; Billingsley P., 1999, CONVERGENCE PROBABIL; Chow Y. S., 2003, PROBABILITY THEORY I; Ernst D, 2005, J MACH LEARN RES, V6, P503; Farahmand A., 2011, ADV NEURAL INFORM PR, V24; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Mnih V., 2013, P NIPS DEEP LEARN WO; Moore A.W., 1990, THESIS; Rummery G. A., 1994, TECHNICAL REPORT; Shaked M, 2007, SPRINGER SER STAT, P3; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R.S., 2011, REINFORCEMENT LEARNI; Sutton RS, 1996, ADV NEUR IN, V8, P1038; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Vilches V. M., 2016, BASIC REINFORCEMENT; Watkins CJCH., 1989, THESIS; Zaremba W., 2016, CORR	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907032
C	Lucas, T; Shmelkov, K; Schmid, C; Alahari, K; Verbeek, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lucas, Thomas; Shmelkov, Konstantin; Schmid, Cordelia; Alahari, Karteek; Verbeek, Jakob			Adaptive Density Estimation for Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Unsupervised learning of generative models has seen tremendous progress over recent years, in particular due to generative adversarial networks (GANs), variational autoencoders, and flow-based models. GANs have dramatically improved sample quality, but suffer from two drawbacks: (i) they mode-drop, i.e., do not cover the full support of the train data, and (ii) they do not allow for likelihood evaluations on held-out data. In contrast, likelihood-based training encourages models to cover the full support of the train data, but yields poorer samples. These mutual shortcomings can in principle be addressed by training generative latent variable models in a hybrid adversarial-likelihood manner. However, we show that commonly made parametric assumptions create a conflict between them, making successful hybrid models non trivial. As a solution, we propose to use deep invertible transformations in the latent variable decoder. This approach allows for likelihood computations in image space, is more efficient than fully invertible models, and can take full advantage of adversarial training. We show that our model significantly improves over existing hybrid models: offering GAN-like samples, IS and FID scores that are competitive with fully adversarial models, and improved likelihood scores.	[Lucas, Thomas; Schmid, Cordelia; Alahari, Karteek; Verbeek, Jakob] INRIA, Rocquencourt, France; [Shmelkov, Konstantin] Huawei, Noahs Ark Lab, Shenzhen, Peoples R China; Univ Grenoble Alpes, CNRS, Grenoble INP, INRIA,LJK, F-38000 Grenoble, France	Inria; Huawei Technologies; Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Lucas, T (corresponding author), INRIA, Rocquencourt, France.				LabEx PERSYVAL-Lab [ANR-11-LABX0025-01]; Indo-French project EVEREST - CEFIPRA [5302-1]; ANR (AVENUE project) [ANR-18-CE23-0011];  [ANR16-CE23-0006]	LabEx PERSYVAL-Lab; Indo-French project EVEREST - CEFIPRA(French National Research Agency (ANR)); ANR (AVENUE project)(French National Research Agency (ANR)); 	This work was supported in part by the grants ANR16-CE23-0006 "Deep in France", LabEx PERSYVAL-Lab (ANR-11-LABX0025-01) as well as the Indo-French project EVEREST (no. 5302-1) funded by CEFIPRA and a grant from ANR (AVENUE project ANR-18-CE23-0011),	[Anonymous], 2017, ICLR; [Anonymous], 2014, ICLR; Arbel M., 2018, NEURIPS; Bachman P., 2016, NEURIPS; Bishop C.M, 2006, PATTERN RECOGN; Bottou L., 2017, ICML; Brock Andrew, 2019, ICLR; Chen L., 2018, AISTATS; Chen X., 2017, ICLR; Darrell T, 2017, ICLR; De Vries H., 2017, NEURIPS; Dorta G., 2018, CVPR; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Elfeki M., 2018, ARXIV ORG PDF 1812 0; Goodfellow Ian, 2014, 27 INT C NEURAL INFO; Grover A., 2018, FLOW GAN COMBINING M; Gulrajani I, 2017, ICLR; Gulrajani I., 2017, NEURIPS; Heusel M., 2017, NEURIPS; Ho J., 2019, ICML; Hou X., 2017, WACV; Karras Tero, 2018, ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2018, NEURIPS; Kingma Diederik P., 2016, NEURIPS; Larsen A., 2016, ICML; Li Y., 2015, ICML; Lin Z., 2018, NEURIPS; Litany Or, 2018, CVPR; Lucas T., 2018, ECML; Lucas T., 2018, ICML; Makhzani A., 2016, ICLR; Mathieu Michael, 2016, P INT C LEARN REPR I; Menick Jacob, 2019, ICLR; Miyato Takeru, 2018, ICLR; Ramachandran P., 2017, ICLR WORKSH; Rezende D.J., 2014, ICML; REZENDE DJ, 2015, ICML; Rosca Mihaela, 2017, ARXIV170604987; Saatchi Y., 2017, NEURIPS; Sajjadi M. S. M., 2018, NEURIPS; Salimans T., 2016, NEURIPS; Salimans Tim, 2018, ICLR; Salimans Tim, 2017, ICLR; Shmelkov K., 2018, ECCV; Sonderby Casper Kaae, 2016, NEURIPS; Sonderby Casper Kaae, 2017, ICLR; Thanh-Tung Hoang, 2019, ICLR; Ulyanov D., 2018, AAAI; Van Den Oord A., 2016, ICML; Zhang H., 2019, ICML	51	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903061
C	Mair, S; Brefeld, U		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mair, Sebastian; Brefeld, Ulf			Coresets for Archetypal Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Archetypal analysis represents instances as linear mixtures of prototypes (the archetypes) that lie on the boundary of the convex hull of the data. Archetypes are thus often better interpretable than factors computed by other matrix factorization techniques. However, the interpretability comes with high computational cost due to additional convexity-preserving constraints. In this paper, we propose efficient coresets for archetypal analysis. Theoretical guarantees are derived by showing that quantization errors of k-means upper bound archetypal analysis; the computation of a provable absolute-coreset can be performed in only two passes over the data. Empirically, we show that the coresets lead to improved performance on several data sets.	[Mair, Sebastian; Brefeld, Ulf] Leuphana Univ, Luneburg, Germany	Leuphana University Luneburg	Mair, S (corresponding author), Leuphana Univ, Luneburg, Germany.	mair@leuphana.de; brefeld@leuphana.de		Mair, Sebastian/0000-0003-2949-8781				Bachem O, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1119, DOI 10.1145/3219819.3219973; Bauckhage C, 2015, WORKSH NEW CHALL NEU; Bertin-Mahieux T., 2011, ISMIR; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Campbell T, 2018, PR MACH LEARN RES, V80; Chang CC, 2001, IEEE IJCNN, P1031, DOI 10.1109/IJCNN.2001.939502; Chen YS, 2014, PROC CVPR IEEE, P1478, DOI 10.1109/CVPR.2014.192; CUTLER A, 1994, TECHNOMETRICS, V36, P338, DOI 10.2307/1269949; Huggins Jonathan, 2016, P ADV NEUR INF PROC, P4080; Ionescu C., 2011, INT C COMP VIS; Ionescu CM, 2014, IEEE INT CONF AUTO; Lucic M., 2016, ARTIFICIAL INTELLIGE; Mair S., 2017, P INT C MACH LEARN S, P2305; Mei J., 2018, P EUR C COMP VIS, P486; Morup M, 2012, NEUROCOMPUTING, V80, P54, DOI 10.1016/j.neucom.2011.06.033; Munteanu A, 2018, ADV NEUR IN, V31; Tsang IW, 2005, J MACH LEARN RES, V6, P363	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307028
C	Mania, H; Miller, J; Schmidt, L; Hardt, M; Recht, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mania, Horia; Miller, John; Schmidt, Ludwig; Hardt, Moritz; Recht, Benjamin			Model Similarity Mitigates Test Set Overuse	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Excessive reuse of test data has become commonplace in today's machine learning workflows. Popular benchmarks, competitions, industrial scale tuning, among other applications, all involve test data reuse beyond guidance by statistical confidence bounds. Nonetheless, recent replication studies give evidence that popular benchmarks continue to support progress despite years of extensive reuse. We proffer a new explanation for the apparent longevity of test data: Many proposed models are similar in their predictions and we prove that this similarity mitigates overfitting. Specifically, we show empirically that models proposed for the ImageNet ILSVRC benchmark agree in their predictions well beyond what we can conclude from their accuracy levels alone. Likewise, models created by large scale hyperparameter search enjoy high levels of similarity. Motivated by these empirical observations, we give a non-asymptotic generalization bound that takes similarity into account, leading to meaningful confidence bounds in practical settings.	[Mania, Horia; Miller, John; Schmidt, Ludwig; Hardt, Moritz; Recht, Benjamin] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Mania, H (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	hmania@berkeley.edu; miller_john@berkeley.edu; ludwig@berkeley.edu; hardt@berkeley.edu; brecht@berkeley.edu			ONR [N00014-17-1-2191, N00014-17-1-2401, N00014-18-1-2833]; DARPA [FA8750-18-C-0101, W911NF-16-1-0552]; Siemens Futuremakers Fellowship; National Science Foundation Graduate Research Fellowship Program [DGE 1752814]; Amazon AWS AI Research Award	ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Siemens Futuremakers Fellowship; National Science Foundation Graduate Research Fellowship Program(National Science Foundation (NSF)); Amazon AWS AI Research Award	We thank Vitaly Feldman for helpful discussions. This work is generously supported in part by ONR awards N00014-17-1-2191, N00014-17-1-2401, and N00014-18-1-2833, the DARPA Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs, a Siemens Futuremakers Fellowship, an Amazon AWS AI Research Award, a gift from Microsoft Research, and the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1752814.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bassily R., 2016, S THEOR COMP STOC; Blum A., 2015, ICML 2015; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dwork C., 2015, S THEOR COMP STOC; Feldman V, 2019, PR MACH LEARN RES, V97; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langford J., 2002, THESIS; Li Liam, 2019, ABS190207638 CORR; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Recht B, 2019, PR MACH LEARN RES, V97; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Smith A., 2017, INFORM PRIVACY STABI; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Yadav C., 2019, DIGITS; Zrnic T, 2019, PR MACH LEARN RES, V97	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901060
C	Mariet, Z; Ovadia, Y; Snoek, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mariet, Zelda; Ovadia, Yaniv; Snoek, Jasper			DPPNET: Approximating Determinantal Point Processes with Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Determinantal point processes (DPPs) provide an elegant and versatile way to sample sets of items that balance the quality with the diversity of selected items. For this reason, they have gained prominence in many machine learning applications that rely on subset selection. However, sampling from a DPP over a ground set of size N is a costly operation, requiring in general an O(N-3) preprocessing cost and an O(Nk(3)) sampling cost for subsets of size k. We approach this problem by introducing DPPNETs: generative deep models that produce DPP-like samples for arbitrary ground sets. We develop an inhibitive attention mechanism based on transformer networks that captures a notion of dissimilarity between feature vectors. We show theoretically that such an approximation is sensible as it maintains the guarantees of inhibition or dissimilarity that makes DPPs so powerful and unique. Empirically, we show across multiple datasets that DPPNET is orders of magnitude faster than competing approaches for DPP sampling, while generating high-likelihood samples and performing as well as DPPs on downstream tasks.	[Mariet, Zelda] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Mariet, Zelda; Ovadia, Yaniv; Snoek, Jasper] Google Brain, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Google Incorporated	Mariet, Z (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	zelda@csail.mit.edu; yovadia@google.com; jsnoek@google.com						Affandi R. H., 2013, ARTIFICIAL INTELLIGE; Anari N., 2016, C LEARN THEOR; Bach FR, 2003, J MACH LEARN RES, V3, P1, DOI 10.1162/153244303768966085; Badanidiyuru A., 2012, SODA; Ben Hough J, 2006, PROBAB SURV, V3, P206, DOI 10.1214/154957806000000078; Borcea J, 2009, J AM MATH SOC, V22, P521; Borodin A, 2005, J STAT PHYS, V121, P291, DOI 10.1007/s10955-005-7583-z; Borodin A., 2009, DETERMINANTAL POINT; Chen L., ADV NEURAL INFORM PR, V31; Cotter A., 2018, ABS180600050; Derezinski M., 2019, EXACT SAMPLING DETER; Derezinski M., 2018, FAST DETERMINANTAL P; Devanur N. R., 2013, APPROXIMATION SUBMOD; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Gartrell M, 2017, AAAI CONF ARTIF INTE, P1912; Gillenwater J, 2019, PR MACH LEARN RES, V97; Ginibre J., 1965, J MATH PHYS, V6, P3; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gotovos A., 2015, NEURAL INFORM PROCES; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Hastie T, 2009, ELEMENTS STAT LEARNI; Hennig P., 2016, EXACT SAMPLING DETER; Hough J. B., 2009, U LECT SERIES, V51; Johansson K, 2004, COMMUN MATH PHYS, V252, P111, DOI 10.1007/s00220-004-1186-4; Kingma D.P, P 3 INT C LEARNING R; Krause A, 2014, TRACTABILITY, P71; Kulesza Alex, 2012, DETERMINANTAL POINT; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li C., 2016, ADV NEURAL INFORM PR; Li CT, 2016, PR MACH LEARN RES, V48; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mariet Z., 2016, ADV NEURAL INFORM PR; Mariet Z., 2019, AISTATS; Mariet Z, 2018, ADV NEUR IN, V31; MCKAY MD, 1979, TECHNOMETRICS, V21, P239, DOI 10.2307/1268522; MEHTA ML, 1960, NUCL PHYS, V18, P420, DOI 10.1016/0029-5582(60)90414-4; Nystrom E., 1930, ACTA MATH, V54; Osogami T., 2018, DYNAMIC DETERMINANTA; ROBERTAZZI TG, 1989, SIAM J SCI STAT COMP, V10, P341, DOI 10.1137/0910022; Shen H, 2009, IEEE T SIGNAL PROCES, V57, P3498, DOI 10.1109/TSP.2009.2022857; Sra, 2016, P INT C LEARN REPR I; Talwalkar A, 2013, J MACH LEARN RES, V14, P3129; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zaheer Manzil, 2017, ADV NEURAL INFORM PR, V2, P8; Zhang C, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017)	49	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303024
C	Marinescu, R; Dechter, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Marinescu, Radu; Dechter, Rina			Counting the Optimal Solutions in Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PROBABILISTIC INFERENCE; GENERAL SCHEME; AND/OR SEARCH	We introduce #opt, a new inference task for graphical models which calls for counting the number of optimal solutions of the model. We describe a novel variable elimination based approach for solving this task, as well as a depth-first branch and bound algorithm that traverses the AND/OR search space of the model. The key feature of the proposed algorithms is that their complexity is exponential in the induced width of the model only. It does not depend on the actual number of optimal solutions. Our empirical evaluation on various benchmarks demonstrates the effectiveness of the proposed algorithms compared with existing depth-first and best-first search based approaches that enumerate explicitly the optimal solutions.	[Marinescu, Radu] IBM Res, Dublin, Ireland; [Dechter, Rina] Univ Calif Irvine, Irvine, CA 92697 USA	University of California System; University of California Irvine	Marinescu, R (corresponding author), IBM Res, Dublin, Ireland.	radu.marinescu@ie.ibm.com; dechter@ics.uci.edu						Bayardo RJ, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P157; Bensana E., 1999, Constraints, V4, P293, DOI 10.1023/A:1026488509554; Bistarelli S, 1997, J ACM, V44, P201, DOI 10.1145/256303.256306; Brglez F., 1996, IEEE INT S CIRC SYST; Cabon B., 1999, Constraints, V4, P79, DOI 10.1023/A:1009812409930; Chakraborty S, 2014, AAAI CONF ARTIF INTE, P1722; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Dechter R, 2004, LECT NOTES COMPUT SC, V3258, P731; Dechter R, 2003, J ACM, V50, P107, DOI 10.1145/636865.636866; DECHTER R, 1985, J ACM, V32, P505, DOI 10.1145/3828.3830; Dechter R, 2013, SYNTHESIS LECT ARTIF; Dechter R., 2003, CONSTRAINT PROCESSIN; Dechter R, 2007, ARTIF INTELL, V171, P73, DOI 10.1016/j.artint.2006.11.003; Ermon S., 2013, P 30 INT C MACH LEAR; Fishelson M, 2005, HUM HERED, V59, P41, DOI 10.1159/000084736; Flerova N, 2016, J ARTIF INTELL RES, V55, P889, DOI 10.1613/jair.4985; Fremont DJ, 2017, AAAI CONF ARTIF INTE, P3885; Hadzic T., 2006, TECHNICAL REPORT; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Kask K, 2004, LECT NOTES COMPUT SC, V3258, P317; Kask K, 2001, ARTIF INTELL, V129, P91, DOI 10.1016/S0004-3702(01)00107-2; Kjaerulff U., 1990, TECHNICAL REPORT; Kohlas J, 2008, ARTIF INTELL, V172, P1360, DOI 10.1016/j.artint.2008.03.003; Lam W, 2017, J ARTIF INTELL RES, V60, P287, DOI 10.1613/jair.5475; Lawler E., 1956, OPER RES, V14, P699; Marinescu R, 2017, 31 AAAI C ART INT, P1749; Marinescu R, 2009, ARTIF INTELL, V173, P1457, DOI 10.1016/j.artint.2009.07.003; Marinescu R, 2009, ARTIF INTELL, V173, P1492, DOI 10.1016/j.artint.2009.07.004; Mateescu R, 2008, J ARTIF INTELL RES, V33, P465, DOI 10.1613/jair.2605; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Sang Tian, 2005, AAAI05 P 20 NAT C AR, V1, P475; SHACHTER RD, 1988, OPER RES, V36, P589, DOI 10.1287/opre.36.4.589; Vassilevska V, 2009, ACM S THEORY COMPUT, P455; Zeng JY, 2010, LECT N BIOINFORMAT, V6044, P550, DOI 10.1007/978-3-642-12683-3_36	34	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903070
C	Marx, CT; Phillips, RL; Friedler, SA; Scheidegger, C; Venkatasubramanian, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Marx, Charles T.; Phillips, Richard Lanas; Friedler, Sorelle A.; Scheidegger, Carlos; Venkatasubramanian, Suresh			Disentangling Influence: Using disentangled representations to audit model predictions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Motivated by the need to audit complex and black box models, there has been extensive research on quantifying how data features influence model predictions. Feature influence can be direct (a direct influence on model outcomes) and indirect (model outcomes are influenced via proxy features). Feature influence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper, we develop disentangled influence audits, a procedure to audit the indirect influence of features. Specifically, we show that disentangled representations provide a mechanism to identify proxy features in the dataset, while allowing an explicit computation of feature influence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled influence audits can both detect proxy features and show, for each individual or in aggregate, which of these proxy features affects the classifier being audited the most. In this respect, our method is more powerful than existing methods for ascertaining feature influence.	[Marx, Charles T.; Friedler, Sorelle A.] Haverford Coll, Haverford, PA 19041 USA; [Phillips, Richard Lanas] Cornell Univ, Ithaca, NY 14853 USA; [Scheidegger, Carlos] Univ Arizona, Tucson, AZ 85721 USA; [Venkatasubramanian, Suresh] Univ Utah, Salt Lake City, UT 84112 USA	Haverford College; Cornell University; University of Arizona; Utah System of Higher Education; University of Utah	Marx, CT (corresponding author), Haverford Coll, Haverford, PA 19041 USA.	cmarx@haverford.edu; rlp246@cornell.edu; sorelle@cs.haverford.edu; cscheid@cs.arizona.edu; suresh@cs.utah.edu			NSF [DMR-1709351, IIS-1633387, IIS-1633724, IIS-1815238]; DARPA SD2 program; Arnold and Mabel Beckman Foundation	NSF(National Science Foundation (NSF)); DARPA SD2 program; Arnold and Mabel Beckman Foundation	This research was funded in part by the NSF under grants DMR-1709351, IIS-1633387, IIS-1633724, and IIS-1815238, by the DARPA SD2 program, and the Arnold and Mabel Beckman Foundation. The Titan Xp used for this research was donated by the NVIDIA Corporation.	Adebayo Julius, 2016, ARXIV161104967; Adler P, 2018, KNOWL INF SYST, V54, P95, DOI 10.1007/s10115-017-1116-3; Alemi Alexander A., 2016, INT C LEARN REPR; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Blocki J, 2016, P IEEE CSFW, P371, DOI 10.1109/CSF.2016.33; Chen T. Q. R., 2018, ADV NEURAL INFORM PR, P2610; Creager E., 2019, ARXIV190602589; Edwards JP, 2016, J HIGH ENERGY PHYS, DOI 10.1007/JHEP01(2016)033; Esmaeili B, 2019, 22 INT C ART INT STA, P2525; Friedler SA, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P329, DOI 10.1145/3287560.3287589; Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009; Henelius A, 2014, DATA MIN KNOWL DISC, V28, P1503, DOI 10.1007/s10618-014-0368-8; Higgins I., 2018, ARXIV PREPRINT ARXIV; Hirsch T, 2017, DIS'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON DESIGNING INTERACTIVE SYSTEMS, P95, DOI 10.1145/3064663.3064703; Kim B., 2017, ARXIV171111279; Koh PW, 2017, PR MACH LEARN RES, V70; Kumar Abhishek., 2017, INT C LEARN REPR; Locatello F., 2018, ARXIV181112359; Locatello Francesco, 2019, ARXIV190513662; Lundberg S.M., 2017, ADV NEUR IN, P4765; Madras D., 2018, P 35 INT C MACH LEAR; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Molnar C., INTERPRETABLE MACHIN, DOI DOI 10.1007/S10290-014-0202-9; Ribeiro MT, 2016, P ACM KDD; Tschannen M., 2018, ARXIV181205069; Ustun B, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P10, DOI 10.1145/3287560.3287566	27	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304049
C	Mazur, D; Egiazarian, V; Morozov, S; Babenko, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mazur, Denis; Egiazarian, Vage; Morozov, Stanislav; Babenko, Artem			Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning useful representations is a key ingredient to the success of modern machine learning. Currently, representation learning mostly relies on embedding data into Euclidean space. However, recent work has shown that data in some domains is better modeled by non-euclidean metric spaces, and inappropriate geometry can result in inferior performance. In this paper, we aim to eliminate the inductive bias imposed by the embedding space geometry. Namely, we propose to map data into more general non-vector metric spaces: a weighted graph with a shortest path distance. By design, such graphs can model arbitrary geometry with a proper configuration of edges and weights. Our main contribution is PRODIGE: a method that learns a weighted graph representation of data end-to-end by gradient descent. Greater generality and fewer model assumptions make PRODIGE more powerful than existing embedding-based approaches. We confirm the superiority of our method via extensive experiments on a wide range of tasks, including classification, compression, and collaborative filtering.	[Mazur, Denis; Morozov, Stanislav; Babenko, Artem] Yandex, Moscow, Russia; [Egiazarian, Vage] Skoltech, Moscow, Russia; [Morozov, Stanislav] Lomonosov Moscow State Univ, Moscow, Russia; [Babenko, Artem] Natl Res Univ Higher Sch Econ, Moscow, Russia	Lomonosov Moscow State University; HSE University (National Research University Higher School of Economics)	Mazur, D (corresponding author), Yandex, Moscow, Russia.	denismazur@yandex-team.ru; Vage.egiazarian@skoltech.ru; stanis-morozov@yandex.ru; artem.babenko@phystech.edu	Morozov, Stanislav/AAO-3283-2020	Egiazarian, Vage/0000-0003-4444-9769	Russian Science Foundation [19-41-04109]	Russian Science Foundation(Russian Science Foundation (RSF))	Authors would like to thank David Talbot for his numerous suggestions on paper readability. We would also like to thank anonymous meta-reviewer for his insightful feedback. Vage Egiazarian was supported by the Russian Science Foundation under Grant 19-41-04109.	Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Chung J. S., 2018, ARXIV180605622; De Sa Christopher, 2018, ARXIV180403329; Donahue J, 2014, PR MACH LEARN RES, V32; Escolano F, 2011, LECT NOTES COMPUT SC, V6854, P194, DOI 10.1007/978-3-642-23672-3_24; Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Gu Albert, 2018, LEARNING MIXED CURVA; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Ivanov S, 2018, PR MACH LEARN RES, V80; Kang Z, 2020, IEEE T CYBERNETICS, V50, P1833, DOI 10.1109/TCYB.2018.2887094; Karasuyama M, 2017, MACH LEARN, V106, P307, DOI 10.1007/s10994-016-5607-3; Khrulkov Valentin, 2019, IEEE C COMP VIS PATT; Kingma D.P, P 3 INT C LEARNING R; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Louizos Christos, 2018, INT C LEARN REPR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Mikolov T., 2013, ARXIV; Nickel M, 2017, ADV NEUR IN, V30; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; RYAN Hopkins, 2015, FIN METR SPAC THEIR; Srinivas S, 2017, IEEE COMPUT SOC CONF, P455, DOI 10.1109/CVPRW.2017.61; Tifrea Alexandru, 2018, ARXIV181006546; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinh T.D.Q., 2018, AAAI; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Yu Y, 2019, PR MACH LEARN RES, V97; Zheng HY, 2015, 2015 IEEE TRUSTCOM/BIGDATASE/ISPA, VOL 1, P1, DOI 10.1109/Trustcom.2015.350; Zheng X, 2018, ADV NEUR IN, V31	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306086
C	Meding, K; Janzing, D; Scholkopf, B; Wichmann, FA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meding, Kristof; Janzing, Dominik; Schoelkopf, Bernhard; Wichmann, Felix A.			Perceiving the arrow of time in autoregressive motion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CAUSATION; MODEL	Understanding the principles of causal inference in the visual system has a long history at least since the seminal studies by Albert Michotte. Many cognitive and machine learning scientists believe that intelligent behavior requires agents to possess causal models of the world. Recent ML algorithms exploit the dependence structure of additive noise terms for inferring causal structures from observational data, e.g. to detect the direction of time series; the arrow of time. This raises the question whether the subtle asymmetries between the time directions can also be perceived by humans. Here we show that human observers can indeed discriminate forward and backward autoregressive motion with non-Gaussian additive independent noise, i.e. they appear sensitive to subtle asymmetries between the time directions. We employ a so-called frozen noise paradigm enabling us to compare human performance with four different algorithms on a trial-by-trial basis: A causal inference algorithm exploiting the dependence structure of additive noise terms, a neurally inspired network, a Bayesian ideal observer model as well as a simple heuristic. Our results suggest that all human observers use similar cues or strategies to solve the arrow of time motion discrimination task, but the human algorithm is significantly different from the three machine algorithms we compared it to. In fact, our simple heuristic appears most similar to our human observers.	[Meding, Kristof; Wichmann, Felix A.] Univ Tubingen, Neural Informat Proc Grp, Tubingen, Germany; [Janzing, Dominik] Amazon Res Tubingen, Tubingen, Germany; [Schoelkopf, Bernhard] Max Planck Inst Intelligent Syst, Empir Inference Dept, Tubingen, Germany	Eberhard Karls University of Tubingen; Max Planck Society	Meding, K (corresponding author), Univ Tubingen, Neural Informat Proc Grp, Tubingen, Germany.	kristof.meding@uni-tuebingen.de; janzind@amazon.com; bs@tuebingen.mpg.de; felix.wichmann@uni-tuebingen.de		Wichmann, Felix/0000-0002-2592-634X; Meding, Kristof/0000-0001-5073-2347	German Research Foundation (DFG) [SFB1233]; Robust Vision: Inference Principles and Neural Mechanisms, TP4 Causal inference strategies in human vision [276693517]	German Research Foundation (DFG)(German Research Foundation (DFG)); Robust Vision: Inference Principles and Neural Mechanisms, TP4 Causal inference strategies in human vision	This work was supported by the German Research Foundation (DFG): SFB1233, Robust Vision (project number 276693517): Inference Principles and Neural Mechanisms, TP4 Causal inference strategies in human vision (F.W. and B.S.).	Agus TR, 2010, NEURON, V66, P610, DOI 10.1016/j.neuron.2010.04.014; Akaike H., 1973, 2 INT S INF THEOR, P267, DOI DOI 10.1007/978-1-4612-1694-0_15; Bauer S, 2016, PR MACH LEARN RES, V48; Brainard David H., 2007, PERCEPTION, V36, P1; Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357; Cheng PW, 1997, PSYCHOL REV, V104, P367, DOI 10.1037/0033-295X.104.2.367; Danks David, 2010, OXFORD HDB CAUSATION, P447; Danks David, 2003, ADV NEURAL INFORM PR, V15, P83; Fodor J. A., 1983, MODULARITY MIND ESSA; Gallagher Regan M., 2018, BIORXIV; Geirhos R., 2019, P INT C LEARNING REP, P1; Geirhos R, 2018, ADV NEUR IN, V31; Geisler WS., 2003, VISUAL NEUROSCIENCES, P825, DOI [10.7551/mitpress/7131.003.0061, DOI 10.7551/MITPRESS/7131.003.0061]; Gerhard HE, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002873; Gerstenberg T., 2015, P 37 ANN C COGN SCI, P782; Goodman ND, 2011, PSYCHOL REV, V118, P110, DOI 10.1037/a0021336; Gopnik A., 2007, CAUSAL LEARNING PSYC; Goris RLT, 2013, PSYCHOL REV, V120, P472, DOI 10.1037/a0033136; Griffiths TL, 2005, COGNITIVE PSYCHOL, V51, P334, DOI 10.1016/j.cogpsych.2005.05.004; GUTTMAN N, 1963, J ACOUST SOC AM, V35, P610, DOI 10.1121/1.1918551; Heider F, 1944, AM J PSYCHOL, V57, P243, DOI 10.2307/1416950; Hoyer P.O., 2009, ADV NEURAL INFORM PR, P689; Janzing D, 2016, NEW J PHYS, V18, DOI 10.1088/1367-2630/18/9/093052; Janzing D, 2010, J STAT PHYS, V138, P767, DOI 10.1007/s10955-009-9897-8; Kubricht JR, 2017, TRENDS COGN SCI, V21, P749, DOI 10.1016/j.tics.2017.06.002; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Kushnir T., 2003, P ANN M COGN SCI SOC, V25; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; LESLIE AM, 1987, COGNITION, V25, P265, DOI 10.1016/S0010-0277(87)80006-9; Malinsky D, 2018, PHILOS COMPASS, V13, DOI 10.1111/phc3.12470; Martinez-Garcia M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00008; Michotte A. E, 1963, PERCEPTION CAUSALITY; Moors P, 2017, PEERJ, V5, DOI 10.7717/peerj.2932; Peters J, 2017, ADAPT COMPUT MACH LE; Peters J, 2014, J MACH LEARN RES, V15, P2009; Peters Jonas, 2009, P 26 INT C MACH LEAR, P801; Pickup LC, 2014, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2014.262; Price H., 1997, TIMES ARROW ARCHIMED; Reichenbach H., 1956, DIRECTION TIME; Rips LJ, 2011, PERSPECT PSYCHOL SCI, V6, P77, DOI 10.1177/1745691610393525; Rolfs M, 2013, CURR BIOL, V23, P250, DOI 10.1016/j.cub.2012.12.017; Rust NC, 2005, NAT NEUROSCI, V8, P1647, DOI 10.1038/nn1606; Schoenfelder VH, 2013, J ACOUST SOC AM, V134, P447, DOI 10.1121/1.4807561; Scholl BJ, 2013, SOCIAL PERCEPTION: DETECTION AND INTERPRETATION OF ANIMACY, AGENCY, AND INTENTION, P197; Schutt HH, 2017, J VISION, V17, DOI 10.1167/17.12.12; Schutt HH, 2016, VISION RES, V122, P105, DOI 10.1016/j.visres.2016.02.002; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Spirtes P., 2000, CAUSATION PREDICTION; Steng E, 2019, PLOS COMPUT BIOL, V15, P1; Steyvers M, 2003, COGNITIVE SCI, V27, P453, DOI 10.1016/S0364-0213(03)00010-7; Sun X., 2006, P 9 INT S ART INT MA, P1; Szegedy Christian, 2014, ARXIV13126199V4; Tenenbaum JB, 2001, ADV NEUR IN, V13, P59; Waskom ML, 2018, J VISION, V18, DOI 10.1167/18.6.9; Wei DL, 2018, PROC CVPR IEEE, P8052, DOI 10.1109/CVPR.2018.00840; Wichmann FA, 2001, PERCEPT PSYCHOPHYS, V63, P1293, DOI 10.3758/BF03194544; Wilson EB, 1927, J AM STAT ASSOC, V22, P209, DOI 10.2307/2276774; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647	62	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302032
C	Meintrup, S; Munteanu, A; Rohde, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meintrup, Stefan; Munteanu, Alexander; Rohde, Dennis			Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the k-median clustering problem for high-dimensional polygonal curves with finite but unbounded number of vertices. We tackle the computational issue that arises from the high number of dimensions by defining a Johnson-Lindenstrauss projection for polygonal curves. We analyze the resulting error in terms of the Frechet distance, which is a tractable and natural dissimilarity measure for curves. Our clustering algorithms achieve sublinear dependency on the number of input curves via subsampling. Also, we show that the Frechet distance can not be approximated within any factor of less than root 2 by probabilistically reducing the dependency on the number of vertices of the curves. As a consequence we provide a fast, CUDA-parallelized version of the Alt and Godau algorithm for computing the Frechet distance and use it to evaluate our results empirically.	[Meintrup, Stefan] TU Dortmund Univ, Fac Comp Sci, Dortmund, Germany; [Munteanu, Alexander] TU Dortmund Univ, Dortmund Data Sci Ctr, Dortmund, Germany; [Rohde, Dennis] TU Dortmund Univ, Chair Efficient Algorithms & Complex Theory, Dortmund, Germany	Dortmund University of Technology; Dortmund University of Technology; Dortmund University of Technology	Meintrup, S (corresponding author), TU Dortmund Univ, Fac Comp Sci, Dortmund, Germany.	stefan.meintrup@tu-dortmund.de; alexander.munteanu@tu-dortmund.de; dennis.rohde@tu-dortmund.de		Rohde, Dennis/0000-0001-8984-1962	German Science Foundation (DFG) [SFB 876]; Dortmund Data Science Center (DoDSc)	German Science Foundation (DFG)(German Research Foundation (DFG)); Dortmund Data Science Center (DoDSc)	We thank the anonymous reviewers for their valuable comments. This work was supported by the German Science Foundation (DFG), Collaborative Research Center SFB 876 "Providing Information by Resource-Constrained Analysis", project C4 and by the Dortmund Data Science Center (DoDSc).	Ackermann MR, 2010, ACM T ALGORITHMS, V6, DOI 10.1145/1824777.1824779; Agarwal PK, 2013, SIAM J COMPUT, V42, P442, DOI 10.1137/110830046; Aghabozorgi S, 2015, INFORM SYST, V53, P16, DOI 10.1016/j.is.2015.04.007; ALT H, 1995, INT J COMPUT GEOM AP, V5, P75, DOI 10.1142/S0218195995000064; Bachem O, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1119, DOI 10.1145/3219819.3219973; Baldus J., 2018, CORR; Billsus D., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P46; Braverman Vladimir, 2019, CORR; Bringmann K., 2019, SOCG; Buchin K., 2019, CORR; Buchin K., 2019, P 30 ANN ACM SIAM S, P2922, DOI [10.1137/1.9781611975482.181, DOI 10.1137/1.9781611975482.181]; Buchin K, 2017, 25TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2017), DOI 10.1145/3139958.3140064; Chapados N., 2008, ADV NEURAL INFORM PR, V20, P265; Daniely A., 2012, CORR; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Diitsch F., 2017, P 25 ACM SIGSPATIAL; Driemel A., 2016, PROC 27 ANN ACMSIAM, P766; Driemel A., 2018, P 16 INT WORKSH APPR, P218; Efrat A, 2007, J MATH IMAGING VIS, V27, P203, DOI 10.1007/s10851-006-0647-0; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Feldman D, 2010, PROC APPL MATH, V135, P630; Hamilton J.D., 1994, TIME SERIES ANAL, DOI 10.2307/j.ctv14jx6sm; Hastad Johan, 2007, THEORY COMPUTING, V3, P211, DOI DOI 10.4086/TOC.2007.V003A011; HONG ZQ, 1991, PATTERN RECOGN, V24, P211, DOI 10.1016/0031-3203(91)90063-B; Indyk P., 2000, THESIS; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Liao TW, 2005, PATTERN RECOGN, V38, P1857, DOI 10.1016/j.patcog.2005.01.025; Lucas DD, 2015, GEOSCI INSTRUM METH, V4, P121, DOI 10.5194/gi-4-121-2015; Munteanu A., 2018, ADV NEURAL INFORM PR, P6562; Roughgarden T, 2019, COMMUN ACM, V62, P88, DOI 10.1145/3232535; SHEEHY D. R., 2014, SOCG 14, P328; Sohler C, 2018, ANN IEEE SYMP FOUND, P802, DOI 10.1109/FOCS.2018.00081; Suresh V., 2011, ALENEX, P164; Wegener I., 2005, COMPLEXITY THEORY EX; Zhang JX, 2007, GEO-SPAT INF SCI, V10, P44, DOI 10.1007/s11806-007-0003-6; Zimmer C., 2018, ADV NEURAL INFORM PR, V31, P2730	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904046
C	Menon, AK; Rawat, AS; Reddi, SJ; Kumar, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Menon, Aditya Krishna; Rawat, Ankit Singh; Reddi, Sashank J.; Kumar, Sanjiv			Multilabel reductions: what is my loss optimising?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONSISTENCY; MULTICLASS	Multilabel classification is a challenging problem arising in applications ranging from information retrieval to image tagging. A popular approach to this problem is to employ a reduction to a suitable series of binary or multiclass problems (e.g., computing a softmax based cross-entropy over the relevant labels). While such methods have seen empirical success, less is understood about how well they approximate two fundamental performance measures: precision@ k and recall@ k. In this paper, we study five commonly used reductions, including the one-versus-all reduction, a reduction to multiclass classification, and normalisedversions of the same, wherein the contribution of each instance is normalised by the number of relevant labels. Our main result is a formal justification of each reduction: we explicate their underlying risks, and show they are each consistent with respect to either precision or recall. Further, we show that in general no reduction can be optimal for both measures. We empirically validate our results, demonstrating scenarios where normalised reductions yield recall gains over unnormalised counterparts.	[Menon, Aditya Krishna; Rawat, Ankit Singh; Reddi, Sashank J.; Kumar, Sanjiv] Google Res, New York, NY 10011 USA	Google Incorporated	Menon, AK (corresponding author), Google Res, New York, NY 10011 USA.	adityakmenon@google.com; sashank@google.com; ankitsrawat@google.com; sanjivk@google.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Agarwal S, 2014, J MACH LEARN RES, V15, P1653; Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741; Baker YS, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P10, DOI 10.1109/ISI.2013.6578776; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009; Brinker K, 2006, FRONT ARTIF INTEL AP, V141, P489; Buja Andreas, 2005, TECHNICAL REPORT; Cheng W., 2010, P 27 INT C MACH LEAR, P279; Dembczynski K, 2012, MACH LEARN, V88, P5, DOI 10.1007/s10994-012-5285-8; Jain H, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P528, DOI 10.1145/3289600.3290979; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Jernite Y, 2017, PR MACH LEARN RES, V70; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Kar P., 2014, ADV NEURAL INFORM PR, P694; Kar P, 2015, PR MACH LEARN RES, V37, P189; Koyejo Oluwasanmi O, 2015, NIPS, V28, P3321; LAPIN M, 2015, ADV NEURAL INFORM PR, P325; Lapin M, 2018, IEEE T PATTERN ANAL, V40, P1533, DOI 10.1109/TPAMI.2017.2751607; Liu Li-Ping, 2016, IJCAI; Pires Bernardo Avila, 2016, ARXIV PREPRINT ARXIV; Prabhu Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P993, DOI 10.1145/3178876.3185998; RAMASWAMY HG, P C LEARN THEOR; Reddi Sashank J, 2019, P MACHINE LEARNING R, P1940; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Rifkin R, 2004, J MACH LEARN RES, V5, P101; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Tasche Dirk, 2018, ABS180403077 CORR; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1; Tsoumakas G, 2007, LECT NOTES ARTIF INT, V4701, P406; Wu Xi-Zhu, 2017, P MACHINE LEARNING R, P3780; Wydmuch M., 2018, ADV NEURAL INFORM PR; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yang Forest, 2019, ABS190111141 CORR; Yen Ian E.H., 2017, P 23 ACM SIGKDD INT, P545; Yu H.-F., 2014, P 31 INT C INT C MAC, V32; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang T, 2004, ANN STAT, V32, P56	40	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902025
C	Metelli, AM; Likmeta, A; Restelli, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Metelli, Alberto Maria; Likmeta, Amarildo; Restelli, Marcello			Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BOUNDS	How does the uncertainty of the value function propagate when performing temporal difference learning? In this paper, we address this question by proposing a Bayesian framework in which we employ approximate posterior distributions to model the uncertainty of the value function and Wasserstein barycenters to propagate it across state-action pairs. Leveraging on these tools, we present an algorithm, Wasserstein Q-Learning (WQL), starting in the tabular case and then, we show how it can be extended to deal with continuous domains. Furthermore, we prove that, under mild assumptions, a slight variation of WQL enjoys desirable theoretical properties in the tabular setting. Finally, we present an experimental campaign to show the effectiveness of WQL on finite problems, compared to several RL algorithms, some of which are specifically designed for exploration, along with some preliminary results on Atari games.	[Metelli, Alberto Maria; Likmeta, Amarildo; Restelli, Marcello] Politecn Milan, DEIB, Milan, Italy	Polytechnic University of Milan	Metelli, AM (corresponding author), Politecn Milan, DEIB, Milan, Italy.	albertomaria.metelli@polimi.it; amarildo.likmeta@polimi.it; marcello.restelli@polimi.it	Metelli, Alberto Maria/AAY-5206-2020	Metelli, Alberto Maria/0000-0002-3424-5212; Restelli, Marcello/0000-0002-6322-1076				Abdullah M. A., 2019, ARXIV190713196; Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Azar MG, 2017, PR MACH LEARN RES, V70; Azizzadenesheli K, 2018, ARXIV180204412; Bartlett RA, 2009, 2009 ICSE WORKSHOP ON SOFTWARE ENGINEERING FOR COMPUTATIONAL SCIENCE AND ENGINEERING, P35, DOI 10.1109/SECSE.2009.5069160; Bellemare M., 2016, NEURIPS; Bellemare MG, 2017, PR MACH LEARN RES, V70; Berry Donald A., 1985, MONOGRAPHS STAT APPL, V5, P71; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; D'Eramo C, 2017, AAAI CONF ARTIF INTE, P1840; D'Eramo C, 2016, PR MACH LEARN RES, V48; Dabney W, 2018, AAAI CONF ARTIF INTE, P2892; Dearden R, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P761; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Fruit Ronan, 2018, JMLR WORKSHOP C P, P1573; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jiang N, 2017, PR MACH LEARN RES, V70; Jin C., 2018, ADV NEURAL INFORM PR; Kakade Sham M., 2003, THESIS; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Koenig S., 1992, TECHNICAL REPORT; Lattimore T, 2014, THEOR COMPUT SCI, V558, P125, DOI 10.1016/j.tcs.2014.09.029; ODonoghue B., 2018, P ICML, P3839; Ortner P., 2007, ADV NEURAL INFORM PR, V19, P49; Osband I, 2016, PR MACH LEARN RES, V48; Osband Ian, 2018, ADV NEURAL INFORM PR, V31, P8626; Ostrovski G., 2017, ARXIV170301310; Pacchiano Aldo, 2019, ARXIV190604349; Pazis Jason, 2016, ADV NEURAL INFORM PR, V29, P3891; Puterman M.L., 2014, MARKOV DECISION PROC; Rummery G.A., 1994, ON LINE Q LEARNING U, V37; Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Szita Istvan, 2010, INT C MACH LEARN ICM, P1031; Teh, 2018, P MACHINE LEARNING R, P29; TEWARI A, 2008, ADV NEURAL INFORM PR, P1505; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Van Hasselt H, 2010, ADV NEURAL INFORM PR, P2613; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; van Seijen H, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P177; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Watkins CJCH., 1989, THESIS	52	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304035
C	Meyerson, E; Miikkulainen, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meyerson, Elliot; Miikkulainen, Risto			Modular Universal Reparameterization: Deep Multi-task Learning Across Diverse Domains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEURAL-NETWORK	As deep learning applications continue to become more diverse, an interesting question arises: Can general problem solving arise from jointly learning several such diverse tasks? To approach this question, deep multi-task learning is extended in this paper to the setting where there is no obvious overlap between task architectures. The idea is that any set of (architecture,task) pairs can be decomposed into a set of potentially related subproblems, whose sharing is optimized by an efficient stochastic algorithm. The approach is first validated in a classic synthetic multi-task learning benchmark, and then applied to sharing across disparate architectures for vision, NLP, and genomics tasks. It discovers regularities across these domains, encodes them into sharable modules, and combines these modules systematically to improve performance in the individual tasks. The results confirm that sharing learned functionality across diverse domains and architectures is indeed beneficial, thus establishing a key ingredient for general problem solving in the future.	[Meyerson, Elliot; Miikkulainen, Risto] Cognizant, Austin, TX USA; [Miikkulainen, Risto] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Meyerson, E (corresponding author), Cognizant, Austin, TX USA.	elliot.meyerson@cognizant.com; risto@cs.utexas.edu						Abel D, 2018, PR MACH LEARN RES, V80; Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300; [Anonymous], 2016, P CVPR; [Anonymous], P ICLR; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bartlett PL, 1997, ADV NEUR IN, V9, P134; Bilen H, 2016, ADV NEUR IN, V29; Brunskill E, 2014, PR MACH LEARN RES, V32, P316; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Chen Z., 2018, P ICML 2018; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Deb K, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P653, DOI 10.1145/2908812.2908952; Devin Coline, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2169, DOI 10.1109/ICRA.2017.7989250; Doerr Benjamin, 2008, P 10 ANN C COMPUTATI, P929; Dong DX, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1723; Eisenberg B, 2008, STAT PROBABIL LETT, V78, P135, DOI 10.1016/j.spl.2007.05.011; Feng Y, 2011, ISBE 2011: 2011 INTERNATIONAL CONFERENCE ON BIOMEDICINE AND ENGINEERING, VOL 3, P521; Gomez A. N., 2017, ABS170605137 CORR; Ha D., 2017, P ICLR; Hashimoto Kazuma, 2017, P 2017 C EMP METH NA, P1923, DOI DOI 10.18653/V1/D17-1206; Huang JT, 2013, INT CONF ACOUST SPEE, P7304, DOI 10.1109/ICASSP.2013.6639081; Huang Z, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3625; Jaderberg Max, 2017, P ICLR; Jing XY, 2015, IEEE INT C BIOINFORM, P91; Jung C, 2017, CELL, V170, P35, DOI 10.1016/j.cell.2017.05.044; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; KROGH A, 1992, ADV NEUR IN, V4, P950; Kumar N, 2012, PR ELECTROMAGN RES S, P1725; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liang J., 2018, P GECCO; Long Mingsheng, 2017, NIPS; Lu Y., 2017, P CVPR; Luong M.-T., 2016, P ICLR; Merity S., 2018, P ICLR; Meyerson E., 2018, P ICML; Meyerson E., 2018, P ICLR; Neumann F, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3742; Paske A., 2017, AUTOMATIC DIFFERENTI; Ranjan R., 2016, ARXIV160301249; Rebuffi SA, 2018, PROC CVPR IEEE, P8119, DOI 10.1109/CVPR.2018.00847; Seltzer ML, 2013, INT CONF ACOUST SPEE, P6965, DOI 10.1109/ICASSP.2013.6639012; Shazeer N., 2017, P ICLR; Socher R., 2016, ABS160907843 CORR; Stanley KO, 2009, ARTIF LIFE, V15, P185, DOI 10.1162/artl.2009.15.2.15202; Sudholt Dirk., 2018, P GENETIC EVOLUTIONA, P1523; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Teh Y.W., 2017, NIPS, P4499; Thrun S., 2012, LEARNING LEARN; Witt C, 2013, COMB PROBAB COMPUT, V22, P294, DOI 10.1017/S0963548312000600; Wolsey L.A., 2014, INTEGER COMBINATORIA; Wu ZZ, 2015, INT CONF ACOUST SPEE, P4460, DOI 10.1109/ICASSP.2015.7178814; Yang Y., 2017, P ICLR; Yang Y., 2015, P ICLR; Zagoruyko S., 2016, BRIT MACHINE VISION; Zaremba W., 2014, ABS14092329 CORR; Zeng HY, 2016, BIOINFORMATICS, V32, P121, DOI 10.1093/bioinformatics/btw255; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7	61	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307087
C	Miao, N; Zhou, H; Zhao, CQ; Shi, WX; Li, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Miao, Ning; Zhou, Hao; Zhao, Chengqi; Shi, Wenxian; Li, Lei			Kernelized Bayesian Softmax for Text Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural models for text generation require a softmax layer with proper word embeddings during the decoding phase. Most existing approaches adopt single point embedding for each word. However, a word may have multiple senses according to different context, some of which might be distinct. In this paper, we propose KerBS, a novel approach for learning better embeddings for text generation. KerBS embodies two advantages: a) it employs a Bayesian composition of embeddings for words with multiple senses; b) it is adaptive to semantic variances of words and robust to rare sentence context by imposing learned kernels to capture the closeness of words (senses) in the embedding space. Empirical studies show that KerBS significantly boosts the performance of several text generation tasks.	[Miao, Ning; Zhou, Hao; Zhao, Chengqi; Shi, Wenxian; Li, Lei] ByteDance AI Lab, Beijing, Peoples R China		Miao, N (corresponding author), ByteDance AI Lab, Beijing, Peoples R China.	miaoning@bytedance.com; zhouhao.nlp@bytedance.com; zhaochengqi.d@bytedance.com; shiwenxian@bytedance.com; lileilab@bytedance.com		Li, Lei/0000-0003-3095-9776				Alec Radford TS, 2018, TECHNICAL REPORT; Ba J., 2017, P 3 INT C LEARN REPR; Bahdanau D., 2015, ICLR 2015; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Chen T, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P15; Chen X., 2014, EMNLP 2014 2014 C EM, P1025, DOI DOI 10.3115/V1/D14-1110; Chung J., 2014, NIPS 2014 WORKSH DEE; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Hovy, 2015, P 2015 C N AM CHAPT, P683, DOI DOI 10.3115/V1/N15-1070; Huang Eric, 2012, P 50 ANN M ASS COMP, V1, P873; inan Hakan, 2017, ICLR 2017; Koehn P., 2007, ACL; Li Yanran, 2017, ARXIV171003957; Liu Y, 2015, AAAI CONF ARTIF INTE, P2418; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Papineni K., 2002, ACL 2002; Pennington J., 2014, P 2014 C EMPIRICAL M, P1532; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Reisinger J., 2010, HUMAN LANGUAGE TECHN, P109; Riloff Ellen, 2018, P 2018 C EMP METH NA; Sennrich Rico, 2016, ACL 2016; Sordoni Alessandro, 2015, P 2015 C N AM CHAPT, P196, DOI DOI 10.3115/V1/N15-1020; Sun Chi, 2018, ARXIV180807016; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tian F., 2014, P COLING 2014 25 INT, P151; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vilnis Luke, 2015, P ICLR 2015; Wu ZH, 2015, AAAI CONF ARTIF INTE, P2188; Yang Zhilin, 2018, ICLR	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904019
C	Min, S; Maglaras, C; Moallemi, CC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Min, Seungki; Maglaras, Costis; Moallemi, Ciamac C.			Thompson Sampling with Information Relaxation Penalties	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION; DUALITY	We consider a finite-horizon multi-armed bandit (MAB) problem in a Bayesian setting, for which we propose an information relaxation sampling framework. With this framework, we define an intuitive family of control policies that include Thompson sampling (TS) and the Bayesian optimal policy as endpoints. Analogous to TS, which, at each decision epoch pulls an arm that is best with respect to the randomly sampled parameters, our algorithms sample entire future reward realizations and take the corresponding best action. However, this is done in the presence of "penalties" that seek to compensate for the availability of future information. We develop several novel policies and performance bounds for MAB problems that vary in terms of improving performance and increasing computational complexity between the two endpoints. Our policies can be viewed as natural generalizations of TS that simultaneously incorporate knowledge of the time horizon and explicitly consider the exploration-exploitation trade-off. We prove associated structural results on performance bounds and suboptimality gaps. Numerical experiments suggest that this new class of policies perform well, in particular in settings where the finite time horizon introduces significant exploration-exploitation tension into the problem.	[Min, Seungki; Maglaras, Costis; Moallemi, Ciamac C.] Columbia Business Sch, New York, NY 10027 USA	Columbia University	Min, S (corresponding author), Columbia Business Sch, New York, NY 10027 USA.							Berry D.A., 1985, BANDIT PROBLEMS SEQU; BRADT RN, 1956, ANN MATH STAT, V27, P1060, DOI 10.1214/aoms/1177728073; Brown DB, 2017, OPER RES, V65, P1355, DOI 10.1287/opre.2017.1631; Brown DB, 2010, OPER RES, V58, P785, DOI 10.1287/opre.1090.0796; Bubeck, 2013, ADV NEURAL INFORM PR, P638; Davis M. H. A., 1994, DETERMINISTIC APPROA; Ding Wenkui, 2013, P 27 AAAI C ART INT; GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148; Gutin E., 2016, P 30 INT C NEURAL IN, P3161; Haugh M, 2014, SIAM J FINANC MATH, V5, P316, DOI 10.1137/120896761; Haugh M, 2012, OPER RES LETT, V40, P521, DOI 10.1016/j.orl.2012.08.010; Kaufmann E., 2012, P 15 INT C ARTIFICIA, P592; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Marchal Olivier, 2017, SUBGAUSSIANITY BETA; ROCKAFELLAR RT, 1991, MATH OPER RES, V16, P119, DOI 10.1287/moor.16.1.119; Russo D, 2018, OPER RES, V66, P230, DOI 10.1287/opre.2017.1663; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303053
C	Monachou, F; Ashlagi, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Monachou, Faidra; Ashlagi, Itai			Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The increasing popularity of online two-sided markets such as ride-sharing, accommodation and freelance labor platforms, goes hand in hand with new socioeconomic challenges. One major issue remains the existence of bias and discrimination against certain social groups. We study this problem using a two-sided large market model with employers and workers mediated by a platform. Employers who seek to hire workers face uncertainty about a candidate worker's skill level. Therefore, they base their hiring decision on learning from past reviews about an individual worker as well as on their (possibly misspecified) prior beliefs about the ability level of the social group the worker belongs to. Drawing upon the social learning literature with bounded rationality and limited information, uncertainty combined with social bias leads to unequal hiring opportunities between workers of different social groups. Although the effect of social bias decreases as the number of reviews increases (consistent with empirical findings), minority workers still receive lower expected payoffs. Finally, we consider a simple directed matching policy (DM), which combines learning and matching to make better matching decisions for minority workers. Under this policy, there exists a steady-state equilibrium, in which DM reduces the discrimination gap.	[Monachou, Faidra; Ashlagi, Itai] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Monachou, F (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	monachou@stanford.edu; iashlagi@stanford.edu						Abrahao B, 2017, P NATL ACAD SCI USA, V114, P9848, DOI 10.1073/pnas.1604234114; Agrawal Rajeev, 1989, IEEE T AUTOMAT CONTR, V34, P3; Altonji JG, 2001, Q J ECON, V116, P313, DOI 10.1162/003355301556329; Ameri M., 2017, NO ROOM INN DISABILI; [Anonymous], [No title captured]; Arrow K., 1973, DISCRIMINATION LABOR, P3; BANERJEE AV, 1992, Q J ECON, V107, P797, DOI 10.2307/2118364; Becker G. S, 1957, EC DISCRIMINATION; Besbes O, 2018, OPER RES, V66, P597, DOI 10.1287/opre.2017.1676; BIKHCHANDANI S, 1992, J POLIT ECON, V100, P992, DOI 10.1086/261849; Bohren J Aislinn, 2019, AM EC REV; Bordalo Pedro, 2016, TECHNICAL REPORT; COATE S, 1993, AM ECON REV, V83, P1220; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Crapis D, 2017, MANAGE SCI, V63, P3586, DOI 10.1287/mnsc.2016.2526; DEGROOT MH, 1974, J AM STAT ASSOC, V69, P118, DOI 10.2307/2285509; Edelman B, 2017, AM ECON J-APPL ECON, V9, P1, DOI 10.1257/app.20160213; ELLISON G, 1993, J POLIT ECON, V101, P612, DOI 10.1086/261890; ELLISON G, 1995, Q J ECON, V110, P93, DOI 10.2307/2118512; Epstein LG, 2010, BE J THEOR ECON, V10; Fryer R, 2008, BE J THEOR ECON, V8; Fryer RG, 2007, J PUBLIC ECON, V91, P1151, DOI 10.1016/j.jpubeco.2006.05.015; Ge Yanbo, 2016, TECHNICAL REPORT; Golub B, 2010, AM ECON J-MICROECON, V2, P112, DOI 10.1257/mic.2.1.112; Hannak A, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P1914, DOI 10.1145/2998181.2998327; Hu L, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1389, DOI 10.1145/3178876.3186044; Ifrach Bar, 2014, ACM SIGMETRICS Performance Evaluation Review, V41; Jackson M.O., 2011, HDB SOCIAL EC, V1A, P133; Johari R, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P119, DOI 10.1145/3033274.3084095; Joseph Matthew, 2016, NIPS, P325; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Levin J, 2009, BE J THEOR ECON, V9; Levy Karen, 2017, DESIGNING DISCRIMINA; Pan JF, 2016, ADV COMPUT SCI TECH, P161; PHELPS ES, 1972, AM ECON REV, V62, P659; Pronin E, 2002, PERS SOC PSYCHOL B, V28, P369, DOI 10.1177/0146167202286008; Rosenblat A, 2017, POLICY INTERNET, V9, P256, DOI 10.1002/poi3.153; Schwartzstein J, 2014, J EUR ECON ASSOC, V12, P1423, DOI 10.1111/jeea.12104; Smith L, 2000, ECONOMETRICA, V68, P371, DOI 10.1111/1468-0262.00113; Le TN, 2016, IEEE INT SYMP INFO, P2089, DOI 10.1109/ISIT.2016.7541667; Vaccari S., 2018, SOCIAL LEARNING ONLI	41	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302017
C	Monteiller, P; Claici, S; Chien, E; Mirzazadeh, F; Solomon, J; Yurochkin, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Monteiller, Pierre; Claici, Sebastian; Chien, Edward; Mirzazadeh, Farzaneh; Solomon, Justin; Yurochkin, Mikhail			Alleviating Label Switching with Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MIXTURE; BARYCENTERS	Label switching is a phenomenon arising in mixture model posterior inference that prevents one from meaningfully assessing posterior statistics using standard Monte Carlo procedures. This issue arises due to invariance of the posterior under actions of a group; for example, permuting the ordering of mixture components has no effect on the likelihood. We propose a resolution to label switching that leverages machinery from optimal transport. Our algorithm efficiently computes posterior statistics in the quotient space of the symmetry group. We give conditions under which there is a meaningful solution to label switching and demonstrate advantages over alternative approaches on simulated and real data.	[Monteiller, Pierre] ENS Ulm, Ulm, Germany; [Claici, Sebastian; Chien, Edward; Solomon, Justin] MIT, CSAIL, Cambridge, MA 02139 USA; [Claici, Sebastian; Chien, Edward; Mirzazadeh, Farzaneh; Solomon, Justin; Yurochkin, Mikhail] MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA; [Mirzazadeh, Farzaneh; Yurochkin, Mikhail] IBM Res, San Jose, CA USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); International Business Machines (IBM)	Monteiller, P (corresponding author), ENS Ulm, Ulm, Germany.	pierre.monteiller@ens.fr; sclaici@mit.edu; edchien@mit.edu; farzaneh@ibm.com; jsolomon@mit.edu; mikhail.yurochkin@ibm.com			Army Research Office [W911NF1710068]; Air Force Office of Scientific Research [FA9550-19-1-031]; National Science Foundation [IIS-1838071]; Amazon Research Award; MIT-IBM Watson Al Laboratory; Toyota-CSAIL Joint Research Center; QCRI-CSAIL Computer Science Research Program	Army Research Office; Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Science Foundation(National Science Foundation (NSF)); Amazon Research Award; MIT-IBM Watson Al Laboratory(International Business Machines (IBM)); Toyota-CSAIL Joint Research Center; QCRI-CSAIL Computer Science Research Program	J. Solomon acknowledges the generous support of Army Research Office grant W911NF1710068, Air Force Office of Scientific Research award FA9550-19-1-031, of National Science Foundation grant IIS-1838071, from an Amazon Research Award, from the MIT-IBM Watson Al Laboratory, from the Toyota-CSAIL Joint Research Center, from the QCRI-CSAIL Computer Science Research Program, and from a gift from Adobe Systems. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of these organizations.	Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Alvarez-Melis David, 2018, P 2018 C EMP METH NA, P1881, DOI DOI 10.18653/V1/D18-1214; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bandeira A. S., 2014, P 5 C INN THEOR COMP, P459; Carlier G, 2015, ESAIM-MATH MODEL NUM, V49, P1621, DOI 10.1051/m2an/2015033; Carpenter B., 2017, J STAT SOFTW, DOI [10.18637/jss.v076.i01, DOI 10.18637/JSS.V076.I01]; CASELLA G, 1992, AM STAT, V46, P167, DOI 10.2307/2685208; Celeux G, 2000, J AM STAT ASSOC, V95, P957, DOI 10.2307/2669477; Claici S., 2018, P 35 INT C MACH LEAR, P998; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; DIEBOLT J, 1994, J ROY STAT SOC B MET, V56, P363; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Fadell Edward R., 2001, SPRINGER MG MATH, DOI 10.1007/978-3-642-56446-8; Genevay A, 2018, PR MACH LEARN RES, V84; Jasra A, 2005, STAT SCI, V20, P50, DOI 10.1214/088342305000000016; Kim YH, 2017, ADV MATH, V307, P640, DOI 10.1016/j.aim.2016.11.026; KOBAYASHI S, 1995, IEEE MTT-S, P391, DOI 10.1109/MWSYM.1995.405962; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Lott J, 2009, ANN MATH, V169, P903, DOI 10.4007/annals.2009.169.903; Marin JM, 2005, HANDB STAT, V25, P459, DOI 10.1016/S0169-7161(05)25016-2; McLachlan GJ, 2019, ANNU REV STAT APPL, V6, P355, DOI 10.1146/annurev-statistics-031017-100325; Muzellec Boris, 2018, ADV NEURAL INFORM PR, P10237; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Papastamoulis P, 2015, ARXIV150302271; Perry A, 2017, CORR; Peyre G., 2018, COMPUTATIONAL OPTIMA; Santambrogio F, 2015, PROG NONLINEAR DIFFE, V87, P1, DOI 10.1007/978-3-319-20828-2_1; Solomon J., 2018, AMS SHORT COURS DIS; Srivastava S, 2015, JMLR WORKSH CONF PRO, V38, P912; Staib M, 2017, ADV NEURAL INFORM PR, P2644; Uribe CA, 2018, IEEE DECIS CONTR P, P6544, DOI 10.1109/CDC.2018.8619160; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Zwart JP, 2003, IEE P-RADAR SON NAV, V150, P411, DOI 10.1049/ip-rsn:20030428	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905030
C	Mroueh, Y; Sercu, T; Rigotti, M; Padhi, I; Dos Santos, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mroueh, Youssef; Sercu, Tom; Rigotti, Mattia; Padhi, Inkit; Dos Santos, Cicero			Sobolev Independence Criterion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FALSE DISCOVERY RATE	We propose the Sobolev Independence Criterion (SIC), an interpretable dependency measure between a high dimensional random variable X and a response variable Y. SIC decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. SIC can be seen as a gradient regularized Integral Probability Metric (IPM) between the joint distribution of the two random variables and the product of their marginals. We use sparsity inducing gradient penalties to promote input sparsity of the critic of the IPM. In the kernel version we show that SIC can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. We then present a neural version of SIC where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. We conduct experiments validating SIC for feature selection in synthetic and real-world experiments. We show that SIC enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the False Discovery Rate.	[Mroueh, Youssef; Dos Santos, Cicero] IBM Res, Yorktown Hts, NY 10598 USA; MIT IBM Watson lab, Cambridge, MA USA; [Sercu, Tom] Facebook AI Res, Menlo Pk, CA USA; [Dos Santos, Cicero] Amazon AWS AI, New York, NY USA	International Business Machines (IBM); Facebook Inc	Mroueh, Y (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	mroueh@us.ibm.com; mrigotti@us.ibm.com; inkit.padhi@ibm.com	Rigotti, Mattia/H-7252-2019	Rigotti, Mattia/0000-0001-6466-2810				Arbel Michael, 2018, ADV NEURAL INF PROCE; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bach Francis, 2011, OPTIMIZATION SPARSIT; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Barber RF, 2015, ANN STAT, V43, P2055, DOI 10.1214/15-AOS1337; Barra F, 2019, EXPERT OPIN BIOL TH, V19, P343, DOI 10.1080/14712598.2019.1581761; Barretina J, 2012, NATURE, V483, P603, DOI 10.1038/nature11003; Beck Amir, 2003, OPER RES LETT; Belghazi Mohamed Ishmael, 2018, MINE MUTUAL INFORM N; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman Leo, 2001, RANDOM FORESTS MACH; Candes Emmanuel, 2018, PANNING GOLD MODEL X; Drucker Harris, 1992, IMPROVING GEN PERFOR; Feng J., 2017, SPARSE INPUT NEURAL; Feng Jean, 2017, ARXIV PREPRINT ARXIV; Fukumizu K., 2009, INTEGRAL PROBABILITY; Gretton A., 2008, ADV NEURAL INFORM PR, V20; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS 2017; Hastie T, 2009, ELEMENTS STAT LEARNI; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kingma D.P, P 3 INT C LEARNING R; Liang Faming, 2018, J AM STAT ASS, V113; Melis David Alvarez, 2018, ADV NEURAL INFORM PR, V31; Mroueh Y., 2018, P INT C LEARN REPR; Mroueh Youssef, 2019, AISTATS; Muandet Krikamol, 2017, KERNEL MEAN EMBEDDIN; Ozair Sherjil, 2019, WASSERSTEIN DEPENDEN; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Perez E, 2017, ARXIV170703017; Razaviyayn Meisam, 2013, SIAM J OPTIMIZATION; Rhee SY, 2006, P NATL ACAD SCI USA, V103, P17355, DOI 10.1073/pnas.0607274103; Rosasco Lorenzo, 2013, J MACH LEARN RES; Sesia Matteo, 2017, R TUTORIAL KNOCKOFFS; Shrikumar A, 2017, PR MACH LEARN RES, V70; Song Le, 2012, J MACH LEARN RES; Tansey W., 2018, ARXIV181100645; Tseng P., 2001, J OPTIM THEORY APPL, V109; van den Oord Aaron, 2016, ARXIV160605328; Vinod H.D., 1976, J ECONOMETRICS; Wahba Grace, 1975, NUMERISCHE MATH, V24; Yamada Yutaro, 2018, DEEP SUPERVISED FEAT; Ye Mao, 2018, P 35 INT C MACH LEAR	46	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901017
C	Muke, N; Neu, G; Rosasco, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Muecke, Nicole; Neu, Gergely; Rosasco, Lorenzo			Beating SGD Saturation with Tail-Averaging and Minibatching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC-APPROXIMATION; ALGORITHMS	While stochastic gradient descent (SGD) is one of the major workhorses in machine learning, the learning properties of many practically used variants are still poorly understood. In this paper, we consider least squares learning in a nonparametric setting and contribute to filling this gap by focusing on the effect and interplay of multiple passes, mini-batching and averaging, in particular tail averaging. Our results show how these different variants of SGD can be combined to achieve optimal learning rates, providing practical insights. A novel key result is that tail averaging allows faster convergence rates than uniform averaging in the nonparametric setting. Further, we show that a combination of tail-averaging and minibatching allows more aggressive step-size choices than using any one of said components.	[Muecke, Nicole] Univ Stuttgart, Inst Stochast & Applicat, Stuttgart, Germany; [Neu, Gergely] Univ Pompeu Fabra, Barcelona, Spain; [Rosasco, Lorenzo] Univ Genoa, Ist Italiano Tecnol, Genoa, Italy; [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA	University of Stuttgart; Pompeu Fabra University; Istituto Italiano di Tecnologia - IIT; University of Genoa; Massachusetts Institute of Technology (MIT)	Muke, N (corresponding author), Univ Stuttgart, Inst Stochast & Applicat, Stuttgart, Germany.	nicole.muecke@mathematik.uni-stuttgart.de; gergely.neu@gmail.com; lrosasco@mit.edu			German Research Foundation under DFG [STE 1074/4-1]; La Caixa Banking Foundation through the Junior Leader Postdoctoral Fellowship Program; Google Faculty Research Award; AFOSR [FA9550-17-1-0390, BAA-AFRL-AFOSR-2016-0007]; EU H2020-MSCA-RISE project [MADS -DLV-777826]	German Research Foundation under DFG(German Research Foundation (DFG)); La Caixa Banking Foundation through the Junior Leader Postdoctoral Fellowship Program; Google Faculty Research Award(Google Incorporated); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); EU H2020-MSCA-RISE project	Nicole Micke is supported by the German Research Foundation under DFG Grant STE 1074/4-1. Gergely Neu was supported by La Caixa Banking Foundation through the Junior Leader Postdoctoral Fellowship Program and a Google Faculty Research Award. Lorenzo Rosasco acknowledges the financial support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS -DLV-777826.	Aguech R, 2000, SIAM J CONTROL OPTIM, V39, P872, DOI 10.1137/S0363012998333852; [Anonymous], 2012, PROC INT C MACH LEAR; [Anonymous], 2016, FDN TRENDS IN OPTIMI; [Anonymous], 2015, ADV NEURAL INFORM PR; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022; Blanchard G., 2017, FDN COMPUTATIONAL MA, V18, P971; Caponnetto A., 2006, FDN COMPUTATIONAL MA, V7, P331, DOI [10.1007/s10208-006-0196-8.2, DOI 10.1007/S10208-006-0196-8.2]; Cotter A., 2011, P NEURIPS GRAN SPAIN; Cucker F, 2002, B AM MATH SOC, V39, P1; De Vito E, 2005, FOUND COMPUT MATH, V5, P59, DOI 10.1007/s10208-004-0134-1; Dieuleveut A, 2017, J MACH LEARN RES, V18; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Engl HW, 1996, REGULARIZATION INVER, V375; Garrigos G., 2017, ARXIV PREPRINT ARXIV; Guo ZC, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/aa72b2; Gyorfi L, 1996, SIAM J CONTROL OPTIM, V34, P31, DOI 10.1137/S0363012992226661; Hardt M, 2016, PR MACH LEARN RES, V48; Lin J., 2016, CORR; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; Lin J., 2017, JOURNALO F MACHINE L, V18; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Neu G., 2018, P 31 C LEARNING THEO, V75, P3222; Orabona F, 2014, ADV NEUR IN, V27; Pillaud-Vivien L., 2018, CORR; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Rakhlin A, 2011, ARXIV11095647; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Ruppert D., 1988, TECHNICAL REPORT; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shamir O., 2013, P 30 INT C MACH LEAR; Smale S, 2006, FOUND COMPUT MATH, V6, P145, DOI 10.1007/s10208-004-0160-z; Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Zhang T., 2003, ADV NEURAL INFORM PR, V2003	38	0	0	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904026
C	Mueller, J; Syrgkanis, V; Taddy, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mueller, Jonas; Syrgkanis, Vasilis; Taddy, Matt			Low-Rank Bandit Methods for High-Dimensional Dynamic Pricing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider dynamic pricing with many products under an evolving but low-dimensional demand model. Assuming the temporal variation in cross-elasticities exhibits low-rank structure based on fixed (latent) features of the products, we show that the revenue maximization problem reduces to an online bandit convex optimization with side information given by the observed demands. We design dynamic pricing algorithms whose revenue approaches that of the best fixed price vector in hindsight, at a rate that only depends on the intrinsic rank of the demand model and not the number of products. Our approach applies a bandit convex optimization algorithm in a projected low-dimensional space spanned by the latent product features, while simultaneously learning this span via online singular value decomposition of a carefully-crafted matrix containing the observed demands.	[Mueller, Jonas] MIT, CSAIL, Cambridge, MA 02139 USA; [Syrgkanis, Vasilis] Microsoft Res, Cambridge, MA USA; [Taddy, Matt] Chicago Booth, Chicago, IL USA	Massachusetts Institute of Technology (MIT); Microsoft	Mueller, J (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	jonasmueller@csail.mit.edu; vasy@microsoft.com; taddy@chicagobooth.edu	Mueller, Jonas/AAY-6891-2020					Besbes O, 2009, OPER RES, V57, P1407, DOI 10.1287/opre.1080.0640; Brand M, 2006, LINEAR ALGEBRA APPL, V415, P20, DOI 10.1016/j.laa.2005.07.021; Bubeck S., 2017, P 49 ANN ACM SIGACT; Bubeck Sebastien, 2012, C LEARN THEOR; Cohen Maxime C, 2016, ACM C EC COMP; Dani V, 2007, NEURAL INFORM PROCES; Dean S., 2017, ARXIV171001688; den Boer AV, 2014, MANAGE SCI, V60, P770, DOI 10.1287/mnsc.2013.1788; Djolonga J., 2013, NEURAL INFORM PROCES; Flaxman A. D., 2005, P 16 ANN ACM SIAM S; Gopalan Aditya, 2016, ARXIV160901508; Hazan E, 2014, ADV NEURAL INFORM PR; Hazan E., 2016, INT C MACH LEARN; Hazan E., 2016, C LEARN THEOR; Houthakker H. S., 1966, CONSUMER DEMAND US 1; Javanmard A, 2017, J MACH LEARN RES, V18, P1; Javanmard Adel, 2016, ARXIV160907574; Keskin NB, 2014, OPER RES, V62, P1142, DOI 10.1287/opre.2014.1294; Kleinberg Robert, 2003, P 44 ANN IEEE S FDN; Misra K., 2017, DYNAMIC ONLINE PRICI; Mueller J, 2018, J AM STAT ASSOC, V113, P1296, DOI 10.1080/01621459.2017.1341412; Rakhlin A., 2013, C LEARN THEOR; Rigollet P., 2015, HIGH DIMENSIONAL STA; Rudelson M, 2008, ADV MATH, V218, P600, DOI 10.1016/j.aim.2008.01.010; Sen R., 2017, ARTIFICIAL INTELLIGE; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Stange P., 2008, APP MATH MECH, V8, P10827, DOI DOI 10.1002/PAMM.200810827; WITT U, 1986, BEHAV SCI, V31, P173, DOI 10.1002/bs.3830310304; Yu Y, 2015, BIOMETRIKA, V102, P315, DOI 10.1093/biomet/asv008; Zhao F., 2016, INT JOINT C ART INT	32	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907016
C	Ndiaye, E; Takeuchi, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ndiaye, Eugene; Takeuchi, Ichiro			Computing Full Conformal Prediction Set with Approximate Homotopy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					If you are predicting the label y of a new object with Y, how confident are you that y = (y) over cap Conformal prediction methods provide an elegant framework for answering such question by building a 100(1 - alpha)% confidence region without assumptions on the distribution of the data. It is based on a refitting procedure that parses all the possibilities for y to select the most likely ones. Although providing strong coverage guarantees, conformal set is impractical to compute exactly for many regression problems. We propose efficient algorithms to compute conformal prediction set using approximated solution of (convex) regularized empirical risk minimization. Our approaches rely on a new homotopy continuation technique for tracking the solution path with respect to sequential changes of the observations. We also provide a detailed analysis quantifying its complexity.	[Ndiaye, Eugene] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan; [Takeuchi, Ichiro] Nagoya Inst Technol, Nagoya, Aichi, Japan	RIKEN; Nagoya Institute of Technology	Ndiaye, E (corresponding author), RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.	eugene.ndiaye@riken.jp; takeuchi.ichiro@nitech.ac.jp			MEXT KAKENHI [17H00758, 16H06538]; JST CREST [JPMJCR1502]; RIKEN Center for Advanced Intelligence Project; JST support program for starting up innovation-hub on materials research by information integration initiative	MEXT KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); RIKEN Center for Advanced Intelligence Project; JST support program for starting up innovation-hub on materials research by information integration initiative	We would like to thank the reviewers for their valuable feedbacks and detailed comments which contributed to improve the quality of this paper. This work was partially supported by MEXT KAKENHI (17H00758, 16H06538), JST CREST (JPMJCR1502), RIKEN Center for Advanced Intelligence Project, and JST support program for starting up innovation-hub on materials research by information integration initiative.	Bach F., 2012, OPTIMIZATION SPARSIT, V1st; Balasubramanian V., 2014, CONFORMAL PREDICTION; Brocker J., 2011, NONLINEAR PROCESSES; Chang Y-C., 2007, QUALITY QUANTITY; Chen W., 2018, STAT; Garrigues P., 2009, ADV NEURAL INFORM PR, P489; Gartner B., 2012, J COMPUTATIONAL GEOM; Giesen J., 2012, ADV NEURAL INFORM PR; Gruber M., 2010, REGRESSION ESTIMATOR; Hoerl A. E., 1970, TECHNOMETRICS; Kalnay E, 1996, B AM METEOROLOGICAL; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Lei J., 2019, BIOMETRIKA; Ndiaye E., 2019, ICML; Ndiaye E, 2017, J MACH LEARN RES, V18; Nesterov Y., 2018, APPL OPTIMIZATION; Nouretdinov I., 2001, ICML; Pedregosa F., 2011, J MACH LEARN RES, V12, P3426; Rockafellar R. T., 1997, CONVEX ANAL; Shafer G., 2008, J MACHINE LEARNING R; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Vovk V., 2005, ALGORITHMIC LEARNING, DOI DOI 10.1007/B106715; Wasserman, 2018, J AM STAT ASS	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301038
C	Neklyudov, K; Egorov, E; Vetrov, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Neklyudov, Kirill; Egorov, Evgenii; Vetrov, Dmitry			The Implicit Metropolis-Hastings Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent works propose using the discriminator of a GAN to filter out unrealistic samples of the generator. We generalize these ideas by introducing the implicit Metropolis-Hastings algorithm. For any implicit probabilistic model and a target distribution represented by a set of samples, implicit Metropolis-Hastings operates by learning a discriminator to estimate the density-ratio and then generating a chain of samples. Since the approximation of density ratio introduces an error on every step of the chain, it is crucial to analyze the stationary distribution of such chain. For that purpose, we present a theoretical result stating that the discriminator loss upper bounds the total variation distance between the target distribution and the stationary distribution. Finally, we validate the proposed algorithm both for independent and Markov proposals on CIFAR-10, CelebA and ImageNet datasets.	[Neklyudov, Kirill; Vetrov, Dmitry] Natl Res Univ Higher Sch Econ, HSE, Samsung HSE Lab, Moscow, Russia; [Egorov, Evgenii] Skolkovo Inst Sci & Technol, Skoltech, Moscow, Russia; [Vetrov, Dmitry] Samsung AI Ctr, Moscow, Russia	HSE University (National Research University Higher School of Economics); Skolkovo Institute of Science & Technology	Neklyudov, K (corresponding author), Natl Res Univ Higher Sch Econ, HSE, Samsung HSE Lab, Moscow, Russia.	k.necludov@gmail.com; egorov.evgenyy@ya.ru; vetrovd@yandex.ru			Samsung Research, Samsung Electronics; Russian Science Foundation [19-71-30020]	Samsung Research, Samsung Electronics; Russian Science Foundation(Russian Science Foundation (RSF))	This research is in part based on the work supported by Samsung Research, Samsung Electronics. Dmitry Vetrov and Kirill Neklyudov were supported by the Russian Science Foundation grant no. 19-71-30020.	[Anonymous], 2014, ICLR; Arora S., 2017, ARXIV170608224; Azadi Samaneh, 2018, ARXIV181006758; Barratt S., 2018, ARXIV180101973; Bottou L., 2017, ARXIV170107875STATML; Brock A., 2018, ARXIV; Donahue J., 2016, ARXIV160509782; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Ledig C., 2017, PROC CVPR IEEE, P4681, DOI [10.1109/CVPR.2017.19, DOI 10.1109/CVPR.2017.19]; LI CL, 2017, ADV NEURAL INFORM PR, P203, DOI DOI 10.1109/ICICTA.2017.52; Meyn S. P., 2012, MARKOV CHAINS STOCHA; Neklyudov K., 2018, ARXIV181007151; Orey S, 1971, LECT NOTES LIMIT THE; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pollard D., 2000, ASYMPTOPIA UNPUB; Radford A., 2015, P COMP C; Roberts GO, 2001, STAT SCI, V16, P351, DOI 10.1214/ss/1015346320; Rosenthal JS., 2004, PROBAB SURV, V1, P20, DOI [10.1214/154957804100000024, DOI 10.1214/154957804100000024]; Salimans T., 2016, ADV NEUR IN, P2234; Turner R., 2018, P INT C MACH LEARN P	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905059
C	Nemeth, C; Lindsten, F; Filippone, M; Hensman, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nemeth, Christopher; Lindsten, Fredrik; Filippone, Maurizio; Hensman, James			Pseudo-Extended Markov Chain Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				HORSESHOE; INFERENCE; SAMPLER	Sampling from posterior distributions using Markov chain Monte Carlo (MCMC) methods can require an exhaustive number of iterations, particularly when the posterior is multi-modal as the MCMC sampler can become trapped in a local mode for a large number of iterations. In this paper, we introduce the pseudo-extended MCMC method as a simple approach for improving the mixing of the MCMC sampler for multi-modal posterior distributions. The pseudo-extended method augments the state-space of the posterior using pseudo-samples as auxiliary variables. On the extended space, the modes of the posterior are connected, which allows the MCMC sampler to easily move between well-separated posterior modes. We demonstrate that the pseudo-extended approach delivers improved MCMC sampling over the Hamiltonian Monte Carlo algorithm on multi-modal posteriors, including Boltzmann machines and models with sparsity-inducing priors.	[Nemeth, Christopher] Univ Lancaster, Dept Math & Stat, Lancaster, England; [Lindsten, Fredrik] Linkoping Univ, Dept Comp & Informat Sci, Linkoping, Sweden; [Filippone, Maurizio] EURECOM, Dept Data Sci, Sophia Antipolis, France; [Hensman, James] PROWLER Io, Cambridge, England	Lancaster University; Linkoping University; IMT - Institut Mines-Telecom; EURECOM	Nemeth, C (corresponding author), Univ Lancaster, Dept Math & Stat, Lancaster, England.	c.nemeth@lancaster.ac.uk; fredrik.lindsten@liu.se; maurizio.filippone@eurecom.fr; james@prowler.io	Nemeth, Christopher/I-2881-2019	Nemeth, Christopher/0000-0002-9084-3866; Hensman, James/0000-0002-4989-3589	UK Engineering and Physical Sciences Research Council [EP/S00159X/1, EP/R01860X/1]; Swedish Research Council [2016-04278]; Swedish Foundation for Strategic Research [ICA16-0015]; Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; AXA Research Fund; Agence Nationale de la Recherche [ANR-18-CE46-0002]	UK Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Swedish Research Council(Swedish Research CouncilEuropean Commission); Swedish Foundation for Strategic Research(Swedish Foundation for Strategic Research); Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; AXA Research Fund(AXA Research Fund); Agence Nationale de la Recherche(French National Research Agency (ANR))	CN gratefully acknowledges the support of the UK Engineering and Physical Sciences Research Council grants EP/S00159X/1 and EP/R01860X/1. FL is financially supported by the Swedish Research Council (project 2016-04278), by the Swedish Foundation for Strategic Research (project ICA16-0015) and by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. MF gratefully acknowledges support from the AXA Research Fund and the Agence Nationale de la Recherche (grant ANR-18-CE46-0002).	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Andrieu C, 2009, ANN STAT, V37, P697, DOI 10.1214/07-AOS574; Beaumont MA, 2003, GENETICS, V164, P1139; Bishop C.M, 2006, PATTERN RECOGN; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Calderhead B, 2009, COMPUT STAT DATA AN, V53, P4028, DOI 10.1016/j.csda.2009.07.025; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Carvalho CM, 2010, BIOMETRIKA, V97, P465, DOI 10.1093/biomet/asq017; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Geyer C., 1991, COMP SCI STAT, P156, DOI DOI 10.1080/01621459.1995.10476590; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Graham Michelle, 2017, 2017 IEEE International Ultrasonics Symposium (IUS), DOI 10.1109/ULTSYM.2017.8091520; Griffin JE, 2013, BAYESIAN ANAL, V8, P691, DOI 10.1214/13-BA827; Hertz J., 1991, INTRO THEORY NEURAL, V1; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Jasra A, 2007, STAT COMPUT, V17, P263, DOI 10.1007/s11222-007-9028-9; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kou SC, 2006, ANN STAT, V34, P1581, DOI 10.1214/009053606000000515; MARINARI E, 1992, EUROPHYS LETT, V19, P451, DOI 10.1209/0295-5075/19/6/002; Mclachlan G., 2000, WILEY SER PROB STAT; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neal Radford M, 2012, BAYESIAN LEARNING NE, V118; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Piironen J, 2017, ELECTRON J STAT, V11, P5018, DOI 10.1214/17-EJS1337SI; Roberts GO, 1997, ANN APPL PROBAB, V7, P110, DOI 10.1214/aoap/1034625254; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x; Salakhutdinov R., 2010, P 27 INT C MACH LEAR, P943; STEWART GW, 1980, SIAM J NUMER ANAL, V17, P403, DOI 10.1137/0717034; Tak H., 2016, ARXIV160105633; Zhang Y, 2012, ADV NEURAL INFORM PR, V25, P3194	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304033
C	Nguyen, VA; Shafieezadeh-Abadeh, S; Yue, MC; Kuhn, D; Wiesemann, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nguyen, Viet Anh; Shafieezadeh-Abadeh, Soroosh; Yue, Man-Chung; Kuhn, Daniel; Wiesemann, Wolfram			Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BAYESIAN COMPUTATION; VARIATIONAL INFERENCE; SELECTION	The likelihood function is a fundamental component in Bayesian statistics. However, evaluating the likelihood of an observation is computationally intractable in many applications. In this paper, we propose a non-parametric approximation of the likelihood that identifies a probability measure which lies in the neighborhood of the nominal measure and that maximizes the probability of observing the given sample point. We show that when the neighborhood is constructed by the Kullback-Leibler divergence, by moment conditions or by the Wasserstein distance, then our optimistic likelihood can be determined through the solution of a convex optimization problem, and it admits an analytical expression in particular cases. We also show that the posterior inference problem with our optimistic likelihood approximation enjoys strong theoretical performance guarantees, and it performs competitively in a probabilistic classification task.	[Nguyen, Viet Anh; Shafieezadeh-Abadeh, Soroosh; Kuhn, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Yue, Man-Chung] Hong Kong Polytech Univ, Hong Kong, Peoples R China; [Wiesemann, Wolfram] Imperial Coll, Business Sch, London, England	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Hong Kong Polytechnic University; Imperial College London	Nguyen, VA (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	viet-anh.nguyen@epfl.ch; soroosh.shafiee@epfl.ch; manchung.yue@polyu.edu.hk; daniel.kuhn@epfl.ch; ww@imperial.ac.uk		Yue, Man-Chung/0000-0002-7992-9490	Swiss National Science Foundation [BSCGI0_157733]; EPSRC [EP/M028240/1, EP/M027856/1, EP/N020030/1]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We gratefully acknowledge financial support from the Swiss National Science Foundation under grant BSCGI0_157733 as well as the EPSRC grants EP/M028240/1, EP/M027856/1 and EP/N020030/1.	Baldi P., 2001, BIOINFORMATICS MACHI; Beaumont MA, 2002, GENETICS, V162, P2025; Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; Bertsimas D, 2005, SIAM J OPTIMIZ, V15, P780, DOI 10.1137/S1052623401399903; Bertsimas D., 2000, HDB SEMIDEFINITE PRO, DOI 10.1007/978-1-4615-4381-7_16; Bi J., 2005, ADV NEURAL INFORM PR, P161; Bishop C.M, 2006, PATTERN RECOGN; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Brochu E., 2010, ARXIV10122599; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cohen M. B., 2018, ARXIV181007896; Csillery K, 2010, TRENDS ECOL EVOL, V25, P410, DOI 10.1016/j.tree.2010.04.001; Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961; Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Gao S., 2016, P 30 INT C NEURAL IN, P487; Gorbach N. S., 2017, ADV NEURAL INFORM PR, P4809; Hanasusanto GA, 2017, OPER RES, V65, P751, DOI 10.1287/opre.2016.1583; Haynes MA, 1997, J STAT PLAN INFER, V65, P45, DOI 10.1016/S0378-3758(97)00050-5; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Jojic N, 2001, PROC CVPR IEEE, P199; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kuhn D., 2019, INFORMS TUTORIALS OP, P3; Liang P., 2007, EMPIRICAL METHODS NA; Likas AC, 2004, IEEE T SIGNAL PROCES, V52, P2222, DOI 10.1109/TSP.2004.831119; MARSHALL AW, 1960, ANN MATH STAT, V31, P1001, DOI 10.1214/aoms/1177705673; Mengersen KL, 2013, P NATL ACAD SCI USA, V110, P1321, DOI 10.1073/pnas.1208827110; Minka T.P., 2001, P 17 C UNC ART INT, P362; Mohamed S., 2015, P NEURIPS, P2125; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Nakajima S., 2012, ADV NEURAL INFORM PR, P971; Naseem Tahira, 2010, P 2010 C EMP METH NA; Nguyen V. A., 2018, ARXIV180507194; Nguyen V. A., 2019, ADV NEURAL INFORM PR; Park M, 2016, JMLR WORKSH CONF PRO, V51, P398; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Raj A, 2014, GENETICS, V197, P573, DOI 10.1534/genetics.114.164350; Sanguinetti G, 2006, BIOINFORMATICS, V22, P2775, DOI 10.1093/bioinformatics/btl473; Schervish M. J., 1995, THEORY STAT; Shafieezadeh-Abadeh S., 2019, J MACHINE LEARNING R, V20, P1; Sinha A., 2018, P INT C LEARN REPR; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Toni T, 2009, J R SOC INTERFACE, V6, P187, DOI 10.1098/rsif.2008.0172; Villani C., 2008, OPTIMAL TRANSPORT OL; Wiesemann W, 2014, OPER RES, V62, P1358, DOI 10.1287/opre.2014.1314; Woolrich MW, 2004, NEUROIMAGE, V21, P1732, DOI 10.1016/j.neuroimage.2003.12.023	50	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907052
C	Ortner, R; Pirotta, M; Fruit, R; Lazaric, A; Maillard, OA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ortner, Ronald; Pirotta, Matteo; Fruit, Ronan; Lazaric, Alessandro; Maillard, Odalric-Ambrym			Regret Bounds for Learning State Representations in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of online reinforcement learning when several state representations (mapping histories to a discrete state space) are available to the learning agent. At least one of these representations is assumed to induce a Markov decision process (MDP), and the performance of the agent is measured in terms of cumulative regret against the optimal policy giving the highest average reward in this MDP representation. We propose an algorithm (UCB-MS) with (O) over tilde(root T) regret in any communicating MDP The regret bound shows that UCB-MS automatically adapts to the Markov model and improves over the currently known best bound of order (O) over tilde (T-2/3).	[Ortner, Ronald] Univ Leoben, Leoben, Austria; [Pirotta, Matteo; Lazaric, Alessandro] Facebook AI Res, New York, NY USA; [Fruit, Ronan; Maillard, Odalric-Ambrym] INRIA Lille, Sequel Team, Lille, France	University of Leoben; Facebook Inc	Ortner, R (corresponding author), Univ Leoben, Leoben, Austria.	rortner@unileoben.ac.at; pirotta@fb.com; ronan.fruit@inria.fr; lazaric@fb.com; odalric.maillard@inria.fr		Ortner, Ronald/0000-0001-6033-2208	Austrian Science Fund (FWF) [I 3437-N33]; CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020; French Ministry of Higher Education and Research, Inria Lille - Nord Europe, CRIStAL; French Agence Nationale de la Recherche [ANR-16-CE40-0002]	Austrian Science Fund (FWF)(Austrian Science Fund (FWF)); CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020; French Ministry of Higher Education and Research, Inria Lille - Nord Europe, CRIStAL; French Agence Nationale de la Recherche(French National Research Agency (ANR))	This work has been supported by the Austrian Science Fund (FWF): I 3437-N33 in the framework of the CHIST-ERA ERA-NET (DELTA project). Odalric-Ambrym Maillard was supported by CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020, the French Ministry of Higher Education and Research, Inria Lille - Nord Europe, CRIStAL, and the French Agence Nationale de la Recherche, under grant ANR-16-CE40-0002 (project BADASS).	Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bartlett P. L., 2009, P 25 C UNC ART INT, P25; de Bruin T., 2018, IEEE ROBOT AUTOM LET, V3, P1394, DOI DOI 10.1109/LRA.2018.2800101; Fruit R., 2019, TUTORIAL ALT 19; Fruit Ronan, 2018, JMLR WORKSHOP C P, P1573; Fruit Ronan, 2018, P 32 INT C NEUR INF, P2998; Hutter M, 2009, J ARTIFICIAL GEN INT, V1, P3; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jonschkowski R., 2014, ROBOTICS SCI SYSTEMS; Lesort T., 2018, NEURAL NETWORKS; Maillard O.-A., 2012, ADV NEURAL PROCESSIN, V24, P2627; Maillard Odalric-Ambrym, 2013, P 30 INT C MACH LEAR, V28, P543; Nguyen Phuong, 2013, JMLR WS CP AISTATS, P463; Ortner R, 2014, LECT NOTES ARTIF INT, V8776, P140, DOI 10.1007/978-3-319-11662-4_11; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904039
C	Orvieto, A; Lucchi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Orvieto, Antonio; Lucchi, Aurelien			Continuous-time Models for Stochastic Optimization Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose new continuous-time formulations for first-order stochastic optimization algorithms such as mini-batch gradient descent and variance-reduced methods. We exploit these continuous-time models, together with simple Lyapunov analysis as well as tools from stochastic calculus, in order to derive convergence bounds for various types of non-convex functions. Guided by such analysis, we show that the same Lyapunov arguments hold in discrete-time, leading to matching rates. In addition, we use these models and Ito calculus to infer novel insights on the dynamics of SGD, proving that a decreasing learning rate acts as time warping or, equivalently, as landscape stretching.	[Orvieto, Antonio; Lucchi, Aurelien] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Orvieto, A (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	orvietoa@ethz.ch		Lucchi, Aurelien/0000-0001-7015-2710				Allen-Zhu Z, 2017, PR MACH LEARN RES, V70; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Arora Sanjeev, 2015, P MACH LEARN RES; Babanezhad Harikandeh R., 2015, ADV NEURAL INFORM PR, V28, P2251; Balles Lukas, 2016, ARXIV161205086; Benaim M, 1998, ERGOD THEOR DYN SYST, V18, P53, DOI 10.1017/S0143385798097557; Benaim M, 2008, PERFORM EVALUATION, V65, P823, DOI 10.1016/j.peva.2008.03.005; Betancourt M., 2018, ARXIV180203653; BLACK F, 1973, J POLIT ECON, V81, P637, DOI 10.1086/260062; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chen G, 2015, 6TH INTERNATIONAL SYMPOSIUM ON HIGH-TEMPERATURE METALLURGICAL PROCESSING, P739; Chen T.Q., 2018, ADV NEURAL INFORM PR; Ciccone Marco, 2018, ARXIV180407209; Daneshmand H, 2018, PR MACH LEARN RES, V80; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Durrett R., 2010, PROBABILITY THEORY E, V4th ed., DOI 10.1017/CBO9780511779398; Einstein A, 1905, ANN PHYS-BERLIN, V17, P549, DOI 10.1002/andp.19053220806; Feng Yuanyuan, 2019, ARXIV190200635; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Girsanov I. V., 1960, THEOR PROBAB APPL, V5, P285, DOI DOI 10.1137/1105027; Hardt M, 2018, J MACH LEARN RES, V19; He Li, 2018, ARXIV180502991; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Hu Wenqing, 2017, ARXIV170507562; Huang JW, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P598; Ikeda N., 1981, STOCHASTIC DIFFERENT; Ito K., 1944, P IMP ACAD, V20, P519, DOI [DOI 10.3792/PIA/1195572786, 10.3792/pia/1195572786]; Jastrzgbski Stanislaw, 2017, ARXIV171104623; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Krichene W., 2017, ADV NEURAL INFORM PR, V30, P6796; Kushner HJ., 2003, STOCHASTIC APPROXIMA; Lei LH, 2017, ADV NEUR IN, V30; Mandt S, 2016, PR MACH LEARN RES, V48; Mao X., 2007, STOCHASTIC DIFFERENT, V2nd ed.; Mertikopoulos P, 2018, SIAM J OPTIMIZ, V28, P163, DOI 10.1137/16M1105682; Moulines Eric, 2011, P NIPS, V24, P451; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y., 2018, APPL OPTIMIZATION; Oksendal B., 1990, J THEOR PROBAB, V3, P207, DOI DOI 10.1007/BF01045159; Oksendal B., 1998, STOCHASTIC DIFFERENT, DOI [DOI 10.1007/978-3-642-14394-6, 10.1007/978-3-642-14394-6]; Orvieto Antonio, 2019, SHADOWING PROPERTIES; Perko Lawrence, 2013, DIFFERENTIAL EQUATIO, V7; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi SJ, 2016, PR MACH LEARN RES, V48; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Shi B., 2019, ARXIV190203694; Simsekli U, 2019, PR MACH LEARN RES, V97; Stroock D. W., 1979, Multidimensional diffusion processes; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Wigner E. P., 1990, MATH SCI, P291, DOI DOI 10.1142/1104; Wilson A. C., 2016, ARXIV161102635; Xu, 2018, INT C MACH LEARN, P5488; Xu Pan, 2018, INT C ART INT STAT, P1087; Zhang H., 2013, ARXIV13034645; Zhang HX, 2016, SPRINGER THESES-RECO, P1, DOI 10.1007/978-3-662-48816-4_1; Zhang J., 2018, ARXIV180500521	68	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904028
C	Orvieto, A; Lucchi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Orvieto, Antonio; Lucchi, Aurelien			Shadowing Properties of Optimization Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIFFERENTIAL-EQUATIONS; BEHAVIOR; ERROR	Ordinary differential equation (ODE) models of gradient-based optimization methods can provide insights into the dynamics of learning and inspire the design of new algorithms. Unfortunately, this thought-provoking perspective is weakened by the fact that - in the worst case - the error between the algorithm steps and its ODE approximation grows exponentially with the number of iterations. In an attempt to encourage the use of continuous-time methods in optimization, we show that, if some additional regularity on the objective is assumed, the ODE representations of Gradient Descent and Heavy-ball do not suffer from the aforementioned problem, once we allow for a small perturbation on the algorithm initial condition. In the dynamical systems literature, this phenomenon is called shadowing. Our analysis relies on the concept of hyperbolicity, as well as on tools from numerical analysis.	[Orvieto, Antonio; Lucchi, Aurelien] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	ETH Zurich	Orvieto, A (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	orvietoa@ethz.ch		Lucchi, Aurelien/0000-0001-7015-2710				[Anonymous], 2013, ICML 3; [Anonymous], 1964, COMP MATH MATH PHYS+; Anosov D. V., 1967, P STEKLOV I MATH+, V90; Aradijo Vitor, 2009, HYPERBOLIC DYNAMICAL; Aulbach B, 1996, 6 LECT DYNAMICAL SYS; Betancourt M., 2018, ARXIV180203653; BEYN WJ, 1987, SIAM J NUMER ANAL, V24, P1095, DOI 10.1137/0724072; BOWEN R, 1975, J DIFFER EQUATIONS, V18, P333, DOI 10.1016/0022-0396(75)90065-0; Brin M., 2002, INTRO DYNAMICAL SYST; Cabot A, 2009, T AM MATH SOC, V361, P5983, DOI 10.1090/S0002-9947-09-04785-0; Carrillo JA, 2006, ARCH RATION MECH AN, V179, P217, DOI 10.1007/s00205-005-0386-1; Chen T.Q., 2018, ADV NEURAL INFORM PR; CHOW SN, 1994, SIAM J SCI COMPUT, V15, P959, DOI 10.1137/0915058; Ciccone Marco, 2018, ARXIV180407209; Coleman R., 2012, CALCULUS NORMED VECT; COOMES BA, 1995, NUMER MATH, V69, P401, DOI 10.1007/s002110050100; Daneshmand Hadi, 2018, ARXIV180305999; Du S.S., 2017, ADV NEURAL INFORM PR, P1067; Feng Yuanyuan, 2019, ARXIV190200635; Gavurin M.K., 1958, IZV U MATH, P18; Ghadimi E, 2015, 2015 EUROPEAN CONTROL CONFERENCE (ECC), P310, DOI 10.1109/ECC.2015.7330562; Hairer E., 2003, Acta Numerica, V12, P399, DOI 10.1017/S0962492902000144; Hayes W, 2005, APPL NUMER MATH, V53, P299, DOI 10.1016/j.apnum.2004.08.011; Irwin M.C, 2001, SMOOTH DYNAMICAL SYS, V17; Jung A., 2017, FRONT APPL MATH STAT, V3, P18, DOI [10.3389/fams.2017.00018, DOI 10.3389/FAMS.2017.00018]; Khalil H. K., 2002, NONLINEAR SYSTEMS, V3; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Krichene W., 2017, ADV NEURAL INFORM PR, P6796; Kulakova Anastasiya, 2018, ARXIV181100658; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Lanford O. E.  III, 1985, Regular and Chaotic Motions in Dynamic Systems. Proceedings of the Fifth International School of Mathematical Physics and NATO Advanced Study Institute, P73; LARSSON S, 1994, SIAM J NUMER ANAL, V31, P1000, DOI 10.1137/0731053; Lee J. D., 2016, ARXIV160204915; Levy K. Y., 2016, ARXIV161104831; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; LUBICH C, 1995, SIAM J NUMER ANAL, V32, P1296, DOI 10.1137/0732060; Maddison Chris J, 2018, ARXIV180905042; Nesterov Y., 2018, SPRINGER OPTIM APPL; Ombach J., 1993, ANN POL MATH, V58, P253; Orvieto Antonio, 2018, ARXIV181002565; Palmer Kenneth James, 2013, SHADOWING DYNAMICAL, V501; Perko L, 2013, DIFFERENTIAL EQUATIO, V7; SAUER T, 1991, NONLINEARITY, V4, P961, DOI 10.1088/0951-7715/4/3/018; SHI B, 2018, ARXIV181008907; Shi B., 2019, ARXIV190203694; SMALE S, 1967, B AM MATH SOC, V73, P747, DOI 10.1090/S0002-9904-1967-11798-1; Su WJ, 2016, J MACH LEARN RES, V17; VANVLECK ES, 1995, SIAM J SCI COMPUT, V16, P1177, DOI 10.1137/0916068; Wilson A. C., 2016, ARXIV161102635; Xu, 2018, INT C MACH LEARN, P5488; Xu Pan, 2018, INT C ART INT STAT, P1087; Zhang J., 2018, ARXIV180500521	55	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904035
C	Osama, M; Zachariah, D; Stoica, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Osama, Muhammad; Zachariah, Dave; Stoica, Peter			Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS	A spatial point process can be characterized by an intensity function which predicts the number of events that occur across space. In this paper, we develop a method to infer predictive intensity intervals by learning a spatial model using a regularized criterion. We prove that the proposed method exhibits out-of-sample prediction performance guarantees which, unlike standard estimators, are valid even when the spatial model is misspecified. The method is demonstrated using synthetic as well as real spatial data.	[Osama, Muhammad; Zachariah, Dave; Stoica, Peter] Uppsala Univ, Dept Informat Technol, Div Syst & Control, Uppsala, Sweden	Uppsala University	Osama, M (corresponding author), Uppsala Univ, Dept Informat Technol, Div Syst & Control, Uppsala, Sweden.	muhammad.osama@it.uu.se; dave.zachariah@it.uu.se; peter.stoica@it.uu.se			Swedish Research Council [2017 -04610, 2018 -05040]	Swedish Research Council(Swedish Research CouncilEuropean Commission)	The work was supported by the Swedish Research Council (contract numbers 2017 -04610 and 2018 -05040).	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043; Cressie N., 2015, STAT SPATIO TEMPORAL; Devroye L., 1986, 1986 Winter Simulation Conference Proceedings, P260, DOI 10.1145/318242.318443; DIGGLE P, 1985, J R STAT SOC C-APPL, V34, P138; Diggle P., 2014, STAT ANAL SPATIAL SP; Diggle P.J., 2013, STAT ANAL SPATIAL SP, V0, DOI [10.1201/b15326, DOI 10.1201/B15326]; Diggle PJ, 2013, STAT SCI, V28, P542, DOI 10.1214/13-STS441; Flaxman S., 2018, ARXIV180102858; Illian JB, 2012, ANN APPL STAT, V6, P1499, DOI 10.1214/11-AOAS530; John S., 2018, LARGE SCALE COX PROC; Johnson O. O., 2019, ARXIV190109551; Lei J, 2018, J AM STAT ASSOC, V113, P1094, DOI 10.1080/01621459.2017.1307116; Lloyd C, 2015, PR MACH LEARN RES, V37, P1814; NationalInstituteofJustice, REAL TIM CRIM FOR CH; Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x; Tibshirani R., 2015, STAT LEARNING SPARSI; Vovk V., 2005, ALGORITHMIC LEARNING, DOI DOI 10.1007/B106715; Walder C. J., 2017, P MACHINE LEARNING R, P3579; Williams Christopher KI, 2006, GAUSSIAN PROCESSES M, V2	22	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903054
C	Ozay, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ozay, Mete			Fine-grained Optimization of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EIGENVALUE	In recent studies, several asymptotic upper bounds on generalization errors on deep neural networks (DNNs) are theoretically derived. These bounds are functions of several norms of weights of the DNNs, such as the Frobenius and spectral norms, and they are computed for weights grouped according to either input and output channels of the DNNs. In this work, we conjecture that if we can impose multiple constraints on weights of DNNs to upper bound the norms of the weights, and train the DNNs with these weights, then we can attain empirical generalization errors closer to the derived theoretical bounds, and improve accuracy of the DNNs. To this end, we pose two problems. First, we aim to obtain weights whose different norms are all upper bounded by a constant number. To achieve these bounds, we propose a two-stage renormalization procedure; (i) normalization of weights according to different norms used in the bounds, and (ii) reparameterization of the normalized weights to set a constant and finite upper bound of their norms. In the second problem, we consider training DNNs with these renormalized weights. To this end, we first propose a strategy to construct joint spaces (manifolds) of weights according to different constraints in DNNs. Next, we propose a fine-grained SGD algorithm (FG-SGD) for optimization on the weight manifolds to train DNNs with assurance of convergence to minima. Experimental analyses show that image classification accuracy of baseline DNNs can be boosted using FG-SGD on collections of manifolds identified by multiple constraints.				meteozay@gmail.com	Ozay, Mete/L-4869-2013					Absil PA, 2012, SIAM J OPTIMIZ, V22, P135, DOI 10.1137/100802529; Arora S, 2018, PR MACH LEARN RES, V80; BAI ZD, 1988, J MULTIVARIATE ANAL, V26, P166, DOI 10.1016/0047-259X(88)90078-4; BAI ZD, 1993, ANN PROBAB, V21, P1275, DOI 10.1214/aop/1176989118; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Brutzkus A, 2017, PR MACH LEARN RES, V70; Chen Haoran, 2016, ABS160906434 CORR; Cho M., 2017, ADV NEURAL INFORM PR; Cho M., 2017, NIPS, P5231; Du Simon S., 2018, P 35 INT C MACH LEAR; Ge Rong, 2017, P 5 INT C LEARN REP; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Golowich N., 2018, PROC C LEARN THEORY, P297; Hardt Moritz, 2017, P 5 INT C LEARN REP; Hu J., 2018, P IEEE C COMP VIS PA; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Khalid O, 2009, THIRD INTERNATIONAL WORKSHOP ON VIRTUALIZATION TECHNOLOGIES IN DISTRIBUTED COMPUTING (VTDC-09), P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lezcano-Casado Mario, 2019, ADV NEURAL INFORM PR; Lezcano-Casado Mario, 2019, P 39 INT C MACH LEAR; Lui YM, 2012, J MACH LEARN RES, V13, P3297; Luo JC, 2020, IEEE T SYST MAN CY-S, V50, P3658, DOI 10.1109/TSMC.2018.2855685; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Neyshabur Behnam, 2018, P 6 INT C LEARN REP; Ozay Mete, 2018, AAAI C ART INT; Nguyen Q, 2017, PR MACH LEARN RES, V70; Raghu M, 2017, PR MACH LEARN RES, V70; Salimans T, 2016, ADV NEURAL INFORM PR; Santurkar Shibani, 2018, ADV NEURAL INFORM PR; Suzuki T, 2018, PMLR, P1397; Xie B, 2017, PR MACH LEARN RES, V54, P1216; Xie Guotian, 2018, DECOUPLED CONVOLUTIO; Yun Chulhee, 2018, ABS180203487 CORR; Yun Chulhee, 2018, P 6 INT C LEARN REP; Zhang Chiyuan, 2017, P 5 INT C LEARN REP; Zhou P, 2018, PR MACH LEARN RES, V80	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301044
C	Panageas, I; Piliouras, G; Wang, X		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Panageas, Ioannis; Piliouras, Georgios; Wang, Xiao			First-order methods almost always avoid saddle points: The case of vanishing step-sizes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In a series of papers [17, 22, 16], it was established that some of the most commonly used first order methods almost surely (under random initializations) and with step-size being small enough, avoid strict saddle points, as long as the objective function f is C-2 and has Lipschitz gradient. The key observation was that first order methods can be studied from a dynamical systems perspective, in which instantiations of Center-Stable manifold theorem allow for a global analysis. The results of the aforementioned papers were limited to the case where the step-size alpha is constant, i.e., does not depend on time (and bounded from the inverse of the Lipschitz constant of the gradient of f). It remains an open question whether or not the results still hold when the step-size is time dependent and vanishes with time. In this paper, we resolve this question on the affirmative for gradient descent, mirror descent, manifold descent and proximal point. The main technical challenge is that the induced (from each first order method) dynamical system is time non-homogeneous and the stable manifold theorem is not applicable in its classic form. By exploiting the dynamical systems structure of the aforementioned first order methods, we are able to prove a stable manifold theorem that is applicable to time non-homogeneous dynamical systems and generalize the results in [16] for vanishing step-sizes.	[Panageas, Ioannis; Piliouras, Georgios; Wang, Xiao] SUTD, Singapore, Singapore	Singapore University of Technology & Design	Panageas, I (corresponding author), SUTD, Singapore, Singapore.	ioannis@sutd.edu.sg; georgios@sutd.edu.sg; xiao_wang@sutd.edu.sg			SRG [ISTD 2018 136]; NRF fellowship; MOE AcRF Tier 2 Grant [2016-T2-1-170, PIE-SGP-AI-2018-01]; NRF 2018 Fellowship [NRF-NRFF2018-07]	SRG; NRF fellowship; MOE AcRF Tier 2 Grant; NRF 2018 Fellowship	Ioannis Panageas acknowledges SRG ISTD 2018 136 and NRF fellowship for AI. Georgios Piliouras and Xiao Wang acknowledge MOE AcRF Tier 2 Grant 2016-T2-1-170, grant PIE-SGP-AI-2018-01 and NRF 2018 Fellowship NRF-NRFF2018-07. We thank Tony Roberts for pointers to the literature of stability of non-autonomous dynamical systems.	[Anonymous], 2016, ADV NEURAL INFORM PR; Arora S., 2012, THEORY COMPUTING; Barreira L, 2008, LECT NOTES MATH, V1926, pVII; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Bubeck S6bastien, 2014, CORR; Chatziafratis Vaggos, 2019, DEPTH WIDTH TRADE OF; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Chotibut Thiparat, 2018, FAMILY CHAOTIC UNPUB; Chotibut Thiparat, 2019, ROUTE CHAOS ROUTING; Daskalakis C., 2018, P 32 INT C NEURAL IN, V31, P9256; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Ge R., 2017, ARXIV170400708; Jordan M. I., 2018, DYNAMICAL SYMPLECTIC; Kim SJ, 2016, PORTL INT CONF MANAG, P2460, DOI 10.1109/PICMET.2016.7806794; Kleinberg Robert, 2009, ACM S THEOR COMP STO; Lee J. D., 2019, MATH PROGRAMMING; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Mehta R, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P73, DOI 10.1145/2688073.2688118; Nagarajan Sai Ganesh, 2019, CORR; Nar Kamil, 2018, ADV NEURAL INFORM PR, P3440; Palaiopanos G., 2017, NIPS; Panageas I., 2017, P 8 INN THEOR COMP S, V67; Panageas Ioannis, 2019, P INT C MACH LEARN, P4961; Pascanu R., 2014, ARXIV14054604; PEMANTLE R, 1990, ANN PROBAB, V18, P698, DOI 10.1214/aop/1176990853; Potzsche C, 2009, NUMER MATH, V112, P449, DOI 10.1007/s00211-009-0215-9; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306047
C	Patel, KK; Dieuleveut, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Patel, Kumar Kshitij; Dieuleveut, Aymeric			Communication trade-offs for Local-SGD with large step size	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC-APPROXIMATION	Synchronous mini-batch SGD is state-of-the-art for large-scale distributed machine learning. However, in practice, its convergence is bottlenecked by slow communication rounds between worker nodes. A natural solution to reduce communication is to use the "local-SGD" model in which the workers train their model independently and synchronize every once in a while. This algorithm improves the computation-communication trade-off but its convergence is not understood very well. We propose a non-asymptotic error analysis, which enables comparison to one-shot averaging i.e., a single communication round among independent workers, and mini-batch averagingi.e., communicating at every step. We also provide adaptive lower bounds on the communication frequency for large step-sizes (t(-alpha), alpha is an element of(1/2, 1)) and show that local-SGD reduces communication by a factor of O(root T/P-3/2), with T the total number of gradients and P machines.	[Patel, Kumar Kshitij; Dieuleveut, Aymeric] Ecole Polytech Fed Lausanne, MLO, Lausanne, Switzerland; [Patel, Kumar Kshitij] TTIC Toyota Technol Inst Chicago, Chicago, IL 60637 USA; [Dieuleveut, Aymeric] Ecole Polytech, CMAP, Palaiseau, France	Ecole Polytechnique Federale de Lausanne; Institut Polytechnique de Paris	Patel, KK (corresponding author), Ecole Polytech Fed Lausanne, MLO, Lausanne, Switzerland.; Patel, KK (corresponding author), TTIC Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	kkpatel@ttic.edu; aymeric.dieuleveut@polytechnique.edu						[Anonymous], 2012, ICML; BACH F., 2013, ADV NEURAL INFORM PR; BACH F., 2011, ADV NEURAL INFORM PR, P451; Bach F, 2014, J MACH LEARN RES, V15, P595; DEFOSSEZ A., 2015, P INT C ART INT STAT; Dekel O, 2012, J MACH LEARN RES, V13, P165; Delalleau O., 2007, PARALLEL STOCHASTIC; Dieuleveut A., 2018, ANN STAT; Dieuleveut A., 2016, J MACHINE LEARNING R; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Duchi J. C., 2015, ARXIV E PRINTS; FABIAN V, 1968, ANN MATH STAT, V39, P1327, DOI 10.1214/aoms/1177698258; Gadat S., 2017, ARXIV E PRINTS; Godichon A. B., 2017, ARXIV E PRINTS; Goyal Priya, 2017, ARXIV E PRINTS; Jain P., 2016, ARXIV E PRINTS; Jain P., 2017, ARXIV170408227; Kamp Michael, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P623, DOI 10.1007/978-3-662-44848-9_40; Keskar N. Shirish, 2016, ARXIV E PRINTS; Kyrola A., 2017, ABS170602677 ARXIV; Lacoste-Julien S., 2012, 12122002 ARXIV; Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612; Li Q, 2015, INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND COMMUNICATION ENGINEERING (CSCE 2015), P1; Lin T., 2018, ARXIV E PRINTS; MCDONALD R, 2010, HUMAN LANGUAGE TECHN, V2010, P456; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2008, AUTOMATICA, V44, P1559, DOI 10.1016/j.automatica.2008.01.017; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Rakhlin A., 2011, ARXIV E PRINTS; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rosenblatt JD, 2016, INF INFERENCE, V5, P379, DOI 10.1093/imaiai/iaw013; Ruppert D., 1988, TECHNICAL REPORT; SHALEV- SHWARTZ S., 2009, P INT C LEARN THEOR; Shamir O., 2013, P 30 INT C MACH LEAR; Stich S. U., 2019, ICLR 2019; TakA6 M., 2013, P 30 INT C INT C MAC, V28, P111; Tsybakov A. B., 2003, P ANN C COMP LEARN T; You Y., 2017, ARXIV E PRINTS; Yu H., 2018, ARXIV E PRINTS; Zhang Hantian, 2016, ARXIV161105402; Zhang J., 2016, ARXIV E PRINTS; Zhang T., 2004, P C MACH LEARN ICML; Zhang Y., 2012, ADV NEURAL INFORM PR, P1502; Zhu DL, 1996, SIAM J OPTIMIZ, V6, P714, DOI 10.1137/S1052623494250415; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	48	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905027
C	Pathak, D; Lu, C; Darrell, T; Isola, P; Efros, AA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pathak, Deepak; Lu, Chris; Darrell, Trevor; Isola, Phillip; Efros, Alexei A.			Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Contemporary sensorimotor learning approaches typically start with an existing complex agent (e.g., a robotic arm), which they learn to control. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to dynamically self-assemble into composite bodies while also learning to coordinate their behavior to control these bodies. Each primitive agent consists of a limb with a motor attached at one end. Limbs may choose to link up to form collectives. When a limb initiates a link-up action, and there is another limb nearby, the latter is magnetically connected to the 'parent' limb's motor. This forms a new single agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. We evaluate the performance of these dynamic and modular agents in simulated environments. We demonstrate better generalization to test-time changes both in the environment, as well as in the structure of the agent, compared to static and monolithic baselines. Project video and code are available at https://pathak22.github.io/modular-assemblies/.	[Pathak, Deepak; Lu, Chris; Darrell, Trevor; Efros, Alexei A.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Isola, Phillip] MIT, Cambridge, MA 02139 USA	University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT)	Efros, AA (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.				Berkeley DeepDrive; Valrhona reinforcement learning fellowship; Facebook graduate fellowship	Berkeley DeepDrive; Valrhona reinforcement learning fellowship; Facebook graduate fellowship(Facebook Inc)	We would like to thank Igor Mordatch, Chris Atkeson, Abhinav Gupta, and the members of BAIR for fruitful discussions and comments. This work was supported in part by Berkeley DeepDrive and the Valrhona reinforcement learning fellowship. DP is supported by the Facebook graduate fellowship.	Andreas J., 2016, CVPR, P9; Battaglia P, 2018, ARXIV180601242; Battaglia P. W., 2018, ARXIV180601261, P9; Brockman Greg, 2016, arXiv; Daudelin J, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aat4983; De Lasa M., 2010, ACM T GRAPHICS TOG; Gilmer Justin, 2017, ARXIV170401212; Gilpin K., 2008, IJRR; Ha D., 2018, ARXIV181003779; Huang D. A., 2018, ARXIV180703480; John D., 1994, TELECOMMUNICATIONS E, V3; Juliani A., 2018, ARXIV180902627; Kipf TN, 2016, P INT C LEARN REPR; Mark Yim, 2007, IEEE ROBOTICS AUTOMA; MURATA S, 2007, IEEE ROBOTICS AUTOMA; Murphy K. P., 1999, P 15 C UNC ART INT; Romanishin J. W., 2013, IROS; Scarselli F., 2009, IEEE T NEURAL NETWOR; Schaff C., 2018, ARXIV180101432; Schulman J., 2017, ABS170706347 CORR; Sims K., 1994, COMPUTER GRAPHICS IN; Stoy K, 2010, SELF RECONFIGURABLE; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tu X., 1994, P 21 ANN C COMP GRAP; Von Neumann J., 1966, IEEE T NEURAL NETWOR; Wampler K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531366; Wang R., 2019, PAIRED OPEN ENDED TR; Wang Tingwu, 2018, ICLR; Wooldridge, 2009, INTRO MULTIAGENT SYS; Wright J, 2007, HIST SOC ISLAM WORLD, P9; Yang Z., 2018, NEURIPS; Yim M., 2000, ICRA; Zoph B., 2016, ICLR	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302031
C	Peng, BH; Chen, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Peng, Binghui; Chen, Wei			Adaptive Influence Maximization with Myopic Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	We study the adaptive influence maximization problem with myopic feedback under the independent cascade model: one sequentially selects k nodes as seeds one by one from a social network, and each selected seed returns the immediate neighbors it activates as the feedback available for later selections, and the goal is to maximize the expected number of total activated nodes, referred as the influence spread. We show that the adaptivity gap, the ratio between the optimal adaptive influence spread and the optimal non-adaptive influence spread, is at most 4 and at least e/(e - 1), and the approximation ratios with respect to the optimal adaptive influence spread of both the non-adaptive greedy and adaptive greedy algorithms are at least 1/4 (1 - 1/e) and at most e(2) +1/(e+1)(2) < 1 - 1/e. Moreover, the approximation ratio of the non-adaptive greedy algorithm is no worse than that of the adaptive greedy algorithm, when considering all graphs. Our result confirms a long-standing open conjecture of Golovin and Krause (2011) on the constant approximation ratio of adaptive greedy with myopic feedback, and it also suggests that adaptive greedy may not bring much benefit under myopic feedback.	[Peng, Binghui] Columbia Univ, New York, NY 10027 USA; [Chen, Wei] Microsoft Res, Redmond, WA USA	Columbia University; Microsoft	Peng, BH (corresponding author), Columbia Univ, New York, NY 10027 USA.	bp2601@columbia.edu; weic@microsoft.com		Chen, Wei/0000-0003-0065-3610	National Natural Science Foundation of China [61433014]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	Wei Chen is partially supported by the National Natural Science Foundation of China (Grant No. 61433014).	Asadpour A, 2016, MANAGE SCI, V62, P2374, DOI 10.1287/mnsc.2015.2254; Asadpour A, 2008, LECT NOTES COMPUT SC, V5385, P477, DOI 10.1007/978-3-540-92185-1_53; Badanidiyuru A, 2016, P ANN ACM SIAM S DIS, P414; Bradac Domagoj, 2019, P 23 INT C RAND COMP; Chen W., 2013, INFORM INFLUENCE PRO; Chen Wei, 2018, P 30 INT S ALG COMP; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gupta A., 2016, SODA 16, P1731, DOI [10.1137/1.9781611974331.ch120, DOI 10.1137/1.9781611974331.CH120]; Gupta A, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1688; Han K, 2018, PROC VLDB ENDOW, V11, P1029, DOI 10.14778/3213880.3213883; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Li YC, 2018, IEEE T KNOWL DATA EN, V30, P1852, DOI 10.1109/TKDE.2018.2807843; Salha G, 2018, 2018 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P455; Seeman L, 2013, ANN IEEE SYMP FOUND, P459, DOI 10.1109/FOCS.2013.56; Singer Y, 2016, ACM SIGECOM EXCH, V15, P32, DOI 10.1145/2994501.2994503; Sun LC, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2249, DOI 10.1145/3219819.3220101; Tang J, 2019, INT CONF MANAGE DATA, P1096, DOI 10.1145/3299869.3319881; Tong GM, 2017, IEEE ACM T NETWORK, V25, P112, DOI 10.1109/TNET.2016.2563397; Wang R., 2019, ARXIV190200192; Yuan Jing, 2017, 26 INT JOINT C ART I	20	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305055
C	Peysakhovich, A; Kroer, C; Lerer, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Peysakhovich, Alexander; Kroer, Christian; Lerer, Adam			Robust Multi-agent Counterfactual Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PLAY; MARKET; MODEL	We consider the problem of using logged data to make predictions about what would happen if we changed the 'rules of the game' in a multi-agent system. This task is difficult because in many cases we observe actions individuals take but not their private information or their full reward functions. In addition, agents are strategic, so when the rules change, they will also change their actions. Existing methods (e.g. structural estimation, inverse reinforcement learning) assume that agents' behavior comes from optimizing some utility or that the system is in equilibrium. They make counterfactual predictions by using observed actions to learn the underlying utility function (a.k.a. type) and then solving for the equilibrium of the counterfactual environment. This approach imposes heavy assumptions such as the rationality of the agents being observed and a correct model of the environment and agents' utility functions. We propose a method for analyzing the sensitivity of counterfactual conclusions to violations of these assumptions, which we call robust multi-agent counterfactual prediction (RMAC). We provide a first-order method for computing RMAC bounds. We apply RMAC to classic environments in market design: auctions, school choice, and social choice.	[Peysakhovich, Alexander; Lerer, Adam] Facebook AI Res, New York, NY 10003 USA; [Kroer, Christian] Facebook Core Data Sci, New York, NY USA	Facebook Inc; Facebook Inc	Peysakhovich, A (corresponding author), Facebook AI Res, New York, NY 10003 USA.							Abdulkadiroglu A, 2005, AM ECON REV, V95, P368, DOI 10.1257/000282805774669637; Abdulkadiroglu A, 1998, ECONOMETRICA, V66, P689, DOI 10.2307/2998580; Abdulkadiroglu A, 2011, AM ECON REV, V101, P1; Agarwal N, 2015, AM ECON REV, V105, P1939, DOI 10.1257/aer.20131006; Athey S, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P5, DOI 10.1145/2783258.2785466; Athey Susan, 2010, 6 AD AUCT WORKSH, V15; Bergemann D, 2005, ECONOMETRICA, V73, P1771, DOI 10.1111/j.1468-0262.2005.00638.x; BERRY S, 1995, ECONOMETRICA, V63, P841, DOI 10.2307/2171802; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Brown N., 2018, ARXIV181100164; Budish E, 2011, J POLIT ECON, V119, P1061, DOI 10.1086/664613; Camerer Colin F, 2011, ADV BEHAV EC; Caragiannis I, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P305, DOI 10.1145/2940716.2940726; Chawla Shuchi, 2017, ARXIV170804699; Conitzer V, 2007, MACH LEARN, V67, P23, DOI 10.1007/s10994-006-0143-1; Conitzer Vincent, 2002, P 18 ANN C UNC ART I, P103; Dekel E, 2004, GAME ECON BEHAV, V46, P282, DOI 10.1016/S0899-8256(03)00121-0; Dutting P., 2017, ARXIV170603459; Erev I, 1998, AM ECON REV, V88, P848; Feng Z, 2018, AAMAS; Fudenberg Drew, 2016, ACM Transactions on Economics and Computation, V4, DOI 10.1145/2956581; Fudenberg D., 1998, THEORY LEARNING GAME, V2; Guruswami V, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1164; Haile PA, 2003, J POLIT ECON, V111, P1, DOI 10.1086/344801; Hartline Jason, 2015, ADV NEURAL INFORM PR, V28, P3061; Klemperer P, 2002, J ECON PERSPECT, V16, P169, DOI 10.1257/0895330027166; Kroer C., 2015, P ACM C EC COMP EC, P817; Lowe R., 2017, P INT C NEUR INF PRO, P6379; Manski C., 2003, PARTIAL IDENTIFICATI; Mennle Timo, 2015, TRADE OFFS SCH CHOIC; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Nekipelov Denis, 2015, P 16 ACM C EC COMPUT, P1, DOI [10.1145/2764468.2764522, DOI 10.1145/2764468.2764522]; Porter D, 2003, P NATL ACAD SCI USA, V100, P11153, DOI 10.1073/pnas.1633736100; Roth A.E., 1992, HDB GAME THEORY EC A, V1, P485; Roth AE, 2005, AM ECON REV, V95, P376, DOI 10.1257/000282805774669989; Roth AE, 2002, ECONOMETRICA, V70, P1341, DOI 10.1111/1468-0262.00335; Roth AE, 1999, AM ECON REV, V89, P748, DOI 10.1257/aer.89.4.748; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303011
C	Phan, M; Abbasi-Yadkori, Y; Domke, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Phan, My; Abbasi-Yadkori, Yasin; Domke, Justin			Thompson Sampling with Approximate Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the effects of approximate inference on the performance of Thompson sampling in the k-armed bandit problems. Thompson sampling is a successful algorithm for online decision-making but requires posterior inference, which often must be approximated in practice. We show that even small constant inference error (in alpha-divergence) can lead to poor performance (linear regret) due to underexploration (for alpha < 1) or over-exploration (for alpha > 0) by the approximation. While for alpha > 0 this is unavoidable, for alpha <= 0 the regret can be improved by adding a small amount of forced exploration even when the inference error is a large constant.	[Phan, My; Domke, Justin] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Abbasi-Yadkori, Yasin] VinAI, Hanoi, Vietnam	University of Massachusetts System; University of Massachusetts Amherst	Phan, M (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	myphan@cs.umass.edu; yasin.abbasi@gmail.com; domke@cs.umass.edu						Agrawal S., 2013, ARTIF INTELL, P99; Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Carpenter B., 2017, J STAT SOFTWARE, V76; Doucet A., 2009, HDB NONLINEAR FILTER, V12, P656; Gopalan A, 2014, PR MACH LEARN RES, V32; Kawale J., 2015, ADV NEURAL INFORM PR, V28, P1297; Kveton B, 2019, PR MACH LEARN RES, V97; Liu CY, 2016, LECT NOTES ARTIF INT, V9925, P321, DOI 10.1007/978-3-319-46379-7_22; Lu X., 2017, ADV NEURAL INFORM PR, V30, P3260; Minka T., 2018, INFERNET 03; Minka T., 2005, DIVERGENCE MEASURES; Ordentlich E, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P29; Osband I, 2016, PR MACH LEARN RES, V48; Qin C, 2017, ADV NEURAL INFORM PR, V30, P5381; Riquelme C., 2018, INT C LEARN REPR ICL; Russo D., 2016, C LEARNING THEORY, P1417; Russo D, 2016, J MACH LEARN RES, V17; Russo DJ, 2018, FOUND TRENDS MACH LE, V11, P1, DOI 10.1561/2200000070; Salvatier J, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.55; Tran Dustin, 2016, ARXIV161009787; Urteaga I., 2018, AI STAT, P698	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900040
C	Nguyen, PH; Nguyen, LM; van Dijk, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Phuong Ha Nguyen; Nguyen, Lam M.; van Dijk, Marten			Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the convergence of Stochastic Gradient Descent (SGD) for strongly convex objective functions. We prove for all t a lower bound on the expected convergence rate after the t-th SGD iteration; the lower bound is over all possible sequences of diminishing step sizes. It implies that recently proposed sequences of step sizes at ICML 2018 and ICML 2019 are universally close to optimal in that the expected convergence rate after each iteration is within a factor 32 of our lower bound. This factor is independent of dimension d. We offer a framework for comparing with lower bounds in state-of-the-art literature and when applied to SGD for strongly convex objective functions our lower bound is a significant factor 775 center dot d larger compared to existing work.	[Phuong Ha Nguyen; van Dijk, Marten] Univ Connecticut, Elect & Comp Engn, Storrs, CT 06269 USA; [Nguyen, Lam M.] Thomas J Watson Res Ctr, IBM Res, Yorktown Hts, NY USA	University of Connecticut; International Business Machines (IBM)	Nguyen, PH (corresponding author), Univ Connecticut, Elect & Comp Engn, Storrs, CT 06269 USA.	phuongha.ntu@gmail.com; LamNguyen.MLTD@ibm.com; marten.van_dijk@uconn.edu	van Dijk, Marten/ABC-2807-2020	van Dijk, Marten/0000-0001-9388-8050	AFOSR MURI [FA9550-14-1-0351]	AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI)	We thank the reviewers for useful suggestions to improve the paper. Phuong Ha Nguyen and Marten van Dijk were supported in part by AFOSR MURI under award number FA9550-14-1-0351.	Agarwal Alekh, 2010, INFORM THEORETIC LOW; [Anonymous], 2013, ICML 3; BACH F., 2011, ADV NEURAL INFORM PR, P451; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bottou L., 2016, ARXIV160604838; Boyd S, 2004, CONVEX OPTIMIZATION; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gower Robert Mansel, 2019, ARXIV190109401; Hastie T., 2013, ELEMENTS STAT LEARNI, DOI DOI 10.1007/978-0-387-84858-7_16; HOGAN E, 2013, NIPS, P315; Kingma D.P, P 3 INT C LEARNING R; Le Roux Nicolas, 2013, ARXIV13086370; Leblond Remi, 2018, ARXIV180103749; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 2004, INTRO LECT CONVEX OP, P15; Nguyen L. M., 2018, ARXIV181112403; Nguyen Lam, 2018, ICML; Nguyen Lam M., 2017, ICML; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Zou Fangyu, 2018, ARXIV181109358	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303063
C	Planche, B; Rong, XJ; Wu, ZY; Karanam, S; Kosch, H; Tian, YL; Ernst, J; Hutter, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Planche, Benjamin; Rong, Xuejian; Wu, Ziyan; Karanam, Srikrishna; Kosch, Harald; Tian, YingLi; Ernst, Jan; Hutter, Andreas			Incremental Scene Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a method to incrementally generate complete 2D or 3D scenes with the following properties: (a) it is globally consistent at each step according to a learned scene prior, (b) real observations of a scene can be incorporated while observing global consistency, (c) unobserved regions can be hallucinated locally in consistence with previous observations, hallucinations and global priors, and (d) hallucinations are statistical in nature, i.e., different scenes can be generated from the same observations. To achieve this, we model the virtual scene, where an active agent at each step can either perceive an observed part of the scene or generate a local hallucination. The latter can be interpreted as the agent's expectation at this step through the scene and can be applied to autonomous navigation. In the limit of observing real data at each point, our method converges to solving the SLAM problem. It can otherwise sample entirely imagined scenes from prior distributions. Besides autonomous agents, applications include problems where large data is required for building robust real-world applications, but few samples are available. We demonstrate efficacy on various 2D as well as 3D data. [GRAPHICS] .	[Planche, Benjamin; Hutter, Andreas] Siemens Corp Technol, Munich, Germany; [Planche, Benjamin; Kosch, Harald] Univ Passau, Passau, Germany; [Rong, Xuejian; Tian, YingLi] CUNY City Coll, New York, NY 10031 USA; [Rong, Xuejian; Wu, Ziyan; Karanam, Srikrishna; Ernst, Jan] Siemens Corp Technol, Princeton, NJ USA	Siemens AG; Siemens Germany; University of Passau; City University of New York (CUNY) System; City College of New York (CUNY); Siemens AG	Planche, B (corresponding author), Siemens Corp Technol, Munich, Germany.; Planche, B (corresponding author), Univ Passau, Passau, Germany.	benjamin.planche@siemens.com; xrong@ccny.cuny.edu; ziyanwu@siemens.com; srikrishna.karanam@siemens.com; harald.kosch@uni-passau.de; ytian@ccny.cuny.edu; jan.ernst@siemens.com; andreas.hutter@siemens.com	Planche, Benjamin/AAW-4073-2021	Planche, Benjamin/0000-0002-6110-6437				Ammirato P., 2017, ICRA; [Anonymous], ARXIV171111017; Chaplot Devendra Singh, 2018, ICLR; Choudhary Siddharth, 2015, ICRA; Durrant-Whyte Hugh, 2006, IEEE ROBOTICS AUTOMA, V13; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Flynn J., 2016, CVPR; Fraccaro Marco, 2018, 180409401 ARXIV; Geiger A., 2012, CVPR; Hedman P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982420; Henriques Joao F, 2018, CVPR; Ji D., 2017, CVPR; Liu Z., 2015, ICCV; Parisotto E., 2018, ICLR; Parisotto Emilio, 2018, GLOBAL POSE ESTIMATI; Park C, 2017, ICSA BOOK SER STAT, P3, DOI 10.1007/978-981-10-5194-4_1; Pritzel A., 2017, 170301988 ARXIV; Qi C. R., 2017, CVPR; Rosenbaum Dan, 2018, LEARNING MODELS VISU; Song S., 2017, CVPR; Sun Shao-Hua, 2018, ECCV; Tatarchenko M., 2016, ECCV; Tateno Keisuke, 2017, CVPR; Wang T.-C., 2018, CVPR; Wang Z., 2003, ACSSC; Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435; Wydmuch M., 2018, IEEE T GAMES; Xu Dan, 2018, 180504409 ARXIV; Zhang Jingwei, 2017, NEURAL SLAM LEARNING; Zhang L, 2012, IEEE IMAGE PROC, P1477, DOI 10.1109/ICIP.2012.6467150; Zhou T., 2016, ECCV	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301063
C	Qian, Q; Qian, XY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qian, Qian; Qian, Xiaoyuan			The Implicit Bias of AdaGrad on Separable Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the implicit bias of AdaGrad on separable linear classification problems. We show that AdaGrad converges to a direction that can be characterized as the solution of a quadratic optimization problem with the same feasible set as the hard SVM problem. We also give a discussion about how different choices of the hyperparameters of AdaGrad might impact this direction. This provides a deeper understanding of why adaptive methods do not seem to have the generalization ability as good as gradient descent does in practice.	[Qian, Qian] Ohio State Univ, Dept Stat, Columbus, OH 43210 USA; [Qian, Xiaoyuan] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Liaoning, Peoples R China	Ohio State University; Dalian University of Technology	Qian, Q (corresponding author), Ohio State Univ, Dept Stat, Columbus, OH 43210 USA.	qian.216@osu.edu; xyqian@dlut.edu.cn						Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gunasekar Suriya, 2017, IMPLICIT REGULARIZAT; Gunasekar Suriya, 2018, P 35 INT C MACH LEAR; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Keskar N. S., 2016, ICLR; Kingma D.P, P 3 INT C LEARNING R; Neyshabur B., 2017, GEOMETRY OPTIMIZATIO; Neyshabur B., 2015, INT C LEARN REPR; Soudry D., 2018, IMPLICIT BIAS GRADIE; Telgarsky M., 2013, INT C MACHINE LEARNI, P307; Wilson AC, 2017, MARGINAL VALUE ADAPT, P1; Zeng HQ, 2017, PROC INT CONF RECON	13	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307074
C	Qian, W; Zhang, YQ; Chen, YD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qian, Wei; Zhang, Yuqian; Chen, Yudong			Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This work studies the location estimation problem for a mixture of two rotation invariant log-concave densities. We demonstrate that Least Squares EM, a variant of the EM algorithm, converges to the true location parameter from a randomly initialized point. Moreover, we establish the explicit convergence rates and sample complexity bounds, revealing their dependence on the signal-to-noise ratio and the tail property of the log-concave distributions. Our analysis generalizes previous techniques for proving the convergence results of Gaussian mixtures, and highlights that an angle-decreasing property is sufficient for establishing global convergence for Least Squares EM.	[Qian, Wei; Zhang, Yuqian; Chen, Yudong] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14853 USA	Cornell University	Qian, W (corresponding author), Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14853 USA.	wq34@cornell.edu; yz2557@cornell.edu; yudong.chen@cornell.edu		Zhang, Yuqian/0000-0001-6080-9125; Qian, Wei/0000-0003-4997-604X	NSF [CCF-1657420, CCF-1704828]	NSF(National Science Foundation (NSF))	W. Qian and Y. Chen are partially supported by NSF grants CCF-1657420 and CCF-1704828.		0	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304076
C	Qiao, XY; Duan, JX; Cheng, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qiao, Xingye; Duan, Jiexin; Cheng, Guang			Rates of Convergence for Large-scale Nearest Neighbor Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Nearest neighbor is a popular class of classification methods with many desirable properties. For a large data set which cannot be loaded into the memory of a single machine due to computation, communication, privacy, or ownership limitations, we consider the divide and conquer scheme: the entire data set is divided into small subsamples, on which nearest neighbor predictions are made, and then a final decision is reached by aggregating the predictions on subsamples by majority voting. We name this method the big Nearest Neighbor (bigNN) classifier, and provide its rates of convergence under minimal assumptions, in terms of both the excess risk and the classification instability, which are proven to be the same rates as the oracle nearest neighbor classifier and cannot be improved. To significantly reduce the prediction time that is required for achieving the optimal rate, we also consider the pre-training acceleration technique applied to the bigNN method, with proven convergence rate. We find that in the distributed setting, the optimal choice of the neighbor k should scale with both the total sample size and the number of partitions, and there is a theoretical upper limit for the latter. Numerical studies have verified the theoretical findings.	[Qiao, Xingye] Binghamton Univ, Dept Math Sci, New York, NY 13902 USA; [Duan, Jiexin; Cheng, Guang] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA	State University of New York (SUNY) System; State University of New York (SUNY) Binghamton; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Qiao, XY (corresponding author), Binghamton Univ, Dept Math Sci, New York, NY 13902 USA.	qiao@math.binghamton.edu; duan32@purdue.edu; chengg@purdue.edu	Duan, Jiexin/AAG-7009-2021	Qiao, Xingye/0000-0003-0937-9822	National Science Foundation [DMS-1712907, DMS-1811812, DMS-1821183]; Office of Naval Research [ONR N00014-18-2759]	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research)	Guang Cheng's research was partially supported by the National Science Foundation (DMS-1712907, DMS-1811812, DMS-1821183,) and the Office of Naval Research (ONR N00014-18-2759). In addition, Guang Cheng is a member of the Institute for Advanced Study at Princeton University and a visiting Fellow of the Deep Learning Program at the Statistical and Applied Mathematical Sciences Institute (Fall 2019); he would like to thank both institutes for the hospitality.	Alabduljalil M.A., 2013, P 6 ACM INT C WEB SE, P203; Anastasiu D. C., 2017, J PARALLELAND DISTRI; Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Battey H., 2015, ARXIV PREPRINT ARXIV; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Candanedo LM, 2016, ENERG BUILDINGS, V112, P28, DOI 10.1016/j.enbuild.2015.11.071; Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437; Chawla NV, 2004, J MACH LEARN RES, V5, P421; Chen XY, 2014, STAT SINICA, V24, P1655, DOI 10.5705/ss.2013.088; Cover T, 1968, P HAW INT C SYST SCI, V415, P413; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Dasgupta S, 2013, EDGAR ALLAN POE IN CONTEXT, P313; DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633; Dietterich T. G, 1994, ADV NEURAL INFORM PR, V6, P216; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Fan J., 2017, ARXIV170206488; Fix E., 1951, TECH REP; FRITZ J, 1975, IEEE T INFORM THEORY, V21, P552, DOI 10.1109/TIT.1975.1055443; Gottlieb LA, 2014, IEEE T INFORM THEORY, V60, P5750, DOI 10.1109/TIT.2014.2339840; Guyon I., 2005, ADV NEURAL INFORM PR, V17, P545; GYORFI L, 1981, IEEE T INFORM THEORY, V27, P362, DOI 10.1109/TIT.1981.1056344; Hall P, 2005, J R STAT SOC B, V67, P363, DOI 10.1111/j.1467-9868.2005.00506.x; Hall P, 2008, ANN STAT, V36, P2135, DOI 10.1214/07-AOS537; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Kohler M, 2007, IEEE T INFORM THEORY, V53, P1735, DOI 10.1109/TIT.2007.894625; Kontorovich A., 2017, ADV NEURAL INFORM PR, P1573; Kontorovich A, 2015, JMLR WORKSH CONF PRO, V38, P480; Kpotufe S, 2017, J MACH LEARN RES, V18; KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248; Lee JD, 2017, J MACH LEARN RES, V18; Lichman M., 2013, UCI MACHINE LEARNING; Mammen E, 1999, ANN STAT, V27, P1808; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Shang Z., 2017, J MACH LEARN RES, V18, P3809; Slaney M, 2008, IEEE SIGNAL PROC MAG, V25, P128, DOI 10.1109/MSP.2007.914237; Sun WW, 2016, J AM STAT ASSOC, V111, P1254, DOI 10.1080/01621459.2015.1089772; Tsybakov AB, 2004, ANN STAT, V32, P135; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; WAGNER TJ, 1971, IEEE T INFORM THEORY, V17, P566, DOI 10.1109/TIT.1971.1054698; Xue Lirong, 2018, INT C ART INT STAT, P1628; Yeh IC, 2009, EXPERT SYST APPL, V36, P2473, DOI 10.1016/j.eswa.2007.12.020; Zhang Yuchen, 2013, C LEARN THEOR, P592617; Zhao TQ, 2016, ANN STAT, V44, P1400, DOI 10.1214/15-AOS1410	47	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902040
C	Qin, CL; Martens, J; Gowal, S; Krishnan, D; Dvijotham, K; Fawzi, A; De, S; Stanforth, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qin, Chongli; Martens, James; Gowal, Sven; Krishnan, Dilip; Dvijotham, Krishnamurthy (Dj); Fawzi, Alhussein; De, Soham; Stanforth, Robert			Adversarial Robustness through Local Linearization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Adversarial training is an effective methodology to train deep neural networks which are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradientobfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR- 10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with l(infinity) adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.	[Qin, Chongli; Martens, James; Gowal, Sven; Dvijotham, Krishnamurthy (Dj); Fawzi, Alhussein; De, Soham; Stanforth, Robert] DeepMind, London, England; [Krishnan, Dilip] Google, Mountain View, CA 94043 USA	Google Incorporated	Qin, CL (corresponding author), DeepMind, London, England.	chongliqin@google.com						Athalye A., 2018, P 35 INT C MACH LEAR; Buckman J., 2018, INT C LEARN REPR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dhillon GS., 2018, P 6 INT C LEARN REPR; Ding Gavin Weiguang, 2020, INT C LEARN REPR; Goodfellow I. J., 2014, ARXIV14126572; Gowal Sven, 2019, ARXIV191009338; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin A, 2016, INT C LEARN REPR SAN; Ma Xingjun, 2018, INT C LEARN REPR; Madry A., 2018, ARXIV PREPRINT ARXIV; Moosavi-Dezfooli S.-M., 2018, ARXIV181109716; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Simard P., 1992, ADV NEURAL INFORM PR, P895; Song Y, 2017, ARXIV171010766; Tsipras Dimitris, 2018, ARXIV180512152; van den Oord Aaron, 2018, ARXIV180205666; Xie C., 2018, ARXIV181203411; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang H., 2019, ARXIV190108573	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905049
C	Qiu, JL; Huang, G; Lee, TS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qiu, Jielin; Huang, Ge; Lee, Tai Sing			Visual Sequence Learning in Hierarchical Prediction Networks and Primate Visual Cortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPUTATIONAL ARCHITECTURE; REPRESENTATION; EXPERIENCE; NEOCORTEX; MEMORY; MODEL	In this paper we developed a computational hierarchical network model to understand the spatiotemporal sequence learning effects observed in the primate visual cortex. The model is a hierarchical recurrent neural model that learns to predict video sequences using the incoming video signals as teaching signals. The model performs fast feedforward analysis using a deep convolutional neural network with sparse convolution and feedback synthesis using a stack of LSTM modules. The network learns a representational hierarchy by minimizing its prediction errors of the incoming signals at each level of the hierarchy. We found that recurrent feedback in this network lead to the development of semantic cluster of global movement patterns in the population codes of the units at the lower levels of the hierarchy. These representations facilitate the learning of relationship among movement patterns, yielding state-of-the-art performance in long range video sequence predictions on benchmark datasets. Without further tuning, this model automatically exhibits the neurophysiological correlates of visual sequence memories that we observed in the early visual cortex of awake monkeys, suggesting the principle of self-supervised prediction learning might be relevant to understanding the cortical mechanisms of representational learning.	[Qiu, Jielin; Lee, Tai Sing] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Huang, Ge; Lee, Tai Sing] Carnegie Mellon Univ, Neurosci Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Qiu, JL (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.	jielinq@andrew.cmu.edu; taislee@andrew.cmu.edu			National Science Foundation [1816568]	National Science Foundation(National Science Foundation (NSF))	This work is supported by National Science Foundation (1816568). We thank Yimeng Zhang, Mert Inan, Harold Rockwell, Siming Yan, Maureen Kelly and David Pane for their technical assistance.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Babaeizadeh Mohammad, 2017, ARXIV171011252; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Cooke SF, 2015, CURR OPIN NEUROBIOL, V35, P57, DOI 10.1016/j.conb.2015.06.008; Cooke SF, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2013.0284; Dave A, 2017, PROC CVPR IEEE, P2067, DOI 10.1109/CVPR.2017.223; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Denton E, 2018, PR MACH LEARN RES, V80; Dijkstra N, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05888-8; Ebert Frederik, 2017, ARXIV171005268; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Freedman DJ, 2006, NATURE, V443, P85, DOI 10.1038/nature05078; Friston K, 2018, NAT NEUROSCI, V21, P1019, DOI 10.1038/s41593-018-0200-7; Goroshin R, NIPS; GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x; Han F, 2008, NEURON, V60, P321, DOI 10.1016/j.neuron.2008.08.026; Han K, 2018, ADV NEUR IN, V31; Hawkins J, 2006, HIERARCHICAL TEMPORA; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G, 2018, J NEUROSCI, V38, P8967, DOI 10.1523/JNEUROSCI.0664-18.2018; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Kersten D, 2003, CURR OPIN NEUROBIOL, V13, P150, DOI 10.1016/S0959-4388(03)00042-4; Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005; Lee Alex X, 2018, ARXIV180401523; Lee TS, 2015, P IEEE, V103, P1359, DOI 10.1109/JPROC.2015.2434601; Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434; Liu Xingyu, 2017, ABS180206367 CORR; Lotter W, 2018, ARXIV180510734; Lotter W., 2016, ARXIV160508104; Mathieu Michael, 2015, ARXIV151105440; McClelland James L., 1999, COMPLEMENTARY LEARNI; MCCLELLAND JL, 1985, J EXP PSYCHOL GEN, V114, P159, DOI 10.1037/0096-3445.114.2.159; Meyer T, 2014, NAT NEUROSCI, V17, P1388, DOI 10.1038/nn.3794; Meyer T, 2011, P NATL ACAD SCI USA, V108, P19401, DOI 10.1073/pnas.1112895108; Mruczek REB, 2007, J NEUROSCI, V27, P8533, DOI 10.1523/JNEUROSCI.2106-07.2007; MUMFORD D, 1992, BIOL CYBERN, V66, P241, DOI 10.1007/BF00198477; MUMFORD D, 1991, BIOL CYBERN, V65, P135, DOI 10.1007/BF00202389; Nayebi A., 2018, ADV NEURAL INFORM PR, P5290; OReilly R. C., 2014, LEARNING TIME THALAM; Palm R. B., 2012, PREDICTION CANDIDATE; Pan BW, 2018, PROC CVPR IEEE, P1536, DOI 10.1109/CVPR.2018.00166; Patraucean Viorica, 2015, ARXIV151106309; Ramachandran S, 2017, J NEUROPHYSIOL, V118, P374, DOI 10.1152/jn.00136.2017; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; ULLMAN S, 1995, CEREB CORTEX, V5, P1, DOI 10.1093/cercor/5.1.1; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Villegas R, 2017, PR MACH LEARN RES, V70; Wang YB, 2017, ADV NEUR IN, V30; Wang Yunbo, 2018, ARXIV180406300, DOI DOI 10.48550/ARXIV.1804.06300; Wang Yunbo, 2019, ICLR; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wen HG, 2018, PR MACH LEARN RES, V80; Wichers N, 2018, PR MACH LEARN RES, V80; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xu S, 2012, NAT NEUROSCI; Xu ZR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2940; Yan S, 2019, RECURRENT FEEDBACK I; Yao HS, 2007, NAT NEUROSCI, V10, P772, DOI 10.1038/nn1895	63	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308040
C	Raghavan, G; Thomson, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Raghavan, Guruprasad; Thomson, Matt			Neural networks grown and self-organized by noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CORTEX	Living neural networks emerge through a process of growth and self-organization that begins with a single cell and results in a brain, an organized and functional computational device. Artificial neural networks, however, rely on human-designed, hand-programmed architectures for their remarkable performance. Can we develop artificial computational devices that can grow and self-organize without human intervention? In this paper, we propose a biologically inspired developmental algorithm that can 'grow' a functional, layered neural network from a single initial cell. The algorithm organizes inter-layer connections to construct retinotopic pooling layers. Our approach is inspired by the mechanisms employed by the early visual system to wire the retina to the lateral geniculate nucleus (LGN), days before animals open their eyes. The key ingredients for robust self-organization are an emergent spontaneous spatiotemporal activity wave in the first layer and a local learning rule in the second layer that 'learns' the underlying activity pattern in the first layer. The algorithm is adaptable to a wide-range of input-layer geometries, robust to malfunctioning units in the first layer, and so can be used to successfully grow and self-organize pooling architectures of different pool-sizes and shapes. The algorithm provides a primitive procedure for constructing layered neural networks through growth and self-organization. We also demonstrate that networks grown from a single unit perform as well as hand-crafted networks on MNIST. Broadly, our work shows that biologically inspired developmental algorithms can be applied to autonomously grow functional 'brains' in-silico.	[Raghavan, Guruprasad] CALTECH, Dept Bioengn, Pasadena, CA 91125 USA; [Thomson, Matt] CALTECH, Biol & Biol Engn, Pasadena, CA 91125 USA	California Institute of Technology; California Institute of Technology	Raghavan, G (corresponding author), CALTECH, Dept Bioengn, Pasadena, CA 91125 USA.	graghava@caltech.edu; mthomson@caltech.edu						Anton-Bolanos Noelia, 2019, SCIENCE; Arlotta P, 2017, CURR OPIN NEUROBIOL, V42, pA1, DOI 10.1016/j.conb.2017.01.003; Bystron I, 2008, NAT REV NEUROSCI, V9, P110, DOI 10.1038/nrn2252; Denny CA, 2017, FRONT NEURAL CIRCUIT, V11, DOI 10.3389/fncir.2017.00023; Eglen SJ, 2009, HFSP J, V3, P176, DOI 10.2976/1.3079539; Elsken T., 2018, J MACH LEARN RES; Elsken Thomas, 2018, ARXIV180409081; FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7; FUKUSHIMA K, 1991, IEEE T NEURAL NETWOR, V2, P355, DOI 10.1109/72.97912; Glickfeld LL, 2017, ANNU REV VIS SCI, V3, P251, DOI 10.1146/annurev-vision-102016-061331; Godfrey KB, 2007, PLOS COMPUT BIOL, V3, P2408, DOI 10.1371/journal.pcbi.0030245; Hanks TD, 2017, NEURON, V93, P15, DOI 10.1016/j.neuron.2016.12.003; Haussler A, 1983, J THEOR NEUROBIOL, V2, P47; Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440; Koch G, 2015, RADICAL PHILOS, P28; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kutscher Brett, 2004, SCI STKE, V2004; LeCun Y., 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130; MEISTER M, 1991, SCIENCE, V252, P939, DOI 10.1126/science.2035024; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Padoa-Schioppa C, 2017, NEURON, V96, P736, DOI 10.1016/j.neuron.2017.09.031; Peirce JW, 2015, J VISION, V15, DOI 10.1167/15.7.5; Rakic P., 2000, Novartis Foundation Symposium, V228, P30; Real E., 2018, ARXIV180201548; Real E, 2017, PR MACH LEARN RES, V70; Sandini Giulio, 1993, ROBOTS BIOL SYSTEMS, P553; Saxe A M, 2011, ICML, V2, P6; Scaramuzza D., 2014, COMPUTER VISION REF9, P552; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Song W., 2015, STANDFORD CS224D REP; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Swindale NV, 1996, NETWORK-COMP NEURAL, V7, P161, DOI 10.1088/0954-898X/7/2/002; SWINDALE NV, 1980, BIOL SCI, V208, P243; Tan HM, 2017, WIRES COGN SCI, V8, DOI 10.1002/wcs.1424; WILLSHAW DJ, 1976, PROC R SOC SER B-BIO, V194, P431, DOI 10.1098/rspb.1976.0087; Wong ROL, 1999, ANNU REV NEUROSCI, V22, P29, DOI 10.1146/annurev.neuro.22.1.29; Xiong YA, 2010, P NATL ACAD SCI USA, V107, P17079, DOI 10.1073/pnas.1011271107; Zhou Yanqi, 2018, P C SYSML; Zoph B., 2016, ICLR; Zubler F, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.025.2009	41	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301084
C	Rajendran, V; LeVine, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rajendran, Vickram; LeVine, William			Accurate Layerwise Interpretable Competence Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Estimating machine learning performance "in the wild" is both an important and unsolved problem. In this paper, we seek to examine, understand, and predict the pointwise competence of classification models. Our contributions are twofold: First, we establish a statistically rigorous definition of competence that generalizes the common notion of classifier confidence; second, we present the ALICE (Accurate Layerwise Interpretable Competence Estimation) Score, a pointwise competence estimator for any classifier. By considering distributional,data, and model uncertainty, ALICE empirically shows accurate competence estimation in common failure situations such as class-imbalanced datasets, out-of-distribution datasets, and poorly trained models. Our contributions allow us to accurately predict the competence of any classification model given any input and error function. We compare our score with state-of-the-art confidence estimators such as model confidence and Trust Score, and show significant improvements in competence prediction over these methods on datasets such as DIGITS, CIFAR10, and CIFAR100.	[Rajendran, Vickram; LeVine, William] Johns Hopkins Univ, Appl Phys Lab, Laurel, MD 20723 USA	Johns Hopkins University; Johns Hopkins University Applied Physics Laboratory	Rajendran, V (corresponding author), Johns Hopkins Univ, Appl Phys Lab, Laurel, MD 20723 USA.	vickram.rajendran@jhuapl.edu; william.levine@jhuapl.edu			JHU/APL Internal Research and Development (IRAD) program	JHU/APL Internal Research and Development (IRAD) program	The authors would like to thank the JHU/APL Internal Research and Development (IRAD) program for funding this research.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 1993, P 6 ANN C COMPUTATIO, DOI DOI 10.1145/168304.168306; Chen L.-C., 2017, RETHINKING ATROUS CO; Chen Tongfei, 2018, ARXIV180505396; Ding Yukun, 2019, CORR; Farhadi A., 2018, YOLOV3 INCREMENTAL I, P1804; Gal Y., 2016, U CAMBRIDGE; Ghahramani Zoubin, 2015, DEEP LEARN WORKSH IC, V1, P2; Jiang H, 2018, ADV NEURAL INFORM PR, V1, P5541; Kendall A., 2017, CORR, V1703.04309; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky Alex, CIFAR 100, P4, DOI DOI 10.1109/ICCV.2017.324; Krizhevsky Alex, CIFAR 10; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Lee JD, 2004, HUM FACTORS, V46, P50, DOI 10.1518/hfes.46.1.50.30392; Lee K., 2017, ARXIV171109325; Lee K, 2018, PROCEEDINGS OF THE 2018 CONFERENCE ON RESEARCH IN ADAPTIVE AND CONVERGENT SYSTEMS (RACS 2018), P167, DOI 10.1145/3264746.3264813; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Malinin Andrey, 2018, NEURAL INFORM PROCES, V3, P6; Mandelbaum A., 2017, ARXIV PREPRINT ARXIV; Netzer Y., 2011, P NIPS WORKSH DEEP L; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Oberdiek P., 2018, ANNPR; Ribeiro M. T., 2016, CORR; Rudin C, 2019, NAT MACH INTELL, V1, P206, DOI 10.1038/s42256-019-0048-x; Snell J., 2017, CORR; SUBRAMANYA A, 2017, CORR; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Zadrozny B., 2002, TRANSFORMING CLASSIF; Zhu LH, 2017, 2017 IEEE SECOND INTERNATIONAL CONFERENCE ON DATA SCIENCE IN CYBERSPACE (DSC), P213, DOI 10.1109/DSC.2017.89	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905063
C	Ramstedt, S; Pal, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ramstedt, Simon; Pal, Christopher			Real-Time Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Markov Decision Processes (MDPs), the mathematical framework underlying most algorithms in Reinforcement Learning (RL), are often used in a way that wrongfully assumes that the state of an agent's environment does not change during action selection. As RL systems based on MDPs begin to find application in real-world, safety-critical situations, this mismatch between the assumptions underlying classical MDPs and the reality of real-time computation may lead to undesirable outcomes. In this paper, we introduce a new framework, in which states and actions evolve simultaneously and show how it is related to the classical MDP formulation. We analyze existing algorithms under the new real-time formulation and show why they are suboptimal when used in real time. We then use those insights to create a new algorithm Real-Time Actor-Critic (RTAC) that outperforms the existing state-of-the-art continuous control algorithm Soft Actor-Critic both in real-time and non-real-time settings. Code and videos can be found at github.com/rmst/rtrl.	[Ramstedt, Simon] Univ Montreal, Mila Element AI, Montreal, PQ, Canada; [Pal, Christopher] Polytech Montreal, Mila Element AI, Montreal, PQ, Canada	Universite de Montreal; Universite de Montreal; Polytechnique Montreal	Ramstedt, S (corresponding author), Univ Montreal, Mila Element AI, Montreal, PQ, Canada.	simonramstedt@gmail.com; christopher.pal@polymtl.ca			Open Philanthropy Project	Open Philanthropy Project	This work was completed during a part-time internship at Element AI and was supported by the Open Philanthropy Project.	Ba J., 2017, P 3 INT C LEARN REPR; Brockman G., 2016, OPENAI GYM; Firoiu Vlad, 2018, ABS181007286 CORR; Fujimoto S., 2018, ARXIV180209477; Gu SX, 2016, PR MACH LEARN RES, V48; Haarnoja T., 2018, P 35 INT C MACH LEAR; Haarnoja Tuomas, 2018, ARXIV181205905; Heess N., 2015, NIPS; Hwangbo J, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aau5872; Hwangbo J, 2017, IEEE ROBOT AUTOM LET, V2, P2096, DOI 10.1109/LRA.2017.2720851; Ibrahim Cyril, 2019, AVENUE; Konda VR, 2000, ADV NEUR IN, V12, P1008; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D., 2014, ICML; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Tallec L, 2019, ARXIV190109732; TESAURO G, 1994, NEURAL COMPUT, V6, P215, DOI 10.1162/neco.1994.6.2.215; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; van Hasselt H, 2016, ADV NEUR IN, V29; Walsh Thomas J, 2008, AUTON AGENT MULTI-AG, V18, P83	25	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303010
C	Reich, D; Todoki, A; Dowsley, R; De Cock, M; Nascimento, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Reich, Devin; Todoki, Ariel; Dowsley, Rafael; De Cock, Martine; Nascimento, Anderson			Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation: An Application to Hate-Speech Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				UNCONDITIONALLY SECURE; EFFICIENT; PROTOCOLS	Classification of personal text messages has many useful applications in surveillance, e-commerce, and mental health care, to name a few. Giving applications access to personal texts can easily lead to (un)intentional privacy violations. We propose the first privacy-preserving solution for text classification that is provably secure. Our method, which is based on Secure Multiparty Computation (SMC), encompasses both feature extraction from texts, and subsequent classification with logistic regression and tree ensembles. We prove that when using our secure text classification method, the application does not learn anything about the text, and the author of the text does not learn anything about the text classification model used by the application beyond what is given by the classification result itself. We perform end-to-end experiments with an application for detecting hate speech against women and immigrants, demonstrating excellent runtime results without loss of accuracy.	[Reich, Devin; Todoki, Ariel; De Cock, Martine; Nascimento, Anderson] Univ Washington, Sch Engn & Technol, Tacoma, WA 98402 USA; [Dowsley, Rafael] Bar Ilan Univ, Dept Comp Sci, IL-5290002 Ramat Gan, Israel; [De Cock, Martine] Univ Ghent, Dept Appl Math Comp Sci & Stat, Ghent, Belgium	University of Washington; University of Washington Tacoma; Bar Ilan University; Ghent University	Reich, D (corresponding author), Univ Washington, Sch Engn & Technol, Tacoma, WA 98402 USA.	dreich@uw.edu; atodoki@uw.edu; rafael@dowsley.net; mdecock@uw.edu; andclay@uw.edu		Nascimento, Anderson/0000-0002-8298-6250; De Cock, Martine/0000-0001-7917-0771				Allison Peter Ray, 2017, BBC; Almeida TA, 2011, DOCENG 2011: PROCEEDINGS OF THE 2011 ACM SYMPOSIUM ON DOCUMENT ENGINEERING, P259; Attrapadung Nuttapong, 2017, 15 ANN C PRIV SEC TR; Barreto Paulo S. L. M., 2017, 2017993 CRYPT EPRINT; Basile V., 2019, P 13 INT WORKSH SEM; BOST Raphael, 2015, NDSS, P4325; Canetti R, 2001, ANN IEEE SYMP FOUND, P136, DOI 10.1109/sfcs.2001.959888; CARTER JL, 1979, J COMPUT SYST SCI, V18, P143, DOI 10.1016/0022-0000(79)90044-8; Ciampi M, 2018, LECT NOTES COMPUT SC, V11035, P464, DOI 10.1007/978-3-319-98113-0_25; Clifton C., 2002, SIGKDD EXPLOR NEWSLE, V4, P28, DOI [10.1145/772862.772867, DOI 10.1145/772862.772867]; Costantino G, 2017, IEEE SYMP COMP COMMU, P890, DOI 10.1109/ISCC.2017.8024639; Cramer R., 2015, SECURE MULTIPARTY CO; de Hoogh S, 2014, LECT NOTES COMPUT SC, V8437, P179, DOI 10.1007/978-3-662-45472-5_12; Demmler D., 2015, NDSS; Dottling Nico, UNCONDITIONAL COMPOS, P164; Dowsley Rafael, 2016, THESIS; Dowsley Rafael, 2010, INT WORKSH INF SEC A, P337; Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1; Falk B.H., 2018, 2018238 CRYPT EPRINT; Farnadi G, 2016, USER MODEL USER-ADAP, V26, P109, DOI 10.1007/s11257-016-9171-0; Garay J, 2007, LECT NOTES COMPUT SC, V4450, P330; Grondahl T., 2018, P 11 ACM WORKSH ART; Hofheinz D., 2007, MORAVIACRYPT 2005, V37; Kaya SV, 2007, LECT NOTES COMPUT SC, V4819, P280; Mohassel P, 2017, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2017.12; O'Dea Bridianne, 2015, Internet Interventions, V2, P183, DOI 10.1016/j.invent.2015.03.005; Penard W., 2008, CRYPTOGRAPHY CONTEXT, P1; Pinkas B, 2018, ACM T PRIV SECUR, V21, DOI 10.1145/3154794; Reece AG, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-12961-9; Riazi MS, 2018, PROCEEDINGS OF THE 2018 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS'18), P707, DOI 10.1145/3196494.3196522; Rivest RL, 1999, UNCONDITIONALLY SECU; Sahami M, 1998, LEARN TEXT CAT PAP 1, V62, P98; Van Hee C, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0203794; Veugen T, 2015, IEEE J-STSP, V9, P1217, DOI 10.1109/JSTSP.2015.2429117; Weggenmann B, 2018, ACM/SIGIR PROCEEDINGS 2018, P305, DOI 10.1145/3209978.3210008	53	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303071
C	Reimherr, M; Awan, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Reimherr, Matthew; Awan, Jordan			KNG: The K-Norm Gradient Mechanism	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PRIVACY	This paper presents a new mechanism for producing sanitized statistical summaries that achieve differential privacy, called the K-Norm Gradient Mechanism, or KNG. This new approach maintains the strong flexibility of the exponential mechanism, while achieving the powerful utility performance of objective perturbation. KNG starts with an inherent objective function (often an empirical risk), and promotes summaries that are close to minimizing the objective by weighting according to how far the gradient of the objective function is from zero. Working with the gradient instead of the original objective function allows for additional flexibility as one can penalize using different norms. We show that, unlike the exponential mechanism, the noise added by KNG is asymptotically negligible compared to the statistical error for many problems. In addition to theoretical guarantees on privacy and utility, we confirm the utility of KNG empirically in the settings of linear and quantile regression through simulations.	[Reimherr, Matthew; Awan, Jordan] Penn State Univ, Dept Stat, State Coll, PA 16802 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University	Reimherr, M (corresponding author), Penn State Univ, Dept Stat, State Coll, PA 16802 USA.	mreimherr@psu.edu; awan@psu.edu		Awan, Jordan/0000-0001-9404-7499	NSF [SES-153443, DMS 1712826, SES 1853209]	NSF(National Science Foundation (NSF))	This research was supported in part by NSF DMS 1712826, NSF SES 1853209, and NSF SES-153443 to The Pennsylvania State University. The first author is also grateful for the hospitality of the Simons Institute for the Theory of Computing at UC Berkeley.	Aggarwal CC, 2008, ADV DATABASE SYST, V34, P11; Awan J., 2018, ARXIV E PRINTS; Awan J., 2019, P 36 INT C INT C MAC, P374; Awan J., 2018, ADV NEURAL INFORM PR; Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148; Bun Mark, 2016, TCC; Canonne CL, 2019, ACM S THEORY COMPUT, P310, DOI 10.1145/3313276.3316336; Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289; Chaudhuri K, 2013, J MACH LEARN RES, V14, P2905; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; DWORK C, 2006, CALIBRATING NOISE SE, V3876, P265, DOI DOI 10.1007/11681878_14; Dwork C, 2017, ANNU REV STAT APPL, V4, P61, DOI 10.1146/annurev-statistics-060116-054123; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2009, ACM S THEORY COMPUT, P371; Dwork Cynthia, 2016, CORR; Friedman A., 2010, P 16 ACM SIGKDD INT, P493, DOI DOI 10.1145/1835804.1835868; GORI M, 1992, IEEE T PATTERN ANAL, V14, P76, DOI 10.1109/34.107014; Hao L, 2007, QUANTILE REGRESSION; Hardt M, 2010, ACM S THEORY COMPUT, P705; Karwa Vishesh, 2017, CORR; Kattis Assimakis, 2017, INT S COMP GEOM SOCG, V77, DOI 10.4230/LIPIcs.SoCG.2017.45; Kifer D., 2012, J PRIVACY CONFIDENTI, V4, P5; KIFER D, 2012, J MACH LEARN RES, V23, P1; Lane J, 2014, PRIVACY, BIG DATA, AND THE PUBLIC GOOD: FRAMEWORKS FOR ENGAGEMENT, pXI; Machanavajjhala A, 2015, COMMUN ACM, V58, P58, DOI 10.1145/2660766; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Minami K., 2016, ADV NEURAL INFORM PR, P956; Sheffet O, 2017, PR MACH LEARN RES, V70; Smith A, 2011, ACM S THEORY COMPUT, P813; Van der Vaart A.W., 2000, CAMBRIDGE SERIES STA, DOI DOI 10.1017/CBO9780511802256; WANG YX, 2015, JMLR P, V37, P2493; Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651; Zhang J, 2012, PROC VLDB ENDOW, V5, P1364, DOI 10.14778/2350229.2350253	37	0	0	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901080
C	Reimherr, M; Awan, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Reimherr, Matthew; Awan, Jordan			Elliptical Perturbations for Differential Privacy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGRESSION	We study elliptical distributions in locally convex vector spaces, and determine conditions when they can or cannot be used to satisfy differential privacy (DP). A requisite condition for a sanitized statistical summary to satisfy DP is that the corresponding privacy mechanism must induce equivalent probability measures for all possible input databases. We show that elliptical distributions with the same dispersion operator, C, are equivalent if the difference of their means lies in the Cameron-Martin space of C. In the case of releasing finite-dimensional summaries using elliptical perturbations, we show that the privacy parameter epsilon can be computed in terms of a one-dimensional maximization problem. We apply this result to consider multivariate Laplace, t, Gaussian, and K-norm noise. Surprisingly, we show that the multivariate Laplace noise does not achieve epsilon-DP in any dimension greater than one. Finally, we show that when the dimension of the space is infinite, no elliptical distribution can be used to give epsilon-DP; only (epsilon, delta)-DP is possible.	[Reimherr, Matthew; Awan, Jordan] Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Reimherr, M (corresponding author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.	mreimherr@psu.edu; awan@psu.edu		Awan, Jordan/0000-0001-9404-7499	NSF [SES 1853209, DMS 1712826, SES-153443, SES-1853209]; Simons Institute for the Theory of Computing at UC Berkeley	NSF(National Science Foundation (NSF)); Simons Institute for the Theory of Computing at UC Berkeley	Research supported in part by NSF DMS 1712826, NSF SES 1853209, and the Simons Institute for the Theory of Computing at UC Berkeley.; Research supported in part by NSF SES-153443 and NSF SES-1853209.	Alda F, 2017, AAAI CONF ARTIF INTE, P1705; [Anonymous], 1991, FUNCTIONAL ANAL, DOI DOI 10.1070/IM8580; Awan J., ARXIV190110864; Awan J., 2019, ARXIV E PRINTS; Awan J., 2018, ADV NEURAL INFORM PR; BILLINGSLEY P., 1979, PROBABILITY MEASURE; Bogachev V. I., 1998, GAUSSIAN MEASURES, V62; Bosq D., 2000, LECT NOTES STAT, VVolume 149, P283, DOI [10.1007/978-1-4612-1154-9, DOI 10.1007/978-1-4612-1154-9]; Bun Mark, 2019, ARXIV190602830; Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Couillet R, 2016, J MULTIVARIATE ANAL, V143, P249, DOI 10.1016/j.jmva.2015.08.021; DWORK C, 2006, CALIBRATING NOISE SE, V3876, P265, DOI DOI 10.1007/11681878_14; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Fang K.T., 2017, SYMMETRIC MULTIVARIA; Frahm G, 2003, STAT PROBABIL LETT, V63, P275, DOI 10.1016/S0167-7152(03)00092-0; Gaboardi Marco, 2016, ICML 16 P 33 INT C I, V48; Geng Q, 2016, IEEE T INFORM THEORY, V62, P925, DOI 10.1109/TIT.2015.2504967; Ghosh A, 2009, ACM S THEORY COMPUT, P351; Goes J., 2017, ARXIV170608020; Hall R, 2013, J MACH LEARN RES, V14, P703; Hardt M, 2010, ACM S THEORY COMPUT, P705; Hoffmann-Jorgensen J., 1972, MATH SCAND, V28, P257; Kallenberg O., 2006, FDN MODERN PROBABILI; Karwa V., 2016, NIPS 2015 WORKSH LEA; Karwa V, 2016, ANN STAT, V44, P87, DOI 10.1214/15-AOS1358; KIFER D, 2012, J MACH LEARN RES, V23, P1; Kifer D, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2514689; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mirshani A., 2017, ARXIV171106660; Ollila E., 2019, IEEE T SIGNAL PROCES; Schmidt R., 2003, CREDIT RISK MEASUREM, P267; Smith M. T., 2018, PROC INT C ARTIF INT, P1195; Soloveychik I, 2014, IEEE T SIGNAL PROCES, V62, P5251, DOI 10.1109/TSP.2014.2348951; Stegun I. A., 1965, HDB MATH FUNCTIONS F, V55; Sun Y, 2016, IEEE T SIGNAL PROCES, V64, P3576, DOI 10.1109/TSP.2016.2546222; Vu D, 2009, INT CONF DAT MIN WOR, P138, DOI 10.1109/ICDMW.2009.52; Wang Yue, 2018, J PRIVACY CONFIDENTI, V8; Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651; Williams O., 2010, P 23 INT C NEURAL IN, P2451; Yu F, 2014, LECT NOTES COMPUT SC, V8744, P170, DOI 10.1007/978-3-319-11257-2_14; Zhang J, 2012, PROC VLDB ENDOW, V5, P1364, DOI 10.14778/2350229.2350253	44	0	0	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901078
C	Ren, WB; Liu, J; Shroff, NB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ren, Wenbo; Liu, Jia; Shroff, Ness B.			On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper studies the problem of finding the exact ranking from noisy comparisons. A noisy comparison over a set of m items produces a noisy outcome about the most preferred item, and reveals some information about the ranking. By repeatedly and adaptively choosing items to compare, we want to fully rank the items with a certain confidence, and use as few comparisons as possible. Different from most previous works, in this paper, we have three main novelties: (i) compared to prior works, our upper bounds (algorithms) and lower bounds on the sample complexity (aka number of comparisons) require the minimal assumptions on the instances, and are not restricted to specific models; (ii) we give lower bounds and upper bounds on instances with unequal noise levels; and (iii) this paper aims at the exact ranking without knowledge on the instances, while most of the previous works either focus on approximate rankings or study exact ranking but require prior knowledge. We first derive lower bounds for pairwise ranking (i.e., compare two items each time), and then propose (nearly) optimal pairwise ranking algorithms. We further make extensions to listwise ranking (i.e., comparing multiple items each time). Numerical results also show our improvements against the state of the art.	[Ren, Wenbo] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA; [Liu, Jia] Iowa State Univ, Dept Comp Sci, Ames, IA USA; [Shroff, Ness B.] Ohio State Univ, Dept Elect & Comp Engn & Comp Sci & Engn, Columbus, OH 43210 USA	University System of Ohio; Ohio State University; Iowa State University; University System of Ohio; Ohio State University	Ren, WB (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.	ren.453@osu.edu; jialiu@iastate.edu; shroff.11@osu.edu	Liu, Jia/ABB-2195-2020; Liu, Jia/G-2981-2016	Liu, Jia/0000-0001-8844-3233	NSF [ECCS-1818791, CCF-1758736, CNS-1758757, CNS-1446582, CNS-1901057]; ONR [N00014-17-1-2417]; AFRL [FA8750-18-1-0107]; Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIT) [2017-0-00692]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFRL(United States Department of DefenseUS Air Force Research Laboratory); Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIT)	This work has been supported in part by NSF grants ECCS-1818791, CCF-1758736, CNS-1758757, CNS-1446582, CNS-1901057; ONR grant N00014-17-1-2417; AFRL grant FA8750-18-1-0107, and by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIT), (2017-0-00692, Transport-aware Streaming Technique Enabling Ultra Low-Latency AR/VR Services).	Agarwal Alekh, 2017, C LEARN THEOR, P12; Arratia R., 1989, B MATH BIOL; Baltrunas L., 2010, P 4 ACM C RECOMMENDE, P119, DOI DOI 10.1145/1864708.1864733; Chen X., 2018, P 29 ANN ACM SIAM S; Chen X., 2013, ACM C WEB SEARCH DAT; Chen Y., 2015, INT C MACH LEARN; Chen Y., 2017, STAT; Conitzer V., 2005, P 6 ACM C EL COMM; Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]; Falahatgar M., 2017, ADV NEURAL INFORM PR; Falahatgar M, 2017, PR MACH LEARN RES, V70; Falahatgar M, 2018, PR MACH LEARN RES, V80; Farrell R. H., 1964, ANN MATH STAT; Feige U., 1994, SIAM J COMPUTING; Heckel R., 2016, ARXIV160608842; Heckel R, 2018, PR MACH LEARN RES, V84; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jamieson K., 2014, C LEARN THEOR, P423; Jang M., 2017, ADV NEURAL INFORM PR; Kalyanakrishnan S., 2010, INT C MACH LEARN; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Katariya S, 2018, PR MACH LEARN RES, V84; Kaufmann E., 2013, C LEARNING THEORY, P228; Luce R Duncan, 2012, INDIVIDUAL CHOICE BE; Mannor S., 2004, J MACHINE LEARNING R; Mohajer S., 2016, COMM CONTR COMP ALL; Negahban S., 2016, OPERATIONS RES; Rand D, 2012, P AAAI C ART INT, V26; Ren W., 2018, ARXIV180602970; Ren W., 2019, 22 INT C ART INT STA; Saha A., 2019, P MACHINE LEARNING R; Saha A., 2019, ARXIV190300558; Shah N. B., 2017, J MACHINE LEARNING R; Shah NB, 2016, PR MACH LEARN RES, V48; SZORENYI B., 2015, ADV NEURAL INFORM PR	35	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901062
C	Robinson, J; Sra, S; Jegelka, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Robinson, Joshua; Sra, Suvrit; Jegelka, Stefanie			Flexible Modeling of Diversity with Strongly Log-Concave Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Strongly log-concave (SLC) distributions are a rich class of discrete probability distributions over subsets of some ground set. They are strictly more general than strongly Rayleigh (SR) distributions such as the well-known determinantal point process. While SR distributions offer elegant models of diversity, they lack an easy control over how they express diversity. We propose SLC as the right extension of SR that enables easier, more intuitive control over diversity, illustrating this via examples of practical importance. We develop two fundamental tools needed to apply SLC distributions to learning and inference: sampling and mode finding. For sampling we develop an MCMC sampler and give theoretical mixing time bounds. For mode finding, we establish a weak log-submodularity property for SLC functions and derive optimization guarantees for a distorted greedy algorithm.	[Robinson, Joshua; Sra, Suvrit; Jegelka, Stefanie] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Robinson, J (corresponding author), MIT, Cambridge, MA 02139 USA.	joshrob@mit.edu; suvrit@mit.edu; stefje@csail.mit.edu			NSF-BIGDATA award; Defense Advanced Research Projects Agency [YFA17 N66001-17-1-4039]	NSF-BIGDATA award; Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported by an NSF-BIGDATA award and the Defense Advanced Research Projects Agency (grant number YFA17 N66001-17-1-4039). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. We thank Matt Staib for helpful comments on the draft.	Adiprasito K, 2018, ANN MATH, V188, P381, DOI 10.4007/annals.2018.188.2.1; Anari N., 2016, JMLR WORKSHOP C P, V49, P103; Anari N, 2019, ACM S THEORY COMPUT, P1, DOI 10.1145/3313276.3316385; Anari N, 2018, ANN IEEE SYMP FOUND, P35, DOI 10.1109/FOCS.2018.00013; Anari Nima, 2018, ABS181101600 CORR; [Anonymous], 2019, ARXIV190203719; Ben Hough J, 2006, PROBAB SURV, V3, P206, DOI 10.1214/154957806000000078; Borcea J, 2009, J AM MATH SOC, V22, P521; Branden P, 2007, ADV MATH, V216, P302, DOI 10.1016/j.aim.2007.05.011; Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675; Buchbinder N, 2015, SIAM J COMPUT, V44, P1384, DOI 10.1137/130929205; Celis L Elisa, 2018, ARXIV180204023; Christensen J. P. R., 1984, HARMONIC ANAL SEMIGR, V100; Civril A, 2013, ALGORITHMICA, V65, P159, DOI 10.1007/s00453-011-9582-6; Cryan Mary, 2019, ARXIV190306081; Das Abhimanyu, 2011, COMPUTING RES REPOSI; Derezinski M., 2017, ADV NEURAL INFORM PR, P3084; Diaconis Persi, 1991, ANN APPL PROBAB, V1, P36, DOI DOI 10.1214/aoap/1177005980; DJOLONGA J., 2015, INT C MACH LEARN ICM; Djolonga Josip, 2014, NEURAL INFORM PROCES; Djolonga Josip, 2018, NEURAL INFORM PROCES; Dupuy C, 2018, PR MACH LEARN RES, V84; Elfeki Mohamed, 2018, ARXIV181200068; Feldman Moran, 2018, ARXIV181003813; Gartrell M, 2017, AAAI CONF ARTIF INTE, P1912; Gillenwater J., 2012, ADV NEURAL INFORM PR; Gotovos A., 2015, NEURAL INFORM PROCES; Gotovos Alkis, 2019, STRONG LOG CONCAVITY; Gurvits L, 2009, ADVANCES IN COMBINATORIAL MATHEMATICS, P61, DOI 10.1007/978-3-642-03562-3_4; Harshaw Christopher, 2019, ARXIV190409354; Huh June, 2018, P INT C MATH ZUR 199; Iyer Rishabh, 2015, C ART INT STAT AISTA; Jegelka Stefanie, 2018, NEURIPS 2018 TUTORIA; Jerabek E, 2004, ANN PURE APPL LOGIC, V129, P1, DOI 10.1016/j.apal.2003.12.003; Khanna Rajiv, 2017, ARXIV170302723; Kulesza Alex, 2011, ICML; Kwok J. T., 2012, ADV NEURAL INFORM PR, P2996; Li C., 2017, ADV NEURAL INFORM PR, P5038; Li C, 2016, IEEE DATA COMPR CONF, P614, DOI 10.1109/DCC.2016.41; Li C, 2016, 2016 PROGRESS IN ELECTROMAGNETICS RESEARCH SYMPOSIUM (PIERS), P4188, DOI 10.1109/PIERS.2016.7735574; Lin H., 2012, UNCERTAINTY ARTIFICI; Mariet Z., 2016, INT C LEARN REPR; Mariet Z., 2016, ADV NEURAL INFORM PR, V29, P2694; Mariet Z, 2015, PR MACH LEARN RES, V37, P2389; Mariet Zelda, 2017, ADV NEURAL INFORM PR; Mariet Zelda, 2019, ARXIV190102051; Mariet Zelda E, 2018, ADV NEURAL INFORM PR, P4459; Raghavan T. E. S., 1997, NONNEGATIVE MATRICES, V64; Rodomanov A., 2019, ARXIV190404587	51	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906084
C	Rolnick, D; Ahuja, A; Schwarz, J; Lillicrap, TP; Wayne, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rolnick, David; Ahuja, Arun; Schwarz, Jonathan; Lillicrap, Timothy P.; Wayne, Greg			Experience Replay for Continual Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Interacting with a complex world involves continual learning, in which tasks and data distributions change over time. A continual learning system should demonstrate both plasticity (acquisition of new knowledge) and stability (preservation of old knowledge). Catastrophic forgetting is the failure of stability, in which new experience overwrites previous experience. In the brain, replay of past experience is widely believed to reduce forgetting, yet it has been largely overlooked as a solution to forgetting in deep reinforcement learning. Here, we introduce CLEAR, a replay-based method that greatly reduces catastrophic forgetting in multi-task reinforcement learning. CLEAR leverages off-policy learning and behavioral cloning from replay to enhance stability, as well as on-policy learning to preserve plasticity. We show that CLEAR performs better than state-of-the-art deep learning techniques for mitigating forgetting, despite being significantly less complicated and not requiring any knowledge of the individual tasks being learned.	[Rolnick, David] Univ Penn, Philadelphia, PA 19104 USA; [Ahuja, Arun; Schwarz, Jonathan; Lillicrap, Timothy P.; Wayne, Greg] DeepMind, London, England	University of Pennsylvania	Rolnick, D (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	drolnick@seas.upenn.edu; arahuja@google.com; schwarzjn@google.com; countzero@google.com; gregwayne@google.com	Ahuja, Arun/GSD-3117-2022					[Anonymous], 2017, P NATL ACAD SCI; Beattie C., 2016, ARXIV161203801; Benna MK, 2016, NAT NEUROSCI, V19, P1697, DOI 10.1038/nn.4401; Espeholt L., 2018, ICML; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Furlanello Tommaso, 2018, ICML; Grossberg Stephen, 1982, STUDIES MIND BRAIN, P1; Gu Shixiang, 2017, NEURIPS; Hayes T. L., 2018, ARXIV180905922; Hester Todd, 2018, AAAI; Isele David, 2018, AAAI; Kaplanis Christos, 2019, ICML; Kaplanis Christos, 2018, ICML; Leimer Pascal, 2019, BIORXIV; Li Z., 2017, TPAMI; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Lopez-Paz D., 2017, ADV NEURAL INF PROCE, P6467; McClelland JL, 1998, ANN NY ACAD SCI, V843, P153, DOI 10.1111/j.1749-6632.1998.tb08212.x; Milan Kieran, 2016, NEURIPS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos Remi, 2016, NEURIPS; ODonoghue B., 2017, ICLR; Oh Junhyuk, 2018, ICML; Parisi G. I., 2019, NEURAL NETWORKS; Ring MB, 1997, MACH LEARN, V28, P77, DOI 10.1023/A:1007331723572; Rios Amanda, 2018, ARXIV18110114; Rusu A. A., 2016, PROGR NEURAL NETWORK; Schaul T., 2016, ICLR; Schwarz J., 2018, ICML; Shin H, 2017, NEURIPS; Wang Ziyu, 2017, ICLR; Zenke Friedemann, 2017, ICML	32	0	0	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300032
C	Rosenberg, A; Mansour, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rosenberg, Aviv; Mansour, Yishay			Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MARKOV DECISION-PROCESSES	We consider online learning in episodic loop-free Markov decision processes (MDPs), where the loss function can change arbitrarily between episodes. The transition function is fixed but unknown to the learner, and the learner only observes bandit feedback (not the entire loss function). For this problem we develop no-regret algorithms that perform asymptotically as well as the best stationary policy in hindsight. Assuming that all states are reachable with probability beta > 0 under any policy, we give a regret bound of (O) over tilde (L vertical bar X vertical bar root vertical bar A vertical bar T/beta), where T is the number of episodes, X is the state space, A is the action space, and L is the length of each episode. When this assumption is removed we give a regret bound of (O) over tilde (L-3/2 vertical bar X parallel to A vertical bar T-1/4(3/4)), that holds for an arbitrary transition function. To our knowledge these are the first algorithms that in our setting handle both bandit feedback and an unknown transition function.	[Rosenberg, Aviv; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Mansour, Yishay] Google Res, Tel Aviv, Israel	Tel Aviv University; Google Incorporated	Rosenberg, A (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.	avivros007@gmail.com; mansour.yishay@gmail.com			Israel Science Foundation (ISF); Tel Aviv University Yandex Initiative in Machine Learning	Israel Science Foundation (ISF)(Israel Science Foundation); Tel Aviv University Yandex Initiative in Machine Learning	This work was supported in part by a grant from the Israel Science Foundation (ISF) and by the Tel Aviv University Yandex Initiative in Machine Learning.	[Anonymous], 2010, COLT; Auer Peter, 2008, ADV NEURAL INFORM PR, P89; Azar MG, 2017, PR MACH LEARN RES, V70; Bartlett RA, 2009, 2009 ICSE WORKSHOP ON SOFTWARE ENGINEERING FOR COMPUTATIONAL SCIENCE AND ENGINEERING, P35, DOI 10.1109/SECSE.2009.5069160; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; Jin C., 2018, ADV NEURAL INFORM PR; NEU G, 2012, ARTIF INTELL, P805; Neu G, 2014, IEEE T AUTOMAT CONTR, V59, P676, DOI 10.1109/TAC.2013.2292137; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rosenberg A, 2019, PR MACH LEARN RES, V97	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302023
C	Sahni, H; Buckley, T; Abbeel, P; Kuzovkin, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sahni, Himanshu; Buckley, Toby; Abbeel, Pieter; Kuzovkin, Ilya			Addressing Sample Complexity in Visual Tasks Using HER and Hallucinatory GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reinforcement Learning (RL) algorithms typically require millions of environment interactions to learn successful policies in sparse reward settings. Hindsight Experience Replay (HER) was introduced as a technique to increase sample efficiency by reimagining unsuccessful trajectories as successful ones by altering the originally intended goals. However, it cannot be directly applied to visual environments where goal states are often characterized by the presence of distinct visual features. In this work, we show how visual trajectories can be hallucinated to appear successful by altering agent observations using a generative model trained on relatively few snapshots of the goal. We then use this model in combination with HER to train RL agents in visual settings. We validate our approach on 3D navigation tasks and a simulated robotics application and show marked improvement over baselines derived from previous work.	[Sahni, Himanshu] Georgia Inst Technol, Atlanta, GA 30332 USA; [Buckley, Toby; Abbeel, Pieter; Kuzovkin, Ilya] OffWorld Inc, Pasadena, CA USA; [Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA	University System of Georgia; Georgia Institute of Technology; University of California System; University of California Berkeley	Sahni, H (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	hsahni3@gatech.edu	Kuzovkin, Ilya (Eli)/AAD-1476-2022	Kuzovkin, Ilya (Eli)/0000-0001-6054-8607				Andrychowicz M., 2017, ADV NEURAL INFORM PR; [Anonymous], 2018, CORR; Brock Andrew, 2018, ARXIV180911096; Brockman G., 2016, OPENAI GYM; Burgess CP, 2018, ARXIV180403599; Chevalier-Boisvert Maxime, 2018, GYM MINIWORLD ENV OP; Edwards A. D., 2018, CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal A., 2018, CORR; Gulrajani I, 2017, P NIPS 2017; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; Held D., 2017, CORR; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Karras T., 2017, PROGR GROWING GANS I; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koenig N., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2149; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Montemerlo Michael, 2002, AAAI IAAI, DOI DOI 10.1007/S00244-005-7058-X; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Odena A, 2017, PR MACH LEARN RES, V70; Pinto L, 2017, PR MACH LEARN RES, V70; Pong V., 2018, PROC 2 WORKSHOP LIFE; Randlov J., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P463; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schmidhuber J, 1990, MAKING WORLD DIFFERE; Schroecker Y., 2019, INT C LEARN REPR ICL; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Van Hasselt Hado, 2016, P AAAI C ART INT, V30; Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558; Wise M., 2011, REP 119 SPECIFICATIO; Wydmuch M., 2016, ARXIV160502097, P1, DOI DOI 10.1109/CIG.2016.7860433; Xie A., 2018, CORR; Zamora I., 2016, ARXIV160805742	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305078
C	Salehi, F; Trouleau, W; Grossglauser, M; Thiran, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Salehi, Farnood; Trouleau, William; Grossglauser, Matthias; Thiran, Patrick			Learning Hawkes Processes from a Handful of Events	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIRECTED INFORMATION	Learning the causal-interaction network of multivariate Hawkes processes is a useful task in many applications. Maximum-likelihood estimation is the most common approach to solve the problem in the presence of long observation sequences. However, when only short sequences are available, the lack of data amplifies the risk of overfitting and regularization becomes critical. Due to the challenges of hyper-parameter tuning, state-of-the-art methods only parameterize regularizers by a single shared hyper-parameter, hence limiting the power of representation of the model. To solve both issues, we develop in this work an efficient algorithm based on variational expectation-maximization. Our approach is able to optimize over an extended set of hyper-parameters. It is also able to take into account the uncertainty in the model parameters by learning a posterior distribution over them. Experimental results on both synthetic and real datasets show that our approach significantly outperforms state-of-the-art methods under short observation sequences.	[Salehi, Farnood; Trouleau, William; Grossglauser, Matthias; Thiran, Patrick] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Ecole Polytechnique Federale de Lausanne	Salehi, F (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	farnood.salehi@epfl.ch; william.trouleau@epfl.ch; matthias.grossglauser@epfl.ch; patrick.thiran@epfl.ch			Swiss National Science Foundation [200021-182407]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	We would like to thank Negar Kiyavash and Jalal Etesami for the valuable discussions and insightful feedback at the early stage of this work. The work presented in this paper was supported in part by the Swiss National Science Foundation under grant number 200021-182407.	Achab M, 2017, PR MACH LEARN RES, V70; [Anonymous], 2014, ICML; Arnold A, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P66; Bacry E., 2015, ARXIV150100725; Bacry E., 2015, MARKET MICROSTRUCTUR, V1, P1; Bamler R., 2017, P 34 INT C MACH LEAR; Bamler Robert, 2019, P C UNC ART INT; Barkan O, 2017, AAAI CONF ARTIF INTE, P3135; Beal MJ, 2003, BAYESIAN STATISTICS 7, P453; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Daley D.J., 2003, PROBABILITY ITS APPL, VI; Eichler M, 2017, J TIME SER ANAL, V38, P225, DOI 10.1111/jtsa.12213; Eichler M, 2012, PROBAB THEORY REL, V153, P233, DOI 10.1007/s00440-011-0345-8; Etesami J., 2016, P 32 C UNC ART INT, P162; Figueiredo Flavio, 2018, P 32 INT C NEUR INF, P2975; Garske T, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0308; Hansen NR, 2015, BERNOULLI, V21, P83, DOI 10.3150/13-BEJ562; Ji Geng, 2017, S ADV APPROXIMATE BA; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Lemonnier Remi, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P161, DOI 10.1007/978-3-662-44851-9_11; Linderman Scott W., 2017, NEURIPS S ADV APPROX; Linderman SW, 2015, ARXIV150703228; Quinn CJ, 2015, IEEE T INFORM THEORY, V61, P6887, DOI 10.1109/TIT.2015.2478440; Rao Arvind, 2008, Journal of Bioinformatics and Computational Biology, V6, P493, DOI 10.1142/S0219720008003515; Rezende D. J., 2014, P 31 INT C MACH LEAR; Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Xu H, 2016, INT C MACH LEARN, P1717; Xu HT, 2017, PR MACH LEARN RES, V70; Zhang Cheng, 2018, IEEE T PATT ANAL MAC; Zhou K, 2013, ICML; Zhou K., 2013, ARTIF INTELL, P641	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904037
C	Salim, A; Kovalev, D; Richtarik, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Salim, Adil; Kovalev, Dmitry; Richtarik, Peter			Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ERGODIC CONVERGENCE; MONOTONE	We propose a new algorithm-Stochastic Proximal Langevin Algorithm (SPLA)-for sampling from a log concave distribution. Our method is a generalization of the Langevin algorithm to potentials expressed as the sum of one stochastic smooth term and multiple stochastic nonsmooth terms. In each iteration, our splitting technique only requires access to a stochastic gradient of the smooth term and a stochastic proximal operator for each of the nonsmooth terms. We establish nonasymptotic sublinear and linear convergence rates under convexity and strong convexity of the smooth term, respectively, expressed in terms of the KL divergence and Wasserstein distance. We illustrate the efficiency of our sampling technique through numerical simulations on a Bayesian learning task.	[Salim, Adil; Kovalev, Dmitry; Richtarik, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia	King Abdullah University of Science & Technology	Salim, A (corresponding author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.		Richtarik, Peter/O-5797-2018					Ambrosio L., 2008, LECT MATH; BACH F., 2011, ADV NEURAL INFORM PR, P451; Vu BC, 2013, ADV COMPUT MATH, V38, P667, DOI 10.1007/s10444-011-9254-8; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Bernton E., 2018, ARXIV180208671; Bianchi P, 2019, J CONVEX ANAL, V26, P397; Bianchi P, 2016, SIAM J OPTIMIZ, V26, P2235, DOI 10.1137/15M1017909; Bianchi P, 2016, J OPTIMIZ THEORY APP, V171, P90, DOI 10.1007/s10957-016-0978-y; Brezis H., 1973, N HOLLAND MATH STUDI, V5; Chambolle A., 2010, THEORETICAL FDN NUME, P263, DOI [10.1515/9783110226157.263, DOI 10.1515/9783110226157.263]; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chatterji N., 2018, ARXIV180205431; Cheng X., 2017, ARXIV170703663; Condat L, 2013, J OPTIMIZ THEORY APP, V158, P460, DOI 10.1007/s10957-012-0245-9; Dalalyan A., 2019, STOCHASTIC PROCESSES; Dalalyan A. S., 2018, ARXIV180709382; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Davis D, 2017, SET-VALUED VAR ANAL, V25, P829, DOI 10.1007/s11228-017-0421-z; Durmus A., 2019, J MACHINE LEARNING R, V20, P1; Durmus A, 2018, SIAM J IMAGING SCI, V11, P473, DOI 10.1137/16M1108340; Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238; Hazan E, 2016, PR MACH LEARN RES, V48; Hsieh Y.-P., 2018, ADV NEURAL INFORM PR, P2878; Kotz S., 2012, LAPLACE DISTRIBUTION; KUSHNER HJ, 2003, APPL MATH NEW YORK, V35; Leskovec J, 2014, SNAP DATASETS STANFO; Ma Y, 2019, ARXIV190708990; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2, P2, DOI DOI 10.1201/B10905-6; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; PASSTY GB, 1979, J MATH ANAL APPL, V72, P383, DOI 10.1016/0022-247X(79)90234-8; Patrascu A, 2018, J MACH LEARN RES, V18; RICHTARIK P, 2017, ARXIV170601108; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rockafellar R. T., 1982, Stochastics, V7, P173, DOI 10.1080/17442508208833217; Rosasco L., 2015, ARXIV150700852; Salim A., 2019, IEEE T AUTOMATIC CON; Santambrogio F, 2017, B MATH SCI, V7, P87, DOI 10.1007/s13373-017-0101-1; Schechtman S., 2019, CAP; Siheng Chen, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P8267, DOI 10.1109/ICASSP.2014.6855213; Simsekli U, 2017, PR MACH LEARN RES, V70; Sollich P, 2002, MACH LEARN, V46, P21, DOI 10.1023/A:1012489924661; Tansey Wesley, 2015, ARXIV150506475; Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189; Toulis P., 2015, ARXIV151000967; van Erven T, 2014, IEEE T INFORM THEORY, V60, P3797, DOI 10.1109/TIT.2014.2320500; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang YJ, 2016, J MACH LEARN RES, V17, P1; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wibisono A., 2018, ARXIV180208089; Xu XF, 2015, BAYESIAN ANAL, V10, P909, DOI 10.1214/14-BA929; Yurtsever A, 2016, ADV NEUR IN, V29; Zou D., 2018, INT C UNC ART INT; Zou Difan, 2019, P MACHINE LEARNING R, P2936	53	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306063
C	Salman, H; Yang, G; Li, J; Zhang, PC; Zhang, H; Razenshteyn, I; Bubeck, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Salman, Hadi; Yang, Greg; Li, Jerry; Zhang, Pengchuan; Zhang, Huan; Razenshteyn, Ilya; Bubeck, Sebastien			Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to l(2)-norm adversarial perturbations. In this paper, we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers, and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably l(2)-robust classifiers by a significant margin on ImageNet and CIFAR- 10, establishing the state-of-the-art for provable l(2)-defenses. Moreover, we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further.	[Salman, Hadi; Yang, Greg; Li, Jerry; Zhang, Pengchuan; Zhang, Huan; Razenshteyn, Ilya; Bubeck, Sebastien] Microsoft Res AI, Redmond, WA USA		Salman, H (corresponding author), Microsoft Res AI, Redmond, WA USA.	hadi.salman@microsoft.com; gregyang@microsoft.com; jerrl@microsoft.com; penzhan@microsoft.com; t-huzhan@microsoft.com; ilyaraz@microsoft.com; sebubeck@microsoft.com	Zhang, Pengchuan/AAR-3769-2021					Athalye A., 2018, P 35 INT C MACH LEAR; Athalye Anish, 2018, ARXIV180403286; Cao XY, 2017, ANN COMPUT SECURITY, P278, DOI 10.1145/3134600.3134606; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carmon Yair, 2019, ARXIV190513736; Chen P.-Y., 2018, NIPS, P4939; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cohen Jeremy M, 2019, ARXIV190202918; Croce F., 2018, ARXIV181007481; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dvijotham K, 2018, ARXIV180510265; Dvijotham K., 2018, UAI; Ehlers R, 2017, LECT NOTES COMPUT SC, V10482, P269, DOI 10.1007/978-3-319-68167-2_19; Fischetti Matteo, 2017, ARXIV171206174; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Goodfellow I. J., 2015, ICLR; Gowal Sven, 2018, EFFECTIVENESS INTERV; JAISINGHANI D, 2018, ADV NEURAL INFORM PR, P823, DOI DOI 10.1145/3241539.3267717; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin A, 2016, INT C LEARN REPR SAN; Lecuyer M., 2018, ARXIV180203471; Lee K., 2019, ARXIV190109960; Li B., 2018, ARXIV180903113, P1; Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23; Lomuscio A., 2017, CORR; Madry A., 2018, ARXIV PREPRINT ARXIV; Mirman M, 2018, PR MACH LEARN RES, V80; Raghunathan A., 2018, NEURIPS; Raghunathan Aditi, 2018, INT C LEARNING REPRE; Rony J, 2018, ARXIV181109600; SALMAN H, 2019, ADV NEURAL INFORM PR, P9832; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Tjeng V., 2019, P 7 INT C LEARN REPR; van den Oord Aaron, 2018, ARXIV180205666; Wang S., 2018, ARXIV181102625; WANG SQ, 2018, ADV NEURAL INFORM PR, P636, DOI DOI 10.1145/3240508.3240615; Weng Tsui-Wei, 2018, INT C MACH LEARN; Wong E., 2018, ADV NEURAL INFORM PR; Wong E, 2018, PR MACH LEARN RES, V80	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902087
C	Sanmartin, EF; Damrich, S; Hamprecht, FA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sanmartin, Enrique Fita; Damrich, Sebastian; Hamprecht, Fred A.			Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FRAMEWORK; ALGORITHM; PATHS; TREE	The seeded Watershed algorithm / minimax semi-supervised learning on a graph computes a minimum spanning forest which connects every pixel / unlabeled node to a seed / labeled node. We propose instead to consider all possible spanning forests and calculate, for every node, the probability of sampling a forest connecting a certain seed with that node. We dub this approach "Probabilistic Watershed". Leo Grady (2006) already noted its equivalence to the Random Walker / Harmonic energy minimization. We here give a simpler proof of this equivalence and establish the computational feasibility of the ProbabilisticWatershed with Kirchhoff's matrix tree theorem. Furthermore, we show a new connection between the RandomWalker probabilities and the triangle inequality of the effective resistance. Finally, we derive a new and intuitive interpretation of the Power Watershed.	[Sanmartin, Enrique Fita; Damrich, Sebastian; Hamprecht, Fred A.] Heidelberg Univ, IWR, HCI, D-69115 Heidelberg, Germany	Ruprecht Karls University Heidelberg	Sanmartin, EF (corresponding author), Heidelberg Univ, IWR, HCI, D-69115 Heidelberg, Germany.	fita@stud.uni-heidelberg.de; sebastian.damrich@iwr.uni-heidelberg.de; fred.hamprecht@iwr.uni-heidelberg.de			DFG [HA-4364 8-1]	DFG(German Research Foundation (DFG))	The authors would like to thank Prof. Marco Saerens for his profound and constructive comments as well as the anonymous reviewers for their helpful remarks. We would like to express our gratitude to Lorenzo Cerrone, who also shared the edge weights of [11], and Laurent Najman for the useful discussions about the Random Walker and Power Watershed algorithms, respectively. We also acknowledge partial financial support of the DFG under grant No. DFG HA-4364 8-1.	Allene, 2007, MATH MORPHOLOGY ITS, V1, P253; ANGULO J, 2007, INT S MATH MORPH, V8, P265; Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305; Beier T, 2017, NAT METHODS, V14, P101, DOI 10.1038/nmeth.4151; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Beucher S., 1993, MATH MORPHOLOGY IMAG, P433, DOI DOI 10.1201/9781482277234-12; Beucher S., 1979, INT WORKSH IM PROC R, V132; Biggs N, 1997, B LOND MATH SOC, V29, P641, DOI 10.1112/S0024609397003305; Bockelmann N., 2019, INT C MED IM DEEP LE; Bui V, 2018, I S BIOMED IMAGING, P1352; Cerrone L., 2019, CVPR; Challa A, 2019, IEEE SIGNAL PROC LET, V26, P720, DOI 10.1109/LSP.2019.2905155; Chazelle B, 2000, J ACM, V47, P1028, DOI 10.1145/355541.355562; Chebotarev PY, 1997, AUTOMAT REM CONTR+, V58, P1505; Couprie C, 2011, IEEE T PATTERN ANAL, V33, P1384, DOI 10.1109/TPAMI.2010.200; Couprie M, 2005, J MATH IMAGING VIS, V22, P231, DOI 10.1007/s10851-005-4892-4; Cousty J, 2009, IEEE T PATTERN ANAL, V31, P1362, DOI 10.1109/TPAMI.2008.173; CREMI, 2017, MICCAI CHALL CIRC RE; Criminisi A, 2008, LECT NOTES COMPUT SC, V5302, P99, DOI 10.1007/978-3-540-88682-2_9; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Fernandes SEN, 2019, IEEE T SMART GRID, V10, P3226, DOI 10.1109/TSG.2018.2821765; FOUSS F, 2016, ALG MOD NETW DATA, P1; Francoisse K, 2017, NEURAL NETWORKS, V90, P90, DOI 10.1016/j.neunet.2017.03.010; Ghosh A, 2008, SIAM REV, V50, P37, DOI 10.1137/050645452; GRADY L. J., 2010, DISCRETE CALCULUS; Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233; Kim KH, 2014, PATTERN RECOGN LETT, V45, P17, DOI 10.1016/j.patrec.2014.02.020; Kirchhoff G., 1847, ANN PHYS-BERLIN, V148, P497, DOI 10.1002/andp.18471481202; Kivimaki I, 2014, PHYSICA A, V393, P600, DOI 10.1016/j.physa.2013.09.016; Koo Terry, 2007, P 2007 JOINT C EMP M, P141; Liu Z, 2018, INT C PATT RECOG, P1187, DOI 10.1109/ICPR.2018.8546283; MAGGS BM, 1988, INFORM PROCESS LETT, V26, P291, DOI 10.1016/0020-0190(88)90185-8; Malmberg F, 2014, PATTERN RECOGN LETT, V47, P80, DOI 10.1016/j.patrec.2014.03.016; Mensch A, 2018, PR MACH LEARN RES, V80; Fernandes SEN, 2019, PATTERN ANAL APPL, V22, P703, DOI 10.1007/s10044-017-0677-9; Ofverstedt J, 2019, LECT NOTES COMPUT SC, V11414, P75, DOI 10.1007/978-3-030-14085-4_7; Senelle M, 2014, IEEE T PATTERN ANAL, V36, P1268, DOI 10.1109/TPAMI.2013.227; Straehle CN, 2012, PROC CVPR IEEE, P765, DOI 10.1109/CVPR.2012.6247747; Teixeira A.S., 2013, WORKSHOP MINING LEAR, V24, P27; Teixeira AS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0119315; TSEN FSP, 1994, IEEE T RELIAB, V43, P600, DOI 10.1109/24.370220; Tutte William, 1984, ENCY MATH ITS APPL, V21; Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315; Winkler G., 2012, IMAGE ANAL RANDOM FI, V27; Wolf S, 2017, IEEE I CONF COMP VIS, P2030, DOI 10.1109/ICCV.2017.222; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu X., 2003, INT C MACH LEARN	47	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302074
C	Sasy, S; Ohrimenko, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sasy, Sajin; Ohrimenko, Olga			Oblivious Sampling Algorithms for Private Data Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study secure and privacy-preserving data analysis based on queries executed on samples from a dataset. Trusted execution environments (TEEs) can be used to protect the content of the data during query computation, while supporting differential-private (DP) queries in TEEs provides record privacy when query output is revealed. Support for sample-based queries is attractive due to privacy amplification since not all dataset is used to answer a query but only a small subset. However, extracting data samples with TEEs while proving strong DP guarantees is not trivial as secrecy of sample indices has to be preserved. To this end, we design efficient secure variants of common sampling algorithms. Experimentally we show that accuracy of models trained with shuffling and sampling is the same for differentially private models for MNIST and CIFAR-10, while sampling provides stronger privacy guarantees than shuffling.	[Sasy, Sajin] Univ Waterloo, Waterloo, ON, Canada; [Sasy, Sajin; Ohrimenko, Olga] Microsoft Res, Redmond, WA 98052 USA	University of Waterloo; Microsoft	Sasy, S (corresponding author), Univ Waterloo, Waterloo, ON, Canada.; Sasy, S (corresponding author), Microsoft Res, Redmond, WA 98052 USA.			Ohrimenko, Olga/0000-0002-9735-0538				Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Allen J., 2019, C NEUR INF PROC SYST; Andrew G, 2019, TENSORFLOW PRIVACY; [Anonymous], 1987, ACM S THEOR COMP STO; Balle B, 2018, ARXIV180701647; Bassily R., 2014, S FDN COMP SCI FOCS; Beimel A, 2014, MACH LEARN, V94, P401, DOI 10.1007/s10994-013-5404-1; Bittau A., 2017, ACM S OP SYST PRINC; Brasser F., 2017, USENIX WORKSH OFF TE; Carlini N., 2019, USENIX SEC S; Chan T., 2019, PROC 30 ANN ACM SIAM, P2448; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Erlingsson Ulfar, 2019, ACM SIAM S DISCR ALG; Fredrikson M, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1322, DOI 10.1145/2810103.2813677; Goldreich O, 1996, J ACM, V43, P431, DOI 10.1145/233551.233553; Goodrich MT, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P684, DOI 10.1145/2591796.2591830; Gotzfried J., 2017, EUR WORKSH SYST SEC; Hoekstra M., 2013, WORKSH HARDW ARCH SU; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Li N., 2012, P ACM S INF COMP COM; McMahan H.B., 2018, INT C LEARN REPR; McSherry F. D., 2009, SIGMOD; Melis L., 2019, IEEE S SEC PRIV S P; Nissim K., 2007, ACM S THEOR COMP STO; Ohrimenko O., 2016, USENIX SEC S; OHRIMENKO O., 2014, INT C AUT LANG PROGR, V8573; OHRIMENKO O., 2015, ACM C COMP COMM SEC; OSVIK D. A., 2006, RSA C CRYPT TRACK CT; Patel S., 2018, INT C AUT LANG PROGR; Riondato M, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P413, DOI 10.1145/2556195.2556224; Riondato M, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2629586; Shokri R., 2017, IEEE S SEC PRIV SP; STEFANOV E, 2013, ACM C COMP COMM SEC; Tramer Florian, 2019, INT C LEARNING REPRE; WANG X. S., 2014, ACM C COMP COMM SEC; Wang Y., 2019, ARTIFICIAL INTELLIGE; Wang Y.-X., 2015, INT C MACH LEARN ICM; Xu Yuanzhong, 2015, IEEE S SEC PRIV SP; Yu L., 2019, IEEE S SEC PRIV S P; Zheng Wenting, 2017, USENIX S NETW SYST D	41	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306049
C	Saxena, S; Tuzel, O; DeCoste, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Saxena, Shreyas; Tuzel, Oncel; DeCoste, Dennis			Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent works have shown that learning from easier instances first can help deep neural networks (DNNs) generalize better. However, knowing which data to present during different stages of training is a challenging problem. In this work, we address this problem by introducing data parameters. More specifically, we equip each sample and class in a dataset with a learnable parameter (data parameters), which governs their importance in the learning process. During training, at each iteration, as we update the model parameters, we also update the data parameters. These updates are done by gradient descent and do not require hand-crafted rules or design. When applied to image classification task on CIFARlO, CIFAR100, WebVision and ImageNet datasets, and object detection task on KITTI dataset, learning a dynamic curriculum via data parameters leads to consistent gains, with-out any increase in model complexity or training time. When applied to a noisy dataset, the proposed method learns to learn from clean images and improves over the state-of-the-art methods by 14%. To the best of our knowledge, our work is the first curriculum learning method to show gains on large scale image classification and detection tasks.	[Saxena, Shreyas; Tuzel, Oncel; DeCoste, Dennis] Apple, Cupertino, CA 95014 USA	Apple Inc	Saxena, S (corresponding author), Apple, Cupertino, CA 95014 USA.	shreyas-saxena@apple.com; otuzel@apple.com; ddecoste@apple.com						[Anonymous], 2009, ICML; Chang Haw-Shiuan, 2017, NIPS; Chen XJ, 2015, PROCEEDING OF THE 5TH INTERNATIONAL YELLOW RIVER FORUM ON ENSURING WATER RIGHT OF THE RIVER'S DEMAND AND HEALTHY RIVER BASIN MAINTENANCE, VOL I, P3; Choudhury Monojit, 2017, INT C NAT LANG PROC; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dong XY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P279, DOI 10.1145/3123266.3123455; Fan Yang, 2018, ICLR; Geiger A., 2013, INT J ROBOTICS RES; Graves A., 2017, ICML; Guo C., 2017, ICML; Guo S., 2018, ECCV; Hacohen Guy, 2019, POWER CURRICULUM LEA; He Kaiming, 2016, CPVR; Ilg E., 2017, CVPR; Jiang L., 2018, ICML; Jiang Lu, 2014, ACM INT C MULT; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kumar M.P., 2010, NIPS; Lee Yong Jae, 2011, CVPR; Li Siyang, 2017, MULTIPLE INSTANCE CU; Li Wen, 2017, ARXIV170802862; Liang Jian, 2016, SIGIR; Liu W., 2016, ECCV; Pi T., 2016, IJCAI; Ren M., 2018, ICML; Santoro A., 2016, ICML; Simonyan Karen, 2015, INT C LEARN REPR; Spitkovsky V. I., 2009, NIPS; Supancic III J., 2013, CVPR; Svetlik M., 2017, AAAI; Tang Kevin, 2012, ADV NEURAL INFORM PR, P647; Verleysen Michel, 2013, IEEE T NEURAL NETWOR; Wang J., 2018, ICPR; Weinshall Daphna, 2018, CURRICULUM LEARNING; Weinshall Daphna, 2018, THEORY CURRICULUM LE; Wu Lijun, 2018, NIPS; Xiao T., 2015, CVPR; Zagoruyko S., 2016, BMVC; Zhang C., 2017, ICLR; Zhao Q., 2015, AAAI; Zhou H.Y., 2017, ICCV	41	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902069
C	Schroder, C; James, B; Lagnado, L; Berens, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Schroeder, Cornelius; James, Ben; Lagnado, Leon; Berens, Philipp			Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The inherent noise of neural systems makes it difficult to construct models which accurately capture experimental measurements of their activity. While much research has been done on how to efficiently model neural activity with descriptive models such as linear-nonlinear-models (LN), Bayesian inference for mechanistic models has received considerably less attention. One reason for this is that these models typically lead to intractable likelihoods and thus make parameter inference difficult. Here, we develop an approximate Bayesian inference scheme for a fully stochastic, biophysically inspired model of glutamate release at the ribbon synapse, a highly specialized synapse found in different sensory systems. The model translates known structural features of the ribbon synapse into a set of stochastically coupled equations. We approximate the posterior distributions by updating a parametric prior distribution via Bayesian updating rules and show that model parameters can be efficiently estimated for synthetic and experimental data from in vivo two-photon experiments in the zebrafish retina. Also, we find that the model captures complex properties of the synaptic release such as the temporal precision and outperforms a standard GLM. Our framework provides a viable path forward for linking mechanistic models of neural activity to measured data.	[Schroeder, Cornelius; Berens, Philipp] Univ Tubingen, Inst Ophthalm Res, Tubingen, Germany; [James, Ben; Lagnado, Leon] Univ Sussex, Sch Life Sci, Brighton, E Sussex, England	Eberhard Karls University of Tubingen; Eberhard Karls University Hospital; University of Sussex	Schroder, C (corresponding author), Univ Tubingen, Inst Ophthalm Res, Tubingen, Germany.	cornelius.schroeder@uni-tuebingen.de; bmjame02@gmail.com; l.lagnado@sussex.ac.uk; philipp.berens@uni-tuebingen.de	Schröder, Cornelius/AAY-3776-2021; Berens, Philipp/A-5653-2009	Berens, Philipp/0000-0002-0199-4727	German Ministry of Education and Research (BMBF) [01GQ1601, 01IS18052C, 01IS18039A]; German Research Foundation [BE5601/4-1, EXC 2064, 390727645]; European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant [674901]; Wellcome Trust [102905/Z/13/Z]	German Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); German Research Foundation(German Research Foundation (DFG)); European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant(SKA South Africa); Wellcome Trust(Wellcome TrustEuropean Commission)	We thank Sofie-Helene Seibel for help in the experiments and genetics for this project and for the BC image in Fig. 4A, Christian Behrens for providing the PR/BC schema in Fig. 1B, Jan Lause for his detailed feedback on the manuscript. The study was funded by the German Ministry of Education and Research (BMBF, 01GQ1601, 01IS18052C and 01IS18039A) and the German Research Foundation (BE5601/4-1, EXC 2064, project number 390727645). In addition, this project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 674901 and the Wellcome Trust (Investigator Award 102905/Z/13/Z).	Aljadeff J, 2016, NEURON, V91, P221, DOI 10.1016/j.neuron.2016.05.039; [Anonymous], 2017, P 31 INT C NEURAL IN; Avissar M., 2007, J NEUROSCIENCE; Baden T, 2014, PLOS BIOL, V12, DOI 10.1371/journal.pbio.1001972; Baden T, 2013, TRENDS NEUROSCI, V36, P480, DOI 10.1016/j.tins.2013.04.006; Darnet Lea, 2019, NATURE NEUROSCIENCE; Franke K, 2017, NATURE, V542, P439, DOI 10.1038/nature21394; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Greenberg D., 2019, INT C MACH LEARN PML, P2404; Guo R, 2014, CRIT CARE MED, V42; Hisakado M, 2006, J PHYS A-MATH GEN, V39, P15365, DOI 10.1088/0305-4470/39/50/005; Holt Matthew, 2004, CURRENT BIOL; Lagnado L, 2015, ANNU REV VIS SCI, V1, P235, DOI 10.1146/annurev-vision-082114-035709; LoGiudice L, 2009, NEUROSCIENTIST, V15, P380, DOI 10.1177/1073858408331373; Marvin Jonathan S., 2018, NATURE METHODS; Matthews G, 2010, NAT REV NEUROSCI, V11, P812, DOI 10.1038/nrn2924; Murphy Kevin P., 2007, TECHNICAL REPORT; Murray I., 2018, P 22 INT C ART INT S; Ozuysal Y, 2012, NEURON, V73, P1002, DOI 10.1016/j.neuron.2011.12.029; Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0; Papamakarios G, 2016, ADV NEURAL INFORM PR, V29, P1028; Peterson A. J., 2014, J NEUROSCIENCE; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Real E, 2017, CURR BIOL, V27, P189, DOI 10.1016/j.cub.2016.11.040; SCHNAPF JL, 1990, J PHYSIOL-LONDON, V427, P681, DOI 10.1113/jphysiol.1990.sp018193; Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X; Sikora MA, 2005, J NEUROSCI METH, V145, P47, DOI 10.1016/j.jneumeth.2004.11.023; Singer Joshua H., 2004, NATURE NEUROSCIENCE; Sterling P, 2005, TRENDS NEUROSCI, V28, P20, DOI 10.1016/j.tins.2004.11.009; Von Gersdorff Henrique, 1996, NEURON; VONGERSDORFF H, 1994, NATURE, V367, P735, DOI 10.1038/367735a0; Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319; Zenisek D., 2000, NATURE	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307012
C	Schumann, C; Lang, Z; Foster, JS; Dickerson, JP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Schumann, Candice; Lang, Zhi; Foster, Jeffrey S.; Dickerson, John P.			Making the Cut: A Bandit-based Approach to Tiered Interviewing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Given a huge set of applicants, how should a firm allocate sequential resume screenings, phone interviews, and in-person site visits? In a tiered interview process, later stages (e.g., in-person visits) are more informative, but also more expensive than earlier stages (e.g., resume screenings). Using accepted hiring models and the concept of structured interviews, a best practice in human resources, we cast tiered hiring as a combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting. The goal is to select a subset of arms (in our case, applicants) with some combinatorial structure. We present new algorithms in both the probably approximately correct (PAC) and fixed-budget settings that select a near-optimal cohort with provable guarantees. We show via simulations on real data from one of the largest US-based computer science graduate programs that our algorithms make better hiring decisions or use less budget than the status quo.	[Schumann, Candice; Lang, Zhi; Dickerson, John P.] Univ Maryland, College Pk, MD 20742 USA; [Foster, Jeffrey S.] Tufts Univ, Medford, MA 02155 USA	University System of Maryland; University of Maryland College Park; Tufts University	Schumann, C (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	schumann@cs.umd.edu; zlang@cs.umd.edu; jfoster@cs.tufts.edu; john@cs.umd.edu			NSF IIS RI CAREER Award [1846237]	NSF IIS RI CAREER Award	Schumann and Dickerson were supported by NSF IIS RI CAREER Award #1846237. We thank Google for gift support, University of Maryland professors David Jacobs and Ramani Duraiswami for helpful input, and the anonymous reviewers for helpful comments.	Angwin J., 2016, MACHINE BIAS; Ashkan A., 2015, IJCAI; Breaugh James A, 2000, J MANAGEMENT; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cao Wei, 2015, NEURIPS; Chamberlain Andrew, 2017, LONG DOES IT TAKE HI; Chen Shouyuan, 2014, NEURIPS; Desrochers Pierre, 2001, GROWTH CHANGE; Ding Wenkui, 2013, AAAL; Freedman R., 2018, AAAI; Harris Michael M, 1989, PERSONAL PSYCHOL; Haussler David, 1993, FDN KNOWLEDGE ACQUIS; Hunt V., 2015, DIVERSITY MATTERS; Jain Shweta, 2014, INCENTIVE COMPATIBLE; Jun Kwang-Sung, 2016, AISTATS; Katz-Samuels Julian, 2019, AISTATS; Katz-Samuels Julian, 2018, ICML; Kent J. D, 2016, HOLISTIC REV GRADUAT; Krause A, 2014, TRACTABILITY, P71; Levashina Julia, 2014, PERSONNEL PSYCHOL; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Noothigattu Ritesh, 2018, AAAI; Posthuma Richard A, 2002, PERSONAL PSYCHOL; Radlinski F, 2008, P 25 INT C MACH LEAR, DOI DOI 10.1145/1390156.1390255; Schmader Toni, 2007, SEX ROLES; Schumann Candice, 2019, AAMAS; Singla Adish, 2015, AAAI; Singla Adish, 2015, HCOMP; Society for Human Resource Management, 2016, HUM CAP BENCHM REP; Vardarlier Pelin, 2014, SOCIAL BEHAV SCI; Xia Y., 2016, PROC 25 INT JOINT C, P2210; Yue Yisong, 2011, NEURIPS	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304062
C	Sealfon, A; Ullman, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sealfon, Adam; Ullman, Jonathan			Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We give a simple, computationally efficient, and node-differentially-private algorithm for estimating the parameter of an Erdos-Renyi graph-that is, estimating p in a G(n; p)-with near-optimal accuracy. Our algorithm nearly matches the information-theoretically optimal exponential-time algorithm for the same problem due to Borgs et al. (FOCS 2018). More generally, we give an optimal, computationally efficient, private algorithm for estimating the edge-density of any graph whose degree distribution is concentrated in a small interval.	[Sealfon, Adam] MIT, Cambridge, MA 02139 USA; [Sealfon, Adam] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Ullman, Jonathan] Northeastern Univ, Boston, MA 02115 USA	Massachusetts Institute of Technology (MIT); University of California System; University of California Berkeley; Northeastern University	Sealfon, A (corresponding author), MIT, Cambridge, MA 02139 USA.; Sealfon, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	asealfon@berkeley.edu; jullman@ccs.neu.edu			NSF MACS [CNS-1413920]; DARPA/NJIT Palisade [491512803]; Sloan/NJIT [996698]; MIT/IBM [W1771646]; NSF [CCF-1718088, CCF-1750640, CNS-1816028]	NSF MACS; DARPA/NJIT Palisade; Sloan/NJIT; MIT/IBM(International Business Machines (IBM)); NSF(National Science Foundation (NSF))	Part of this work was done while the authors were visiting the Simons Institute for the Theory of Computing. AS is supported by NSF MACS CNS-1413920, DARPA/NJIT Palisade 491512803, Sloan/NJIT 996698, and MIT/IBM W1771646. JU is supported by NSF grants CCF-1718088, CCF-1750640, and CNS-1816028. The authors are grateful to Adam Smith for helpful discussions.	Blocki J., 2013, P 4 C INNOVATIONS TH, P87; Blocki J, 2012, ANN IEEE SYMP FOUND, P410, DOI 10.1109/FOCS.2012.67; Borgs C, 2018, ANN IEEE SYMP FOUND, P533, DOI 10.1109/FOCS.2018.00057; Bun M., 2019, SMOOTH SENSITI UNPUB; Bun M, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1, DOI 10.1145/2591796.2591877; Canonne C. L., 2019, ARXIV190511947; Cummings R., 2018, ARXIV180408645; Dinur I., 2003, P 22 ACM SIGMOD SIGA, P202, DOI DOI 10.1145/773153.773173; Dwork C, 2008, LECT NOTES COMPUT SC, V5157, P469, DOI 10.1007/978-3-540-85174-5_26; Dwork C, 2007, ACM S THEORY COMPUT, P85, DOI 10.1145/1250790.1250804; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, ANN IEEE SYMP FOUND, P650, DOI 10.1109/FOCS.2015.46; Gupta A, 2012, LECT NOTES COMPUT SC, V7194, P339, DOI 10.1007/978-3-642-28914-9_19; Hay M, 2009, IEEE DATA MINING, P169, DOI 10.1109/ICDM.2009.11; Homer N, 2008, PLOS GENET, V4, DOI 10.1371/journal.pgen.1000167; Karwa V, 2016, ANN STAT, V44, P87, DOI 10.1214/15-AOS1358; Karwa V, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2611523; Kasiviswanathan SP, 2013, LECT NOTES COMPUT SC, V7785, P457, DOI 10.1007/978-3-642-36594-2_26; Kasiviswanathan SP, 2010, ACM S THEORY COMPUT, P775; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Raskhodnikova S, 2016, ANN IEEE SYMP FOUND, P495, DOI 10.1109/FOCS.2016.60; Xiao Q, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P911, DOI 10.1145/2623330.2623642	22	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303072
C	Sebbouh, O; Gazagnadou, N; Jelassi, S; Bach, F; Gower, RM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sebbouh, Othmane; Gazagnadou, Nidham; Jelassi, Samy; Bach, Francis; Gower, Robert M.			Towards closing the gap between the theory and practice of SVRG	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Amongst the very first variance reduced stochastic methods for solving the empirical risk minimization problem was the SVRG method [13]. SVRG is an inner-outer loop based method, where in the outer loop a reference full gradient is evaluated, after which m 2 N steps of an inner loop are executed where the reference gradient is used to build a variance reduced estimate of the current gradient. The simplicity of the SVRG method and its analysis have lead to multiple extensions and variants for even non-convex optimization. Yet there is a significant gap between the parameter settings that the analysis suggests and what is known to work well in practice. Our first contribution is that we take several steps towards closing this gap. In particular, the current analysis shows that m should be of the order of the condition number so that the resulting method has a favorable complexity. Yet in practice m = n works well regardless of the condition number, where n is the number of data points. Furthermore, the current analysis shows that the inner iterates have to be reset using averaging after every outer loop. Yet in practice SVRG works best when the inner iterates are updated continuously and not reset. We provide an analysis of these aforementioned practical settings and show that they achieve the same favorable complexity as the original analysis (with slightly better constants). Our second contribution is to provide a more general analysis than had been previously done by using arbitrary sampling, which allows us to analyse virtually all forms of mini-batching through a single theorem. Since our setup and analysis reflect what is done in practice, we are able to set the parameters such as the mini-batch size and step size using our theory in such a way that produces a more efficient algorithm in practice, as we show in extensive numerical experiments.	[Sebbouh, Othmane; Gazagnadou, Nidham; Gower, Robert M.] Telecom Paris, Inst Polytech Paris, LTCI, Paris, France; [Jelassi, Samy] Princeton Univ, ORFE Dept, Princeton, NJ 08544 USA; [Bach, Francis] PSL Res Univ, INRIA Ecole Normale Super, Paris, France	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Princeton University; UDICE-French Research Universities; PSL Research University Paris	Sebbouh, O (corresponding author), Telecom Paris, Inst Polytech Paris, LTCI, Paris, France.	othmane.sebbouh@gmail.com; nidham.gazagnadou@telecom-paris.fr; sjelassi@princeton.edu; francis.bach@inria.fr; robert.gower@telecom-paris.fr	Gower, Robert Mansel/Y-8838-2019	Gower, Robert Mansel/0000-0002-2268-9780	DIM Math Innov Region Ile-de-France [ED574 - FMJH]; LabEx LMH [ANR-11-LABX-0056-LMH]	DIM Math Innov Region Ile-de-France; LabEx LMH	RMG acknowledges the support by grants from DIM Math Innov Region Ile-de-France (ED574 - FMJH), reference ANR-11-LABX-0056-LMH, LabEx LMH.	Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Asuncion A, 2007, UCI MACHINE LEARNING; Babanezhad Harikandeh R., 2015, ADV NEURAL INFORM PR, V28, P2251; Bubeck S., 2015, FDN TRENDS MACHINE L; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Gazagnadou N, 2019, PR MACH LEARN RES, V97; Gower Robert M, 2018, ARXIV180502632; Hofmann T., 2015, ADV NEURAL INFORM PR, V28, P2305; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Kone~cny J., 2017, FRONT APPL MATH STAT, V3, P1; Konecny J, 2016, IEEE J-STSP, V10, P242, DOI 10.1109/JSTSP.2015.2505682; Kovalev Dmitry, 2019, ARXIV190108689; Kulunchakov A., 2019, P INT C MACH LEARN, P3541; Murata Tomoya, 2017, ADV NEURAL INFORM PR, P608; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2008, AUTOMATICA, V44, P1559, DOI 10.1016/j.automatica.2008.01.017; Nguyen LM, 2017, PR MACH LEARN RES, V70; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Raj A., ARXIV180500982; Reddi SJ, 2016, PR MACH LEARN RES, V48; Robbins H., 1985, HERBERT ROBBINS SELE, P111; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300059
C	Seff, A; Zhou, WD; Damani, F; Doyle, A; Adams, RP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Seff, Ari; Zhou, Wenda; Damani, Farhan; Doyle, Abigail; Adams, Ryan P.			Discrete Object Generation with Reversible Inductive Construction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GRAPHS; DISCOVERY; RIGIDITY	The success of generative modeling in continuous domains has led to a surge of interest in generating discrete data such as molecules, source code, and graphs. However, construction histories for these discrete objects are typically not unique and so generative models must reason about intractably large spaces in order to learn. Additionally, structured discrete domains are often characterized by strict constraints on what constitutes a valid object and generative models must respect these requirements in order to produce useful novel samples. Here, we present a generative model for discrete objects employing a Markov chain where transitions are restricted to a set of local operations that preserve validity. Building off of generative interpretations of denoising autoencoders, the Markov chain alternates between producing 1) a sequence of corrupted objects that are valid but not from the data distribution, and 2) a learned reconstruction distribution that attempts to fix the corruptions while also preserving validity. This approach constrains the generative model to only produce valid objects, requires the learner to only discover local modifications to the objects, and avoids marginalization over an unknown and potentially large space of construction histories. We evaluate the proposed approach on two highly structured discrete domains, molecules and Laman graphs, and find that it compares favorably to alternative methods at capturing distributional statistics for a host of semantically relevant metrics.	[Seff, Ari; Damani, Farhan; Doyle, Abigail; Adams, Ryan P.] Princeton Univ, Princeton, NJ 08544 USA; [Zhou, Wenda] Columbia Univ, New York, NY USA	Princeton University; Columbia University	Seff, A (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	aseff@princeton.edu; wz2335@columbia.edu; fdamani@princeton.edu; agdoyle@princeton.edu; rpa@princeton.edu	Zhou, Wenda/AFF-2109-2022	Zhou, Wenda/0000-0001-5549-7884	Alfred P. Sloan Foundation; NSF [IIS-1421780]; DataX Program at Princeton University from the Schmidt Futures Foundation; Department of Defense through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program; NIH Research Facility Improvement Grant [1G20RR030893-01]; New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) [C090171]	Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); NSF(National Science Foundation (NSF)); DataX Program at Princeton University from the Schmidt Futures Foundation; Department of Defense through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program; NIH Research Facility Improvement Grant(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR)	We would like to thank Wengong Jin, Michael Galvin, Dieterich Lawson, and members of the Princeton Laboratory for Intelligent Probabilistic Systems for valuable discussion and feedback. This work was partially funded by the Alfred P. Sloan Foundation, NSF IIS-1421780, and the DataX Program at Princeton University through support from the Schmidt Futures Foundation. AS was supported by the Department of Defense through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program. We acknowledge computing resources from Columbia University's Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010.	Alain Guillaume, 2016, GSNS GENERATIVE STOC; Bengio Y., 2013, P 26 INT C NEUR INF, P899; Bettig B, 2011, J COMPUT INF SCI ENG, V11, DOI 10.1115/1.3593408; Brown N, 2019, J CHEM INF MODEL, V59, P1096, DOI 10.1021/acs.jcim.8b00839; Comer John, 2007, LIPOPHILICITY PROFIL, P275; Dai H., 2018, SYNTAX DIRECTED VARI; DeCao N., 2018, ICML WORKSH THEOR FD; Duvenaud David K, 2015, P NIPS; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Ertl P, 2009, J CHEMINFORMATICS, V1, DOI 10.1186/1758-2946-1-8; GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683; Gilmer J, 2017, PR MACH LEARN RES, V70; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Haas R, 2005, COMP GEOM-THEOR APPL, V31, P31, DOI 10.1016/j.comgeo.2004.07.003; Henneberg L, 1911, GRAPHISCHE STATIK ST; Jacobs DJ, 1997, J COMPUT PHYS, V137, P346, DOI 10.1006/jcph.1997.5809; Janz Dave, 2018, INT C LEARN REPR; Jin WG, 2018, PR MACH LEARN RES, V80; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Kusner MJ, 2017, PR MACH LEARN RES, V70; LAMAN G, 1970, J ENG MATH, V4, P331, DOI 10.1007/BF01534980; Landrum G., 2006, RDKIT OPEN SOURCE CH; Li Y., 2018, ARXIV; Liu Q, 2018, ADV NEUR IN, V31; Mendez D, 2019, NUCLEIC ACIDS RES, V47, pD930, DOI 10.1093/nar/gky1075; MONTANAR.U, 1974, INFORM SCIENCES, V7, P95, DOI 10.1016/0020-0255(74)90008-5; Moussaoui Adel, 2016, THESIS; Murali V., 2018, INT C LEARN REPR; Preuer K, 2018, J CHEM INF MODEL, V58, P1736, DOI 10.1021/acs.jcim.8b00234; Richard Bickerton G., 2012, NATURE CHEM, V4; SCHWEITZER PJ, 1968, J APPL PROBAB, V5, P401, DOI 10.2307/3212261; Segler MHS, 2018, ACS CENTRAL SCI, V4, P120, DOI 10.1021/acscentsci.7b00512; Simonovsky Martin, 2018, ICAN; Sterling T, 2015, J CHEM INF MODEL, V55, P2324, DOI 10.1021/acs.jcim.5b00559; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; You JX, 2018, PR MACH LEARN RES, V80; Zhou LL, 2017, DESTECH TRANS COMP, P55	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902003
C	Sepehr, F; Materassi, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sepehr, Firoozeh; Materassi, Donatello			An Algorithm to Learn Polytree Networks with Hidden Nodes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LATENT; MODELS; TREES	Ancestral graphs are a prevalent mathematical tool to take into account latent (hidden) variables in a probabilistic graphical model. In ancestral graph representations, the nodes are only the observed (manifest) variables and the notion of rn-separation fully characterizes the conditional independence relations among such variables, bypassing the need to explicitly consider latent variables. However, ancestral graph models do not necessarily represent the actual causal structure of the model, and do not contain information about, for example, the precise number and location of the hidden variables. Being able to detect the presence of latent variables while also inferring their precise location within the actual causal structure model is a more challenging task that provides more information about the actual causal relationships among all the model variables, including the latent ones. In this article, we develop an algorithm to exactly recover graphical models of random variables with underlying polytree structures when the latent nodes satisfy specific degree conditions. Therefore, this article proposes an approach for the full identification of hidden variables in a polytree. We also show that the algorithm is complete in the sense that when such degree conditions are not met, there exists another polytree with fewer number of latent nodes satisfying the degree conditions and entailing the same independence relations among the observed variables, making it indistinguishable from the actual polytree.	[Sepehr, Firoozeh; Materassi, Donatello] Univ Tennessee, 1520 Middle Dr, Knoxville, TN 37996 USA	University of Tennessee System; University of Tennessee Knoxville	Sepehr, F (corresponding author), Univ Tennessee, 1520 Middle Dr, Knoxville, TN 37996 USA.	dawn@utk.edu; dmateras@utk.edu			NSF [1553504]	NSF(National Science Foundation (NSF))	This work has been partially supported by NSF (CNS CAREER #1553504).	Anandkumar A, 2013, ANN STAT, V41, P401, DOI 10.1214/12-AOS1070; Choi MJ, 2011, J MACH LEARN RES, V12, P1771; Diestel R., 2010, GRAPH THEORY, V4, DOI 10.1007/978-3-642-14279-6; Erdos PL, 1999, THEOR COMPUT SCI, V221, P77, DOI 10.1016/S0304-3975(99)00028-6; Jernite Yacine, 2013, NIPS, P2355; Koller D., 2009, PROBABILISTIC GRAPHI; Li Guangdi, 2014, WORKSH LEARN TRACT P; Materassi D, 2016, MED C CONTR AUTOMAT, P1331, DOI 10.1109/MED.2016.7535992; Mossel E, 2007, IEEE ACM T COMPUT BI, V4, P108, DOI 10.1109/TCBB.2007.1010; Mourad R, 2013, J ARTIF INTELL RES, V47, P157, DOI 10.1613/jair.3879; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Rebane G., 1987, RECOVERY CAUSAL POLY, P222; Richardson T, 2002, ANN STAT, V30, P962; Sepehr F, 2020, IEEE T AUTOMAT CONTR, V65, P1014, DOI 10.1109/TAC.2019.2915153; Spirtes R, 2000, CAUSATION PREDICTION, V81; Verma T., 1988, P 4 WORKSH UNC ART I, P352; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906073
C	Shah, V; Blanchet, J; Johari, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shah, Virag; Blanchet, Jose; Johari, Ramesh			Semi-Parametric Dynamic Contextual Pricing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Motivated by the application of real-time pricing in e-commerce platforms, we consider the problem of revenue-maximization in a setting where the seller can leverage contextual information describing the customer's history and the product's type to predict her valuation of the product. However, her true valuation is unobservable to the seller, only binary outcome in the form of success-failure of a transaction is observed. Unlike in usual contextual bandit settings, the optimal price/arm given a covariate in our setting is sensitive to the detailed characteristics of the residual uncertainty distribution. We develop a semi-parametric model in which the residual distribution is non-parametric and provide the first algorithm which learns both regression parameters and residual distribution with (O) over tilde(root n) regret. We empirically test a scalable implementation of our algorithm and observe good performance.	[Shah, Virag; Blanchet, Jose; Johari, Ramesh] Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA	Stanford University	Shah, V (corresponding author), Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA.	virag@stanford.edu; jblanche@stanford.edu; rjohari@stanford.edu			National Science Foundation [DMS-1820942, DMS-1838576, CNS-1544548, CNS-1343253]	National Science Foundation(National Science Foundation (NSF))	This work was supported in part by National Science Foundation Grants DMS-1820942, DMS-1838576, CNS-1544548, and CNS-1343253. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. We would like to thank Linjia Wu for reading and checking our proofs.	Alon N., 2013, ADV NEURAL INFORM PR, P1610; Amin K., 2014, ADV NEURAL INFORM PR, P622; Broder J, 2012, OPER RES, V60, P965, DOI 10.1287/opre.1120.1057; Caron S., 2012, P 28 C UNC ART INT U; Cohen A, 2016, PR MACH LEARN RES, V48; Cohen M. C., 2016, P 2016 ACM C EC COMP; den Boer A. V., 2015, DYNAMIC PRICING LEAR; den Boer AV, 2014, MATH OPER RES, V39, P863, DOI 10.1287/moor.2013.0636; den Boer AV, 2014, MANAGE SCI, V60, P770, DOI 10.1287/mnsc.2013.1788; Frahm G., 2004, THESIS; Greenewald K., 2017, ADV NEURAL INFORM PR, P5977; Javanmard A., 2019, J MACHINE LEARNING R; Keskin, 2019, PERSONALIZED DYNAMIC; Keskin NB, 2014, OPER RES, V62, P1142, DOI 10.1287/opre.2014.1294; Kleinberg R. D., 2003, IEEE S FDN COMP SCI; Krishnamurthy A., 2018, P 35 INT C MACH LEAR; Langford John, 2008, ADV NEURAL INFORM PR; Le Guen T., 2008, THESIS; Lykouris T, 2021, P MACHINE LEARNING R, V75, P979; Mannor S., 2011, NIPS, P684; Mao J., 2018, ADV NEURAL INFORM PR, P5648; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; NAMBIAR M, 2019, MANAGEMENT SCI; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Qiang S., 2019, DYNAMIC PRICING DEMA; Slivkins A., 2011, ANN C LEARN THEOR	27	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302037
C	Shayestehmanesh, H; Azami, S; Mehta, NA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shayestehmanesh, Hamid; Azami, Sajjad; Mehta, Nishant A.			Dying Experts: Efficient Algorithms with Optimal Regret Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study a variant of decision-theoretic online learning in which the set of experts that are available to Learner can shrink over time. This is a restricted version of the well-studied sleeping experts problem, itself a generalization of the fundamental game of prediction with expert advice. Similar to many works in this direction, our benchmark is the ranking regret. Various results suggest that achieving optimal regret in the fully adversarial sleeping experts problem is computationally hard. This motivates our relaxation where any expert that goes to sleep will never again wake up. We call this setting "dying experts" and study it in two different cases: the case where the learner knows the order in which the experts will die and the case where the learner does not. In both cases, we provide matching upper and lower bounds on the ranking regret in the fully adversarial setting. Furthermore, we present new, computationally efficient algorithms that obtain our optimal upper bounds.	[Shayestehmanesh, Hamid; Azami, Sajjad; Mehta, Nishant A.] Univ Victoria, Dept Comp Sci, Victoria, BC, Canada	University of Victoria	Shayestehmanesh, H (corresponding author), Univ Victoria, Dept Comp Sci, Victoria, BC, Canada.	hamidshayestehmanesh@uvic.ca; sajjadazami@uvic.ca; nmehta@uvic.ca			NSERC [RGPIN-2018-03942]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported by the NSERC Discovery Grant RGPIN-2018-03942.	[Anonymous], 2011, ADV NEURAL INFORM PR; Blum A, 1997, MACH LEARN, V26, P5, DOI 10.1023/A:1007335615132; Blum A, 2007, J MACH LEARN RES, V8, P1307; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chakrabarti D., 2009, ADV NEURAL INFORM PR, V21; de Rooij S, 2014, J MACH LEARN RES, V15, P1281; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Freund Yoav, 1997, P 29 ANN ACM S THEOR; Gofer E, 2013, COLT, P618; Kale S., 2016, ADV NEURAL INFORM PR, V29, P2181; Kanade V., 2014, ACM T COMPUTATION TH, V6; Kanade Varun, 2009, P INT C ART INT STAT, V5; Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7; Mourtada J., 2017, P 28 INT C ALG LEARN, V76, P517; Neu G., 2014, ADV NEURAL INFORM PR, P2780; Orabona Francesco, 2015, ARXIV151102176; TALAGRAND M, 1993, ANN PROBAB, V21, P362, DOI 10.1214/aop/1176989409; Talagrand Michel, 2005, THE GENERIC CHAINING, V154; Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901059
C	Shen, YK; Tan, S; Hosseini, A; Lin, ZH; Sordoni, A; Courville, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shen, Yikang; Tan, Shawn; Hosseini, Arian; Lin, Zhouhan; Sordoni, Alessandro; Courville, Aaron			Ordered Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LANGUAGES	Stack-augmented recurrent neural networks (RNNs) have been of interest to the deep learning community for some time. However, the difficulty of training memory models remains a problem obstructing the widespread use of such models. In this paper, we propose the Ordered Memory architecture. Inspired by Ordered Neurons (Shen et al., 2018), we introduce a new attention-based mechanism and use its cumulative probability to control the writing and erasing operation of memory. We also introduce a new Gated Recursive Cell to compose lower level representations into higher level representation. We demonstrate that our model achieves strong performance on the logical inference task (Bowman et al., 2015) and the ListOps (Nangia and Bowman, 2018) task. We can also interpret the model to retrieve the induced tree structure, and find that these induced structures align with the ground truth. Finally, we evaluate our model on the Stanford Sentiment Treebank tasks (Socher et al., 2013), and find that it performs comparatively with the state-of-the-art methods in the literature(2).	[Shen, Yikang; Tan, Shawn; Hosseini, Arian; Lin, Zhouhan; Sordoni, Alessandro; Courville, Aaron] Univ Montreal, Mila, Montreal, PQ, Canada; [Shen, Yikang; Hosseini, Arian; Sordoni, Alessandro] Microsoft Res, Montreal, PQ, Canada	Universite de Montreal	Shen, YK (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.; Shen, YK (corresponding author), Microsoft Res, Montreal, PQ, Canada.	yi-kang.shen@umontreal.ca; tanjings@mila.quebec; arian.hosseini9@gmail.com						Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; [Anonymous], 2013, PREPRINT ARXIV 1308; [Anonymous], 2018, ARXIV181009536; Ba L.J, 2016, P C WORKSH NEUR INF; Bahdanau D., 2015, P 3 INT C LEARNING R; Bowman S. R., 2016, ARXIV160306021; Bowman S. R., 2015, ARXIV150604834; Brahma Siddhartha, 2018, IMPROVED SENTENCE MO; Chelba C, 2000, COMPUT SPEECH LANG, V14, P283, DOI 10.1006/csla.2000.0147; CHEN SF, 1995, P 33 ANN M ASS COMP, P228; Choi J., 2018, P 2018 ASS ADV ART I; Cohen SB, 2011, MUSCULOSKELETAL EXAMINATION OF THE SHOULDER: MAKING THE COMPLEX SIMPLE, P52; Das S., 1992, P 14 ANN C COGN SCI, P14; Dehghani Mostafa, 2018, ARXOIV180703819; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dowty David., 2007, DIRECT COMPOSITIONAL, P23; Dyer Chris, 2016, P 2016 C N AM CHAPT, P199, DOI DOI 10.18653/V1/N16-1024; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Graves A, 2014, NEURAL TURING MACHIN; Graves A., 2016, ADAPTIVE COMPUTATION; Grefenstette E., 2015, ADV NEURAL INF PROCE, P1828; Gulcehre Caglar, 2017, ARXIV170108718; Havrylov Serhii, 2019, P NAACL HLT; Jacob AP, 2018, REPRESENTATION LEARNING FOR NLP, P154; Jernite Y., 2016, ARXIV161106188; Joulin A, 2015, ADV NEUR IN, V28; Kai Sheng Tai, 2015, ARXIV150300075; KNUTH DE, 1965, INFORM CONTROL, V8, P607, DOI 10.1016/S0019-9958(65)90426-2; Kuncoro A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1426; Lake Brenden M, 2017, ARXIV17110350; Liu Xiaodong, 2019, CORR; Looks M, 2017, ARXIV170202181; Loula Joao, 2018, ARXIV180707545; Maillard J., 2017, ARXIV170509189; Manning, 2014, ARXIV14061827; McCann Bryan, 2017, P ADV NEURAL INFORM, P6294; Mozer M. C., 1993, ADV NEURAL INFORM PR, P863; Munkhdalai T, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P11; Nangia Nikita, 2018, ARXIV180406028; Peters Matthew, 2018, DEEP CONTEXTUALIZED, P2227, DOI [10.18653/v1/N18-1202, DOI 10.18653/V1/N18-1202]; POLLACK JB, 1990, ARTIF INTELL, V46, P77, DOI 10.1016/0004-3702(90)90005-K; Radford A., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1704.01444; Roark B, 2001, COMPUT LINGUIST, V27, P249, DOI 10.1162/089120101750300526; SCHUTZENBERGER MP, 1963, INFORM CONTROL, V6, P246, DOI 10.1016/S0019-9958(63)90306-1; Shieber S. M., 1983, 21st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, P113; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Socher Richard, 2010, P NIPS 2010 DEEP LEA; Sutskever I., 2018, IMPROVING LANGUAGE U; Tan S, 2016, INT CONF ACOUST SPEE, P5965, DOI 10.1109/ICASSP.2016.7472822; Tran Ke, 2018, ARXIV180303585; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Weston J., 2014, ARXIV14103916; Williams A, 2018, T ASSOC COMPUT LING, V6, P253, DOI DOI 10.1162/TACL_A_00019; Yogatama D, 2018, MEMORY ARCHITECTURES; Yogatama Dani, 2016, ARXIV161109100; ZENG Z, 1994, IEEE T NEURAL NETWOR, V5, P320, DOI 10.1109/72.279194	60	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305008
C	Shi, JH; Shea-Brown, E; Buice, MA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shi, Jianghong; Shea-Brown, Eric; Buice, Michael A.			Comparison Against Task Driven Artificial Neural Networks Reveals Functional Organization of Mouse Visual Cortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPECIALIZATION; MODELS	Partially inspired by features of computation in visual cortex, deep neural networks compute hierarchical representations of their inputs. While these networks have been highly successful in machine learning, it remains unclear to what extent they can aid our understanding of cortical function. Several groups have developed metrics that provide a quantitative comparison between representations computed by networks and representations measured in cortex. At the same time, neuroscience is well into an unprecedented phase of large-scale data collection, as evidenced by projects such as the Allen Brain Observatory. Despite the magnitude of these efforts, in a given experiment only a fraction of units are recorded, limiting the information available about the cortical representation. Moreover, only a finite number of stimuli can be shown to an animal over the course of a realistic experiment. These limitations raise the question of how and whether metrics that compare representations of deep networks are meaningful on these datasets. Here, we empirically quantify the capabilities and limitations of these metrics due to limited image presentations and neuron samples. We find that the comparison procedure is robust to different choices of stimuli set and the level of subsampling that one might expect in a large-scale brain survey with thousands of neurons. Using these results, we compare the representations measured in the Allen Brain Observatory in response to natural image presentations to deep neural network. We show that the visual cortical areas are relatively high order representations (in that they map to deeper layers of convolutional neural networks). Furthermore, we see evidence of a broad, more parallel organization rather than a sequential hierarchy, with the primary area VISp (V1) being lower order relative to the other areas.	[Shi, Jianghong; Shea-Brown, Eric] Univ Washington, Dept Appl Math, Seattle, WA 98195 USA; [Buice, Michael A.] Allen Inst Brain Sci, Seattle, WA 98109 USA	University of Washington; University of Washington Seattle; Allen Institute for Brain Science	Shi, JH (corresponding author), Univ Washington, Dept Appl Math, Seattle, WA 98195 USA.	jhshi@uw.edu; etsb@uw.edu; michaelbu@alleninstitute.org			NIH Graduate training grant in neural computation and engineering [R90DA033461]	NIH Graduate training grant in neural computation and engineering(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We thank Tianqi Chen, Saskia de Vries, Michael Oliver for helpful discussions, and Rich Pang, Gabrielle Gutierrez for comments on the draft. We thank the Allen Institute for Brain Science founder, Paul G. Allen, for his vision, encouragement, and support. We acknowledge the NIH Graduate training grant in neural computation and engineering (R90DA033461).	Andermann ML, 2011, NEURON, V72, P1025, DOI 10.1016/j.neuron.2011.11.013; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755; de Vries SEJ, 2018, BIORXIV; Diedrichsen J, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005508; DORRI F, 2019, BIOSTATISTICS, V2, DOI DOI 10.1038/S42003-019-0291-Z; Goodfellow I., 2016, DEEP LEARNING; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Harris Julie A, 2018, BIORXIV; Henaff Olivier J., 2019, NATURE NEUROSCIENCE; Jewell S, 2018, ANN APPL STAT, V12, P2457, DOI 10.1214/18-AOAS1162; Jiang DY, 2017, J AM HEART ASSOC, V6, DOI 10.1161/JAHA.117.005537; Khaligh-Razavi Seyed-Mahdi, 2014, PLOS COMPUTATIONAL B, V10, P11; Kornblith Simon, 2019, P 36 INT C MACH LEAR, V97, P3519; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Marshel JH, 2011, NEURON, V72, P1040, DOI 10.1016/j.neuron.2011.12.004; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Morcos A., 2018, ADV NEUR INF PROC SY, V31, P5732; Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321; Prusky GT, 2000, VISION RES, V40, P2201, DOI 10.1016/S0042-6989(00)00081-X; Raghu M., 2017, ADV NEURAL INFORM PR, V30, P6076; Simonyan K., 2015, ARXIV PREPRINT ARXIV; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305073
C	Shi, WS; Yu, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shi, Weishi; Yu, Qi			Integrating Bayesian and Discriminative Sparse Kernel Machines for Multi-class Active Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel active learning (AL) model that integrates Bayesian and discriminative kernel machines for fast and accurate multi-class data sampling. By joining a sparse Bayesian model and a maximum margin machine under a unified kernel machine committee (KMC), the proposed model is able to identify a small number of data samples that best represent the overall data space while accurately capturing the decision boundaries. The integration is conducted using the maximum entropy discrimination framework, resulting in a joint objective function that contains generalized entropy as a regularizer. Such a property allows the proposed AL model to choose data samples that more effectively handle non-separable classification problems. Parameter learning is achieved through a principled optimization framework that leverages convex duality and sparse structure of KMC to efficiently optimize the joint objective function. Key model parameters are used to design a novel sampling function to choose data samples that can simultaneously improve multiple decision boundaries, making it an effective sampler for problems with a large number of classes. Experiments conducted over both synthetic and real data and comparison with competitive AL methods demonstrate the effectiveness of the proposed model.	[Shi, Weishi; Yu, Qi] Rochester Inst Technol, Rochester, NY 14623 USA	Rochester Institute of Technology	Shi, WS (corresponding author), Rochester Inst Technol, Rochester, NY 14623 USA.	ws7586@rit.edu; qi.yu@rit.edu			NSF IIS award [IIS-1814450]; ONR [N00014-18-1-2875]	NSF IIS award(National Science Foundation (NSF)); ONR(Office of Naval Research)	This research was supported in part by an NSF IIS award IIS-1814450 and an ONR award N00014-18-1-2875. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency.	Andersen M. S., 2013, CVXOPT PYTHON PACKAG; Bishop C.M, 2006, PATTERN RECOGN; Dudik M, 2007, J MACH LEARN RES, V8, P1217; Faul AC, 2002, ADV NEUR IN, V14, P383; Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881; Jaakkola T, 2000, ADV NEUR IN, V12, P470; Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310; Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627; Kottke D, 2016, FRONT ARTIF INTEL AP, V285, P586, DOI 10.3233/978-1-61499-672-9-586; Mac Aodha O, 2014, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2014.79; Mandel MI, 2006, MULTIMEDIA SYST, V12, P3, DOI 10.1007/s00530-006-0032-2; MOSEK ApS, 2019, MOSEK OPT API PYTH 8; Roy Nicholas, 2001, P 18 INT C MACH LEAR, P441; Shi WS, 2018, IEEE DATA MINING, P1230, DOI 10.1109/ICDM.2018.00164; Shi Weishi, 2019, SOURCE CODE DATA; Silva C, 2007, ICMLA 2007: SIXTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P130; Tong S., 2001, PROC ACM INT C MULTI, V9, P107; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414; Yang BS, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P917; Zhang T, 2004, ANN STAT, V32, P56; Zhu J, 2009, J MACH LEARN RES, V10, P2531	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302030
C	Shi, YG; Siddharth, N; Paige, B; Torr, PHS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shi, Yuge; Siddharth, N.; Paige, Brooks; Torr, Philip H. S.			Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning generative models that span multiple data modalities, such as vision and language, is often motivated by the desire to learn more useful, generalisable representations that faithfully capture common underlying factors between the modalities. In this work, we characterise successful learning of such models as the fulfilment of four criteria: i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration. Here, we propose a mixture-of-experts multimodal variational autoencoder (MMVAE) to learn generative models on different sets of modalities, including a challenging image <-> language dataset, and demonstrate its ability to satisfy all four criteria, both qualitatively and quantitatively. Code, data, and models are provided at this url.	[Shi, Yuge; Siddharth, N.; Torr, Philip H. S.] Univ Oxford, Dept Engn Sci, Oxford, England; [Paige, Brooks] Alan Turing Inst, London, England; [Paige, Brooks] Univ Cambridge, Cambridge, England	University of Oxford; University of Cambridge	Shi, YG (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	yshi@robots.ox.ac.uk; nsid@robots.ox.ac.uk; bpaige@turing.ac.uk; philip.torr@eng.ox.ac.uk			ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC [Seebibyte EP/M013774/1]; EPSRC/MURI [EP/N019474/1]; Royal Academy of Engineering; FiveAI; Remarkdip through their PhD Scholarship Programme; Alan Turing Institute under the EPSRC [EP/N510129/1]	ERC(European Research Council (ERC)European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Royal Academy of Engineering(Royal Academy of Engineering - UK); FiveAI; Remarkdip through their PhD Scholarship Programme; Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	YS, NS, and PHST were supported by the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1, with further support from the Royal Academy of Engineering and FiveAI. YS was additionally supported by Remarkdip through their PhD Scholarship Programme. BP is supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1.	[Anonymous], 2017, ARXIV171202950; [Anonymous], 2014, ABS14123474 CORR; Ba J., 2017, P 3 INT C LEARN REPR; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Barsalou LW, 2008, ANNU REV PSYCHOL, V59, P617, DOI 10.1146/annurev.psych.59.103006.093639; BAUER MI, 1993, PSYCHOL SCI, V4, P372, DOI 10.1111/j.1467-9280.1993.tb00584.x; Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI [10.1145/860435.860460, DOI 10.1145/860435.860460]; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Burda Y., 2015, INT C LEARN REPR; Cremer C., 2017, INT C LEARN REPR WOR; Fan JE, 2018, COGNITIVE SCI, V42, P2670, DOI 10.1111/cogs.12676; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kartsaklis D, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P212; Le T, 2018, INT C LEARN REPR; Ledig C., 2017, PROC CVPR IEEE, P4681, DOI [10.1109/CVPR.2017.19, DOI 10.1109/CVPR.2017.19]; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Lipton ZC, 2018, COMMUN ACM, V61, P36, DOI 10.1145/3233231; Liu M.-Y., 2019, FEW SHOT UNSUPERVISE; LIU MY, 2017, P ASM 36 INT C OC; Long M., 2015, P 32 INT C MACH LEAR, V1, P97; Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136; Massiceti D., 2018, IEEE C COMP VIS PATT; Massiceti Daniela, 2018, NEURIPS WORKSH CRIT; Mathieu E, 2019, PR MACH LEARN RES, V97; Mukherjee T., 2017, CORR; Ngiam J, 2011, P 28 INT C MACH LEAR, V28, P689, DOI DOI 10.5555/3104482.3104569; Pandey G, 2017, IEEE IJCNN, P308, DOI 10.1109/IJCNN.2017.7965870; Pham Ngoc-Quan, 2016, P 2016 C EMP METH NA, P1153, DOI DOI 10.18653/V1/D16-1123; Pu Y., 2016, ADV NEURAL INF PROCE, P2352; Quiroga RQ, 2009, CURR BIOL, V19, P1308, DOI 10.1016/j.cub.2009.06.060; Rainforth T., 2018, INT C MACH LEARN ICM; Reddi S.J., 2018, INT C LEARN REPR; Robert C., 2013, MONTE CARLO STAT MET; Roeder G, 2017, ADV NEUR IN, V30; Silberer C, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P721; SISKIND JM, 1995, ARTIF INTELL REV, V8, P371, DOI 10.1007/BF00849726; Sohn Kihyuk, 2015, NEURAL INFORM PROCES; Stein BE, 2009, HEARING RES, V258, P4, DOI 10.1016/j.heares.2009.03.012; Suzuki M., 2017, INT C LEARN REPR WOR; Taigman Y, 2017, INT C LEARN REPR; Tian Y., 2019, ARXIV190208261; Tsai Y. H., 2019, INT C LEARN REPR; Tucker George, 2019, INT C LEARN REPR; Vedantam R., 2018, 6 INT C LEARN REPR I; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang Weiran, 2016, 161003454 ARXIV; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Wu M., 2018, P 32 INT C NEUR INF, P5580; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yildirim Ilker, 2014, PERCEPTION CONCEPTIO; Zhao S., 2017, ARXIV170208658; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	56	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907038
C	Simeoni, M; Kashani, S; Hurley, P; Vetterli, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Simeoni, Matthieu; Kashani, Sepand; Hurley, Paul; Vetterli, Martin			DeepWave: A Recurrent Neural-Network for Real-Time Acoustic Imaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INVERSE PROBLEMS; FRAMEWORK	We propose a recurrent neural-network for real-time reconstruction of acoustic camera spherical maps. The network, dubbed DeepWave, is both physically and algorithmically motivated: its recurrent architecture mimics iterative solvers from convex optimisation, and its parsimonious parametrisation is based on the natural structure of acoustic imaging problems. Each network layer applies successive filtering, biasing and activation steps to its input, which can be interpreted as generalised deblurring and sparsification steps. To comply with the irregular geometry of spherical maps, filtering operations are implemented efficiently by means of graph signal processing techniques. Unlike commonly-used imaging network architectures, DeepWave is moreover capable of directly processing the complex-valued raw microphone correlations, learning how to optimally back-project these into a spherical map. We propose moreover a smart physically-inspired initialisation scheme that attains much faster training and higher performance than random initialisation. Our real-data experiments show DeepWave has similar computational speed to the state-of-the-art delay-and-sum imager with vastly superior resolution. While developed primarily for acoustic cameras, DeepWave could easily be adapted to neighbouring signal processing fields, such as radio astronomy, radar and sonar.	[Simeoni, Matthieu] IBM Zurich Res Lab, Zurich, Switzerland; [Simeoni, Matthieu; Kashani, Sepand; Vetterli, Martin] Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland; [Hurley, Paul] Western Sydney Univ, Sydney, NSW, Australia	International Business Machines (IBM); Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Western Sydney University	Simeoni, M (corresponding author), IBM Zurich Res Lab, Zurich, Switzerland.	meo@zurich.ibm.com; sepand.kashani@epfl.ch; paul.hurley@westernsydney.edu.au; martin.vetterli@epfl.ch	Simeoni, Matthieu/S-5427-2016	Simeoni, Matthieu/0000-0002-4927-3697; Hurley, Paul/0000-0001-8736-5843	Swiss National Science Foundation [200021 181978/1]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	Matthieu Simeoni and Sepand Kashani have contributed equally to this work. Sepand Kashani was in part supported by the Swiss National Science Foundation grant number 200021 181978/1, "SESAM - Sensing and Sampling: Theory and Algorithms".	Baydin AG, 2018, J MACH LEARN RES, V18; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Benesty J, 2008, SPRINGER TOP SIGN PR, V1, P1; Besson Adrien, 2019, IEEE T COMPUTATIONAL; Bezzam E, 2017, INT CONF ACOUST SPEE, P6591, DOI 10.1109/ICASSP.2017.8005297; BIEMOND J, 1990, P IEEE, V78, P856, DOI 10.1109/5.53403; Brooks TF, 2006, J SOUND VIB, V294, P856, DOI 10.1016/j.jsv.2005.12.046; Brusniak L, 2006, AIAA PAPER 2006 2715; Chambolle A, 2015, J OPTIMIZ THEORY APP, V166, P968, DOI 10.1007/s10957-015-0746-4; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Chu N, 2014, APPL ACOUST, V76, P197, DOI 10.1016/j.apacoust.2013.08.007; Chu ZG, 2014, MECH SYST SIGNAL PR, V48, P404, DOI 10.1016/j.ymssp.2014.03.012; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Foucart S., 2017, B AM MATH SOC, V54, P151; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Gupta H, 2018, IEEE T MED IMAGING, V37, P1440, DOI 10.1109/TMI.2018.2832656; Hansen RK, 1996, ACOUST IMAG, V22, P607; Hardin Doug P, 2016, DOLOMITES RES NOTES, V9; Hurley P, 2017, INT CONF ACOUST SPEE, P3380, DOI 10.1109/ICASSP.2017.7952783; Hurley P, 2016, INT CONF ACOUST SPEE, P2877, DOI 10.1109/ICASSP.2016.7472203; Jean Matthieu Martin, 2019, TECHNICAL REPORT; Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099; JINADASA KG, 1988, LINEAR ALGEBRA APPL, V101, P73, DOI 10.1016/0024-3795(88)90143-7; Jones Charles H, 1975, US Patent, Patent No. [3,898,608, 3898608]; Kashani Sepand, 2019, OPTIMIZATION NOTES, P6; Kim K, 2005, IEE P-RADAR SON NAV, V152, P263, DOI 10.1049/ip-rsn:20045015; Krim H., 1996, IEEE SIGNAL PROCESSI; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li H., 2018, P MACHINE LEARNING R, V95, P614; Liang Jingwei, 2018, ARXIV180704005; Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391; Meyer A., 2006, P BEBEC, P1; Michel V., 2013, AMC, V10, P12; Pan Hanjie, 2017, LABEX WIFI REFERENCE; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Perdios D, 2017, IEEE INT ULTRA SYM; Perraudin Nathanael, 2019, ASTRONOMY COMPUTING; Rafaely B., 2015, FUNDAMENTALS SPHERIC, V8, DOI 10.1007/978-3-319-99561-8; Robin Scheibler, 2018, PYRAMIC DATASET 48 C; Romberg J, 2008, IEEE SIGNAL PROC MAG, V25, P14, DOI 10.1109/MSP.2007.914729; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Scheibler R, 2018, INT WORKSH ACOUSTIC, P226; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Sijtsma Pieter, 2007, International Journal of Aeroacoustics, V6, P357, DOI 10.1260/147547207783359459; Sun J., 2016, ADV NEUR IN, P10; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; van der Veen A.- J., 2013, HDB SIGNAL PROCESSIN, P421; Wiaux Y, 2009, MON NOT R ASTRON SOC, V395, P1733, DOI 10.1111/j.1365-2966.2009.14665.x; Wijnholds SJ, 2008, IEEE J-STSP, V2, P613, DOI 10.1109/JSTSP.2008.2004216; Xu L., 2014, INT C NEUR INF PROC, V27, P1790; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	57	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907001
C	Singh, G; Ganvir, R; Puschel, M; Vechev, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Singh, Gagandeep; Ganvir, Rupanshu; Puschel, Markus; Vechev, Martin			Beyond the Single Neuron Convex Barrier for Neural Network Certification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new parametric framework, called k-ReLU, for computing precise and scalable convex relaxations used to certify neural networks. The key idea is to approximate the output of multiple ReLUs in a layerjointly instead of separately. This joint relaxation captures dependencies between the inputs to different ReLUs in a layer and thus overcomes the convex barrier imposed by the single neuron triangle relaxation and its approximations. The framework is parametric in the number of k ReLUs it considers jointly and can be combined with existing verifiers in order to improve their precision. Our experimental results show that k-ReLU enables significantly more precise certification than existing state-of-the-art verifiers while maintaining scalability.	[Singh, Gagandeep; Ganvir, Rupanshu; Puschel, Markus; Vechev, Martin] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Singh, G (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	gsingh@inf.ethz.ch; rganvir@student.ethz.ch; pueschel@inf.ethz.ch; martin.vechev@inf.ethz.ch						[Anonymous], 2018, ERAN ETH ROBUSTNESS; [Anonymous], 2018, PYCDDLIB; [Anonymous], INT C LEARN REPR ICL; Balunovic M., 2019, ADV NEURAL INFORM PR, P15287; Boopathy Akhilan, 2019, AAAI C ART INT AAAI; Bunel R., 2018, ADV NEURAL INFORM PR, P4795; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cousot P., 1978, POPL, P84; Dvijotham K., 2018, P UNC ART INT UAI, P162; Dvijotham K. D., 2019, P UNC ART INT UAI, P164; Ehlers Ruediger, 2017, AUTOMATED TECHNOLOGY; Fukuda K, 2001, COMP GEOM-THEOR APPL, V20, P13, DOI 10.1016/S0925-7721(01)00032-3; Gehr T., 2018, 2018 IEEE S SEC PRIV, P948; Gurobi Optimization LLC, 2018, GUROBI OPTIMIZER REF; Katz G., 2017, COMPUTER AIDED VER 1; Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P6; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Madry Aleksander, 2018, P INT C LEARN REPR I; Mirman M, 2018, PR MACH LEARN RES, V80; Mohapatra J., 2019, VERIFYING ROBUSTNESS; Raghunathan A., 2018, NEURIPS; Ruan W., 2018, P INT JOINT C ART IN; Salman Hadi, 2019, CORR; Singh G., 2019, PACMPL, V3; Singh G., 2019, INT C LEARNING REPRE; Singh G., 2017, ACM SIGPLAN NOTICES; Singh G., 2018, P ADV NEUR INF PROC, P10825; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Wang S., 2018, ADV NEURAL INFORM PR; Weng L., 2018, PROC INT C MACH LEAR, V80, P5276; Wong Eric, 2017, ARXIV PREPRINT ARXIV; Yang Y., 2019, CORRECTNESS VERIFICA; Zhang H., 2018, P ADV NEUR INF PROC	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906072
C	Song, L; Li, YW; Li, ZM; Yu, G; Sun, HB; Sun, J; Zheng, NN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Lin; Li, Yanwei; Li, Zeming; Yu, Gang; Sun, Hongbin; Sun, Jian; Zheng, Nanning			Learnable Tree Filter for Structure-preserving Feature Transform	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning discriminative global features plays a vital role in semantic segmentation. And most of the existing methods adopt stacks of local convolutions or non-local blocks to capture long-range context. However, due to the absence of spatial structure preservation, these operators ignore the object details when enlarging receptive fields. In this paper, we propose the learnable tree filter to form a generic tree filtering module that leverages the structural property of minimal spanning tree to model long-range dependencies while preserving the details. Furthermore, we propose a highly efficient linear-time algorithm to reduce resource consumption. Thus, the designed modules can be plugged into existing deep neural networks conveniently. To this end, tree filtering modules are embedded to formulate a unified framework for semantic segmentation. We conduct extensive ablation studies to elaborate on the effectiveness and efficiency of the proposed method. Specifically, it attains better performance with much less overhead compared with the classic PSP block and Non-local operation under the same backbone. Our approach is proved to achieve consistent improvements on several benchmarks without bells-and-whistles. Code and models are available at https://github.com/StevenGrove/TreeFilter-Torch.	[Song, Lin; Sun, Hongbin; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China; [Li, Yanwei] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China; [Li, Yanwei] Univ Chinese Acad Sci, Beijing, Peoples R China; [Li, Zeming; Yu, Gang; Sun, Jian] Megvii Inc Face, Beijing, Peoples R China	Xi'an Jiaotong University; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Sun, HB (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China.	stevengrove@stu.xjtu.edu.cn; liyanwei2017@ia.ac.cn; lizeming@megvii.com; yugang@megvii.com; hsun@mail.xjtu.edu.cn; sunjian@megvii.com; nnzheng@mail.xjtu.edu.cn	li, yan/GTI-4638-2022	Yu, Gang/0000-0001-5570-2710	National Key R&D Program of China [2017YFA0700800]	National Key R&D Program of China	We would like to thank Lingxi Xie for his valuable suggestions. This research was supported by the National Key R&D Program of China (No. 2017YFA0700800).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chandra S, 2017, IEEE I CONF COMP VIS, P5113, DOI 10.1109/ICCV.2017.546; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Everingham Mark, 2010, IJCV; GALLAGER R, 1983, ACM T PROGRAMMING LA; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Harley AW, 2017, IEEE I CONF COMP VIS, P5048, DOI 10.1109/ICCV.2017.539; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Li YM, 2019, PROC CVPR IEEE, P8712, DOI 10.1109/CVPR.2019.00892; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Liang Xiaodan, 2018, IEEE C COMP VIS PATT; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu SF, 2017, ADV NEUR IN, V30; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo WJ, 2016, ADV NEUR IN, V29; Peng C, 2017, PROC CVPR IEEE, P682, DOI 10.1109/CVPR.2017.80; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Xie XF, 2015, IEEE INT VAC ELECT C; Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388; Yang Qingxiong, 2015, IEEE T PATTERN ANAL; Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199; Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35; Zhang, 2018, EUR C COMP VIS; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhao Hengshuang, 2018, EUR C COMP VIS; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301067
C	Song, Z; Wang, RS; Yang, LF; Zhang, HY; Zhong, PL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Zhao; Wang, Ruosong; Yang, Lin F.; Zhang, Hongyang; Zhong, Peilin			Efficient Symmetric Norm Regression via Linear Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We provide efficient algorithms for overconstrained linear regression problems with size n x d when the loss function is a symmetric norm (a norm invariant under sign-flips and coordinate-permutations). An important class of symmetric norms are Orlicz norms, where for a function G and a vector y is an element of R-n, the corresponding Orlicz norm parallel to y parallel to(G) is defined as the unique value a such that Sigma(n)(i=1) G(vertical bar y(i)vertical bar/alpha) = 1. When the loss function is an Orlicz norm, our algorithm produces a (1 + epsilon) - approximate solution for an arbitrarily small constant epsilon > 0 in input-sparsity time, improving over the previously best-known algorithm which produces a d " polylog n -approximate solution. When the loss function is a general symmetric norm, our algorithm produces a root d . polylog n . mmc(l)-approximate solution in input-sparsity time, where mmc(l) is a quantity related to the symmetric norm under consideration. To the best of our knowledge, this is the first input-sparsity time algorithm with provable guarantees for the general class of symmetric norm regression problem. Our results shed light on resolving the universal sketching problem for linear regression, and the techniques might be of independent interest to numerical linear algebra problems more broadly.	[Song, Zhao] Univ Washington, Seattle, WA 98195 USA; [Wang, Ruosong] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Yang, Lin F.] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [Zhang, Hongyang] Toyota Technol Inst, Chicago, IL USA; [Zhong, Peilin] Columbia Univ, New York, NY 10027 USA	University of Washington; University of Washington Seattle; Carnegie Mellon University; University of California System; University of California Los Angeles; Toyota Technological Institute - Chicago; Columbia University	Song, Z (corresponding author), Univ Washington, Seattle, WA 98195 USA.	magic.linuxkde@gmail.com; ruosongw@andrew.cmu.edu; linyang@ee.ucla.edu; hongyanz@ttic.edu; pz2225@columbia.edu			NSF [IIS-1763562, CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955, CCF-1740833]; Simons Foundation [491119]; Google Research Award; Google Ph.D. fellowship; Office of Naval Research (ONR) [N00014-18-1-2562, N00014-18-1-2861]; Nvidia NVAIL award	NSF(National Science Foundation (NSF)); Simons Foundation; Google Research Award(Google Incorporated); Google Ph.D. fellowship(Google Incorporated); Office of Naval Research (ONR)(Office of Naval Research); Nvidia NVAIL award	P. Zhong is supported in part by NSF grants (CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955 and CCF-1740833), Simons Foundation (#491119), Google Research Award and a Google Ph.D. fellowship. R. Wang is supported in part by NSF grant IIS-1763562, Office of Naval Research (ONR) grants (N00014-18-1-2562, N00014-18-1-2861), and Nvidia NVAIL award. Part of this work was done while Z. Song, L. F. Yang, H. Zhang and P. Zhong were interns at IBM Research - Almaden and while Z. Song, R. Wang and H. Zhang were visiting the Simons Institute for the Theory of Computing. Z. Song and P. Zhong would like to thank Alexandr Andoni, Kenneth L. Clarkson, Yin Tat Lee, Eric Price, Clifford Stein and David P. Woodruff for insight discussions.	Ailon N., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P557, DOI 10.1145/1132516.1132597; Andoni A, 2018, ANN IEEE SYMP FOUND, P159, DOI 10.1109/FOCS.2018.00024; Andoni A, 2017, ACM S THEORY COMPUT, P902, DOI 10.1145/3055399.3055418; Andoni Alexandr, 2018, P MACHINE LEARNING R, P224; Argyriou A., 2012, ADV NEURAL INFORM PR; Auerbach H., 1930, THESIS; Blasiok J, 2017, ACM S THEORY COMPUT, P716, DOI 10.1145/3055399.3055424; Branden P., 2018, ARXIV180903255; Braverman V, 2016, PODS'16: PROCEEDINGS OF THE 35TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P261, DOI 10.1145/2902251.2902282; Braverman V, 2010, ACM S THEORY COMPUT, P281; Clarkson Kenneth, 2019, P MACHINE LEARNING R, P1262; Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Clarkson Kenneth L, 2015, P 26 ANN ACM SIAM S, P921; Cohen MB, 2015, ACM S THEORY COMPUT, P183, DOI 10.1145/2746539.2746567; Cohen Michael B, 2016, P 27 ANN ACM SIAM S, P278; Dadush D., 2018, FOCS; Dasgupta A, 2009, SIAM J COMPUT, V38, P2060, DOI 10.1137/070696507; Indyk Piotr, 2005, P 37 ANN ACM S THEOR; Lee Y.T., 2018, ARXIV181206243; Li J., 2019, COLT; LI M, 2013, FDN COMPUTER SCI, P127, DOI DOI 10.1109/FOCS.2013.22; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; McDonald A. M., 2014, PROC 27 INT C NEURAL, P3644; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Nakos V., 2019, ROBUST SPARSE UNPUB; Nelson J, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P101; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Sohler C, 2011, ACM S THEORY COMPUT, P755; Song Zhao, 2019, NEURIPS; Wang R., 2019, SODA, P1825; Wojtaszczyk P., 1996, BANACH SPACES ANAL, V25; Woodruff D., 2013, C LEARN THEOR, P546; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Yang J., 2013, INT C MACH LEARN, P881	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300075
C	Song, Z; Woodruff, DP; Zhong, PL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Zhao; Woodruff, David P.; Zhong, Peilin			Average Case Column Subset Selection for Entrywise l(1)-Norm Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ROBUST; APPROXIMATION; ALGORITHMS	We study the column subset selection problem with respect to the entrywise l(1)-norm loss. It is known that in the worst case, to obtain a good rank-k approximation to a matrix, one needs an arbitrarily large n(Omega(1)) number of columns to obtain a (1 + epsilon)-approximation to the best entrywise l(1)-norm low rank approximation of an n x n matrix. Nevertheless, we show that under certain minimal and realistic distributional settings, it is possible to obtain a (1+epsilon)-approximation with a nearly linear running time and poly(k/epsilon) + O(k log n) columns. Namely, we show that if the input matrix A has the form A = B + E, where B is an arbitrary rank-k matrix, and E is a matrix with i.i.d. entries drawn from any distribution mu for which the (1 + gamma)-th moment exists, for an arbitrarily small constant 7 > 0, then it is possible to obtain a (1 + epsilon)-approximate column subset selection to the entrywise l(1)-norm in nearly linear time. Conversely we show that if the first moment does not exist, then it is not possible to obtain a (1 + epsilon)-approximate subset selection algorithm even if one chooses any n(o(1)) columns. This is the first algorithm of any kind for achieving a (1 + epsilon)-approximation for entrywise l(1)-norm loss low rank approximation.	[Song, Zhao] Univ Washington, Seattle, WA 98195 USA; [Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zhong, Peilin] Columbia Univ, New York, NY 10027 USA	University of Washington; University of Washington Seattle; Carnegie Mellon University; Columbia University	Song, Z (corresponding author), Univ Washington, Seattle, WA 98195 USA.	magic.linuxkde@gmail.com; dwoodruf@cs.cmu.edu; pz2225@columbia.edu			Office of Naval Research (ONR) [N00014- 18-1-2562]; NSF [CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955, CCF-1740833]; Simons Foundation [491119]; Google Research Award; Google Ph.D. fellowship	Office of Naval Research (ONR)(Office of Naval Research); NSF(National Science Foundation (NSF)); Simons Foundation; Google Research Award(Google Incorporated); Google Ph.D. fellowship(Google Incorporated)	David P. Woodruff was supported in part by Office of Naval Research (ONR) grant N00014- 18-1-2562. Part of this work was done while he was visiting the Simons Institute for the Theory of Computing. Peilin Zhong is supported in part by NSF grants (CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955 and CCF-1740833), Simons Foundation (#491119 to Alexandr Andoni), Google Research Award and a Google Ph.D. fellowship. Part of this work was done while Zhao Song and Peilin Zhong were interns at IBM Research - Almaden and while Zhao Song was visiting the Simons Institute for the Theory of Computing.	Asuncion A, 2007, UCI MACHINE LEARNING; Ban Frank, 2019, SODA; Bourgain J, 2015, ACM S THEORY COMPUT, P499, DOI 10.1145/2746539.2746541; Brooks JP, 2013, COMPUT STAT DATA AN, V61, P83, DOI 10.1016/j.csda.2012.11.007; Brooks JP, 2013, APPL MATH LETT, V26, P51, DOI 10.1016/j.aml.2012.03.031; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chierichetti Flavio, 2017, ICML; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen Michael B, 2016, P 27 ANN ACM SIAM S, P278; Dasgupta A, 2009, SIAM J COMPUT, V38, P2060, DOI 10.1137/070696507; Gillis Nicolas, 2015, ARXIV150909236; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Jot Sapan, 2012, PCAL1 IMPLEMENTATION; Ke Q., 2003, CMUCS03172; Ke QF, 2005, PROC CVPR IEEE, P739; Kim E, 2015, IEEE T NEUR NET LEAR, V26, P237, DOI 10.1109/TNNLS.2014.2312535; Kwak N, 2008, IEEE T PATTERN ANAL, V30, P1672, DOI 10.1109/TPAMI.2008.114; Latala R, 1997, ANN PROBAB, V25, P1502; MANDELBROT B, 1960, INT ECON REV, V1, P79, DOI 10.2307/2525289; Markopoulos P. P., 2016, ARXIV E PRINTS; Markopoulos PP, 2014, IEEE T SIGNAL PROCES, V62, P5046, DOI 10.1109/TSP.2014.2338077; Markopoulos Panos P., 2013, P 10 INT S WIR COMM, P1; MAURER A., 2003, J INEQUAL PURE APPL, V4, P6; Meng D., 2013, P AAAI C ARTIFICIAL, P704; Meng Xiangrui, P 45 ANN ACM S THEOR, P91; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Park Young Woong, 2016, ARXIV160902997; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Song Z., 2017, P 49 ANN S THEOR COM; Song Zhao, 2019, ARXIV191001788; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901071
C	Steeg, GV; Harutyunyan, H; Moyer, D; Galstyan, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Steeg, Greg Ver; Harutyunyan, Hrayr; Moyer, Daniel; Galstyan, Aram			Fast structure learning with modular regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INDEPENDENT COMPONENT ANALYSIS; INFORMATION; SELECTION	Estimating graphical model structure from high-dimensional and undersampled data is a fundamental problem in many scientific fields. Existing approaches, such as GLASSO, latent variable GLASSO, and latent tree models, suffer from high computational complexity and may impose unrealistic sparsity priors in some cases. We introduce a novel method that leverages a newly discovered connection between information-theoretic measures and structured latent factor models to derive an optimization objective which encourages modular structures where each observed variable has a single latent parent. The proposed method has linear stepwise computational complexity w.r.t. the number of observed variables. Our experiments on synthetic data demonstrate that our approach is the only method that recovers modular structure better as the dimensionality increases. We also use our approach for estimating covariance structure for a number of real-world datasets and show that it consistently outperforms state-of-the-art estimators at a fraction of the computational cost. Finally, we apply the proposed method to high-resolution fMRI data (with more than 10(5) voxels) and show that it is capable of extracting meaningful patterns.	[Steeg, Greg Ver; Harutyunyan, Hrayr; Moyer, Daniel; Galstyan, Aram] Univ Southern Calif, Inst Informat Sci, Marina Del Rey, CA 90292 USA	University of Southern California	Steeg, GV (corresponding author), Univ Southern Calif, Inst Informat Sci, Marina Del Rey, CA 90292 USA.	gregv@isi.edu; hrayrh@isi.edu; moyerd@usc.edu; galstyan@isi.edu	Thompson, Paul M/C-4194-2018	Thompson, Paul M/0000-0002-4720-8867	USC Annenberg Fellowship; DARPA [W911NF-16-1-0575, W911NF-17-C-0011]; Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) [2016-16041100004]	USC Annenberg Fellowship; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA)	We thank Andrey Lokhov, Marc Vuffray, and Seyoung Yun for valuable conversations about this work and we thank anonymous reviews whose comments have greatly improved this manuscript. H. Harutyunyan is supported by USC Annenberg Fellowship. This work is supported in part by DARPA via W911NF-16-1-0575 and W911NF-17-C-0011, and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via 2016-16041100004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing-the official policies, either expressed or implied, of DARPA, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.	Anderson T.W., 1956, P 3 BERKELEY S MATH, V5th ed., P111; [Anonymous], 2013, NIPS; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Cattell R.B., 1952, FACTOR ANAL INTRO MA; Chandrasekaran V., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1610, DOI 10.1109/ALLERTON.2010.5707106; Chandrasekaran V, 2012, ANN STAT, V40, P1935, DOI 10.1214/11-AOS949; Choi MJ, 2011, J MACH LEARN RES, V12, P1771; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Fan JQ, 2008, J ECONOMETRICS, V147, P186, DOI 10.1016/j.jeconom.2008.09.017; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Gao Shuyang, 2018, ARXIV180205822; Hsieh CJ, 2014, J MACH LEARN RES, V15, P2911; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; Kingma D.P, P 3 INT C LEARNING R; Kummerfeld E, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1655, DOI 10.1145/2939672.2939838; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Li QF, 2018, J AM STAT ASSOC, V113, P380, DOI 10.1080/01621459.2016.1256815; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Liu Y., 2013, ADV NEURAL INFORM PR; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Meng ZS, 2014, PR MACH LEARN RES, V32, P1269; Misra Sidhant, 2017, ARXIV170304886; Monti MM, 2007, NEUROIMAGE, V37, P1005, DOI 10.1016/j.neuroimage.2007.04.069; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Poldrack RA, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms9885; Steeg Greg Ver, 2015, ARTIFICIAL INTELLIGE; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tishby Naftali, 2000, PHYSICS0004057 ARXIV; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Veld Giel Op't, 2016, P 50 ANN C INF SYST; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang W, 2010, IEEE INT SYMP INFO, P1373, DOI 10.1109/ISIT.2010.5513573; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066; WYNER AD, 1975, IEEE T INFORM THEORY, V21, P163, DOI 10.1109/TIT.1975.1055346; Zhang NL, 2017, AAAI CONF ARTIF INTE, P4891; Zhang Nevin L, 2017, 31 AAAI C ART INT; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907027
C	Stirn, A; Jebara, T; Knowles, DA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Stirn, Andrew; Jebara, Tony; Knowles, David A.			A New Distribution on the Simplex with Auto-Encoding Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We construct a new distribution for the simplex using the Kumaraswamy distribution and an ordered stick-breaking process. We explore and develop the theoretical properties of this new distribution and prove that it exhibits symmetry (exchangeability) under the same conditions as the well-known Dirichlet. Like the Dirichlet, the new distribution is adept at capturing sparsity but, unlike the Dirichlet, has an exact and closed form reparameterization-making it well suited for deep variational Bayesian modeling. We demonstrate the distribution's utility in a variety of semi-supervised auto-encoding tasks. In all cases, the resulting models achieve competitive performance commensurate with their simplicity, use of explicit probability models, and abstinence from adversarial training.	[Stirn, Andrew; Jebara, Tony; Knowles, David A.] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA; [Stirn, Andrew; Knowles, David A.] New York Genome Ctr, New York, NY 10013 USA; [Jebara, Tony] Spotify Technol SA, Stockholm, Sweden; [Knowles, David A.] Columbia Univ, Data Sci Inst, New York, NY USA	Columbia University; Spotify; Columbia University	Stirn, A (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.; Stirn, A (corresponding author), New York Genome Ctr, New York, NY 10013 USA.	andrew.stirn@cs.columbia.edu; ebara@cs.columbia.edu; daknowles@cs.columbia.edu			NSF [III-1526914]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF grant III-1526914.	[Anonymous], ICLR 2016; Berthelot D., 2019, ARXIV190502249; Chen X, 2016, ADV NEUR IN, V29; Figurnov Mikhail, 2018, ADV NEURAL INFORM PR, V31, P441; Frigyik BA., 2010, TECHNICAL REPORT; Hazewinkel Michiel, 1990, ENCY MATH, V6; Hinz Tobias, 2018, CORR; HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196; Jankowiak M, 2018, PR MACH LEARN RES, V80; Ji X., 2018, CORR; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kumaraswamy Ponnambalam, 1980, J HYDROLOGY; Laine Samuli, 2016, CORR; Makhzani A, 2016, INT C LEARN REPR ICL; Makhzani A., 2017, P ADV NEUR INF PROC, P1975; Naesseth Christian, 2017, P 20 INT C ART INT S; Nalisnick Eric, 2016, WORKSH BAYES DEEP LE; Nalisnick Eric, 2017, INT C LEARN REPR ICL; Rasmus A., 2015, CORR; Ruiz Francisco R, 2016, ADV NEURAL INFORM PR, P460; Springenberg J. T., 2016, INT C LEARN REPR ICL; Srivastava Akash, 2017, INT C LEARN REPR ICL; Tarvainen Antti, 2017, CORR, Vabs/1703; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905035
C	Sun, J; Chen, TY; Giannakis, GB; Yang, ZY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Jun; Chen, Tianyi; Giannakis, Georgios B.; Yang, Zaiyue			Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The present paper develops a novel aggregated gradient approach for distributed machine learning that adaptively compresses the gradient communication. The key idea is to first quantize the computed gradients, and then skip less informative quantized gradient communications by reusing outdated gradients. Quantizing and skipping result in 'lazy' worker-server communications, which justifies the term Lazily Aggregated Quantized gradient that is henceforth abbreviated as LAQ. Our LAQ can provably attain the same linear convergence rate as the gradient descent in the strongly convex case, while effecting major savings in the communication overhead both in transmitted bits as well as in communication rounds. Empirically, experiments with real data corroborate a significant communication reduction compared to existing gradient- and stochastic gradient-based algorithms.	[Sun, Jun] Zhejiang Univ, Hangzhou 310027, Peoples R China; [Chen, Tianyi] Rensselaer Polytech Inst, Troy, NY 12180 USA; [Giannakis, Georgios B.] Univ Minnesota, Minneapolis, MN 55455 USA; [Yang, Zaiyue] Southern Univ Sci & Technol, Shenzhen 518055, Peoples R China	Zhejiang University; Rensselaer Polytechnic Institute; University of Minnesota System; University of Minnesota Twin Cities; Southern University of Science & Technology	Sun, J (corresponding author), Zhejiang Univ, Hangzhou 310027, Peoples R China.	sunjun16sj@gmail.com; chent18@rpi.edu; georgios@umn.edu; yangzy3@sustc.edu.cn	Yang, Zaiyue/AHB-2796-2022		Shenzhen Committee on Science and Innovations [GJHZ20180411143603361]; Department of Science and Technology of Guangdong Province [2018A050506003]; Natural Science Foundation of China [61873118]; China Scholarship Council; NSF [1500713, 1711471]	Shenzhen Committee on Science and Innovations; Department of Science and Technology of Guangdong Province; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Scholarship Council(China Scholarship Council); NSF(National Science Foundation (NSF))	This work by J. Sun and Z. Yang is supported in part by the Shenzhen Committee on Science and Innovations under Grant GJHZ20180411143603361, in part by the Department of Science and Technology of Guangdong Province under Grant 2018A050506003, and in part by the Natural Science Foundation of China under Grant 61873118. The work by J. Sun is also supported by China Scholarship Council. The work by G. Giannakis is supported in part by NSF 1500713, and 1711471.	Alistarh D., 2017, ADV NEURAL INF PROCE, P1709; Alistarh D., 2018, ADV NEURAL INFORM PR, P5973; [Anonymous], P C INT SPEECH COMM; Arjevani Y., 2015, ADV NEURAL INFORM PR, V28, P1756; Bernstein J, 2018, PR MACH LEARN RES, V80; Chen T., P ADV NIPS MONTR QC, P5050; Chen Tianyi, 2019, IEEE T AUTOMAT UNPUB; Gurbuzbalaban M, 2017, SIAM J OPTIMIZ, V27, P1035, DOI 10.1137/15M1049695; Heafield, 2017, P C EMP METH NAT LAN; Jiang P, 2018, ADV NEUR IN, V31; Jin R., 2019, P INT C MACH LEARN L; Jordan M. I., 2018, J AM STAT ASS; Kamp M., 2018, P EUR C MACH LEARN K, P393; Karimireddy SP, 2019, PR MACH LEARN RES, V97; Konecny J., 2018, FRONT APPL MATH STAT, V4, P62; Koneny J., 2016, 161005492 ARXIV; LeCun Y., 2010, MNIST HANDWRITTEN DI, V2, P18; Li JL, 2017, INT CONF MACH LEARN, P35; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; Lian X, 2017, P ADV NEUR INF PROC, P5330; Magnusson S., 2019, ARXIV190211163; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Msechu EJ, 2012, IEEE T SIGNAL PROCES, V60, P400, DOI 10.1109/TSP.2011.2171686; Nedic A, 2018, P IEEE, V106, P953, DOI 10.1109/JPROC.2018.2817461; Peterson L. L., 2007, COMPUTER NETWORKS SY; Seide F., 2014, P C INT SPEECH COMM; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Stich S. U., 2018, P ADV NEUR INF PROC, P4447; TakuY, 2019, 190109269 ARXIV; Wang H., 2018, ADV NEURAL INFORM PR; Wang Jianyu, 2018, 180807576 ARXIV; Wangni J., 2018, P C NEUR INF PROC SY, P1299; Wen W., 2017, P NIPS, P1509; Wu J., 2018, ARXIV180608054; Zhang S., 2015, NEURAL INFORM PROCES, P685	36	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303037
C	Sun, RX; Linderman, SW; Kinsella, IA; Paninski, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Ruoxi; Linderman, Scott W.; Kinsella, Ian August; Paninski, Liam			Scalable Bayesian inference of dendritic voltage via spatiotemporal recurrent state space models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent advances in optical voltage sensors have brought us closer to a critical goal in cellular neuroscience: imaging the full spatiotemporal voltage on a dendritic tree. However, current sensors and imaging approaches still face significant limitations in SNR and sampling frequency; therefore statistical denoising methods remain critical for understanding single-trial spatiotemporal dendritic voltage dynamics. Previous denoising approaches were either based on an inadequate linear voltage model or scaled poorly to large trees. Here we introduce a scalable fully Bayesian approach. We develop a generative nonlinear model that requires few parameters per dendritic compartment but is nonetheless flexible enough to sample realistic spatiotemporal data. The model captures potentially different dynamics in each compartment and leverages biophysical knowledge to constrain intra- and inter-compartmental dynamics. We obtain a full posterior distribution over spatiotemporal voltage via an efficient augmented block-Gibbs sampling algorithm. The nonlinear smoother model outperforms previously developed linear methods, and scales to much larger systems than previous methods based on sequential Monte Carlo approaches.	[Sun, Ruoxi; Kinsella, Ian August; Paninski, Liam] Columbia Univ, New York, NY 10027 USA; [Linderman, Scott W.] Stanford Univ, Stanford, CA 94305 USA	Columbia University; Stanford University	Sun, RX (corresponding author), Columbia Univ, New York, NY 10027 USA.			Linderman, Scott/0000-0002-3878-9073	ONR; Simons Foundation; NIH [1U01NS103489-01, 1U19NS113201, EB22913]	ONR(Office of Naval Research); Simons Foundation; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We gratefully acknowledge support from the ONR and Simons Foundation, and from NIH grants 1U01NS103489-01, 1U19NS113201, and EB22913.	Abdelfattah AS, 2019, SCIENCE, V365, P699, DOI 10.1126/science.aav6416; Adam Y, 2019, NATURE, V569, P413, DOI 10.1038/s41586-019-1166-7; Allen Institute for Brain Science, 2015, ALL CELL TYP DAT; [Anonymous], 2017, P 31 INT C NEURAL IN; Cajal S.R., 1911, HISTOLOGIE SYSTEME N, V2, P891, DOI [10.5962/bhl.title.48637, DOI 10.5962/BHL.TITLE.48637]; Chamberland S, 2017, ELIFE, V6, DOI 10.7554/eLife.25690; Hochbaum DR, 2014, NAT METHODS, V11, P825, DOI [10.1038/NMETH.3000, 10.1038/nmeth.3000]; HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717; Huys QJM, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000379; Kannan M, 2018, NAT METHODS, V15, P1108, DOI 10.1038/s41592-018-0188-7; Koch C., 2004, BIOPHYSICS COMPUTATI; Le T, 2018, INT C LEARN REPR; Linderman Scott W., 2017, P 20 INT C ART INT S; Maddison C. J., 2017, ADV NEURAL INFORM PR, P6573; Marshall JD, 2016, CELL, V167, P1650, DOI 10.1016/j.cell.2016.11.021; Meng L, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/6/065006; Miyazawa H, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23906-1; Naesseth CA, 2018, PR MACH LEARN RES, V84; Nassar J., 2019, INT C LEARN REPR ICL; O'Leary T, 2013, P NATL ACAD SCI USA, V110, pE2645, DOI 10.1073/pnas.1309966110; Paninski L, 2010, J COMPUT NEUROSCI, V28, P211, DOI 10.1007/s10827-009-0200-4; Piatkevich KD, 2019, NATURE, V574, P413, DOI 10.1038/s41586-019-1641-1; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; SONTAG ED, 1981, IEEE T AUTOMAT CONTR, V26, P346, DOI 10.1109/TAC.1981.1102596; Stuart G, 2016, DENDRITES; Tuckwell H. C., 1988, INTRO THEORETICAL NE, V2; Windle J., 2014, ARXIV14050506	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901076
C	Syrgkanis, V; Lei, V; Oprescu, M; Hei, MG; Battocchi, K; Lewis, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Syrgkanis, Vasilis; Lei, Victor; Oprescu, Miruna; Hei, Maggie; Battocchi, Keith; Lewis, Greg			Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIABLE ESTIMATION	We consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. Such settings arise in A/B tests with an intent-to-treat structure, where the experimenter randomizes over which user will receive a recommendation to take an action, and we are interested in the effect of the downstream action. We develop a statistical learning approach to the estimation of heterogeneous effects, reducing the problem to the minimization of an appropriate loss function that depends on a set of auxiliary models (each corresponding to a separate prediction task). The reduction enables the use of all recent algorithmic advances (e.g. neural nets, forests). We show that the estimated effect model is robust to estimation errors in the auxiliary models, by showing that the loss satisfies a Neyman orthogonality criterion. Our approach can be used to estimate projections of the true effect model on simpler hypothesis spaces. When these spaces are parametric, then the parameter estimates are asymptotically normal, which enables construction of confidence sets. We applied our method to estimate the effect of membership on downstream webpage engagement on TripAdvisor, using as an instrument an intent-to-treat A/B test among 4 million TripAdvisor users, where some users received an easier membership sign-up process. We also validate our method on synthetic data and on public datasets for the effects of schooling on income.	[Syrgkanis, Vasilis; Oprescu, Miruna; Hei, Maggie; Battocchi, Keith; Lewis, Greg] Microsoft Res, Redmond, WA 98052 USA; [Lei, Victor] TripAdvisor, Needham, MA USA	Microsoft	Syrgkanis, V (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	vasy@microsoft.com; vlei@tripadvisor.com; moprescu@microsoft.com; Maggie.Hei@microsoft.com; kebatt@microsoft.com; glewis@microsoft.com						Abadie A, 2003, J ECONOMETRICS, V113, P231, DOI 10.1016/S0304-4076(02)00201-4; Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1; Aronow PM, 2013, POLIT ANAL, V21, P492, DOI 10.1093/pan/mpt013; Athey S., 2017, ARXIV PREPRINT ARXIV; Athey S, 2019, ANN STAT, V47, P1148, DOI 10.1214/18-AOS1709; Card D, 2001, ECONOMETRICA, V69, P1127, DOI 10.1111/1468-0262.00237; Card D., 1993, TECHNICAL REPORT; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Chernozhukov Victor, 2018, ARXIV180604823; Chernozhukov Victor, 2018, ECONOMET J, V21, P1; Foster D.J., 2019, ARXIV PREPRINT ARXIV; Hartford J, 2017, PR MACH LEARN RES, V70; Hudson J, 2011, ECON LETT, V113, P112, DOI 10.1016/j.econlet.2011.05.053; IMBENS GW, 1994, ECONOMETRICA, V62, P467, DOI 10.2307/2951620; Kiinzel S6ren R, 2017, ARXIV170603461; Klein TJ, 2010, J ECONOMETRICS, V155, P99, DOI 10.1016/j.jeconom.2009.08.006; Lundberg S.M., 2017, ADV NEUR IN, P4765; Lundberg SM, 2018, CONSISTENT INDIVIDUA; Newey WK, 2003, ECONOMETRICA, V71, P1565, DOI 10.1111/1468-0262.00459; Nie X., 2017, ARXIV171204912; Okui R, 2012, STAT SINICA, V22, P173, DOI 10.5705/ss.2009.265; Wang JF, 2015, 2015 10TH INTERNATIONAL CONFERENCE ON BROADBAND AND WIRELESS COMPUTING, COMMUNICATION AND APPLICATIONS (BWCCA 2015), P120, DOI 10.1109/BWCCA.2015.49	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906081
C	Tagasovska, N; Ackerer, D; Vatter, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tagasovska, Natasa; Ackerer, Damien; Vatter, Thibault			Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce the vine copula autoencoder (VCAE), a flexible generative model for high-dimensional distributions built in a straightforward three-step procedure. First, an autoencoder (AE) compresses the data into a lower dimensional representation. Second, the multivariate distribution of the encoded data is estimated with vine copulas. Third, a generative model is obtained by combining the estimated distribution with the decoder part of the AE. As such, the proposed approach can transform any already trained AE into a flexible generative model at a low computational cost. This is an advantage over existing generative models such as adversarial networks and variational AEs which can be difficult to train and can impose strong assumptions on the latent space. Experiments on MNIST, Street View House Numbers and Large-Scale CelebFaces Attributes datasets show that VCAEs can achieve competitive results to standard baselines.	[Tagasovska, Natasa] HEC Lausanne, Dept Informat Syst, Lausanne, Switzerland; [Ackerer, Damien] Swissquote Bank, Gland, Switzerland; [Vatter, Thibault] Columbia Univ, Dept Stat, New York, NY USA	Columbia University	Tagasovska, N (corresponding author), HEC Lausanne, Dept Informat Syst, Lausanne, Switzerland.	natasa.tagasovska@unil.ch; damien.ackerer@swissquote.ch; thibault.vatter@columbia.edu						Aas K, 2016, ECONOMETRICS, V4, DOI 10.3390/econometrics4040043; Aas K, 2009, INSUR MATH ECON, V44, P182, DOI 10.1016/j.insmatheco.2007.02.001; Bedford T, 2002, ANN STAT, V30, P1031; Bedford T, 2001, ANN MATH ARTIF INTEL, V32, P245, DOI 10.1023/A:1016725902970; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bouchacourt D., 2018, AAAI; BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918; Brechmann EC, 2012, CAN J STAT, V40, P68, DOI 10.1002/cjs.10141; Brechmann EC, 2015, J MULTIVARIATE ANAL, V138, P19, DOI 10.1016/j.jmva.2015.02.012; Brechmann EC, 2014, COMPUT STAT DATA AN, V77, P233, DOI 10.1016/j.csda.2014.03.002; Brechmann EC, 2013, J STAT SOFTW, V52; Brechmann EC, 2015, APPL STOCH MODEL BUS, V31, P495, DOI 10.1002/asmb.2043; Chang Yale, 2016, AISTATS; Chavdarova Tatjana, 2018, CVPR; Cooke R.M., 2010, DEPEND MODEL, P37; Czado C., 2010, COPULA THEORY ITS AP, P93, DOI DOI 10.1007/978-3-642-12465-5_4; Czado C, 2012, STAT MODEL, V12, P229, DOI 10.1177/1471082X1101200302; Czado Claudia, 2012, COPULAE MATH QUANTIT, V36; Dissmann J, 2013, COMPUT STAT DATA AN, V59, P52, DOI 10.1016/j.csda.2012.08.010; Elidan G., 2013, COPULAE MATH QUANTIT, P39, DOI DOI 10.1007/978-3-642-35407-6_3; Geenens G, 2017, BERNOULLI, V23, P1848, DOI 10.3150/15-BEJ798; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grnarova Paulina, 2018, ARXIV181105512; Gulrajani I, 2017, P NIPS 2017; Haff IH, 2013, BERNOULLI, V19, P462, DOI 10.3150/12-BEJ413; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Higgins I., 2017, P INT C LEARN REPR T; Hinton G, 1994, ADV NEURAL INFORM PR, V6, DOI DOI 10.1021/jp906511z; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Joe H, 2014, DEPENDENCE MODELING; Joe H., 1997, MULTIVARIATE MODELS; Killiches Matthias, 2017, STAT COMPUT, P1; Kingma D.P, P 3 INT C LEARNING R; Kulkarni V., 2018, GENERATIVE MODELS SI; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; LeCun Y., 2010, MNIST HANDWRITTEN DI; Liang QZ, 2014, 2014 IEEE 12TH INTERNATIONAL CONFERENCE ON DEPENDABLE, AUTONOMIC AND SECURE COMPUTING (DASC)/2014 IEEE 12TH INTERNATIONAL CONFERENCE ON EMBEDDED COMPUTING (EMBEDDEDCOM)/2014 IEEE 12TH INTERNATIONAL CONF ON PERVASIVE INTELLIGENCE AND COMPUTING (PICOM), P475, DOI 10.1109/DASC.2014.91; Liu H, 2009, J MACH LEARN RES, V10, P2295; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lopez-Paz D., 2016, THESIS; Lopez-Paz David, 2016, INT C LEARN REPR ICL, DOI DOI 10.1109/PAC.2017.13.ARXIV:1511.0; Lopez-Paz David, 2013, NEURIPS; Makhzani Alireza, 2016, ICLR WORKSH; Metz L, 2016, ICLR; Nagler T., 2018, RVINECOPULIB HIGH PE; Nagler T, 2017, DEPEND MODEL, V5, P99, DOI 10.1515/demo-2017-0007; Nagler T, 2016, J MULTIVARIATE ANAL, V151, P69, DOI 10.1016/j.jmva.2016.07.003; Nagler Thomas, 2018, KDE1D UNIVARIATE KER; Nagler Thomas, 2017, VINECOPULIB HIGH PER; Nelsen RB., 2006, INTRO COPULAS, V2nd; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; PRIM RC, 1957, AT&T TECH J, V36, P1389, DOI 10.1002/j.1538-7305.1957.tb01515.x; R Development Core Team, 2018, R LANG ENV STAT COMP; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Renyi A., 1959, ACTA MATH ACAD SCI H, V10, DOI [DOI 10.1007/BF02024507, 10.1007/BF02024507]; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; ROSENBLATT M, 1952, ANN MATH STAT, V23, P470, DOI 10.1214/aoms/1177729394; Salimans T, 2016, ADV NEUR IN, V29; Scaillet Olivier, 2007, TECHNICAL REPORT; Schaling B., 2011, BOOST C LIB; Schepsmeier U, 2014, STAT PAP, V55, P525, DOI 10.1007/s00362-013-0498-x; SHEATHER SJ, 1991, J ROY STAT SOC B MET, V53, P683; Sklar A., 1959, PUBL I STAT U PARIS, V8, P229, DOI DOI 10.1007/978-3-642-33590-7; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Stober J, 2013, COMPUTATION STAT, V28, P2679, DOI 10.1007/s00180-013-0423-8; Stober Jakob, 2012, SERIES QUANTITATIVE; Tagasovska N., 2018, ARXIV180110579; Theis L., 2015, ICLR; Tolstikhin I. O., 2017, ADV NEURAL INFORM PR, P5424, DOI DOI 10.5555/3295222.3295294; Tran Dustin, 2015, NEURIPS; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Xiao H., 2017, FASHION MNIST NOVEL; Xie JY, 2016, PR MACH LEARN RES, V48; Xu Q., 2018, EMPIRICAL STUDY EVAL	81	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306052
C	Talebi, MS; Maillard, OA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Talebi, Mohammad Sadegh; Maillard, Odalric-Ambrym			Learning Multiple Markov Chains via Adaptive Allocation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FINITE; PROBABILITIES; FUNCTIONALS; BOUNDS	We study the problem of learning the transition matrices of a set of Markov chains from a single stream of observations on each chain. We assume that the Markov chains are ergodic but otherwise unknown. The learner can sample Markov chains sequentially to observe their states. The goal of the learner is to sequentially select various chains to learn transition matrices uniformly well with respect to some loss function. We introduce a notion of loss that naturally extends the squared loss for learning distributions to the case of Markov chains, and further characterize the notion of being uniformly good in all problem instances. We present a novel learning algorithm that efficiently balances exploration and exploitation intrinsic to this problem, without any prior knowledge of the chains. We provide finite-sample PAC-type guarantees on the performance of the algorithm. Further, we show that our algorithm asymptotically attains an optimal loss.	[Talebi, Mohammad Sadegh; Maillard, Odalric-Ambrym] Inria Lille Nord Europe, SequeL Team, Lille, France		Talebi, MS (corresponding author), Inria Lille Nord Europe, SequeL Team, Lille, France.	sadegh.talebi@inria.fr; odalric.maillard@inria.fr			CPER Nord-Pas-de-Calais/FEDER DATA Advanced data science and technologies 2015-2020; French Ministry of Higher Education and Research, Inria; French Agence Nationale de la Recherche (ANR) [ANR-16-CE40-0002]	CPER Nord-Pas-de-Calais/FEDER DATA Advanced data science and technologies 2015-2020; French Ministry of Higher Education and Research, Inria; French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR))	This work has been supported by CPER Nord-Pas-de-Calais/FEDER DATA Advanced data science and technologies 2015-2020, the French Ministry of Higher Education and Research, Inria, and the French Agence Nationale de la Recherche (ANR), under grant ANR-16-CE40-0002 (project BADASS).	Antos A, 2008, LECT NOTES ARTIF INT, V5254, P287, DOI 10.1007/978-3-540-87987-9_25; Antos A, 2010, THEOR COMPUT SCI, V411, P2712, DOI 10.1016/j.tcs.2010.04.007; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; BILLINGSLEY P, 1961, ANN MATH STAT, V32, P12, DOI 10.1214/aoms/1177705136; Carpentier A., 2012, NIPS, P251; Carpentier A, 2015, J MACH LEARN RES, V16, P2231; Carpentier A, 2014, THEOR COMPUT SCI, V558, P77, DOI 10.1016/j.tcs.2014.09.027; Carpentier A, 2011, LECT NOTES ARTIF INT, V6925, P189, DOI 10.1007/978-3-642-24412-4_17; Carpentier Alexandra, 2012, ADV NEURAL INFORM PR, P1961; Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295; Craig BA, 2002, HEALTH ECON, V11, P33, DOI 10.1002/hec.654; Dance CR, 2019, J MACH LEARN RES, V20; Dror HA, 2008, J AM STAT ASSOC, V103, P288, DOI 10.1198/016214507000001346; Etore P, 2010, METHODOL COMPUT APPL, V12, P335, DOI 10.1007/s11009-008-9108-0; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; Gamarnik D, 2003, IEEE T INFORM THEORY, V49, P338, DOI 10.1109/TIT.2002.806131; HAO Y, 2018, ADV NEURAL INFORM PR, P642; HAVIV M, 1984, ADV APPL PROBAB, V16, P804, DOI 10.2307/1427341; Hsu D. J., 2015, ADV NEURAL INFORM PR, P1459; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155; KIPNIS C, 1986, COMMUN MATH PHYS, V104, P1, DOI 10.1007/BF01210789; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Levin D. A., 2009, MARKOV CHAINS MIXING; Lezaud P, 1998, ANN APPL PROBAB, V8, P849; Meyn S. P., 2012, MARKOV CHAINS STOCHA; MUSHKIN M, 1989, IEEE T INFORM THEORY, V35, P1277, DOI 10.1109/18.45284; Neufeld J, 2014, PR MACH LEARN RES, V32, P1944; Norris JR, 1998, MARKOV CHAINS; Orlitsky Alon, 2015, C LEARN THEOR, P1066; Ortner R, 2014, THEOR COMPUT SCI, V558, P62, DOI 10.1016/j.tcs.2014.09.026; Paulin T, 2015, 2015 8TH IFIP WIRELESS AND MOBILE NETWORKING CONFERENCE (WMNC), P200, DOI 10.1109/WMNC.2015.39; Rio E., 2017, ASYMPTOTIC THEORY WE; Tarbouriech J., 2019, P 22 INT C ART INT S, P974; Tekin C, 2012, IEEE T INFORM THEORY, V58, P5588, DOI 10.1109/TIT.2012.2198613; Welton NJ, 2005, MED DECIS MAKING, V25, P633, DOI 10.1177/0272989X05282637; Wolfer G., 2019, ALGORITHMIC LEARNING, P903	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905004
C	Talwar, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Talwar, Kunal			Computational Separations between Sampling and Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Two commonly arising computational tasks in Bayesian learning are Optimization (Maximum A Posteriori estimation) and Sampling (from the posterior distribution). In the convex case these two problems are efficiently reducible to each other. Recent work [Ma et al., 2019] shows that in the non-convex case, sampling can sometimes be provably faster. We present a simpler and stronger separation. We then compare sampling and optimization in more detail and show that they are provably incomparable: there are families of continuous functions for which optimization is easy but sampling is NP-hard, and vice versa. Further, we show function families that exhibit a sharp phase transition in the computational complexity of sampling, as one varies the natural temperature parameter. Our results draw on a connection to analogous separations in the discrete setting which are well-studied.	[Talwar, Kunal] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Talwar, K (corresponding author), Google Brain, Mountain View, CA 94043 USA.	kunal@google.com						[Anonymous], 2015, FOUND TRENDS MACH LE, V8, P232, DOI 10.1561/2200000050; [Anonymous], 2016, FDN TRENDS IN OPTIMI; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Impagliazzo R, 2001, J COMPUT SYST SCI, V62, P367, DOI 10.1006/jcss.2000.1727; Jerrum M, 2004, J ACM, V51, P671, DOI 10.1145/1008731.1008738; JUSTESEN J, 1972, IEEE T INFORM THEORY, V18, P652, DOI 10.1109/TIT.1972.1054893; Karp RM., 1972, COMPLEXITY COMPUTER, P85; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Lovasz L, 2006, ANN IEEE SYMP FOUND, P57; Ma YA, 2019, P NATL ACAD SCI USA, V116, P20881, DOI 10.1073/pnas.1820003116; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 2014, INTRO LECT CONVEX OP; Roberts G., 2002, METHODOL COMPUT APPL, V4, P337, DOI DOI 10.1023/A:1023562417138; ROSSKY PJ, 1978, J CHEM PHYS, V69, P4628, DOI 10.1063/1.436415; Sly A, 2012, ANN IEEE SYMP FOUND, P361, DOI 10.1109/FOCS.2012.56; TODA S, 1991, SIAM J COMPUT, V20, P865, DOI 10.1137/0220053; Tosh Christopher, 2019, P MACHINE LEARNING R, V99, P2993; Vadhan Salil, 2002, COMPUTATIONAL COMPLE; Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906065
C	Tanaka, H; Nayebi, A; Maheswaranathan, N; McIntosh, L; Baccus, SA; Ganguli, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tanaka, Hidenori; Nayebi, Aran; Maheswaranathan, Niru; McIntosh, Lane; Baccus, Stephen A.; Ganguli, Surya			From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PATTERNS	Recently, deep feedforward neural networks have achieved considerable success in modeling biological sensory processing, in terms of reproducing the input-output map of sensory neurons. However, such models raise profound questions about the very nature of explanation in neuroscience. Are we simply replacing one complex system (a biological circuit) with another (a deep network), without understanding either? Moreover, beyond neural representations, are the deep network's computational mechanisms for generating neural responses the same as those in the brain? Without a systematic approach to extracting and understanding computational mechanisms from deep neural network models, it can be difficult both to assess the degree of utility of deep learning approaches in neuroscience, and to extract experimentally testable hypotheses from deep networks. We develop such a systematic approach by combining dimensionality reduction and modern attribution methods for determining the relative importance of interneurons for specific visual computations. We apply this approach to deep network models of the retina, revealing a conceptual understanding of how the retina acts as a predictive feature extractor that signals deviations from expectations for diverse spatiotemporal stimuli. For each stimulus, our extracted computational mechanisms are consistent with prior scientific literature, and in one case yields a new mechanistic hypothesis. Thus overall, this work not only yields insights into the computational mechanisms underlying the striking predictive capabilities of the retina, but also places the framework of deep networks as neuroscientific models on firmer theoretical foundations, by providing a new roadmap to go beyond comparing neural representations to extracting and understand computational mechanisms.	[Tanaka, Hidenori] NTT Res Inc, Phys & Informat Labs, East Palo Alto, CA 94303 USA; [Tanaka, Hidenori; Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA; [Nayebi, Aran; Maheswaranathan, Niru; McIntosh, Lane] Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA; [Baccus, Stephen A.] Stanford Univ, Dept Neurobiol, Stanford, CA 94305 USA; [Maheswaranathan, Niru; Ganguli, Surya] Google Inc, Google Brain, Mountain View, CA USA	Stanford University; Stanford University; Stanford University; Google Incorporated	Tanaka, H (corresponding author), NTT Res Inc, Phys & Informat Labs, East Palo Alto, CA 94303 USA.; Tanaka, H (corresponding author), Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.	tanaka8@stanford.edu; sganguli@stanford.edu	Tanaka, Hidenori/GZM-8327-2022		NEI [R01EY022933, R01EY025087, P30-EY026877]; Simons foundation; James S. McDonnell foundation; NSF [1845166]	NEI(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); Simons foundation; James S. McDonnell foundation; NSF(National Science Foundation (NSF))	We thank Daniel Fisher for insightful discussions and support. We thank the Masason foundation (HT), grants from the NEI (R01EY022933, R01EY025087, P30-EY026877) (SAB), and the Simons, James S. McDonnell foundations, and NSF Career 1845166 (SG) for funding.	Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007; Bashivan P, 2019, SCIENCE, V364, P453, DOI 10.1126/science.aav9436; Berry MJ, 1999, NATURE, V398, P334, DOI 10.1038/18678; BULLOCK TH, 1994, ELECTROEN CLIN NEURO, V91, P42, DOI 10.1016/0013-4694(94)90017-5; BULLOCK TH, 1990, J NEUROPHYSIOL, V64, P903, DOI 10.1152/jn.1990.64.3.903; Cadena Santiago A., 2017, BIORXIV, DOI [10.1101/201764, DOI 10.1101/201764]; Chen EY, 2014, J NEUROSCI, V34, P15557, DOI 10.1523/JNEUROSCI.1460-13.2014; Chen EY, 2013, J NEUROSCI, V33, P120, DOI 10.1523/JNEUROSCI.3749-12.2013; Deshmukh Nikhil Rajiv, 2015, THESIS; Dhamdhere K., 2019, P 7 INT C LEARN REPR; Gao J, 2009, NETWORK-COMP NEURAL, V20, P106, DOI 10.1080/09548980902991705; GLASER JI, 2019, PROGR NEUROBIOLOGY; Gollisch T, 2008, BIOL CYBERN, V99, P263, DOI 10.1007/s00422-008-0252-y; Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639; Gollisch T, 2010, NEURON, V65, P150, DOI 10.1016/j.neuron.2009.12.009; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Konishi M, 2003, ANNU REV NEUROSCI, V26, P31, DOI 10.1146/annurev.neuro.26.041002.131123; Maheswaranathan N, 2018, BIORXIV340943, DOI [10.1101/340943v1, DOI 10.1101/340943, 10.1101/340943]; Maheswaranathan Niru, 2019, ADV NEURAL INFORM PR; Marder E, 2007, ANNU REV PHYSIOL, V69, P291, DOI 10.1146/annurev.physiol.69.031905.161516; McIntosh LT, 2016, ADV NEUR IN, V29; Schwartz G, 2008, J NEUROPHYSIOL, V99, P1787, DOI 10.1152/jn.01025.2007; Schwartz G, 2007, NEURON, V55, P958, DOI 10.1016/j.neuron.2007.07.042; Schwartz G, 2007, NAT NEUROSCI, V10, P552, DOI 10.1038/nn1887; Sundararajan M, 2017, PR MACH LEARN RES, V70; Vaney DI, 2012, NAT REV NEUROSCI, V13, P194, DOI 10.1038/nrn3165; Walker EY, 2019, NAT NEUROSCI, V22, P2060, DOI 10.1038/s41593-019-0517-x; Werner B, 2008, J NEUROPHYSIOL, V100, P1087, DOI 10.1152/jn.90527.2008; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	30	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900016
C	Tang, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tang, Cheng			Exponentially convergent stochastic k-PCA without variance reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present Matrix Krasulina, an algorithm for online k-PCA, by generalizing the classic Krasulina's method [1] from vector to matrix case. We show, both theoretically and empirically, that the algorithm naturally adapts to data low-rankness and converges exponentially fast to the ground-truth principal subspace. Notably, our result suggests that despite various recent efforts to accelerate the convergence of stochastic-gradient based methods by adding a O(n)-time variance reduction step, for the k-PCA problem, a truly online SGD variant suffices to achieve exponential convergence on intrinsically low-rank data.	[Tang, Cheng] Amazon AI, New York, NY 10001 USA		Tang, C (corresponding author), Amazon AI, New York, NY 10001 USA.	tcheng@amazon.com						Allen-Zhu Z, 2017, ANN IEEE SYMP FOUND, P487, DOI 10.1109/FOCS.2017.51; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; Arora Raman, 2013, ADV NEURAL INFORM PR, P1815; Balcan M-F., 2016, C LEARNING THEORY, P284; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976; Balzano L, 2018, P IEEE, V106, P1293, DOI 10.1109/JPROC.2018.2847041; Boutsidis C., 2015, P 26 ANN ACM SIAM S, P887, DOI DOI 10.1137/1.9781611973730.61; Chen J., 2018, ARXIV E PRINTS; De Sa C., 2014, ARXIV E PRINTS; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Krasulina T.P., 1969, USSR COMP MATH MATH, V9, P189, DOI DOI 10.1016/0041-5553(69)90135-9; LeCun Y, 2010, ATT LAB; Li CL, 2016, JMLR WORKSH CONF PRO, V51, P473; Loukas Andreas, 2017, P INT C MACH LEARN, V70, P2228; Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR, P2886; Nie JZ, 2016, J MACH LEARN RES, V17, P1; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Parkhi O. M., 2015, BRIT MACH VIS C; Shalev-Shwartz Shai, 2009, COLT 2009; Shamir O, 2016, PR MACH LEARN RES, V48; Shamir O, 2016, PR MACH LEARN RES, V48; Shamir O, 2015, PR MACH LEARN RES, V37, P144; Vu V., 2012, INT C ARTIFICIAL INT, P1278; Vu VQ, 2013, ANN STAT, V41, P2905, DOI 10.1214/13-AOS1151; Warmuth Manfred K., 2006, ADV NEURAL INFORM PR, P1481; Zhang Dejiao, 2015, AISTATS	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904010
C	Tao, CY; Chen, LQ; Dai, SY; Chen, JY; Bai, K; Wang, D; Feng, JF; Lu, WL; Bobashev, G; Carin, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tao, Chenyang; Chen, Liqun; Dai, Shuyang; Chen, Junya; Bai, Ke; Wang, Dong; Feng, Jianfeng; Lu, Wenlian; Bobashev, Georgiy; Carin, Lawrence			On Fenchel Mini-Max Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STATISTICAL-MODELS; INFERENCE	Inference, estimation, sampling and likelihood evaluation are four primary goals of probabilistic modeling. Practical considerations often force modeling approaches to make compromises between these objectives. We present a novel probabilistic learning framework, called Fenchel Mini-Max Learning (FML), that accommodates all four desiderata in a flexible and scalable manner. Our derivation is rooted in classical maximum likelihood estimation, and it overcomes a longstanding challenge that prevents unbiased estimation of unnormalized statistical models. By reformulating MLE as a mini-max game, FML enjoys an unbiased training objective that (i) does not explicitly involve the intractable normalizing constant and (ii) is directly amendable to stochastic gradient descent optimization. To demonstrate the utility of the proposed approach, we consider learning unnormalized statistical models, nonparametric density estimation and training generative models, with encouraging empirical results presented.	[Tao, Chenyang; Chen, Liqun; Dai, Shuyang; Chen, Junya; Bai, Ke; Wang, Dong; Carin, Lawrence] Duke Univ, Elect & Comp Engn, Durham, NC 27708 USA; [Chen, Junya; Lu, Wenlian] Fudan Univ, Sch Math Sci, Shanghai, Peoples R China; [Feng, Jianfeng] Fudan Univ, ISTBI, Shanghai, Peoples R China; [Bobashev, Georgiy] RTI Int, Res Triangle Pk, NC USA	Duke University; Fudan University; Fudan University; Research Triangle Institute	Tao, CY (corresponding author), Duke Univ, Elect & Comp Engn, Durham, NC 27708 USA.	chenyang.tao@duke.edu; lcarin@duke.edu		Tao, Chenyang/0000-0002-2155-8180; Wang, Dong/0000-0002-2169-2937	DARPA; DOE; NIH; ONR; NSF; RTI Internal research & development funds; China Scholarship Council (CSC); Shanghai Municipal Science and Technology Major Project; ZJ Lab [2018SHZDZX01]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); RTI Internal research & development funds; China Scholarship Council (CSC)(China Scholarship Council); Shanghai Municipal Science and Technology Major Project; ZJ Lab	The authors would like to thank the anonymous reviewers for their insightful comments. This research was supported in part by DARPA, DOE, NIH, ONR, NSF and RTI Internal research & development funds. J Chen was partially supported by China Scholarship Council (CSC). W Lu and J Feng were supported by the Shanghai Municipal Science and Technology Major Project and ZJ Lab (No. 2018SHZDZX01).	Alain G, 2014, J MACH LEARN RES, V15, P3563; Alemi AA, 2018, PR MACH LEARN RES, V80; Ba J., 2017, P 3 INT C LEARN REPR; Berger T., 1971, RATE DISTORTION THEO; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Bottou L., 2017, ICML; Brockman G., 2016, OPENAI GYM; Brown P. F., 1992, Computational Linguistics, V18, P31; Burda Y., 2016, ICLR; Burda Y, 2015, JMLR WORKSH CONF PRO, V38, P102; Casella G, 2002, DUXBURY; Chen L., 2018, AISTATS; Chen X, 2016, ADV NEUR IN, V29; Cremer C., 2018, ARXIV180103558; Dai B., 2018, ARXIV181102228; Dai Bo, 2018, NIPS BAYES DEEP LEAR; Dheeru D., 2019, UCI MACHINE LEARNING; Duan Y, 2016, INT C MACH LEARN, P1329; Dziugaite G. K., 2015, UAL; Fagan F., 2018, ARXIV180308577; Feng Y., 2017, UAI; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Geyer C.J., 1991, MARKOV CHAIN MONTE C; GEYER CJ, 1994, J ROY STAT SOC B MET, V56, P261; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo J., 2018, AAAI; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Gutmann M., 2012, ARXIV12023727; Gutmann MU, 2018, STAT COMPUT, V28, P411, DOI 10.1007/s11222-017-9738-6; Gutmann MU, 2012, J MACH LEARN RES, V13, P307; Haarnoja T., 2017, ICML; Heusel M., 2017, NIPS; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Lemarechal Claude, 2012, FUNDAMENTALS CONVEX; Li Chunyuan, 2019, ADVERSARIAL LEARNING; Li Ke, 2018, IMPLICIT MAXIMUM LIK, P2; Li Y., 2018, ICLR; Li Y., 2015, ICML; Li Y., 2016, NIPS; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lindsay BG, 1995, NSF CBMS REGIONAL C, V5, P1; Liu J. S., 2008, MONTE CARLO STRATEGI; Liu Q., 2016, NIPS; Mescheder L., 2017, ICML; Mohamed Shakir, 2016, ARXIV161003483; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Nguyen XuanLong, 2008, ADV NEURAL INFORM PR, P1089; Nowozin Sebastian, 2016, P ADV NEURAL INFORM; Papamakarios George, 2017, ARXIV170507057; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Rainforth T., 2017, NIPS WORKSH; REZENDE DJ, 2015, ICML; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Salimans Tim, 2016, ADV NEURAL INFORM PR; Smola A, 2006, COLT; Sriperumbudur Bharath, 2017, J MACHINE LEARNING R, V18, P1830; Sutherland Dougal J, 2018, AISTATS; Tao C., 2018, ICML; Tao Chenyang, 2019, ICML; Warde-Farley D., 2017, ICLR; Wu Y., 2017, ICLR; Zhang Cheng, 2017, ABS171105597 CORR	65	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902010
C	Tay, Y; Tuan, LA; Zhang, A; Wang, SH; Hui, SC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tay, Yi; Luu Anh Tuan; Zhang, Aston; Wang, Shuohang; Hui, Siu Cheung			Compositional De-Attention Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Attentional models are distinctly characterized by their ability to learn relative importance, i.e., assigning a different weight to input values. This paper proposes a new quasi-attention that is compositional in nature, i.e., learning whether to add, subtract or nullify a certain vector when learning representations. This is strongly contrasted with vanilla attention, which simply re-weights input tokens. Our proposed Compositional De-Attention (CoDA) is fundamentally built upon the intuition of both similarity and dissimilarity (negative affinity) when computing affinity scores, benefiting from a greater extent of expressiveness. We evaluate CoDA on six NLP tasks, i.e. open domain question answering, retrieval/ranking, natural language inference, machine translation, sentiment analysis and text2code generation. We obtain promising experimental results, achieving state-of-the-art performance on several tasks/datasets.	[Tay, Yi; Hui, Siu Cheung] Nanyang Technol Univ, Singapore, Singapore; [Luu Anh Tuan] MIT, CSAIL, Cambridge, MA 02139 USA; [Zhang, Aston] Amazon AI, Seattle, WA USA; [Wang, Shuohang] Microsoft Dynam 365 AI Res, Dana Point, CA USA	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Massachusetts Institute of Technology (MIT)	Tay, Y (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	ytay017@gmail.com		Hui, Siu Cheung/0000-0001-5397-4472				Bahdanau D., 2015, P 3 INT C LEARNING R; Bowman Samuel, 2015, P 2015 C EMP METH NA, P632; Buck Christian, 2017, ARXIV170507830; Das R., 2019, INT C LEARN REPR; Dhingra Bhuwan, 2017, QUASAR DATASETS QUES; Dunn Matthew, 2017, ARXIV170405179; Graves A, 2014, NEURAL TURING MACHIN; Huang Po-Sen, 2017, ARXIV170605565; Khot Tushar, 2018, 32 AAAI C ART INT; Laha A., 2018, ADV NEURAL INFORM PR, P6422; Lowe R., 2015, ARXIV150608909; Luong M. -T., STANFORD NEURAL MACH; Luong M-T, 2015, ARXIV150804025; Martins AFT, 2016, PR MACH LEARN RES, V48; Martins Andre FT, 2019, ARXIV190505702; Polosukhin I., 2018, ARXIV180204335; Seo Minjoon, 2016, ARXIV11101603; Tay Yi, 2018, ADV NEURAL INFORM PR, P4911; Trask A, 2018, ADV NEUR IN, V31; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wang M., 2007, P JOINT C EMP METH N, P1; Wang Shuohang, 2017, ARXIV170900023; Wangperawong Artit, 2018, CORR; Welleck Sean, 2018, ARXIV181100671; Williams Adina, 2017, ARXIV170405426; Xu K, 2015, INT C MACH LEARN, P2048; Yang Y., 2015, P C EMP METH NAT LAN, P2013	28	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306017
C	Teng, YF; Gao, WB; Chalus, F; Choromanska, A; Goldfarb, D; Weller, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Teng, Yunfei; Gao, Wenbo; Chalus, Francois; Choromanska, Anna; Goldfarb, Donald; Weller, Adrian			Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider distributed optimization under communication constraints for training deep learning models. We propose a new algorithm, whose parameter updates rely on two forces: a regular gradient step, and a corrective direction dictated by the currently best-performing worker (leader). Our method differs from the parameter-averaging scheme EASGD [1] in a number of ways: (i) our objective formulation does not change the location of stationary points compared to the original optimization problem; (ii) we avoid convergence decelerations caused by pulling local workers descending to different local minima to each other (i.e. to the average of their parameters); (iii) our update by design breaks the curse of symmetry (the phenomenon of being trapped in poorly generalizing sub-optimal solutions in symmetric non-convex landscapes); and (iv) our approach is more communication efficient since it broadcasts only parameters of the leader rather than all workers. We provide theoretical analysis of the batch version of the proposed algorithm, which we call Leader Gradient Descent (LGD), and its stochastic variant (LSGD). Finally, we implement an asynchronous version of our algorithm and extend it to the multi-leader setting, where we form groups of workers, each represented by its own local leader (the best performer in a group), and update each worker with a corrective direction comprised of two attractive forces: one to the local, and one to the global leader (the best performer among all workers). The multi-leader setting is well-aligned with current hardware architecture, where local workers forming a group lie within a single computational node and different groups correspond to different nodes. For training convolutional neural networks, we empirically demonstrate that our approach compares favorably to state-of-the-art baselines.				yt1208@nyu.edu; wg2279@columbia.edu; chalusf3@gmail.com; ac5455@nyu.edu; goldfarb@columbia.edu; aw665@cam.ac.uk			NSF [IIS-1838061]; Alan Turing Institute under EPSRC [EP/N510129/1, TU/B/000074]; Leverhulme Trust via the CFI; David MacKay Newton research fellowship at Darwin College	NSF(National Science Foundation (NSF)); Alan Turing Institute under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Leverhulme Trust via the CFI; David MacKay Newton research fellowship at Darwin College	WG and DG were supported in part by NSF Grant IIS-1838061. AW acknowledges support from the David MacKay Newton research fellowship at Darwin College, The Alan Turing Institute under EPSRC grant EP/N510129/1 & TU/B/000074, and the Leverhulme Trust via the CFI.	Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, ARXIV151101029; Baldassi C, 2016, P NATL ACAD SCI USA, V113, pE7655, DOI 10.1073/pnas.1608103113; Ben-Nun T., 2018, ABS180209941 CORR; Bottou L., 1998, ONLINE ALGORITHMS ST; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chaudhari P., 2018, SYSML; Chaudhari Pratik, 2017, ICLR POSTER; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ge R., 2017, ICML; Ge R., 2016, NEURIPS, P2973; Gholami A, 2018, SPAA'18: PROCEEDINGS OF THE 30TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P77, DOI 10.1145/3210377.3210394; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Izmailov P, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P876; Jastrzebski S., 2018, ICLR WORKSH TRACK; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968; Keskar Nitish Shirish, 2017, INT C LEARN REPR, DOI [10.48550/arxiv.1609.04836, DOI 10.48550/ARXIV.1609.04836]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky Alex, CIFAR 10; Li XR, 2019, J AM STAT ASSOC, V114, P1651, DOI 10.1080/01621459.2018.1512863; Lian XR, 2018, PR MACH LEARN RES, V80; Liang SY, 2018, PR MACH LEARN RES, V80; Lumsdaine, 2016, ABS161106334 ARXIV; Ma S., 2018, ICML; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Sergeev Alexander, 2018, ARXIV180205799; Smith S.L., 2018, P ICLR; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Weston J., 2014, EMNLP; You Y., 2018, ICLR	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901045
C	Thekumprampil, KK; Jain, P; Netrapalli, P; Oh, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Thekumprampil, Kiran Koshy; Jain, Prateek; Netrapalli, Praneeth; Oh, Sewoong			Efficient Algorithms for Smooth Minimax Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIATIONAL-INEQUALITIES; LINEAR CONVERGENCE; MONOTONE-OPERATORS; POINT	This paper studies first order methods for solving smooth minimax optimization problems min(x) max(y) g(x, y) where g(.,.) is smooth and g(x,.) is concave for each x. In terms of g(., y), we consider two settings - strongly convex and nonconvex - and improve upon the best known rates in both. For strongly-convex g(., y), for all y, we propose a new direct optimal algorithm combining Mirror-Prox and Nesterov's AGD, and show that it can find global optimum in (O) over tilde (1/k(2)) iterations, improving over current state-of-the-art rate of O(1/k). We use this result along with an inexact proximal point method to provide (O) over tilde (1/k(1/3)) rate for finding stationary points in the nonconvex setting where g(., y) can be nonconvex. This improves over current best-known rate of O(1/k(1/5)). Finally, we instantiate our result for finite nonconvex minimax problems, i.e., min(x) max(1<i<m) f(i) (x), with nonconvex f(i) (.), to obtain convergence rate of O(m(1/3) root logm /k(1/3)).	[Thekumprampil, Kiran Koshy] Univ Illinois Urban Champaign, Champaign, IL 61820 USA; [Jain, Prateek; Netrapalli, Praneeth] Microsoft Res, Bengaluru, India; [Oh, Sewoong] Univ Washington, Seattle, WA 98195 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Washington; University of Washington Seattle	Thekumprampil, KK (corresponding author), Univ Illinois Urban Champaign, Champaign, IL 61820 USA.	thekump2@illinois.edu; prajain@microsoft.com; praneeth@microsoft.com; sewoong@cs.washington.edu			NSF [CCF-1927712, RI-1929955]	NSF(National Science Foundation (NSF))	This work is partially supported by NSF awards CCF-1927712 and RI-1929955.	Alkousa M., 2019, ARXIV190603620; Bansal Nikhil, 2017, ARXIV171204581; Berger J. O., 2013, STAT DECISION THEORY; Bertsekas D. P., 2009, CONVEX OPTIMIZATION; Bertsekas D. P., 2014, CONSTRAINED OPTIMIZA; BRUCK RE, 1977, J MATH ANAL APPL, V61, P159, DOI 10.1016/0022-247X(77)90152-4; Chambolle A, 2016, ACTA NUMER, V25, P161, DOI 10.1017/S096249291600009X; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Chen YM, 2017, MATH PROGRAM, V165, P113, DOI 10.1007/s10107-017-1161-4; Chen YM, 2014, SIAM J OPTIMIZ, V24, P1779, DOI 10.1137/130919362; Davis D., 2018, ARXIV180202988; Du SS, 2019, PR MACH LEARN RES, V89, P196; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hamedani E.Y., 2018, ARXIV PREPRINT ARXIV; He Y, 2016, SIAM J OPTIMIZ, V26, P29, DOI 10.1137/14096757X; Jin C., 2019, ARXIV190200618; Juditsky Anatoli, 2011, OPTIMIZATION MACHINE, V30, P149; Kakade S. M., 2009, UNPUB; Kar Purushottam, 2015, ARXIV150506813; Kinderlehrer D., 1980, INTRO VARIATIONAL IN, V31; Komiya H., 1988, KODAI MATH J, P5; Komiyama Junpei, 2018, INT C MACH LEARN, P2742; Kong W., 2019, ARXIV190513433; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lu S., 2019, ARXIV190208294; Madry A., 2018, ARXIV PREPRINT ARXIV; Mokhtari A., 2019, ARXIV190108511; Myerson RB, 2013, GAME THEORY; Nedic A, 2009, J OPTIMIZ THEORY APP, V142, P205, DOI 10.1007/s10957-009-9522-7; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovski Arkadi, 1981, EKONOM MAT METODY, V17, P344; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 1998, INTRO LECT CONVEX PR, VI; Nouiehed M., 2018, ARXIV181002024; Nouiehed Maher, 2019, ARXIV190208297; Ouyang Y., 2018, ARXIV180802901; Sanjabi M., 2018, P NEURIPS MONTR QC, P7091; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; TSENG P, 1995, J COMPUT APPL MATH, V60, P237, DOI 10.1016/0377-0427(94)00094-H; Xie Z., 2019, ARXIV190607691; Xu Y, 2017, ARXIV171105812; Xu YY, 2018, COMPUT OPTIM APPL, V70, P91, DOI 10.1007/s10589-017-9972-z; Zhao Renbo, 2019, ARXIV190301687	51	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904034
C	Thune, TS; Cesa-Bianchi, N; Seldin, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Thune, Tobias Sommer; Cesa-Bianchi, Nicolo; Seldin, Yevgeny			Nonstochastic Multiarmed Bandits with Unrestricted Delays	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					dWe investigate multiarmed bandits with delayed feedback, where the delays need neither be identical nor bounded. We first prove that "delayed" Exp3 achieves the 0 (\/(KT D) In K) regret bound conjectured by Cesa-Bianchi et al. [2019] in the case of variable, but bounded delays. Here, K is the number of actions and D is the total delay over T rounds. We then introduce a new algorithm that lifts the requirement of bounded delays by using a wrapper that skips rounds with excessively large delays. The new algorithm maintains the same regret bound, but similar to its predecessor requires prior knowledge of D and T. For this algorithm we then construct a novel doubling scheme that forgoes the prior knowledge requirement under the assumption that the delays are available at action time (rather than at loss observation time). This assumption is satisfied in a broad range of applications, including interaction with servers and service providers. The resulting oracle regret bound is of order min Sf3 +,3 In K + (KT + D3)1,3), where 1Sf31 is the number of observations with delay exceeding 3, and D3 is the total delay of observations with delay below 3. The bound relaxes to 0 (,/(KT D) In K), but we also provide examples where D3 < D and the oracle bound has a polynomially better dependence on the problem parameters.	[Thune, Tobias Sommer; Seldin, Yevgeny] Univ Copenhagen, Copenhagen, Denmark; [Cesa-Bianchi, Nicolo] DSRC, Milan, Italy; [Thune, Tobias Sommer; Cesa-Bianchi, Nicolo] Univ Milan, Milan, Italy	University of Copenhagen; University of Milan	Thune, TS (corresponding author), Univ Copenhagen, Copenhagen, Denmark.; Thune, TS (corresponding author), Univ Milan, Milan, Italy.	tobias.thune@di.ku.dk; nicolo.cesa-bianchi@unimi.it; seldin@di.ku.dk	Seldin, Yevgeny/G-8955-2015	Seldin, Yevgeny/0000-0003-3152-4635	Google Focused Award Algorithms and Learning for AI (ALL4AI); MIUR PRIN grant Algorithms, Games, and Digital Markets (ALGADIMAR); Independent Research Fund Denmark [9040-00361B]	Google Focused Award Algorithms and Learning for AI (ALL4AI)(Google Incorporated); MIUR PRIN grant Algorithms, Games, and Digital Markets (ALGADIMAR); Independent Research Fund Denmark	Nicolo Cesa-Bianchi gratefully acknowledges partial support by the Google Focused Award Algorithms and Learning for AI (ALL4AI) and by the MIUR PRIN grant Algorithms, Games, and Digital Markets (ALGADIMAR). Yevgeny Seldin acknowledges partial support by the Independent Research Fund Denmark, grant number 9040-00361B.	Arya Sakshi, 2019, ARXIV190200819; Cesa-Bianchi N, 2019, J MACH LEARN RES, V20; Cesa-Bianchi Nicolo, 2018, P INT C COMP LEARN T; Chapelle O., 2011, NIPS, P2249; Chapelle Olivier, 2014, P INT C KNOWL DISC D; Dudik M, 2011, P 27 C UNC ART INT, P169; Garg Siddhant, 2019, ARXIV191001161; Garrabrant S., 2016, ARXIV160405280; Ghosh Avishek, 2018, P ANN ALL C COMM CON; Joulani Pooria, 2016, P AAAI C ART INT; Li Bingcong, 2019, P INT C ART INT STAT; Mandel Travis, 2015, P AAAI C ART INT; Mann TA, 2018, ARXIV PREPRINT ARXIV; Mesterharm C., 2007, THESIS; Mesterharm Chris, 2005, P INT C ALG LEARN TH; Neu Gergely, 2014, IEEE T AUTOMATIC CON, V59; Pike-Burke Ciara, 2018, P INT C MACH LEARN I; Quanrud Kent, 2015, ADV NEURAL INFORM PR; Shamir Ohad, 2017, P INT C MACH LEARN I; Vernade C, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Zimmert Julian, 2019, ARXIV191006054; Zimmert Julian, 2019, P INT C ART INT STAT; Zinkevich M., 2009, ADV NEURAL INFORM PR	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306053
C	Toso, M; Campbell, NDF; Russell, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Toso, Matteo; Campbell, Neill D. F.; Russell, Chris			Fixing Implicit Derivatives: Trust-Region Based Learning of Continuous Energy Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				QUASI-NEWTON MATRICES	We present a new technique for the learning of continuous energy functions that we refer to as Wibergian Learning. One common approach to inverse problems is to cast them as an energy minimisation problem, where the minimum cost solution found is used as an estimator of hidden parameters. Our new approach formally characterises the dependency between weights that control the shape of the energy function, and the location of minima, by describing minima as fixed points of optimisation methods. This allows for the use of gradient-based end-to-end training to integrate deep-learning and the classical inverse problem methods. We show how our approach can be applied to obtain state-of-the-art results in the diverse applications of tracker fusion and multiview 3D reconstruction.	[Toso, Matteo; Russell, Chris] Univ Surrey, CVSSP, Guildford, Surrey, England; [Campbell, Neill D. F.] Univ Bath, Bath, Avon, England; [Russell, Chris] Alan Turing Inst, London, England	University of Surrey; University of Bath	Toso, M (corresponding author), Univ Surrey, CVSSP, Guildford, Surrey, England.		Russell, Chris/AAU-6819-2021	Russell, Chris/0000-0002-1942-7296; Russell, Chris/0000-0003-1665-1759	RCUK Centre for the Analysis of Motion, Entertainment Research and Applications (CAMERA) [EP/M023281/1]; Royal Society	RCUK Centre for the Analysis of Motion, Entertainment Research and Applications (CAMERA); Royal Society(Royal Society of London)	We acknowledge funding from the RCUK Centre for the Analysis of Motion, Entertainment Research and Applications (CAMERA, EP/M023281/1) and the Royal Society.	Amos B, 2017, PR MACH LEARN RES, V70; BakIr G., 2007, PREDICTING STRUCTURE; Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187; Bertinetto L., 2019, P INT C LEARN REPR, P1; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; BYRD RH, 1994, MATH PROGRAM, V63, P129, DOI 10.1007/BF01582063; Clark R, 2018, LECT NOTES COMPUT SC, V11212, P291, DOI 10.1007/978-3-030-01237-3_18; Daniel C, 2016, AAAI CONF ARTIF INTE, P1519; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Finn C, 2017, PR MACH LEARN RES, V70; Franceschi L, 2018, PR MACH LEARN RES, V80; Hong JH, 2015, IEEE I CONF COMP VIS, P4130, DOI 10.1109/ICCV.2015.470; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jenni Simon, 2018, EUR C COMP VIS ECCV; Kingma D.P, P 3 INT C LEARNING R; Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1; Kurach K, 2016, ERCIM NEWS, P25; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.1090/S0025-5718-1980-0572855-7; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Nunez Juan Carlos, 2018, NEUROCOMPUTING, V323; Okatani T, 2007, INT J COMPUT VISION, V72, P329, DOI 10.1007/s11263-006-9785-5; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Sabour Sara, 2017, PROC 31 INT C NEURAL; Samuel KGG, 2009, PROC CVPR IEEE, P477, DOI 10.1109/CVPRW.2009.5206774; SORENSEN DC, 1982, SIAM J NUMER ANAL, V19, P409, DOI 10.1137/0719026; Strelow D, 2012, PROC CVPR IEEE, P1584, DOI 10.1109/CVPR.2012.6247850; Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934; Taskar B, 2004, ADV NEUR IN, V16, P25; Taskar Ben, 2005, P 22 INT C MACH LEAR, P896; Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Trumble M., 2017, 2017 BRIT MACH VIS C; Vapnik V.N., 2006, ESTIMATION DEPENDENC, VVolume 40; WIBERG T, 1976, 2 S COMP STAT, P229; Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201; ZAGORUYKO S, 2015, 2015 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2015.7299064; Zeiler Matthew D, 2012, ARXIV12125701; Zhou Xingyi, 2017, IEEE INT C COMP VIS; Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301046
C	Traore, A; Berar, M; Rakotomamonjy, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Traore, Abraham; Berar, Maxime; Rakotomamonjy, Alain			Singleshot : a scalable Tucker tensor decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper introduces a new approach for the scalable Tucker decomposition problem. Given a tensor X, the algorithm proposed, named Singleshot, allows to perform the inference task by processing one subtensor drawn from X at a time. The key principle of our approach is based on the recursive computations of the gradient and on cyclic update of the latent factors involving only one single step of gradient descent. We further improve the computational efficiency of Singleshot by proposing an inexact gradient version named Singleshotinexact. The two algorithms are backed with theoretical guarantees of convergence and convergence rates under mild conditions. The scalabilty of the proposed approaches, which can be easily extended to handle some common constraints encountered in tensor decomposition (e.g non-negativity), is proven via numerical experiments on both synthetic and real data sets.	[Traore, Abraham; Berar, Maxime; Rakotomamonjy, Alain] Univ Rouen Normandy, LITIS EA4108, Mont St Aignan, France; [Rakotomamonjy, Alain] Criteo Paris, Criteo AI Lab, Paris, France		Traore, A (corresponding author), Univ Rouen Normandy, LITIS EA4108, Mont St Aignan, France.	abraham.traore@etu.univ-rouen.fr; maxime.berar@univ-rouen.fr; alain.rakoto@insa-rouen.fr			Normandie Projet GRR-DAISI; European funding FEDER DAISI; LEAUDS Project of the French National Research Agency (ANR) [ANR-18-CE23-0020]	Normandie Projet GRR-DAISI; European funding FEDER DAISI; LEAUDS Project of the French National Research Agency (ANR)(French National Research Agency (ANR))	This work was supported by grants from the Normandie Projet GRR-DAISI, European funding FEDER DAISI and LEAUDS ANR-18-CE23-0020 Project of the French National Research Agency (ANR).	Alistarh D., 2018, NEURIPS, P5977; Austin Woody, 2016, 2016 IEEE INT PAR DI; Ballard Grey, 2019, TUCKERMPI PARALLEL C; Baskaran Muthu, 2012, P IEEE C HIGH PERF E, P1; Beckmann S, 2018, ADV STEM EDU, P117, DOI 10.1007/978-3-319-61434-2_6; Caiafa CF, 2010, LINEAR ALGEBRA APPL, V433, P557, DOI 10.1016/j.laa.2010.03.020; Cattell RB, 1944, PSYCHOMETRIKA, V9, P267, DOI 10.1007/BF02288739; Chakaravarthy VT, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P374, DOI 10.1145/3205289.3205315; Che M., 2018, ADV COMPUT MATH, V45, P1; Choi Dongjin, 2017, CORR; Cichocki Andrzej, 2009, NONNEGATIVE MATRIX T, P2; De Lathauwer L, 2004, LINEAR ALGEBRA APPL, V391, P31, DOI 10.1016/j.laa.2004.01.016; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Drineas P, 2007, LINEAR ALGEBRA APPL, V420, P553, DOI 10.1016/j.laa.2006.08.023; Erdos D, 2013, IEEE DATA MINING, P1037, DOI 10.1109/ICDM.2013.141; Harper F. Maxwell, 2015, KSII T INTERNET INF; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; Jeon I, 2015, PROC INT CONF DATA, P1047, DOI 10.1109/ICDE.2015.7113355; Kaya O, 2016, PROC INT CONF PARAL, P103, DOI 10.1109/ICPP.2016.19; Kolda Tamara, 2006, WORKSH LINK AN COUNT, V7; Kolda TG, 2008, IEEE DATA MINING, P363, DOI 10.1109/ICDM.2008.89; Kossaifi J., 2018, TENSORLY TENSOR LEAR; Lee D, 2018, IEEE DATA MINING, P1098, DOI 10.1109/ICDM.2018.00142; Li Xiaoshan, 2013, STAT BIOSCIENCES, V04; Li XS, 2018, PROC INT CONF DATA, P1144, DOI 10.1109/ICDE.2018.00106; Lin CY, 2009, PROC INT CONF DATA, P1483, DOI 10.1109/ICDE.2009.140; MAHONEY MW, 2006, P 12 ANN ACM SIGKDD, P327, DOI DOI 10.1145/1150402.1150440; Navasca C, 2015, MATH VIS, P93, DOI 10.1007/978-3-319-15090-1_5; Oh J, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P761, DOI 10.1145/3018661.3018721; Oh S, 2018, PROC INT CONF DATA, P1120, DOI 10.1109/ICDE.2018.00104; Park Moonjeong, 2019, VEST VERY SPARSE TUC, V04; Park Namyong, 2019, VLDB J, P1; Perros I, 2015, IEEE DATA MINING, P943, DOI 10.1109/ICDM.2015.29; Shin K, 2017, IEEE T KNOWL DATA EN, V29, P100, DOI 10.1109/TKDE.2016.2610420; Sidiropoulos Nicholas D., 2014, PARALLEL RANDOMLY CO; Smith S., 2017, EURO PAR; Sun J., 2008, ACM T KNOWL DISCOV D, V2, P1, DOI [DOI 10.1145/1409620.1409621, 10.1145/1409620.1409621]; Sun Jimeng, 2009, SDM; Tseng P, 2007, MATH PROGRAM, V117, P387; Tsourakakis Charalampos E., 2009, SDM; Tucker L. R., 1963, PROBLEMS MEASURING C, V15, P122; Xu Y, 2017, DESTECH TRANS SOC, P247; Zdunek Rafal, 2008, INTELL NEUROSCI, V2008; Zhao Q., 2015, CORR; Zhe S., 2016, ADV NEURAL INFORM PR, P928; Zhe S, 2015, AISTATS; Zhe Shandian, 2016, AAAI; Zhou GX, 2015, IEEE T IMAGE PROCESS, V24, P4990, DOI 10.1109/TIP.2015.2478396; Zhou Guoxu, 2014, CORR	49	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306032
C	Tu, RB; Zhang, K; Bertilson, BC; Kjellstrom, H; Zhang, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tu, Ruibo; Zhang, Kun; Bertilson, Bo Christer; Kjellstrom, Hedvig; Zhang, Cheng			Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFERENCE; LATENT	Discovery of causal relations from observational data is essential for many disciplines of science and real-world applications. However, unlike other machine learning algorithms, whose development has been greatly fostered by a large amount of available benchmark datasets, causal discovery algorithms are notoriously difficult to be systematically evaluated because few datasets with known ground-truth causal relations are available. In this work, we handle the problem of evaluating causal discovery algorithms by building a flexible simulator in the medical setting. We develop a neuropathic pain diagnosis simulator, inspired by the fact that the biological processes of neuropathic pathophysiology are well studied with well-understood causal influences. Our simulator exploits the causal graph of the neuropathic pain pathology and its parameters in the generator are estimated from real-life patient cases. We show that the data generated from our simulator have similar statistics as real-world data. As a clear advantage, the simulator can produce infinite samples without jeopardizing the privacy of real-world patients. Our simulator provides a natural tool for evaluating various types of causal discovery algorithms, including those to deal with practical issues in causal discovery, such as unknown confounders, selection bias, and missing data. Using our simulator, we have evaluated extensively causal discovery algorithms under various settings.	[Tu, Ruibo; Kjellstrom, Hedvig] KTH Royal Inst Technol, Stockholm, Sweden; [Zhang, Kun] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Bertilson, Bo Christer] Karolinska Inst, Stockholm, Sweden; [Zhang, Cheng] Microsoft Res, Cambridge, England	Royal Institute of Technology; Carnegie Mellon University; Karolinska Institutet; Microsoft	Tu, RB (corresponding author), KTH Royal Inst Technol, Stockholm, Sweden.	ruibo@kth.se; kunz1@cmu.edu; bo.bertilson@ki.se; hedvig@kth.se; Cheng.Zhang@microsoft.com	Bertilson, Bo Christer/AAK-9552-2021		National Institutes of Health [NIH-iRO1EB022858-01, FAINR01EB022858, NIH-iRO1LM012087, NIH-5U54HG008540-02, FAIN-U54HG008540]; United States Air Force [FA8650-17-C-7715]; National Science Foundation EAGER [IIS-1829681]	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); United States Air Force(United States Department of Defense); National Science Foundation EAGER	Kun Zhang would like to acknowledge the support by National Institutes of Health under Contract No. NIH-iRO1EB022858-01, FAINR01EB022858, NIH-iRO1LM012087, NIH-5U54HG008540-02, and FAIN-U54HG008540, by the United States Air Force under Contract No. FA8650-17-C-7715, and by National Science Foundation EAGER Grant No. IIS-1829681. The National Institutes of Health, the U.S. Air Force, and the National Science Foundation are not responsible for the views reported in this article. In addition, the authors thank Akshaya Thippur Sridatta and Tino Weinkauf for the help of the audio dubbing of the 3-minute introduction video at https://youtu.be/1UvVnIbj SX8 and the visualization of the causal graph.	Amodei D, 2016, PR MACH LEARN RES, V48; BAREINBOIM E, 2008, PANEL DISCUSSION DAT; Bewley A, 2018, ARXIV181203823; Brockman G., 2016, OPENAI GYM; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Claassen T., 2012, ARXIV12104866; Cobbe K., 2018, ARXIV181202341, P1; Colloca L, 2017, NAT REV DIS PRIMERS, V3, DOI 10.1038/nrdp.2017.2; Colombo D, 2012, ANN STAT, V40, P294, DOI 10.1214/11-AOS940; Correa JD, 2017, AAAI CONF ARTIF INTE, P3740; Cortes C, 2008, LECT NOTES ARTIF INT, V5254, P38, DOI 10.1007/978-3-540-87987-9_8; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dixit A, 2016, CELL, V167, P1853, DOI 10.1016/j.cell.2016.11.038; Garant D., 2016, ARXIV160804698; Geng CR, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13646-z; Hoyer PO, 2008, INT J APPROX REASON, V49, P362, DOI 10.1016/j.ijar.2008.02.006; Johnson M., 2016, P 25 INT JOINT C ART, P4246; Kallus N., 2019, 22 INT C ART INT STA, P2281; Lim Bryan, 2018, P NIPS, P7483; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mohan K, 2013, ADV NEUTRAL INFORM P, V26, P1277; Mohan K, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5082; Mooij JM, 2016, J MACH LEARN RES, V17; Oberst M, 2019, PR MACH LEARN RES, V97; Ohnmeiss DD, 1999, CLIN J PAIN, V15, P210, DOI 10.1097/00002508-199909000-00008; Osama M., 2019, ARXIV190109919; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Peters J, 2017, ADAPT COMPUT MACH LE; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Ramsey JD, 2019, OPEN PHILOS, V2, P39, DOI 10.1515/opphil-2019-0005; RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.2307/2335739; Runge J, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10105-3; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Shpitser I., 2016, ADV NEURAL INFORM PR, P3144; Spines P., 2004, TETRAD PROJECT CAUSA; Spirtes P., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P499; Spirtes P., 2000, CAUSATION PREDICTION; Strobl E. V., INT J DATA SCI ANAL, P1; Tanaka Y, 2006, SPINE, V31, pE568, DOI 10.1097/01.brs.0000229261.02816.48; Tu R., 2019, P MACH LEARN RES, V89, P1762; Wang F, 2017, P IEEE C COMP VIS PA, P3156, DOI DOI 10.1109/TMI.2019.2893944; Zhang C., 2016, MLHC; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001; Zhang K., 2016, UAI; Zhang K, 2018, NATL SCI REV, V5, P26, DOI 10.1093/nsr/nwx137	48	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904044
C	Tu, T; Paisley, J; Haufe, S; Sajda, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tu, Tao; Paisley, John; Haufe, Stefan; Sajda, Paul			A state-space model for inferring effective connectivity of latent neural dynamics from simultaneous EEG/fMRI	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BRAIN; INFERENCE; RESPONSES; EEG	Inferring effective connectivity between spatially segregated brain regions is important for understanding human brain dynamics in health and disease. Non-invasive neuroimaging modalities, such as electroencephalography (EEG) and functional magnetic resonance imaging (fMRI), are often used to make measurements and infer connectivity. However most studies do not consider integrating the two modalities even though each is an indirect measure of the latent neural dynamics and each has its own spatial and/or temporal limitations. In this study, we develop a linear state-space model to infer the effective connectivity in a distributed brain network based on simultaneously recorded EEG and fMRI data. Our method first identifies task-dependent and subject-dependent regions of interest (ROI) based on the analysis of fMRI data. Directed influences between the latent neural states at these ROIs are then modeled as a multivariate autogressive (MVAR) process driven by various exogenous inputs. The latent neural dynamics give rise to the observed scalp EEG measurements via a biophysically informed linear EEG forward model. We use a mean-field variational Bayesian approach to infer the posterior distribution of latent states and model parameters. The performance of the model was evaluated on two sets of simulations. Our results emphasize the importance of obtaining accurate spatial localization of ROls from fMRI. Finally, we applied the model to simultaneously recorded EEG-fMRI data from 10 subjects during a Face-Car-House visual categorization task and compared the change in connectivity induced by different stimulus categories.	[Tu, Tao; Paisley, John; Sajda, Paul] Columbia Univ, New York, NY 10027 USA; [Haufe, Stefan] Charite Univ Med Berlin, Berlin, Germany	Columbia University; Free University of Berlin; Humboldt University of Berlin; Charite Universitatsmedizin Berlin	Tu, T (corresponding author), Columbia Univ, New York, NY 10027 USA.	tt2531@columbia.edu; jpaisley@columbia.edu; stefan.haufe@charite.de; psajda@columbia.edu			Army Research Laboratory [W911NF-10-2-0022]; Army Research Office [W911NF-16-1-0507]	Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); Army Research Office	This work was supported by the Army Research Laboratory (Cooperative Agreement W911NF-10-2-0022) and the Army Research Office (Grant W911NF-16-1-0507).	Bishop C.M, 2006, PATTERN RECOGN; Cheung BLP, 2010, IEEE T BIO-MED ENG, V57, P2122, DOI 10.1109/TBME.2010.2050319; Dale AM, 2000, NEURON, V26, P55, DOI 10.1016/S0896-6273(00)81138-1; David O, 2006, NEUROIMAGE, V30, P1255, DOI 10.1016/j.neuroimage.2005.10.045; Dong S, 2019, IEEE T BIO-MED ENG, V66, P2152, DOI 10.1109/TBME.2018.2884169; Friston KJ, 2008, NEUROIMAGE, V39, P1104, DOI 10.1016/j.neuroimage.2007.09.048; Friston KJ, 2011, BRAIN CONNECT, V1, P13, DOI 10.1089/brain.2011.0008; Friston KJ, 2003, NEUROIMAGE, V19, P1273, DOI 10.1016/S1053-8119(03)00202-7; Griffin JE, 2010, BAYESIAN ANAL, V5, P171, DOI 10.1214/10-BA507; Hallez H, 2007, J NEUROENG REHABIL, V4, DOI 10.1186/1743-0003-4-46; HAMALAINEN MS, 1994, MED BIOL ENG COMPUT, V32, P35, DOI 10.1007/BF02512476; Haufe S, 2010, IEEE T BIO-MED ENG, V57, P1954, DOI 10.1109/TBME.2010.2046325; Hong KS, 2014, BIOMED OPT EXPRESS, V5, P1778, DOI 10.1364/BOE.5.001778; Janoos Firdaus, 2011, BIENN INT C INF PROC, P588; LEONARD T, 1992, ANN STAT, V20, P1669, DOI 10.1214/aos/1176348885; Li Yitong, 2017, ADV NEURAL INFORM PR, P4620; Liu AK, 1998, P NATL ACAD SCI USA, V95, P8945, DOI 10.1073/pnas.95.15.8945; Muraskin J, 2018, NEUROIMAGE, V180, P211, DOI 10.1016/j.neuroimage.2017.06.059; Ryali S, 2011, NEUROIMAGE, V54, P807, DOI 10.1016/j.neuroimage.2010.09.052; Stephan KE, 2010, WIRES COGN SCI, V1, P446, DOI 10.1002/wcs.58; Suk HI, 2016, NEUROIMAGE, V129, P292, DOI 10.1016/j.neuroimage.2016.01.005; Tu T, 2017, J NEUROSCI, V37, P12226, DOI 10.1523/JNEUROSCI.1677-17.2017; Wipf D. P., 2008, ADV NEURAL INFORM PR, V20, P1625; Yang Y., 2016, P ADV NEUR INF PROC, P1234	24	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304064
C	Turchetta, M; Berkenkamp, F; Krause, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Turchetta, Matteo; Berkenkamp, Felix; Krause, Andreas			Safe Exploration for Interactive Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.	[Turchetta, Matteo; Berkenkamp, Felix; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	ETH Zurich	Turchetta, M (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	matteotu@inf.ethz.ch; befelix@inf.ethz.ch; krausea@ethz.ch		Krause, Andreas/0000-0001-7260-9673	Max Planck ETH Center for Learning Systems; European Research Council (ERC) under the European Union [815943]	Max Planck ETH Center for Learning Systems; European Research Council (ERC) under the European Union(European Research Council (ERC))	This research was partially supported by the Max Planck ETH Center for Learning Systems and by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme grantagreement No 815943.	Akametalu AK, 2014, IEEE DECIS CONTR P, P1424, DOI 10.1109/CDC.2014.7039601; Berkenkamp F., 2016, ARXIV PREPRINT ARXIV; Berkenkamp Felix, 2017, ADV NEURAL INFORM PR, P908; Biyik Erdem, 2019, P AM CONTR C ACC JUL; Chowdhury SR, 2017, PR MACH LEARN RES, V70; Gelbart MA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P250; Koller Torsten, 2018, P IEEE C DEC CONTR C; Marco A., 2017, P IEEE INT C ROB AUT, P1557, DOI DOI 10.1109/ICRA.2017.7989186; MCEWEN AS, 2007, J GEOPHYS RES-PLANET, V112, DOI DOI 10.1029/2005JE002605; Moldovan T.M., 2012, P 29 INT C MACH LEAR, P1451, DOI DOI 10.5555/3042573.3042759; MSL, 2007, MSL LAND SIT SEL US; Peters J, 2017, ADAPT COMPUT MACH LE; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Schreiter J, 2015, LECT NOTES ARTIF INT, V9286, P133, DOI 10.1007/978-3-319-23461-8_9; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sui YA, 2018, PR MACH LEARN RES, V80; Sui YA, 2015, PR MACH LEARN RES, V37, P997; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Turchetta M., 2016, ADV NEURAL INFORM PR, P4312; Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2; Wachi Akifumi, 2018, ASS ADVANCEMENT ARTI; Wang Z, 2017, PR MACH LEARN RES, V70; WHITE DJ, 1993, J OPER RES SOC, V44, P1073, DOI 10.2307/2583870; Wu YF, 2016, PR MACH LEARN RES, V48	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302084
C	Uppal, A; Singh, S; Poczos, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Uppal, Ananya; Singh, Shashank; Poczos, Barnabas			Nonparametric Density Estimation and Convergence of GANs under Besov IPM Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the problem of estimating a nonparametric probability density under a large family of losses called Besov IPMs, which include, for example, L-p distances, total variation distance, and generalizations of both Wasserstein and Kolmogorov-Smirnov distances. For a wide variety of settings, we provide both lower and upper bounds, identifying precisely how the choice of loss function and assumptions on the data interact to determine the minimax optimal convergence rate. We also show that linear distribution estimates, such as the empirical distribution or kernel density estimator, often fail to converge at the optimal rate. Our bounds generalize, unify, or improve several recent and classical results. Moreover, IPMs can be used to formalize a statistical model of generative adversarial networks (GANs). Thus, we show how our results imply bounds on the statistical error of a GAN, showing, for example, that GANs can strictly outperform the best linear estimator.	[Uppal, Ananya] Carnegie Mellon Univ, Dept Math Sci, Pittsburgh, PA 15213 USA; [Singh, Shashank; Poczos, Barnabas] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Singh, Shashank] Google, Mountain View, CA 94043 USA	Carnegie Mellon University; Carnegie Mellon University; Google Incorporated	Uppal, A (corresponding author), Carnegie Mellon Univ, Dept Math Sci, Pittsburgh, PA 15213 USA.	auppal@andrew.cmu.edu; sss1@cs.cmu.edu; bapoczos@cs.cmu.edu						Abbasnejad Ehsan, 2018, DEEP LIPSCHITZ NETWO; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bottou L, 2018, LECT NOTES ARTIF INT, V11100, P229, DOI 10.1007/978-3-319-99492-5_11; Choi E, 2017, P MACH LEARN HEALTHC; Daniel WW, 1978, APPL NONPARAMETRIC S; Dizaji KG, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1435, DOI 10.1145/3219819.3220114; Dong H.-W., 2019, ARXIV190108753, P08753; Donoho DL, 1996, ANN STAT, V24, P508; DUDLEY RM, 1969, ANN MATH STAT, V40, P40, DOI 10.1214/aoms/1177697802; DUDLEY RM, 1972, Z WAHRSCHEINLICHKEIT, V22, P323, DOI 10.1007/BF00532491; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Evans LC., 1998, PARTIAL DIFFERENTIAL, DOI DOI 10.1090/GSM/019; Feng BQ, 2017, CHIN CONT DECIS CONF, P1220, DOI 10.1109/CCDC.2017.7978704; Gidel G., 2018, ARXIV180704740; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Kadurin A, 2017, MOL PHARMACEUT, V14, P3098, DOI 10.1021/acs.molpharmaceut.7b00346; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lei Jing, 2018, ARXIV180410556; Liang T., 2018, ARXIV PREPRINT ARXIV; Liang Tengyuan, 2017, ARXIV171208244; Liang Tengyuan, 2018, ARXIV180206132; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mescheder L, 2018, PR MACH LEARN RES, V80; Meyer Y., 1992, WAVELETS OPERATORS, V1; Mohamed Shakir, 2016, ARXIV161003483; Mroueh Youssef, 2017, P 6 INT C LEARN REPR; Nagarajan V, 2017, ADV NEUR IN, V30; Nemirovski A.S, 1985, AKAD NAUK SSSR TEKHN, V23, p[1, 50]; Pollard D., 1990, NSF CBMS REGIONAL C; Sadhanala Veeranjaneyulu, 2019, INT C ART INT STAT; Sanchez-Lengeling B., 2017, OBJECTIVE REINFORCED, DOI [10.26434/chemrxiv.5309668.v3, DOI 10.26434/CHEMRXIV5309668V3]; SINGH S., 2018, ADV NEURAL INFORM PR, p31 10246; Singh Shashank, 2018, ARXIV180208855; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Sriperumbudur BK, 2010, IEEE INT SYMP INFO, P1428, DOI 10.1109/ISIT.2010.5513626; Suzuki Taiji, 2018, ARXIV181008033; Tang F, 2017, IEEE ANN INT CONF CY, P551, DOI 10.1109/CYBER.2017.8446518; Tao Terence, 2011, TYPE DIAGRAM FUNCTIO; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wassermann L., 2006, ALL NONPARAMETRIC ST; Weed J., 2019, COLT; Weed J, 2017, ARXIV170700087; Yang Zhen, 2017, ARXIV17030488; Yosida Kosaku, 1995, CLASSICS MATH, V11, P14; Zellinger Werner, 2019, INFORM SCI; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629	59	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900065
C	van Seijen, H; Fatemi, M; Tavakoli, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		van Seijen, Harm; Fatemi, Mehdi; Tavakoli, Arash			Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ENVIRONMENT	In an effort to better understand the different ways in which the discount factor affects the optimization process in reinforcement learning, we designed a set of experiments to study each effect in isolation. Our analysis reveals that the common perception that poor performance of low discount factors is caused by (too) small action-gaps requires revision. We propose an alternative hypothesis that identifies the size-difference of the action-gap across the state-space as the primary cause. We then introduce a new method that enables more homogeneous action-gaps by mapping value estimates to a logarithmic space. We prove convergence for this method under standard assumptions and demonstrate empirically that it indeed enables lower discount factors for approximate reinforcement-learning methods. This in turn allows tackling a class of reinforcement-learning problems that are challenging to solve with traditional methods.	[van Seijen, Harm; Fatemi, Mehdi] Microsoft Res Montreal, Montreal, PQ, Canada; [Tavakoli, Arash] Imperial Coll London, London, England	Imperial College London	van Seijen, H (corresponding author), Microsoft Res Montreal, Montreal, PQ, Canada.	harm.vanseijen@microsoft.com; mehdi.fatemi@microsoft.com; a.tavakoli@imperial.ac.uk						[Anonymous], 2018, P 35 INT C MACH LEAR; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2016, AAAI CONF ARTIF INTE, P1476; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Castro Pablo Samuel, 2018, ARXIV181206110; Farahmand AM, 2011, ADV NEURAL INFORM PR, P172; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Jaakkola T., 1994, ADV NEURAL INFORM PR; Machado MC, 2018, J ARTIF INTELL RES, V61, P523, DOI 10.1613/jair.5699; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pohlen T., 2018, ARXIV PREPRINT ARXIV; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Silver D., 2018, ARXIV PREPRINT ARXIV; Sutton RS, 1996, ADV NEUR IN, V8, P1038; Wang ZY, 2016, PR MACH LEARN RES, V48; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905075
C	Vaquero, M; Cortes, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vaquero, Miguel; Cortes, Jorge			Convergence-Rate-Matching Discretization of Accelerated Optimization Flows Through Opportunistic State-Triggered Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A recent body of exciting work seeks to shed light on the behavior of accelerated methods in optimization via high-resolution differential equations. These differential equations are continuous counterparts of the discrete-time optimization algorithms, and their convergence properties can be characterized using the powerful tools provided by classical Lyapunov stability analysis. An outstanding question of pivotal importance is how to discretize these continuous flows while maintaining their convergence rates. This paper provides a novel approach through the idea of opportunistic state-triggered control. We take advantage of the Lyapunov functions employed to characterize the rate of convergence of high-resolution differential equations to design variable-stepsize forward-Euler discretizations that preserve the Lyapunov decay of the original dynamics. The philosophy of our approach is not limited to forward-Euler discretizations and may be combined with other integration schemes.	[Vaquero, Miguel; Cortes, Jorge] Univ Calif San Diego, Mech & Aerosp Engn, San Diego, CA 92103 USA	University of California System; University of California San Diego	Vaquero, M (corresponding author), Univ Calif San Diego, Mech & Aerosp Engn, San Diego, CA 92103 USA.	mivaquerovallina@ucsd.edu; cortes@ucsd.edu			NSF [CNS-1446891]; AFOSR [FA9550-15-1-0108]	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was supported by NSF Award CNS-1446891 and AFOSR Award FA9550-15-1-0108.	Alvarez F, 2002, J MATH PURE APPL, V81, P747, DOI 10.1016/S0021-7824(01)01253-3; Attouch H., 2019, ARXIV PREPRINT ARXIV; Betancourt M., 2018, ARXIV180203653; Bubeck S., 2015, ARXIV150608187MATHOC; Dolk VS, 2017, IEEE T AUTOMAT CONTR, V62, P34, DOI 10.1109/TAC.2016.2536707; Fermi J., 2011, IFAC P VOLUMES, V44, P8724; G. Franca, 2019, ARXIV190304100; Goebel R., 2012, HYBRID DYNAMICAL SYS; Hairer E., 2010, GEOMETRIC NUMERICAL; Hairer E, 1993, NONSTIFF PROBLEMS; Heemels WPMH, 2012, IEEE DECIS CONTR P, P3270, DOI 10.1109/CDC.2012.6425820; Heemels WPMH, 2011, IEEE DECIS CONTR P, P2571; Hu B, 2017, IEEE INT CONGR BIG, P549, DOI 10.1109/BigDataCongress.2017.84; Kolarijani AS, 2018, PR MACH LEARN RES, V80; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Maddison Chris J, 2018, ARXIV180905042; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Nowzari C, 2019, AUTOMATICA, V105, P1, DOI 10.1016/j.automatica.2019.03.009; Ong P, 2018, IEEE DECIS CONTR P, P951, DOI 10.1109/CDC.2018.8619524; Polyak B.T., 1964, USSR COMP MATH MATH+, V4, P1, DOI [https://doi.org/10.1016/0041-5553(64)90137-5, DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; SHI B, 2018, ARXIV181008907; Shi B., 2019, ARXIV190203694; Su WJ, 2016, J MACH LEARN RES, V17; Velasco M, 2009, IEEE DECIS CONTR P, P6238, DOI 10.1109/CDC.2009.5400541; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Wilson A. C., 2018, ARXIV161102635; Zhang Jingzhao, 2018, P 32 INT C NEUR INF, P3904; Zhu Zeyuan Allen, 2017, 8 INN THEOR COMP SCI	29	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901040
C	Verma, A; Hanawal, MK; Rajkumar, A; Sankaran, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Verma, Arun; Hanawal, Manjesh K.; Rajkumar, Arun; Sankaran, Raman			Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we study CensoredSemi-Bandits, a novel variant of the semi-bandits problem. The learner is assumed to have a fixed amount of resources, which it allocates to the arms at each time step. The loss observed from an arm is random and depends on the amount of resources allocated to it. More specifically, the loss equals zero if the allocation for the arm exceeds a constant (but unknown) threshold that can be dependent on the arm. Our goal is to learn a feasible allocation that minimizes the expected loss. The problem is challenging because the loss distribution and threshold value of each arm are unknown. We study this novel setting by establishing its 'equivalence' to Multiple-Play Multi-Armed Bandits (MP-MAB) and Combinatorial Semi-Bandits. Exploiting these equivalences, we derive optimal algorithms for our setting using the existing algorithms for MP-MAB and Combinatorial Semi-Bandits. Experiments on synthetically generated data validate performance guarantees of the proposed algorithms.	[Verma, Arun; Hanawal, Manjesh K.] Indian Inst Technol, Dept IEOR, Bombay, Maharashtra, India; [Rajkumar, Arun] IIT Madras, Dept CSE, Madras, Tamil Nadu, India; [Sankaran, Raman] LinkedIn India, Bengaluru, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Bombay; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Verma, A (corresponding author), Indian Inst Technol, Dept IEOR, Bombay, Maharashtra, India.	v.arun@iitb.ac.in; mhanwal@iitb.ac.in; arunr@cse.iitm.ac.in; rsankara@linkedin.com			DST, Government of India; IIT Bombay [16IRCCSG010]; SERB	DST, Government of India(Department of Science & Technology (India)); IIT Bombay; SERB(Department of Science & Technology (India)Science Engineering Research Board (SERB), India)	Arun Verma would like to thank travel support from Google and NeuriPS. Manjesh K. Hanawal would like to thank the support from INSPIRE faculty fellowships from DST, Government of India, SEED grant (16IRCCSG010) from IIT Bombay, and Early Career Research (ECR) Award from SERB. Initial discussions of this work were done when Raman Sankaran was at Conduent Labs India.	Abernethy J. D., 2016, ADV NEURAL INFORM PR, P4889; Adler N, 2014, ANN OPER RES, V221, P9, DOI 10.1007/s10479-012-1275-2; ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; Badanidiyuru A, 2018, J ACM, V65, DOI 10.1145/3164539; Bartok Gabor, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P305, DOI 10.1007/978-3-642-34106-9_25; Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663; Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen S, 2013, IMBALANCED LEARNING: FOUNDATIONS, ALGORITHMS, AND APPLICATIONS, P151; Combes R., 2015, P 28 INT C NEUR INF, P2116; Curtin KM, 2010, NETW SPAT ECON, V10, P125, DOI 10.1007/s11067-007-9035-6; Dagan Y., 2018, ALT, P268; Gholami S, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P823; Hifi M, 2013, DISCRETE OPTIM, V10, P320, DOI 10.1016/j.disopt.2013.08.003; Jain Lalit, 2018, INT C MACH LEARN, P2211; Komiyama J, 2015, PR MACH LEARN RES, V37, P1152; Lattimore T., 2015, NEURIPS, P964; Lattimore T, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P477; LIN Q, 2016, ADV NEURAL INFORM PR, P1659, DOI DOI 10.1145/2882903.2882923; Nguyen TH, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P767; Rajkumar Arun, 2014, ADV NEURAL INFORM PR, P3482; Rosenfeld A, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3814; Sinha A, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5494; Zhang C, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1351; Zhang C, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P196; Zhong FW, 2018, IEEE WINT CONF APPL, P1001, DOI 10.1109/WACV.2018.00115	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906021
C	Nguyen, VA; Shafieezadeh-Abadeh, S; Yue, MC; Kuhn, D; Wiesemann, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Viet Anh Nguyen; Shafieezadeh-Abadeh, Soroosh; Yue, Man-Chung; Kuhn, Daniel; Wiesemann, Wolfram			Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPACE; MATRICES	A fundamental problem arising in many areas of machine learning is the evaluation of the likelihood of a given observation under different nominal distributions. Frequently, these nominal distributions are themselves estimated from data, which makes them susceptible to estimation errors. We thus propose to replace each nominal distribution with an ambiguity set containing all distributions in its vicinity and to evaluate an optimistic likelihood, that is, the maximum of the likelihood over all distributions in the ambiguity set. When the proximity of distributions is quantified by the Fisher-Rao distance or the Kullback-Leibler divergence, the emerging optimistic likelihoods can be computed efficiently using either geodesic or standard convex optimization techniques. We showcase the advantages of working with optimistic likelihoods on a classification problem using synthetic as well as empirical data.	[Viet Anh Nguyen; Shafieezadeh-Abadeh, Soroosh; Kuhn, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Yue, Man-Chung] Hong Kong Polytech Univ, Hong Kong, Peoples R China; [Wiesemann, Wolfram] Imperial Coll, Business Sch, London, England	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Hong Kong Polytechnic University; Imperial College London	Nguyen, VA (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	viet-anh.nguyen@epfl.ch; soroosh.shafiee@epfl.ch; manchung.yue@polyu.edu.hk; daniel.kuhn@epfl.ch; ww@imperial.ac.uk		Yue, Man-Chung/0000-0002-7992-9490	Swiss National Science Foundation [BSCGI0-157733]; EPSRC [EP/M028240/1, EP/M027856/1, EP/N020030/1]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We gratefully acknowledge financial support from the Swiss National Science Foundation under grant BSCGI0-157733 as well as the EPSRC grants EP/M028240/1, EP/M027856/1 and EP/N020030/1.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Arnaudon M., 2013, IEEE J SELECTED TOPI, V7, P595; Arnaudon M, 2013, IEEE J-STSP, V7, P595, DOI 10.1109/JSTSP.2013.2261798; Atkinson C., 1981, SANKHYA A, V43, P345; Bache K., 2013, CI MACHINE LEARNING; Bauer M, 2016, B LOND MATH SOC, V48, P499, DOI 10.1112/blms/bdw020; Bento GC, 2017, J OPTIMIZ THEORY APP, V173, P548, DOI 10.1007/s10957-017-1093-4; Bhatia R., 2009, POSITIVE DEFINITE MA; Bi J., 2005, ADV NEURAL INFORM PR; Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347; Boyd S, 2004, CONVEX OPTIMIZATION; Bridson MR., 2013, METRIC SPACES NONPOS; Brochu E., 2009, TECHNICAL REPORT UBC; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; CAMPBELL LL, 1986, P AM MATH SOC, V98, P135, DOI 10.2307/2045782; Cencov N. N., 2000, STATISTICALDECISION; Cover TM, 2006, ELEMENTS INFORM THEO; Cox DR, 2013, J R STAT SOC B, V75, P207, DOI 10.1111/rssb.12003; Cox D. R., 1961, PROCEEDINGSOF FOURTH, P105; Dwivedi Y, 2016, NEURAL PLAST, V2016, DOI 10.1155/2016/7383724; Esfahani P.M., 2018, ARXIV PREPRINT ARXIV; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Hardle W., 2015, APPL MULTIVARIATE ST; Jayasumana S, 2013, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2013.17; Lang S., 2012, NDAMENTALS DIFFE; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Liu C., 2019, APPL MATH OPTIMIZATI; McLachlan Geoffrey J., 2004, DISCRIMINANT ANAL ST, V544; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Nguyen V. A., 2019, ADV NEURAL INFORM PR; Norton M., 2017, ARXIV PREPRINT ARXIV; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Said S, 2017, IEEE T INFORM THEORY, V63, P2153, DOI 10.1109/TIT.2017.2653803; SAVAGE RP, 1982, T AM MATH SOC, V274, P239, DOI 10.2307/1999507; Schoenberg R., 1997, Computational Economics, V10, P251, DOI 10.1023/A:1008669208700; Sra S, 2015, SIAM J OPTIMIZ, V25, P713, DOI 10.1137/140978168; Srinivas N., 2010, INT C MACHINE LEARNI; Takatsu A, 2011, OSAKA J MATH, V48, P1005; Tripuraneni N., 2018, C ON LEARNING THEORY, V75, P650; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Villani C., 2008, OPTIMAL TRANSPORT OL; Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319; Zhang H., 2016, C LEARN THEOR, P1617; Zhang H., 2018, C ON LEARNING THEORY, V75, P1703	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905058
C	Vladymyrov, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vladymyrov, Max			No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIMENSIONALITY REDUCTION	Nonlinear embedding manifold learning methods provide invaluable visual insights into the structure of high-dimensional data. However, due to a complicated nonconvex objective function, these methods can easily get stuck in local minima and their embedding quality can be poor. We propose a natural extension to several manifold learning methods aimed at identifying pressured points, i.e. points stuck in poor local minima and have poor embedding quality. We show that the objective function can be decreased by temporarily allowing these points to make use of an extra dimension in the embedding space. Our method is able to improve the objective function value of existing methods even after they get stuck in a poor local minimum.	[Vladymyrov, Max] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Vladymyrov, M (corresponding author), Google Res, Mountain View, CA 94043 USA.	mxv@google.com						[Anonymous], 2016, USE T SNE EFFECTIVEL, DOI DOI 10.23915/DISTILL.00002; BECKER M, 2003, J HIGH ENERGY PHYS; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Carreira-Perpinan M., 2010, P 27 INT C MACH LEAR; Cook J., 2007, P 11 INT WORKSH ART; de Silva V., 2003, GLOBAL VERSUS LOCAL, P721; Kiros R., 2015, ADV NEURAL INF PROCE, P3294; Lee JH, 2009, 2009 IEEE 10TH ANNUAL WIRELESS AND MICROWAVE TECHNOLOGY CONFERENCE, P72; Lespinats S, 2011, COMPUT GRAPH FORUM, V30, P113, DOI 10.1111/j.1467-8659.2010.01835.x; McInnes L., 2018, ARXIV180203426; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Peltonen J, 2015, MACH LEARN, V99, P189, DOI 10.1007/s10994-014-5464-x; Roweis ST., 2003, STOCHASTIC NEIGHBOR, P857; Saul LK, 2004, J MACH LEARN RES, V4, P119, DOI 10.1162/153244304322972667; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venna J, 2010, J MACH LEARN RES, V11, P451; Vladymyrov M., 2012, P ICML, V29, P345; Vladymyrov M, 2014, JMLR WORKSH CONF PRO, V33, P968; Yang Z., P 230 INT C MACH LEA, P127; Yang ZR, 2015, JMLR WORKSH CONF PRO, V38, P1088	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300062
C	Wald, Y; Noy, N; Wiesel, A; Elidan, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wald, Yoav; Noy, Nofar; Wiesel, Ami; Elidan, Gal			Globally Optimal Learning for Structured Elliptical Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COVARIANCE; CONVEXITY	Heavy tailed and contaminated data are common in various applications of machine learning. A standard technique to handle regression tasks that involve such data, is to use robust losses, e.g., the popular Huber's loss. In structured problems, however, where there are multiple labels and structural constraints on the labels are imposed (or learned), robust optimization is challenging, and more often than not the loss used is simply the negative log-likelihood of a Gaussian Markov random field. In this work, we analyze robust alternatives. Theoretical understanding of such problems is quite limited, with guarantees on optimization given only for special cases and non-structured settings. The core of the difficulty is the non-convexity of the objective function, implying that standard optimization algorithms may converge to sub-optimal critical points. Our analysis focuses on loss functions that arise from elliptical distributions, which appealingly include most loss functions proposed in the literature as special cases. We show that, even though these problems are non-convex, they can be optimized efficiently. Concretely, we prove that at the limit of infinite training data, due to algebraic properties of the problem, all stationary points are globally optimal. Finally, we demonstrate the empirical appeal of using these losses for regression on synthetic and real-life data.	[Wald, Yoav; Noy, Nofar; Wiesel, Ami; Elidan, Gal] Hebrew Univ Jerusalem, Jerusalem, Israel; [Wald, Yoav; Wiesel, Ami; Elidan, Gal] Google Res, Mountain View, CA 94043 USA	Hebrew University of Jerusalem; Google Incorporated	Wald, Y (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.; Wald, Y (corresponding author), Google Res, Mountain View, CA 94043 USA.	yoav.wald@mail.huji.ac.il; nofar.noy@mail.huji.ac.il; awiesel@google.com; elidan@google.com			ISF [1339/15]	ISF(Israel Science Foundation)	We thank Elad Mezuman and Amir Globerson for fruitful discussions, and Guy Shalev for preparing the river discharge dataset. This research was partially supported by ISF grant 1339/15.	[Anonymous], 2013, NIPS; Bausson S, 2007, IEEE SIGNAL PROC LET, V14, P425, DOI 10.1109/LSP.2006.888400; BESAG J, 1986, J R STAT SOC B, V48, P259; CAMBANIS S, 1981, J MULTIVARIATE ANAL, V11, P368, DOI 10.1016/0047-259X(81)90082-8; Finegold M. A, 2009, P 25 C UNC ART INT, P169; Frahm G., 2004, THESIS; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Guo Y., 2019, P 35 C UNC ART INT; Hsieh C.-J., 2011, ADV NEURAL INFORM PR, P2330; Huber P., 1981, ROBUST STAT; Koller D., 2009, PROBABILISTIC GRAPHI; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Liu H., 2012, ADV NEURAL INFORM PR, V25, P800; Liu H, 2009, J MACH LEARN RES, V10, P2295; Liu M, 2018, INT SYM COMPUT INTEL, P206, DOI 10.1109/ISCID.2018.10148; Mazumder R, 2012, ELECTRON J STAT, V6, P2125, DOI 10.1214/12-EJS740; McCarter C, 2016, JMLR WORKSH CONF PRO, V51, P529; Mei S., 2016, ARXIV160706534; Noy N., 2019, 2019 8 IEEE INT WORK; Pascal F, 2013, IEEE T SIGNAL PROCES, V61, P5960, DOI 10.1109/TSP.2013.2282909; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Soloveychik I, 2015, IEEE T SIGNAL PROCES, V63, P418, DOI 10.1109/TSP.2014.2376911; Taskar B, 2004, ADV NEUR IN, V16, P25; TYLER DE, 1987, ANN STAT, V15, P234, DOI 10.1214/aos/1176350263; Ushio T., 2004, AQUA AMSR E, V2003; Vemulapalli R, 2016, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2016.351; Vogel D, 2011, BIOMETRIKA, V98, P935, DOI 10.1093/biomet/asr037; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wiesel A, 2012, IEEE T SIGNAL PROCES, V60, P6182, DOI 10.1109/TSP.2012.2218241; Wytock M, 2013, P 30 INT C MACH LEAR, P1265; Yang E, 2015, ADV NEUR IN, V28; YAO K, 1973, IEEE T INFORM THEORY, V19, P600, DOI 10.1109/TIT.1973.1055076; Zhang T, 2013, IEEE T SIGNAL PROCES, V61, P4141, DOI 10.1109/TSP.2013.2267740	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905019
C	Wang, AY; Tarr, MJ; Wehbe, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Aria Y.; Tarr, Michael J.; Wehbe, Leila			Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS; CORTEX; SCENES	Convolutional neural networks (CNNs) trained for object classification have been widely used to account for visually-driven neural responses in both human and primate brains. However, because of the generality and complexity of object classification, despite the effectiveness of CNNs in predicting brain activity, it is difficult to draw specific inferences about neural information processing using CNNderived representations. To address this problem, we used learned representations drawn from 21 computer vision tasks to construct encoding models for predicting brain responses from BOLD5000-a large-scale dataset comprised of fMRI scans collected while observers viewed over 5000 naturalistic scene and object images. Encoding models based on task features predict activity in different regions across the whole brain. Features from 3D tasks such as keypoint/edge detection explain greater variance compared to 2D tasks-a pattern observed across the whole brain. Using results across all 21 task representations, we constructed a "task graph" based on the spatial layout of well-predicted brain areas from each task. A comparison of this brain-derived task structure to the task structure derived from transfer learning accuracy demonstrate that tasks with higher transferability make similar predictions for brain responses from different regions. These results-arising out of state-ofthe-art computer vision methods-help reveal the task-specific architecture of the human visual system.	[Wang, Aria Y.; Tarr, Michael J.; Wehbe, Leila] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, AY (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ariawang@cmu.edu; michaeltarr@cmu.edu; lwehbe@cmu.edu		Wang, Aria/0000-0002-3094-7786; Tarr, Michael/0000-0003-4724-1744				Agrawal P, 2014, ARXIV14075104; Bonner MF, 2017, P NATL ACAD SCI USA, V114, P4793, DOI 10.1073/pnas.1618228114; Chang N, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0052-3; Dwivedi Kshitij, 2018, BIORXIV; Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001; Epstein R, 1998, NATURE, V392, P598, DOI 10.1038/33402; Ferrara K, 2016, NEUROPSYCHOLOGIA, V89, P180, DOI 10.1016/j.neuropsychologia.2016.05.012; Gao JS, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00023; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Harel A, 2013, CEREB CORTEX, V23, P947, DOI 10.1093/cercor/bhs091; Huth AG, 2016, NATURE, V532, P453, DOI 10.1038/nature17637; Kamps FS, 2016, NEUROIMAGE, V132, P417, DOI 10.1016/j.neuroimage.2016.02.062; Kell AJE, 2018, NEURON, V98, P630, DOI 10.1016/j.neuron.2018.03.044; Kornblith S, 2013, NEURON, V79, P766, DOI 10.1016/j.neuron.2013.06.015; Koushik Jayanth, 2017, MICROSOFT COCO COMMO; Koushik Jayanth, 2017, TORCH GEL; Kravitz DJ, 2011, J NEUROSCI, V31, P7322, DOI 10.1523/JNEUROSCI.4588-10.2011; Lescroart MD, 2019, NEURON, V101, P178, DOI 10.1016/j.neuron.2018.11.004; Lescroart MD, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00135; Naselaris T, 2011, NEUROIMAGE, V56, P400, DOI 10.1016/j.neuroimage.2010.07.073; Park S, 2015, CEREB CORTEX, V25, P1792, DOI 10.1093/cercor/bht418; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Stansbury DE, 2013, NEURON, V79, P1025, DOI 10.1016/j.neuron.2013.06.034; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907019
C	Wang, LX; Yang, ZR; Wang, ZR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Lingxiao; Yang, Zhuoran; Wang, Zhaoran			Statistical-Computational Tradeoffs in High-Dimensional Single Index Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the statistical-computational tradeoffs in a high dimensional single index model Y = f (X-inverted perpendicular beta*) + epsilon, where f is unknown, X is a Gaussian vector and beta* is s-sparse with unit norm. When Cov(Y, X-inverted perpendicular beta*) not equal 0, [43] shows that the direction and support of beta* can be recovered using a generalized version of Lasso. In this paper, we investigate the case when this critical assumption fails to hold, where the problem becomes considerably harder. Using the statistical query model to characterize the computational cost of an algorithm, we show that when Cov(Y, X-inverted perpendicular beta*) = 0 and Cov(Y, X-inverted perpendicular beta*)(2)) > 0, no computationally tractable algorithms can achieve the information -theoretic limit of the minimax risk. This implies that one must pay an extra computational cost for the nonlinearity involved in the model.	[Wang, Lingxiao; Wang, Zhaoran] Northwestern Univ, Evanston, IL 60208 USA; [Yang, Zhuoran] Princeton Univ, Princeton, NJ 08544 USA	Northwestern University; Princeton University	Wang, LX (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.	lingxiaowang2022@u.northwestern.edu; zy6@princeton.edu; zhaoranwang@gmail.com	Wang, Zhaoran/P-7113-2018						0	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902009
C	Wang, MY; Zeng, YC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Miaoyan; Zeng, Yuchen			Multiway clustering via tensor block models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECOMPOSITIONS	We consider the problem of identifying multiway block structure from a large noisy tensor. Such problems arise frequently in applications such as genomics, recommendation system, topic modeling, and sensor network localization. We propose a tensor block model, develop a unified least-square estimation, and obtain the theoretical accuracy guarantees for multiway clustering. The statistical convergence of the estimator is established, and we show that the associated clustering procedure achieves partition consistency. A sparse regularization is further developed for identifying important blocks with elevated means. The proposal handles a broad range of data types, including binary, continuous, and hybrid observations. Through simulation and application to two real datasets, we demonstrate the outperformance of our approach over previous methods.	[Wang, Miaoyan; Zeng, Yuchen] Univ Wisconsin, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Wang, MY (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	miaoyan.wang@wisc.edu; yzeng58@wisc.edu			NSF [DMS-1915978]; University of Wisconsin-Madison, Office of the Vice Chancellor for Research and Graduate Education; Wisconsin Alumni Research Foundation	NSF(National Science Foundation (NSF)); University of Wisconsin-Madison, Office of the Vice Chancellor for Research and Graduate Education; Wisconsin Alumni Research Foundation	This research was supported by NSF grant DMS-1915978 and the University of Wisconsin-Madison, Office of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation.	Abdi H., 2003, ENCYCL RES METHODS S, V6, P792, DOI DOI 10.4135/9781412950589.N690; Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], 2018, ARXIV181106055; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; Chen Kehui, 2019, BIOMETRIKA; Chi EC, 2018, ARXIV180306518; DARTON RA, 1980, STATISTICIAN, V29, P167, DOI 10.2307/2988040; Gao C., 2016, J MACH LEARN RES, V17, P5602; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; Hore V, 2016, NAT GENET, V48, P1094, DOI 10.1038/ng.3624; Jegelka S, 2009, LECT NOTES ARTIF INT, V5809, P368, DOI 10.1007/978-3-642-04414-4_30; Kang YB, 2015, J THEOR PROBAB, V28, P1007, DOI 10.1007/s10959-013-0537-5; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kolda TG, 2008, IEEE DATA MINING, P363, DOI 10.1109/ICDM.2008.89; Madeira SC, 2004, IEEE ACM T COMPUT BI, V1, P24, DOI 10.1109/TCBB.2004.2; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; O'Leary NA, 2016, NUCLEIC ACIDS RES, V44, pD733, DOI 10.1093/nar/gkv1189; Tan KM, 2014, J COMPUT GRAPH STAT, V23, P985, DOI 10.1080/10618600.2013.852554; Tang Yichuan, 2013, INT C MACH LEARN, P163; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Wang Miaoyan, 2019, ANN APPL STAT; Wang Miaoyan, 2018, ARXIV181105076; Zhang A., 2018, IEEE T INFORM THEORY; Zhou H, 2013, J AM STAT ASSOC, V108, P540, DOI 10.1080/01621459.2013.776499	27	0	0	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300065
C	Wang, WL; Tao, CY; Gan, Z; Wang, GY; Chen, LQ; Zhang, XY; Zhang, RY; Yang, Q; Henao, R; Carin, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Wenlin; Tao, Chenyang; Gan, Zhe; Wang, Guoyin; Chen, Liqun; Zhang, Xinyuan; Zhang, Ruiyi; Yang, Qian; Henao, Ricardo; Carin, Lawrence			Improving Textual Network Learning with Variational Homophilic Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The performance of many network learning applications crucially hinges on the success of network embedding algorithms, which aim to encode rich network information into low-dimensional vertex-based vector representations. This paper considers a novel variational formulation of network embeddings, with special focus on textual networks. Different from most existing methods that optimize a discriminative objective, we introduce Variational Homophilic Embedding (VHE), a fully generative model that learns network embeddings by modeling the semantic (textual) information with a variational autoencoder, while accounting for the structural (topology) information through a novel homophilic prior design. Homophilic vertex embeddings encourage similar embedding vectors for related (connected) vertices. The proposed VHE promises better generalization for downstream tasks, robustness to incomplete observations, and the ability to generalize to unseen vertices. Extensive experiments on real-world networks, for multiple tasks, demonstrate that the proposed method consistently achieves superior performance relative to competing state-of-the-art approaches.	[Wang, Wenlin; Tao, Chenyang; Wang, Guoyin; Chen, Liqun; Zhang, Xinyuan; Zhang, Ruiyi; Yang, Qian; Henao, Ricardo; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA; [Gan, Zhe] Microsoft Dynam 365 AI Res, Dana Point, CA USA	Duke University	Wang, WL (corresponding author), Duke Univ, Durham, NC 27706 USA.	wenlin.wang@duke.edu	Zhang, Ruiyi/AAB-8402-2021		DARPA; DOE; NIH; ONR; NSF	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by DARPA, DOE, NIH, ONR and NSF.	Ahmed A., 2013, WWW; Ainsworth S., 2018, ARXIV180206765; Airoldi E., 2008, JMLR; [Anonymous], 2016, ARXIV; [Anonymous], 2014, ICLR; Barabasi A.-L., 2016, NETW SCI; Battaglia Peter W, 2018, ARXIV 180601261; Belkin Mikhail, 2002, NEURIPS; Blei D. M., 2017, J AM STAT ASS; Chen Jifan, 2016, KDD; Chen Siheng, 2017, ARXIV170205764; Cui P., 2018, IEEE T KNOWLEDGE DAT; Dereniowski Dariusz, 2003, PPAM; Donahue J., 2016, ARXIV160509782; Fan R.-E., 2008, JMLR; Friedman J, 2001, SPRINGER SERIES STAT, V10; Grover A., 2016, KDD; Hanley J. A., 1982, RADIOLOGY; Jacovi A., 2018, ARXIV PREPRINT ARXIV; Jebara T., 2012, MACHINE LEARNING DIS, V755; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2016, VARIATIONAL GRAPH AU; Le T. M. V., 2014, ICDM; Lerer A, 2019, ARXIV190312287; Leskovec J., 2005, KDD; Liu Qi, 2018, NEURIPS; Maaten L. v. d, 2008, JMLR; MCCALLUM A, 2000, INFORM RETRIEVAL; McPherson M., 2001, ANN REV SOCIOLOGY; Newman MEJ, 2018, NATURE PHYS; PEROZZI B, 2014, KDD; Petersen K. B., 2012, MATRIX COOKBOOK; Shen Dinghan, 2018, ARXIV180505361; Shen Dinghan, 2018, EMNLP; Simonovsky Martin, 2018, ICANN; Sun X., 2016, ARXIV161002906; Tang J., 2015, WWW; Tang L., 2009, KDD; Tomczak J., 2018, AISTATS; Tu C., 2016, IJCAI; Tu Cunchao, 2017, ACL; Wang D., 2016, KDD; Wang G., 2018, ACL; Wang W., 2018, AAAI; Wang W, 2017, ARXIV171209783; Wang Wenlin, 2019, NAACL; Wang Xiao, 2017, AAAI; Yan Ting, 2018, JASA; Yang C., 2015, IJCAI; Yang Qian, 2019, EMNLP; Zhang Xiuming, 2018, NEURIPS	51	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302011
C	Wang, Y; Jiang, ZY; Chen, XH; Xu, PF; Zhao, Y; Lin, YY; Wang, ZY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Yue; Jiang, Ziyu; Chen, Xiaohan; Xu, Pengfei; Zhao, Yang; Lin, Yingyan; Wang, Zhangyang			E-2-Train: Training State-of-the-art CNNs with Over 80% Less Energy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Convolutional neural networks (CNNs) have been increasingly deployed to edge devices. Hence, many efforts have been made towards efficient CNN inference in resource-constrained platforms. This paper attempts to explore an orthogonal direction: how to conduct more energy-efficient training of CNNs, so as to enable on-device training? We strive to reduce the energy cost during training, by dropping unnecessary computations, from three complementary levels: stochastic mini-batch dropping on the data level; selective layer update on the model level; and sign prediction for low-cost, low-precision back-propagation, on the algorithm level. Extensive simulations and ablation studies, with real energy measurements from an FPGA board, confirm the superiority of our proposed strategies and demonstrate remarkable energy savings for training. For example, when training ResNet-74 on CIFAR-10, we achieve aggressive energy savings of >90% and >60%, while incurring a top-1 accuracy loss of only about 2% and 1.2%, respectively. When training ResNet-110 on CIFAR-100, an over 84% training energy saving is achieved without degrading inference accuracy.	[Jiang, Ziyu; Chen, Xiaohan; Wang, Zhangyang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA; [Wang, Yue; Xu, Pengfei; Zhao, Yang; Lin, Yingyan] Rice Univ, Dept Elect & Comp Engn, Houston, TX 77251 USA	Texas A&M University System; Texas A&M University College Station; Rice University	Wang, Y (corresponding author), Rice Univ, Dept Elect & Comp Engn, Houston, TX 77251 USA.	yw68@rice.edu; jiangziyu@tamu.edu; chernxh@tamu.edu; px5@rice.edu; zy34@rice.edu; yingyan.lin@rice.edu; atlaswang@tamu.edu			NSF RTML [1937592, 1937588]	NSF RTML	The work is in part supported by the NSF RTML grant (1937592, 1937588). The authors would like to thank all anonymous reviewers for their tremendously useful comments to help improve our work.	Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Akiba T., 2017, ARXIV PREPRINT ARXIV; Alistarh D., 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2018, ADV NEURAL INFORM PR; [Anonymous], 2017, P ICLR; Bankman D, 2019, IEEE J SOLID-ST CIRC, V54, P158, DOI 10.1109/JSSC.2018.2869150; Banner R, 2018, ARXIV180511046; Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26; Bernstein J, 2018, PR MACH LEARN RES, V80; BERTSEKAS D. P., 2011, OPTIMIZATION MACHINE, V2010, P1; Chen WJ, 2019, PROC CVPR IEEE, P7234, DOI 10.1109/CVPR.2019.00741; Chen YH, 2017, IEEE J SOLID-ST CIRC, V52, P127, DOI 10.1109/JSSC.2016.2616357; Chen Z., 2018, GATERNET DYNAMIC FIL; Chin T.-W., 2019, ARXIV190202910; Cho M., 2017, ARXIV170802188; Coleman Cody, 2019, SELECT VIA PROXY EFF; Daneshmand H, 2018, PR MACH LEARN RES, V80; De Sa C, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P561, DOI 10.1145/3079856.3080248; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Greff Klaus, 2017, 5 INT C LEARN REPR I; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Gurbuzbalaban M., 2015, ARXIV151008560; Han S., 2016, P INT C LEARNING REP; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hoffer E., 2018, ARXIV180301814, P2160; Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Jia X., 2018, ARXIV PREPRINT ARXIV; Johnson T. B., 2018, ADV NEURAL INFORM PR, V31; Kaiser Lukasz, 2017, ARXIV170605137; Keskar N.S., 2016, ABS160904836; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kyrola A., 2017, ABS170602677 ARXIV; Li JL, 2017, INT CONF MACH LEARN, P35; Lin J, 2017, ADV NEUR IN, V30; Lin Y, 2017, IEEE INT CONF COMMUN, P782; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Lym Sangkug, 2019, PRUNETRAIN FAST NEUR; Recht B., 2012, BENEATH VALLEY NONCO; Sahoo D., 2017, ARXIV171103705; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Seide F, 2014, INTERSPEECH, P1058; Shamir O, 2016, ADV NEUR IN, V29; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1; Wang Yue, 2019, ARXIV190704523; Wangni J., 2018, P C NEUR INF PROC SY, P1299; Wen W., 2017, P NIPS, P1509; Wu JR, 2018, PR MACH LEARN RES, V80; Wu Shuang, 2018, IEEE T NEURAL NETWOR; Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919; Xilinx Inc, 2019, DIG ZEDB ZYNQ 7000 A; Yang GD, 2019, PR MACH LEARN RES, V97; Yang TJ, 2017, PROC CVPR IEEE, P6071, DOI 10.1109/CVPR.2017.643; Yang YZ, 2018, IEEE INT SYMP PARAL, P407, DOI 10.1109/BDCloud.2018.00069; Yang You, 2018, P 47 INT C PAR PROC; Yu JC, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P548, DOI 10.1145/3079856.3080215; Zhang Chiyuan, 2019, ARXIV190201996; [No title captured]	66	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305017
C	Wang, YH; He, H; Tan, XY; Gan, YZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Yuhui; He, Hao; Tan, Xiaoyang; Gan, Yaozhong			Trust Region-Guided Proximal Policy Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Proximal policy optimization (PPO) is one of the most popular deep reinforcement learning (RL) methods, achieving state-of-the-art performance across a wide range of challenging tasks. However, as a model-free RL method, the success of PPO relies heavily on the effectiveness of its exploratory policy search. In this paper, we give an in-depth analysis on the exploration behavior of PPO, and show that PPO is prone to suffer from the risk of lack of exploration especially under the case of bad initialization, which may lead to the failure of training or being trapped in bad local optima. To address these issues, we proposed a novel policy optimization method, named Trust Region-Guided PPO (TRGPPO), which adaptively adjusts the clipping range within the trust region. We formally show that this method not only improves the exploration ability within the trust region but enjoys a better performance bound compared to the original PPO as well. Extensive experiments verify the advantage of the proposed method.	[Wang, Yuhui; He, Hao; Tan, Xiaoyang; Gan, Yaozhong] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, MIIT Key Lab Pattern Anal & Machine Intelligence, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing, Peoples R China	Nanjing University of Aeronautics & Astronautics	Wang, YH (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, MIIT Key Lab Pattern Anal & Machine Intelligence, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing, Peoples R China.	y.wang@nuaa.edu.cn; hugo@nuaa.edu.cn; x.tan@nuaa.edu.cn; yzgancn@nuaa.edu.cn	王, 宇慧/GXZ-7548-2022; tan, xiao/GZL-0264-2022; He, Hao/GMX-3028-2022		National Science Foundation of China [61976115, 61672280, 61732006]; AI+ Project of NUAA [56XZA18009]; Postgraduate Research & Practice Innovation Program of Jiangsu Province [KYCX19_0195]	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); AI+ Project of NUAA; Postgraduate Research & Practice Innovation Program of Jiangsu Province	This work is partially supported by National Science Foundation of China (61976115,61672280, 61732006), AI+ Project of NUAA(56XZA18009), Postgraduate Research & Practice Innovation Program of Jiangsu Province (KYCX19_0195). We would also like to thank Yao Li, Weida Li, Xin Jin, as well as the anonymous reviewers, for offering thoughtful comments and helpful advice on earlier versions of this work.	[Anonymous], 2017, MASTERING CHESS SHOG; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Brockman G., 2016, OPENAI GYM; Chen Guoxing, 2018, CORR; Dey A. K., 2010, ICML; Duan Y, 2016, INT C MACH LEARN, P1329; Fakoor Rasool, 2019, UNCERTAINTY ARTIFICI; Fortunato M., 2018, P INT C LEARN REPR, P1; Haarnoja T., 2018, INT C MACH LEARN, P1856; Haarnoja T, 2017, PR MACH LEARN RES, V70; Hesse C., 2017, OPENAI BASELINES; Levine S, 2016, J MACH LEARN RES, V17; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pathak D., 2017, INT C MACH LEARN ICM, V2017; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; POWELL MJD, 1970, NUMERICAL METHODS NO; Schulman J., 2016, INT C LEARN REPR; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wang Yuhui, 2019, UNCERTAINTY ARTIFICI; Wu Y., 2017, CORR	24	0	0	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300057
C	Wanigasekara, N; Yu, CL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wanigasekara, Nirandika; Yu, Christina Lee			Nonparametric Contextual Bandits in an Unknown Metric Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Consider a nonparametric contextual multi-arm bandit problem where each arm a is an element of [K] is associated to a nonparametric reward function f(a) : [0, 1] -> R mapping from contexts to the expected reward. Suppose that there is a large set of arms, yet there is a simple but unknown structure amongst the arm reward functions, e.g. finite types or smooth with respect to an unknown metric space. We present a novel algorithm which learns data-driven similarities amongst the arms, in order to implement adaptive partitioning of the context-arm space for more efficient learning. We provide regret bounds along with simulations that highlight the algorithm's dependence on the local geometry of the reward functions.	[Wanigasekara, Nirandika] Natl Univ Singapore, Comp Sci, Singapore, Singapore; [Yu, Christina Lee] Cornell Univ, Operat Res & Informat Engn, Ithaca, NY 14853 USA	National University of Singapore; Cornell University	Wanigasekara, N (corresponding author), Natl Univ Singapore, Comp Sci, Singapore, Singapore.	nirandiw@comp.nus.edu.sg; cleeyu@cornell.edu			National University of Singapore; A*STAR -SERC PSF Grant [1521200084]	National University of Singapore(National University of Singapore); A*STAR -SERC PSF Grant(Agency for Science Technology & Research (A*STAR))	This work is supported by the National University of Singapore and A*STAR -SERC PSF Grant 1521200084. We thank Professor David S. Rosenblum for his support of this project through insightful discussions and feedback.	Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Bastani H, 2015, ONLINE DECISION MAKI; Beygelzimer A., 2010, ARXIV10024058; Deshmukh A. A., 2017, ADV NEURAL INFORM PR, P4848; Dudik M., 2011, ARXIV11062369; Gentile C, 2014, PR MACH LEARN RES, V32, P757; Guan M. Y., 2018, 32 AAA C ART INT; Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36; He J, 2010, NINTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, VOLS I-III, P1061; Kleinberg R, 2005, P 17 INT C NEUR INF, V17, P697; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Krishnamurthy Akshay, 2019, ARXIV190201520, P2025; Lale S., 2019, ABS190109490 CORR; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li S, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P539, DOI 10.1145/2911451.2911548; Lu T., 2009, TECHNICAL REPORT; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Qian W., 2016, J MACHINE LEARNING R, P5181; Rigollet P., 2010, ARXIVL0031630; SLIVKINS A, 2011, ADV NEURAL INFORM PR, P1602; Wu QY, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P529, DOI 10.1145/2911451.2911528; Yang YH, 2002, ANN STAT, V30, P100	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906035
C	Wei, CL; Lee, JD; Liu, Q; Ma, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wei, Colin; Lee, Jason D.; Liu, Qiang; Ma, Tengyu			Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent works have shown that on sufficiently over-parametrized neural nets, gradient descent with relatively large initialization optimizes a prediction function in the RKHS of the Neural Tangent Kernel (NTK). This analysis leads to global convergence results but does not work when there is a standard l(2) regularizer, which is useful to have in practice. We show that sample efficiency can indeed depend on the presence of the regularizer: we construct a simple distribution in d dimensions which the optimal regularized neural net learns with O(d) samples but the NTK requires Omega(d(2)) samples to learn. To prove this, we establish two analysis tools: i) for multi-layer feedforward ReLU nets, we show that the global minimizer of a weakly-regularized cross-entropy loss is the max normalized margin solution among all neural nets, which generalizes well; ii) we develop a new technique for proving lower bounds for kernel methods, which relies on showing that the kernel cannot focus on informative features. Motivated by our generalization results, we study whether the regularized global optimum is attainable. We prove that for infinite-width two-layer nets, noisy gradient descent optimizes the regularized neural net loss to a global minimum in polynomial iterations.	[Wei, Colin; Ma, Tengyu] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Lee, Jason D.] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA; [Liu, Qiang] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	Stanford University; Princeton University; University of Texas System; University of Texas Austin	Wei, CL (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	colinwei@stanford.edu; jasonlee@princeton.edu; lqiang@cs.texas.edu; tengyuma@stanford.edu			NSF Graduate Research Fellowship; ARO under MURI Award [W911NF-11-1-0303]	NSF Graduate Research Fellowship(National Science Foundation (NSF)); ARO under MURI Award	CW acknowledges the support of a NSF Graduate Research Fellowship. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-0303. This is part of the collaboration between US DOD, UK MOD and UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative. We also thank Nati Srebro and Suriya Gunasekar for helpful discussions in various stages of this work.	Allen-Zhu Zeyuan, 2018, ARXIV181104918; [Anonymous], 2018, ARXIV180301206; [Anonymous], ARXIV181103962; [Anonymous], 2018, ARXIV180301905; [Anonymous], 2017, ARXIV171206541; ARORA S., 2018, ARXIV180206509; Arora Sanjeev, 2018, ARXIV180205296; Arora Sanjeev, 2019, ARXIV190108584; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Belkin M., 2018, ARXIV180201396; Bengio Y., 2006, ADV NEURAL INFORM PR, P123; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; CAO Y., 2019, ARXIV190201384; Chaudhari P., 2016, ARXIV161101838; Chizat L., 2018, NOTE LAZY TRAINING S; Daniely Amit, 2017, ARXIV PREPRINT ARXIV; Dou Xialiang, 2019, ARXIV190107114; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Dziugaite Gintare Karolina, 2017, ARXIV170311008; Garriga-Alonso A., 2018, ARXIV180805587; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Gunasekar S, 2018, ARXIV180208246; Gunasekar S., 2018, ARXIV180600468; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; Hardt M, 2015, ARXIV150901240; Jacot A., 2018, ARXIV180607572; Jacot A, 2018, ARXIV180509545, P8571; Ji Z., 2018, ARXIV180307300; Koltchinskii V, 2002, ANN STAT, V30, P1; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lee Jaehoon, 2017, ARXIV171100165; Li Y., 2018, C LEARN THEOR, V75, P2; Livni R., 2014, NIPS, V1, P855; Ma C, 2017, ARXIV PREPRINT ARXIV; Mei Song, 2018, ARXIV180406561; Morcos A. S., 2018, ICLR POSTER; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Neyshabur B., 2018, ARXIV180512076; Neyshabur Behnam, 2014, ARXIV14126614; Neyshabur Behnam, 2017, ARXIV170709564; NG AY, 2004, P 21 INT C MACH LEAR, P78, DOI DOI 10.1145/1015330.1015435; Nguyen Q., 2017, ARXIV170408045; Novak R., 2018, ARXIV181005148; O'Donnell R, 2014, ANAL BOOLEAN FUNCTIO; Rosset S, 2004, J MACH LEARN RES, V5, P941; Rosset S, 2004, ADV NEUR IN, V16, P1237; Rosset S, 2007, LECT NOTES COMPUT SC, V4539, P544, DOI 10.1007/978-3-540-72927-3_39; Rotskoff Grant M, 2018, STAT-US; Rudelson M, 2013, ELECT COMMUNICATIONS, V18; Safran I, 2016, PR MACH LEARN RES, V48; Santambrogio F, 2017, B MATH SCI, V7, P87, DOI 10.1007/s13373-017-0101-1; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; Sirignano J., 2018, MEAN FIELD ANAL NEUR; Soltanolkotabi M, 2019, IEEE T INFORM THEORY, V65, P742, DOI 10.1109/TIT.2018.2854560; Soudry D, 2018, INT C LEARN REPR; Soudry D., 2016, ARXIV PREPRINT ARXIV; Theodor Misiakiewicz and, 2019, ARXIV190206015; Venturi L., 2018, ARXIV PREPRINT ARXIV; Wei Colin, 2019, ARXIV190503684; Yang G., 2019, ARXIV190204760; Yehudai Gilad, 2019, ARXIV190400687; Yue YF, 2018, INT POW ELEC APPLICA, P1668; Zhang Chiyuan, 2016, ARXIV161103530; Zhu J, 2004, ADV NEUR IN, V16, P49; Zou D, 2018, ARXIV181108888	76	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901035
C	Wortsman, M; Farhadi, AF; Rastegari, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wortsman, Mitchell; Farhadi, Ali; Rastegari, Mohammad			Discovering Neural Wirings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The success of neural networks has driven a shift in focus from feature engineering to architecture engineering. However, successful networks today are constructed using a small and manually defined set of building blocks. Even in methods of neural architecture search (NAS) the network connectivity patterns are largely constrained. In this work we propose a method for discovering neural wirings. We relax the typical notion of layers and instead enable channels to form connections independent of each other. This allows for a much larger space of possible networks. The wiring of our network is not fixed during training - as we learn the network parameters we also learn the structure itself. Our experiments demonstrate that our learned connectivity outperforms hand engineered and randomly wired networks. By learning the connectivity of MobileNetV1 [12] we boost the ImageNet accuracy by 10% at similar to 41M FLOPs. Moreover, we show that our method generalizes to recurrent and continuous time networks. Our work may also be regarded as unifying core aspects of the neural architecture search problem with sparse neural network learning. As NAS becomes more fine grained, finding a good architecture is akin to finding a sparse subnetwork of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse subnetworks of predefined architectures in a single training run. Though we only ever use a small percentage of the weights during the forward pass, we still play the so-called initialization lottery [8] with a combinatorial number of subnetworks. Code and pretrained models are available at https://github.com/allenai/dnw while additional visualizations may be found at https://mitchellnw.github.io/blog/2019/dnw/.	[Wortsman, Mitchell; Farhadi, Ali; Rastegari, Mohammad] PRIOR Allen Inst AI, Seattle, WA 98103 USA; [Wortsman, Mitchell; Farhadi, Ali] Univ Washington, Seattle, WA 98195 USA; [Farhadi, Ali; Rastegari, Mohammad] XNOR AI, Seattle, WA USA	University of Washington; University of Washington Seattle	Wortsman, M (corresponding author), PRIOR Allen Inst AI, Seattle, WA 98103 USA.; Wortsman, M (corresponding author), Univ Washington, Seattle, WA 98195 USA.	mitchnw@cs.washington.edu; ali@xnor.ai; mohammad@xnor.ai			DARPA [N66001-19-2-4031]; NSF [IIS-165205, IIS-1637479, IIS-1703166]; Sloan Fellowship; NVIDIA Artificial Intelligence Lab; Allen Institute for Artificial Intelligence; AI2 fellowship	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Sloan Fellowship(Alfred P. Sloan Foundation); NVIDIA Artificial Intelligence Lab; Allen Institute for Artificial Intelligence; AI2 fellowship	We thank Sarah Pratt, Mark Yatskar and the Beaker team. We also thank Tim Dettmers for his assistance and guidance in the experiments regarding sparse networks. This work is in part supported by DARPA N66001-19-2-4031, NSF IIS-165205, NSF IIS-1637479, NSF IIS-1703166, Sloan Fellowship, NVIDIA Artificial Intelligence Lab, the Allen Institute for Artificial Intelligence, and the AI2 fellowship for AI. Computations on beaker.org were supported in part by credits from Google Cloud.	Bengio Yoshua, 2013, ARXIVABS13083432; Cai H, 2019, INT C LEARNING REPRE; Chen Tian Qi, 2018, NEURIPS; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Deng J., 2009, CVPR 2009; Dettmers Tim, 2019, ARXIVABS19074840; Dupont Emilien, 2019, ABS190401681 CORR; Frankle Jonathan, 2019, ICLR 2019; Frankle Jonathan, 2019, ABS190301611 CORR; Gomez Aidan N., 2019, LEARNING SPARSE NETW; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Howard A. G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ioffe S., 2015, P 32 INT C INT C MAC, V37, P448; Jang Eric, 2016, ARXIVABS161101144; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Krogh A., 1991, NIPS; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2019, ABS180609055 CORR; Louizos Christos, 2018, ABS171201312 CORR; Ma Ningning, 2018, ECCV; Molchanov Dmitry, 2017, ICML; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Prabhu Ameya, 2017, ECCV; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Savarese Pedro H. P., 2019, ABS190209701 ARXIV; SHAMPINE LF, 1986, MATH COMPUT, V46, P135, DOI 10.1090/S0025-5718-1986-0815836-3; Tan Mingxing, 2018, ABS180711626 CORR; Tian Yuandong, 2019, ARXIVABS190513405; Ulyanov D., 2016, ABS160708022 CORR; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Wu Bichen, 2018, ARXIV181203443; Xie Saining, 2019, ABS190401569 CORR; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zoph B., 2016, ICLR	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302065
C	Wu, BZ; Zhao, SW; Chen, CC; Xu, HY; Wang, L; Zhang, XL; Sun, GY; Zhou, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Bingzhe; Zhao, Shiwan; Chen, ChaoChao; Xu, Haoyang; Wang, Li; Zhang, Xiaolu; Sun, Guangyu; Zhou, Jun			Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we aim to understand the generalization properties of generative adversarial networks (GANs) from a new perspective of privacy protection. Theoretically, we prove that a differentially private learning algorithm used for training the GAN does not overfit to a certain degree, i.e., the generalization gap can be bounded. Moreover, some recent works, such as the Bayesian GAN, can be reinterpreted based on our theoretical insight from privacy protection. Quantitatively, to evaluate the information leakage of well-trained GAN models, we perform various membership attacks on these models. The results show that previous Lipschitz regularization techniques are effective in not only reducing the generalization gap but also alleviating the information leakage of the training dataset.	[Wu, Bingzhe; Xu, Haoyang; Sun, Guangyu] Peking Univ, Beijing, Peoples R China; [Zhao, Shiwan] IBM Res, Armonk, NY USA; [Chen, ChaoChao; Wang, Li; Zhang, Xiaolu; Zhou, Jun] Ant Financial, Hangzhou, Peoples R China; [Sun, Guangyu] Peking Univ, Adv Inst Informat Technol, Beijing, Peoples R China	Peking University; International Business Machines (IBM); Peking University	Sun, GY (corresponding author), Peking Univ, Beijing, Peoples R China.; Sun, GY (corresponding author), Peking Univ, Adv Inst Informat Technol, Beijing, Peoples R China.	wubingzhe@pku.edu.cn; zhaosw@cn.ibm.com; chaochao.ccc@antfin.com; xuhaoyang@pku.edu.cn; aymond.wangl@antfin.com; yueyin.zxl@antfin.com; gsun@pku.edu.cn; jun.zhoujun@antfin.com	Chen, Chaochao/AAK-2475-2020	Chen, Chaochao/0000-0003-1419-964X	NSF China [61572045]; Beijing Academy of Artificial Intelligence	NSF China(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence	This work is supported by NSF China 61832020, NSF China 61572045, and Beijing Academy of Artificial Intelligence.	Arjovsky M., 2017, ARXIV170107875; Bau D., 2019, ICLR; Brock Andrew, 2017, 5 INT C LEARN REPR I; Carlini N., 2018, 180208232 ARXIV; Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986; Cummings R., 2016, P 29 C LEARN THEOR C, P772; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dwork C., 2011, ENCY CRYPTOGRAPHY SE, P338, DOI DOI 10.1007/978-1-4419-5906-5_752; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Gary B., 2014, LABELED FACES WILD U; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Hayes J., 2017, CORR; He Hao, 2019, INT C LEARN REPR; Janowczyk A., 2016, J PATHOLOGY INFORN, V7; Karras T, 2018, CORR; Kingma D.P., 2015, INT C LEARNING REPRE; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Miyato Takeru, 2018, 6 INT C LEARNING REP, P8; Mou W., 2018, P MACHINE LEARNING R, P605; Qi G-J, 2017, CORR; Radford A., 2016, 4 INT C LEARN REPR I; Saatci Y., 2017, P NIPS, P3622; Salimans T, 2016, ADV NEUR IN, V29; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Shokri R, 2017, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2017.41; Truex S., 2018, CORR; Vershynin R., 2018, HIGH DIMENSIONAL PRO, V47; Wang L, 2015, I NAVIG SAT DIV INT, P2493; Wang Yu-Xiang, 2016, J MACH LEARN RES, V17; Wu B., 2017, ARXIV COMPUTER VISIO; Yeom S, 2018, P IEEE COMPUT SECUR, P268, DOI 10.1109/CSF.2018.00027; Zhang H., 2018, CORR; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	39	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300028
C	Wu, L; Wang, QC; Ma, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Lei; Wang, Qingcan; Ma, Chao			Global Convergence of Gradient Descent for Deep Linear Residual Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We analyze the global convergence of gradient descent for deep linear residual networks by proposing a new initialization: zero-asymmetric (ZAS) initialization. It is motivated by avoiding stable manifolds of saddle points. We prove that under the ZAS initialization, for an arbitrary target matrix, gradient descent converges to an E-optimal point in O (L-3 log(1/epsilon) iterations, which scales polynomially with the network depth L. Our result and the exp(Omega(L)) convergence time for the standard initialization (Xavier or near-identity) [18] together demonstrate the importance of the residual structure and the initialization in the optimization for deep linear neural networks, especially when L is large.	[Wu, Lei; Wang, Qingcan; Ma, Chao] Princeton Univ, Program Appl & Computat Math, Princeton, NJ 08544 USA	Princeton University	Wu, L (corresponding author), Princeton Univ, Program Appl & Computat Math, Princeton, NJ 08544 USA.	leiwu@princeton.edu; qingcanw@princeton.edu; chaom@princeton.edu			ONR [N00014-13-1-0338]	ONR(Office of Naval Research)	We are grateful to Prof. Weinan E for helpful discussions, and the anonymous reviewers for valuable comments and suggestions. This work is supported in part by a gift to Princeton University from iFlytek and the ONR grant N00014-13-1-0338.	Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Arora S., 2019, INT C LEARN REPR; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Bartlett P., 2018, INT C MACH LEARN, P520; Du S.S., 2019, ARXIV191003016; Du SS, 2019, PR MACH LEARN RES, V97; Du Simon S, 2018, GRADIENT DESCENT FIN; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Ji Z., 2018, PROC INT C LEARN REP; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Laurent T, 2018, PR MACH LEARN RES, V80; Ma T, 2017, P 5 INT C LEARN REPR; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Rotskoff Grant M, 2018, STAT-US; Saxe A., 2014, INT C LEARNING REPRE; Shamir O., 2018, ARXIV180908587; Vorontsov E, 2017, PR MACH LEARN RES, V70; Weinan E Chao, 2019, ARXIV190405263; Xiao H., 2017, ARXIV 170807747; Zeng HQ, 2017, PROC INT CONF RECON; Zhang H., 2019, ARXIV190104684; Zou D, 2018, ARXIV181108888	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905008
C	Wu, SS; Sanghavi, S; Dimakis, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Shanshan; Sanghavi, Sujay; Dimakis, Alexandros G.			Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SELECTION	We characterize the effectiveness of a classical algorithm for recovering the Markov graph of a general discrete pairwise graphical model from i.i.d. samples. The algorithm is (appropriately regularized) maximum conditional log-likelihood, which involves solving a convex program for each node; for Ising models this is l(1)-constrained logistic regression, while for more general alphabets an l(2,1) group-norm constraint needs to be used. We show that this algorithm can recover any arbitrary discrete pairwise graphical model, and also characterize its sample complexity as a function of model width, alphabet size, edge parameter accuracy, and the number of variables. We show that along every one of these axes, it matches or improves on all existing results and algorithms for this problem. Our analysis applies a sharp generalization error bound for logistic regression when the weight vector has an l(1) (or l(2,1)) constraint and the sample vector has an l(infinity) (or l(2,infinity)) constraint. We also show that the proposed convex programs can be efficiently solved in (O) over tilde (n(2)) running time (where n is the number of variables) under the same statistical guarantees. We provide experimental results to support our analysis.	[Wu, Shanshan; Sanghavi, Sujay; Dimakis, Alexandros G.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Wu, SS (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	shanshan@utexas.edu; sanghavi@mail.utexas.edu; dimakis@austin.utexas.edu			NSF [1302435, 1564000, 1618689, DMS 1723052, CCF 1763702, AF 1901292]	NSF(National Science Foundation (NSF))	This research has been supported by NSF Grants 1302435, 1564000, and 1618689, DMS 1723052, CCF 1763702, AF 1901292 and research gifts by Google, Western Digital and NVIDIA.	AKKARAM S, 2010, ADV NEURAL INFORM PR, P37; [Anonymous], 2009, ADV NEURAL INFORM PR; Aurell E, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.090201; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Ben-Tal A., 2013, LECT MODERN CONVEX O; Bresler G, 2015, ACM S THEORY COMPUT, P771, DOI 10.1145/2746539.2746631; Bubeck S., 2015, FDN TRENDS MACHINE L; Eagle N, 2009, P NATL ACAD SCI USA, V106, P15274, DOI 10.1073/pnas.0900282106; Hamilton L., 2017, ADV NEURAL INFORM PR, P2463; Jalali Ali, 2011, P 14 INT C ART INT S, P378; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Koh KM, 2007, J MACH LEARN RES, V8, P1519; Lee Su-In, 2007, ADV NEURAL INFORM PR, P817; Marbach D, 2012, NAT METHODS, V9, P796, DOI [10.1038/NMETH.2016, 10.1038/nmeth.2016]; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Rigollet P., 2017, LECT NOTES HIGH DIME; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; VONNEUMANN J, 1949, ANN MATH, V50, P401; Vuffray M., 2020, ADV NEURAL INFORM PR; Vuffray M., 2016, ADV NEURAL INFORM PR, P2595; Yang E., 2012, ADV NEURAL INFORM PR, P1358; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308013
C	Wu, S; Wang, GR; Tang, P; Chen, F; Shi, LP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Shuang; Wang, Guanrui; Tang, Pei; Chen, Feng; Shi, Luping			Convolution with even-sized kernels and symmetric padding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Compact convolutional neural networks gain efficiency mainly through depthwise convolutions, expanded channels and complex topologies, which contrarily aggravate the training process. Besides, 3X3 kernels dominate the spatial representation in these models, whereas even-sized kernels (2X2, 4X4) are rarely adopted. In this work, we quantify the shift problem occurs in even-sized kernel convolutions by an information erosion hypothesis, and eliminate it by proposing symmetric padding on four sides of the feature maps (C2sp, C4sp). Symmetric padding releases the generalization capabilities of even-sized kernels at little computational cost, making them outperform 3X3 kernels in image classification and generation tasks. Moreover, C2sp obtains comparable accuracy to emerging compact models with much less memory and time consumption during training. Symmetric padding coupled with even-sized convolutions can be neatly implemented into existing frameworks, providing effective elements for architecture designs, especially on online and continual learning occasions where training efforts are emphasized.	[Wu, Shuang; Wang, Guanrui; Tang, Pei; Shi, Luping] Tsinghua Univ, Beijing Innovat Ctr Future Chip, Ctr Brain Inspired Comp Res, Dept Precis Instrument, Beijing, Peoples R China; [Chen, Feng] Tsinghua Univ, Beijing Innovat Ctr Future Chip, Ctr Brain Inspired Comp Res, Dept Automat, Beijing, Peoples R China	Tsinghua University; Tsinghua University	Chen, F (corresponding author), Tsinghua Univ, Beijing Innovat Ctr Future Chip, Ctr Brain Inspired Comp Res, Dept Automat, Beijing, Peoples R China.	chenfeng@mail.tsinghua.edu.cn; lpshi@mail.tsinghua.edu.cn	Shi, Lu/AAG-5414-2021		NSFC [61836004]; Brain-Science Special Program of Beijing [Z181100001518006]; Suzhou-Tsinghua innovation leading program [2016SZ0102]; National Key R&D Program of China [2018YFE0200200]	NSFC(National Natural Science Foundation of China (NSFC)); Brain-Science Special Program of Beijing; Suzhou-Tsinghua innovation leading program; National Key R&D Program of China	We thank the reviewers for their valuable suggestions and insightful comments. This work is partially supported by the Project of NSFC No. 61836004, the Brain-Science Special Program of Beijing under Grant Z181100001518006, the Suzhou-Tsinghua innovation leading program 2016SZ0102 and the National Key R&D Program of China 2018YFE0200200.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Aghdasi F, 1996, IEEE T IMAGE PROCESS, V5, P611, DOI 10.1109/83.491337; [Anonymous], 2018, INT C LEARN REPR; Brock A, 2019, P PERVASIVE DISPLAYS, DOI DOI 10.1145/3205873.3205877; Chen YH, 2017, IEEE J SOLID-ST CIRC, V52, P127, DOI 10.1109/JSSC.2016.2616357; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; DeVries T., 2017, P 2017 COMPUTER VISI; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han S., 2016, IEEE ICC, DOI DOI 10.1109/ICC.2016.7511104; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Howard A. G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jeon Y, 2018, ADV NEUR IN, V31; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karras Tero, 2018, INT C LEARN REPR; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurach K., 2018, ARXIV180704720; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Loshchilov I, 2017, INT C LEARN REPR; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; MCGIBNEY G, 1993, MAGNET RESON MED, V30, P51, DOI 10.1002/mrm.1910300109; Miyato Takeru, 2018, 6 INT C LEARNING REP, P8; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Odena A., 2016, DISTILL, V1, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003]; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T., 2016, ADV NEUR IN, P2234; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Silberman N., 2017, TENSORFLOWSLIM IMAGE; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Williams S, 2009, COMMUN ACM, V52, P65, DOI 10.1145/1498765.1498785; Wu BC, 2018, PROC CVPR IEEE, P9127, DOI 10.1109/CVPR.2018.00951; Wu S., 2018, INT C LEARN REPR; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu F., 2015, ARXIVABS150603365 CO; Yu F., 2016, INT C LEARN REPRESEN; Zambaldi V., 2019, INT C LEARN REPR; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	48	0	0	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301022
C	Xia, XB; Liu, TL; Wang, NN; Han, B; Gong, C; Niu, G; Sugiyama, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xia, Xiaobo; Liu, Tongliang; Wang, Nannan; Han, Bo; Gong, Chen; Niu, Gang; Sugiyama, Masashi			Are Anchor Points Really Indispensable in Label-Noise Learning?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In label-noise learning, the noise transition matrix, denoting the probabilities that clean labels flip into noisy labels, plays a central role in building statistically consistent classifiers. Existing theories have shown that the transition matrix can be learned by exploiting anchor points (i.e., data points that belong to a specific class almost surely). However, when there are no anchor points, the transition matrix will be poorly learned, and those previously consistent classifiers will significantly degenerate. In this paper, without employing anchor points, we propose a transition-revision (T-Revision) method to effectively learn transition matrices, leading to better classifiers. Specifically, to learn a transition matrix, we first initialize it by exploiting data points that are similar to anchor points, having high noisy class posterior probabilities. Then, we modify the initialized matrix by adding a slack variable, which can be learned and validated together with the classifier by using noisy data. Empirical results on benchmark-simulated and real-world label-noise datasets demonstrate that without using exact anchor points, the proposed method is superior to state-of-the-art label-noise learning methods.	[Xia, Xiaobo; Liu, Tongliang] Univ Sydney, Sydney, NSW, Australia; [Xia, Xiaobo; Wang, Nannan] Xidian Univ, Xian, Peoples R China; [Han, Bo; Niu, Gang; Sugiyama, Masashi] RIKEN, Wako, Saitama, Japan; [Gong, Chen] Nanjing Univ Sci & Technol, Nanjing, Peoples R China; [Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan	University of Sydney; Xidian University; RIKEN; Nanjing University of Science & Technology; University of Tokyo	Xia, XB (corresponding author), Univ Sydney, Sydney, NSW, Australia.; Xia, XB (corresponding author), Xidian Univ, Xian, Peoples R China.		Sugiyama, Masashi/AEO-1176-2022; Liu, Tongliang/AAA-1506-2021	Sugiyama, Masashi/0000-0001-6658-6743; Liu, Tongliang/0000-0002-9640-6472	Australian Research Council [DP180103424, DE190101473]; National Natural Science Foundation of China [61922066, 61876142]; CCF-Tencent Open Fund; NSF of China [61602246, 61973162]; NSF of Jiangsu Province [BK20171430]; Fundamental Research Funds for the Central Universities [30918011319]; "Young Elite Scientists Sponsorship Program" by CAST [2018QNRC001]; International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study; Brain-Inspired Technology Co., Ltd.	Australian Research Council(Australian Research Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CCF-Tencent Open Fund; NSF of China(National Natural Science Foundation of China (NSFC)); NSF of Jiangsu Province; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); "Young Elite Scientists Sponsorship Program" by CAST; International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study; Brain-Inspired Technology Co., Ltd.	TLL was supported by Australian Research Council Project DP180103424 and DE190101473. NNW was supported by National Natural Science Foundation of China under Grants 61922066, 61876142, and the CCF-Tencent Open Fund. CG was supported by NSF of China under Grants 61602246, 61973162, NSF of Jiangsu Province under Grants BK20171430, the Fundamental Research Funds for the Central Universities under Grants 30918011319, and the "Young Elite Scientists Sponsorship Program" by CAST under Grants 2018QNRC001. MS was supported by the International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study. XBX and TLL would give special thanks to Haifeng Liu and Brain-Inspired Technology Co., Ltd. for their support of GPUs used for this research.	Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Cheng J., 2017, ARXIV170903768; Goldberger J., 2017, ICLR; Golowich N., 2018, PROC C LEARN THEORY, P297; Gretton A, 2009, NEURAL INF PROCESS S, P131; Guo SX, 2018, 2018 13TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P135, DOI 10.1109/WCICA.2018.8630502; Han B., 2018, ARXIV180508193, P5836; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; Kawaguchi K., 2017, ARXIV PREPRINT ARXIV; Kremer J, 2018, PR MACH LEARN RES, V84; Krizhevsky A., 2009, LEARNING MULTIPLE LA; LeCun Yann, MNIST DATABASE HANDW; Ledoux M., 2013, PROBABILITY BANACH S, P86; Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Lu J, 2018, PR MACH LEARN RES, V80; Ma X., 2018, P INT C MACH LEARN, P3361; Malach E, 2017, NEURIPS, P960; Mohri M., 2018, FDN MACHINE LEARNING; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Neyshabur Behnam, 2018, P 6 INT C LEARN REPR; NORTHCUTT C, 2017, UAI; Patrini G., 2017, P IEEE C COMP VIS PA, P1944; Ramaswamy HG, 2016, PR MACH LEARN RES, V48; Reed S., 2015, ICLR; Ren MY, 2018, PR MACH LEARN RES, V80; Scott C., 2013, P 26 ANN C LEARN THE, P489; Scott C, 2015, JMLR WORKSH CONF PRO, V38, P838; Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; Thekumparampil K.K., 2018, ADV NEURAL INFORM PR, P10271; Vahdat A, 2017, ADV NEURAL INFORM PR, P5596; Vandermeulen Robert A, 2019, ANN STAT; Vandermeulen Robert A, 2016, ARXIV160700071; Vapnik V., 2013, NATURE STAT LEARNING; Veit A., 2017, P IEEE C COMP VIS PA, P839; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Yu X., 2019, ICML; Yu X., 2018, ECCV; Yu XY, 2018, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2018.00471; Zhang C., 2017, ICLR; Zhang Z., 2018, ADV NEURAL INFORM PR, P8778	49	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306080
C	Xie, YJ; Jiang, HM; Liu, F; Zhao, T; Zha, HY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xie, Yujia; Jiang, Haoming; Liu, Feng; Zhao, Tuo; Zha, Hongyuan			Meta Learning with Relational Information for Short Sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper proposes a new meta-learning method - named HARMLESS (HAwkes Relational Meta LEarning method for Short Sequences) for learning heterogeneous point process models from short event sequence data along with a relational network. Specifically, we propose a hierarchical Bayesian mixture Hawkes process model, which naturally incorporates the relational information among sequences into point process modeling. Compared with existing methods, our model can capture the underlying mixed-community patterns of the relational network, which simultaneously encourages knowledge sharing among sequences and facilitates adaptive learning for each individual sequence. We further propose an efficient stochastic variational meta expectation maximization algorithm that can scale to large problems. Numerical experiments on both synthetic and real data show that HARMLESS outperforms existing methods in terms of predicting the future events.	[Xie, Yujia; Jiang, Haoming] Georgia Tech, Coll Comp, Atlanta, GA 30332 USA; [Liu, Feng] Florida Atlantic Univ, Boca Raton, FL 33431 USA; [Zhao, Tuo] Georgia Tech, Coll Engn, Atlanta, GA USA; [Zha, Hongyuan] Chinese Univ Hong Kong, Inst Data & Decis Analyt, Shenzhen, Peoples R China; [Zha, Hongyuan] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen, Peoples R China	University System of Georgia; Georgia Institute of Technology; State University System of Florida; Florida Atlantic University; University System of Georgia; Georgia Institute of Technology; Chinese University of Hong Kong, Shenzhen	Xie, YJ (corresponding author), Georgia Tech, Coll Comp, Atlanta, GA 30332 USA.	Xie.Yujia000@gmail.com; jianghm@gatech.edu; FLIU2016@fau.edu; tuo.zhao@isye.gatech.edu; zhahy@cuhk.edu.cn			Shenzhen Institute of Artificial Intelligence and Robotics for Society; Shenzhen Research Institute of Big Data; NSF IIS [1717916]; NSF CMMI [1745382]	Shenzhen Institute of Artificial Intelligence and Robotics for Society; Shenzhen Research Institute of Big Data; NSF IIS(National Science Foundation (NSF)); NSF CMMI(National Science Foundation (NSF))	This work is partially supported by the grant NSF IIS 1717916 and NSF CMMI 1745382. Part of the work done by Hongyuan Zha is supported by Shenzhen Institute of Artificial Intelligence and Robotics for Society, and Shenzhen Research Institute of Big Data.	Achab M, 2017, PR MACH LEARN RES, V70; Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Andersen, 2009, HDB FINANCIAL TIME S, P953, DOI DOI 10.1007/978-3-540-71297-8_41; Bacry E, 2012, EUR PHYS J B, V85, DOI 10.1140/epjb/e2012-21005-8; Bengio Y., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), DOI 10.1109/IJCNN.1991.155621; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Blundell C, 2012, NIPS; Box G.E.P., 2011, BAYESIAN INFERENCE S; CHALMERS D. J., 1991, CONNECTIONIST MODELS, P81, DOI [DOI 10.1016/B978-1-4832-1448-1.50014-7, 10.1016/B978-1-4832-1448-1.50014-7]; CLEEREMANS A, 1991, J EXP PSYCHOL GEN, V120, P235, DOI 10.1037/0096-3445.120.3.235; Eichler M, 2017, J TIME SER ANAL, V38, P225, DOI 10.1111/jtsa.12213; Farajtabar M., 2016, ADV NEURAL INFORM PR, P4718; Farajtabar M, 2017, PR MACH LEARN RES, V70; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, ADV NEURAL INFORM PR, P9516; Fox EW, 2016, J AM STAT ASSOC, V111, P564, DOI 10.1080/01621459.2015.1135802; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Grant Erin, 2018, INT C LEARN REPR; Hansen NR, 2015, BERNOULLI, V21, P83, DOI 10.3150/13-BEJ562; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kobayashi Ryota, 2016, P 10 INT AAAI C WEB; Koch G., 2015, ICML DEEP LEARNING W; LAUB P. J., 2015, ARXIV150702822; LI L., 2013, P 22 ACM INT C INF K; Linderman SW, 2014, PR MACH LEARN RES, V32, P1413; Luo DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3685; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Mei HY, 2017, ADV NEUR IN, V30; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Nichol Alex, 2018, ARXIV180302999; OGATA Y, 1999, SEISMICITY PATTERNS, V155, P471; Paranjape A, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P601, DOI 10.1145/3018661.3018731; Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5; RAVI S., 2018, AMORTIZED BAYESIAN M; Ravi Sachin, 2016, OPTIMIZATION MODEL F, P1; Reynaud-Bouret P, 2010, ANN STAT, V38, P2781, DOI 10.1214/10-AOS806; Santoro A, 2016, PR MACH LEARN RES, V48; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; TRAN L., 2015, P 2015 SIAM INT C DA; TRIVEDI R., 2018, DYREP LEARNING REPRE; VINYALS O., 2016, ADV NEURAL INFORM PR; Xie JR, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501657; XU H., 2017, ADV NEURAL INFORM PR; XU H., 2017, ARXIV171005115; Xu HT, 2017, PR MACH LEARN RES, V70; YANG S.- H., 2013, INT C MACH LEARN; Zarezade A, 2017, AAAI CONF ARTIF INTE, P238; Zhang Yu, 2017, ARXIV170708114, DOI DOI 10.1109/TKDE.2021.3070203; ZHAO Q., 2015, P 21 ACM SIGKDD INT; ZHOU K., 2013, ARTIFICIAL INTELLIGE	54	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901052
C	Xu, CG; Elhamifar, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Chengguang; Elhamifar, Ehsan			Deep Supervised Summarization: Algorithm and Application to Learning Instructions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We address the problem of finding representative points of datasets by learning from multiple datasets and their ground-truth summaries. We develop a supervised subset selection framework, based on the facility location utility function, which learns to map datasets to their ground-truth representatives. To do so, we propose to learn representations of data so that the input of transformed data to the facility location recovers their ground-truth representatives. Given the NP-hardness of the utility function, we consider its convex relaxation based on sparse representation and investigate conditions under which the solution of the convex optimization recovers ground-truth representatives of each dataset. We design a loss function whose minimization over the parameters of the data representation network leads to satisfying the theoretical conditions, hence guaranteeing recovering groundtruth summaries. Given the non-convexity of the loss function, we develop an efficient learning scheme that alternates between representation learning by minimizing our proposed loss given the current assignments of points to ground-truth representatives and updating assignments given the current data representation. By experiments on the problem of learning key-steps (subactivities) of instructional videos, we show that our proposed framework improves the state-of-the-art supervised subset selection algorithms.	[Xu, Chengguang; Elhamifar, Ehsan] Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA	Northeastern University	Xu, CG (corresponding author), Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA.	xu.cheng@husky.neu.edu; eelhami@ccs.neu.edu			DARPA Young Faculty Award [D18AP00050]; NSF [IIS-1657197]; ONR [N000141812132]; ARO [W911NF1810300]	DARPA Young Faculty Award; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO	This work is supported by DARPA Young Faculty Award (D18AP00050), NSF (IIS-1657197), ONR (N000141812132) and ARO (W911NF1810300). Chengguang Xu would like to thank Dat Huynh and Zwe Naing for their help and advice with some of the implementations during his research assistantship at MCADS lab, which resulted in this work.	Awasthi P, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P191, DOI 10.1145/2688073.2688116; Borodin A., 2000, COMMUNICATIONS MATH, V211; Buchbinder N., 2012, ANN S FDN COMP SCI; Cai Sijia, 2018, EUR C COMP VIS; Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025; Chao WL, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P191; Charikar M, 2002, J COMPUT SYST SCI, V65, P129, DOI 10.1006/jcss.2002.1882; Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981; de Avila S. E. F., 2011, PATTERN RECOGNITION, V32; del Molino A. G., 2017, AAAI; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Elhamifar E., 2017, NEURAL INFORM PROCES; Elhamifar E., 2014, WORLD C INT FED AUT; Elhamifar E., 2012, NEURAL INFORM PROCES; Elhamifar E, 2019, IEEE I CONF COMP VIS, P6350, DOI 10.1109/ICCV.2019.00644; Elhamifar E, 2019, PR MACH LEARN RES, V97; Elhamifar E, 2015, AAAI CONF ARTIF INTE, P629; Elhamifar E, 2016, IEEE T PATTERN ANAL, V38, P2182, DOI 10.1109/TPAMI.2015.2511748; Elhamifar E, 2011, PROC CVPR IEEE; Elhamifar Ehsan, 2017, IEEE C COMP VIS PATT; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Esser E, 2012, IEEE T IMAGE PROCESS, V21, P3239, DOI 10.1109/TIP.2012.2190081; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gillenwater J. A., 2014, P NEURIPS, P3149; Gong B., 2014, ADV NEURAL INFORM PR, P2069; Guillory A., 2010, INT C MACH LEARN; Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928; Hadlock F., 1975, SIAM J COMPUTING, V4; Hartline J., 2008, WORLD WID WEB C; Hong R., 2009, SIGMM WORKSH; Joshi S, 2009, SYMP VLSI CIRCUITS, P52; Kang H.-W., 2006, COMPUTER VISION PATT, P1331, DOI DOI 10.1109/CVPR.2006.284; Khosla A, 2013, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2013.348; Kim G, 2014, PROC CVPR IEEE, P3882, DOI 10.1109/CVPR.2014.496; Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239; Krause Andreas, 2014, SUBMODULAR FUNCTION, P5; Krause A, 2008, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2008/07/001; Kuehne H, 2014, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2014.105; Lazic N., 2007, INT C ART INT STAT; Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820; Li S., 2011, AUTOMATA LANGUAGES P; Li Y., 2010, WIAMIS WORKSH; Lin H., 2012, C UNC ART INT; Liu Tiecheng, 2002, EUR C COMP VIS; Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350; Ma Y, 2002, P 10 ACM INT C MULT, P533, DOI DOI 10.1145/641007.641116; McSherry D., 2002, ADV CASE BASED REASO; Misra I., 2014, WINT C APPL COMP VIS; Motwani R., 1995, RANDOMIZED ALGORITHM; Nellore A., 2015, INFORM COMPUTATION; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Ngo C. W., 2013, INT C COMP VIS; Panda R., 2017, INT C COMP VIS; Panda R, 2017, PROC CVPR IEEE, P4274, DOI 10.1109/CVPR.2017.455; Potapov Danila, 2014, EUR C COMP VIS; Rochan Mrigank, 2018, EUR C COMP VIS; Shah A., 2013, C UNC ART INT; Sharghi A., 2016, EUR C COMP VIS; Sharghi Aidean, 2018, EUR C COMP VIS; Simon I, 2007, IEEE I CONF COMP VIS, P274; Song H.O., 2017, IEEE C COMP VIS PATT; Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154; Tschiatschek Sebastian, 2014, ADV NEURAL INFORM PR, P1413; Zhang H. J., 1997, PATTERN RECOGNITION, V30; Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120; Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47; Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322	68	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301014
C	Xu, DF; Martin-Martin, R; Huang, DA; Zhu, YK; Savarese, S; Fei-Fei, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Danfei; Martin-Martin, Roberto; Huang, De-An; Zhu, Yuke; Savarese, Silvio; Fei-Fei, Li			Regression Planning Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent learning-to-plan methods have shown promising results on planning directly from observation space. Yet, their ability to plan for long-horizon tasks is limited by the accuracy of the prediction model. On the other hand, classical symbolic planners show remarkable capabilities in solving long-horizon tasks, but they require predefined symbolic rules and symbolic states, restricting their real-world applicability. In this work, we combine the benefits of these two paradigms and propose a learning-to-plan method that can directly generate a long-term symbolic plan conditioned on high-dimensional observations. We borrow the idea of regression (backward) planning from classical planning literature and introduce Regression Planning Networks (RPN), a neural network architecture that plans backward starting at a task goal and generates a sequence of intermediate goals that reaches the current observation. We show that our model not only inherits many favorable traits from symbolic planning, e.g., the ability to solve previously unseen tasks, but also can learn from visual inputs in an end-to-end manner. We evaluate the capabilities of RPN in a grid world environment and a simulated 3D kitchen environment featuring complex visual scene and long task horizon, and show that it achieves near-optimal performance in completely new task instances.	[Xu, Danfei; Martin-Martin, Roberto; Huang, De-An; Savarese, Silvio; Fei-Fei, Li] Stanford Univ, Stanford, CA 94305 USA; [Zhu, Yuke] Stanford Univ, NVIDIA Res, Stanford, CA 94305 USA	Stanford University; Stanford University	Xu, DF (corresponding author), Stanford Univ, Stanford, CA 94305 USA.			Martin-Martin, Roberto/0000-0002-9586-2759	JD.com American Technologies Corporation ("JD") under the SAIL-JD AI Research Initiative	JD.com American Technologies Corporation ("JD") under the SAIL-JD AI Research Initiative	This work has been partially supported by JD.com American Technologies Corporation ("JD") under the SAIL-JD AI Research Initiative. This article solely reflects the opinions and conclusions of its authors and not JD or any entity associated with JD.com.	Agrawal P., 2016, P ADV NEURAL INFORM, P5074; Akkoyunlu E. A., 1973, SIAM Journal on Computing, V2, P1, DOI 10.1137/0202001; Andreas J, 2017, PR MACH LEARN RES, V70; [Anonymous], ROBOTICS RES; Asai M, 2018, AAAI CONF ARTIF INTE, P6094; Cai J., 2017, ARXIV170406611; Chevalier-Boisvert M., 2018, GITHUB REPOSITORY; Chitnis R, 2019, IEEE INT CONF ROBOT, P7865, DOI 10.1109/ICRA.2019.8794342; Corneil Dane, 2018, INT C MACH LEARN, P1057; Coumans E., PYBULLET PYTHON MODU; Darrell T., 2018, ARXIV181105432; FIKES RE, 1971, ARTIF INTELL, V2, P189, DOI 10.1016/0004-3702(71)90010-5; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, ARXIV181210972; Hafner D., 2018, ARXIV181104551; Higgins Irina, 2017, ICLR 2017, P2; Irsoy Ozan, 2014, ADV NEURAL INFORM PR, V27, P2096; Kaelbling L. P., 2011, ICRA; Kaelbling LP, 2013, INT J ROBOT RES, V32, P1194, DOI 10.1177/0278364913484072; Kipf TN, 2016, P INT C LEARN REPR; KORF RE, 1987, ARTIF INTELL, V33, P65, DOI 10.1016/0004-3702(87)90051-8; Kurutach T., 2018, ADV NEURAL INFORM PR, P8733; LaValle S. M., 1998, RAPIDLY EXPLORING RA; LOZANOPEREZ T, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300101; Mcdermott D., 1998, PDDL PLANNING DOMAIN; Oh J., 2015, P ADV NEUR INF PROC, P2863; Oh J, 2017, PR MACH LEARN RES, V70; Pathak D., 2018, ICLR; Reed Scott, 2016, ICLR; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sohn S., 2018, ARXIV180707665; Srinivas A., 2018, ICML, P4739; Waldinger Richard, 1975, ACHIEVING SEVERAL GO; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746; WELD DS, 1994, AI MAG, V15, P27; Wu Jiajun, 2017, ADV NEURAL INFORM PR, P153; Xu D, 2017, CVPR, P5410	39	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301032
C	Xu, K; Li, CX; Zhu, J; Zhang, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Kun; Li, Chongxuan; Zhu, Jun; Zhang, Bo			Multi-object Generation with Amortized Structural Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POSTERIOR REGULARIZATION	Deep generative models (DGMs) have shown promise in image generation. However, most of the existing methods learn a model by simply optimizing a divergence between the marginal distributions of the model and the data, and often fail to capture rich structures, such as attributes of objects and their relationships, in an image. Human knowledge is a crucial element to the success of DGMs to infer these structures, especially in unsupervised learning. In this paper, we propose amortized structural regularization (ASR), which adopts posterior regularization (PR) to embed human knowledge into DGMs via a set of structural constraints. We derive a lower bound of the regularized log-likelihood in PR and adopt the amortized inference technique to jointly optimize the generative model and an auxiliary recognition model for inference efficiently. Empirical results show that ASR outperforms the DGM baselines in terms of inference performance and sample quality.	[Xu, Kun; Li, Chongxuan; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Inst AI, Dept Comp Sci & Tech, THBI Lab,BNRist Ctr,State Key Lab Intell Tech & S, Beijing, Peoples R China	Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, Inst AI, Dept Comp Sci & Tech, THBI Lab,BNRist Ctr,State Key Lab Intell Tech & S, Beijing, Peoples R China.	kunxu.thu@gmail.com; chongxuanli1991@gmail.com; dcszj@tsinghua.edu.cn; dcszb@tsinghua.edu.cn			National Key Research and Development Program of China [2017YFA0700904]; NSFC [61620106010, 61621136008]; Beijing NSF Project [L172037]; Beijing Academy of Artificial Intelligence (BAAI),; Tiangong Institute for Intelligent Computing; JP Morgan Faculty Research Program; NVIDIA NVAIL Program; GPU/DGX Acceleration; Chinese postdoctoral innovative talent support program; Shuimu Tsinghua Scholar	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Beijing NSF Project; Beijing Academy of Artificial Intelligence (BAAI),; Tiangong Institute for Intelligent Computing; JP Morgan Faculty Research Program; NVIDIA NVAIL Program; GPU/DGX Acceleration; Chinese postdoctoral innovative talent support program; Shuimu Tsinghua Scholar	This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008), Beijing NSF Project (No. L172037), Beijing Academy of Artificial Intelligence (BAAI), Tiangong Institute for Intelligent Computing, the JP Morgan Faculty Research Program and the NVIDIA NVAIL Program with GPU/DGX Acceleration. C. Li was supported by the Chinese postdoctoral innovative talent support program and Shuimu Tsinghua Scholar.	Abadi M, 2015, P 12 USENIX S OPERAT; Bilen Hakan, 2014, BRIT MACH VIS C, V3; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Burgess Christopher P, 2019, ARXIV190111390; Dilokthanakul Nat, 2016, ARXIV161102648; Eslami SM, 2016, NEURIPS, V1; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Greff K, 2019, PR MACH LEARN RES, V97; Higgins M, 2017, PALGR COMMUN, V3, DOI 10.1057/s41599-017-0005-4; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu ZT, 2018, ADV NEUR IN, V31; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kosiorek AR, 2018, ADV NEUR IN, V31; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li CX, 2018, ADV NEUR IN, V31; Li CX, 2015, ADV NEUR IN, V28; Mei SK, 2014, PR MACH LEARN RES, V32; Shu R, 2018, ADV NEUR IN, V31; van den Oord A, 2016, PR MACH LEARN RES, V48; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Xu K., 2018, ARXIV180703877; Zhu J, 2014, J MACH LEARN RES, V15, P1799	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306060
C	Xu, QQ; Sun, XW; Yang, ZY; Cao, XC; Huang, QM; Yao, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Qianqian; Sun, Xinwei; Yang, Zhiyong; Cao, Xiaochun; Huang, Qingming; Yao, Yuan			iSplit LBI: Individualized Partial Ranking with Ties via Split LBI	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Due to the inherent uncertainty of data, the problem of predicting partial ranking from pairwise comparison data with ties has attracted increasing interest in recent years. However, in real-world scenarios, different individuals often hold distinct preferences. It might be misleading to merely look at a global partial ranking while ignoring personal diversity. In this paper, instead of learning a global ranking which is agreed with the consensus, we pursue the tie-aware partial ranking from an individualized perspective. Particularly, we formulate a unified framework which not only can be used for individualized partial ranking prediction, but also be helpful for abnormal user selection. This is realized by a variable splitting-based algorithm called iSplitLBI. Specifically, our algorithm generates a sequence of estimations with a regularization path, where both the hyperparameters and model parameters are updated. At each step of the path, the parameters can be decomposed into three orthogonal parts, namely, abnormal signals, personalized signals and random noise. The abnormal signals can serve the purpose of abnormal user selection, while the abnormal signals and personalized signals together are mainly responsible for individual partial ranking prediction. Extensive experiments on simulated and real-world datasets demonstrate that our new approach significantly outperforms state-of-the-art alternatives.	[Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China; [Sun, Xinwei] Microsoft Res Asia, Beijing, Peoples R China; [Yang, Zhiyong; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China; [Yang, Zhiyong; Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Tech, Beijing, Peoples R China; [Huang, Qingming] Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing, Peoples R China; [Cao, Xiaochun; Huang, Qingming] Peng Cheng Lab, Shenzhen, Peoples R China; [Yao, Yuan] Hong Kong Univ Sci & Technol, Dept Math, Hong Kong, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Microsoft; Microsoft Research Asia; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Peng Cheng Laboratory; Hong Kong University of Science & Technology	Xu, QQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.	xuqianqian@ict.ac.cn; xinsun@microsoft.com; yangzhiyong@iie.ac.cn; caoxiaochun@iie.ac.cn; qmhuang@ucas.ac.cn; yuany@ust.hk			National Key R&D Program of China [2016YFB0800403]; National Natural Science Foundation of China [61620106009, U1636214, 61836002, U1803264, U1736219, 61672514, 61976202]; National Basic Research Program of China (973 Program) [2015CB351800]; Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-SYS013]; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]; Peng Cheng Laboratory Project of Guangdong Province [PCL2018KP004]; Beijing Natural Science Foundation [4182079]; Youth Innovation Promotion Association CAS; Hong Kong Research Grant Council (HKRGC) [16303817]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Basic Research Program of China (973 Program)(National Basic Research Program of China); Key Research Program of Frontier Sciences, CAS; Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences); Peng Cheng Laboratory Project of Guangdong Province; Beijing Natural Science Foundation(Beijing Natural Science Foundation); Youth Innovation Promotion Association CAS; Hong Kong Research Grant Council (HKRGC)(Hong Kong Research Grants Council)	This work was supported in part by the National Key R&D Program of China (Grant No. 2016YFB0800403), in part by National Natural Science Foundation of China: 61620106009, U1636214, 61836002, U1803264, U1736219, 61672514 and 61976202, in part by National Basic Research Program of China (973 Program): 2015CB351800, in part by Key Research Program of Frontier Sciences, CAS: QYZDJ-SSW-SYS013, in part by the Strategic Priority Research Program of Chinese Academy of Sciences, Grant No. XDB28000000, in part by Peng Cheng Laboratory Project of Guangdong Province PCL2018KP004, in part by Beijing Natural Science Foundation (4182079), in part by Youth Innovation Promotion Association CAS, and in part by Hong Kong Research Grant Council (HKRGC) grant 16303817.	Burger M, 2005, LECT NOTES COMPUT SC, V3752, P25; Cheng WW, 2010, LECT NOTES ARTIF INT, V6321, P215, DOI 10.1007/978-3-642-15880-3_20; Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]; Gionis A., 2006, P ACM SIGKDD C KNOWL, P561; He XN, 2018, ACM/SIGIR PROCEEDINGS 2018, P355, DOI 10.1145/3209978.3209981; Hu HQ, 2016, PROC INT CONF DATA, P61, DOI 10.1109/ICDE.2016.7498229; Huang, 2018, INT C ART INT STAT, P2047; Huang C., 2016, P 30 INT C NEUR INF, P3377; Huang CD, 2020, APPL COMPUT HARMON A, V48, P1, DOI 10.1016/j.acha.2017.12.004; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Jiang ZS, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P288, DOI 10.1145/3159652.3159715; Kamar E., 2015, AAAI C HUM COMP CROW; Lebanon G, 2008, J MACH LEARN RES, V9, P2401; Li GL, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1463, DOI 10.1145/3035918.3064036; Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3; Lu T., 2011, ICML, P145; Lu T, 2014, J MACH LEARN RES, V15, P3783; Lu Y, 2015, ANN ALLERTON CONF, P1473, DOI 10.1109/ALLERTON.2015.7447183; Negahban S., 2012, ADV NEURAL INFORM PR, P2474; Osher S, 2016, APPL COMPUT HARMON A, V41, P436, DOI 10.1016/j.acha.2016.01.002; Osting B., 2013, INT C MACH LEARN, P489; Sheshadri A., 2013, AAAI C HUM COMP CROW; Sun Xinwei, 2017, LNCS, P107, DOI DOI 10.1007/978-3-319-66179-713; Venanzi M, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P155, DOI 10.1145/2566486.2567989; Xu QQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P591, DOI 10.1145/3240508.3240597; Xu QQ, 2019, IEEE T PATTERN ANAL, V41, P844, DOI 10.1109/TPAMI.2018.2817205; Xu QQ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P841, DOI 10.1145/2964284.2964298; Xu Q, 2011, 2011 INTERNATIONAL CONFERENCE ON COMPUTER, ELECTRICAL, AND SYSTEMS SCIENCES, AND ENGINEERING (CESSE 2011), P39; Yi J., 2013, P 1 AAAI C HUM COMP, P207; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang ZX, 2013, INTERNATIONAL CONFERENCE ON EARTH AND ENVIRONMENTAL SCIENCE, P19; Zhao, 2018, INT C MACH LEARN, P5907; Zheng YD, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1031, DOI 10.1145/2723372.2749430	33	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303084
C	Xu, TY; Zou, SF; Liang, YB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Tengyu; Zou, Shaofeng; Liang, Yingbin			Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC-APPROXIMATION; CONVERGENCE RATE	Gradient-based temporal difference (GTD) algorithms are widely used in off-policy learning scenarios. Among them, the two time-scale TD with gradient correction (TDC) algorithm has been shown to have superior performance. In contrast to previous studies that characterized the non-asymptotic convergence rate of TDC only under identical and independently distributed (i.i.d.) data samples, we provide the first non-asymptotic convergence analysis for two time-scale TDC under a non-i.i.d. Markovian sample path and linear function approximation. We show that the two time-scale TDC can converge as fast as O(logt/t(2/3)) under diminishing stepsize, and can converge exponentially fast under constant stepsize, but at the cost of a non-vanishing error. We further propose a TDC algorithm with blockwisely diminishing stepsize, and show that it asymptotically converges with an arbitrarily small error at a blockwisely linear convergence rate. Our experiments demonstrate that such an algorithm converges as fast as TDC under constant stepsize, and still enjoys comparable accuracy as TDC under diminishing stepsize.	[Xu, Tengyu; Liang, Yingbin] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA; [Zou, Shaofeng] SUNY Buffalo, Dept Elect Engn, Buffalo, NY USA	Ohio State University; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Xu, TY (corresponding author), Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA.	xu.3260@osu.edu; szou3@buffalo.edu; liang.889@osu.edu	Xu, Tengyu/ABC-1399-2020		U.S. National Science Foundation [CCF-1761506, ECCS-1818904, CCF-1801855]	U.S. National Science Foundation(National Science Foundation (NSF))	The work of T. Xu and Y. Liang was supported in part by the U.S. National Science Foundation under the grants CCF-1761506, ECCS-1818904, and CCF-1801855.	ARCHIBALD TW, 1995, J OPER RES SOC, V46, P354, DOI 10.2307/2584329; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; BENAIM M, 2018, T AUTOMATIC CONTROL, V64, P2614; Bhandari J., 2018, ARXIV180602450, P1691; Borkar V. S, 2009, STOCHASTIC APPROXIMA, V48; Borkar VS, 2018, ANN ALLERTON CONF, P504, DOI 10.1109/ALLERTON.2018.8636078; Dalal G., 2018, COLT; Dalal G., 2018, P AAAI C ART INT; Degris T., 2014, P INT C MACH LEARN I; Hu B, 2019, ADV NEUR IN, V32; Karmakar P., 2016, ARXIV160102217; Karmakar P, 2018, MATH OPER RES, V43, P130, DOI 10.1287/moor.2017.0855; Konda VR, 2004, ANN APPL PROBAB, V14, P796, DOI 10.1214/105051604000000116; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Maei H. R., 2018, ARXIV180207842; Maei H. R., 2010, P ART GEN INT AGI; Maei Hamid Reza, 2011, THESIS; Mokkadem A, 2006, ANN APPL PROBAB, V16, P1671, DOI 10.1214/105051606000000448; Srikant L. Y. R., 2019, ARXIV190200923; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2008, ADV NEURAL INFORM PR, V21, P1609; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tadic VB, 2004, P AMER CONTR CONF, P3802; Tsitsiklis JN, 1997, ADV NEUR IN, V9, P1075; Wang Yue, 2017, ADV NEURAL INFORM PR, P5510; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Yaji V., 2016, ARXIV161105961; Yang T., 2018, ARXIV181203934; Yu H., 2017, ARXIV171209652; Zou S., 2019, P 33 INT C NEUR INF	37	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902028
C	Xu, Y; Jin, R; Yang, TB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Yi; Jin, Rong; Yang, Tianbao			Non-asymptotic Analysis of Stochastic Methods for Non-Smooth Non-Convex Regularized Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIABLE SELECTION; MINIMIZATION; CONVERGENCE; ALGORITHM	Stochastic Proximal Gradient (SPG) methods have been widely used for solving optimization problems with a simple (possibly non-smooth) regularizer in machine learning and statistics. However, to the best of our knowledge no non-asymptotic convergence analysis of SPG exists for non-convex optimization with a non-smooth and non-convex regularizer. All existing non-asymptotic analysis of SPG for solving non-smooth non-convex problems require the non-smooth regularizer to be a convex function, and hence are not applicable to a non-smooth non-convex regularized problem. This work initiates the analysis to bridge this gap and opens the door to non-asymptotic convergence analysis of non-smooth non-convex regularized problems. We analyze several variants of mini-batch SPG methods for minimizing a non-convex objective that consists of a smooth non-convex loss and a non-smooth non-convex regularizer. Our contributions are two-fold: (i) we show that they enjoy the same complexities as their counterparts for solving convex regularized non-convex problems in terms of finding an approximate stationary point; (ii) we develop more practical variants using dynamic mini-batch size instead of a fixed mini-batch size without requiring the target accuracy level of solution. The significance of our results is that they improve upon the-state-of-art results for solving non-smooth non-convex regularized problems. We also empirically demonstrate the effectiveness of the considered SPG methods in comparison with other peer stochastic methods.	[Xu, Yi; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52246 USA; [Jin, Rong] Alibaba Grp, Machine Intelligence Technol, Bellevue, WA 98004 USA	University of Iowa; Alibaba Group	Xu, Y (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52246 USA.	yi-xu@uiowa.edu; jinrong.jr@alibaba-inc.com; tianbao-yang@uiowa.edu			National Science Foundation [IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	The authors thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1545995).	Allen-Zhu Z, 2017, PR MACH LEARN RES, V70; [Anonymous], [No title captured]; Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9; Bertsekas D. P., 2014, CONSTRAINED OPTIMIZA; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Bot RI, 2016, EURO J COMPUT OPTIM, V4, P3, DOI 10.1007/s13675-015-0045-8; Boyd S, 2004, CONVEX OPTIMIZATION; Bredies K, 2015, J OPTIMIZ THEORY APP, V165, P78, DOI 10.1007/s10957-014-0614-7; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Cao WF, 2013, J VIS COMMUN IMAGE R, V24, P31, DOI 10.1016/j.jvcir.2012.10.006; Carmon Y., 2017, ARXIVABS171011606; Chen Z., 2018, ARXIV180906754; Clarke FH, 1990, OPTIMIZATION NONSMOO; Davis D., 2018, FDN COMPUTATIONAL MA, P1; Davis D, 2019, SIAM J OPTIMIZ, V29, P207, DOI 10.1137/18M1178244; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Goodfellow I., DEEP LEARNING; Han S., 2015, 4 INT C LEARN REPR; HAN S., 2015, ARXIV151000149; Li GY, 2016, MATH PROGRAM, V159, P371, DOI 10.1007/s10107-015-0963-5; Li GY, 2015, SIAM J OPTIMIZ, V25, P2434, DOI 10.1137/140998135; Li Z., 2018, ADV NEURAL INFORM PR, P5569; Lin Zhouchen, 2018, ADV NEURAL INFORM PR, P687; Luenberger D. G., 2015, LINEAR NONLINEAR PRO, V228; Metel M. R., 2019, ARXIV190108369; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nguyen L. M., 2017, ARXIV170507261; Nguyen LM, 2017, PR MACH LEARN RES, V70; An NT, 2017, OPTIMIZATION, V66, P129, DOI 10.1080/02331934.2016.1253694; Nitanda A, 2017, PR MACH LEARN RES, V54, P470; Paquette C., 2018, AISTATS 2018, P1; Pham N. H., 2019, ARXIV190205679; Polino Antonio, 2018, P REPR INT C LEARN; Poliquin RA, 2000, T AM MATH SOC, V352, P5231, DOI 10.1090/S0002-9947-00-02550-2; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Rockafellar RT., 1998, VARIATIONAL ANAL, DOI 10.1007/978-3-642-02431-3; Tao XT, 2018, C ELECT INSUL DIEL P, P378, DOI 10.1109/CEIDP.2018.8544867; Thi H. A. L., 2017, P 34 INT C MACH LEAR, P3394; Wang Zhe, 2018, ARXIV181010690; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Xu Y., 2018, ARXIV181111829; Xu Y., 2018, ARXIV180507880; Xu ZB, 2012, IEEE T NEUR NET LEAR, V23, P1013, DOI 10.1109/TNNLS.2012.2197412; Yang L, 2018, ARXIV171106831; Yu YL, 2015, JMLR WORKSH CONF PRO, V38, P1107; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhong LW, 2014, AAAI CONF ARTIF INTE, P2206; Zhou D., 2018, P 32 INT C NEUR INF, P3925; Zhou D., 2019, ARXIV190111224	52	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302061
C	Yan, SB; Chaudhuri, K; Javidi, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yan, Songbai; Chaudhuri, Kamalika; Javidi, Tara			The Label Complexity of Active Learning from Observational Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Counterfactual learning from observational data involves learning a classifier on an entire population based on data that is observed conditioned on a selection policy. This work considers this problem in an active setting, where the learner additionally has access to unlabeled examples and can choose to get a subset of these labeled by an oracle. Prior work on this problem uses disagreement-based active learning, along with an importance weighted loss estimator to account for counterfactuals, which leads to a high label complexity. We show how to instead incorporate a more efficient counterfactual risk minimizer into the active learning algorithm. This requires us to modify both the counterfactual risk to make it amenable to active learning, as well as the active learning process to make it amenable to the risk. We provably demonstrate that the result of this is an algorithm which is statistically consistent as well as more label-efficient than prior work.	[Yan, Songbai; Chaudhuri, Kamalika; Javidi, Tara] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Yan, SB (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	yansongbai@eng.ucsd.edu; kamalika@cs.ucsd.edu; tjavidi@eng.ucsd.edu			NSF [CCF 1513883, 1719133]	NSF(National Science Foundation (NSF))	We thank NSF under CCF 1513883 and 1719133 for support.	Agarwal A., 2017, ARXIV170306180; Atan Onur, 2019, P MACHINE LEARNING R, V89, P1891; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; BORJESSON PO, 1979, IEEE T COMMUN, V27, P639, DOI 10.1109/TCOM.1979.1094433; Bottou L, 2013, J MACH LEARN RES, V14, P3207; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Dasgupta S., 2007, NIPS; Garnett R., 2015, ADV NEURAL INFORM PR, V28, P2755; Hanneke S, 2007, ICML; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; Hsu D., 2010, THESIS; Kale David C., 2015, P 2015 SIAM INT C DA, P514; Krishnamurthy A, 2017, PR MACH LEARN RES, V70; Maurer A., 2009, P 22 C LEARNING THEO; Namkoong Hongseok, 2017, NEURIPS, P2971; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; Saha A, 2011, LECT NOTES ARTIF INT, V6913, P97, DOI 10.1007/978-3-642-23808-6_7; Sundin Iiris, 2019, ARXIV190405268; Swaminathan A, 2015, PR MACH LEARN RES, V37, P814; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498; Yan SB, 2018, PR MACH LEARN RES, V80; Zhang Chicheng, 2019, ARXIV190100301; Zhang Z., 2016, 30 AAAI C ART INT; Zubkov AM, 2013, THEOR PROBAB APPL+, V57, P539, DOI 10.1137/S0040585X97986138	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301076
C	Yan, X; Wu, Q; Zhang, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yan, Xing; Wu, Qi; Zhang, Wen			Cross-sectional Learning of Extremal Dependence among Financial Assets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TAIL-DEPENDENCE; COPULA; MODEL; RISK	We propose a novel probabilistic model to facilitate the learning of multivariate tail dependence of multiple financial assets. Our method allows one to construct from known random vectors, e.g., standard normal, sophisticated joint heavy-tailed random vectors featuring not only distinct marginal tail heaviness, but also flexible tail dependence structure. The novelty lies in that pairwise tail dependence between any two dimensions is modeled separately from their correlation, and can vary respectively according to its own parameter rather than the correlation parameter, which is an essential advantage over many commonly used methods such as multivariate t or elliptical distribution. It is also intuitive to interpret, easy to track, and simple to sample comparing to the copula approach. We show its flexible tail dependence structure through simulation. Coupled with a GARCH model to eliminate serial dependence of each individual asset return series, we use this novel method to model and forecast multivariate conditional distribution of stock returns, and obtain notable performance improvements in multi-dimensional coverage tests. Besides, our empirical finding about the asymmetry of tails of the idiosyncratic component as well as the market component is interesting and worth to be well studied in the future.	[Yan, Xing; Wu, Qi] City Univ Hong Kong, Sch Data Sci, Hong Kong, Peoples R China; [Zhang, Wen] JD Digits, Beijing, Peoples R China	City University of Hong Kong	Wu, Q (corresponding author), City Univ Hong Kong, Sch Data Sci, Hong Kong, Peoples R China.	yanxing128@gmail.com; qiwu55@cityu.edu.hk; zhangwen.jd@gmail.com			City University of Hong Kong [SRGFd 7005300]; Hong Kong Research Grants Council [24200514, 14211316, 14206117]	City University of Hong Kong(City University of Hong Kong); Hong Kong Research Grants Council(Hong Kong Research Grants Council)	Qi WU acknowledges the financial support from the City University of Hong Kong grant SRGFd 7005300, and the Hong Kong Research Grants Council, particularly the Early Career Scheme 24200514 and the General Research Funds 14211316 and 14206117. This work was undertaken in part while Xing YAN was working with JD Digits.	Aas K, 2009, INSUR MATH ECON, V44, P182, DOI 10.1016/j.insmatheco.2007.02.001; Aderounmu A.A., 2014, ASSESSING TAIL DEPEN; Balla E, 2014, J FINANC STABIL, V15, P195, DOI 10.1016/j.jfs.2014.10.002; Beine M, 2010, J BANK FINANC, V34, P184, DOI 10.1016/j.jbankfin.2009.07.014; BOLLERSLEV T, 1986, J ECONOMETRICS, V31, P307, DOI 10.1016/0304-4076(86)90063-1; Chan Y, 2008, INSUR MATH ECON, V42, P763, DOI 10.1016/j.insmatheco.2007.08.008; CLAYTON DG, 1978, BIOMETRIKA, V65, P141, DOI 10.2307/2335289; Demarta S, 2005, INT STAT REV, V73, P111, DOI 10.1111/j.1751-5823.2005.tb00254.x; Embrechts P., 2001, MODELLING DEPENDENCE; ENGLE RF, 1982, ECONOMETRICA, V50, P987, DOI 10.2307/1912773; Frahm G, 2005, INSUR MATH ECON, V37, P80, DOI 10.1016/j.insmatheco.2005.05.008; Jondeau E, 2006, J INT MONEY FINANC, V25, P827, DOI 10.1016/j.jimonfin.2006.04.007; Kole E, 2007, J BANK FINANC, V31, P2405, DOI 10.1016/j.jbankfin.2006.09.010; Kupiec P, 1995, TECHNIQUES VERIFYING; Lesniewski A, 2016, ASYMPTOTICS PORTFOLI; Peng X., 2009, WORKING PAPER; Poon SH, 2004, REV FINANC STUD, V17, P581, DOI 10.1093/rfs/hhg058; Poulin A, 2007, J HYDROL ENG, V12, P394, DOI 10.1061/(ASCE)1084-0699(2007)12:4(394); Scholzel C, 2008, NONLINEAR PROC GEOPH, V15, P761, DOI 10.5194/npg-15-761-2008; Wu Q, 2019, J ECON DYN CONTROL, V109, DOI 10.1016/j.jedc.2019.103771; Yan X, 2018, ADV NEURAL INFORM PR, P1582	22	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303080
C	Yang, F; Liu, LQ; Wu, YF; Lipton, ZC; Ravikumar, P; Cohen, WW; Mitchell, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Fan; Liu Leqi; Wu, Yifan; Lipton, Zachary C.; Ravikumar, Pradeep; Cohen, William W.; Mitchell, Tom			Game Design for Eliciting Distinguishable Behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECISION; VALIDITY	The ability to inferring latent psychological traits from human behavior is key to developing personalized human-interacting machine learning systems. Approaches to infer such traits range from surveys to manually-constructed experiments and games. However, these traditional games are limited because they are typically designed based on heuristics. In this paper, we formulate the task of designing behavior diagnostic games that elicit distinguishable behavior as a mutual information maximization problem, which can be solved by optimizing a variational lower bound. Our framework is instantiated by using prospect theory to model varying player traits, and Markov Decision Processes to parameterize the games. We validate our approach empirically, showing that our designed games can successfully distinguish among players with different traits, outperforming manually-designed ones by a large margin.	[Yang, Fan; Liu Leqi; Wu, Yifan; Lipton, Zachary C.; Ravikumar, Pradeep; Cohen, William W.; Mitchell, Tom] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Cohen, William W.] Google Inc, Mountain View, CA USA	Carnegie Mellon University; Google Incorporated	Yang, F (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ffanyangl@cs.cmu.edu; leqil@cs.cmu.edu; yw4@cs.cmu.edu; zlipton@cmu.edu; pradeepr@cs.cmu.edu; wcohen@cs.cmu.edu; tom.mitchell@cmu.edu	Liu, Leqi/HGA-0678-2022		Google; ONR [N000141812861]; AI Ethics and Governance Fund; J. P. Morgan	Google(Google Incorporated); ONR(Office of Naval Research); AI Ethics and Governance Fund; J. P. Morgan	W.C. acknowledges the support of Google. L.L. and P.R. acknowledge the support of ONR via N000141812861. Z.L. acknowledges the support of the AI Ethics and Governance Fund. This research was supported in part by a grant from J. P. Morgan.	Barber D, 2004, ADV NEUR IN, V16, P201; BECHARA A, 1994, COGNITION, V50, P7, DOI 10.1016/0010-0277(94)90018-3; Buelow MT, 2009, NEUROPSYCHOL REV, V19, P102, DOI 10.1007/s11065-009-9083-4; Canossa Alessandro, 2013, C COMP INT GAM CIG; Charness G, 2013, J ECON BEHAV ORGAN, V87, P43, DOI 10.1016/j.jebo.2012.12.023; Chen X, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY (CIT), P172, DOI 10.1109/CIT.2016.65; COHEN S, 1983, J HEALTH SOC BEHAV, V24, P385, DOI 10.2307/2136404; Crosetto P, 2016, EXP ECON, V19, P613, DOI 10.1007/s10683-015-9457-9; Diener E, 2010, SOC INDIC RES, V97, P143, DOI 10.1007/s11205-009-9493-y; Dombrovski AY, 2010, AM J PSYCHIAT, V167, P699, DOI 10.1176/appi.ajp.2009.09030407; Gumbel E.J., 1954, STAT THEORY EXTREME, V33; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Howard Ronald A, 1960, DYNAMIC PROGRAMMING; Jang Eric, 2016, ARXIV16110144; KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185; Kingma D.P, P 3 INT C LEARNING R; Kroenke K, 2001, J GEN INTERN MED, V16, P606, DOI 10.1046/j.1525-1497.2001.016009606.x; Levine Sergey, 2018, ARXIV180500909; Maddison Chris J, 2017, ICLR; McGuire JT, 2012, COGNITION, V124, P216, DOI 10.1016/j.cognition.2012.03.008; Moustafa AA, 2008, J NEUROSCI, V28, P12294, DOI 10.1523/JNEUROSCI.3116-08.2008; Nielsen TS, 2015, IEEE CONF COMPU INTE, P185, DOI 10.1109/CIG.2015.7317941; Ratliff Lillian J, 2017, ARXIV170309842; Richard S., 2018, REINFORCEMENT LEARNI; Russell DW, 1996, J PERS ASSESS, V66, P20, DOI 10.1207/s15327752jpa6601_2; Tekofsky Shoshannah, 2013, INT C FDN DIG GAM; Yee Nick, 2011, C HUM FACT COMP SYST; Ziebart Brian D, 2010, MODELING PURPOSEFUL	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304066
C	Yang, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Greg			Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the tensor programs technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there.	[Yang, Greg] Microsoft Res AI, Redmond, WA 98052 USA		Yang, G (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	gregyang@microsoft.com							0	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901056
C	Yang, J; Sun, SY; Roy, DM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Jun; Sun, Shengyang; Roy, Daniel M.			Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ORACLE INEQUALITIES	The developments of Rademacher complexity and PAC-Bayesian theory have been largely independent. One exception is the PAC-Bayes theorem of Kakade, Sridharan, and Tewari [21], which is established via Rademacher complexity theory by viewing Gibbs classifiers as linear operators. The goal of this paper is to extend this bridge between Rademacher complexity and state-of-the-art PAC-Bayesian theory. We first demonstrate that one can match the fast rate of Catoni's PAC-Bayes bounds [8] using shifted Rademacher processes [27, 43, 44]. We then derive a new fast-rate PAC-Bayes bound in terms of the "flatness" of the empirical risk surface on which the posterior concentrates. Our analysis establishes a new framework for deriving fast-rate PAC-Bayes bounds and yields new insights on PAC-Bayesian theory.	[Yang, Jun; Roy, Daniel M.] Univ Toronto, Dept Stat Sci, Vector Inst, Toronto, ON, Canada; [Sun, Shengyang] Univ Toronto, Dept Comp Sci, Vector Inst, Toronto, ON, Canada	University of Toronto; University of Toronto	Yang, J (corresponding author), Univ Toronto, Dept Stat Sci, Vector Inst, Toronto, ON, Canada.	jun@utstat.toronto.edu; ssy@cs.toronto.edu; droy@utstat.toronto.edu			Alexander Graham Bell Canada Graduate Scholarship (NSERC CGS D); Ontario Graduate Scholarship (OGS); Queen Elizabeth II Graduate Scholarship in Science and Technology (QEII-GSST); Borealis AI Global Fellowship Award; Connaught New Researcher Award; Connaught Fellowship; NSERC; Ontario Early Researcher Award	Alexander Graham Bell Canada Graduate Scholarship (NSERC CGS D); Ontario Graduate Scholarship (OGS)(Ontario Graduate Scholarship); Queen Elizabeth II Graduate Scholarship in Science and Technology (QEII-GSST); Borealis AI Global Fellowship Award; Connaught New Researcher Award; Connaught Fellowship; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Ontario Early Researcher Award(Ministry of Research and Innovation, Ontario)	We would like to also thank Peter Bartlett, Gintare Karolina Dziugaite, Roger Grosse, Yasaman Mahdaviyeh, Zacharie Naulet, and Sasha Rakhlin for helpful discussions. In particular, the authors would like to thank Sasha Rakhlin for introducing us to the work of Kakade, Sridharan, and Tewari [21]. The work benefitted also from constructive feedback from anonymous referees. JY was supported by an Alexander Graham Bell Canada Graduate Scholarship (NSERC CGS D), Ontario Graduate Scholarship (OGS), and Queen Elizabeth II Graduate Scholarship in Science and Technology (QEII-GSST). SS was supported by a Borealis AI Global Fellowship Award, Connaught New Researcher Award, and Connaught Fellowship. DMR was supported by an NSERC Discovery Grant and Ontario Early Researcher Award.	Alquier P, 2018, MACH LEARN, V107, P887, DOI 10.1007/s10994-017-5690-0; Audibert JY, 2007, J MACH LEARN RES, V8, P863; Audibert JY, 2009, ANN STAT, V37, P1591, DOI 10.1214/08-AOS623; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Begin L, 2016, JMLR WORKSH CONF PRO, V51, P435; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Catoni o, 2007, LECT NOTES MONOGRAPH, V56; Dziugaite GK, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Dziugaite GK, 2018, PR MACH LEARN RES, V80; Dziugaite GK, 2018, ADV NEUR IN, V31; Germain P, 2015, J MACH LEARN RES, V16, P787; Germain Pascal, 2016, ADV NEURAL INFORM PR, P1884; Gine E, 2006, ANN PROBAB, V34, P1143, DOI 10.1214/009117906000000070; Grunwald P.D., 2019, ALGORITHMIC LEARNING, V98, P433; Guedj Benjamin, 2019, PRIMER PAC BAYESIAN; Hanneke S, 2016, J MACH LEARN RES, V17; Hanneke S, 2015, J MACH LEARN RES, V16, P3487; Kakade Sham M., 2008, NIPS; Koltchinskii V, 2002, ANN STAT, V30, P1; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Lacarelle A, 2007, SPRINGER PROC PHYS, V117, P769; Langford J, 2005, J MACH LEARN RES, V6, P273; LecuE G., 2013, ARXIV13054825; Lecue G, 2012, ELECTRON J STAT, V6, P1803, DOI 10.1214/12-EJS730; Lever G, 2013, THEOR COMPUT SCI, V473, P4, DOI 10.1016/j.tcs.2012.10.013; Liang T., 2015, C LEARNING THEORY, P1260; London B, 2017, ADV NEUR IN, V30; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; McAllester D., 2013, ARXIV PREPRINT ARXIV; McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435; Mendelson S, 2014, GOD AND NATURE IN THE THOUGHT OF MARGARET CAVENDISH, P27; Mendelson S, 2017, ANN STAT, V45, P1835, DOI 10.1214/16-AOS1510; Neyshabur Behnam, 2017, ARXIV170709564; Shawe-Taylor J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P2, DOI 10.1145/267460.267466; Smith Samuel L, 2017, ARXIV171006451; Thiemann N., 2016, ARXIV160805610; Tolstikhin Ilya O., 2013, ADV NEURAL INFORM PR, P109; van Erven T, 2015, J MACH LEARN RES, V16, P1793; vanErven T., 2014, ARXIV14051580; Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771; Wegkamp M, 2003, ANN STAT, V31, P252, DOI 10.1214/aos/1046294464; Zhivotovskiy N, 2018, THEOR COMPUT SCI, V742, P27, DOI 10.1016/j.tcs.2017.12.029	44	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902043
C	Yang, Q; Huo, ZY; Wang, WL; Huang, H; Carin, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Qian; Huo, Zhouyuan; Wang, Wenlin; Huang, Heng; Carin, Lawrence			Ouroboros: On Accelerating Training of Transformer-Based Language Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Language models are essential for natural language processing (NLP) tasks, such as machine translation and text summarization. Remarkable performance has been demonstrated recently across many NLP domains via a Transformer-based language model with over a billion parameters, verifying the benefits of model size. Model parallelism is required if a model is too large to fit in a single computing device. Current methods for model parallelism either suffer from backward locking in backpropagation or are not applicable to language models. We propose the first model-parallel algorithm that speeds the training of Transformer-based language models. We also prove that our proposed algorithm is guaranteed to converge to critical points for non-convex problems. Extensive experiments on Transformer and Transformer-XL language models demonstrate that the proposed algorithm obtains a much faster speedup beyond data parallelism, with comparable or better accuracy.	[Yang, Qian; Wang, Wenlin; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Huo, Zhouyuan; Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA	Duke University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Yang, Q (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	qian.yang@duke.edu			DARPA; DOE; NIH; ONR; NSF	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by DARPA, DOE, NIH, ONR and NSF.	Al-Rfou Rami, 2018, ARXIV180804444; Bahdanau D., 2015, P 3 INT C LEARNING R; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Cheng Yong, 2016, ARXIV161104928; Dai Z., 2019, ARXIV190102860; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fan F, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS, ELECTRONICS AND CONTROL (ICCSEC), P310, DOI 10.1109/ICCSEC.2017.8446691; Gupta Ankush, 2018, 32 AAAI C ART INT; Hinton G., 2012, COURSERA LECT SLIDES; Huo Z., 2018, ADV NEURAL INFORM PR, P6659; Huo Z., 2018, ARXIV180410574; Inan H., 2016, ARXIV161101462; Jaderberg M, 2017, PR MACH LEARN RES, V70; Juan H, 2019, 2019 INTERNATIONAL CONFERENCE ON ARTS, MANAGEMENT, EDUCATION AND INNOVATION (ICAMEI 2019), P41, DOI 10.23977/icamei.2019.008; Kingma D.P, P 3 INT C LEARNING R; Kyrola A., 2017, ABS170602677 ARXIV; Li Hang, 2017, ARXIV171100279; Loshchilov I., 2017, ARXIV171105101; Loshchilov Ilya, 2016, INT C LEARN REPR; Luong M-T, 2015, ARXIV150804025; Mahoney Matt, 2011, LARGE TEXT COMPRESSI; Nallapati Ramesh, 2016, ABSTRACTIVE TEXT SUM; Passonneau RJ, 2018, INT J ARTIF INTELL E, V28, P29, DOI 10.1007/s40593-016-0128-6; Press O., 2016, ARXIV160805859; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Yadan Omry, 2013, ARXIV13125853; Yang Q, 2016, AAAI CONF ARTIF INTE, P2673; Yang Qian, 2019, EMNLP	37	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305050
C	Yang, S; Shen, YY; Sanghavi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Shuo; Shen, Yanyao; Sanghavi, Sujay			Interaction Hard Thresholding: Consistent Sparse Quadratic Regression in Sub-quadratic Time and Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SELECTION	Quadratic regression involves modeling the response as a (generalized) linear function of not only the features x3, but also of quadratic terms x31x32. The inclusion of such higher-order "interaction terms" in regression often provides an easy way to increase accuracy in already -high -dimensional problems. However, this explodes the problem dimension from linear O(p) to quadratic 0 (p2), and it is common to look for sparse interactions (typically via heuristics). In this paper we provide a new algorithm Interaction Hard Thresholding (IntHT) which is the first one to provably accurately solve this problem in sub -quadratic time and space. It is a variant of Iterative Hard Thresholding; one that uses the special quadratic structure to devise a new way to (approx.) extract the top elements of a p2 size gradient in sub -p2 time and space. Our main result is to theoretically prove that, in spite of the many speedup -related approximations, IntHT linearly converges to a consistent estimate under standard high -dimensional sparse recovery assumptions. We also demonstrate its value via synthetic experiments. Moreover, we numerically show that IntHT can be extended to higher-order regression problems, and also theoretically analyze an SVRG variant of IntHT.	[Yang, Shuo] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA; [Shen, Yanyao; Sanghavi, Sujay] Univ Texas Austin, ECE Dept, Austin, TX 78712 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Yang, S (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	yangshuo_ut@utexas.edu; shenyanyao@utexas.edu; sanghavi@mail.utexas.edu			NSF [1302435, 1564000]	NSF(National Science Foundation (NSF))	We would like to acknowledge NSF grants 1302435 and 1564000 for supporting this research.	Abboud A, 2017, ANN IEEE SYMP FOUND, P25, DOI 10.1109/FOCS.2017.12; Ballard G, 2015, IEEE DATA MINING, P11, DOI 10.1109/ICDM.2015.46; Bien J, 2013, ANN STAT, V41, P1111, DOI 10.1214/13-AOS1096; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Chen LJ, 2018, LEIBNIZ INT PR INFOR, V102, DOI 10.4230/LIPIcs.CCC.2018.14; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Choi NH, 2010, J AM STAT ASSOC, V105, P354, DOI 10.1198/jasa.2010.tm08281; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Hao B., 2018, ARXIV180109326; Hao N, 2018, J AM STAT ASSOC, V113, P615, DOI 10.1080/01621459.2016.1264956; Hao N, 2014, J AM STAT ASSOC, V109, P1285, DOI 10.1080/01621459.2014.881741; Kar P., 2014, ADV NEURAL INFORM PR, P685; Kocaoglu M., 2014, ADV NEURAL INFORM PR, V3122; Li X., 2016, NONCONVEX SPARSE LEA; Li Y, 2015, STAT APPL GENET MOL, V14, P265, DOI 10.1515/sagmb-2014-0073; Lim M, 2015, J COMPUT GRAPH STAT, V24, P627, DOI 10.1080/10618600.2014.938812; Liu Liu, 2018, ARXIV180511643; MANSOUR Y, 1995, SIAM J COMPUT, V24, P357, DOI 10.1137/S0097539792239291; Murata Tomoya, 2018, ADV NEURAL INFORM PR, P5317; Nguyen N, 2017, IEEE T INFORM THEORY, V63, P6869, DOI 10.1109/TIT.2017.2749330; Pagh R, 2013, ACM T COMPUT THEORY, V5, DOI 10.1145/2493252.2493254; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Shen J., 2017, J MACH LEARN RES, V18, P7650; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Sipser M., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P566, DOI 10.1109/SFCS.1994.365734; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Williams R, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1207; Wu J, 2010, GENET EPIDEMIOL, V34, P275, DOI 10.1002/gepi.20459; Yu Hsiang-Fu, 2017, ADV NEURAL INFORM PR, P5453; Zhang KQ, 2018, PR MACH LEARN RES, V84; Zhang N, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON POWER AND RENEWABLE ENERGY (ICPRE), P378	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307089
C	Yang, YX; Wang, HX; Kiyavash, N; He, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Yingxiang; Wang, Haoxiang; Kiyavash, Negar; He, Niao			Learning Positive Functions with Pseudo Mirror Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CUTTING PLANE ALGORITHM	The nonparametric learning of positive-valued functions appears widely in machine learning, especially in the context of estimating intensity functions of point processes. Yet, existing approaches either require computing expensive projections or semidefinite relaxations, or lack convexity and theoretical guarantees after introducing nonlinear link functions. In this paper, we propose a novel algorithm, pseudo mirror descent, that performs efficient estimation of positive functions within a Hilbert space without expensive projections. The algorithm guarantees positivity by performing mirror descent with an appropriately selected Bregman divergence, and a pseudo-gradient is adopted to speed up the gradient evaluation procedure in practice. We analyze both asymptotic and nonasymptotic convergence of the algorithm. Through simulations, we show that pseudo mirror descent outperforms the state-of-the-art benchmarks for learning intensities of Poisson and multivariate Hawkes processes, in terms of both computational efficiency and accuracy.	[Yang, Yingxiang; Wang, Haoxiang; He, Niao] UIUC, Champaign, IL 61820 USA; [Kiyavash, Negar] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	University of Illinois System; University of Illinois Urbana-Champaign; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Yang, YX (corresponding author), UIUC, Champaign, IL 61820 USA.	yyang172@illinois.edu; hwang264@illinois.edu; negar.kiyavash@epfl.ch; niaohe@illinois.edu	YANG, Yingxiang/GQI-2269-2022		MURI [ARMY W911NF-15-1-0479]; ONR [W911NF-15-1-0479]; NSF [CCF-1755829, CMMI-1761699]	MURI(MURI); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This work was supported in part by MURI grant ARMY W911NF-15-1-0479, ONR grant W911NF-15-1-0479, NSF CCF-1755829 and NSF CMMI-1761699.	Andersen M, 2013, ABEL EE UCLA EDU CVX; Bacry Emmanuel, 2017, ARXIV170703003; Bagnell J.A., 2015, NIPS WORKSH OPT OPT2; Bauschke H. H., 2011, CONVEX ANAL MONOTONE, V408; Betro B, 2004, MATH PROGRAM, V101, P479, DOI 10.1007/s10107-003-0492-5; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Bubeck S., 2015, FDN TRENDS MACHINE L; Carstensen L, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-456; Chen X, 2008, CELL, V133, P1106, DOI 10.1016/j.cell.2008.04.043; Craciun P, 2015, IEEE WINT CONF APPL, P177, DOI 10.1109/WACV.2015.31; Embrechts P, 2011, J APPL PROBAB, V48A, P367; Farajtabar M, 2015, ADV NEURAL INFORM PR, P1954; Farajtabar M, 2017, PR MACH LEARN RES, V70; Flaxman S, 2017, ELECTRON J STAT, V11, P5081, DOI 10.1214/17-EJS1339SI; Goodfellow Ian J., 2015, INT C LEARNING REPRE; Grant M., 2014, CVX MATLAB SOFTWARE; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Kobayashi M, 2013, IEEE INT WORK SIGN P, P200, DOI 10.1109/SPAWC.2013.6612040; Kortanek KO, 1993, SIAM J OPTIMIZ, V3, P901, DOI 10.1137/0803047; Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497; Lewis Erik, 2011, NONPARAMETRIC ALGORI; Mei H., 2017, ADV NEURAL INFORM PR, P6754; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nemirovski Arkadi. S., 1983, PROBLEMCOMPLEXITY ME; Papp D, 2017, SIAM J OPTIMIZ, V27, P1858, DOI 10.1137/15M1053578; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Poljak BT, 1973, AUTOMAT REM CONTR+, V34, P45; Prestel Alexander, 2013, 17 PROBL REAL ALG; Rosasco L, 2010, J MACH LEARN RES, V11, P905; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Wahba G., 1990, SPLINE MODELS OBSERV, V59; Wen W., 2017, P NIPS, P1509; Wu J., 2018, ARXIV180608054; Wu SY, 1999, COMPUT MATH APPL, V38, P23, DOI 10.1016/S0898-1221(99)00203-5; Xiao S, 2017, AAAI CONF ARTIF INTE, P1597; Xu H, 2016, INT C MACH LEARN, P1717; Yang Y., 2017, NEURAL INFORM PROCES	39	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905078
C	Yang, Z; Luong, T; Salakhutdinov, R; Le, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Zhilin; Thang Luong; Salakhutdinov, Ruslan; Quoc Le			Mixtape: Breaking the Softmax Bottleneck Efficiently	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The softmax bottleneck has been shown to limit the expressiveness of neural language models. Mixture of Softmaxes (MoS) is an effective approach to address such a theoretical limitation, but are expensive compared to softmax in terms of both memory and time. We propose Mixtape, an output layer that breaks the softmax bottleneck more efficiently with three novel techniques-logit space vector gating, sigmoid tree decomposition, and gate sharing. On four benchmarks including language modeling and machine translation, the Mixtape layer substantially improves the efficiency over the MoS layer by 3.5x to 10.5x while obtaining similar performance. A network equipped with Mixtape is only 20% to 34% slower than a softmax-based network with 10-30K vocabulary sizes, and outperforms softmax in perplexity and translation quality.	[Yang, Zhilin; Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Thang Luong; Quoc Le] Google Brain, Mountain View, CA USA	Carnegie Mellon University; Google Incorporated	Yang, Z (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zhiliny@cs.cmu.edu; thangluong@google.com; rsalakhu@cs.cmu.edu; qvl@google.com						Ahmed K, 2017, ARXIV171102132; Al-Rfou Rami, 2018, ARXIV180804444; Baevski A., 2018, PROC INT C LEARN REP; Dai Z., 2019, ARXIV190102860; Edunov S., 2018, ARXIV PREPRINT ARXIV; Joulin A., 2017, P INT C MACH LEARN I, P1302; Kanai S., 2018, ADV NEURAL INFORM PR; Kong Xiang, 2018, ARXIV180909296; Merity Stephen, 2017, ICLR; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Ott M., 2018, WMT; Sennrich Rico, 2015, ARXIV PREPRINT ARXIV; Shaw P., 2018, P NAACL HLT; Shazeer N., 2018, ADV NEURAL INFORM PR, P10435; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Yang Zhilin, 2017, ARXIV171103953	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308039
C	Yang, ZY; Xu, QQ; Jiang, YBY; Cao, XC; Huang, QM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Zhiyong; Xu, Qianqian; Jiang, Yangbangyan; Cao, Xiaochun; Huang, Qingming			Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In multi-task learning, a major challenge springs from a notorious issue known as negative transfer, which refers to the phenomenon that sharing the knowledge with dissimilar and hard tasks often results in a worsened performance. To circumvent this issue, we propose a novel multi-task learning method, which simultaneously learns latent task representations and a block-diagonal Latent Task Assignment Matrix (LTAM). Different from most of the previous work, pursuing the Block-Diagonal structure of LTAM (assigning latent tasks to output tasks) alleviates negative transfer via punishing inter-group knowledge transfer and sharing. This goal is challenging since our notion of Block-Diagonal Property extends the traditional notion for homogeneous and square matrices. In this paper, we propose a spectral regularizer which is proven to leverage the expected structure. Practically, we provide a relaxation scheme which improves the flexibility of the model. With the objective function given, we then propose an alternating optimization method, which reveals an interesting connection between our method and the optimal transport problem. Finally, the method is demonstrated on a simulation dataset, three real-world benchmark datasets and further applied to two personalized attribute learning datasets.	[Yang, Zhiyong; Jiang, Yangbangyan; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China; [Yang, Zhiyong; Jiang, Yangbangyan; Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China; [Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Tech, Beijing, Peoples R China; [Huang, Qingming] Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing, Peoples R China; [Cao, Xiaochun; Huang, Qingming] Peng Cheng Lab, Shenzhen, Peoples R China	Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Peng Cheng Laboratory	Huang, QM (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.; Huang, QM (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Tech, Beijing, Peoples R China.; Huang, QM (corresponding author), Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing, Peoples R China.; Huang, QM (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	yangzhiyong@iie.ac.cn; xuqianqian@ict.ac.cn; jiangyangbangyan@iie.ac.cn; caoxiaochun@iie.ac.cn; qmhuang@ucas.ac.cn			National Natural Science Foundation of China [61620106009, U1636214, 61836002, 61861166002, 61672514, 61976202]; National Basic Research Program of China (973 Program) [2015CB351800]; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]; Science and Technology Development Fund of Macau SAR [0001/2018/AFJ]; Beijing Natural Science Foundation [61971016, L182057, 4182079]; Peng Cheng Laboratory Project of Guangdong Province [PCL2018KP004]; Youth Innovation Promotion Association CAS; Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-SYS013]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Basic Research Program of China (973 Program)(National Basic Research Program of China); Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences); Science and Technology Development Fund of Macau SAR; Beijing Natural Science Foundation(Beijing Natural Science Foundation); Peng Cheng Laboratory Project of Guangdong Province; Youth Innovation Promotion Association CAS; Key Research Program of Frontier Sciences, CAS	This work was supported in part by National Natural Science Foundation of China: 61620106009, U1636214, 61836002, 61861166002, 61672514 and 61976202, in part by National Basic Research Program of China (973 Program): 2015CB351800, in part by Key Research Program of Frontier Sciences, CAS: QYZDJ-SSW-SYS013, in part by the Strategic Priority Research Program of Chinese Academy of Sciences, Grant No. XDB28000000, in part by the Science and Technology Development Fund of Macau SAR (File no. 0001/2018/AFJ) Joint Scientific Research Project, in part by Beijing Natural Science Foundation (No. 61971016, L182057, and 4182079), in part by Peng Cheng Laboratory Project of Guangdong Province PCL2018KP004, and in part by Youth Innovation Promotion Association CAS.	[Anonymous], 2018, PAMI; [Anonymous], 2018, ADV NEURAL INFORM PR; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Argyriou Andreas, 2008, ADV NEURAL INFORM PR, P25; Barzilai A, 2015, JMLR WORKSH CONF PRO, V38, P65; Blondel M., 2018, P MACHINE LEARNING R, P880; Cao JJ, 2018, PROC CVPR IEEE, P4290, DOI 10.1109/CVPR.2018.00451; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Crammer Koby, 2012, PROC 25 INT C NEURAL, P1475; Frecon J., 2018, ADV NEURAL INFORM PR, V31, P8311; Gao W, 2016, ARTIF INTELL, V236, P1, DOI 10.1016/j.artint.2016.03.003; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Han L, 2016, AAAI CONF ARTIF INTE, P1638; Heskes T., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P233; Horn R.A., 2013, MATRIX ANAL, VSecond; Jacob L., 2009, P 21TH NIPS, P745; Jeong JY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1589, DOI 10.1145/3219819.3219992; Jiang Y., 2019, ACM MM; Jiang YBY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P718, DOI 10.1145/3240508.3240582; Kang Z., 2011, P INT C MACH LEARN, V2, P4; Kovashka A, 2015, INT J COMPUT VISION, V114, P56, DOI 10.1007/s11263-014-0798-1; Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026; Kshirsagar M, 2017, LECT NOTES ARTIF INT, V10535, P673, DOI 10.1007/978-3-319-71246-8_41; Kumar A., 2012, P INT C MACH LEARN, P1723; Lee H., 2018, P 35 INT C MACH LEAR, P2962; Li YG, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1695, DOI 10.1145/3219819.3220033; Lin Y, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P799; Liu PF, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1, DOI 10.18653/v1/P17-1001; Liu SL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2358; LU CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI DOI 10.1109/TPAMI.2018.2794348; Maurer A, 2016, J MACH LEARN RES, V17; McDonald A. M., 2014, PROC 27 INT C NEURAL, P3644; Nie FP, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2012, DOI 10.1145/3219819.3219951; OVERTON ML, 1992, SIAM J MATRIX ANAL A, V13, P41, DOI 10.1137/0613006; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Seguy V., 2018, ICLR; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077; Xu LL, 2015, AAAI CONF ARTIF INTE, P1931; Yang Z., 2019, IEEE T IMAGE PROCESS; Yang ZY, 2019, AAAI CONF ARTIF INTE, P5660; Yang ZY, 2018, AAAI CONF ARTIF INTE, P515; Yu Y, 2015, BIOMETRIKA, V102, P315, DOI 10.1093/biomet/asv008; Zhong W, 2012, P 29 INT C MACH LEAR; Zhou J., 2011, ARIZONA STATE U, V21; Zhou Jiayu, 2011, Adv Neural Inf Process Syst, V2011, P702; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	53	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305080
C	Ying, R; Bourgeois, D; You, JX; Zitnik, M; Leskovec, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ying, Rex; Bourgeois, Dylan; You, Jiaxuan; Zitnik, Marinka; Leskovec, Jure			GNNExplainer: Generating Explanations for Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GNNEXPLAINER, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNEXPLAINER identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNEXPLAINER can generate consistent and concise explanations for an entire class of instances. We formulate GNNEXPLAINER as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GNNEXPLAINER provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.	[Ying, Rex; Bourgeois, Dylan; You, Jiaxuan; Zitnik, Marinka; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Bourgeois, Dylan] Robust AI, Stanford, CA USA	Stanford University	Ying, R (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	rexying@cs.stanford.edu; dtsbourg@cs.stanford.edu; jiaxuan@cs.stanford.edu; marinka@cs.stanford.edu; jure@cs.stanford.edu	You, Jiaxuan/ABC-7506-2020		DARPA [FA865018C7880]; MSC; NIH [U54EB020405]; ARO [38796-Z8424103]; IARPA [2017-17071900005]; NSF [OAC-1835598]; HDR; Stanford Data Science Initiative; Chan Zuckerberg Biohub; JD.com; Amazon; Boeing; Docomo; Huawei; Hitachi; Observe; Siemens; UST Global	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); MSC; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ARO; IARPA; NSF(National Science Foundation (NSF)); HDR; Stanford Data Science Initiative; Chan Zuckerberg Biohub; JD.com; Amazon; Boeing; Docomo; Huawei(Huawei Technologies); Hitachi; Observe; Siemens(Siemens AG); UST Global	Jure Leskovec is a Chan Zuckerberg Biohub investigator. We gratefully acknowledge the support of DARPA under FA865018C7880 (ASED) and MSC; NIH under No. U54EB020405 (Mobilize); ARO under No. 38796-Z8424103 (MURI); IARPA under No. 2017-17071900005 (HFC), NSF under No. OAC-1835598 (CINES) and HDR; Stanford Data Science Initiative, Chan Zuckerberg Biohub, JD.com, Amazon, Boeing, Docomo, Huawei, Hitachi, Observe, Siemens, UST Global. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.	Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052; Adebayo J., 2018, NEURIPS; [Anonymous], 2015, NIPS; Augasta MG, 2012, NEURAL PROCESS LETT, V35, P131, DOI 10.1007/s11063-011-9207-8; Battaglia Peter W, 2018, ARXIV 180601261; Chen J., 2018, ARXIV180207814; Chen J., 2018, ICML; Chen J., 2018, ICLR; Chen Zhengdao, 2019, ICLR; Cho E., 2011, SIGKDD; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Erhan D, 2009, BERNOULLI, V1341, P1; Finale Doshi-Velez, 2017, RIGOROUS SCI INTERPR; Fisher A., 2018, ARXIV PREPRINT ARXIV, P68; Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009; Hamilton William L., 2017, NIPS; Hooker G., 2004, KDD; Huang W.B., 2018, NEURIPS; Kamar E., 2017, INTERPRETABLE EXPLOR; Kang Bo, 2019, ARXIV190412694; Kingma Diederik P, 2013, NEURIPS; Kipf T. N., 2016, ICLR; Kipf Thomas, 2018, ICML; Koh P. W., 2017, ICML; Kumar S, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P933, DOI 10.1145/3178876.3186141; Lundberg Scott M, 2017, NIPS; Neil D., 2018, ML4H WORKSH NEURIPS; Ribeiro M. T, 2016, KDD; Schmitz G., 1999, IEEE T NEURAL NETWOR; Shrikumar A., 2017, ICML; Sundararajan M., 2017, ICML; Velickovic P., 2018, P 6 INT C LEARN REPR; Xie T., 2018, PHYS REV LETT; Xu K., 2018, ICML; Xu K., 2019, ICRL; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Yeh C., 2018, NEURIPS; Ying R., 2018, KDD; Ying Z., 2018, NEURIPS; You J., 2019, ICML; You J., 2018, GRAPH CONVOLUTIONAL; Zeiler MD, 2014, ECCV; Zhang Muhan, 2018, NIPS; Zhang Z., 2018, ARXIV181204202; Zhou J., 2018, ARXIV181208434; Zilke J., 2016, DISCOVERY SCI; Zintgraf L. M., 2017, ICLR; Zitnik M, 2018, BIOINFORMATICS, V34, P457, DOI 10.1093/bioinformatics/bty294	49	0	0	2	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900079
C	You, JX; Wu, HZ; Barrett, C; Ramanujan, R; Leskovec, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		You, Jiaxuan; Wu, Haoze; Barrett, Clark; Ramanujan, Raghuram; Leskovec, Jure			G2SAT: Learning to Generate SAT Formulas	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The Boolean Satisfiability (SAT) problem is the canonical NP-complete problem and is fundamental to computer science, with a wide array of applications in planning, verification, and theorem proving. Developing and evaluating practical SAT solvers relies on extensive empirical testing on a set of real-world benchmark for-mulas. However, the availability of such real-world SAT formulas is limited. While these benchmark formulas can be augmented with synthetically generated ones, existing approaches for doing so are heavily hand-crafted and fail to simultaneously capture a wide range of characteristics exhibited by real-world SAT instances. In this work, we present G2SAT, the first deep generative framework that learns to generate SAT formulas from a given set of input formulas. Our key insight is that SAT formulas can be transformed into latent bipartite graph representations which we model using a specialized deep generative neural network. We show that G2SAT can generate SAT formulas that closely resemble given real-world SAT instances, as measured by both graph metrics and SAT solver behavior. Further, we show that our synthetic SAT formulas could be used to improve SAT solver performance on real-world benchmarks, which opens up new opportunities for the continued development of SAT solvers and a deeper understanding of their performance.	[You, Jiaxuan; Wu, Haoze; Barrett, Clark; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Ramanujan, Raghuram] Davidson Coll, Davidson, NC 28036 USA	Stanford University; Davidson College	You, JX (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	jiaxuan@cs.stanford.edu; haozewu@stanford.edu; barrett@cs.stanford.edu; raramanujan@davidson.edu; jure@cs.stanford.edu	You, Jiaxuan/ABC-7506-2020	Wu, Haoze/0000-0002-5077-144X; Barrett, Clark/0000-0002-9522-3084	DARPA [FA865018C7880]; MSC; NIH [U54EB020405]; ARO [38796-Z8424103]; IARPA [2017-17071900005]; NSF [OAC-1835598]; HDR; Stanford Data Science Initiative; Chan Zuckerberg Biohub; Enlight Foundation; JD.com; Amazon; Boeing; Docomo; Huawei; Hitachi; Observe; Siemens; UST Global	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); MSC; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ARO; IARPA; NSF(National Science Foundation (NSF)); HDR; Stanford Data Science Initiative; Chan Zuckerberg Biohub; Enlight Foundation; JD.com; Amazon; Boeing; Docomo; Huawei(Huawei Technologies); Hitachi; Observe; Siemens(Siemens AG); UST Global	Jure Leskovec is a Chan Zuckerberg Biohub investigator. We gratefully acknowledge the support of DARPA under No. FA865018C7880 (ASED) and MSC; NIH under No. U54EB020405 (Mobilize); ARO under No. 38796-Z8424103 (MURI); IARPA under No. 2017-17071900005 (HFC); NSF under No. OAC-1835598 (CINES) and HDR; Stanford Data Science Initiative, Chan Zuckerberg Biohub, Enlight Foundation, JD.com, Amazon, Boeing, Docomo, Huawei, Hitachi, Observe, Siemens, UST Global. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.	[Anonymous], 2017, ADV NEURAL INFORM PR; Ansotegui C, 2009, LECT NOTES COMPUT SC, V5732, P127, DOI 10.1007/978-3-642-04244-7_13; Audemard G, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P399; Biere A, 2009, FRONT ARTIF INTEL AP, V185, P457, DOI 10.3233/978-1-58603-929-5-457; Bojchevski A., 2018, P 35 INT C MACH LEAR; Boufkhad Y, 2005, J AUTOM REASONING, V35, P181, DOI 10.1007/s10817-005-9012-z; Braunstein A, 2005, RANDOM STRUCT ALGOR, V27, P201, DOI 10.1002/rsa.20057; Clarke E, 2001, FORM METHOD SYST DES, V19, P7, DOI 10.1023/A:1011276507260; Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111; Cook S. A., 1971, Proceedings of the 3rd annual ACM symposium on theory of computing, P151; De Cao N., 2018, ICML 2018 WORKSH THE; Een N, 2005, LECT NOTES COMPUT SC, V3569, P61, DOI 10.1007/11499107_5; Een N, 2004, LECT NOTES COMPUT SC, V2919, P502, DOI 10.1007/978-3-540-24605-3_37; Escamocher G., 2019, ABS190303592 CORR; Fracastoro G, 2019, INT C LEARN REPR; Ganzinger H, 2004, LECT NOTES COMPUT SC, V3114, P175, DOI 10.1007/978-3-540-27813-9_14; Giraldez-Cru J., 2017, P 26 INT JOINT C ART, P638; Giraldez-Cru J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1952; Grover Aditya, 2018, ARXIV180310459; Hamilton W.L., 2017, IEEE DATA ENG B; Heule M. J. H., 2018, P SAT COMP 2018; Hoos H., 2000, SATLIB ONLINE RESOUR, P283; Jin Wengong, 2018, INT C MACH LEARN; Katsirelos George, 2012, Principles and Practice of Constraint Programming. Proceedings 18th International Conference, CP 2012, P348, DOI 10.1007/978-3-642-33558-7_27; KAUTZ H, 1992, ECAI 92 - 10TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE : PROCEEDINGS, P359; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2016, VARIATIONAL GRAPH AU; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Lauria M, 2017, LECT NOTES COMPUT SC, V10491, P464, DOI 10.1007/978-3-319-66263-3_30; Li Y., 2018, ARXIV; MarquesSilva J., 2009, HDB SATISFIABILITY; Mull N, 2016, LECT NOTES COMPUT SC, V9710, P141, DOI 10.1007/978-3-319-40970-2_10; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Newman ME, 2001, PHYS REV E, V64; Newsham Z, 2014, LECT NOTES COMPUT SC, V8561, P252, DOI 10.1007/978-3-319-09284-3_20; Selman B, 1996, ARTIF INTELL, V81, P17, DOI 10.1016/0004-3702(95)00045-3; Selman B, 1997, INT JOINT CONF ARTIF, P50; Selman B., 1999, CLIQUES COLORING SAT, V26; Selsam D., 2018, ARXIV180203685; Tseitin G., 1968, STUDIES CONSTRUCTIVE, P115; Wu H, 2019, P ASME TURB EXP 2019, P17; Xu K., 2005, P 19 INT JOINT C ART; You J., 2019, INT C MACH LEARN; You J., 2018, INT C MACH LEARN; You Jiaxuan, 2018, ADV NEURAL INFORM PR	45	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902021
C	Yousefi, F; Smith, MT; Alvarez, MA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yousefi, Fariba; Smith, Michael Thomas; Alvarez, Mauricio A.			Multi-task Learning for Aggregated Data using Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Aggregated data is commonplace in areas such as epidemiology and demography. For example, census data for a population is usually given as averages defined over time periods or spatial resolutions (cities, regions or countries). In this paper, we present a novel multi-task learning model based on Gaussian processes for joint learning of variables that have been aggregated at different input scales. Our model represents each task as the linear combination of the realizations of latent processes that are integrated at a different scale per task. We are then able to compute the cross-covariance between the different tasks either analytically or numerically. We also allow each task to have a potentially different likelihood model and provide a variational lower bound that can be optimised in a stochastic fashion making our model suitable for larger datasets. We show examples of the model in a synthetic example, a fertility dataset and an air pollution prediction application.	[Yousefi, Fariba; Smith, Michael Thomas; Alvarez, Mauricio A.] Univ Sheffield, Dept Comp Sci, Sheffield, S Yorkshire, England	University of Sheffield	Yousefi, F (corresponding author), Univ Sheffield, Dept Comp Sci, Sheffield, S Yorkshire, England.	f.yousefi@sheffield.ac.uk; m.t.smith@sheffield.ac.uk; mauricio.alvarez@sheffield.ac.uk		Alvarez, Mauricio A./0000-0002-8980-4472	Engineering and Physical Research Council (EPSRC) [EP/N014162/1]; EPSRC Research Project [EP/R034303/1]	Engineering and Physical Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Research Project(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	MTS and MAA have been financed by the Engineering and Physical Research Council (EPSRC) Research Project EP/N014162/1. MAA has also been financed by the EPSRC Research Project EP/R034303/1.	Alvarez M. A., 2009, ADV NEURAL INFORM PR, P57; Alvarez M. A., 2010, P 13 INT C ART INT S, P25; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Beal M.J, 2003, THESIS; BHOWMIK A, 2015, AISTATS, V38, P93; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Boyle P., 2005, ADV NEURAL INFORM PR, P217; de Freitas N., 2005, P 21 C UNC ART INT, P332; Fernandez-Godino M.G., 2016, REV MULTIFIDELITY MO; Goovaerts P., 1997, GEOSTATISTICS NATURA; Goovaerts P, 2010, MATH GEOSCI, V42, P535, DOI 10.1007/s11004-010-9286-5; Gotway CA, 2002, J AM STAT ASSOC, V97, P632, DOI 10.1198/016214502760047140; Hamelijnck Oliver, 2019, ARXIV190608344; HAUSSMANN M, 2017, CVPR, P810, DOI DOI 10.1109/CVPR.2017.93; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Journel A.G., 1978, MINING GEOSTATISTICS; Kandemir Melih, 2016, P BRIT MACH VIS C BM; Kotzias D, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P597, DOI 10.1145/2783258.2783380; Kyriakidis PC, 2004, GEOGR ANAL, V36, P259, DOI 10.1353/geo.2004.0009; Law H. C. L., 2018, NEURIPS, P6084; Minjang Kim, 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P535, DOI 10.1109/MICRO.2010.49; Moreno-Munoz P., 2018, ADV NEURAL INFORM PR, P6711; Musicant DR, 2007, IEEE DATA MINING, P252, DOI 10.1109/ICDM.2007.50; Patrini Giorgio, 2014, ADV NEURAL INFORM PR, P190; Peherstorfer B, 2018, SIAM REV, V60, P550, DOI 10.1137/16M1082469; Quadrianto N, 2009, J MACH LEARN RES, V10, P2349; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Saul AD, 2016, JMLR WORKSH CONF PRO, V51, P1431; Smith Michael T., 2018, ARXIV180902010; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Tanaka Y, 2019, AAAI CONF ARTIF INTE, P5091; Tanaka Yusuke, 2019, ARXIV190708350; Teh Y.W., 2005, WORKSH ART INT STAT, V2005, P333; Wilson A. G., 2012, ICML; Zhang J.X., 2014, SCALE SPATIAL INFORM	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906070
C	Yu, XM; Chen, YQ; Li, T; Liu, S; Li, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Xiaoming; Chen, Yuanqi; Li, Thomas; Liu, Shan; Li, Ge			Multi-mapping Image-to-Image Translation via Learning Disentanglement	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent advances of image-to-image translation focus on learning the one-to-many mapping from two aspects: multi-modal translation and multi-domain translation. However, the existing methods only consider one of the two perspectives, which makes them unable to solve each other's problem. To address this issue, we propose a novel unified model, which bridges these two objectives. First, we disentangle the input images into the latent representations by an encoder-decoder architecture with a conditional adversarial training in the feature space. Then, we encourage the generator to learn multi-mappings by a random cross-domain translation. As a result, we can manipulate different parts of the latent representations to perform multi-modal and multi-domain translations simultaneously. Experiments demonstrate that our method outperforms state-of-the-art methods. Code will be available at https://github.com/Xiaoming-Yu/DMIT.	[Yu, Xiaoming; Chen, Yuanqi; Li, Thomas; Li, Ge] Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China; [Yu, Xiaoming; Chen, Yuanqi; Li, Ge] Peng Cheng Lab, Shenzhen, Peoples R China; [Li, Thomas] Peking Univ, Adv Inst Informat Technol, Beijing, Peoples R China; [Liu, Shan] Tencent Amer, Palo Alto, CA USA	Peking University; Peng Cheng Laboratory; Peking University	Li, G (corresponding author), Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China.; Li, G (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	xiaomingyu@pku.edu.cn; cyq373@pku.edu.cn; tli@aiit.org.cn; shanl@tencent.com; geli@ece.pku.edu.cn			Shenzhen Municipal Science and Technology Program [JCYJ20170818141146428]; National Engineering Laboratory for Video Technology -Shenzhen Division; National Natural Science Foundation of China; Guangdong Province Scientific Research on Big Data [U1611461]	Shenzhen Municipal Science and Technology Program; National Engineering Laboratory for Video Technology -Shenzhen Division; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Guangdong Province Scientific Research on Big Data	This work was supported in part by Shenzhen Municipal Science and Technology Program (No. JCYJ20170818141146428), National Engineering Laboratory for Video Technology -Shenzhen Division, and National Natural Science Foundation of China and Guangdong Province Scientific Research on Big Data (No. U1611461). In addition, we would like to thank the anonymous reviewers for their helpful and constructive comments.	Alemi A. A., 2017, ICLR; [Anonymous], 2017, NIPS; [Anonymous], 2018, ECCV; [Anonymous], 2018, CVPR; [Anonymous], NIPS; [Anonymous], 2017, ICCV; Ba J., 2017, P 3 INT C LEARN REPR; Chen X, 2016, ADV NEUR IN, V29; Choi Y., 2018, CVPR; Choi Y., 2019, ARXIV191201865; Donahue Jeff, 2016, ICLR; Dong H., 2017, ICCV; Dumoulin V., 2016, ICLR; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; GATYS L. A., 2016, CVPR; Gonzalez-Garcia Abel, 2018, NIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heusel M., 2017, NIPS; Higgins I., 2017, ICLR; Huang X., 2017, ICCV; Huang Xun, 2018, ECCV; Isola P., 2017, CVPR; Karras Tero, 2019, CVPR; Kim T, 2017, ICML; Lample Guillaume, 2017, NIPS; Larsen Anders Boesen Lindbo, 2016, ICML; Liu M. -Y., 2017, NIPS; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mirza M., 2014, ARXIV PREPRINT ARXIV; Miyato Takeru, 2018, ICLR; Nam Seonghyeon, 2018, NIPS; Press Ori, 2019, ICLR; Reed S. E., 2016, CVPR; Romero Andres<prime>, 2019, ICCV WORKSH; Sohn K., 2015, NIPS; Tian Y., 2018, IJCAI; Vo D.M., 2018, ACCV; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang Yaxing, 2019, ACM MM; Xu T., 2018, CVPR; Yang D, 2019, INT C LEARN REPR; Yi Z., 2017, ICCV; Yu A., 2014, CVPR; Yu X., 2018, ARXIV PREPRINT ARXIV; Yu Xiaoming, 2018, ACCV	45	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303003
C	Yun, C; Sra, S; Jadbabaie, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yun, Chulhee; Sra, Suvrit; Jadbabaie, Ali			Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FEEDFORWARD NETWORKS; STORAGE CAPACITY; HIDDEN NEURONS; VC-DIMENSION; BOUNDS; NUMBER	We study finite sample expressivity, i.e., memorization power of ReLU networks. Recent results require N hidden nodes to memorize/interpolate arbitrary N data points. In contrast, by exploiting depth, we show that 3-layer ReLU networks with Omega(root N) hidden nodes can perfectly memorize most datasets with N points. We also prove that width Omega(root N) is necessary and sufficient for memorizing N data points, proving tight bounds on memorization capacity. The sufficiency result can be extended to deeper networks; we show that an L-layer network with W parameters in the hidden layers can memorize N data points if W = Omega(N). Combined with a recent upper bound O(WL log W) on VC dimension, our construction is nearly tight for any fixed L. Subsequently, we analyze memorization capacity of residual networks under a general position assumption; we prove results that substantially reduce the known requirement of N hidden nodes. Finally, we study the dynamics of stochastic gradient descent (SGD), and show that when initialized near a memorizing global minimum of the empirical risk, SGD quickly finds a nearby point with much smaller empirical risk.	[Yun, Chulhee; Sra, Suvrit; Jadbabaie, Ali] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Yun, C (corresponding author), MIT, Cambridge, MA 02139 USA.	chulheey@mit.edu; suvrit@mit.edu; jadbabai@mit.edu			DARPA Lagrange; Korea Foundation for Advanced Studies; NSF-CAREER grant; Amazon Research Award	DARPA Lagrange; Korea Foundation for Advanced Studies; NSF-CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); Amazon Research Award	We thank Alexander Rakhlin for helpful discussion. All the authors acknowledge support from DARPA Lagrange. Chulhee Yun also thanks Korea Foundation for Advanced Studies for their support. Suvrit Sra also acknowledges support from an NSF-CAREER grant and an Amazon Research Award.	[Anonymous], ARXIV181103962; Arpit D., 2017, IMCL, P233; Bartlett P. L., 2019, ARXIV190611300; Bartlett PL, 2019, J MACH LEARN RES, V20, P1; Bartlett PL, 1999, ADV NEUR IN, V11, P190; Baum E. B., 1988, Journal of Complexity, V4, P193, DOI 10.1016/0885-064X(88)90020-9; Belkin M., 2018, ARXIV181211118; BELKIN M., 2018, DOES DATA INTERPOLAT; Bengio, 2011, ADV NEURAL INFORM PR, P666, DOI DOI 10.5555/2986459.2986534; Brutzkus A., 2018, 6 INT C LEARN REPR I; Brutzkus A, 2017, PR MACH LEARN RES, V70; COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; HaoChen J. Z., 2018, ARXIV180610077; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Huang GB, 2003, IEEE T NEURAL NETWOR, V14, P274, DOI 10.1109/TNN.2003.809401; Huang GB, 1998, IEEE T NEURAL NETWOR, V9, P224, DOI 10.1109/72.655045; Huang JW, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P598; HUANG SC, 1991, IEEE T NEURAL NETWOR, V2, P47, DOI 10.1109/72.80290; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kowalczyk A, 1997, NEURAL NETWORKS, V10, P1417, DOI 10.1016/S0893-6080(97)00009-9; Laurent T, 2018, PR MACH LEARN RES, V80; LIANG S, 2017, INT C LEARN REPR; Liang T, 2018, ARXIV180800387; Liang Tengyuan, 2019, ARXIV190810292; Lu Z., 2017, ADV NEURAL INFORM PR, P6231; Ma T, 2017, P 5 INT C LEARN REPR; Mei S., 2019, ARXIV190805355; Nguyen Quynh, 2017, ARXIV171010928; Nilsson N., 1965, LEARNING MACHINES; ROLNICK D, 2018, INT C LEARN REPR; SAFRAN I., 2017, ARXIV171208968; Safran I, 2017, PR MACH LEARN RES, V70; Shamir O, 2016, ADV NEUR IN, V29; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; Soltanolkotabi M., 2017, ARXIV PREPRINT ARXIV; Sontag ED, 1997, NEURAL COMPUT, V9, P337, DOI 10.1162/neco.1997.9.2.337; Soudry D., 2016, ARXIV PREPRINT ARXIV; Sra S., 2018, INT C LEARN REPR; Telgarsky M, 2015, ARXIV150908101V2CSLG; Telgarsky M.., 2016, C LEARNING THEORY, P1517; Tian Y, 2017, EUROPEAN FINANCIAL SYSTEMS 2017: PROCEEDINGS OF THE 14TH INTERNATIONAL SCIENTIFIC CONFERENCE, PT 2, P400; Wang G., 2018, ARXIV180804685; YAMASAKI M, 1993, ICANN 93, P544; Yarotsky D., 2018, PMLR, V75; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002; Yue YF, 2018, INT POW ELEC APPLICA, P1668; Yun C., INT C LEARN REPR; Zeng HQ, 2017, PROC INT CONF RECON; Zhang Xiao, 2018, ARXIV180607808; Zhong K, 2017, PR MACH LEARN RES, V70; Zhou Y., 2019, INT C LEARN REPR; Zhou Yi, 2018, INT C LEARN REPR; Zou D, 2018, ARXIV181108888	58	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907024
C	Yun, C; Sra, S; Jadbabaie, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yun, Chulhee; Sra, Suvrit; Jadbabaie, Ali			Are deep ResNets provably better than linear predictors?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent results in the literature indicate that a residual network (ResNet) composed of a single residual block outperforms linear predictors, in the sense that all local minima in its optimization landscape are at least as good as the best linear predictor. However, these results are limited to a single residual block (i.e., shallow ResNets), instead of the deep ResNets composed of multiple residual blocks. We take a step towards extending this result to deep ResNets. We start by two motivating examples. First, we show that there exist datasets for which all local minima of a fully-connected ReLU network are no better than the best linear predictor, whereas a ResNet has strictly better local minima. Second, we show that even at the global minimum, the representation obtained from the residual block outputs of a 2-block ResNet do not necessarily improve monotonically over subsequent blocks, which highlights a fundamental difficulty in analyzing deep ResNets. Our main theorem on deep ResNets shows under simple geometric conditions that, any critical point in the optimization landscape is either (i) at least as good as the best linear predictor; or (ii) the Hessian at this critical point has a strictly negative eigenvalue. Notably, our theorem shows that a chain of multiple skip-connections can improve the optimization landscape, whereas existing results study direct skip-connections to the last hidden layer or output layer. Finally, we complement our results by showing benign properties of the "near-identity regions" of deep ResNets, showing depth-independent upper bounds for the risk attained at critical points as well as the Rademacher complexity.	[Yun, Chulhee; Sra, Suvrit; Jadbabaie, Ali] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Yun, C (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	chulheey@mit.edu; suvrit@mit.edu; jadbabai@mit.edu			DARPA Lagrange; Korea Foundation for Advanced Studies; NSF-CAREER grant; Amazon Research Award	DARPA Lagrange; Korea Foundation for Advanced Studies; NSF-CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); Amazon Research Award	All the authors acknowledge support from DARPA Lagrange. Chulhee Yun also thanks Korea Foundation for Advanced Studies for their support. Suvrit Sra also acknowledges support from an NSF-CAREER grant and an Amazon Research Award.	[Anonymous], 2017, ARXIV171206541; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; BARTLETT P. L, 2018, ARXIV180405012; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kawaguchi K., 2018, ARXIV181009038; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Laurent T., 2017, ARXIV171210132; Laurent T, 2018, PR MACH LEARN RES, V80; Li H, 2018, ADV NEUR IN, V31; Liang S., 2018, ADV NEURAL INFORM PR, P4355; Liang S., 2018, INT C MACH LEARN, P2840; Lin H., 2018, ARXIV180610909; Ma T, 2017, P 5 INT C LEARN REPR; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Nguyen Q., 2018, INT C LEARN REPR; Nguyen Q, 2017, PR MACH LEARN RES, V70; SAFRAN I., 2017, ARXIV171208968; Shamir O., 2018, ARXIV180406739; Soudry D., 2016, ARXIV PREPRINT ARXIV; Sra S., 2018, INT C LEARN REPR; Swirszcz G., 2016, ARXIV PREPRINT ARXIV; Wu C., 2018, INT C LEARN REPR WOR; Xie B., 2016, ARXIV161103131; Yun C., INT C LEARN REPR; Zhang Huan, 2019, INT C LEARN REPR ICL; Zhou Yi, 2018, INT C LEARN REPR	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907035
C	Yurochkin, M; Agarwal, M; Ghosh, S; Greenewald, K; Hoang, TN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yurochkin, Mikhail; Agarwal, Mayank; Ghosh, Soumya; Greenewald, Kristjan; Trong Nghia Hoang			Statistical Model Aggregation via Parameter Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of aggregating models learned from sequestered, possibly heterogeneous datasets. Exploiting tools from Bayesian nonparametrics, we develop a general meta-modeling framework that learns shared global latent structures by identifying correspondences among local model parameterizations. Our proposed framework is model-independent and is applicable to a wide range of model types. After verifying our approach on simulated data, we demonstrate its utility in aggregating Gaussian topic models, hierarchical Dirichlet process based hidden Markov models, and sparse Gaussian processes with applications spanning text summarization, motion capture analysis, and temperature forecasting.(1)	[Yurochkin, Mikhail; Agarwal, Mayank; Ghosh, Soumya; Greenewald, Kristjan; Trong Nghia Hoang] IBM Res, Zurich, Switzerland; [Yurochkin, Mikhail; Agarwal, Mayank; Ghosh, Soumya; Greenewald, Kristjan; Trong Nghia Hoang] MIT IBM Watson AI Lab, Zurich, Switzerland; [Ghosh, Soumya] Ctr Computat Hlth, Zurich, Switzerland	International Business Machines (IBM)	Yurochkin, M (corresponding author), IBM Res, Zurich, Switzerland.; Yurochkin, M (corresponding author), MIT IBM Watson AI Lab, Zurich, Switzerland.	mikhail.yurochkin@ibm.com; mayank.agarwal@ibm.com; ghoshso@us.ibm.com; kristjan.h.greenewald@ibm.com; nghiaht@ibm.com						[Anonymous], 2019, P AAAI; [Anonymous], 2007, AISTATS; Bauer M., 2016, NEURIPS, P1533; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blomstedt P., 2019, ARXIV190404484; Campbell T., 2015, ADV NEURAL INFORM PR, P280; Campbell T, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P102; Das R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P795; Dean J., 2012, NIPS 12, V1, P1223; Deisenroth MP, 2015, PR MACH LEARN RES, V37, P1481; Dutta R., 2016, ARXIV160309272; Fox E. B., 2008, 25 INT C MACHINE LEA, P312; Fox EB, 2014, ANN APPL STAT, V8, P1281, DOI 10.1214/14-AOAS742; Gal Y., 2014, P NIPS; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Griffiths T.L., 2005, ADV NEURAL INFORM PR; Hasenclever L, 2017, J MACH LEARN RES, V18; Heikkila M. A., 2017, PROC ADV NEURAL INF, P3226; Hoang Q. M., 2019, P ICML, P2742; Hoang T. N., 2018, P AAAI; Hughes M. C., 2015, ADV NEURAL INFORM PR, P1198; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Newman D., 2010, HUMANLANGUAGETECHNOL, P100, DOI DOI 10.5555/1857999.1858011; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Teh Yee W, 2005, ADV NEURAL INFORM PR, P1385, DOI DOI 10.1198/016214506000000302; Thibaux Romain, 2007, INT C ART INT STAT, P564; Hoang TN, 2016, PR MACH LEARN RES, V48; Willsky A. S., 2009, ADV NEURAL INFORM PR, P549; Yurochkin M., 2019, ADV NEURAL INFORM PR, P5949; Yurochkin M, 2019, PR MACH LEARN RES, V97	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902057
C	Yurochkin, M; Fan, ZW; Guha, A; Koutris, P; Nguyen, X		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yurochkin, Mikhail; Fan, Zhiwei; Guha, Aritra; Koutris, Paraschos; Nguyen, XuanLong			Scalable inference of topic evolution via models for latent geometric structures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We develop new models and algorithms for learning the temporal dynamics of the topic polytopes and related geometric objects that arise in topic model based inference. Our model is nonparametric Bayesian and the corresponding inference algorithm is able to discover new topics as the time progresses. By exploiting the connection between the modeling of topic polytope evolution, Beta-Bernoulli process and the Hungarian matching algorithm, our method is shown to be several orders of magnitude faster than existing topic modeling approaches, as demonstrated by experiments working with several million documents in under two dozens of minutes.(1)	[Yurochkin, Mikhail] IBM Res, Yorktown Hts, NY 10598 USA; [Fan, Zhiwei; Koutris, Paraschos] Univ Wisconsin, Madison, WI 53706 USA; [Guha, Aritra; Nguyen, XuanLong] Univ Michigan, Ann Arbor, MI 48109 USA	International Business Machines (IBM); University of Wisconsin System; University of Wisconsin Madison; University of Michigan System; University of Michigan	Yurochkin, M (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	mikhail.yurochkin@ibm.com; zhiwei@cs.wisc.edu; aritra@umich.edu; paris@cs.wisc.edu; xuanlong@umich.edu			NSF CAREER [DMS-1351362]; NSF [CNS-1409303]; Margaret and Herman Sokol Faculty Award	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); Margaret and Herman Sokol Faculty Award	This research is supported in part by grants NSF CAREER DMS-1351362, NSF CNS-1409303, a research gift from Adobe Research and a Margaret and Herman Sokol Faculty Award to XN.	Ahmed A., 2012, ARXIV12033463; [Anonymous], 2009, DIRECTIONAL STAT; [Anonymous], 2007, AISTATS; Banerjee A, 2005, J MACH LEARN RES, V6, P1345; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blei DM, 2006, INT C MACH LEARN ICM, V148, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859]; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Bryant M, 2012, P ADV NEUR INF PROC, V25, P2699; Campbell T., 2015, ADV NEURAL INFORM PR, P280; Fang LX, 2014, J INEQUAL APPL, DOI 10.1186/1029-242X-2014-190; Griffiths T.L., 2005, ADV NEURAL INFORM PR; Griffiths TL, 2011, J MACH LEARN RES, V12, P1185; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Hong L, 2011, P 17 ACM SIGKDD INT, P832; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Newman D., 2008, ADV NEURAL INFORM PR, P1081; Nguyen XL, 2015, BERNOULLI, V21, P618, DOI 10.3150/13-BEJ582; Nguyen X, 2010, BAYESIAN ANAL, V5, P817, DOI 10.1214/10-BA529; Pritchard JK, 2000, GENETICS, V155, P945; Reisinger J., 2010, P 27 INT C MACH LEAR, P903; Teh Y. W., 2006, J AM STAT ASS, V101; Thibaux Romain, 2007, INT C ART INT STAT, P564; Wang L, 2011, ACTA POLYM SIN, P752, DOI 10.3724/SP.J.1105.2011.10220; Wang X., 2006, P 12 ACM SIGKDD INT, P424; Williamson S., 2010, ICML; Willsky A. S., 2009, ADV NEURAL INFORM PR, P549; Yurochkin M., 2017, ADV NEURAL INFORM PR, P3881; Yurochkin M., 2016, ADV NEURAL INFORM PR, P2505; Yurochkin M, 2019, PR MACH LEARN RES, V97; Yurochkin M, 2019, PR MACH LEARN RES, V97; Yurochkin Mikhail, 2019, ADV NEURAL INFORM PR, P10954	32	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306001
C	Zhai, SF; Talbott, W; Guestrin, C; Susskind, JM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhai, Shuangfei; Talbott, Walter; Guestrin, Carlos; Susskind, Joshua M.			Adversarial Fisher Vectors for Unsupervised Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We examine Generative Adversarial Networks (GANs) through the lens of deep Energy Based Models (EBMs), with the goal of exploiting the density model that follows from this formulation. In contrast to a traditional view where the discriminator learns a constant function when reaching convergence, here we show that it can provide useful information for downstream tasks, e.g., feature extraction for classification. To be concrete, in the EBM formulation, the discriminator learns an unnormalized density function (i.e., the negative energy term) that characterizes the data manifold. We propose to evaluate both the generator and the discriminator by deriving corresponding Fisher Score and Fisher Information from the EBM. We show that by assuming that the generated examples form an estimate of the learned density, both the Fisher Information and the normalized Fisher Vectors are easy to compute. We also show that we are able to derive a distance metric between examples and between sets of examples. We conduct experiments showing that the GAN-induced Fisher Vectors demonstrate competitive performance as unsupervised feature extractors for classification and perceptual similarity tasks.	[Zhai, Shuangfei; Talbott, Walter; Guestrin, Carlos; Susskind, Joshua M.] Apple Inc, Cupertino, CA 95014 USA	Apple Inc	Zhai, SF (corresponding author), Apple Inc, Cupertino, CA 95014 USA.	szhai@apple.com; wtalbott@apple.com; guestrin@apple.com; jsusskind@apple.com						Achille Alessandro, 2019, ICCV; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arjovsky M., 2017, ARXIV170107875; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Donahue J., 2016, ARXIV160509782; Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141; Du Y., 2019, ADV NEURAL INFORM PR, V6; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Hjelm R.D., 2018, P INT C LEARN REPR; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Kim T., 2016, ARXIV160603439; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krahenbuhl P., 2015, ARXIV151106856; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; landola Forrest N., 2016, ABS160207360 CORR; LeCun Yann, 2006, PREDICTING STRUCTURE, P2; LI CL, 2017, ADV NEURAL INFORM PR, P203, DOI DOI 10.1109/ICICTA.2017.52; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mescheder L, 2018, PR MACH LEARN RES, V80; Nijkamp Erik, 2019, ARXIV190312370; Noroozi Mehdi, 2016, ABS160309246 CORR, P2; Nowozin S, 2016, ADV NEUR IN, V29; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Roth Kevin, 2017, ABS170509367 CORR; Salimans T, 2016, ADV NEUR IN, V29; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wang D., 2016, ARXIV161101722; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zhai S., 2016, ARXIV161101799; Zhang LH, 2019, PROC CVPR IEEE, P2542, DOI 10.1109/CVPR.2019.00265; Zhang Richard, 2018, ABS180103924 CORR; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902075
C	Zhang, B; Sennrich, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Biao; Sennrich, Rico			Root Mean Square Layer Normalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorn. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%-64% on different models.	[Zhang, Biao; Sennrich, Rico] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland; [Sennrich, Rico] Univ Zurich, Inst Computat Linguist, Zurich, Switzerland	University of Edinburgh; University of Zurich	Zhang, B (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	B.Zhang@ed.ac.uk; sennrich@cl.uzh.ch		Sennrich, Rico/0000-0002-1438-4741	European Union [H2020-ICT-2018-2-825460]; Baidu Scholarship; EPSRC Tier-2 capital grant [EP/P020259/1]	European Union(European Commission); Baidu Scholarship; EPSRC Tier-2 capital grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank the reviewers for their insightful comments, and Antonio Valerio Miceli Barone for his support with weight normalization for MT. This project has received funding from the grant H2020-ICT-2018-2-825460 (ELITR) by the European Union. Biao Zhang also acknowledges the support of the Baidu Scholarship. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service (http://www.hpc.cam.ac.uk) funded by EPSRC Tier-2 capital grant EP/P020259/1.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2018, ARXIV180408771; Arpit D., 2016, ARXIV160301431; Ba L.J, 2016, P C WORKSH NEUR INF; Bahdanau D., 2015, P 3 INT C LEARNING R; Bjorck Nils, 2018, ADV NEURAL INFORM PR, P7694; Chen MX, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P76; Cho K., 2014, P 2014 C EMP METH NA, P1724; Cooijmans T., 2016, P 5 INT C LEARN REPR; Hermann KM, 2015, ADV NEUR IN, V28; Hoffer Elad, 2018, ARXIV180301814; Ioffe S., 2017, ADV NEURAL INFORM PR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2014, CORR; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Laurent C, 2016, INT CONF ACOUST SPEE, P2657, DOI 10.1109/ICASSP.2016.7472159; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Nguyen Toan Q, 2017, ARXIV17101329; Parmar Niki, 2018, ARXIV180205751; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Santurkar S, 2018, ADV NEUR IN, V31; Sennrich Rico, 2017, P SOFTW DEM 15 C EUR, DOI DOI 10.18653/V1/E17-3017; Sennrich Rico, 2015, ARXIV150807909; Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Theano Development Team, 2016, ARXIV E PRINTS; Ulyanov D., 2016, CORR; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vendrov I, 2015, ARXIV151106361; Wu Shuang, 2018, IEEE TRANSACTIONSON; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Zhang Biao, 2019, ARXIV190513324; Zhang H., 2019, ARXIV190104684; Zhou S, 2018, ARXIV180410752	37	0	0	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904007
C	Zhang, HR; Cheng, Y; Conitzer, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Hanrui; Cheng, Yu; Conitzer, Vincent			Distinguishing Distributions When Samples Are Strategically Transformed	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Often, a principal must make a decision based on data provided by an agent. Moreover, typically, that agent has an interest in the decision that is not perfectly aligned with that of the principal. Thus, the agent may have an incentive to select from or modify the samples he obtains before sending them to the principal. In other settings, the principal may not even be able to observe samples directly; instead, she must rely on signals that the agent is able to send based on the samples that he obtains, and he will choose these signals strategically. In this paper, we give necessary and sufficient conditions for when the principal can distinguish between agents of "good" and "bad" types, when the type affects the distribution of samples that the agent has access to. We also study the computational complexity of checking these conditions. Finally, we study how many samples are needed.	[Zhang, Hanrui; Cheng, Yu; Conitzer, Vincent] Duke Univ, Durham, NC 27708 USA	Duke University	Zhang, HR (corresponding author), Duke Univ, Durham, NC 27708 USA.	hrzhang@cs.duke.edu; yucheng@cs.duke.edu; conitzer@cs.duke.edu		Cheng, Yu/0000-0002-0019-2570	NSF [IIS-1814056, IIS-1527434]	NSF(National Science Foundation (NSF))	We are thankful for support from NSF under awards IIS-1814056 and IIS-1527434. We also thank anonymous reviewers for helpful comments.	Awasthi Pranjal, 2017, C LEARN THEOR, P127; Blum A., 2017, ADV NEURAL INFORM PR, V30, P2392; Chen YL, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P27, DOI 10.1145/3219166.3219195; Chen YL, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P9, DOI 10.1145/3219166.3219175; Dekel O, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P884; Diakonikolas Ilias, 2015, P TWENTYSIXTH ANN AC, P1841, DOI [10.1137/1.9781611973730.123, DOI 10.1137/1.9781611973730.123]; Dong JS, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P55, DOI 10.1145/3219166.3219193; GREEN JR, 1986, REV ECON STUD, V53, P447, DOI 10.2307/2297639; Hardt Moritz, 2016, INNOVATIONS THEORETI; Hu L, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P259, DOI 10.1145/3287560.3287597; Jabbari S., 2016, ADV NEURAL INFORM PR, P1570; Liang AN, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P45, DOI 10.1145/3219166.3219214; Meir R, 2012, ARTIF INTELL, V186, P123, DOI 10.1016/j.artint.2012.03.008; SPENCE M, 1973, Q J ECON, V87, P355, DOI 10.2307/1882010; Valiant G, 2017, SIAM J COMPUT, V46, P429, DOI 10.1137/151002526; Yu L, 2011, AUTON AGENT MULTI-AG, V22, P217, DOI 10.1007/s10458-010-9151-4; Zhang Hanrui, 2019, 36 INT C MACH LEARN	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303021
C	Zhang, JY; Khanna, R; Kyrillidis, A; Koyejo, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Jacky Y.; Khanna, Rajiv; Kyrillidis, Anastasios; Koyejo, Oluwasanmi			Learning Sparse Distributions using Iterative Hard Thresholding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIABLE SELECTION	Iterative hard thresholding (IHT) is a projected gradient descent algorithm, known to achieve state of the art performance for a wide range of structured estimation problems, such as sparse inference. In this work, we consider IHT as a solution to the problem of learning sparse discrete distributions. We study the hardness of using IHT on the space of measures. As a practical alternative, we propose a greedy approximate projection which simultaneously captures appropriate notions of sparsity in distributions, while satisfying the simplex constraint, and investigate the convergence behavior of the resulting procedure in various settings. Our results show, both in theory and practice, that IHT can achieve state of the art results for learning sparse distributions.	[Zhang, Jacky Y.; Koyejo, Oluwasanmi] Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA; [Khanna, Rajiv] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; [Kyrillidis, Anastasios] Rice Univ, Dept Comp Sci, Houston, TX 77251 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of California System; University of California Berkeley; Rice University	Zhang, JY (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.	yiboz@illinois.edu; rajivak@berkeley.edu; rajivak@berkeley.edu; sanmi@illinois.edu	Khanna, Rajiv/GPK-2566-2022	Khanna, Rajiv/0000-0003-1314-3126				Akkaram S, 2010, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, DETC 2010, VOL 3, A AND B, P37; Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495; Blumensath Thomas, 2008, CORR; Carvalho CM, 2010, BIOMETRIKA, V97, P465, DOI 10.1093/biomet/asq017; Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006; Dalalyan A., 2019, STOCHASTIC PROCESSES; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Duchi J., 2008, PROC 25 INT C MACH L, P272; Engel Eberhard, DENSITY FUNCTIONAL T; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Huang JZ, 2011, J MACH LEARN RES, V12, P3371; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Ishwaran H, 2005, ANN STAT, V33, P730, DOI 10.1214/009053604000001147; Kar P., 2014, ADV NEURAL INFORM PR, P685; Khanna R, 2017, PR MACH LEARN RES, V54, P1358; Khanna R, 2015, JMLR WORKSH CONF PRO, V38, P453; Khanna Rajiv, 2018, AISTATS; Kim B., 2016, ADV NEURAL INFORM PR, P2280; Kleinberg J., 2006, ALGORITHM DESIGN; Koyejo O, 2014, ADV NEUR IN, V27; Kyrillidis A., 2011, 2011 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), P353, DOI 10.1109/CAMSAP.2011.6136024; Kyrillidis A., 2013, INT C MACH LEARN ICM, V28, P235; Kyrillidis A, 2014, J MATH IMAGING VIS, V48, P235, DOI 10.1007/s10851-013-0434-7; Locatello Francesco, 2018, P 21 INT C ART INT S, V84, P464; Locatello Francesco, 2018, ADV NEURAL INFORM PR, V31, P3401; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Park T, 2008, J AM STAT ASSOC, V103, P681, DOI 10.1198/016214508000000337; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; Tibshirani R., 2015, STAT LEARNING SPARSI; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Wibisono A., 2018, PROC C LEARNING THEO, Vvol 31, P2093	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306073
C	Zhang, RY; Yu, T; Shen, YL; Jin, HX; Chen, CY; Carin, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Ruiyi; Yu, Tong; Shen, Yilin; Jin, Hongxia; Chen, Changyou; Carin, Lawrence			Reward Constrained Interactive Recommendation with Natural Language Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMAGE RETRIEVAL	Text-based interactive recommendation provides richer user feedback and has demonstrated advantages over traditional interactive recommender systems. However, recommendations can easily violate preferences of users from their past natural-language feedback, since the recommender needs to explore new items for further improvement. To alleviate this issue, we propose a novel constraintaugmented reinforcement learning (RL) framework to efficiently incorporate user preferences over time. Specifically, we leverage a discriminator to detect recommendations violating user historical preference, which is incorporated into the standard RL objective of maximizing expected cumulative future rewards. Our proposed framework is general and is further extended to the task of constrained text generation. Empirical results show that the proposed method yields consistent improvement relative to standard RL methods.	[Zhang, Ruiyi; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA; [Yu, Tong; Shen, Yilin; Jin, Hongxia] Samsung Res Amer, Mountain View, CA USA; [Chen, Changyou] Univ Buffalo, Buffalo, NY USA	Duke University; Samsung; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Zhang, RY (corresponding author), Duke Univ, Durham, NC 27706 USA.		Zhang, Ruiyi/AAB-8402-2021					Achiam J, 2017, PR MACH LEARN RES, V70; Aliannejadi M, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P475, DOI 10.1145/3331184.3331265; Altman E., 1999, STOCH MODEL SER; [Anonymous], 2017, ABS170207983 CORR; Bertsekas Dimitri P., 1997, J OPERATIONAL RES SO; Borkar Vivek S, 2005, SYSTEMS CONTROL LETT; Chow Y., 2015, NIPS; Christakopoulou K, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P139, DOI 10.1145/3219819.3219894; Christakopoulou K, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P815, DOI 10.1145/2939672.2939746; Dalal G., 2018, ARXIV180108757; Di Castro Dotan, 2012, ARXIV12066404; Fedus William, 2018, INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grauman K., 2014, CVPR; Grauman K., 2014, ICCV; Greco C, 2017, LECT NOTES ARTIF INT, V10640, P372, DOI 10.1007/978-3-319-70169-1_28; Guo Jiaxian, 2017, ARXIV PREPRINT ARXIV; Guo XX, 2018, ADV NEUR IN, V31; Guo Xiaoxiao, 2019, ARXIV190512794; He K., 2019, P IEEE C COMP VIS PA, P770, DOI DOI 10.1109/CVPR.2016.90; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hesse C., 2017, OPENAI BASELINES; Hill A., 2018, STABLE BASELINES; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu ZT, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Koutsopoulos Iordanis, 2011, ICECN; Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026; Kusner Matt J, 2016, ARXIV161104051; Kveton B, 2015, PR MACH LEARN RES, V37, P767; Lei Wenqiang, WSDM, V20; Levine Sergey, 2013, ICML; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Lin K., 2017, ARXIV PREPRINT ARXIV; Mirzadeh N, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON E-TECHNOLOGY, E-COMMERCE AND E-SERVICE, PROCEEDINGS, P772, DOI 10.1109/EEE.2005.75; Nie W., 2018, ICLR; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; PENG XB, 2018, ACM T GRAPHICS TOG; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510; Shen TX, 2017, ADV NEUR IN, V30; Shixiang Gu, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3389, DOI 10.1109/ICRA.2017.7989385; Sun YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P235, DOI 10.1145/3209978.3210002; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tessler Chen, 2019, ICLR; Thomason Jesse, 2019, C ROB LEARN CORL; Thomee B, 2012, INT J MULTIMED INF R, V1, P71, DOI 10.1007/s13735-012-0014-4; Wang Wenlin, 2019, ARXIV190307137; Wu H, 2004, INT C PATT RECOG, P1009; Yang Zichao, 2018, ADV NEURAL INFORM PR; Yu A, 2017, ADV COMPUT VIS PATT, P119, DOI 10.1007/978-3-319-50077-5_6; Yu LQ, 2017, AAAI CONF ARTIF INTE, P66; Yu Tong, 2019, ACM MULTIMEDIA; Zhang Ruiyi, 2019, RSDM ICML; Zhang YZ, 2017, PR MACH LEARN RES, V70; Zhu Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3602	56	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906083
C	Zhang, RQ; De Sa, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Ruqi; De Sa, Christopher			Poisson-Minibatching for Gibbs Sampling with Convergence Rate Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Gibbs sampling is a Markov chain Monte Carlo method that is often used for learning and inference on graphical models. Minibatching, in which a small random subset of the graph is used at each iteration, can help make Gibbs sampling scale to large graphical models by reducing its computational cost. In this paper, we propose a new auxiliary-variable minibatched Gibbs sampling method, Poisson-minibatching Gibbs, which both produces unbiased samples and has a theoretical guarantee on its convergence rate. In comparison to previous minibatched Gibbs algorithms, Poisson-minibatching Gibbs supports fast sampling from continuous state spaces and avoids the need for a Metropolis-Hastings correction on discrete state spaces. We demonstrate the effectiveness of our method on multiple applications and in comparison with both plain Gibbs and previous minibatched methods.	[Zhang, Ruqi; De Sa, Christopher] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Zhang, RQ (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	rz297@cornell.edu; cdesa@cs.cornell.edu		Zhang, Ruqi/0000-0002-4340-0528				Aida S, 1998, J FUNCT ANAL, V158, P152, DOI 10.1006/jfan.1998.3286; BRUCE AD, 1985, J PHYS A-MATH GEN, V18, pL873, DOI 10.1088/0305-4470/18/14/009; De Sa C., 2018, ARXIV180606086; Dommers S, 2017, STOCH PROC APPL, V127, P3719, DOI 10.1016/j.spa.2017.03.009; Fukushima Masatoshi, 2010, DIRICHLETFORMSAND SY, V19; Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577; Koller Daphne, 2009, PROBABILISTICGRAPHIC; Li W. H., 2017, ARXIV170709705; Maclaurin D, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P543; Michel M, 2015, EPL-EUROPHYS LETT, V112, DOI 10.1209/0295-5075/112/20003; Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480; Olver S., 2013, ARXIV13071223; Peres Yuval, 2017, MARKOV CHAINS MIXING, V107; POTTS RB, 1952, P CAMB PHILOS SOC, V48, P106, DOI 10.1017/S0305004100027419; Quiroz M., 2016, ARXIV160308232; Rudolf D., 2011, ARXIV11083201; Trefethen Lloyd N, 2013, APPROXIMATION THEORY, V128; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	18	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304088
C	Zhang, ST; Whiteson, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Shangtong; Whiteson, Shimon			DAC: The Double Actor-Critic Architecture for Learning Options	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We reformulate the option framework as two parallel augmented MDPs. Under this novel formulation, all policy optimization algorithms can be used off the shelf to learn intra-option policies, option termination conditions, and a master policy over options. We apply an actor-critic algorithm on each augmented MDP, yielding the Double Actor-Critic (DAC) architecture. Furthermore, we show that, when state-value functions are used as critics, one critic can be expressed in terms of the other, and hence only one critic is necessary. We conduct an empirical study on challenging robot simulation tasks. In a transfer learning setting, DAC outperforms both its hierarchy-free counterpart and previous gradient-based option learning algorithms.	[Zhang, Shangtong; Whiteson, Shimon] Univ Oxford, Dept Comp Sci, Oxford, England	University of Oxford	Zhang, ST (corresponding author), Univ Oxford, Dept Comp Sci, Oxford, England.	shangtong.zhang@cs.ox.ac.uk; shimon.whiteson@cs.ox.ac.uk			Engineering and Physical Sciences Research Council (EPSRC); European Research Council under the European Union [637713]; NVIDIA	Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council under the European Union(European Research Council (ERC)); NVIDIA	SZ is generously funded by the Engineering and Physical Sciences Research Council (EPSRC). This project has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713). The experiments were made possible by a generous equipment grant from NVIDIA. The authors thank Matthew Smith and Gregory Farquhar for insightful discussions. The authors also thank anonymous reviewers for their valuable feedbacks.	Bacon P.-L., 2017, P 31 AAAI C ART INT; Bacon P. - L., 2018, THESIS; Brockman G., 2016, OPENAI GYM; Ciosek K., 2017, ARXIV170605374; Clemente A. V., 2017, MULTIDISCIPLINARY C; Daniel C., 2016, MACHINE LEARNING; Dayan P., 1993, ADV NEURAL INFORM PR; Dietterich T. G., 2000, J ARTIFICIAL INTELLI; Florensa Carlos, 2017, ARXIV170403012; Haarnoja T., 2018, P 35 INT C MACH LEAR; Hafner D., 2018, ARXIV181104551; Harb J., 2018, P 32 AAAI C ART INT; Klissarov M., 2017, P L; Levy A, 2017, ARXIV PREPRINT ARXIV; Levy K. Y., 2011, P 2011 EUR WORKSH RE; Machado M. C, 2017, ARXIV171011089; Machado MC., 2017, ICML, V5, P3567; McGovern A., 2001, P 18 INT C MACH LEAR; Nachum O., 2018, ADV NEURAL INFORM PR; Niekum S., 2011, ADV NEURAL INFORM PR; Peters J., 2008, NEUROCOMPUTING; Precup D., 2018, TEMPORAL ABSTRACTION; Puterman M.L., 2014, MARKOV DECISION PROC; Riemer M., 2018, ADV NEURAL INFORM PR; Schmidhuber J., 1993, ADAPTIVE BEHAV; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, P 32 INT C MACH LEAR; Silver David, 2012, ARXIV12066473; Smith M., 2018, P 35 INT C MACH LEAR; Stolle M., 2002, INT S ABSTR REF APPR; Sutton R. S., 1999, ARTIFICIAL INTELLIGE; Sutton R. S., 2000, ADV NEURAL INFORM PR; Tassa Y., 2018, ARXIV180100690; Vezhnevets A. S., 2017, P 34 INT C MACH LEAR; Zhang S., 2019, P 33 AAAI C ART INT; Zhang S., 2019, ADV NEURAL INFORM PR	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302005
C	Zhang, WH; Wu, S; Doiron, B; Lee, TS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Wen-Hao; Wu, Si; Doiron, Brent; Lee, Tai Sing			A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CUE INTEGRATION; INFORMATION; PERCEPTION; MOTION	This study provides a normative theory for how Bayesian causal inference can be implemented in neural circuits. In both cognitive processes such as causal reasoning and perceptual inference such as cue integration, the nervous systems need to choose different models representing the underlying causal structures when making inferences on external stimuli. In multisensory processing, for example, the nervous system has to choose whether to integrate or segregate inputs from different sensory modalities to infer the sensory stimuli, based on whether the inputs are from the same or different sources. Making this choice is a model selection problem requiring the computation of Bayes factor, the ratio of likelihoods between the integration and the segregation models. In this paper, we consider the causal inference in multisensory processing and propose a novel generative model based on neural population code that takes into account both stimulus feature and stimulus reliability in the inference. In the case of circular variables such as heading direction, our normative theory yields an analytical solution for computing the Bayes factor, with a clear geometric interpretation, which can be implemented by simple additive mechanisms with neural population code. Numerical simulation shows that the tunings of the neurons computing Bayes factor are consistent with the "opposite neurons" discovered in dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas for visual-vestibular processing. This study illuminates a potential neural mechanism for causal inference in the brain.	[Zhang, Wen-Hao; Lee, Tai Sing] Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA; [Zhang, Wen-Hao; Doiron, Brent] Univ Pittsburgh, Dept Math, Pittsburgh, PA 15260 USA; [Wu, Si] Peking Univ, Sch Elect Engn & Comp Sci, Peking Tsinghua Ctr Life Sci, Inst Brain Res,IDG McGovern, Beijing, Peoples R China	Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Peking University	Zhang, WH (corresponding author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.; Zhang, WH (corresponding author), Univ Pittsburgh, Dept Math, Pittsburgh, PA 15260 USA.	wenhao.zhang@pitt.edu; siwu@pku.edu.cn; bdoiron@pitt.edu; tai@cnbc.cmu.edu		Zhang, Wen-Hao/0000-0001-7641-5024	National Science Foundation [1816568]; Intelligence Advanced Research Projects Activity [D16PC00007]; National Institutes of Health [1U19NS107613-01, R01EB026953]; Vannevar Bush Faculty [N00014-18-1-2002]; Simons Foundation Collaboration on the Global Brain	National Science Foundation(National Science Foundation (NSF)); Intelligence Advanced Research Projects Activity; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Vannevar Bush Faculty; Simons Foundation Collaboration on the Global Brain	This work is supported by National Science Foundation (1816568), Intelligence Advanced Research Projects Activity (D16PC00007), the National Institutes of Health (Grants 1U19NS107613-01 and R01EB026953), the Vannevar Bush Faculty (Fellowship N00014-18-1-2002), and the Simons Foundation Collaboration on the Global Brain. We also thank Rob Kass and Xaq Pitkow for their useful suggestions.	Alais D, 2004, CURR BIOL, V14, P257, DOI 10.1016/j.cub.2004.01.029; Bertin RJV, 2004, EXP BRAIN RES, V154, P11, DOI 10.1007/s00221-003-1524-3; Bishop C.M, 2006, PATTERN RECOGN; Chen AH, 2013, J NEUROSCI, V33, P3567, DOI 10.1523/JNEUROSCI.4522-12.2013; Chen AH, 2011, J NEUROSCI, V31, P12036, DOI 10.1523/JNEUROSCI.0395-11.2011; Clark J.J, 2013, DATA FUSION SENSORY, V105; Dayan P., 2001, THEORETICAL NEUROSCI, P806; Dokka K, 2019, P NATL ACAD SCI USA, V116, P9060, DOI 10.1073/pnas.1820373116; Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a; Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002; Fetsch CR, 2013, NAT REV NEUROSCI, V14, P429, DOI 10.1038/nrn3503; GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885; Gu Y, 2006, J NEUROSCI, V26, P73, DOI 10.1523/JNEUROSCI.2356-05.2006; Gu Y, 2008, NAT NEUROSCI, V11, P1201, DOI 10.1038/nn.2191; Jacobs RA, 1999, VISION RES, V39, P3621, DOI 10.1016/S0042-6989(99)00088-7; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Kording KP, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000943; Kording KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169; Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790; Ma WJ, 2013, MULTISENS RES, V26, P159, DOI 10.1163/22134808-00002407; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057; Sato Y, 2007, NEURAL COMPUT, V19, P3335, DOI 10.1162/neco.2007.19.12.3335; Shams L, 2010, TRENDS COGN SCI, V14, P425, DOI 10.1016/j.tics.2010.07.001; Wallace MT, 2004, EXP BRAIN RES, V158, P252, DOI 10.1007/s00221-004-1899-9; Wang C, 2016, PROCEEDINGS OF THE 2016 12TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P1180, DOI 10.1109/WCICA.2016.7578430; Wozny DR, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000871; Zhang WH, 2019, ELIFE, V8, DOI 10.7554/eLife.43753; Zhang WH, 2016, J NEUROSCI, V36, P532, DOI 10.1523/JNEUROSCI.0578-15.2016	29	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303075
C	Zhang, XR; Khalili, MM; Tekin, C; Liu, MY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Xueru; Khalili, Mohammad Mahdi; Tekin, Cem; Liu, Mingyan			Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Machine Learning (ML) models trained on data from multiple demographic groups can inherit representation disparity [7] that may exist in the data: the model may be less favorable to groups contributing less to the training process; this in turn can degrade population retention in these groups over time, and exacerbate representation disparity in the long run. In this study, we seek to understand the interplay between ML decisions and the underlying group representation, how they evolve in a sequential framework, and how the use of fairness criteria plays a role in this process. We show that the representation disparity can easily worsen over time under a natural user dynamics (arrival and departure) model when decisions are made based on a commonly used objective and fairness criteria, resulting in some groups diminishing entirely from the sample pool in the long run. It highlights the fact that fairness criteria have to be defined while taking into consideration the impact of decisions on user dynamics. Toward this end, we explain how a proper fairness criterion can be selected based on a general user dynamics model.	[Zhang, Xueru; Khalili, Mohammad Mahdi; Liu, Mingyan] Univ Michigan, Ann Arbor, MI 48109 USA; [Tekin, Cem] Bilkent Univ, Ankara, Turkey	University of Michigan System; University of Michigan; Ihsan Dogramaci Bilkent University	Zhang, XR (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	xueru@umich.edu; khalili@umich.edu; cemtekin@ee.bilkent.edu.tr; mingyan@umich.edu		Liu, Mingyan/0000-0003-3295-9200	NSF [CNS-1616575, CNS-1646019, CNS-1739517]; BAGEP 2019 Award of the Science Academy	NSF(National Science Foundation (NSF)); BAGEP 2019 Award of the Science Academy	This work is supported by the NSF under grants CNS-1616575, CNS-1646019, CNS-1739517. The work of Cem Tekin was supported by BAGEP 2019 Award of the Science Academy.	[Anonymous], 2014, ADV NEURAL INFORM PR; [Anonymous], EMOTION READING TECH; Barocas S., 2019, FAIRNESS MACHINE LEA; Blum Avrim, 2018, ADV NEURAL INFORM PR, P8386; Dimitrakakis C, 2019, AAAI CONF ARTIF INTE, P509; Dua D., 2017, UCI MACHINE LEARNING; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Harwell Drew, 2018, AMAZONS ALEXA GOOGLE; Hashimoto Tatsunori, 2018, P 35 INT C MACH LEAR, P2; Heidari H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2248; Heidari Hoda, 2019, P 39 INT C MACH LEAR, P2692; Hu L, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1389, DOI 10.1145/3178876.3186044; Jabbari S, 2017, PR MACH LEARN RES, V70; Kannan S, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P240, DOI 10.1145/3287560.3287578; Liu L. T., 2018, P MACH LEARN RES, P3150; Tiwari Abhishek, 2017, KBIAS FAIRNESS MACHI; Valera I., 2018, NEURAL INF PROCESS S, P1769	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906088
C	Zhao, H; Tsai, YHH; Salakhutdinov, R; Gordon, GJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Han; Tsai, Yao-Hung Hubert; Salakhutdinov, Ruslan; Gordon, Geoffrey J.			Learning Neural Networks with Adaptive Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BACKPROPAGATION; RIDGE	Feed-forward neural networks can be understood as a combination of an intermediate representation and a linear hypothesis. While most previous works aim to diversify the representations, we explore the complementary direction by performing an adaptive and data-dependent regularization motivated by the empirical Bayes method. Specifically, we propose to construct a matrix-variate normal prior (on weights) whose covariance matrix has a Kronecker product structure. This structure is designed to capture the correlations in neurons through backpropagation. Under the assumption of this Kronecker factorization, the prior encourages neurons to borrow statistical strength from one another. Hence, it leads to an adaptive and data-dependent regularization when training networks on small datasets. To optimize the model, we present an efficient block coordinate descent algorithm with analytical solutions. Empirically, we demonstrate that the proposed method helps networks converge to local optima with smaller stable ranks and spectral norms. These properties suggest better generalizations and we present empirical results to support this expectation. We also verify the effectiveness of the approach on multiclass classification and multitask regression problems with various network structures. Our code is publicly available at: https://github.com/yaohungt/Adaptive-Regularization-Neural-Network.	[Zhao, Han; Tsai, Yao-Hung Hubert; Salakhutdinov, Ruslan; Gordon, Geoffrey J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Gordon, Geoffrey J.] Microsoft Res Montreal, Montreal, PQ, Canada	Carnegie Mellon University	Zhao, H (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	han.zhao@cs.cmu.edu; yaohungt@cs.cmu.edu; rsalakhu@cs.cmu.edu; geoff.gordon@microsoft.com			DARPA XAI project [FA87501720152]; Nvidia GPU grant; DARPA [FA875018C0150]; DARPA SAGAMORE [HR00111990016]; Office of Naval Research [N000141812861]; AFRL CogDeCON; Apple; NVIDIA's GPU support	DARPA XAI project; Nvidia GPU grant; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA SAGAMORE; Office of Naval Research(Office of Naval Research); AFRL CogDeCON; Apple; NVIDIA's GPU support	HZ and GG would like to acknowledge support from the DARPA XAI project, contract #FA87501720152 and an Nvidia GPU grant. YT and RS were supported in part by DARPA grant FA875018C0150, DARPA SAGAMORE HR00111990016, Office of Naval Research grant N000141812861, AFRL CogDeCON, and Apple. YT and RS would also like to acknowledge NVIDIA's GPU support. Last, we thank Denny Wu for suggestions on exploring and analyzing our algorithm in terms of stable rank.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bernardo Jose M, 2001, BAYESIAN THEORY; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; BROWN PJ, 1980, ANN STAT, V8, P64, DOI 10.1214/aos/1176344891; Caruana R, 2001, ADV NEUR IN, V13, P402; Cogswell Michael, 2015, ARXIV PREPRINT ARXIV; Duchi J, 2011, J MACH LEARN RES, V12, P2121; EFRON B, 1977, SCI AM, V236, P119, DOI 10.1038/scientificamerican0577-119; EFRON B, 1973, J AM STAT ASSOC, V68, P117, DOI 10.2307/2284155; Efron B., 2012, LARGE SCALE INFERENC, V1; Efron B, 2016, COMPUTER AGE STAT IN; Gelman A., 2014, BAYESIAN DATA ANAL, DOI [DOI 10.1201/B16018, 10.1201/b16018]; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Goyal Priya, 2017, ARXIV170602677; Grant Erin, 2018, INT C LEARN REPR; Gupta AK., 2018, MATRIX VARIATE DISTR; Gupta V., 2017, ARXIV170606569; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; James W., 1961, P 4 BERKELEY S MATH, V1, P361, DOI DOI 10.1007/978-1-4612-0919-5; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Krizhevsky A, 2009, LEARNING MULTIPLE LA; KROGH A, 1992, ADV NEUR IN, V4, P950; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Long MS, 2017, ADV NEUR IN, V30; Louizos C, 2016, PR MACH LEARN RES, V48; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Mehrotra S, 1992, SIAM J OPTIMIZ, V2, P575, DOI 10.1137/0802028; Nair V, 2010, P 27 INT C MACHINE L, P807; Neyshabur Behnam, 2017, ARXIV170709564; OMAN SD, 1984, J ROY STAT SOC B MET, V46, P544; Robbins H., 1956, TECHNICAL REPORT; Santurkar S, 2018, ADV NEUR IN, V31; Schaal S, 2000, P 17 INT C MACH LEAR, P1079; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sun SY, 2017, PR MACH LEARN RES, V54, P1283; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Zhang G., 2017, ARXIV171202390; Zhao Han, 2019, P 35 C UNC ART INT	43	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903007
C	Zhao, ZB; Xia, LR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Zhibing; Xia, Lirong			Learning Mixtures of Plackett-Luce Models from Structured Partial Orders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MATRIX COMPLETION	Mixtures of ranking models have been widely used for heterogeneous preferences. However, learning a mixture model is highly nontrivial, especially when the dataset consists of partial orders. In such cases, the parameter of the model may not be even identifiable. In this paper, we focus on three popular structures of partial orders: ranked top l(l), l(2) -way, and choice data over a subset of alternatives. We prove that when the dataset consists of combinations of ranked top-l(1) and l(2)-way (or choice data over up to l(2) alternatives), mixture of k Plackett-Luce models is not identifiable when l(1)+l(2) <= 2k - 1 (l(2) is set to 1 when there are no l(2)-way orders). We also prove that under some combinations, including ranked top-3, ranked top-2 plus 2 -way, and choice data over up to 4 alternatives, mixtures of two Plackett-Luce models are identifiable. Guided by our theoretical results, we propose efficient generalized method of moments (GMM) algorithms to learn mixtures of two Plackett-Luce models, which are proven consistent. Our experiments demonstrate the efficacy of our algorithms. Moreover, we show that when full rankings are available, learning from different marginal events (partial orders) provides tradeoffs between statistical efficiency and computational efficiency.	[Zhao, Zhibing; Xia, Lirong] Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Zhao, ZB (corresponding author), Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.	zhaoz6@rpi.edu; xial@cs.rpi.edu	Zhao, Zhibing/AAW-7979-2021		NSF [1453542]; ONR [N00014-17-1-2621]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	We thank all anonymous reviewers for helpful comments and suggestions. This work is supported by NSF #1453542 and ONR #N00014-17-1-2621.	Altman Alon, 2005, P ACM C EL COMM EC V; Ammar A., 2014, P ACM SIGMETRICS INT; [Anonymous], P 33 AAAI C ART INT; Baltrunas L., 2010, P 4 ACM C RECOMMENDE, P119, DOI DOI 10.1145/1864708.1864733; Brandt F, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1915; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen XF, 2013, KEY ENG MATER, V538, P193, DOI 10.4028/www.scientific.net/KEM.538.193; Chierichetti Flavio, 2018, P 35 INT C MACH LEAR; Gormley IC, 2008, J AM STAT ASSOC, V103, P1014, DOI 10.1198/016214507000001049; Gormley IC, 2009, BAYESIAN ANAL, V4, P265, DOI 10.1214/09-BA410; Hunter DR, 2004, ANN STAT, V32, P384; Jamieson Kevin G, 2011, ADV NEURAL INFORM PR, P2240; Jang M., 2016, ARXIV160304153; Khetan A, 2016, J MACH LEARN RES, V17; Lirong Xia, 2019, SYNTHESIS LECT ARTIF; Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3; Lu T, 2014, J MACH LEARN RES, V15, P3783; Luce R, 1959, INDIVIDUAL CHOICE BE; Mao A., 2013, P NAT C ART INT AAAI; Marden JI, 1995, ANAL MODELING RANK D; Maystre Lucas, 2015, ADV NEURAL INFORM PR, P172; Mollica C, 2017, PSYCHOMETRIKA, V82, P442, DOI 10.1007/s11336-016-9530-0; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Newey W.K., 1994, HDB ECONOMETRICS, VIV, DOI DOI 10.1016/S1573-4412(05)80005-4; Oh S., 2014, ADV NEURAL INFORM PR, P595; Pini MS, 2011, ARTIF INTELL, V175, P1272, DOI 10.1016/j.artint.2010.11.009; PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Tkachenko M, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P237, DOI 10.1145/2983323.2983763; Train KE, 2009, DISCRETE CHOICE METHODS WITH SIMULATION, 2ND EDITION, P1, DOI 10.1017/CBO9780511805271; Yu N, 2011, 2011 4TH IEEE INTERNATIONAL CONFERENCE ON BROADBAND NETWORK AND MULTIMEDIA TECHNOLOGY (4TH IEEE IC-BNMT2011), P355, DOI 10.1109/ICBNMT.2011.6155956; Zhao Z., 2016, P 33 INT C MACH LEAR; Zhao Z., 2018, P 32 AAAI C ART INT; Zhao Zhibing, 2018, P 34 C UNC ART INT U	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901074
C	Zheng, K; Luo, HP; Diakonikolas, I; Wang, LW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zheng, Kai; Luo, Haipeng; Diakonikolas, Ilias; Wang, Liwei			Equipping Experts/Bandits with Long-term Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BOUNDS	We propose the first reduction-based approach to obtaining long-term memory guarantees for online learning in the sense of Bousquet and Warmuth [8], by reducing the problem to achieving typical switching regret. Specifically, for the classical expert problem with K actions and T rounds, using our framework we develop various algorithms with a regret bound of order O(root T(SlnT + nlnK)) compared to any sequence of experts with S - 1 switches among n <= min{S,K} distinct experts. In addition, by plugging specific adaptive algorithms into our framework we also achieve the best of both stochastic and adversarial environments simultaneously. This resolves an open problem of Warmuth and Koolen [35]. Furthermore, we extend our results to the sparse multi-armed bandit setting and show both negative and positive results for long-term memory guarantees. As a side result, our lower bound also implies that sparse losses do not help improve the worst-case regret for contextual bandits, a sharp contrast with the non-contextual case.	[Zheng, Kai; Wang, Liwei] Peking Univ, Key Lab Machine Percept, MOE, Sch EECS, Beijing, Peoples R China; [Zheng, Kai; Wang, Liwei] Peking Univ, Ctr Data Sci, Beijing, Peoples R China; [Luo, Haipeng] Univ Southern Calif, Los Angeles, CA 90007 USA; [Diakonikolas, Ilias] Univ Wisconsin, Madison, WI USA	Peking University; Peking University; University of Southern California; University of Wisconsin System; University of Wisconsin Madison	Zheng, K (corresponding author), Peking Univ, Key Lab Machine Percept, MOE, Sch EECS, Beijing, Peoples R China.; Zheng, K (corresponding author), Peking Univ, Ctr Data Sci, Beijing, Peoples R China.	zhengk92@pku.edu.cn; haipengl@usc.edu; ilias.diakonikolas@gmail.com; wanglw@cis.pku.edu.cn			Natioanl Key R&D Program of China [2018YFB1402600]; BJNSF [L172037]; NSF [IIS-1755781, CCF-1652862]; Sloan Research Fellowship	Natioanl Key R&D Program of China; BJNSF; NSF(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	The authors would like to thank Alekh Agarwal, Sebastien Bubeck, Dylan Foster, Wouter Koolen, Manfred Warmuth, and Chen-Yu Wei for helpful discussions. Kai Zheng and Liwei Wang were supported by Natioanl Key R&D Program of China (no. 2018YFB1402600), BJNSF (L172037). Haipeng Luo was supported by NSF Grant IIS-1755781. Ilias Diakonikolas was supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship.	Adamskiy Dmitry, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P290, DOI 10.1007/978-3-642-34106-9_24; Adamskiy D., 2012, P 25 INT C NEUR INF, P135; Agarwal A., 2017, C LEARN THEOR; [Anonymous], 2016, FDN TRENDS IN OPTIMI; [Anonymous], 2012, ADV NEURAL INFORM PR; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2016, C LEARN THEOR; Blum A, 2007, J MACH LEARN RES, V8, P1307; Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654; Bubeck S., 2012, C LEARN THEOR, P42; Bubeck S., 2019, C LEARN THEOR; Bubeck S., 2018, P 29 INT C ALG LEARN, P111; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Christiano P., 2017, THESIS; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; Foster D. J., 2016, ADV NEURAL INFORM PR, P4734; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Freund Yoav, 1997, P 29 ANN ACM S THEOR; Gaillard P., 2014, JMLR WORKSHOP C P, P176; Nguyen HT, 2012, 2012 12TH INTERNATIONAL CONFERENCE ON HYBRID INTELLIGENT SYSTEMS (HIS), P271, DOI 10.1109/HIS.2012.6421346; Hazan E., 2007, ELECT C COMPUTATIONA, V14; Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Jun KS, 2017, ELECTRON J STAT, V11, P5282, DOI 10.1214/17-EJS1379SI; Koolen Wouter M, 2015, COLT, P1155; Langford J., 2008, ADV NEURAL INFORM PR, P817; Luo H., 2018, C LEARN THEOR, P1739; Nunes BAA, 2014, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2014-47; Santarra T., 2019, THESIS; Schapire, 2015, P 28 C LEARN THEOR, P1286; Seldin Y., 2017, C LEARN THEOR; Seldin Y, 2014, PR MACH LEARN RES, V32, P1287; Steinhardt Jacob., 2014, ICML, P1593; Warmuth M. K., 2014, P MACHINE LEARNING R, V35, P1295; Wei C-Y., 2018, C LEARN THEOR, P1263; Zimmert J., 2019, INT C MACH LEARN	38	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305087
C	Zheng, XY; Chen, SX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zheng, Xiangyu; Chen, Song Xi			Partitioning Structure Learning for Segmented Linear Regression Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SELECTION	This paper proposes a partitioning structure learning method for segmented linear regression trees (SLRT), which assigns linear predictors over the terminal nodes. The recursive partitioning process is driven by an adaptive split selection algorithm that maximizes, at each node, a criterion function based on a conditional Kendall's tau statistic that measures the rank dependence between the regressors and the fitted linear residuals. Theoretical analysis shows that the split selection algorithm permits consistent identification and estimation of the unknown segments. A sufficiently large tree is induced by applying the split selection algorithm recursively. Then the minimal cost-complexity tree pruning procedure is applied to attain the right-sized tree, that ensures (i) the nested structure of pruned subtrees and (ii) consistent estimation to the number of segments. Implanting the SLRT as a built-in base predictor, we obtain the ensemble predictors by random forests (RF) and the proposed weighted random forests (WRF). The practical performance of the SLRT and its ensemble versions are evaluated via numerical simulations and empirical studies. The latter shows their advantageous predictive performance over a set of state-of-the-art tree-based models on well-studied public datasets.	[Zheng, Xiangyu; Chen, Song Xi] Peking Univ, Beijing, Peoples R China	Peking University	Zheng, XY (corresponding author), Peking Univ, Beijing, Peoples R China.	zhengxiangyu@pku.edu.cn; csx@gsm.pku.edu.cn	Chen, Song Xi/AAN-4984-2021; Chen, Song Xi/ABH-2789-2020	Chen, Song Xi/0000-0002-2338-0873; 	China's National Key Research Special Program Grant [2016YFC0207701]; National Key Basic Research Program [2015CB856000]; National Natural Science Foundation of China [71532001]	China's National Key Research Special Program Grant; National Key Basic Research Program(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This research is funded by China's National Key Research Special Program Grant 2016YFC0207701, National Key Basic Research Program Grant 2015CB856000 and National Natural Science Foundation of China grant 71532001.	Alexander WP., 1996, J COMPUTATIONAL GRAP, V5, P156, DOI DOI 10.1080/10618600.1996.10474702; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Fan X., 2018, NEURIPS; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Gonzalo J, 2002, J ECONOMETRICS, V110, P319, DOI 10.1016/S0304-4076(02)00098-2; Karalic A., 1992, ECAI; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Kim HJ, 2009, STAT SINICA, V19, P597; Kim J, 2008, J MULTIVARIATE ANAL, V99, P2016, DOI 10.1016/j.jmva.2008.02.028; Li KC, 2000, J AM STAT ASSOC, V95, P547, DOI 10.2307/2669398; Liu J, 1997, STAT SINICA, V7, P497; Loh W.-Y., 2018, GUIDE CLASSIFICATION; Loh WY, 2013, ANN APPL STAT, V7, P495, DOI 10.1214/12-AOAS596; Loh WY, 2002, STAT SINICA, V12, P361; Malerba D, 2004, IEEE T PATTERN ANAL, V26, P612, DOI 10.1109/TPAMI.2004.1273937; Oiwa H., 2014, NEURIPS; Olshen R., 1984, CLASSIFICATION REGRE; Perron P, 2006, J ECONOMETRICS, V134, P373, DOI 10.1016/j.jeconom.2005.06.030; Potts D, 2005, MACH LEARN, V61, P5, DOI 10.1007/s10994-005-1121-8; Quinlan J. R., 1993, ICML; Torgo L., 2000, ICML; Trevor H., 2014, ELEMENTS STAT LEARNI, P106; Wang J., 2012, NEURIPS; Wang Y., 1997, ECML	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302024
C	Zhong, PL; Mo, YC; Xiao, C; Chen, PY; Zheng, CX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhong, Peilin; Mo, Yuchen; Xiao, Chang; Chen, Pengyu; Zheng, Changxi			Rethinking Generative Mode Coverage: A Pointwise Guaranteed Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many generative models have to combat missing modes. The conventional wisdom to this end is by reducing through training a statistical distance (such as f-divergence) between the generated distribution and provided data distribution. But this is more of a heuristic than a guarantee. The statistical distance measures a global, but not local, similarity between two distributions. Even if it is small, it does not imply a plausible mode coverage. Rethinking this problem from a game-theoretic perspective, we show that a complete mode coverage is firmly attainable. If a generative model can approximate a data distribution moderately well under a global statistical distance measure, then we will be able to find a mixture of generators that collectively covers every data point and thus every mode, with a lower-bounded generation probability. Constructing the generator mixture has a connection to the multiplicative weights update rule, upon which we propose our algorithm. We prove that our algorithm guarantees complete mode coverage. And our experiments on real and synthetic datasets confirm better mode coverage over recent approaches, ones that also use generator mixtures but rely on global statistical distances.	[Zhong, Peilin; Mo, Yuchen; Xiao, Chang; Chen, Pengyu; Zheng, Changxi] Columbia Univ, New York, NY 10027 USA	Columbia University	Zhong, PL (corresponding author), Columbia Univ, New York, NY 10027 USA.	peilin@cs.columbia.edu; yuchen.mo@columbia.edu; chang@cs.columbia.edu; pengyu.chen@columbia.edu; cxz@cs.columbia.edu			National Science Foundation [CAREER-1453101, 1816041, 1910839, 1703925, 1421161, 1714818, 1617955, 1740833]; Simons Foundation [491119]; Google Research Award; Google PhD Fellowship; Snap Research Fellowship; Columbia SEAS CKGSB Fellowship; SoftBank Group	National Science Foundation(National Science Foundation (NSF)); Simons Foundation; Google Research Award(Google Incorporated); Google PhD Fellowship(Google Incorporated); Snap Research Fellowship; Columbia SEAS CKGSB Fellowship; SoftBank Group	This work was supported in part by the National Science Foundation (CAREER-1453101, 1816041, 1910839, 1703925, 1421161, 1714818, 1617955, 1740833), Simons Foundation (#491119 to Alexandr Andoni), Google Research Award, a Google PhD Fellowship, a Snap Research Fellowship, a Columbia SEAS CKGSB Fellowship, and SoftBank Group.	Amari SI, 2016, APPL MATH SCI, V194, P1, DOI 10.1007/978-4-431-55978-8; Arjovsky M., 2017, ARXIV170107875; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Barratt S., 2018, ARXIV180101973; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Brock Andrew, 2018, ARXIV180911096; Cantelli FP, 1933, GIORN I ITAL ATTUARI, V4, P421; Che Tong, 2016, ARXIV161202136; Du Ding-Zhu, 2013, MINIMAX APPL, V4; Dumoulin Vincent, 2016, ARXIV E PRINTS; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grnarova P., 2017, ARXIV170603269; Grover A, 2018, AAAI CONF ARTIF INTE, P3069; Gulrajani I, 2017, P NIPS 2017; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hoang Quan, 2018, ICLR; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li C., 2018, PROC 32 INT C NEURAL, P6072; Lin Z., 2017, ARXIV171204086; Locatello F., 2018, ARXIV180411130; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Metz Luke, 2016, ARXIV161102163; Nowozin S, 2016, ADV NEUR IN, V29; Park D. K., 2018, ARXIV180502481; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Saatci Y., 2017, ADV NEURAL INFORM PR, P3622; Salimans T, 2016, ADV NEUR IN, V29; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tolstikhin I. O., 2017, ADV NEURAL INFORM PR, P5424, DOI DOI 10.5555/3295222.3295294; Wang Y. X., 2016, ARXIV161200991; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893; Xiao C., 2018, P ADV NEUR INF PROC, V31, P2269; Xiao H., 2017, ARXIV 170807747; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302012
C	Zhou, F; Li, TF; Zhou, HB; Ye, JP; Zhu, HT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Fan; Li, Tengfei; Zhou, Haibo; Ye, Jieping; Zhu, Hongtu			Graph-Based Semi-Supervised Learning with Nonignorable Nonresponses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DOUBLY ROBUST ESTIMATION; MEAN FUNCTIONALS; INFERENCE; MODELS	Graph-based semi-supervised learning is very important for many classification tasks, but most existing methods assume that all labelled nodes are randomly sampled. With the presence of nonignorable nonresponse, ignoring all missing nodes can lead to significant estimation bias and handicap the classifiers. To solve this issue, we propose a Graph-based joint model with Nonignorable Missingness (GNM) and develop an imputation and inverse probability weighting estimation approach. We further use graphical neural networks to model nonlinear link functions and then use a gradient descent (GD) algorithm to estimate all the parameters of GNM. We prove the identifiability of the GNM model and validate their predictive performance in both simulations and real data analysis through comparing with models ignoring or misspecifying the missingness mechanism. Our method can achieve up to 7.5% improvement than the baseline model for the document classification task on the Cora dataset.	[Zhou, Fan] Shanghai Univ Finance & Econ, Shanghai, Peoples R China; [Li, Tengfei; Zhou, Haibo; Zhu, Hongtu] Univ N Carolina, Chapel Hill, NC 27515 USA; [Ye, Jieping; Zhu, Hongtu] Didi Chuxing, AI Labs, Beijing, Peoples R China	Shanghai University of Finance & Economics; University of North Carolina; University of North Carolina Chapel Hill	Zhou, F (corresponding author), Shanghai Univ Finance & Econ, Shanghai, Peoples R China.	zhoufan@mail.shufe.edu.cn; tengfei_li@med.unc.edu; zhou@bios.unc.edu; yejieping@didiglobal.com; zhuhongtu@didiglobal.com						[Anonymous], 2016, ARXIV160308861; Bang H, 2005, BIOMETRICS, V61, P962, DOI 10.1111/j.1541-0420.2005.00377.x; Beaumont J.-F, 2000, SURV METHODOL, V26, P131; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Carpenter JR, 2006, J R STAT SOC A STAT, V169, P571, DOI 10.1111/j.1467-985X.2006.00407.x; Chen KN, 2001, J R STAT SOC B, V63, P775, DOI 10.1111/1467-9868.00312; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Kim JK, 2011, J AM STAT ASSOC, V106, P157, DOI 10.1198/jasa.2011.tm10104; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Little R. J., 2019, STAT ANAL MISSING DA, V793; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Qin J, 2002, J AM STAT ASSOC, V97, P193, DOI 10.1198/016214502753479338; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Rosset S., 2005, ADV NEURAL INFORM PR, P1161; RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.2307/2335739; Schafer JL, 2000, J AM STAT ASSOC, V95, P144, DOI 10.2307/2669534; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Tang NS, 2014, STAT SINICA, V24, P723, DOI 10.5705/ss.2012.254; Velickovic P., 2017, STAT-US, V1050, P20; Zhao H, 2013, COMPUT STAT DATA AN, V66, P101, DOI 10.1016/j.csda.2013.03.023; Zhu Xiaojin., 2003, P ICLR, P912	25	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307007
C	Zhou, YQ; Liu, SQ; Siow, JK; Du, XN; Liu, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Yaqin; Liu, Shangqing; Siow, Jingkai; Du, Xiaoning; Liu, Yang			Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.	[Zhou, Yaqin; Liu, Shangqing; Siow, Jingkai; Du, Xiaoning; Liu, Yang] Nanyang Technol Univ, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Liu, SQ; Du, XN (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	yqzhou@ntu.edu.sg; shangqin001@ntu.edu.sg; jingkai001@ntu.edu.sg; xiaoning.du@ntu.edu.sg; yangliu@ntu.edu.sg	Liu, Yang/D-2306-2013; yang, liu/GVU-8760-2022	Liu, Yang/0000-0001-7300-9215; 	Alibaba-NTU JRI project [M4062640.J4A]; Security: A Compositional Approach Of Building Security Verified System [M4192001.023.710079]; National Research Foundation, Prime Ministers Office, Singapore Under its National Cybersecurity RD Program [NRF2018NCR-NCR005-001]	Alibaba-NTU JRI project; Security: A Compositional Approach Of Building Security Verified System; National Research Foundation, Prime Ministers Office, Singapore Under its National Cybersecurity RD Program(National Research Foundation, Singapore)	This work was supported by Alibaba-NTU JRI project (M4062640.J4A), Security: A Compositional Approach Of Building Security Verified System (M4192001.023.710079) and National Research Foundation, Prime Ministers Office, Singapore Under its National Cybersecurity R&D Program (Award No. NRF2018NCR-NCR005-001).	Allamanis M., 2017, LEARNING REPRESENT P; Bloem P., 2018, P 15 EUR SEM WEB C E, P593, DOI [10.1007/978-3-319-93417-4_38, DOI 10.1007/978-3-319-93417-4_38]; Chandramohan M, 2016, FSE'16: PROCEEDINGS OF THE 2016 24TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON FOUNDATIONS OF SOFTWARE ENGINEERING, P678, DOI 10.1145/2950290.2950350; Chen HX, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P2095, DOI 10.1145/3243734.3243849; Dai HJ, 2016, PR MACH LEARN RES, V48; Dam Hoa Khanh, 2017, ARXIV170802368; Du XN, 2019, PROC INT CONF SOFTW, P60, DOI 10.1109/ICSE.2019.00024; Li YK, 2019, ESEC/FSE'2019: PROCEEDINGS OF THE 2019 27TH ACM JOINT MEETING ON EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING, P533, DOI 10.1145/3338906.3338975; Li YK, 2017, ESEC/FSE 2017: PROCEEDINGS OF THE 2017 11TH JOINT MEETING ON FOUNDATIONS OF SOFTWARE ENGINEERING, P627, DOI 10.1145/3106237.3106295; Li Z., 2018, 25 ANN NETW DISTR SY 25 ANN NETW DISTR SY; Mou L., 2016, AAAI, V2, P4; Neuhaus S, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P529; Nguyen V. H., 2010, COMP COMM TECHN RES, P1; Russell RL, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P757, DOI 10.1109/ICMLA.2018.00120; Shin Y, 2011, IEEE T SOFTWARE ENG, V37, P772, DOI 10.1109/TSE.2010.81; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Velickovic P., 2017, STAT-US, V1050, P20; Wang JJ, 2017, P IEEE S SECUR PRIV, P579, DOI 10.1109/SP.2017.23; Wang JJ, 2019, PROC INT CONF SOFTW, P724, DOI 10.1109/ICSE.2019.00081; Xu XJ, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P363, DOI 10.1145/3133956.3134018; Xu ZZ, 2017, PROC INT CONF SOFTW, P462, DOI 10.1109/ICSE.2017.49; Xue Y., 2018, IEEE T SOFTWARE ENG; Yamaguchi F, 2014, P IEEE S SECUR PRIV, P590, DOI 10.1109/SP.2014.44; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480; Ying R., 2018, P 2018 ANN C NEUR IN; Zhang M., 2018, 32 AAAI C ART INT; Zhou YQ, 2017, ESEC/FSE 2017: PROCEEDINGS OF THE 2017 11TH JOINT MEETING ON FOUNDATIONS OF SOFTWARE ENGINEERING, P914, DOI 10.1145/3106237.3117771	28	0	0	6	15	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901079
C	Zhu, MH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhu, Michael H.			Sample Adaptive MCMC	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				HASTINGS; CONVERGENCE; ERGODICITY	For MCMC methods like Metropolis-Hastings, tuning the proposal distribution is important in practice for effective sampling from the target distribution pi. In this paper, we present Sample Adaptive MCMC (SA-MCMC), a MCMC method based on a reversible Markov chain for pi(circle times N) that uses an adaptive proposal distribution based on the current state of N points and a sequential substitution procedure with one new likelihood evaluation per iteration and at most one updated point each iteration. The SA-MCMC proposal distribution automatically adapts within its parametric family to best approximate the target distribution, so in contrast to many existing MCMC methods, SA-MCMC does not require any tuning of the proposal distribution. Instead, SA-MCMC only requires specifying the initial state of N points, which can often be chosen a priori, thereby automating the entire sampling procedure with no tuning required. Experimental results demonstrate the fast adaptation and effective sampling of SA-MCMC.	[Zhu, Michael H.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Zhu, MH (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	mhzhu@cs.stanford.edu						Andrieu C, 2006, ANN APPL PROBAB, V16, P1462, DOI 10.1214/105051606000000286; Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Athreya KB, 1996, ANN STAT, V24, P69; BARKER AA, 1965, AUST J PHYS, V18, P119, DOI 10.1071/PH650119; Bezanson J, 2017, SIAM REV, V59, P65, DOI 10.1137/141000671; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Bugallo MF, 2017, IEEE SIGNAL PROC MAG, V34, P60, DOI 10.1109/MSP.2017.2699226; Bugallo MF, 2015, DIGIT SIGNAL PROCESS, V47, P36, DOI 10.1016/j.dsp.2015.05.014; Cai B, 2008, STAT COMPUT, V18, P421, DOI 10.1007/s11222-008-9051-5; Calderhead B, 2014, P NATL ACAD SCI USA, V111, P17408, DOI 10.1073/pnas.1408184111; Cappe O, 2004, J COMPUT GRAPH STAT, V13, P907, DOI 10.1198/106186004X12803; Cappe O, 2008, STAT COMPUT, V18, P447, DOI 10.1007/s11222-008-9059-x; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Chan Hock Peng, 2015, WORKING PAPER; Dua D., 2019, US; Elvira V, 2019, STAT SCI, V34, P129, DOI 10.1214/18-STS668; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Gelman A, 1992, STAT SCI, V7, P136, DOI 10.1214/ss/1177011136; Gelman A.R., 1994, BAYESIAN STAT, V5, P599; Geyer C.J., 1991, MARKOV CHAIN MONTE C; GILKS WR, 1994, STATISTICIAN, V43, P179, DOI 10.2307/2348942; Goodman J, 2010, COMM APP MATH COM SC, V5, P65, DOI 10.2140/camcos.2010.5.65; Griffin JE, 2013, STAT COMPUT, V23, P123, DOI 10.1007/s11222-011-9296-2; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Holden L, 2009, ANN APPL PROBAB, V19, P395, DOI 10.1214/08-AAP545; Keith JM, 2008, STAT COMPUT, V18, P409, DOI 10.1007/s11222-008-9070-2; Korattikara A, 2014, PR MACH LEARN RES, V32; Leimkuhler B, 2018, STAT COMPUT, V28, P277, DOI 10.1007/s11222-017-9730-1; Lewandowski A., 2010, THESIS; Li WT, 2016, J AM STAT ASSOC, V111, P298, DOI 10.1080/01621459.2015.1006364; Li WT, 2013, J AM STAT ASSOC, V108, P1350, DOI 10.1080/01621459.2013.831980; Liang F., 2011, ADV MARKOV CHAIN MON, V714; Liu JS, 2000, J AM STAT ASSOC, V95, P121, DOI 10.2307/2669532; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; Llorente F, 2019, IEEE SIGNAL PROC LET, V26, P953, DOI 10.1109/LSP.2019.2913470; Luengo D, 2013, INT CONF ACOUST SPEE, P6148, DOI 10.1109/ICASSP.2013.6638846; Martino L, 2017, STAT COMPUT, V27, P599, DOI 10.1007/s11222-016-9642-5; Martino L, 2016, DIGIT SIGNAL PROCESS, V58, P64, DOI 10.1016/j.dsp.2016.07.013; Martino L, 2015, IEEE T SIGNAL PROCES, V63, P4422, DOI 10.1109/TSP.2015.2440215; Martino Luca, 2018, DIGITAL SIGNAL PROCE; Mengersen KL, 2003, BAYESIAN STATISTICS 7, P277; Mengersen KL, 1996, ANN STAT, V24, P101; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Neal R., 2011, ARXIV11010387; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2, P2, DOI DOI 10.1201/B10905-6; Neal RM, 2004, ADV NEUR IN, V16, P401; Owen A, 2000, J AM STAT ASSOC, V95, P135, DOI 10.2307/2669533; Robert CP., 2004, SPRINGER TEXTS STAT, V2nd; Roberts GO, 1997, ANN APPL PROBAB, V7, P110, DOI 10.1214/aoap/1034625254; Roberts GO, 2007, J APPL PROBAB, V44, P458, DOI 10.1239/jap/1183667414; Roberts GO, 1996, BIOMETRIKA, V83, P95, DOI 10.1093/biomet/83.1.95; Tjelmeland H., 2004, TECHNICAL REPORT; Warnes Gregory R, 2000, THESIS	56	0	0	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900063
C	Zhu, ZH; Ding, TY; Tsakiris, MC; Robinson, DP; Vidal, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhu, Zhihui; Ding, Tianyu; Tsakiris, Manolis C.; Robinson, Daniel P.; Vidal, Rene			A Linearly Convergent Method for Non-Smooth Non-Convex Optimization on the Grassmannian with Applications to Robust Subspace and Dictionary Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS; SPARSE	Minimizing a non-smooth function over the Grassmannian appears in many applications in machine learning. In this paper we show that if the objective satisfies a certain Riemannian regularity condition (RRC) with respect to some point in the Grassmannian, then a projected Riemannian subgradient method with appropriate initialization and geometrically diminishing step size converges at a linear rate to that point. We show that for both the robust subspace learning method Dual Principal Component Pursuit (DPCP) and the Orthogonal Dictionary Learning (ODL) problem, the RRC is satisfied with respect to appropriate points of interest, namely the subspace orthogonal to the sought subspace for DPCP and the orthonormal dictionary atoms for ODL. Consequently, we obtain in a unified framework significant improvements for the convergence theory of both methods.	[Zhu, Zhihui; Vidal, Rene] Johns Hopkins Univ, MINDS, Baltimore, MD 21218 USA; [Ding, Tianyu] Johns Hopkins Univ, AMS, Baltimore, MD 21218 USA; [Tsakiris, Manolis C.] ShanghaiTech Univ, SIST, Shanghai, Peoples R China; [Robinson, Daniel P.] Lehigh Univ, ISE, Bethlehem, PA 18015 USA	Johns Hopkins University; Johns Hopkins University; ShanghaiTech University; Lehigh University	Zhu, ZH (corresponding author), Johns Hopkins Univ, MINDS, Baltimore, MD 21218 USA.	zzhu29@jhu.edu; tding1@jhu.edu; mtsakiris@shanghaitech.edu.cn; daniel.p.robinson@lehigh.edu; rvidal@jhu.edu	Zhu, Zhihui/AAR-5029-2020	Zhu, Zhihui/0000-0002-3856-0375; Robinson, Daniel/0000-0003-0251-4227	NSF [1704458]; ShanghaiTech grant [2017F0203-000-16]; Northrop Grumman Mission Systems Research in Applications for Learning Machines (REALM) initiative	NSF(National Science Foundation (NSF)); ShanghaiTech grant; Northrop Grumman Mission Systems Research in Applications for Learning Machines (REALM) initiative	This research is supported in part by NSF grant 1704458, ShanghaiTech grant 2017F0203-000-16, and the Northrop Grumman Mission Systems Research in Applications for Learning Machines (REALM) initiative. Zhihui Zhu would like to thank Xiao Li and Dr. Anthony Man-Cho So (CUHK) for fruitful discussions about regularity conditions for non-smooth optimization problems.	[Anonymous], 2017, ARXIV171103247; Arora S., 2015, J MACHINE LEARNING R; Bai Y., 2019, INT C LEARN REPR; Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976; Boumal N., 2016, IMA J NUMERICAL ANAL; BURKE JV, 1993, SIAM J CONTROL OPTIM, V31, P1340, DOI 10.1137/0331063; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chen S., 2018, ARXIV181100980; Curtis FE, 2017, OPTIM METHOD SOFTW, V32, P148, DOI 10.1080/10556788.2016.1208749; Curtis FE, 2015, MATH PROGRAM COMPUT, V7, P399, DOI 10.1007/s12532-015-0086-2; Davis D., 2018, ARXIV180302461; Ding T., 2019, P INT C MACH LEARN; Duchi J. C., 2017, ARXIV170502356; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Engan K., 1999, IEEE INT C AC SPEECH; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Goffin J.-L., 2012, SUBGRADIENT OPTIMIZA; Grohs P, 2016, IMA J NUMER ANAL, V36, P1167, DOI 10.1093/imanum/drv043; Guo P, 2018, INT POW ELEC APPLICA, P113; Hamm J., 2008, P INT C MACH LEARN I, P376, DOI DOI 10.1145/1390156.1390204; Higham N, 1995, MATRIX PROCRUSTES PR; Kim C., 2019, IEEE T PATTERN ANAL; Lee Jason D, 2017, ARXIV171007406; Lerman G., 2017, INFORM INF A J IMA, V7, P277; Lerman G, 2018, P IEEE, V106, P1380, DOI 10.1109/JPROC.2018.2853141; Lerman G, 2015, FOUND COMPUT MATH, V15, P363, DOI 10.1007/s10208-014-9221-0; Li X, 2018, ARXIV180909237; Markopoulos PP, 2014, IEEE T SIGNAL PROCES, V62, P5046, DOI 10.1109/TSP.2014.2338077; Maunu T, 2019, J MACH LEARN RES, V20; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Qu Q., 2019, ADV NEURAL INFORM PR; QU Q., 2014, P ADV NEUR INF PROC, V27, P3401; Rahmani M., 2016, ARXIV160904789; Slama R, 2015, PATTERN RECOGN, V48, P556, DOI 10.1016/j.patcog.2014.08.011; Soltanolkotabi M., 2013, ROBUST SUBSPACE CLUS; Stewart G., 1990, MATRIX PERTURBATION; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Tsakiris M., 2015, P IEEE INT C COMP VI, P10; Tsakiris MC, 2017, PR MACH LEARN RES, V70; Tsakiris MC, 2018, J MACH LEARN RES, V19; Usevich K, 2014, AUTOMATICA, V50, P1656, DOI 10.1016/j.automatica.2014.04.010; VIAL JP, 1983, MATH OPER RES, V8, P231, DOI 10.1287/moor.8.2.231; Wang P, 2019, INT CONF ACOUST SPEE, P8147, DOI 10.1109/ICASSP.2019.8682499; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Yang WH, 2014, PAC J OPTIM, V10, P415; You C, 2017, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR.2017.460; Zhang T, 2014, J MACH LEARN RES, V15, P749; Zhao FQ, 2018, 2018 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL & ELECTRONICS ENGINEERING AND COMPUTER SCIENCE (ICEEECS 2018), P322; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157; Zhu Z., 2018, NEURAL INFORM PROCES	51	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901011
C	Zhu, ZH; Li, QW; Yang, XS; Tang, GG; Wakin, MB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhu, Zhihui; Li, Qiuwei; Yang, Xinshuo; Tang, Gongguo; Wakin, Michael B.			Distributed Low-rank Matrix Factorization With Exact Consensus	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE; ALGORITHM; OPTIMIZATION	Low-rank matrix factorization is a problem of broad importance, owing to the ubiquity of low-rank models in machine learning contexts. In spite of its non-convexity, this problem has a well-behaved geometric landscape, permitting local search algorithms such as gradient descent to converge to global minimizers. In this paper, we study low-rank matrix factorization in the distributed setting, where local variables at each node encode parts of the overall matrix factors, and consensus is encouraged among certain such variables. We identify conditions under which this new problem also has a well-behaved geometric landscape, and we propose an extension of distributed gradient descent (DGD) to solve this problem. The favorable landscape allows us to prove convergence to global optimality with exact consensus, a stronger result than what is provided by off-the-shelf DGD theory.	[Zhu, Zhihui] Johns Hopkins Univ, Math Inst Data Sci, Baltimore, MD 21218 USA; [Li, Qiuwei; Yang, Xinshuo; Tang, Gongguo; Wakin, Michael B.] Colorado Sch Mines, Dept Elect Engn, Golden, CO 80401 USA; [Zhu, Zhihui] Univ Denver, Dept Elect & Comp Engn, Denver, CO 80208 USA	Johns Hopkins University; Colorado School of Mines; University of Denver	Zhu, ZH (corresponding author), Johns Hopkins Univ, Math Inst Data Sci, Baltimore, MD 21218 USA.	zzhu29@jhu.edu; qiuli@mines.edu; xinshuoyang@mines.edu; gtang@mines.edu; mwakin@mines.edu	Zhu, Zhihui/AAR-5029-2020; Li, Qiuwei/AAH-4942-2019; Wakin, Michael/G-1582-2012	Zhu, Zhihui/0000-0002-3856-0375; Li, Qiuwei/0000-0002-2306-6649; Wakin, Michael/0000-0002-2165-4586	DARPA Lagrange Program under ONR/SPAWAR [N660011824020]	DARPA Lagrange Program under ONR/SPAWAR	This work was supported by the DARPA Lagrange Program under ONR/SPAWAR contract N660011824020. The views, opinions and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.	Attouch H, 2009, MATH PROGRAM, V116, P5, DOI 10.1007/s10107-007-0133-5; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Bolte J., 2014, COMPUTER VISION IMAG, V146, P459; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Chang TH, 2015, IEEE T SIGNAL PROCES, V63, P482, DOI 10.1109/TSP.2014.2367458; Chen Antong, 2012, THESIS; Chen YD, 2018, IEEE SIGNAL PROC MAG, V35, P14, DOI 10.1109/MSP.2018.2821706; Chi Y., 2019, IEEE T SIGNAL PROCES; Chi YJ, 2018, IEEE SIGNAL PROC MAG, V35, P178, DOI 10.1109/MSP.2018.2832197; Daneshmand A., 2018, ARXIV180908694; Ge R., 2017, ARXIV170400708; Jakovetic D, 2015, IEEE T AUTOMAT CONTR, V60, P922, DOI 10.1109/TAC.2014.2363299; Jakovetic D, 2014, IEEE T AUTOMAT CONTR, V59, P1131, DOI 10.1109/TAC.2014.2298712; Jin Chi, 2017, ARXIV170300887; Kempe D, 2003, ANN IEEE SYMP FOUND, P482, DOI 10.1109/SFCS.2003.1238221; Kim SJ, 2016, PORTL INT CONF MANAG, P2460, DOI 10.1109/PICMET.2016.7806794; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Lee J. D., MATH PROGRAMMING, P1; Li Q., 2019, ARXIV190409712; Li Q., 2018, INFORM INFERENCE J I; Li Q., 2019, 2019 IEEE 8 INT WORK; Li Q, 2019, COMPUTING IN CIVIL ENGINEERING 2019: DATA, SENSING, AND ANALYTICS, P393; Li Qiuwei, 2017, ARXIV170401265; Li S., 2019, NEURIPS; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Lu ST, 2019, PR MACH LEARN RES, V97; Mokhtari A, 2017, IEEE T SIGNAL PROCES, V65, P146, DOI 10.1109/TSP.2016.2617829; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Notarnicola I., 2017, COMP ADV MULT AD PRO, P1; Nouiehed M., 2018, ARXIV180302968; Royer C. W., 2018, ARXIV180302924; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Sun J., 2016, THESIS; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Uschmajew A., 2018, CRITICAL POINTS QUAD; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Zeng JS, 2018, IEEE T SIGNAL PROCES, V66, P2834, DOI 10.1109/TSP.2018.2818081; Zhang Xiao, 2018, INT C MACH LEARN, P5857; Zhu Z, 2017, ARXIV170301256; Zhu ZH, 2018, IEEE T SIGNAL PROCES, V66, P3614, DOI 10.1109/TSP.2018.2835403	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900006
C	Zou, DF; Xu, P; Gu, QQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zou, Difan; Xu, Pan; Gu, Quanquan			Stochastic Gradient Hamiltonian Monte Carlo Methods with Recursive Variance Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MCMC	Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) algorithms have received increasing attention in both theory and practice. In this paper, we propose a Stochastic Recursive Variance-Reduced gradient HMC (SRVR-HMC) algorithm. It makes use of a semi-stochastic gradient estimator that recursively accumulates the gradient information to reduce the variance of the stochastic gradient. We provide a convergence analysis of SRVR-HMC for sampling from a class of non-log-concave distributions and show that SRVR-HMC converges faster than all existing HMC-type algorithms based on underdamped Langevin dynamics. Thorough experiments on synthetic and real-world datasets validate our theory and demonstrate the superiority of SRVR-HMC.	[Zou, Difan; Xu, Pan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Zou, DF (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	knowzou@cs.ucla.edu; panxu@cs.ucla.edu; qgu@cs.ucla.edu	X, Pan/GVS-4402-2022; Xu, Pan/AAH-3620-2019	Xu, Pan/0000-0002-2559-8622	National Science Foundation [IIS-1855099, IIS-1906169]	National Science Foundation(National Science Foundation (NSF))	We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation BIGDATA IIS-1855099 and CAREER Award IIS-1906169. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Baker J, 2019, STAT COMPUT, V29, P599, DOI 10.1007/s11222-018-9826-2; Bardenet R, 2017, J MACH LEARN RES, V18, P1; Barkhagen M, 2018, ARXIV181202709; Betancourt M, 2017, BERNOULLI, V23, P2257, DOI 10.3150/16-BEJ810; Betancourt M, 2015, PR MACH LEARN RES, V37, P533; Bolley F., 2005, ANN FAC SCI TOULOUSE, V14, P331, DOI 10.5802/afst.1095; Bou-Rabee Nawaf, 2018, ARXIV180500452; Brosse N, 2018, ADV NEUR IN, V31; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chatterji NS, 2018, PR MACH LEARN RES, V80; Chen C., 2017, ARXIV170901180; CHEN C., 2015, ADV NEURAL INFORM PR, P2269; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Cheng X., 2018, ARXIV180501648; Cheng X., 2018, C LEARNING THEORY, P300; Coffey W., 2012, LANGEVIN EQUATION AP; Dalalyan A. S., 2017, ARXIV170404752; Dalalyan A. S., 2017, ARXIV171000095; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Dang KD, 2019, J MACH LEARN RES, V20; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154; Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238; Eberle A., 2017, ARXIV170301617; Erdogdu MA., 2018, ADV NEURAL INFORM PR, Vvol 31, P9671; Gao X., 2018, ARXIV180904618; GYONGY I, 1986, PROBAB THEORY REL, V71, P501, DOI 10.1007/BF00699039; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Ji KY, 2019, PR MACH LEARN RES, V97; KLOEDEN PE, 1992, J STAT PHYS, V66, P283, DOI 10.1007/BF01060070; Langevin P, 1908, CR HEBD ACAD SCI, V146, P530; Lei LH, 2017, ADV NEUR IN, V30; Li Z., 2018, ARXIV180311159; Lichman M, 2013, UCI MACHINE LEARNING; Liptser R.S., 2013, STAT RANDOM PROCESSE, V5; LIU LY, 2018, ADV NEURAL INFORM PR, P686; Ma Y.A., 2015, ARXIV150604696, P2917; Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3; Mou W., 2018, P MACHINE LEARNING R, P605; Moulines E., 2019, ARXIV190513142; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Nguyen Lam M, 2017, ARXIV170300102; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Teh Y. W., 2016, J MACH LEARN RES, V17, P193; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Wang Zhe, 2018, ARXIV181010690; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xu P., 2018, ADV NEURAL INFORM PR, P3126; Zhang Y., 2017, P 2017 C LEARN THEOR, V65, P1; Zhou D., 2018, ARXIV180608782; Zou D., 2018, P INT C UNC ART INT P INT C UNC ART INT; Zou DF, 2018, PR MACH LEARN RES, V80; Zou Difan, 2019, P MACHINE LEARNING R, P2936	57	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303078
C	Zou, DF; Gu, QQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zou, Difan; Gu, Quanquan			An Improved Analysis of Training Over-parameterized Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A recent line of research has shown that gradient-based algorithms with random initialization can converge to the global minima of the training loss for over-parameterized (i.e., sufficiently wide) deep neural networks. However, the condition on the width of the neural network to ensure the global convergence is very stringent, which is often a high-degree polynomial in the training sample size n (e.g., O(n(24))). In this paper, we provide an improved analysis of the global convergence of (stochastic) gradient descent for training deep neural networks, which only requires a milder over-parameterization condition than previous work in terms of the training sample size and other problem-dependent parameters. The main technical contributions of our analysis include (a) a tighter gradient lower bound that leads to a faster convergence of the algorithm, and (b) a sharper characterization of the trajectory length of the algorithm. By specializing our result to two-layer (i.e., one-hidden-layer) neural networks, it also provides a milder over-parameterization condition than the best-known result in prior work.	[Zou, Difan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Zou, DF (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	knowzou@cs.ucla.edu; qgu@cs.ucla.edu			National Science Foundation [IIS-1906169, IIS-1855099]; Salesforce Deep Learning Research Award	National Science Foundation(National Science Foundation (NSF)); Salesforce Deep Learning Research Award	We thank the anonymous reviewers and area chair for their helpful comments. We also thank Jinshan Zeng for his helpful comment on the proof in the earlier version of our work. This research was sponsored in part by the National Science Foundation CAREER Award IIS-1906169, BIGDATA IIS-1855099, and Salesforce Deep Learning Research Award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Allen-Zhu Zeyuan, 2018, ARXIV181104918; Allen-Zhu Zeyuan, 2018, ARXIV181012065; [Anonymous], 2018, ARXIV180301206; [Anonymous], ARXIV181103962; [Anonymous], 2018, ARXIV180801204; Arora Sanjeev, 2019, ARXIV190108584; BRUTZKUS A., 2017, P 34 INT C MACH LEAR, V70; CAO Y., 2019, ARXIV190201384; Chizat L., 2018, NOTE LAZY TRAINING S; Du Simon S, 2018, GRADIENT DESCENT FIN; Du Simon S., 2018, INT C LEARN REPR; Du SS., 2019, P 7 INT C LEARN REPR; GAO W., 2019, 22 INT C ART INT STA; He K., 2016, IEEE C COMPUTER VISI; Jacot A., 2018, ADV NEURAL INFORM PR; LI Y., 2017, ARXIV170509886; Oymak S., 2019, ARXIV190204674; Tian Yuandong, 2017, ARXIV170300560; Wu Xiaoxia, 2019, ARXIV190207111; Zhang Chiyuan, 2016, ARXIV161103530; Zhang H., 2019, ARXIV190307120; Zhang Xiao, 2018, ARXIV180607808; Zhong  Kai, 2017, ARXIV170603175; Zou D, 2018, ARXIV181108888	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302009
C	Zou, SF; Xu, TY; Liang, YB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zou, Shaofeng; Xu, Tengyu; Liang, Yingbin			Finite-Sample Analysis for SARSA with Linear Function Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIFFERENCE; CONVERGENCE; ITERATION	SARSA is an on-policy algorithm to learn a Markov decision process policy in reinforcement learning. We investigate the SARSA algorithm with linear function approximation under the non-i.i.d. data, where a single sample trajectory is available. With a Lipschitz continuous policy improvement operator that is smooth enough, SARSA has been shown to converge asymptotically [28, 23]. However, its non-asymptotic analysis is challenging and remains unsolved due to the non-i.i.d. samples and the fact that the behavior policy changes dynamically with time. In this paper, we develop a novel technique to explicitly characterize the stochastic bias of a type of stochastic approximation procedures with time-varying Markov transition kernels. Our approach enables non-asymptotic convergence analyses of this type of stochastic approximation algorithms, which may be of independent interest. Using our bias characterization technique and a gradient descent type of analysis, we provide the finite-sample analysis on the mean square error of the SARSA algorithm. We then further study a fitted SARSA algorithm, which includes the original SARSA algorithm and its variant in [28] as special cases. This fitted SARSA algorithm provides a more general framework for iterative on-policy fitted policy iteration, which is more memory and computationally efficient. For this fitted SARSA algorithm, we also provide its finite-sample analysis.	[Zou, Shaofeng] SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14228 USA; [Xu, Tengyu; Liang, Yingbin] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; Ohio State University	Zou, SF (corresponding author), SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14228 USA.	szou3@buffalo.edu; xu.3260@osu.edu; liang.889@osu.edu	Xu, Tengyu/ABC-1399-2020		U.S. National Science Foundation [CCF-1761506, ECCS-1818904, CCF-1801855]	U.S. National Science Foundation(National Science Foundation (NSF))	We would like to thank the anonymous reviewer and the Area Chair for their valuable comments. The work of T. Xu and Y. Liang was supported in partby the U.S. National Science Foundation under Grants CCF-1761506, ECCS-1818904, and CCF-1801855.	Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Asadi K., 2016, P INT C MACH LEARN I; Bertsekas D. P., 2012, APPROXIMATE DYNAMIC, V2; Bhandari Jalaj, 2018, C LEARN THEOR, P1691; Boyan JA, 2002, MACH LEARN, V49, P233, DOI 10.1023/A:1017936530646; Bubeck S., 2015, FDN TRENDS MACHINE L; Dalal G., 2018, P AAAI C ART INT AAA; Dalal G., 2018, COLT; De Farias DP, 2000, J OPTIMIZ THEORY APP, V105, P589, DOI 10.1023/A:1004641123405; Farahmand A.-M., 2010, P ADV NEUR INF PROC; Ghavamzadeh M., 2010, P ADV NEUR INF PROC; Gordon G.J., 1996, CHATTERING SARSA CMU CHATTERING SARSA CMU; Gordon GJ, 2001, ADV NEUR IN, V13, P1040; Kushner H, 2010, WILEY INTERDISCIP RE, V2, P87, DOI 10.1002/wics.57; Lacoste-Julien Simon, 2012, ARXIV12122002; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Lakshminarayanan C., 2018, P INT C ART INT STAT; Lazaric A., 2016, J MACHINE LEARNING R, V17, P583; Lazaric A., 2010, P INT C MACH LEARN I; Lazaric A, 2012, J MACH LEARN RES, V13, P3041; Melo F.S., 2008, P 25 INT C MACH LEAR, P664, DOI [DOI 10.1145/1390156.1390240, 10.1145/1390156.1390240]; Munos R, 2008, J MACH LEARN RES, V9, P815; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Perkins T.J., 2003, P ADV NEUR INF PROC, P1627; Perkins T. J., 2002, P 19 INT C MACH LEAR, P490; Pires B. A., 2012, P INT C MACH LEARN I; Prashanth L., 2013, P JOINT EUR C MACH L; Rummery G. A., 1994, TECHNICAL REPORT; Shah D, 2018, ADV NEUR IN, V31; Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559; Srikant R., 2019, P ANN C LEARN THEOR; Tagorti M., 2015, P INT C MACH LEARN I; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Tu S., 2018, P INT C MACH LEARN I; Xu T., 2019, P ADV NEUR INF PROC; Yang Zhuora, 2019, ARXIV190100137	39	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900028
C			Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R					On Robustness of Principal Component Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MATRIX COMPLETION; NOISY	Consider the setting of Linear Regression where the observed response variables, in expectation, are linear functions of the p-dimensional covariates. Then to achieve vanishing prediction error, the number of required samples scales faster than p sigma(2), where sigma(2) is a bound on the noise variance. In a high-dimensional setting where p is large but the covariates admit a low-dimensional representation (say r << p), then Principal Component Regression (PCR), cf. [36], is an effective approach; here, the response variables are regressed with respect to the principal components of the covariates. The resulting number of required samples to achieve vanishing prediction error now scales faster than r sigma(2) (<< p sigma(2)). Despite the tremendous utility of PCR, its ability to handle settings with noisy, missing, and mixed (discrete and continuous) valued covariates is not understood and remains an important open challenge, cf. [24]. As the main contribution of this work, we address this challenge by rigorously establishing that PCR is robust to noisy, sparse, and possibly mixed valued covariates. Specifically, under PCR, vanishing prediction error is achieved with the number of samples scaling as r max(sigma(2); rho(-4) log(5)(p)), where rho denotes the fraction of observed (noisy) covariates. We establish generalization error bounds on the performance of PCR, which provides a systematic approach in selecting the correct number of components r in a data-driven manner. The key to our result is a simple, but powerful equivalence between (i) PCR and (ii) Linear Regression with covariate pre-processing via Hard Singular Value Thresholding (HSVT). From a technical standpoint, this work advances the state-of-the-art analysis for HSVT by establishing stronger guarantees with respect to the parallel to center dot parallel to(2,infinity)-error for the estimated matrix rather than the Frobenius norm/mean-squared error (MSE) as is commonly done in the matrix estimation / completion literature.										Abadie A, 2003, AM ECON REV, V93, P113, DOI 10.1257/000282803321455188; Abadie A., 2010, J AM STAT ASSOC; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Agarwal Anish, 2018, Proceedings of the ACM on Measurement and Analysis of Computing Systems, V2, DOI 10.1145/3287319; Airoldi EM., 2013, ADV NEURAL INFORM PR, V26, P692; ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3; Amjad M., 2019, ARXIV190506400; Amjad M, 2018, J MACH LEARN RES, V19; Anandkumar A., 2013, C LEARNING THEORY, P867; [Anonymous], 2015, ARXIV150903025; Athey S., 2017, MATRIX COMPLETION ME; Athey S, 2017, J ECON PERSPECT, V31, P3, DOI 10.1257/jep.31.2.3; Bair E, 2006, J AM STAT ASSOC, V101, P119, DOI 10.1198/016214505000000628; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Belloni A., 2017, ARXIV170808353; Belloni A, 2017, J R STAT SOC B, V79, P939, DOI 10.1111/rssb.12196; Bishop CM, 1999, ADV NEUR IN, V11, P382; Borgs C., 2015, ARXIV150806675; Candes E., 2015, INVERSE PROBL, V23, P969; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Chao GQ, 2019, MACH LEARN KNOW EXTR, V1, P341, DOI 10.3390/make1010020; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Chen Y., 2013, INT C MACHINE LEARNI, P160; Chen Y., 2012, ARXIV12060823; Datta A, 2017, ANN STAT, V45, P2400, DOI 10.1214/16-AOS1527; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Doudchenko N., 2016, W22791 NBER; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Hopkins SB, 2017, ANN IEEE SYMP FOUND, P379, DOI 10.1109/FOCS.2017.42; JOLLIFFE IT, 1982, APPL STAT-J ROY ST C, V31, P300, DOI 10.2307/2348005; Kaul A, 2015, J MULTIVARIATE ANAL, V140, P72, DOI 10.1016/j.jmva.2015.04.009; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854; Rosenbaum M., 2013, PROBABILITY STAT BAC, V9, P276; Rosenbaum M, 2010, ANN STAT, V38, P2620, DOI 10.1214/10-AOS793; Shah D., 2018, ARXIV181211917; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Udell M., 2017, ARXIV170507474; Vershynin R., 2010, ARXIV10113027; Vershynin R., 2018, HIGH DIMENSIONAL PRO, V47; Wainwright M.J., 2019, HIGH DIMENSIONAL STA, V48; Wan SK, 2018, ECON LETT, V164, P121, DOI 10.1016/j.econlet.2018.01.019; Wedin P.-A., 1972, BIT (Nordisk Tidskrift for Informationsbehandling), V12, P99, DOI 10.1007/BF01932678; Zhang Y., 2015, ARXIV150908588	55	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901051
C	Aaronson, S; Chen, XY; Hazan, E; Kale, S; Nayak, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Aaronson, Scott; Chen, Xinyi; Hazan, Elad; Kale, Satyen; Nayak, Ashwin			Online Learning of Quantum States	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Suppose we have many copies of an unknown n-qubit state rho. We measure some copies of rho using a known two-outcome measurement E-1, then other copies using a measurement E-2, and so on. At each stage t, we generate a current hypothesis omega(t) about the state rho, using the outcomes of the previous measurements. We show that it is possible to do this in a way that guarantees that vertical bar Tr (E-i omega(t)) - Tr (E-i rho)vertical bar, the error in our prediction for the next measurement, is at least epsilon at most O(n/epsilon(2)) times. Even in the "non-realizable" setting-where there could be arbitrary noise in the measurement outcomes-we show how to output hypothesis states that incur at most O(root Tn) excess loss over the best possible state on the first T measurements. These results generalize a 2007 theorem by Aaronson on the PAC-learnability of quantum states, to the online and regret-minimization settings. We give three different ways to prove our results-using convex optimization, quantum postselection, and sequential fat-shattering dimension-which have different advantages in terms of parameters and portability.	[Aaronson, Scott] UT Austin, Austin, TX 78712 USA; [Chen, Xinyi; Hazan, Elad] Google AI Princeton, Princeton, NJ USA; [Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA; [Kale, Satyen] Google AI, New York, NY USA; [Nayak, Ashwin] Univ Waterloo, Waterloo, ON, Canada	University of Texas System; University of Texas Austin; Princeton University; University of Waterloo	Aaronson, S (corresponding author), UT Austin, Austin, TX 78712 USA.	aaronson@cs.utexas.edu; xinyic@google.com; ehazan@cs.princeton.edu; satyenkale@google.com; ashwin.nayak@uwaterloo.ca		Hazan, Elad/0000-0002-1566-3216	Vannevar Bush Faculty Fellowship from the US Department of Defense; NSF Alan T. Waterman Award; NSERC Canada	Vannevar Bush Faculty Fellowship from the US Department of Defense(United States Department of Defense); NSF Alan T. Waterman Award; NSERC Canada(Natural Sciences and Engineering Research Council of Canada (NSERC))	Supported by a Vannevar Bush Faculty Fellowship from the US Department of Defense. Part of this work was done while the author was supported by an NSF Alan T. Waterman Award.; Research supported in part by NSERC Canada.	Aaronson S., 2016, 28 MCGILL INV WORKSH; AARONSON S, 2006, P IEEE C COMP COMPL, P261; Aaronson S., 2004, THEORY COMPUTING, V1, P1; Aaronson S, 2007, P ROY SOC A-MATH PHY, V463, P3089, DOI 10.1098/rspa.2007.0113; Aaronson S, 2018, ACM S THEORY COMPUT, P325, DOI 10.1145/3188745.3188802; Aaronson S, 2014, SIAM J COMPUT, V43, P1131, DOI 10.1137/110856939; Ambainis A, 2002, J ACM, V49, P496, DOI 10.1145/581771.581773; Arora S, 2016, J ACM, V63, DOI 10.1145/2837020; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Audenaert KMR, 2005, J MATH PHYS, V46, DOI 10.1063/1.2044667; Badescu C., 2017, TECHNICAL REPORT; Bhatia R., 1997, MATRIX ANAL, V169, DOI DOI 10.1007/978-1-4612-0653-8; Carlen EA, 2014, J MATH PHYS, V55, DOI 10.1063/1.4871575; Gao Jingliang, 2015, PHYS REV, V92; Hazan E, 2015, FDN TRENDS OPTIMIZAT, V2; Nayak A., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P369, DOI 10.1109/SFFCS.1999.814608; O'Donnell R, 2016, ACM S THEORY COMPUT, P899, DOI 10.1145/2897518.2897544; Rakhlin A, 2015, J MACH LEARN RES, V16, P155; Rocchetto A, 2017, ARXIV170500345; Rocchetto Andrea, 2017, ARXIV171200127; Watrous J., 2018, THEORY QUANTUM INFOR, DOI DOI 10.1017/9781316848142; Wilde MM, 2013, P ROY SOC A-MATH PHY, V469, DOI 10.1098/rspa.2013.0259	24	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003051
C	Ahmed, A; Aghasi, A; Hand, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ahmed, Ali; Aghasi, Alireza; Hand, Paul			Blind Deconvolutional Phase Retrieval via Convex Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				TRANSMISSION; LIGHT; CRYSTALLOGRAPHY	We consider the task of recovering two real or complex m-vectors from phaseless Fourier measurements of their circular convolution. Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a nontrivial convex relaxation of the bilinear measurements from convolution. We prove that if the two signals belong to known random subspaces of dimensions k and n, then they can be recovered up to the inherent scaling ambiguity with m > > (k + n) log(2) m phaseless measurements. Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based Rademacher complexity estimates. Additionally, we provide an ADMM implementation of the method and provide numerical experiments that verify the theory.	[Ahmed, Ali] Informat Technol Univ, Dept Elect Engn, Lahore, Pakistan; [Aghasi, Alireza] Georgia State Univ, Dept Business Analyt, Atlanta, GA 30303 USA; [Hand, Paul] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA	University System of Georgia; Georgia State University; Northeastern University	Ahmed, A (corresponding author), Informat Technol Univ, Dept Elect Engn, Lahore, Pakistan.	ali.ahmed@itu.edu.pk; aaghasi@gsu.edu; p.hand@northeastern.edu			NSF [DMS 1464525]	NSF(National Science Foundation (NSF))	PH acknowledges support from NSF DMS 1464525.	Aghasi A, 2017, CONF REC ASILOMAR C, P1622, DOI 10.1109/ACSSC.2017.8335633; Aghasi Alireza, 2017, ARXIV170204342; Azhar AH, 2013, IEEE PHOTONIC TECH L, V25, P171, DOI 10.1109/LPT.2012.2231857; Azhar AH, 2010, IEEE GLOBE WORK, P1052, DOI 10.1109/GLOCOMW.2010.5700095; Bahmani S., 2017, ARXIV170205327; Bahmani S, 2017, PR MACH LEARN RES, V54, P252; Bunk O, 2007, ACTA CRYSTALLOGR A, V63, P306, DOI 10.1107/S0108767307021903; Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004; Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099; Elser Veit, 2017, ARXIV170600399; Fienup C., 1987, IMAGE RECOVERY THEOR, Vvol 231, pp 275; Goldstein Tom, 2018, IEEE T INFORM THEORY; Goodman J, 2008, INTRO FOURIER OPTICS; Koltchinskii V, 2015, INT MATH RES NOTICES, V2015, P12991, DOI 10.1093/imrn/rnv096; Lecue Guillaume, 2017, J MACHINE LEARNING R, V18, P5356; Mendelson S., 2014, C LEARN THEOR, P25; Miao JW, 2008, ANNU REV PHYS CHEM, V59, P387, DOI 10.1146/annurev.physchem.59.032607.093642; MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394; Retamal JRD, 2015, OPT EXPRESS, V23, P33656, DOI 10.1364/OE.23.033656; Roman V., 2012, COMPRESSED SENSING T; van de Geer S, 2013, PROBAB THEORY REL, V157, P225, DOI 10.1007/s00440-012-0455-y	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004057
C	Alemi, AA; Fischer, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Alemi, Alexander A.; Fischer, Ian			GILBO: One Metric to Measure Them All	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a simple, tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO (Generative Information Lower BOund). It offers a data-independent measure of the complexity of the learned latent variable description, giving the log of the effective description length. It is well-defined for both vAEs and GANs. We compute the GILBO for 800 GANs and VAEs each trained on four datasets (MNIsT, FashionmNIST, ciEAR-10 and CelebA) and discuss the results.	[Alemi, Alexander A.; Fischer, Ian] Google AI, San Jose, CA 95136 USA		Alemi, AA (corresponding author), Google AI, San Jose, CA 95136 USA.	alemi@google.com; iansf@google.com						Agakov Felix Vsevolodovich, 2006, THESIS; Alemi Alex, 2017, ICML; [Anonymous], 2018, ICML P MACH LEARN RE; Arora Sanjeev, 2017, CORR; Ba J., 2017, P 3 INT C LEARN REPR; Bousquet Olivier, 2017, C NEUR INF PROC SYST; Chen X, 2016, ADV NEUR IN, V29; Dayan P., 2017, ARXIV PREPRINT ARXIV; Gao Weihao, 2017, NEURAL INFORM PROCES; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heusel M., 2017, ARXIV 1806 08500; Im DJ, 2018, INT C LEARN REPR; Kingma D.P., 2015, INT C LEARN REPR ICL; LANDAUER TK, 1986, COGNITIVE SCI, V10, P477, DOI 10.1207/s15516709cog1004_4; LIPTON Z. C., 2017, PRECISE RECOVERY LAT; Marsh C., 2013, INTRO CONTINUOUS ENT; Talts S., 2018, ARXIV180406788; Tishby N., 2015, ARXIV150302406; van den Oord Aaron, 2019, REPRESENTATION LEARN, P4; Wu Yuxin, 2017, INT C LEARN REPR	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001057
C	Allen-Zhu, Z		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Allen-Zhu, Zeyuan			How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MINIMIZATION; ALGORITHMS	Stochastic gradient descent (SGD) gives an optimal convergence rate when minimizing convex stochastic objectives f(x). However, in terms of making the gradients small, the original SGD does not give an optimal rate, even when f(x) is convex. If f(x) is convex, to find a point with gradient norm epsilon, we design an algorithm SGD3 with a near-optimal rate (O) over tilde(epsilon(-2)), improving the best known rate O(epsilon(-8/3)) of [17]. If f(x) is nonconvex, to find its epsilon-approximate local minimum, we de- sign an algorithm SGD5 with rate (O) over tilde(epsilon(-3.5)), where previously SGD variants only achieve (O) over tilde(epsilon(-4)) [6, 14, 30]. This is no slower than the best known stochastic version of Newton's method in all parameter regimes [27].	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA		Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu						Allen-Zhu Z., 2017, ICML; Allen-Zhu Z., 2017, FOCS; Allen-Zhu Z., 2017, STOC; Allen-Zhu Zeyuan, 2016, NEURIPS; ALLENZHU Z, 2018, NEURIPS; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Bubeck S., 2015, FDN TRENDS MACHINE L; Cohen MB, 2017, ANN IEEE SYMP FOUND, P902, DOI 10.1109/FOCS.2017.88; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Ghadimi S., 2015, MATH PROGRAM, V156, P1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Woodworth B., 2016, NEURIPS; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301017
C	Allen-Zhu, Z; Simchi-Levi, D; Wang, XS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Allen-Zhu, Zeyuan; Simchi-Levi, David; Wang, Xinshang			The Lingering of Gradients: How to Reuse Gradients over Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORK REVENUE MANAGEMENT	Classically, the time complexity of a first-order method is estimated by its number of gradient computations. In this paper, we study a more refined complexity by taking into account the "lingering" of gradients: once a gradient is computed at x(k), the additional time to compute gradients at x(k+1), x(k+2), ... may be reduced. We show how this improves the running time of gradient descent and SVRG. For instance, if the "additional time" scales linearly with respect to the traveled distance, then the "convergence rate" of gradient descent can be improved from 1/T to exp(-T-1/3). On the empirical side, we solve a hypothetical revenue management problem on the Yahoo! Front Page Today Module application with 4.6m users to 10(-6) error (or 10(-12) dual error) using 6 passes of the dataset.	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA; [Simchi-Levi, David; Wang, Xinshang] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu; dslevi@mit.edu; xinshang@mit.edu						Agrawal S, 2015, P 26 ANN ACM SIAM S, P1405; Agrawal S, 2014, OPER RES, V62, P876, DOI 10.1287/opre.2014.1289; Alaei S, 2012, PROC 13 ACM C ELECT, P18; Allen-Zhu Zeyuan, 2016, NEURIPS; Allen-Zhu Zeyuan, 2016, ICML; Chu W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1097; Ciocan DF, 2012, MATH OPER RES, V37, P501, DOI 10.1287/moor.1120.0548; Defazio Aaron, 2014, NEURIPS; Devanur Nikhil R, 2012, P 13 ACM C ELECT COM, P388; Feldman J, 2010, LECT NOTES COMPUT SC, V6346, P182, DOI 10.1007/978-3-642-15775-2_16; Feldman J, 2009, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2009.72; Haeupler Bernhard, 2011, Internet and Network Economics. Proceedings 7th International Workshop, WINE 2011, P170, DOI 10.1007/978-3-642-25510-6_15; Harikandeh R., 2015, PROC NEURAL INF PROC, P2251; Hofmann T., 2015, ADV NEURAL INFORM PR, V28, P2305; Jaillet P, 2014, MATH OPER RES, V39, P624, DOI 10.1287/moor.2013.0621; Jasin S, 2012, MATH OPER RES, V37, P313, DOI 10.1287/moor.1120.0537; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Mahdavi Mehrdad, 2013, ADV NEURAL INFORM PR, P674; Manshadi VH, 2012, MATH OPER RES, V37, P559, DOI 10.1287/moor.1120.0551; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Reiman MI, 2008, MATH OPER RES, V33, P257, DOI 10.1287/moor.1070.0288; SHALEVSHWARTZ S, 2016, ICML; Talluri K, 1998, MANAGE SCI, V44, P1577, DOI 10.1287/mnsc.44.11.1577; Wang Xingjie, 2016, WORKING PAPER; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang L., 2013, P 26 INT C NEUR INF, P980; Zhong WL, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2287, DOI 10.1145/2783258.2788565; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	37	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301025
C	Almanza, M; Chierichetti, F; Panconesi, A; Vattani, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Almanza, Matteo; Chierichetti, Flavio; Panconesi, Alessandro; Vattani, Andrea			A Reduction for Efficient LDA Topic Reconstruction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruction. The main technical idea is to show that the distribution over the documents generated by LDA can be transformed into a distribution for a much simpler generative model in which documents are generated from the same set of topics but have a much simpler structure: documents are single topic and topics are chosen uniformly at random. Furthermore, this reduction is approximation preserving, in the sense that approximate distributions - the only ones we can hope to compute in practice - are mapped into approximate distribution in the simplified world. This opens up the possibility of efficiently reconstructing LDA topics in a roundabout way. Compute an approximate document distribution from the given corpus, transform it into an approximate distribution for the single-topic world, and run a reconstruction algorithm in the uniform, single-topic world - a much simpler task than direct LDA reconstruction. We show the viability of the approach by giving very simple algorithms for a generalization of two notable cases that have been studied in the literature, p-separability and matrix-like topics.	[Almanza, Matteo; Chierichetti, Flavio; Panconesi, Alessandro] Sapienza Univ, Rome, Italy; [Vattani, Andrea] Spiketrap, San Francisco, CA USA	Sapienza University Rome	Almanza, M (corresponding author), Sapienza Univ, Rome, Italy.	almanza@di.uniroma1.it; flavio@di.uniroma1.it; ale@di.uniroma1.it; avattani@cs.ucsd.edu			ERC [DMAP 680153]; "Dipartimenti di Eccellenza 2018-2022" grant; Google Focused Research Award; BiCi - Bertinoro international Center for informatics	ERC(European Research Council (ERC)European Commission); "Dipartimenti di Eccellenza 2018-2022" grant; Google Focused Research Award(Google Incorporated); BiCi - Bertinoro international Center for informatics	Supported in part by the ERC Starting Grant DMAP 680153, and by the "Dipartimenti di Eccellenza 2018-2022" grant awarded to the Dipartimento di Informatica at Sapienza.; Supported in part by the ERC Starting Grant DMAP 680153, by a Google Focused Research Award, and by the "Dipartimenti di Eccellenza 2018-2022" grant awarded to the Dipartimento di Informatica at Sapienza.; Supported in part by the ERC Starting Grant DMAP 680153, by a Google Focused Research Award, by the "Dipartimenti di Eccellenza 2018-2022" grant awarded to the Dipartimento di Informatica at Sapienza, and by BiCi - Bertinoro international Center for informatics.	Alvarez-Melis David, 2016, ICWSM; Anandkumar A., 2013, ADV NEURAL INFORM PR, P1986; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Arora Sanjeev, 2013, JMLR WORKSHOP C P, V28, P280; Bansal T., 2014, ADV NEURAL INFORM PR, P1997; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chierichetti Flavio, 2018, ZENODO, DOI [10.5281/zenodo.1470295, DOI 10.5281/ZENODO.1470295]; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hajjem M, 2017, PROCEDIA COMPUT SCI, V112, P761, DOI 10.1016/j.procs.2017.08.166; Hong L., 2010, P 1 WORKSHOP SOCIAL, P80, DOI DOI 10.1145/1964858.1964870; Li CL, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P165, DOI 10.1145/2911451.2911499; McCallum A. K., 2002, MACHINE LEARNING LAN; Newman David, 2008, NIPS DATASET; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; Sridhar VKR, 2015, P 1 WORKSH VECT SPAC, P192; Weng J., 2010, P 3 ACM INT C WEB SE, DOI [10.1145/1718487.1718520, DOI 10.1145/1718487.1718520]; Yan X., 2013, WWW; Yau Chyi-Kwei, 2018, TENSOR LDA; Zhao WNX, 2011, LECT NOTES COMPUT SC, V6611, P338, DOI 10.1007/978-3-642-20161-5_34	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002042
C	Angell, R; Sheldon, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Angell, Rico; Sheldon, Daniel			Inferring Latent Velocities from Weather Radar Data using Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				WSR-88D; MIGRATION	Archived data from the US network of weather radars hold detailed information about bird migration over the last 25 years, including very high-resolution partial measurements of velocity. Historically, most of this spatial resolution is discarded and velocities are summarized at a very small number of locations due to modeling and algorithmic limitations. This paper presents a Gaussian process (GP) model to reconstruct high-resolution full velocity fields across the entire US. The GP faithfully models all aspects of the problem in a single joint framework, including spatially random velocities, partial velocity measurements, station-specific geometries, measurement noise, and an ambiguity known as aliasing. We develop fast inference algorithms based on the FFT; to do so, we employ a creative use of Laplace's method to sidestep the fact that the kernel of the joint process is non-stationary.	[Angell, Rico; Sheldon, Daniel] Univ Massachusetts, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Angell, R (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.	rangell@cs.umass.edu; sheldon@cs.umass.edu			National Science Foundation [1522054, 1661259]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant Nos. 1522054 and 1661259.	Agiomyrgiannakis Y, 2009, IEEE T AUDIO SPEECH, V17, P775, DOI 10.1109/TASL.2008.2008229; [Anonymous], 1993, DOPPLER RADAR WEATHE; Bahlmann C, 2006, PATTERN RECOGN, V39, P115, DOI 10.1016/j.patcog.2005.05.012; Bergen W. R., 1988, Journal of Atmospheric and Oceanic Technology, V5, P305, DOI 10.1175/1520-0426(1988)005<0305:TATDDA>2.0.CO;2; BREITENBERGER E, 1963, BIOMETRIKA, V50, P81; Buler JJ, 2009, IEEE T GEOSCI REMOTE, V47, P2741, DOI 10.1109/TGRS.2009.2014463; CRUM TD, 1993, B AM METEOROL SOC, V74, P1669, DOI 10.1175/1520-0477(1993)074<1669:TWATWO>2.0.CO;2; Dokter Adriaan M., 2010, J ROYAL SOC INTERFAC; Farnsworth A, 2016, ECOL APPL, V26, P752, DOI 10.1890/15-0023; Fulton RA, 1998, WEATHER FORECAST, V13, P377, DOI 10.1175/1520-0434(1998)013<0377:TWRA>2.0.CO;2; Gao JD, 2004, J APPL METEOROL, V43, P934, DOI 10.1175/1520-0450(2004)043<0934:AVTFDD>2.0.CO;2; Horton KG, 2016, SCI REP-UK, V6, DOI 10.1038/srep21249; Horton Kyle G, ECOLOGY LETT; Insanic E, 2012, IEEE T GEOSCI REMOTE, V50, P553, DOI 10.1109/TGRS.2011.2161766; Johnson JT, 1998, WEATHER FORECAST, V13, P263, DOI 10.1175/1520-0434(1998)013<0263:TSCIAT>2.0.CO;2; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kunz TH, 2008, INTEGR COMP BIOL, V48, P1, DOI [10.1093/icb/icn037, 10.1093/icb/icn030]; La Sorte F. A., 2015, ROYAL SOC OPEN SCI, V2, P1; La Sorte FA, 2015, J ANIM ECOL, V84, P1202, DOI 10.1111/1365-2656.12376; OHSMANN M, 1995, LINEAR ALGEBRA APPL, V231, P181, DOI 10.1016/0024-3795(94)00043-3; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; RAY PS, 1983, J CLIM APPL METEOROL, V22, P1444, DOI 10.1175/1520-0450(1983)022<1444:MDRND>2.0.CO;2; RoyChowdhury Aruni, 2016, CVPR WORKSH PERC VIS, P1; Shamoun-Baranes J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0160106; Sheldon Daniel R., 2013, AAAI; Stein ML, 2013, ANN APPL STAT, V7, P1162, DOI 10.1214/13-AOAS627; Stroud JR, 2017, J COMPUT GRAPH STAT, V26, P108, DOI 10.1080/10618600.2016.1152970; Tabary P, 2001, J ATMOS OCEAN TECH, V18, P875, DOI 10.1175/1520-0426(2001)018<0875:RTROTW>2.0.CO;2; Van Doren BM, 2017, P NATL ACAD SCI USA, V114, P11175, DOI 10.1073/pnas.1708574114; Wilson A.G., 2014, COVARIANCE KERNELS F; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775	31	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003053
C	Aoi, MC; Pillow, JW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Aoi, Mikio C.; Pillow, Jonathan W.			Model-based targeted dimensionality reduction for neuronal population data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Summarizing high-dimensional data using a small number of parameters is a ubiquitous first step in the analysis of neuronal population activity. Recently developed methods use "targeted" approaches that work by identifying multiple, distinct low-dimensional subspaces of activity that capture the population response to individual experimental task variables, such as the value of a presented stimulus or the behavior of the animal. These methods have gained attention because they decompose total neural activity into what are ostensibly different parts of a neuronal computation. However, existing targeted methods have been developed outside of the confines of probabilistic modeling, making some aspects of the procedures ad hoc, or limited in flexibility or interpretability. Here we propose a new model-based method for targeted dimensionality reduction based on a probabilistic generative model of the population response data. The low-dimensional structure of our model is expressed as a low-rank factorization of a linear regression model. We perform efficient inference using a combination of expectation maximization and direct maximization of the marginal likelihood. We also develop an efficient method for estimating the dimensionality of each subspace. We show that our approach outperforms alternative methods in both mean squared error of the parameter estimates, and in identifying the correct dimensionality of encoding using simulated data. We also show that our method provides more accurate inference of low-dimensional subspaces of activity than a competing algorithm, demixed PCA.	[Aoi, Mikio C.; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA	Princeton University	Aoi, MC (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	maoi@princeton.edu; pillow@princeton.edu			Simons Foundation [SCGB AWD1004351, AWD543027]; NIH [R01EY017366, R01NS104899]; U19 NIH-NINDS BRAIN Initiative Award [NS104648-01]	Simons Foundation; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); U19 NIH-NINDS BRAIN Initiative Award	This work was supported by grants from the Simons Foundation (SCGB AWD1004351 and AWD543027), the NIH (R01EY017366, R01NS104899) and a U19 NIH-NINDS BRAIN Initiative Award (NS104648-01).	Afshar A, 2011, NEURON, V71, P555, DOI 10.1016/j.neuron.2011.05.047; Bishop C.M, 2006, PATTERN RECOGN; Brody CD, 2003, CEREB CORTEX, V13, P1196, DOI 10.1093/cercor/bhg100; Churchland MM, 2007, CURR OPIN NEUROBIOL, V17, P609, DOI 10.1016/j.conb.2007.11.001; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Harvey CD, 2012, NATURE, V484, P62, DOI 10.1038/nature10918; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1; Kobak D, 2016, ELIFE, V5, DOI 10.7554/eLife.10989; Lawrence N., 2005, J MACHINE LEARNING R, V6, P1816; LIU CH, 1994, BIOMETRIKA, V81, P633; Machens CK, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00126; Machens CK, 2010, J NEUROSCI, V30, P350, DOI 10.1523/JNEUROSCI.3276-09.2010; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Morcos Ari S, 2016, NATURE NEUROSCIENCE; Parthasarathy A, 2017, NAT NEUROSCI, V20, P1770, DOI 10.1038/s41593-017-0003-2; Seely JS, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005164; Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008; Zhao Y., 2016, ARXIV160403053	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA	31274967				2022-12-19	WOS:000461852001025
C	Arbel, M; Sutherland, DJ; Binkowski, M; Gretton, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Arbel, Michael; Sutherland, Dougal J.; Binkowski, Mikolaj; Gretton, Arthur			On gradient regularizers for MMD GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on 160 x 160 CelebA and 64 x 64 unconditional ImageNet.	[Arbel, Michael; Sutherland, Dougal J.; Gretton, Arthur] UCL, Gatsby Computat Neurosci Unit, London, England; [Binkowski, Mikolaj] Imperial Coll London, Dept Math, London, England	University of London; University College London; Imperial College London	Arbel, M (corresponding author), UCL, Gatsby Computat Neurosci Unit, London, England.	michael.n.arbel@gmail.com; dougal@gmail.com; mikbinkowski@gmail.com; arthur.gretton@gmail.com	Arbel, Michael/AAJ-5905-2020	Gretton, Arthur/0000-0003-3169-7624				Amos B., 2017, ICML; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Barratt S., 2018, ARXIV180101973; Bellemare MG, 2017, ARXIV; Berthelot D., 2017, BEGAN BOUNDARY EQUIL; Bottou L., 2017, ICML; Bottou L, 2018, LECT NOTES ARTIF INT, V11100, P229, DOI 10.1007/978-3-319-99492-5_11; Bounliphone W., 2016, ICLR; Bousquet O., 2004, NIPS; Brock A., 2017, ICLR; Dougal J., 2018, ICLR; Dziugaite G. K., 2015, UAI; Genevay A., 2018, AISTATS; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A., 2012, JMLR, V13; Gulrajani Ishaan, 2017, NIPS; Gungor A., 2007, INT J CONT MATH SCI; Heusel M., 2017, NIPS; Huang G., 2018, ARXIV180607755; Huang Xun, 2018, ECCV; Jin Yanghua, 2017, ARXIV170805509; Karras Tero, 2018, ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Li C., 2017, NIPS; Li Y., 2015, ICML 2015; Liu Z., 2015, ICCV; Mescheder Lars, 2018, ICML; Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296; Miyato Takeru, 2018, ICLR; Mroueh Y., 2018, ARXIV180512062; Mroueh Y., 2017, NIPS, P1; Mroueh Y., 2018, P INT C LEARN REPR; Nowozin Sebastian, 2016, P ADV NEURAL INFORM; Radford A., 2016, ICLR; Retherford J. R., 1978, B AM MATH SOC, V84, P681; Roth Kevin, 2017, NIPS; Russakovsky O., 2014, ARXIV, P1, DOI [10.48550/arXiv.1409.0575, DOI 10.48550/ARXIV.1409.0575]; Salimans Tim, 2016, ADV NEURAL INFORM PR; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Sriperumbudur B. K., 2009, NIPS; Sriperumbudur B, 2016, BERNOULLI, V22, P1839, DOI 10.3150/15-BEJ713; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Steinwart I., 2008, SUPPORT VECTOR MACHI; Sutherland D. J., 2017, ICLR; Szegedy C., 2016, CVPR; Unterthiner T., 2018, ICLR; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Weed J., BERNOULLI; Wendland H., 2005, SCATTERED DATA APPRO; Zaremba W., 2013, NIPS	54	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001026
C	Arora, R; Braverman, V; Upadhyay, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Arora, Raman; Braverman, Vladimir; Upadhyay, Jalaj			Differentially Private Robust Low-Rank Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we study the following robust low-rank matrix approximation problem: given a matrix A is an element of R-nxd ,find a rank-k matrix M, while satisfying differential privacy, such that parallel to A - M parallel to(p )<= alpha. OPTk(A) + tau, where parallel to B parallel to(p) is the = entry-wise l(p)-norm of B and OPTk(A) := min(rank(X)<= k) parallel to A - X parallel to(p). It is well known that low-rank approximation w.r.t. entrywise l(p)-norm, for p is an element of [1, 2), yields robustness to gross outliers in the data. We propose an algorithm that guarantees alpha = (O) over tilde (k(2)), tau = (O) over tilde (k(2)(n + kd/epsilon), runs in (O) over tilde((n+d)poly k) time and uses O(k (n+d) log k) space. We study extensions to the streaming setting where entries of the matrix arrive in an arbitrary order and output is produced at the very end or continually. We also study the related problem of differentially private robust principal component analysis (PCA), wherein we return a rank-k projection matrix Pi such that parallel to A - A Pi parallel to(p) <= alpha . OPTk(A) + tau.	[Arora, Raman; Braverman, Vladimir; Upadhyay, Jalaj] Johns Hopkins Univ, Baltimore, MD 21201 USA	Johns Hopkins University	Arora, R (corresponding author), Johns Hopkins Univ, Baltimore, MD 21201 USA.	arora@cs.jhu.edu; vova@cs.jhu.edu; jalaj@jhu.edu			NSF BIGDATA [IIS-1838139, IIS-1546482]; NSF Career [CCF-1652257]; ONR Award [N00014-18-1-2364]	NSF BIGDATA; NSF Career(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR Award	This research was supported in part by NSF BIGDATA grant IIS-1546482, NSF BIGDATA grant IIS-1838139, NSF Career CCF-1652257, and ONR Award N00014-18-1-2364.	Achlioptas D, 2005, LECT NOTES COMPUT SC, V3559, P458, DOI 10.1007/11503415_31; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; [Anonymous], WALL STREET J; Blocki J, 2012, ANN IEEE SYMP FOUND, P410, DOI 10.1109/FOCS.2012.67; Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148; Brooks JP, 2013, COMPUT STAT DATA AN, V61, P83, DOI 10.1016/j.csda.2012.11.007; Bun Mark, 2017, ARXIV171104740; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Drineas Petros, 2002, P 34 ACM S THEORY CO, P82, DOI [10.1145/509907.509922, DOI 10.1145/509907.509922]; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P11, DOI 10.1145/2591796.2591883; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ACM S THEORY COMPUT, P715; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Gillis Nicolas, 2015, ARXIV150909236; Hardt M, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1255; Hardt M, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P331; Jiang Wuxuan, 2015, ARXIV151105680; Kapralov M., 2013, SODA, V5, P1; Ke QF, 2005, PROC CVPR IEEE, P739; Kim E, 2015, IEEE T NEUR NET LEAR, V26, P237, DOI 10.1109/TNNLS.2014.2312535; Markopoulos Panos P, 2017, IEEE T SIGNAL PROCES; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35; Sohler C, 2011, ACM S THEORY COMPUT, P755; Song Z, 2017, ACM S THEORY COMPUT, P688, DOI 10.1145/3055399.3055431; Upadhyay J, 2018, ADV NEUR IN, V31; Upadhyay J, 2013, LECT NOTES COMPUT SC, V8269, P276, DOI 10.1007/978-3-642-42033-7_15; Upadhyay Jalaj, 2014, ARXIV14095414; Zolotarev VladimirM., 1986, ONE DIMENSIONAL STAB, V65	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304017
C	Arora, R; Dinitz, M; Marinov, TV; Mohri, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Arora, Raman; Dinitz, Michael; Marinov, Teodor V.; Mohri, Mehryar			Policy Regret in Repeated Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The notion of policy regret in online learning is a well defined performance measure for the common scenario of adaptive adversaries, which more traditional quantities such as external regret do not take into account. We revisit the notion of policy regret and first show that there are online learning settings in which policy regret and external regret are incompatible: any sequence of play that achieves a favorable regret with respect to one definition must do poorly with respect to the other. We then focus on the game-theoretic setting where the adversary is a self-interested agent. In that setting, we show that external regret and policy regret are not in conflict and, in fact, that a wide class of algorithms can ensure a favorable regret with respect to both definitions, so long as the adversary is also using such an algorithm. We also show that the sequence of play of no-policy regret algorithms converges to a policy equilibrium, a new notion of equilibrium that we introduce. Relating this back to external regret, we show that coarse correlated equilibria, which no-external regret players converge to, are a strict subset of policy equilibria. Thus, in game-theoretic settings, every sequence of play with no external regret also admits no policy regret, but the converse does not hold.	[Arora, Raman; Dinitz, Michael; Marinov, Teodor V.] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA; [Mohri, Mehryar] Courant Inst & Google Res, New York, NY 10012 USA	Johns Hopkins University; Google Incorporated	Arora, R (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA.	arora@cs.jhu.edu; mdinitz@cs.jhu.edu; tmarino2@jhu.edu; mohri@cims.nyu.edu		Dinitz, Michael/0000-0002-2632-966X	NSF BIGDATA grant [IIS-1546482, IIS-1838139]; NSF [CCF-1535987, IIS-1618662, CCF-1464239, AITF CCF-1535887]	NSF BIGDATA grant; NSF(National Science Foundation (NSF))	This work was supported in part by NSF BIGDATA grant IIS-1546482, NSF BIGDATA grant IIS-1838139, NSF CCF-1535987, NSF IIS-1618662, NSF CCF-1464239, and NSF AITF CCF-1535887.	Allen-Zhu Z., 2016, ADV NEURAL INFORM PR, P974; Arora Raman, 2012, P UNC ART INT UAI; Arora Raman, 2012, P INT C MACH LEARN I; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Blum A, 2007, J MACH LEARN RES, V8, P1307; Dar EE, 2009, ACM S THEORY COMPUT, P523; Dekel O, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P459, DOI 10.1145/2591796.2591868; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595; Fudenberg D, 1999, GAME ECON BEHAV, V29, P104, DOI 10.1006/game.1999.0705; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hazan Elad, 2008, NIPS, P625; Heidari H., 2016, P 25 INT JOINT C ART, P1562; Kakade Sham M., 2003, THESIS; Koren T., 2017, ADV NEURAL INFORM PR, P4119; Koren T., 2017, C LEARNING THEORY, P1242; Merhav N, 2002, IEEE T INFORM THEORY, V48, P1947, DOI 10.1109/TIT.2002.1013135; Mohri Mehryar, 2014, ADV NEURAL INFORM PR, V27, P1314; NEU G, 2010, ADV NEURAL INFORM PR, P1804; de Farias DP, 2006, J ACM, V53, P762, DOI 10.1145/1183907.1183911; Roughgarden T, 2015, J ACM, V62, DOI 10.1145/2806883; Saha Ankan, 2012, ARXIV12116158; Stoltz G, 2007, GAME ECON BEHAV, V59, P187, DOI 10.1016/j.geb.2006.04.007; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Yang S, 2017, ADV NEURAL INFORM PR, P5220	28	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001029
C	Auricchio, G; Gualandi, S; Veneroni, M; Bassetti, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Auricchio, Gennaro; Gualandi, Stefano; Veneroni, Marco; Bassetti, Federico			Computing Kantorovich-Wasserstein Distances on d-dimensional histograms using (d+1)-partite graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EARTH-MOVERS-DISTANCE; MINIMUM	This paper presents a novel method to compute the exact Kantorovich-Wasserstein distance between a pair of d-dimensional histograms having n bins each. We prove that this problem is equivalent to an uncapacitated minimum cost flow problem on a (d + 1)-partite graph with (d + 1)n nodes and dn d+1/d arcs, whenever the cost is separable along the principal d-dimensional directions. We show numerically the benefits of our approach by computing the Kantorovich-Wasserstein distance of order 2 among two sets of instances: gray scale images and d-dimensional bio medical histograms. On these types of instances, our approach is competitive with state-of-the-art optimal transport algorithms.	[Auricchio, Gennaro; Gualandi, Stefano; Veneroni, Marco] Univ Pavia, Dipartimento Matemat F Casorati, Pavia, Italy; [Bassetti, Federico] Politecnico Milano, Dipartimento Matemat, Milan, Italy	University of Pavia; Polytechnic University of Milan	Auricchio, G (corresponding author), Univ Pavia, Dipartimento Matemat F Casorati, Pavia, Italy.	gennaro.auricchio01@universitadipavia.it; stefano.gualandi@unipv.it; marco.veneroni@unipv.it; federico.bassetti@polimi.it	Gualandi, Stefano/A-6581-2011	Gualandi, Stefano/0000-0002-2111-3528	Italian Ministry of Education, University and Research (MIUR): Dipartimenti di Eccellenza Program (2018-2022) - Dept. of Mathematics "F. Casorati", University of Pavia; PRIN 2015 Modern Bayesian nonparametric methods [2015SNS29B-002]	Italian Ministry of Education, University and Research (MIUR): Dipartimenti di Eccellenza Program (2018-2022) - Dept. of Mathematics "F. Casorati", University of Pavia; PRIN 2015 Modern Bayesian nonparametric methods	This research was partially supported by the Italian Ministry of Education, University and Research (MIUR): Dipartimenti di Eccellenza Program (2018-2022) - Dept. of Mathematics "F. Casorati", University of Pavia.; The last author's research is partially supported by "PRIN 2015. 2015SNS29B-002. Modern Bayesian nonparametric methods".	Aghaeepour N, 2013, NAT METHODS, V10, P228, DOI [10.1038/NMETH.2365, 10.1038/nmeth.2365]; Ahuja R. K., 1988, NETWORK FLOWS THEORY; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Ambrosio L., 2008, LECT MATH ETH ZURICH, VSecond; [Anonymous], 2014, ICML; Araya M., 2015, P ADV NEUR INF PROC, P2053; Bassetti F, 2006, THEOR PROBAB APPL+, V50, P171, DOI 10.1137/S0040585X97981664; Bassetti F., 2018, ARXIV180400445; Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001; Bernas T, 2008, CYTOM PART A, V73A, P715, DOI 10.1002/cyto.a.20586; Bottou L., 2017, ARXIV170107875STATML; Chizat L., 2016, ARXIV160705816; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Flood M.M., 1953, PAC J MATH, V3, P369; Goldberg AV, 1989, TECHNICAL REPORT; Kovacs P, 2015, OPTIM METHOD SOFTW, V30, P94, DOI 10.1080/10556788.2014.895828; Liero M, 2018, INVENT MATH, V211, P969, DOI 10.1007/s00222-017-0759-8; Ling H, 2007, IEEE T PATTERN ANAL, V29, P840, DOI 10.1109/TPAMI.2007.1058; ORLIN JB, 1993, OPER RES, V41, P338, DOI 10.1287/opre.41.2.338; Orlova DY, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151859; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Peyre G., 2018, 180300567 ARXIV; Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Schrieber J, 2017, IEEE ACCESS, V5, P271, DOI 10.1109/ACCESS.2016.2639065; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Solomon Justin, 2018, 180107745 ARXIV; Vershik AM, 2013, MATH INTELL, V35, P1, DOI 10.1007/s00283-013-9380-x; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5	31	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000031
C	Avalos-Fernandez, M; Nock, R; Ong, CS; Rouar, J; Sun, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Avalos-Fernandez, Marta; Nock, Richard; Ong, Cheng Soon; Rouar, Julien; Sun, Ke			Representation Learning of Compositional Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of learning a low dimensional representation for compositional data. Compositional data consists of a collection of nonnegative data that sum to a constant value. Since the parts of the collection are statistically dependent, many standard tools cannot be directly applied. Instead, compositional data must be first transformed before analysis. Focusing on principal component analysis (PCA), we propose an approach that allows low dimensional representation learning directly from the original data. Our approach combines the benefits of the log-ratio transformation from compositional data analysis and exponential family PCA. A key tool in its derivation is a generalization of the scaled Bregman theorem, that relates the perspective transform of a Bregman divergence to the Bregman divergence of a perspective transform and a remainder conformal divergence. Our proposed approach includes a convenient surrogate (upper bound) loss of the exponential family PCA which has an easy to optimize form. We also derive the corresponding form for nonlinear autoencoders. Experiments on simulated data and microbiome data show the promise of our method.	[Avalos-Fernandez, Marta; Rouar, Julien] Univ Bordeaux, Bordeaux, France; [Nock, Richard; Ong, Cheng Soon; Sun, Ke] Data61, Sydney, NSW, Australia; [Nock, Richard; Ong, Cheng Soon] Australian Natl Univ, Canberra, ACT, Australia; [Nock, Richard] Univ Sydney, Sydney, NSW, Australia	UDICE-French Research Universities; Universite de Bordeaux; Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University; University of Sydney	Avalos-Fernandez, M (corresponding author), Univ Bordeaux, Bordeaux, France.	marta.avalos-fernande@u-bordeaux.fr; richard.nock@data61.csiro.au; cheng.ong@data61.csiro.au; julien.rouar@u-bordeaux.fr; ke.sun@data61.csiro.au	SUN, Ke/X-8444-2019; AVALOS, Marta/T-5017-2019	SUN, Ke/0000-0001-6263-7355; 				AITCHISON J, 1982, J ROY STAT SOC B MET, V44, P139; Aitchison J, 2005, MATH GEOL, V37, P829, DOI 10.1007/s11004-005-7383-7; AITCHISON J, 1983, BIOMETRIKA, V70, P57; Aitchison J, 1994, INST MATH S, V24, P73, DOI 10.1214/lnms/1215463786; Aitchison J., 1986, MONOGRAPHS STAT APPL; Amari S.-I., 2016, INFORM GEOMETRY ITS; Barndorff-Nielsen O., 1978, INFORM EXPONENTIAL F; Boyd S, 2004, CONVEX OPTIMIZATION; Chiquet J., 2018, ANN APPL STAT; Clevert D.-A., 2016, 4 ICLR; Collins M., 2002, NIPS 15; Egozcue JJ, 2003, MATH GEOL, V35, P279, DOI 10.1023/A:1023818214614; Gloor GB, 2016, ANN EPIDEMIOL, V26, P322, DOI 10.1016/j.annepidem.2016.03.003; Greenacre M. J., 2018, COMPOSITIONAL DATA A; Hugerth LW, 2017, FRONT MICROBIOL, V8, DOI 10.3389/fmicb.2017.01561; Lahti L., 2017, TOOLS MICROBIOME ANA; Lahti L, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5344; Landgraf AJ., 2015, THESIS; Lovell D., 2010, EP10994 CSIRO; Marechal P, 2005, J OPTIMIZ THEORY APP, V126, P175, DOI 10.1007/s10957-005-2667-0; Marechal P., 2005, J OPTIMIZATION THEOR, V126, P375; Martin-Fernandez JA, 2018, MATH GEOSCI, V50, P273, DOI 10.1007/s11004-017-9712-z; Nock R., 2016, IEEE T IT, V62, P1; Nock R., 2016, ADV NEURAL INFORM PR, P19; O'Keefe SJD, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7342; Paliy O, 2016, MOL ECOL, V25, P1032, DOI 10.1111/mec.13536; PawlowskyGlahn V, 2011, COMPOSITIONAL DATA ANALYSIS: THEORY AND APPLICATIONS, P1, DOI 10.1002/9781119976462; Reid G., 2016, CAN J MICROBIOL, V12, P1; Sun K, 2014, PR MACH LEARN RES, V32, P1; Tolosona-Delgado R., 2007, TECHNICAL REPORT; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579	32	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001024
C	Bach, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bach, Francis			Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				STOCHASTIC-DOMINANCE	We consider the minimization of submodular functions subject to ordering constraints. We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance. We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th, first, or higher order oracles; these algorithms also lead to improvements without isotonic constraints. Finally, our experiments show that non-convex loss functions can be much more robust to outliers for isotonic regression, while still being solvable in polynomial time.	[Bach, Francis] PSL Res Univ, Dept Informat, Ecole Normale Super, INRIA, Paris, France	Inria; PSL Research University Paris; Ecole Normale Superieure (ENS)	Bach, F (corresponding author), PSL Res Univ, Dept Informat, Ecole Normale Super, INRIA, Paris, France.	francis.bach@ens.fr			European Research Council [SEQUOIA 724063]	European Research Council(European Research Council (ERC)European Commission)	We acknowledge support the European Research Council (grant SEQUOIA 724063).	[Anonymous], 2009, GEOMETRY CUTS METRIC; Bach, 2018, MATH PROGRAM, V175, P1; Bach F., 2013, FDN TRENDS MACHINE L, V6; Bertsekas D.P., 2016, NONLINEAR PROGRAMMIN, V3; BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; CHAMBOLLE A, 2015, SIAM J COMPUT MATH, V1, P29, DOI DOI 10.5802/smai-jcm.3; Chen Xi, 2015, 150901877 ARXIV; Chen YN, 2016, J R STAT SOC B, V78, P729, DOI 10.1111/rssb.12137; Dentcheva D, 2004, OPTIMIZATION, V53, P583, DOI 10.1080/02331930412331327148; GALLO G, 1989, SIAM J COMPUT, V18, P30, DOI 10.1137/0218003; Hampel F. R., 2011, ROBUST STAT APPROACH, V196; Hochbaum DS, 2008, OPER RES, V56, P992, DOI 10.1287/opre.1080.0524; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Jegelka S., 2013, ADV NEURAL INFORM PR; Kakade S. M, 2011, ADV NEURAL INFORM PR; Kim SY, 2003, COMPUT OPTIM APPL, V26, P143, DOI 10.1023/A:1025794313696; LEHMANN EL, 1955, ANN MATH STAT, V26, P399, DOI 10.1214/aoms/1177728487; LEVY H, 1992, MANAGE SCI, V38, P555, DOI 10.1287/mnsc.38.4.555; Lorentz G. G., 1953, AM MATH MON, V60, P176; Luss R, 2014, J COMPUT GRAPH STAT, V23, P192, DOI 10.1080/10618600.2012.741550; Nesterov Y., 2018, APPL OPTIMIZATION; Rudin W., 1986, REAL COMPLEX ANAL, V3; SLEATOR DD, 1983, J COMPUT SYST SCI, V26, P362, DOI 10.1016/0022-0000(83)90006-5; Spouge J, 2003, J OPTIMIZ THEORY APP, V117, P585, DOI 10.1023/A:1023901806339; Stout QF, 2013, ALGORITHMICA, V66, P93, DOI 10.1007/s00453-012-9628-4; Tarjan R, 2006, LECT NOTES COMPUT SC, V4168, P612; TOPKIS DM, 1978, OPER RES, V26, P305, DOI 10.1287/opre.26.2.305; Yu YL, 2016, J PHYS CONF SER, V699, DOI 10.1088/1742-6596/699/1/012016	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300001
C	Bakshi, A; Woodruff, DP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bakshi, Ainesh; Woodruff, David P.			Sublinear Time Low-Rank Approximation of Distance Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	Let P = {p(1), p(2), . . . p(n)} and Q = {q(1),q(2 ). . . q(m)} be two point sets in an arbitrary metric space. Let A represent the m x n pairwise distance matrix with A(i,j) = d(p(i) , q(j)). Such distance matrices are commonly computed in software packages and have applications to learning image manifolds, handwriting recognition, and multi-dimensional unfolding, among other things. In an attempt to reduce their description size, we study low rank approximation of such matrices. Our main result is to show that for any underlying distance metric d, it is possible to achieve an additive error low rank approximation in sublinear time. We note that it is provably impossible to achieve such a guarantee in sublinear time for arbitrary matrices A, and our proof exploits special properties of distance matrices. We develop a recursive algorithm based on additive projection-cost preserving sampling. We then show that in general, relative error approximation in sublinear time is impossible for distance matrices, even if one allows for bicriteria solutions. Additionally, we show that if P = Q and d is the squared Euclidean distance, which is not a metric but rather the square of a metric, then a relative error bicriteria solution can be found in sublinear time. Finally, we empirically compare our algorithm with the singular value decomposition (SVD) and input sparsity time algorithms. Our algorithm is several hundred times faster than the SVD, and about 8-20 times faster than input sparsity methods on real-world and and synthetic datasets of size 10(8). Accuracy-wise, our algorithm is only slightly worse than that of the SVD (optimal) and input-sparsity time algorithms.	[Bakshi, Ainesh; Woodruff, David P.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Bakshi, A (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	abakshi@cs.cmu.edu; dwoodruf@cs.cmu.edu			National Science Foundation [CCF-1815840]	National Science Foundation(National Science Foundation (NSF))	The authors thank partial support from the National Science Foundation under Grant No. CCF-1815840. Part of this work was done while the author was visiting the Simons Institute for the Theory of Computing.	Bourgain J, 2015, ACM S THEORY COMPUT, P499, DOI 10.1145/2746539.2746541; Cattral R., 2002, RECENT ADV COMPUT CO, V1, P296; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Cohen Michael B, 2016, P 27 ANN ACM SIAM S, P278; Demaine ED, 2009, COMP GEOM-THEOR APPL, V42, P429, DOI 10.1016/j.comgeo.2008.04.005; Dokmanic I, 2015, IEEE SIGNAL PROC MAG, V32, P12, DOI 10.1109/MSP.2015.2398954; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Guyon I., 2005, ADV NEURAL INFORM PR, V17, P545; Jain Viren, 2004, DEP PAPERS CIS; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Musco C, 2017, ANN IEEE SYMP FOUND, P672, DOI 10.1109/FOCS.2017.68; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Weinberger KQ, 2004, PROC CVPR IEEE, P988	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303075
C	Balcan, MF; Dick, T; White, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Balcan, Maria-Florina; Dick, Travis; White, Colin			Data-Driven Clustering via Parameterized Lloyd's Families	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Algorithms for clustering points in metric spaces is a long-studied area of research. Clustering has seen a multitude of work both theoretically, in understanding the approximation guarantees possible for many objective functions such as k-median and k-means clustering, and experimentally, in finding the fastest algorithms and seeding procedures for Lloyd's algorithm. The performance of a given clustering algorithm depends on the specific application at hand, and this may not be known up front. For example, a "typical instance" may vary depending on the application, and different clustering heuristics perform differently depending on the instance. In this paper, we define an infinite family of algorithms generalizing Lloyd's algorithm, with one parameter controlling the initialization procedure, and another parameter controlling the local search procedure. This family of algorithms includes the celebrated k-means++ algorithm, as well as the classic farthest-first traversal algorithm. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal clustering algorithm from the class. We show the best parameters vary significantly across datasets such as MNIST, CIFAR, and mixtures of Gaussians. Our learned algorithms never perform worse than k-means++, and on some datasets we see significant improvements.	[Balcan, Maria-Florina; Dick, Travis; White, Colin] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Balcan, MF (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	ninamf@cs.cmu.edu; tdick@cs.cmu.edu; crwhite@cs.cmu.edu			NSF [CCF-1535967, IIS-1618714]; Amazon Research Award; Microsoft Research Faculty Fellowship; National Defense Science & Engineering Graduate (NDSEG) fellowship	NSF(National Science Foundation (NSF)); Amazon Research Award; Microsoft Research Faculty Fellowship(Microsoft); National Defense Science & Engineering Graduate (NDSEG) fellowship	This work was supported in part by NSF grants CCF-1535967, IIS-1618714, an Amazon Research Award, a Microsoft Research Faculty Fellowship, a National Defense Science & Engineering Graduate (NDSEG) fellowship, and by the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program.	Ahmadian  Sara, 2017, P ANN S FDN COMP SCI; Arai  Kohei, 2007, REPORTS FS ENG, V36, P25; Arthur D., 2007, P 18 ANN ACMS S, P1027, DOI DOI 10.1145/1283383.1283494; Arya V, 2004, SIAM J COMPUT, V33, P544, DOI 10.1137/S0097539702416402; Ashtiani H, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P82; Balcan M., 2017, C LEARN THEOR, P213; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Byrka Jaroslaw, 2015, P 26 ANN ACM SIAM S, P737, DOI [10.1137/1.9781611973730.50, DOI 10.1137/1.9781611973730.50]; Charikar M., 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P1, DOI 10.1145/301250.301257; Cohen MB, 2016, ACM S THEORY COMPUT, P9, DOI 10.1145/2897518.2897647; Dasgupta S, 2005, J COMPUT SYST SCI, V70, P555, DOI 10.1016/j.jcss.2004.10.006; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Friedman J., 2015, PACKAGE GLASSO; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; Higgs RE, 1997, J CHEM INF COMP SCI, V37, P861, DOI 10.1021/ci9702858; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kaufman L., 2009, FINDING GROUPS DATA, V344; Kobren  Ari, 2017, P ANN C KNOWL DISC D; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548; Ostrovsky R, 2012, J ACM, V59, DOI 10.1145/2395116.2395117; Pelleg D., 1999, PROC 5 ACM SIGKDD IN, P277, DOI [10.1145/312129.312248, DOI 10.1145/312129.312248]; Pena JM, 1999, PATTERN RECOGN LETT, V20, P1027, DOI 10.1016/S0167-8655(99)00069-0; Pruitt KD, 2012, NUCLEIC ACIDS RES, V40, pD130, DOI 10.1093/nar/gkr1079; Raina R, 2007, 24 ANN INT C MACH LE, V227, P759, DOI [10.1145/1273496.1273592, DOI 10.1145/1273496.1273592]; Wenhao Jiang, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P789, DOI 10.1007/978-3-642-33486-3_50; Yang Q., 2009, P JOINT C 47 ANN M A, P1	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005024
C	Baldi, P; Vershynin, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Baldi, Pierre; Vershynin, Roman			Neuronal Capacity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We define the capacity of a learning machine to be the logarithm of the number (or volume) of the functions it can implement. We review known results, and derive new results, estimating the capacity of several neuronal models: linear and polynomial threshold gates, linear and polynomial threshold gates with constrained weights (binary weights, positive weights), and ReLU neurons. We also derive some capacity estimates and bounds for fully recurrent networks, as well as feedforward networks.	[Baldi, Pierre] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA; [Vershynin, Roman] Univ Calif Irvine, Dept Math, Irvine, CA 92697 USA	University of California System; University of California Irvine; University of California System; University of California Irvine	Baldi, P (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.	pfbaldi@uci.edu; rvershyn@uci.edu			NSF [1839429]; DARPA [D17AP00002]; AFOSR [FA9550-18-1-0031]	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	Work in part supported by grants NSF 1839429 and DARPA D17AP00002 to PB, and AFOSR FA9550-18-1-0031 to RV.	Anthony M, 2001, DISCRETE MATH NEURAL, V8; ASPNES J, 1994, COMBINATORICA, V14, P135, DOI 10.1007/BF01215346; BALDI P, 1988, IEEE T INFORM THEORY, V34, P523, DOI 10.1109/18.6032; Baldi P., 2018, CAPACITY NEURAL NETW; Baldi P., 2018, ARXIV180310868; Chow C-K, 1961, P S SWITCH CIRC THEO, P34; COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137; MUROGA S, 1965, IEEE TRANS ELECTRON, VEC14, P136, DOI 10.1109/PGEC.1965.263958; O'Donnell R, 2008, J COMPUT SYST SCI, V74, P298, DOI 10.1016/j.jcss.2007.06.021; O'Donnell R, 2010, COMBINATORICA, V30, P327, DOI 10.1007/s00493-010-2173-3; Saks M., 1993, SLICING HYPERCUBE, P211; WANG C, 1991, DISCRETE APPL MATH, V31, P51, DOI 10.1016/0166-218X(91)90032-R; Zuev YA., 1989, SOV MATH DOKL, V39, P512; Zuev Yu. A., 1991, DISKRET MAT, V3, P47	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002029
C	Banner, R; Hubara, I; Hoffer, E; Soudry, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Banner, Ron; Hubara, Itay; Hoffer, Elad; Soudry, Daniel			Scalable Methods for 8-bit Training of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase, i.e. after the network has been trained. Extensive research in the field suggests many different quantization schemes. Still, the number of bits required, as well as the best quantization scheme, are yet unknown. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision. Armed with this knowledge, we quantize the model parameters, activations and layer gradients to 8-bit, leaving at a higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied. To the best of the authors' knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset.	[Banner, Ron] Intel, AIPG, Santa Clara, CA 95054 USA; [Hubara, Itay; Hoffer, Elad; Soudry, Daniel] Technion Israel Inst Technol, Haifa, Israel	Intel Corporation; Technion Israel Institute of Technology	Banner, R (corresponding author), Intel, AIPG, Santa Clara, CA 95054 USA.	ron.banner@intel.com; itayhubara@gmail.com; elad.hoffer@gmail.com; daniel.soudry@gmail.com			Israel Science Foundation [31/1031]; Taub foundation	Israel Science Foundation(Israel Science Foundation); Taub foundation	This research was supported by the Israel Science Foundation (grant No. 31/1031), and by the Taub foundation. A Titan Xp used for this research was donated by the NVIDIA Corporation. The authors are pleased to acknowledge that the work reported in this paper was substantially performed at Intel Artificial Intelligence Products Group (AIPG).	Benoit J., 2017, GEMMLOWP SMALL SELF; Biau G., 2015, MATH STAT LIMIT THEO, P21, DOI DOI 10.1007/978-3-319-12442-1_3; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Das D., 2018, MIXED PRECISION TRAI; Gupta S., 2015, P 32 INT C MACH LEAR, V37, P1737; HAN S., 2015, ARXIV151000149; Hoffer Elad, 2018, ARXIV180301814; Hubara I., 2016, NEURIPS; Hubara I., 2016, ARXIV160907061; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Jaderberg M., 2014, ARXIV14053866; Kamath Gautam, 2015, BOUNDS EXPECTATION M; Lin Xiaofan, 2017, NEURIPS, V1, P2; Mishra A., 2018, ARXIV170901134; Miyashita D., 2016, ARXIV160301025; Rodriguez A., 2018, LOWER NUMERICAL PREC, V2018; Soudry D., 2014, PROC 27 INT C NEURAL, P963, DOI DOI 10.5555/2968826.2968934; Ullrich K., 2017, 5 INT C LEARN REPPR; Wen W., 2017, NIPS 17, V30, P1509; Wu S., 2018, INT C LEARN REPR ICL; Wu Shuang, 2018, ARXIV180209769; Wu Y., 2016, GOOGLES NEURAL MACHI; Zhou Shuchang, 2016, P IEEE C COMP VIS PA	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305018
C	Bello, K; Honorio, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bello, Kevin; Honorio, Jean			Learning latent variable structured prediction models with Gaussian perturbations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NUMBER	The standard margin-based structured prediction commonly uses a maximum loss over all possible structured outputs [26, 1, 5, 25]. The large-margin formulation including latent variables [30, 21] not only results in a non-convex formulation but also increases the search space by a factor of the size of the latent space. Recent work [11] has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution, with theoretical guarantees. We extend this work by including latent variables. We study a new family of loss functions under Gaussian perturbations and analyze the effect of the latent space on the generalization bounds. We show that the non-convexity of learning with latent variables originates naturally, as it relates to a tight upper bound of the Gibbs decoder distortion with respect to the latent space. Finally, we provide a formulation using random samples and relaxations that produces a tighter upper bound of the Gibbs decoder distortion up to a statistical accuracy, which enables a polynomial time evaluation of the objective function. We illustrate the method with synthetic experiments and a computer vision application.	[Bello, Kevin; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Bello, K (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	kbellome@purdue.edu; jhonorio@purdue.edu			National Science Foundation [1716609-IIS]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1716609-IIS.	Altun Y., 2003, EUR C SPEECH COMM TE; BENNETT JF, 1956, PSYCHOMETRIKA, V21, P383, DOI 10.1007/BF02296304; BENNETT JF, 1960, PSYCHOMETRIKA, V25, P27, DOI 10.1007/BF02288932; Choi H, 2016, JMLR WORKSH CONF PRO, V51, P667; Collins M, 2004, TEXT SPEECH LANG TEC, P19; Collins Michael, 2004, P 42 ANN M ASS COMPU; Cortes Corinna, 2016, ADV NEURAL INFORM PR, P2514; COVER TM, 1967, SIAM J APPL MATH, V15, P434, DOI 10.1137/0115039; Gane A, 2014, JMLR WORKSH CONF PRO, V33, P247; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Honorio, 2016, UAI; Kulesza A., 2007, ADV NEURAL INFORM PR, P785; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Li MH, 2007, BIOINFORMATICS, V23, P597, DOI 10.1093/bioinformatics/btl660; London B., 2016, NIPS WORKSH OPT MACH; McAllester D., 2007, PREDICTING STRUCTURE, P247; Meshi O, 2016, PR MACH LEARN RES, V48; Neylon T, 2006, THESIS; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Petrov Slav, 2008, ADV NEURAL INFORM PR, P1153; Ping W, 2014, PR MACH LEARN RES, V32, P190; QUATTONI A, 2005, ADV NEURAL INFORM PR; Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124; Sarawagi S., 2008, P 25 INT C MACH LEAR, P888; Taskar B, 2004, ADV NEUR IN, V16, P25; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Volkovs M., 2012, ADV NEURAL INFORM PR, P1313; Wang HY, 2013, COMMUN ACM, V56, P92, DOI [10.1145/2436256.2436276, 10.1145/2436258.2436276]; Wang S. B., 2006, PROC IEEE COMPUT SOC, P1521, DOI DOI 10.1109/CVPR.2006.132; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]; Yuille AL, 2002, ADV NEUR IN, V14, P1033; Zhang Y., 2015, P 2015 C N AM CHAPT, P42; Zhang Yuan, 2014, P 2014 C EMP METH NA, P1013	33	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303017
C	Bellot, A; Schaar, MD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bellot, Alexis; van der Schaar, Mihaela			Multitask Boosting for Survival Analysis with Competing Risks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The co-occurrence of multiple diseases among the general population is an important problem as those patients have more risk of complications and represent a large share of health care expenditure. Learning to predict time-to-event probabilities for these patients is a challenging problem because the risks of events are correlated (there are competing risks) with often only few patients experiencing individual events of interest, and of those only a fraction are actually observed in the data. We introduce in this paper a survival model with the flexibility to leverage a common representation of related events that is designed to correct for the strong imbalance in observed outcomes. The procedure is sequential: outcome-specific survival distributions form the components of nonparametric multivariate estimators which we combine into an ensemble in such a way as to ensure accurate predictions on all outcome types simultaneously. Our algorithm is general and represents the first boosting-like method for time-to-event data with multiple outcomes. We demonstrate the performance of our algorithm on synthetic and real data.	[Bellot, Alexis; van der Schaar, Mihaela] Univ Oxford, Oxford, England; [van der Schaar, Mihaela] Alan Turing Inst, London, England	University of Oxford	Bellot, A (corresponding author), Univ Oxford, Oxford, England.	alexis.bellot@eng.ox.ac.uk; mschaar@turing.ac.uk						Aalen O. O., 1980, LECT NOTES STAT, V2, P1; Alaa A, 2018, INT C MACH LEARN; Alaa Ahmed, 2017, ADV NEURAL INFORM PR; Bellot A., 2018, INT C ART INT STAT, P910; Bellot Alexis, 2018, IEEE J BIOMEDICAL HL; Binder H, 2009, BIOINFORMATICS, V25, P890, DOI 10.1093/bioinformatics/btp088; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Caruana R., 1993, ICML, DOI [DOI 10.1016/B978-1-55860-307-3.50012-5, 10.1016/b978-1-55860-307-3.50012-5]; Chris S., 2008, P 21 INT FLORIDA ART, P306; Drucker H., 1997, P INT C MACHINE LEAR, P107; Fine JP, 1999, J AM STAT ASSOC, V94, P496, DOI 10.2307/2670170; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; G_ornitz N., 2011, PROC 24 INT C NEURAL, P2690; Galar M, 2012, IEEE T SYST MAN CY C, V42, P463, DOI 10.1109/TSMCC.2011.2161285; GRAY RJ, 1988, ANN STAT, V16, P1141, DOI 10.1214/aos/1176350951; He K, 2016, BIOINFORMATICS, V32, P50, DOI 10.1093/bioinformatics/btv517; Ishwaran H, 2014, BIOSTATISTICS, V15, P757, DOI 10.1093/biostatistics/kxu010; Katzman J., 2016, ARXIV160600931; Lee C., 2018, AAAI; Mercer S., 2014, ABC MULTIMORBIDITY; Morrison D, 2016, NPJ PRIM CARE RESP M, V26, DOI 10.1038/npjpcrm.2016.43; PRENTICE RL, 1978, BIOMETRICS, V34, P541, DOI 10.2307/2530374; Ranganath R., 2016, ARXIV160802158; Ridgeway G., 1999, COMPUTING SCI STAT, V31, P172; Solomatine DP, 2004, IEEE IJCNN, P1163; Strobl C, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-307; Wolbers M, 2014, BIOSTATISTICS, V15, P526, DOI 10.1093/biostatistics/kxt059	27	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301038
C	Benson, AR; Kleinberg, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Benson, Austin R.; Kleinberg, Jon			Found Graph Data and Planted Vertex Covers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CORE-PERIPHERY STRUCTURE; BIG DATA; CENTRALITY; DYNAMICS; MODELS	A typical way in which network data is recorded is to measure all interactions involving a specified set of core nodes, which produces a graph containing this core together with a potentially larger set of fringe nodes that link to the core. Interactions between nodes in the fringe, however, are not present in the resulting graph data. For example, a phone service provider may only record calls in which at least one of the participants is a customer; this can include calls between a customer and a non-customer, but not between pairs of non-customers. Knowledge of which nodes belong to the core is crucial for interpreting the dataset, but this metadata is unavailable in many cases, either because it has been lost due to difficulties in data provenance, or because the network consists of "found data" obtained in settings such as counter-surveillance. This leads to an algorithmic problem of recovering the core set. Since the core is a vertex cover, we essentially have a planted vertex cover problem, but with an arbitrary underlying graph. We develop a framework for analyzing this planted vertex cover problem, based on the theory of fixed-parameter tractability, together with algorithms for recovering the core. Our algorithms are fast, simple to implement, and out-perform several baselines based on core-periphery structure on various real-world datasets.	[Benson, Austin R.; Kleinberg, Jon] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Benson, AR (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	arb@cs.cornell.edu; kleinber@cs.cornell.edu			Simons Investigator Award; NSF TRIPODS Award [1740822]; NSF Award [DMS-1830274]	Simons Investigator Award; NSF TRIPODS Award; NSF Award(National Science Foundation (NSF))	We thank Jure Leskovec for providing access to the email-Eu data; Mason Porter and Sang Hoon Lee for providing the Path-Core code; and Travis Martin and Thomas Zhang for providing the belief propagation code. This research was supported in part by a Simons Investigator Award, NSF TRIPODS Award #1740822, and NSF Award DMS-1830274.	ABBE E, 2018, J MACH LEARN RES, V18; Abbe E., 2016, ADV NEURAL INFORM PR, P1334; Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47; Alon N, 1998, RANDOM STRUCT ALGOR, V13, P457, DOI 10.1002/(SICI)1098-2418(199810/12)13:3/4<457::AID-RSA14>3.0.CO;2-W; Bader DA, 2007, LECT NOTES COMPUT SC, V4863, P124; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; Borgatti SP, 1999, SOC NETWORKS, V21, P375; Brandes U, 2001, J MATH SOCIOL, V25, P163, DOI 10.1080/0022250X.2001.9990249; Brandes U, 2008, SOC NETWORKS, V30, P136, DOI 10.1016/j.socnet.2007.11.001; Buneman P, 2001, LECT NOTES COMPUT SC, V1973, P316; BUSS JF, 1993, SIAM J COMPUT, V22, P560, DOI 10.1137/0222038; COMREY AL, 1962, PSYCHOL REP, V11, P15, DOI 10.2466/pr0.1962.11.1.15; Craswell N., 2005, TREC, V5, P199; Csermely P, 2013, J COMPLEX NETW, V1, P93, DOI 10.1093/comnet/cnt016; Cucuringu M, 2016, EUR J APPL MATH, V27, P846, DOI 10.1017/S095679251600022X; Damaschke P, 2006, THEOR COMPUT SCI, V351, P337, DOI 10.1016/j.tcs.2005.10.004; Damaschke P, 2009, J DISCRET ALGORITHMS, V7, P391, DOI 10.1016/j.jda.2009.01.003; Decelle  A., 2011, PHYS REV E, V84; Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y; Downey R.G., 2012, PARAMETERIZED COMPLE; Eagle N, 2006, PERS UBIQUIT COMPUT, V10, P255, DOI 10.1007/s00779-005-0046-3; Easley D., 2010, NETWORKS CROWDS MARK; Eppstein D, 2011, LECT NOTES COMPUT SC, V6630, P364; FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543; Geisberger R, 2008, SIAM PROC S, P90; Gile KJ, 2010, SOCIOL METHODOL, V40, P285, DOI 10.1111/j.1467-9531.2010.01223.x; Hier SP, 2009, SURVEILLANCE: POWER, PROBLEMS, AND POLITICS, P1; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Holme P, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.046111; Jagadish HV, 2014, COMMUN ACM, V57, P86, DOI 10.1145/2611567; Khabbazian M, 2017, ELECTRON J STAT, V11, P4769, DOI 10.1214/17-EJS1358; Kim M, 2011, P 2011 SIAM INT C DA, P47, DOI DOI 10.1137/1.9781611972818.5; Klimt B., 2004, CEAS; Kossinets G, 2006, SOC NETWORKS, V28, P247, DOI 10.1016/j.socnet.2005.07.002; Koutra D., 2013, P 2013 SIAM INT C DA, P162, DOI DOI 10.1137/1.9781611972832.18; KUNY T, 1997, 63 IFLA COUNC GEN C; Laumann E. O., 1989, RES METHODS SOC NETW, V61, P87; Lee SH, 2014, PHYS REV E, V89, DOI 10.1103/PhysRevE.89.032810; Leskovec J, 2010, P 17 INT C WORLD WID; Leskovec J., 2005, P 11 ACM SIGKDD INT; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Lynch C, 2008, NATURE, V455, P28, DOI 10.1038/455028a; Meka  R., 2015, P 47 ANN ACM S THEOR; Monahan T, 2010, SURVEILL SOC, V8, P106; Mossel E., 2014, P MACHINE LEARNING R, P356; Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480; Oard  D., 2006, TECHNICAL REPORT; Panzarasa P, 2009, J AM SOC INF SCI TEC, V60, P911, DOI 10.1002/asi.21015; Peel L, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1602548; Ripeanu M, 2002, IEEE INTERNET COMPUT, V6, P50, DOI 10.1109/4236.978369; Rombach P, 2017, SIAM REV, V59, P619, DOI 10.1137/17M1130046; Romero Daniel M, 2016, P 25 INT C WORLD WID; Ron D., 2010, DISCRETE MATH THEOR, P189; Seshadhri C., 2013, SDM, V4, P5; Simmhan YL, 2005, SIGMOD REC, V34, P31, DOI 10.1145/1084805.1084812; Spielman  D.A., 2010, GRAPHS NETWORKS LECT; Spring N, 2002, ACM SIGCOMM COMP COM, V32, P133, DOI 10.1145/964725.633039; Tan W-C., 2004, IEEE DATA ENG B, V27, P45; Tsiatas  A., 2013, P 22 INT C WORLD WID; Wu  Y., 2006, CEAS; Yin H., 2017, P 23 ACM SIGKDD INT; Zhang X, 2015, PHYS REV E, V91	63	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301035
C	Bhatia, K; Pacchiano, A; Flammarion, N; Bartlett, PL; Jordan, MI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bhatia, Kush; Pacchiano, Aldo; Flammarion, Nicolas; Bartlett, Peter L.; Jordan, Michael I.			Gen-Oja: A Simple and Efficient Algorithm for Streaming Generalized Eigenvector Computation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				STOCHASTIC-APPROXIMATION	In this paper, we study the problems of principal Generalized Eigenvector computation and Canonical Correlation Analysis in the stochastic setting. We propose a simple and efficient algorithm, Gen-Oja, for these problems. We prove the global convergence of our algorithm, borrowing ideas from the theory of fast-mixing Markov chains and two-time-scale stochastic approximation, showing that it achieves the optimal rate of convergence. In the process, we develop tools for understanding stochastic processes with Markovian noise which might be of independent interest.	[Bhatia, Kush; Pacchiano, Aldo; Flammarion, Nicolas; Bartlett, Peter L.; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Bhatia, K (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	kushbhatia@berkeley.edu; pacchiano@berkeley.edu; flammarion@berkeley.edu; peter@berkeley.edu; jordan@cs.berkeley.edu	Jordan, Michael I/C-5253-2013		NSF [IIS-1619362]; BAIR-Huawei PhD Fellowship; Mathematical Data Science program of the Office of Naval Research [N00014-181-2764]; AFOSR [FA9550-17-1-0308]	NSF(National Science Foundation (NSF)); BAIR-Huawei PhD Fellowship; Mathematical Data Science program of the Office of Naval Research(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	We gratefully acknowledge the support of the NSF through grant IIS-1619362. AP acknowledges Huawei's support through a BAIR-Huawei PhD Fellowship. This work was supported in part by the Mathematical Data Science program of the Office of Naval Research under grant number N00014-181-2764. This work was partially supported by AFOSR through grant FA9550-17-1-0308.	Allen-Zhu Z, 2017, ANN IEEE SYMP FOUND, P487, DOI 10.1109/FOCS.2017.51; Andrieu C, 2005, SIAM J CONTROL OPTIM, V44, P283, DOI 10.1137/S0363012902417267; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; [Anonymous], [No title captured]; ARORA R, 2017, ADV NEURAL INFORM PR; BACH F., 2013, ADV NEURAL INFORM PR; Borkar V. S, 2009, STOCHASTIC APPROXIMA, V48; Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3; Chaudhuri K., 2009, PROC INT C MACHINE L, P129, DOI DOI 10.1145/1553374.1553391; Diaconis P, 1999, SIAM REV, V41, P45, DOI 10.1137/S0036144598338446; Dieuleveut Aymeric, 2017, ARXIV170706386; Gao C., 2017, ARXIV170206533; Garber D., 2016, INT C MACH LEARN; Ge R., 2016, INT C INT C MACH; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; JAIN P., 2016, C LEARN THEOR; Kakade SM, 2007, LECT NOTES COMPUT SC, V4539, P82, DOI 10.1007/978-3-540-72927-3_8; LI Y., 2017, INT C MACH LEARN; Lu Yichao, 2014, ADV NEURAL INFORM PR; Ma Z., 2015, INT C INT C MACH LEA; MEYN S., 2009, MARKOV CHAINS STOCHA; Musco Cameron, 2015, ADV NEURAL INFORM PR; NEMIROVSKY A. S., 1983, WILEY INTERSCIENCE S; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Shamir O., 2016, INT C MACH LEARN; Steinsaltz D, 1999, ANN PROBAB, V27, P1952, DOI 10.1214/aop/1022677556; Tripuraneni N., 2018, C LEARN THEOR; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang W, 2016, ADV NEURAL INFORM PR	31	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001055
C	Bresler, G; Park, SM; Persu, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bresler, Guy; Park, Sung Min; Persu, Madalina			Sparse PCA from Sparse Linear Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PRINCIPAL COMPONENTS; OPTIMAL RATES; POWER METHOD; EIGENVALUE; AGGREGATION; SELECTION; RECOVERY	Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but an explicit connection between the two had not been made. We show how to efficiently transform a black-box solver for SLR into an algorithm for SPCA: assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the SPCA algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomial-time algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. We provide experimental results on simulated data comparing our proposed framework to other algorithms for SPCA.	[Bresler, Guy; Park, Sung Min; Persu, Madalina] MIT, Cambridge, MA 02139 USA; [Persu, Madalina] Two Sigma, New York, NY USA	Massachusetts Institute of Technology (MIT)	Bresler, G (corresponding author), MIT, Cambridge, MA 02139 USA.	guy@mit.edu; sp765@mit.edu; mpersu@mit.edu						Amini AA, 2008, IEEE INT SYMP INFO, P2454, DOI 10.1109/ISIT.2008.4595432; Bandeira AS, 2013, IEEE T INFORM THEORY, V59, P3448, DOI 10.1109/TIT.2013.2248414; Berthet Q., 2013, C LEARN THEOR, P1046; Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Bunea F, 2007, LECT NOTES COMPUT SC, V4539, P530, DOI 10.1007/978-3-540-72927-3_38; Bunea F, 2007, ANN STAT, V35, P1674, DOI 10.1214/009053606000001587; Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Dai D, 2014, ELECTRON J STAT, V8, P302, DOI 10.1214/14-EJS886; Do TT, 2008, CONF REC ASILOMAR C, P581, DOI 10.1109/ACSSC.2008.5074472; Fletcher AK, 2009, IEEE T INFORM THEORY, V55, P5758, DOI 10.1109/TIT.2009.2032726; Gamarnik D., 2017, ARXIV171104952; Gataric M., 2017, ARXIV171205630; Johnstone Iain M, 2009, J AM STAT ASS; Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Journee M, 2010, J MACH LEARN RES, V11, P517; Khanna Rajiv, 2015, AISTATS; Koyejo O, 2014, ADV NEUR IN, V27; Krauthgamer  Robert, 2013, TECHNICAL REPORT; Laurent B, 2000, ANN STAT, V28, P1302; Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854; Rudelson M., 2012, C LEARN THEOR, P10; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; van de Geer Sara, 2007, SEM STAT EIDG TECHN; Vu VQ, 2013, ADV NEURAL INFORM PR, V26; Wainwright M, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P961, DOI 10.1109/ISIT.2007.4557348; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Wang TY, 2016, ANN STAT, V44, P1896, DOI 10.1214/15-AOS1369; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zhang T., 2009, NEURAL INFORM PROCES, P1921; Zhang  Yuchen, 2015, ARXIV150303188; Zhang  Yuchen, 2014, ARXIV14021918; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	49	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005052
C	Cai, DN; Mitzenmacher, M; Adams, RP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cai, Diana; Mitzenmacher, Michael; Adams, Ryan P.			A Bayesian Nonparametric View on Count-Min Sketch	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The count-min sketch is a time-and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream. The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n - grams. We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation. In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens. Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees. Using simulated data and text data, we investigate the properties of these estimators. Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.	[Cai, Diana; Adams, Ryan P.] Princeton Univ, Princeton, NJ 08544 USA; [Mitzenmacher, Michael] Harvard Univ, Cambridge, MA 02138 USA	Princeton University; Harvard University	Cai, DN (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	dcai@cs.princeton.edu; michaelm@eecs.harvard.edu; rpa@princeton.edu			NSF [IIS-1421780]; Alfred P. Sloan Foundation	NSF(National Science Foundation (NSF)); Alfred P. Sloan Foundation(Alfred P. Sloan Foundation)	Michael Mitzenmacher was supported in part by NSF grants CCF-1563710, CCF-1535795, CCF-1320231, and CNS-1228598. Ryan Adams was supported in part by NSF IIS-1421780 and the Alfred P. Sloan Foundation.	Aggarwal C. C., 2010, PROC SDM OHIO, P802; Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; Broderick T, 2012, BAYESIAN ANAL, V7, P439, DOI 10.1214/12-BA715; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Chung K., 2013, THEOR COMPUT, V9, P897; Cohen Saar, 2003, ACM SIGMOD, P241, DOI [10.1145/872757.872787, DOI 10.1145/872757.872787]; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Cormode G, 2005, SIAM PROC S, P44; Cormode G, 2011, FOUND TRENDS DATABAS, V4, P1, DOI 10.1561/1900000004; Dwork Cynthia, 2010, P 1 S INN COMP SCI I; Estan C, 2003, ACM T COMPUT SYST, V21, P270, DOI 10.1145/859716.859719; EWENS WJ, 1972, THEOR POPUL BIOL, V3, P87, DOI 10.1016/0040-5809(72)90035-4; Fan L, 2000, IEEE ACM T NETWORK, V8, P281, DOI 10.1109/90.851975; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; GOYAL A, 2009, P 2009 ANN C N AM CH, P00512; Griffiths TL, 2011, J MACH LEARN RES, V12, P1185; Kingman J., 1992, POISSON PROCESSES, V3; Minka T., 2000, TECHNICAL REPORT; MISRA J, 1982, SCI COMPUT PROGRAM, V2, P143, DOI 10.1016/0167-6423(82)90012-0; Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002; Pitman J., 2006, LECT NOTES MATH; Teh Y. W., 2009, P ADV NEUR INF PROC, V22, P1838; Watterson G. A., 1974, Advances in Applied Probability, V6, P463, DOI 10.2307/1426228	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003033
C	Canonne, CL; Diakonikolas, I; Stewart, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Canonne, Clement L.; Diakonikolas, Ilias; Stewart, Alistair			Testing for Families of Distributions via the Fourier Transform	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LOG-CONCAVE; EQUILIBRIA	We study the general problem of testing whether an unknown discrete distribution belongs to a specified family of distributions. More specifically, given a distribution family P and sample access to an unknown discrete distribution P, we want to distinguish (with high probability) between the case that P is an element of P and the case that P is epsilon-far, in total variation distance, from every distribution in P. This is the prototypical hypothesis testing problem that has received significant attention in statistics and, more recently, in computer science. The main contribution of this work is a simple and general testing technique that is applicable to all distribution families whose Fourier spectrum satisfies a certain approximate sparsity property. We apply our Fourier-based framework to obtain near sample-optimal and computationally efficient testers for the following fundamental distribution families: Sums of Independent Integer Random Variables (SIIRVs), Poisson Multinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For the first two, ours are the first non-trivial testers in the literature, vastly generalizing previous work on testing Poisson Binomial Distributions. For the third, our tester improves on prior work in both sample and time complexity.	[Canonne, Clement L.] Stanford Univ, Stanford, CA 94305 USA; [Diakonikolas, Ilias; Stewart, Alistair] Univ Southern Calif, Los Angeles, CA USA	Stanford University; University of Southern California	Canonne, CL (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ccanonne@stanford.edu; diakonik@usc.edu; stewart.al@gmail.com						Acharya J., 2015, P 26 ANN ACM SIAM S, P1829, DOI [10.1137/1.9781611973730.122, DOI 10.1137/1.9781611973730.122]; Acharya  Jayadev, 2015, ADV NEURAL INFORM PR, P3591; An M. Yu., 1995, 9503 DUK U DEP EC, DOI [10.2139/ssrn.1933, DOI 10.2139/SSRN.1933]; Barbour, 1988, J APPL PROBAB A, V25, P175; Barbour A.D., 1992, POISSON APPROXIMATIO; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Bhaskara Aditya, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P423, DOI 10.1007/978-3-642-32512-0_36; Borgs C, 2008, ACM S THEORY COMPUT, P365; Canonne C. L., 2017, ABS170605738 CORR; Canonne C. L., 2017, THEORY COMPUTING SYS; Canonne CL, 2018, ACM S THEORY COMPUT, P735, DOI 10.1145/3188745.3188756; Canonne Clement L., 2015, ELECT C COMPUTATIONA, V22, P63; Chan S., 2014, P 25 ANN ACM SIAM S, P1193, DOI [10.1137/1.9781611973402.88, DOI 10.1137/1.9781611973402.88]; Chen SX, 1997, STAT SINICA, V7, P875; Cheng Y, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P616; Chenz L. H. Y., 2010, ZERO BIAS DISCRETIZE; Daskalakis C., 2014, J EC THEORY; Daskalakis C., 2016, P STOC 16; Daskalakis C, 2007, ANN IEEE SYMP FOUND, P83, DOI 10.1109/FOCS.2007.24; Daskalakis C, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P709; Daskalakis C, 2013, ANN IEEE SYMP FOUND, P217, DOI 10.1109/FOCS.2013.31; Daskalakis C, 2009, ACM S THEORY COMPUT, P75; Daskalakis C, 2008, ANN IEEE SYMP FOUND, P25, DOI 10.1109/FOCS.2008.84; De A., 2015, FOCS; Diakonikolas I., 2016, P 29 C LEARN THEOR C, P850; Diakonikolas I., 2015, P SODA 15; Diakonikolas I., 2016, P STOC 16; Diakonikolas I., 2016, P 29 C LEARN THEOR C, P831; Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274; Goldberg PW, 2017, J COMPUT SYST SCI, V90, P80, DOI 10.1016/j.jcss.2017.07.002; Gopalan P., 2015, FOCS; Gopalan P, 2011, ACM S THEORY COMPUT, P253; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Kruopis J., 1986, LITH MATH J, V26, P37; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Loh W.-L., 1992, ANN APPL PROBAB, V2, P536; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987; Poisson S.D., 1837, RECHERCHES PROBABILI; PRESMAN EL, 1984, THEOR PROBAB APPL+, V28, P393, DOI 10.1137/1128033; Roos B, 1999, J MULTIVARIATE ANAL, V69, P120, DOI 10.1006/jmva.1998.1789; Roos B, 2010, BERNOULLI, V16, P23, DOI 10.3150/08-BEJ171; Rubinfeld R., 2012, XRDS CROSSROADS FAL, V19, P24, DOI DOI 10.1145/2331042.2331052; STANLEY RP, 1989, ANN NY ACAD SCI, V576, P500, DOI 10.1111/j.1749-6632.1989.tb16434.x; Valiant G., 2010, ELECT C COMPUTATIONA, V17; Valiant G, 2017, SIAM J COMPUT, V46, P429, DOI 10.1137/151002526; Valiant G, 2014, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2014.14; Valiant G, 2011, ACM S THEORY COMPUT, P685; Valiant P, 2008, ACM S THEORY COMPUT, P383; Walther G, 2009, STAT SCI, V24, P319, DOI 10.1214/09-STS303	56	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004060
C	Carreira-Perpinan, MA; Tavallali, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Carreira-Perpinan, Miguel A.; Tavallali, Pooya			Alternating Optimization of Decision Trees, with Application to Learning Sparse Oblique Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	Learning a decision tree from data is a difficult optimization problem. The most widespread algorithm in practice, dating to the 1980s, is based on a greedy growth of the tree structure by recursively splitting nodes, and possibly pruning back the final tree. The parameters (decision function) of an internal node are approximately estimated by minimizing an impurity measure. We give an algorithm that, given an input tree (its structure and the parameter values at its nodes), produces a new tree with the same or smaller structure but new parameter values that provably lower or leave unchanged the misclassification error. This can be applied to both axis-aligned and oblique trees and our experiments show it consistently outperforms various other algorithms while being highly scalable to large datasets and trees. Further, the same algorithm can handle a sparsity penalty, so it can learn sparse oblique trees, having a structure that is a subset of the original tree and few nonzero parameters. This combines the best of axis-aligned and oblique trees: flexibility to model correlated data, low generalization error, fast inference and interpretable nodes that involve only a few features in their decision.	[Carreira-Perpinan, Miguel A.; Tavallali, Pooya] Univ Calif Merced, Dept EECS, Merced, CA 95343 USA	University of California System; University of California Merced	Carreira-Perpinan, MA (corresponding author), Univ Calif Merced, Dept EECS, Merced, CA 95343 USA.	mcarreira-perpinan@ucmerced.edu; ptavallali@ucmerced.edu			NSF [IIS-1423515]	NSF(National Science Foundation (NSF))	Work funded in part by NSF award IIS-1423515.	Auer P., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P21; Bennett K.P., 1994, COMP SCI STAT VOL 26, V26, P156; Bennett KP, 1992, DECISION TREE CONSTR; Bertsimas D, 2017, MACH LEARN, V106, P1039, DOI 10.1007/s10994-017-5633-9; Beygelzimer A., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; CARREIRA-PERPINAN M., 2017, ARXIV170701209; Carreira-Perpinan MA, 2018, PROC CVPR IEEE, P8532, DOI 10.1109/CVPR.2018.00890; Carreira-Perpinan MA, 2014, JMLR WORKSH CONF PRO, V33, P10; Criminisi A., 2013, DECISION FORESTCOM; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; Hastie T., 2013, ELEMENTS STAT LEARNI, DOI DOI 10.1007/978-0-387-84858-7_16; HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011; Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Kumar A, 2017, PR MACH LEARN RES, V70; Maimon O., 2007, SERIES MACHINE PERCE, V69; Mathy C, 2015, AAAI CONF ARTIF INTE, P2864; MEGIDDO N, 1988, DISCRETE COMPUT GEOM, V3, P325, DOI 10.1007/BF02187916; Murthy SK, 1994, J ARTIF INTELL RES, V2, P1, DOI 10.1613/jair.63; Norouzi M., 2015, ADV NEURAL INFORM PR, P1720; Norouzi M., 2015, ARXIV150606155; Olshen R., 1984, CLASSIFICATION REGRE; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309; Roman V., 2012, COMPRESSED SENSING T; Schapire RE, 2012, ADAPT COMPUT MACH LE, P1; Tibshirani R., 2015, STAT LEARNING SPARSI, DOI 10.1201/b18401; Tjortjis C, 2002, LECT NOTES COMPUT SC, V2412, P50; Tzirakis P, 2017, ADV DATA ANAL CLASSI, V11, P353, DOI 10.1007/s11634-016-0246-x; Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301022
C	Chen, JC; Zhang, Q; Zhou, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Jiecao; Zhang, Qin; Zhou, Yuan			Tight Bounds for Collaborative PAC Learning via Multiplicative Weights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the collaborative PAC learning problem recently proposed in Blum et al. [3], in which we have k players and they want to learn a target function collaboratively, such that the learned function approximates the target function well on all players' distributions simultaneously. The quality of the collaborative learning algorithm is measured by the ratio between the sample complexity of the algorithm and that of the learning algorithm for a single distribution (called the overhead). We obtain a collaborative learning algorithm with overhead O(ln k), improving the one with overhead O(ln(2) k) in [3]. We also show that an Omega(ln k) overhead is inevitable when k is polynomial bounded by the VC dimension of the hypothesis class. Finally, our experimental study has demonstrated the superiority of our algorithm compared with the one in Blum et al. [3] on real-world datasets.	[Chen, Jiecao; Zhang, Qin; Zhou, Yuan] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA; [Zhou, Yuan] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL USA	Indiana University System; Indiana University Bloomington; University of Illinois System; University of Illinois Urbana-Champaign	Chen, JC (corresponding author), Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.	jiecchen@iu.edu; qzhangcs@indiana.edu; yuanz@illinois.edu			NSF [CCF-1525024, CCF-1844234, IIS-1633215]	NSF(National Science Foundation (NSF))	Jiecao Chen and Qin Zhang are supported in part by NSF CCF-1525024, CCF-1844234 and IIS-1633215. Part of the work was done when Yuan Zhou was visiting the Shanghai University of Finance and Economics.	Balcan M, 2012, COLT; Balcan M.F., 2013, P 26 INT C NEURAL IN, P1995; Blum A., 2017, ADV NEURAL INFORM PR, P2389; Bock R., 2003, INTERNAL NOTE CERN; Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016; Daume Hal  III, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P154, DOI 10.1007/978-3-642-34106-9_15; Daume III H., 2012, ARTIF INTELL, P282; EHRENFEUCHT A, 1989, INFORM COMPUT, V82, P247, DOI 10.1016/0890-5401(89)90002-3; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; FREY PW, 1991, MACH LEARN, V6, P161, DOI 10.1023/A:1022606404104; Guha S, 2017, PROCEEDINGS OF THE 29TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'17), P143, DOI 10.1145/3087556.3087568; Hanneke S., 2016, J MACH LEARN RES, V17, P1319, DOI [10.5555/2946645.2946683, DOI 10.5555/2946645.2946683]; Liang Y., 2014, ADV NEURAL INFORM PR, P3113; Mansour Yishay, 2008, NIPS, P1041; Nguyen H. L., 2018, ARXIV180508356; Wang JL, 2016, JMLR WORKSH CONF PRO, V51, P751	16	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303058
C	Chen, LX; Xu, J; Lu, Z		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Lixing; Xu, Jie; Lu, Zhuo			Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we study the stochastic contextual combinatorial multi-armed bandit (CC-MAB) framework that is tailored for volatile arms and submodular reward functions. CC-MAB inherits properties from both contextual bandit and combinatorial bandit: it aims to select a set of arms in each round based on the side information (a.k.a. context) associated with the arms. By "volatile arms", we mean that the available arms to select from in each round may change; and by "submodular rewards", we mean that the total reward achieved by selected arms is not a simple sum of individual rewards but demonstrates a feature of diminishing returns determined by the relations between selected arms (e.g. relevance and redundancy). Volatile arms and submodular rewards are often seen in many real-world applications, e.g. recommender systems and crowdsourcing, in which multi-armed bandit (MAB) based strategies are extensively applied. Although there exist works that investigate these issues separately based on standard MAB, jointly considering all these issues in a single MAB problem requires very different algorithm design and regret analysis. Our algorithm CC-MAB provides an online decision-making policy in a contextual and combinatorial bandit setting and effectively addresses the issues raised by volatile arms and submodular reward functions. The proposed algorithm is proved to achieve O(cT(2 alpha+D/3 alpha+D) log(T)) regret after a span of T rounds. The performance of CC-MAB is evaluated by experiments conducted on a realworld crowdsourcing dataset, and the result shows that our algorithm outperforms the prior art.	[Chen, Lixing; Xu, Jie] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA; [Lu, Zhuo] Univ S Florida, Dept Elect Engn, Tampa, FL 33620 USA	University of Miami; State University System of Florida; University of South Florida	Chen, LX (corresponding author), Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.	lx.chen@miami.edu; jiexu@miami.edu; zhuolu@usf.edu	Chen, Lixing/ABB-7931-2020	Chen, Lixing/0000-0002-1805-0183; Xu, Jie/0000-0002-0515-1647	Army Research Office [W911NF-18-1-0343]	Army Research Office	L. Chen and J. Xu's work is supported in part by the Army Research Office under Grant W911NF-18-1-0343. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.	Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bnaya Zahy, 2013, AAAI LATE BREAKING D; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chen L., 2017, ADV NEURAL INFORM PR, P141; Chen W., 2013, ICML 2013, P151; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Hazan E, 2012, J MACH LEARN RES, V13, P2903; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Kleinberg R, 2005, P 17 INT C NEUR INF, V17, P697; Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Krause Krause A.A., 2008, ICML TUTORIALS; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; Radanovic G., 2018, P 32 AAAI C ART INT; Radlinski F, 2008, P 25 INT C MACH LEAR, DOI DOI 10.1145/1390156.1390255; Yang P, 2017, IEEE INTERNET THINGS, V4, P1193, DOI 10.1109/JIOT.2017.2726820; Yue Y., 2011, ADV NEURAL INFORM PR, P2483; Zhang Y, 2012, IEEE J SEL AREA COMM, V30, P2136, DOI 10.1109/JSAC.2012.121206	23	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303026
C	Chen, RTQ; Rubanova, Y; Bettencourt, J; Duvenaud, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Ricky T. Q.; Rubanova, Yulia; Bettencourt, Jesse; Duvenaud, David			Neural Ordinary Differential Equations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ADJOINT	We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.	[Chen, Ricky T. Q.; Rubanova, Yulia; Bettencourt, Jesse; Duvenaud, David] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Toronto	Chen, RTQ; Rubanova, Y; Bettencourt, J (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	rtqichen@cs.toronto.edu; rubanova@cs.toronto.edu; jessebett@cs.toronto.edu; duvenaud@cs.toronto.edu	Chen, Ricky Tian Qi/AAS-3168-2021					Alvarez MA, 2011, J MACH LEARN RES, V12, P1459; Andersson J., 2013, GEN PURPOSE SOFTWARE; Ba J., 2017, P 3 INT C LEARN REPR; Baydin AG, 2018, J MACH LEARN RES, V18; Bo Chang, 2017, ARXIV170903698; Bo Chang, 2018, INT C LEARN REPR; Carpenter B., 2015, ARXIV PREPRINT ARXIV; Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9; Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301; Coddington EA, 1955, THEORY ORDINARY DIFF; Dinh L., 2014, ARXIV; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Farrell Patrick, 2013, SIAM J SCI COMPUTING; Figurnov M., 2017, SPATIALLY ADAPTIVE C; Futoma J., 2017, ARXIV E PRINTS; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2211; Graves A., 2016, ADAPTIVE COMPUTATION; Ha David, 2016, ARXIV160909106; Hairer E, 2000, MATH COMPUT SIMUL, Vsecond, DOI [DOI 10.1016/0378-4754(87)90083-8, DOI 10.1007/978-3-662-12607-3]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2012, NEURAL NETWORKS MACH; Jernite Y., 2016, ARXIV161106188; Joel A E, 2018, MATH PROGRAMMING COM; Kingma D.P, P 3 INT C LEARNING R; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kolter J. Z, 2017, P MACHINE LEARNING R, P136; Kutta W., 1901, MATH PHY, V46, P435; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1988, P 1988 CONNECTIONIST, DOI DOI 10.3168/JDS.S0022-0302(88)79586-7; Lipton Zachary C, 2016, MACHINE LEARNING HEA, P253; Long Z., 2017, ARXIV E PRINTS; Lu Y., 2017, FINITE LAYER NEURAL; Maclaurin Dougal, 2015, ICML WORKSH AUT MACH; Mei H., 2017, ADV NEURAL INFORM PR, P6754; Melicher V, 2017, COMPUTATION STAT, V32, P1621, DOI 10.1007/s00180-017-0765-8; Palm C., 1943, ERICSSON TECHNICS; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; PEARLMUTTER BA, 1995, IEEE T NEURAL NETWOR, V6, P1212, DOI 10.1109/72.410363; Pontryagin L.S., 1962, MATH THEORY OPTIMAL; Raissi M., 2018, ARXIV180101236; Raissi M, 2018, J COMPUT PHYS, V357, P125, DOI 10.1016/j.jcp.2017.11.039; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rezende Danilo Jimenez, 2015, ARXIV150505770; Runge C., 1895, MATH ANN, V46, P167, DOI DOI 10.1007/BF01446807; Ruthotto L., 2018, ARXIV180404272; Ryder T., 2018, ARXIV E PRINTS; Schober M, 2014, ADV NEUR IN, V27; Schulam P, 2017, ARXIV170310651; Soleimani H., 2017, ARXIV170402038; Soleimani Hossein, 2017, IEEE T PATTERN ANAL; Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548; Stapor Paul, 2018, BIORXIV; Tomczak Jakub M, 2016, ARXIV161109630; Van den Berg R., 2018, ARXIV180305649; Wiewel S., 2018, ARXIV180210123; Yang Li, 2017, ARXIV170800065	60	0	0	7	38	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001014
C	Chen, S; Banerjee, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Sheng; Banerjee, Arindam			An Improved Analysis of Alternating Minimization for Structured Multi-Response Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multi-response linear models aggregate a set of vanilla linear models by assuming correlated noise across them, which has an unknown covariance structure. To find the coefficient vector, estimators with a joint approximation of the noise covariance are often preferred than the simple linear regression in view of their superior empirical performance, which can be generally solved by alternating-minimization-type procedures. Due to the non-convex nature of such joint estimators, the theoretical justification of their efficiency is typically challenging. The existing analyses fail to fully explain the empirical observations due to the assumption of resampling on the alternating procedures, which requires access to fresh samples in each iteration. In this work, we present a resampling-free analysis for the alternating minimization algorithm applied to the multi-response regression. In particular, we focus on the high-dimensional setting of multi-response linear models with structured coefficient parameter, and the statistical error of the parameter can be expressed by the complexity measure, Gaussian width, which is related to the assumed structure. More importantly, to the best of our knowledge, our result reveals for the first time that the alternating minimization with random initialization can achieve the same performance as the well-initialized one when solving this multi-response regression problem. Experimental results support our theoretical developments.	[Chen, Sheng] Voleon Grp, Berkeley, CA 94704 USA; [Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA; [Chen, Sheng] Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities; University of Minnesota System; University of Minnesota Twin Cities	Chen, S (corresponding author), Voleon Grp, Berkeley, CA 94704 USA.; Chen, S (corresponding author), Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA.	chen2832@umn.edu; banerjee@cs.umn.edu			NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	The research was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo.	Anderson T. W., 2003, INTRO MULTIVARIATE S; [Anonymous], 2016, ADV NEURAL INFORM PR; [Anonymous], 2011, ECONOMETRIC ANAL; [Anonymous], 2015, ADV NEURAL INFORM PR; Banerjee A., 2014, ADV NEURAL INFORM PR; Barber R. F., 2017, ARXIV170307755; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chatterjee S., 2014, ADV NEURAL INFORM PR; Chen S., 2015, NIPS, P2908; Chen S., 2017, ADV NEURAL INFORM PR, P2835; Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278; Ge R., 2017, ARXIV170400708; Goncalves Andre R, 2014, P 23 ACM INT C C INF, P451, DOI DOI 10.1145/2661829.2662091; Goncalves Andre R, 2016, J MACHINE LEARNING R, V17, P1205; GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761; Izenman A., 2008, MODERN MULTIVARIATE; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Kar P., 2014, ADV NEURAL INFORM PR, P685; Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549; Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013; Li Qiuwei, 2016, ARXIV161103060; Ma C., 2017, ARXIV171110467; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Netrapalli P., 2013, NIPS; Oberhofer W., 1974, ECONOMETRICA J ECONO, P579; Oymak S., 2015, ARXIV150704793; Plan Y., 2016, INFORM INFERENCE; Rai Piyush, 2012, P 25 INT C NEUR INF, V25, P3185; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Shen J, 2017, PR MACH LEARN RES, V70; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Sun R., 2015, FOCS; Talagrand M., 2014, UPPER LOWER BOUNDS S; Talagrand M., 2005, SPRINGER MG MATH, P222, DOI 10.1007/3-540-27499-5; Tu S., 2015, ARXIV150703566; Vapnik V.N, 1998, STAT LEARNING THEORY; Vershynin R., 2015, ESTIMATION HIGH DIME, P3; Waldspurger I, 2018, IEEE T INFORM THEORY, V64, P3301, DOI 10.1109/TIT.2018.2800663; WRIGHT J., 2016, ARXIV160206664; Yi XY, 2014, PR MACH LEARN RES, V32, P613; Zheng Q., 2015, ADV NEURAL INFORM PR, P109	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001018
C	Chen, TY; Giannakis, GB; Sun, T; Yin, WT		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Tianyi; Giannakis, Georgios B.; Sun, Tao; Yin, Wotao			LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper presents a new class of gradient methods for distributed machine learning that adaptively skip the gradient calculations to learn with reduced communication and computation. Simple rules are designed to detect slowly-varying gradients and, therefore, trigger the reuse of outdated gradients. The resultant gradient-based algorithms are termed Lazily Aggregated Gradient - justifying our acronym LAG used henceforth. Theoretically, the merits of this contribution are: i) the convergence rate is the same as batch gradient descent in strongly-convex, convex, and nonconvex cases; and, ii) if the distributed datasets are heterogeneous (quantified by certain measurable constants), the communication rounds needed to achieve a targeted accuracy are reduced thanks to the adaptive reuse of lagged gradients. Numerical experiments on both synthetic and real data corroborate a significant communication reduction compared to alternatives.	[Chen, Tianyi; Giannakis, Georgios B.] Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA; [Sun, Tao] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China; [Sun, Tao; Yin, Wotao] Univ Calif Los Angeles, Los Angeles, CA 90095 USA	University of Minnesota System; University of Minnesota Twin Cities; National University of Defense Technology - China; University of California System; University of California Los Angeles	Chen, TY (corresponding author), Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA.	chen3827@umn.edu; georgios@umn.edu; nudtsuntao@163.com; wotaoyin@math.ucla.edu	Giannakis, Georgios/Z-4413-2019	Giannakis, Georgios/0000-0002-0196-0260	NSF [1500713, 1711471, DMS-1720237]; University of Minnesota; China Scholarship Council; ONR [N0001417121]; NIH [1R01GM104975-01]	NSF(National Science Foundation (NSF)); University of Minnesota(University of Minnesota System); China Scholarship Council(China Scholarship Council); ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The work by T. Chen and G. Giannakis is supported in part by NSF 1500713 and 1711471, and NIH 1R01GM104975-01. The work by T. Chen is also supported by the Doctoral Dissertation Fellowship from the University of Minnesota. The work by T. Sun is supported in part by China Scholarship Council. The work by W. Yin is supported in part by NSF DMS-1720237 and ONR N0001417121.	Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D., 2017, ADV NEURAL INF PROCE, P1709; Blatt D, 2007, SIAM J OPTIMIZ, V18, P29, DOI 10.1137/040615961; Bottou  L., 2016, 160604838 ARXIV; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Cannelli  L., 2016, 160704818 ARXIV; Chen T., 2018, P IEEE; Dean J., 2012, NIPS 12, V1, P1223; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Giannakis GB, 2016, SCI COMPUT, P461, DOI 10.1007/978-3-319-41589-5_14; Gurbuzbalaban M, 2017, SIAM J OPTIMIZ, V27, P1035, DOI 10.1137/15M1049695; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Jordan M. I., 2018, J AM STAT ASS; Lan  G., 2017, 170103961 ARXIV; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; Lichman M., 2013, UCI MACHINE LEARNING; Liu J, 2015, J MACH LEARN RES, V16, P285; Liu Y., 2017, PROC IEEE C DECISION, P6696, DOI 10.1109/CDC.2017.8264668; Ma C, 2017, OPTIM METHOD SOFTW, V32, P813, DOI 10.1080/10556788.2016.1278445; McMahan B., 2017, GOOGLE RES BLOG APR; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Smith V, 2017, ADV NEUR IN, V30; Song L., 2007, P 24 INT C MACHINE L, P823, DOI [10.1145/1273496.1273600, DOI 10.1145/1273496.1273600]; Stoica  I., 2017, 171205855 ARXIV; Sun Tao, 2017, ADV NEURAL INFORM PR, P6183; Suresh AT, 2017, PR MACH LEARN RES, V70; Wen W., 2017, P NIPS, P1509; Zhang  Y., 2013, J MACHINE LEARNING R, V14	38	0	0	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305009
C	Chen, XY; Liu, C; Song, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Xinyun; Liu, Chang; Song, Dawn			Tree-to-tree Neural Networks for Program Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.	[Chen, Xinyun; Liu, Chang; Song, Dawn] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Chen, XY (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	xinyun.chen@berkeley.edu; liuchang2005acm@gmail.com; dawnsong@cs.berkeley.edu	Chen, Xinyun/ABZ-9877-2022		National Science Foundation [TWC-1409915]; DARPA D3M [FA8750-17-2-0091]	National Science Foundation(National Science Foundation (NSF)); DARPA D3M	We thank the anonymous reviewers for their valuable comments. This material is in part based upon work supported by the National Science Foundation under Grant No. TWC-1409915, Berkeley DeepDrive, and DARPA D3M under Grant No. FA8750-17-2-0091. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.	Aharoni Roee, 2017, ACL; Allamanis M., 2017, ARXIV PREPRINT ARXIV; Alvarez-Melis David, 2017, P ICLR; Nguyen AT, 2015, IEEE INT CONF AUTOM, P585, DOI 10.1109/ASE.2015.74; [Anonymous], 2015, VISUALIZING UNDERSTA; [Anonymous], ACL; Bahdanau Dzmitry, 2015, ICLR; Balog M., 2017, ICLR; Bradbury  J., 2017, ARXIV170901915; Chen XY, 2016, ADV NEUR IN, V29; Chen Xinyun, 2018, ICLR; Cho S. J. K., 2015, ACL; Devlin J., 2017, ARXIV170307469; Dong L., 2016, ACL; Dyer Chris, 2016, NAACL; Eriguchi Akiko, 2016, ACL; He Di, 2016, NEURAL INFORM PROCES, P2; Karaivanov Svetoslav, 2014, SPLASH 2014, P173, DOI [10.1145/2661136.2661148, DOI 10.1145/2661136.2661148]; Kusner M. J., 2017, P INT C MACHINE LEAR; Ling W., 2016, ACL; Nguyen Anh Tuan, 2013, P 2013 9 JOINT M FDN, P651; Oda Y, 2015, IEEE INT CONF AUTOM, P574, DOI 10.1109/ASE.2015.36; Parisotto Emilio, 2017, ICLR, P2; Rabinovich M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1139, DOI 10.18653/v1/P17-1105; Socher R., 2011, P 28 INT C INT C MAC, P129; Socher Richard, 2011, P C EMP METH NAT LAN, P151; Tai K. S., 2015, P ANN M ASS COMP LIN; Nguyen TD, 2016, 2016 IEEE/ACM 38TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING COMPANION (ICSE-C), P756, DOI 10.1145/2889160.2892661; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vinyals O., 2015, NIPS; Zhang XY, 2016, AER ADV ENG RES, V63, P310	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302055
C	Chierichetti, F; Dasgupta, A; Haddadan, S; Kumar, R; Lattanzi, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chierichetti, Flavio; Dasgupta, Anirban; Haddadan, Shahrzad; Kumar, Ravi; Lattanzi, Silvio			Mallows Models for Top-k Lists	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MIXING TIMES	The classic Mallows model is a widely-used tool to realize distributions on permutations. Motivated by common practical situations, in this paper, we generalize Mallows to model distributions on top-k lists by using a suitable distance measure between top-k lists. Unlike many earlier works, our model is both analytically tractable and computationally efficient. We demonstrate this by studying two basic problems in this model, namely, sampling and reconstruction, from both algorithmic and experimental points of view.	[Chierichetti, Flavio; Haddadan, Shahrzad] Sapienza Univ, Rome, Italy; [Dasgupta, Anirban] IIT, Gandhinagar, India; [Kumar, Ravi] Google, Mountain View, CA USA; [Lattanzi, Silvio] Google, Zurich, Switzerland	Sapienza University Rome; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Gandhinagar; Google Incorporated; Google Incorporated	Chierichetti, F (corresponding author), Sapienza Univ, Rome, Italy.	flavio@di.uniroma1.it; anirban.dasgupta@gmail.com; shahrzad.haddadan@uniroma1.it; ravi.k53@gmail.com; silviolat@gmail.com	Dasgupta, Anirba/AAN-9090-2020	Dasgupta, Anirba/0000-0002-8494-3692; Haddadan, shahrzad/0000-0002-7702-8250	ERC [DMAP 680153]; SIR Grant [RBSI14Q743]; Google Focused Award; "Dipartimenti di Eccellenza 2018-2022" grant	ERC(European Research Council (ERC)European Commission); SIR Grant; Google Focused Award(Google Incorporated); "Dipartimenti di Eccellenza 2018-2022" grant	Flavio Chierichetti and Shahrzad Haddadan were supported by the ERC Starting Grant DMAP 680153, by the SIR Grant RBSI14Q743, by a Google Focused Award, and by the "Dipartimenti di Eccellenza 2018-2022" grant awarded to the Dipartimento di Informatica at Sapienza.	[Anonymous], 2017, INT C MACH LEARN; Awasthi Pranjal, 2014, NIPS, P2609; Black D., 1987, THEORY COMMITTEES EL; Braverman M., 2009, 09101191 ARXIV; Chierichetti F, 2014, P APPR RAND COMB OPT, V28, P604; Chierichetti F, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P85, DOI 10.1145/2688073.2688111; Critchlow Douglas E., 1985, LECT NOTES STAT, V34; De Stefani L, 2016, AAAI CONF ARTIF INTE, P1526; DeConde R, 2006, STAT APPL GENET MOL, V5; Doignon JP, 2004, PSYCHOMETRIKA, V69, P33, DOI 10.1007/BF02295838; Fagin R, 2003, SIAM J DISCRETE MATH, V17, P134, DOI 10.1137/S0895480102412856; FLIGNER MA, 1986, J R STAT SOC B, V48, P359; Hsu DF, 2005, INFORM RETRIEVAL, V8, P449, DOI 10.1007/s10791-005-6994-4; Ilyas IF, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391730; Klementiev A., 2008, P 25 INT C MACH LEAR, P472; Kwak H, 2010, P 19 INT C WORLD WID, P591, DOI [10.1145/1772690.1772751, DOI 10.1145/1772690.1772751]; Lebanon G, 2008, J MACH LEARN RES, V9, P2401; Levin D. A., 2009, MARKOV CHAINS MIXING; Lu T, 2014, J MACH LEARN RES, V15, P3783; MARDEN J. I., 1996, ANAL MODELING RANK D; Martin R, 2006, COMB PROBAB COMPUT, V15, P411, DOI 10.1017/S0963548305007352; Meila M, 2010, J MACH LEARN RES, V11, P3481; Niu Shuzi, 2012, P 21 ACM INT C INF K, P2519; Xia Fen, 2009, ADV NEURAL INFORM PR, V22, P2098; Zhang J., 2007, P 16 INT C WORLD WID, P221, DOI DOI 10.1145/1242572.1242603	28	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304040
C	Cortes, C; Kuznetsov, V; Mohri, M; Storcheus, D; Yang, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cortes, Corinna; Kuznetsov, Vitaly; Mohri, Mehryar; Storcheus, Dmitry; Yang, Scott			Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or n-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a naive implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the n-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.	[Cortes, Corinna; Kuznetsov, Vitaly] Google Res, New York, NY 10011 USA; [Mohri, Mehryar; Storcheus, Dmitry] Courant Inst, New York, NY 10012 USA; [Mohri, Mehryar; Storcheus, Dmitry] Google Res, New York, NY 10012 USA; [Yang, Scott] DE Shaw & Co, New York, NY 10036 USA; [Yang, Scott] Courant Inst Math Sci, New York, NY USA	Google Incorporated; Google Incorporated	Cortes, C (corresponding author), Google Res, New York, NY 10011 USA.	corinna@google.com; vitalyk@google.com; mohri@cims.nyu.edu; dstorcheus@google.com; yangs@cims.nyu.edu			NSF [IIS-1618662]	NSF(National Science Foundation (NSF))	This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662.	Abadi M., 2016, P USENIX; ALLAUZEN C, 2007, P CIAA; Amodei D., 2016, P ICML; Chang Kai-Wei, 2015, P ICML; Chen T., 2015, MXNET FLEXIBLE EFFIC; Cortes C, 2004, J MACH LEARN RES, V5, P1035; Cortes C., 2015, P COLT; Cortes C., 2007, PREDICTING STRUCTURE; Cortes C., 2016, P NIPS; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Doppa JR, 2014, J MACH LEARN RES, V15, P1317; Eban E, 2017, PR MACH LEARN RES, V54, P832; Gimpel K., 2010, P ACL; Graves A., 2014, P ICML; Joachims T., 2005, P ICML; Joachims T., 2006, NEW ALGORITHMS MACRO, V49, P57; Lafferty J., 2001, DEP PAPERS CIS; Lam M., 2015, CVPR; Lucchi A., 2013, P CVPR; Manning CD, 1999, FDN STAT NATURAL LAN; McAllester D. A., 2010, P NIPS; Mohri M., 2003, International Journal of Foundations of Computer Science, V14, P957, DOI 10.1142/S0129054103002114; Mohri M, 1997, COMPUT LINGUIST, V23, P269; Mohri M., 2002, Journal of Automata, Languages and Combinatorics, V7, P321; Mohri M, 2009, MONOGR THEOR COMPUT, P213, DOI 10.1007/978-3-642-01492-5_6; Nadeau D, 2007, LINGUIST INVESTIG, V30, P3; Norouzi M., 2016, P NIPS; Och F. J., 2003, P ACL, V1; Paszke Adam, 2017, P NIPS; Poon H., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P689, DOI 10.1109/ICCVW.2011.6130310; Prabhavalkar R., 2017, ARXIV171201818; Ranjbar M, 2013, IEEE T PATTERN ANAL, V35, P911, DOI 10.1109/TPAMI.2012.168; Ranzato MarcAurelio, 2015, ARXIV; Ross Stephane, 2011, P AISTATS; Schu┬lkopf B., 2004, KERNEL METHODS COMPU; Seide F., 2016, P KDD; Shen S., 2016, P ACL, V1; Sutskever I, 2014, P 27 INT C NEURAL IN, V2; Taskar Ben, 2003, NIPS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Vinyals O., 2015, P 28 INT C NEUR INF, P2692; Vinyals O., 2015, P CVPR; Zhang D., 2008, P IJCNLP	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001036
C	Cui, H; Marinescu, R; Khardon, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cui, Hao; Marinescu, Radu; Khardon, Roni			From Stochastic Planning to Marginal MAP	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					It is well known that the problems of stochastic planning and probabilistic inference are closely related. This paper makes two contributions in this context. The first is to provide an analysis of the recently developed SOGBOFA heuristic planning algorithm that was shown to be effective for problems with large factored state and action spaces. It is shown that SOGBOFA can be seen as a specialized inference algorithm that computes its solutions through a combination of a symbolic variant of belief propagation and gradient ascent. The second contribution is a new solver for Marginal MAP (MMAP) inference. We introduce a new reduction from MMAP to maximum expected utility problems which are suitable for the symbolic computation in SOGBOFA. This yields a novel algebraic gradient-based solver (AGS) for MMAP. An experimental evaluation illustrates the potential of AGS in solving difficult MMAP problems.	[Cui, Hao] Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA; [Marinescu, Radu] IBM Res, Dublin, Ireland; [Khardon, Roni] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA	Tufts University; Indiana University System; Indiana University Bloomington	Cui, H (corresponding author), Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.	hao.cui@tufts.edu; radu.marinescu@ie.ibm.com; rkhardon@iu.edu			NSF [IIS-1616280]; Tufts Technology Services	NSF(National Science Foundation (NSF)); Tufts Technology Services	This work was partly supported by NSF under grant IIS-1616280. Some of the experiments in this paper were performed on the Tufts Linux Research Cluster supported by Tufts Technology Services.	Boutilier C., 1995, New Directions in AI Planning, P157; Cui H., 2016, P 25 INT JOINT C ART, P3075; Cui Hao, 2015, P AAAI C ART INT; Domshlak C., 2006, P 16 INT C AUT PLANN, P243; Nguyen DT, 2017, AAAI CONF ARTIF INTE, P3036; Furmston Thomas, 2010, P AISTATS, P241; Ihler A, 2012, P 28 C UNC ART INT U, P523; Keller T., 2013, P INT C AUT PLANN SC; Kiselev I, 2014, AAAI CONF ARTIF INTE, P3112; Kolobov A., 2012, P INT C AUT PLANN SC; Lee J, 2016, AAAI CONF ARTIF INTE, P3255; Lee Junkyu, 2016, P INT S ART S ART IN; Liu Q, 2013, J MACH LEARN RES, V14, P3165; Marinescu R., 2017, AAAI, P3775; Maua DD, 2016, INT J APPROX REASON, V68, P211, DOI 10.1016/j.ijar.2015.03.007; Mladenov M, 2017, AAAI CONF ARTIF INTE, P1199; Neumann G, 2011, P 28 INT C MACH LEAR, P817; Nitti D, 2015, LECT NOTES ARTIF INT, V9285, P327, DOI 10.1007/978-3-319-23525-7_20; Park JD, 2004, J ARTIF INTELL RES, V21, P101; Pearl J., 1989, MORGAN KAUFMANN SERI; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Sabbadin R, 2012, INT J APPROX REASON, V53, P66, DOI 10.1016/j.ijar.2011.09.007; Sanner S., 2010, RELATIONAL DYN UNPUB; Toussaint Marc, 2006, P INT C MACH LEARN I; van de Meent JW, 2016, JMLR WORKSH CONF PRO, V51, P1195	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303011
C	Dai, B; Dai, HJ; He, NA; Liu, WY; Liu, Z; Chen, JS; Xiao, L; Song, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dai, Bo; Dai, Hanjun; He, Niao; Liu, Weiyang; Liu, Zhen; Chen, Jianshu; Xiao, Lin; Song, Le			Coupled Variational Bayes via Optimization Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Variational inference plays a vital role in learning graphical models, especially on large-scale datasets. Much of its success depends on a proper choice of auxiliary distribution class for posterior approximation. However, how to pursue an auxiliary distribution class that achieves both good approximation ability and computation efficiency remains a core challenge. In this paper, we proposed coupled variational Bayes which exploits the primal-dual view of the ELBO with the variational distribution class generated by an optimization procedure, which is termed optimization embedding. This flexible function class couples the variational distribution with the original parameters in the graphical models, allowing end-to-end learning of the graphical models by back-propagation through the variational distribution. Theoretically, we establish an interesting connection to gradient flow and demonstrate the extreme flexibility of this implicit distribution family in the limit sense. Empirically, we demonstrate the effectiveness of the proposed method on multiple graphical models with either continuous or discrete latent variables comparing to state-of-the-art methods.	[Dai, Bo; Dai, Hanjun; Liu, Weiyang; Liu, Zhen; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Dai, Bo] Google Brain, Mountain View, CA 94043 USA; [He, Niao] Univ Illinois, Champaign, IL USA; [Chen, Jianshu] Tencent AI, Shenzhen, Peoples R China; [Xiao, Lin] Microsoft Res, Montreal, PQ, Canada; [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China	University System of Georgia; Georgia Institute of Technology; Google Incorporated; University of Illinois System; University of Illinois Urbana-Champaign	Dai, B (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.; Dai, B (corresponding author), Google Brain, Mountain View, CA 94043 USA.		Dai, Hanjun/AAQ-8943-2021		NSF [CCF-1755829, CMMI-1761699, IIS-1218749, CAREER IIS-1350983, IIS-1639792 EAGER, IIS-1841351 EAGER, CCF-1836822, CNS-1704701]; NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; Google Cloud; Siemens	NSF(National Science Foundation (NSF)); NIH BIGDATA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); Intel ISTC; NVIDIA; Amazon AWS; Google Cloud(Google Incorporated); Siemens(Siemens AG)	Part of this work was done when BD was with Georgia Tech. NH is supported in part by NSF CCF-1755829 and NSF CMMI-1761699. LS is supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF IIS-1841351 EAGER, NSF CCF-1836822, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA, Amazon AWS, Google Cloud and Siemens.	Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Belanger David, 2017, ARXIV170305667; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Chen J., 2015, ADV NEURAL INFORM PR, P1765; Chien JT, 2018, IEEE T PATTERN ANAL, V40, P318, DOI 10.1109/TPAMI.2017.2677439; Dai B, 2016, JMLR WORKSH CONF PRO, V51, P985; Dai Bo, 2016, ABS160704579 CORR; Dinh L., 2016, ARXIV160508803CSSTAT; Domke Justin, 2012, INT C ARTIFICIAL INT; Doucet A., 2001, SEQUENTIAL MONTE CAR; Gershman S.J., 2012, P 29 INT C MACH LEAR, P235; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greff K., 2017, ADV NEURAL INFORM PR, P6691, DOI DOI 10.5555/3295222.3295414; Hershey J. R., 2014, ARXIV14092574; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jaakkola Tommi S., 1999, IMPROVING MEAN FIELD, P163; Jang Eric, 2017, P 5 INT C LEARN REPR; Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105; Kim Yoon, 2018, ARXIV180202550; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kingma DP, 2 INT C LEARN REPR I, P1; Liu Q, 2017, ADV NEUR IN, V30; Maddison Chris J, 2016, ARXIV161100712; Marino Joseph, 2018, INT C MACH LEARN, P3400; Mescheder L., 2018, ARXIV170104722; Minka T., 2001, THESIS MIT CAMBRIDGE; Mnih Andriy, 2014, INT C MACH LEARN; Neal R. M., 2011, HDB MARKOV CHAIN MON, V2; Neal RM, 1993, CRGTR931 U TOR DEP C; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Otto Felix, 2001, GEOMETRY DISSIPATIVE; Rezende Danilo Jimenez, 2015, ARXIV150505770; Rockafellar RT., 1998, VARIATIONAL ANAL, DOI 10.1007/978-3-642-02431-3; Salimans Tim, 2008, INT C MACH LEARN, P1218; Shapiro A., 2014, LECT STOCHASTIC PROG, V16; Stoyanov Veselin, 2011, P AISTATS; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Tomczak Jakub M, 2016, ARXIV161109630; Tran D., 2015, ARXIV151106499; Turner R. E., 2011, BAYESIAN TIME SERIES, V1, P3; Wainwright M. J., 2003, 649 UC BERK DEP STAT; Zellner Arnold, 1988, AM STAT, V42	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004027
C	Dai, LQ; Tang, L; Xie, Y; Tang, JH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dai, Longquan; Tang, Liang; Xie, Yuan; Tang, Jinhui			Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BILATERAL FILTER; FAST APPROXIMATION	The high-dimensional convolution is widely used in various disciplines but has a serious performance problem due to its high computational complexity. Over the decades, people took a handmade approach to design fast algorithms for the Gaussian convolution. Recently, requirements for various non-Gaussian convolutions have emerged and are continuously getting higher. However, the handmade acceleration approach is no longer feasible for so many different convolutions since it is a time-consuming and painstaking job. Instead, we propose an Acceleration Network (AccNet) which turns the work of designing new fast algorithms to training the AccNet. This is done by: 1, interpreting splatting, blurring, slicing operations as convolutions; 2, turning these convolutions to gCP layers to build AccNet. After training, the activation function g together with AccNet weights automatically define the new splatting, blurring and slicing operations. Experiments demonstrate AccNet is able to design acceleration algorithms for a ton of convolutions including Gaussian/non-Gaussian convolutions and produce state-of-the-art results.	[Dai, Longquan; Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China; [Tang, Liang] CASA Environm Technol Co Ltd, CASA EM&EW IOT Res Ctr, Wuxi, Peoples R China; [Xie, Yuan] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China	Nanjing University of Science & Technology; Chinese Academy of Sciences; Institute of Automation, CAS	Tang, JH (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China.	dailongquan@njust.edu.cn; tangl@casaet.com; yuan.xie@ia.ac.cn; jinhuitang@njust.edu.cn		Tang, Jinhui/0000-0001-9008-222X	973 Program [2014CB347600]; National Natural Science Foundation of China [61701235, 61732007, 61522203, 61772275, 61873293, 61772524]; Fundamental Research Funds for the Central Universities [30917011323]; Beijing Municipal Natural Science Foundation [4182067]	973 Program(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Beijing Municipal Natural Science Foundation(Beijing Natural Science Foundation)	This work was supported by the 973 Program (Project No. 2014CB347600), the National Natural Science Foundation of China (Grant No. 61701235, 61732007, 61522203, 61772275, 61873293 and 61772524), the Fundamental Research Funds for the Central Universities (Grant No. 30917011323) and the Beijing Municipal Natural Science Foundation (Grant No. 4182067).	Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x; [Anonymous], 1998, P IEEE INT C COMP VI; Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009; Baek J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866191; Barron J. T., 2015, IEEE C COMP VIS PATT; Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Chaudhury KN, 2016, IEEE T IMAGE PROCESS, V25, P2519, DOI 10.1109/TIP.2016.2548363; Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1239451.1239554, 10.1145/1276377.1276506]; Cohen N, 2016, PR MACH LEARN RES, V48; Dai LQ, 2016, IEEE T IMAGE PROCESS, V25, P2657, DOI 10.1109/TIP.2016.2549701; Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574; Elboer Elhanan, 2013, IEEE C COMP VIS PATT; Gadde R, 2016, LECT NOTES COMPUT SC, V9905, P597, DOI 10.1007/978-3-319-46448-0_36; GREENGARD L, 1991, SIAM J SCI STAT COMP, V12, P79, DOI 10.1137/0912004; Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9; Jampani V., 2017, IEEE C COMP VIS PATT; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568; Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963; Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8; Porikli F., 2008, IEEE C COMP VIS PATT; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Sidiropoulos ND, 2017, IEEE T SIGNAL PROCES, V65, P3551, DOI 10.1109/TSP.2017.2690524; Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0; Vineet V, 2014, INT J COMPUT VISION, V110, P290, DOI 10.1007/s11263-014-0708-6; Yang Q., 2009, IEEE C COMP VIS PATT; Yang QX, 2015, INT J COMPUT VISION, V112, P307, DOI 10.1007/s11263-014-0764-y; Zbontar J., 2015, IEEE C COMP VIS PATT; Zhang B, 2008, IEEE T IMAGE PROCESS, V17, P664, DOI 10.1109/TIP.2008.919949	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301045
C	Dan, C; Liu, LQ; Aragam, B; Ravikumar, P; Xing, EP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dan, Chen; Liu Leqi; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.			The Sample Complexity of Semi-Supervised Learning with Nonparametric Mixture Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				IDENTIFIABILITY; FINITE; CONVERGENCE	We study the sample complexity of semi-supervised learning (SSL) and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the (unknown) class conditional distributions. Under these assumptions, we establish an Omega(K logK) labeled sample complexity bound without imposing parametric assumptions, where K is the number of classes. Our results suggest that even in nonparametric settings it is possible to learn a near-optimal classifier using only a few labeled samples. Unlike previous theoretical work which focuses on binary classification, we consider general multiclass classification (K > 2), which requires solving a difficult permutation learning problem. This permutation defines a classifier whose classification error is controlled by the Wasserstein distance between mixing measures, and we provide finite-sample results characterizing the behaviour of the excess risk of this classifier. Finally, we describe three algorithms for computing these estimators based on a connection to bipartite graph matching, and perform experiments to illustrate the superiority of the MLE over the majority vote estimator.	[Dan, Chen; Liu Leqi; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA	Carnegie Mellon University	Dan, C (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	cdan@cs.cmu.edu; leqil@cs.cmu.edu; naragam@cs.cmu.edu; pradeepr@cs.cmu.edu; epxing@cs.cmu.edu	Liu, Leqi/HGA-0678-2022		NSF [IIS-1149803, IIS-1664720, DMS-1264033]; ONR [N000141812861]; NIH [R01GM114311, P30DA035778]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	P.R. acknowledges the support of NSF via IIS-1149803, IIS-1664720, DMS-1264033, and ONR via N000141812861. E.X. acknowledges the support of NIH R01GM114311, P30DA035778.	[Anonymous], 2008, P ADV NEUR INF PROC; Aragam B., 2018, ARXIV180204397; Aragam B, 2016, ARXIV151108963; Azizyan M, 2013, ANN STAT, V41, P751, DOI 10.1214/13-AOS1092; BARNDORF.O, 1965, J MATH ANAL APPL, V12, P115, DOI 10.1016/0022-247X(65)90059-4; Ben-David S., 2008, C LEARN THEOR COLT; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; CASTELLI V, 1995, PATTERN RECOGN LETT, V16, P105, DOI 10.1016/0167-8655(94)00074-D; Castelli V, 1996, IEEE T INFORM THEORY, V42, P2102, DOI 10.1109/18.556600; CHEN JH, 1995, ANN STAT, V23, P221, DOI 10.1214/aos/1176324464; Collier O, 2016, J MACH LEARN RES, V17; Cozman F. G., 2003, P 20 INT C MACH LEAR, P99; Dai Z., 2017, ADV NEURAL INFORM PR, P6510; Darnstadt  Malte, 2013, UNLABELED DATA DOES; Devroye L, 2013, PROBABILISTIC THEORY, V31; FLAJOLET P, 1992, DISCRETE APPL MATH, V39, P207, DOI 10.1016/0166-218X(92)90177-C; Flammarion Nicolas, 2016, ARXIV160702435; Fogel F., 2013, ADV NEURAL INFORM PR, P1016; Globerson A., 2017, COLT, P978; Hall P, 2003, ANN STAT, V31, P201; Heinrich  Philippe, 2015, ARXIV150403506; Ho N., 2016, ARXIV160902655; Kaariainen M, 2005, LECT NOTES COMPUT SC, V3559, P127, DOI 10.1007/11503415_9; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lim C. H., 2014, ADV NEURAL INFORM PR, V27, P2168; Newman D.J., 1960, AM MATH MON, V67, P58, DOI 10.2307/2308930; Ho N, 2016, ELECTRON J STAT, V10, P271, DOI 10.1214/16-EJS1105; Niyogi P, 2013, J MACH LEARN RES, V14, P1229; Pananjady A, 2016, ANN ALLERTON CONF, P417, DOI 10.1109/ALLERTON.2016.7852261; Rigollet P, 2007, J MACH LEARN RES, V8, P1369; Seeger  Matthias, 2000, TECHNICAL REPORT; TEICHER H, 1961, ANN MATH STAT, V32, P244, DOI 10.1214/aoms/1177705155; TEICHER H, 1963, ANN MATH STAT, V34, P1265, DOI 10.1214/aoms/1177703862; TEICHER H, 1967, ANN MATH STAT, V38, P1300, DOI 10.1214/aoms/1177698805; Van de Geer S, 2013, ANN STAT, V41, P536, DOI 10.1214/13-AOS1085; Nguyen X, 2013, ANN STAT, V41, P370, DOI 10.1214/12-AOS1065; YAKOWITZ SJ, 1968, ANN MATH STAT, V39, P209, DOI 10.1214/aoms/1177698520; Zhu X., 2009, ADV NEURAL INFORM PR, P1513; Zhu Xiaojin., 2003, P ICLR, P912	40	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003084
C	Dasgupta, S; Dey, A; Roberts, N; Sabato, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dasgupta, Sanjoy; Dey, Akansha; Roberts, Nicholas; Sabato, Sivan			Learning from discriminative feature feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of learning a multi-class classifier from labels as well as simple explanations that we call discriminative features. We show that such explanations can be provided whenever the target concept is a decision tree, or can be expressed as a particular type of multi-class DNF formula. We present an efficient online algorithm for learning from such feedback and we give tight bounds on the number of mistakes made during the learning process. These bounds depend only on the representation size of the target concept and not on the overall number of available features, which could be infinite. We also demonstrate the learning procedure experimentally.	[Dasgupta, Sanjoy; Dey, Akansha; Roberts, Nicholas] Univ Calif San Diego, Dept Comp Sci & Engn, San Diego, CA 92103 USA; [Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel	University of California System; University of California San Diego; Ben Gurion University	Dasgupta, S (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, San Diego, CA 92103 USA.	dasgupta@eng.ucsd.edu; n3robert@ucsd.edu; a1dey@ucsd.edu; sabatos@cs.bgu.ac.il	Sabato, Sivan/U-4730-2017	Sabato, Sivan/0000-0002-7975-0044	National Science Foundation [CCF-1813160]; United-States-Israel Binational Science Foundation (BSF) [2017641]	National Science Foundation(National Science Foundation (NSF)); United-States-Israel Binational Science Foundation (BSF)(US-Israel Binational Science Foundation)	This research was supported by National Science Foundation grant CCF-1813160, and by a United-States-Israel Binational Science Foundation (BSF) grant no. 2017641. Part of the work was done while SD and SS were at the "Foundations of Machine Learning" program at the Simons Institute for the Theory of Computing, Berkeley.	Balcan M.-F., 2006, P 23 INT C MACH LEAR; BLUM A, 1992, MACH LEARN, V9, P373, DOI 10.1023/A:1022653502461; Branson S., 2010, EUR C COMP VIS; CROFT WB, 1990, P 13 INT C RES DEV I, P349; Druck G., 2008, P ACM SPEC INT GROUP; Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843; Mac Aodha O., 2018, IEEE C COMP VIS PATT; Parikh D., 2011, P INT C COMP VIS; Perona P, 2010, P IEEE, V98, P1526, DOI 10.1109/JPROC.2010.2049621; PITT L, 1988, J ACM, V35, P965, DOI 10.1145/48014.63140; Poulis S., 2017, 20 INT C ART INT STA; Raghavan H, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P841; Settles B., 2011, EMPIRICAL METHODS NA; Shalev-Shwartz S., 2016, C LEARNING THEORY, P815; Yang L., 2013, INNOVATIONS THEORETI	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303091
C	Daskalakis, C; Dikkala, N; Jayanti, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Daskalakis, Constantinos; Dikkala, Nishanth; Jayanti, Siddhartha			HOGWILD!-Gibbs Can Be PanAccurate	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an accurate method for estimating probabilities of events on a small number of variables of a graphical model satisfying Dobrushin's condition [DSOR16]. We investigate whether it can be used to accurately estimate expectations of functions of all the variables of the model. Under the same condition, we show that the synchronous (sequential) and asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between their (multivariate) samples remains bounded by O(tau log n), where n is the number of variables in the graphical model, and tau is a measure of the asynchronicity. A similar bound holds for any constant power of the Hamming distance. Hence, the expectation of any function that is Lipschitz with respect to a power of the Hamming distance, can be estimated with a bias that grows logarithmically in n. Going beyond Lipschitz functions, we consider the bias arising from asynchronicity in estimating the expectation of polynomial functions of all variables in the model. Using recent concentration of measure results [DDK17, GLP17, GSS18], we show that the bias introduced by the asynchronicity is of smaller order than the standard deviation of the function value already present in the true model. We perform experiments on a multiprocessor machine to empirically illustrate our theoretical findings.	[Daskalakis, Constantinos; Dikkala, Nishanth; Jayanti, Siddhartha] MIT, EECS, Cambridge, MA 02139 USA; [Daskalakis, Constantinos; Dikkala, Nishanth; Jayanti, Siddhartha] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Daskalakis, C (corresponding author), MIT, EECS, Cambridge, MA 02139 USA.; Daskalakis, C (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	costis@csail.mit.edu; nishanthd@csail.mit.edu; jayanti@mit.edu						Daskalakis C, 2017, ADV NEUR IN, V30; Daskalakis C, 2011, PROBAB THEORY REL, V149, P149, DOI 10.1007/s00440-009-0246-2; Daskalakis Constantinos, 2018, P 29 ANN ACM SIAM S; De Sa C., 2015, NIPS, P2674; De Sa C, 2016, PR MACH LEARN RES, V48; Ellison G, 1993, ECONOMETRICA, V61, P1047, DOI DOI 10.2307/2951493; Felsenstein J., 2004, INFERRING PHYLOGENIE; Geman S, 1987, P INT C MATH, V1, P1496; Gheissari Reza, 2017, ARXIV170600121; Gotze F., 2018, ARXIV180106348; JOHNSON M, 2013, ADV NEURAL INFORM PR, P2715; Levin D. A., 2009, MARKOV CHAINS MIXING; Liu J, 2015, J MACH LEARN RES, V16, P285; Mania H., 2015, ARXIV150706970; Mitliagkas I, 2015, PROC VLDB ENDOW, V8, P874, DOI 10.14778/2757807.2757812; Montanari A, 2010, P NATL ACAD SCI USA, V107, P20196, DOI 10.1073/pnas.1004098107; Noel C., 2014, NIPS WORKSH DISTR MA; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931; Terenin Alexander, 2015, ARXIV150908999; Yu HF, 2012, IEEE DATA MINING, P765, DOI 10.1109/ICDM.2012.168; Zhang C, 2014, PROC VLDB ENDOW, V7, P1283, DOI 10.14778/2732977.2733001	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300004
C	Defossez, A; Zeghidour, N; Usunier, N; Bottou, L; Bach, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Defossez, Alexandre; Zeghidour, Neil; Usunier, Nicolas; Bottou, Leon; Bach, Francis			SING: Symbol-to-Instrument Neural Generator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent progress in deep learning for audio synthesis opens the way to models that directly produce the waveform, shifting away from the traditional paradigm of relying on vocoders or MIDI synthesizers for speech or music generation. Despite their successes, current state-of-the-art neural audio synthesizers such as WaveNet and SampleRNN [24, 17] suffer from prohibitive training and inference times because they are based on autoregressive models that generate audio samples one at a time at a rate of 16kHz. In this work, we study the more computationally efficient alternative of generating the waveform frame-by-frame with large strides. We present SING, a lightweight neural audio synthesizer for the original task of generating musical notes given desired instrument, pitch and velocity. Our model is trained end-to-end to generate notes from nearly 1000 instruments with a single decoder, thanks to a new loss function that minimizes the distances between the log spectrograms of the generated and target waveforms. On the generalization task of synthesizing notes for pairs of pitch and instrument not seen during training, SING produces audio with significantly improved perceptual quality compared to a state-of-the-art autoencoder based on WaveNet [4] as measured by a Mean Opinion Score (MOS), and is about 32 times faster for training and 2; 500 times faster for inference.	[Defossez, Alexandre] PSL Res Univ, INRIA, ENS, Facebook AI Res, Paris, France; [Zeghidour, Neil] PSL Res Univ, CNRS, INRIA, LSCP,ENS,EHESS,Facebook AI Res, Paris, France; [Usunier, Nicolas] Facebook AI Res, Paris, France; [Bottou, Leon] Facebook AI Res, New York, NY USA; [Bach, Francis] PSL Res Univ, Ecole Normale Super, INRIA, Paris, France	Facebook Inc; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); Facebook Inc; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Facebook Inc; Facebook Inc; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Defossez, A (corresponding author), PSL Res Univ, INRIA, ENS, Facebook AI Res, Paris, France.	defossez@fb.com; neilz@fb.com; usunier@fb.com; leonb@fb.com; francis.bach@ens.fr						Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980; Caracalla  Hugo, 2017, DAFX 2017; EBCIOGLU K, 1988, COMPUT MUSIC J, V12, P43, DOI 10.2307/3680335; Engel  Jesse, 2017, 17040129 ARXIV; Gehring J., 2017, P ICML; GOLDSTEIN JL, 1967, J ACOUST SOC AM, V41, P676, DOI 10.1121/1.1910396; GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317; Hadjeres  Gaetan, 2016, 161201010 ARXIV; Haque  Albert, 2018, 180400047 ARXIV; Herremans  Dorien, 2016, MORPHEUS AUTOMATIC M; Hinton G., 2015, ARXIV150302531; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Itakura F, 1968, P 6 INT C ACOUSTICS, pC17; Kalchbrenner  Nal, 2018, 180208435 ARXIV; Kingma D.P, P 3 INT C LEARNING R; Macmillan N. A., 2004, DETECTION THEORY USE, DOI DOI 10.4324/9781410611147; Mehri  Soroush, 2016, 61207837 ARXIV; Oord A.V.D., 2016, SSW; Ping W., 2018, P 6 INT C LEARN REPR; Ribeiro F, 2011, INT CONF ACOUST SPEE, P2416; Sotelo J., 2017, CHAR2WAV END TO END; Sukhbaatar S, 2015, ADV NEUR IN, V28; Taigman Y., 2017, ICLR; van den Oord  Aaron, 2017, 171110433 ARXIV; Wang Y., 2017, 170310135 ARXIV; Williams R.J., 1995, BACKPROPAGATION THEO, V1, P433	26	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003058
C	Delyon, B; Portier, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Delyon, Bernard; Portier, Francois			Asymptotic optimality of adaptive importance sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Adaptive importance sampling (AIS) uses past samples to update the sampling policy q(t). Each stage t is formed with two steps : (i) to explore the space with n(t) points according to q(t) and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in this paper concerns the behavior of empirical sums based on AIS. Without making any assumption on the allocation policy n(t), the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is the same as some "oracle" strategy that knows the targeted sampling policy from the beginning. From a practical perspective, weighted AIS is introduced, a new method that allows to forget poor samples from early stages.	[Delyon, Bernard] Univ Rennes 1, IRMAR, Rennes, France; [Portier, Francois] Univ Paris Saclay, Telecom ParisTech, St Aubin, France	Universite de Rennes; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Delyon, B (corresponding author), Univ Rennes 1, IRMAR, Rennes, France.	bernard.delyon@univ-rennes1.fr; francois.portier@gmail.com						Bardenet R., 2016, ARXIV160500361; Cappe O, 2004, J COMPUT GRAPH STAT, V13, P907, DOI 10.1198/106186004X12803; Cappe O, 2008, STAT COMPUT, V18, P447, DOI 10.1007/s11222-008-9059-x; Chopin N, 2004, ANN STAT, V32, P2385, DOI 10.1214/009053604000000698; Cornuet JM, 2012, SCAND J STAT, V39, P798, DOI 10.1111/j.1467-9469.2011.00756.x; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Delyon B, 2016, BERNOULLI, V22, P2177, DOI 10.3150/15-BEJ725; Douc G. R., 2007, ESAIM-PROBAB STAT, V11, P427, DOI DOI 10.1051/PS:2007028; Douc R, 2007, ANN STAT, V35, P420, DOI 10.1214/009053606000001154; Douc R, 2008, ANN STAT, V36, P2344, DOI 10.1214/07-AOS514; Duchi J. C., 2018, ARXIV180403761; Elvira V., 2015, ARXIV151103095; Erraqabi Akram, 2016, INT C MACH LEARN, P2121; Evans M, 2000, OXFORD STAT SCI SERI; GEWEKE J, 1989, ECONOMETRICA, V57, P1317, DOI 10.2307/1913710; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; Hammersley J. M., 1964, MONTE CARLO METHODS, P50, DOI [10.1007/978-94-009-5819-7_5, DOI 10.1007/978-94-009-5819-7_5]; HANSEN LP, 1982, ECONOMETRICA, V50, P1029, DOI 10.2307/1912775; Jie T., 2010, ADV NEURAL INFORM PR, P1000; KLOEK T, 1978, ECONOMETRICA, V46, P1, DOI 10.2307/1913641; Lou Qi, 2017, ADV NEURAL INFORM PR, P3199; Marin J. M., 2012, ARXIV12112548; Neddermeyer JC, 2009, J AM STAT ASSOC, V104, P788, DOI 10.1198/jasa.2009.0122; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Oh M-S, 1992, J STAT COMPUT SIMUL, V41, P143, DOI [DOI 10.1080/00949659208810398, 10.1080/00949659208810398]; Owen A, 2000, J AM STAT ASSOC, V95, P135, DOI 10.2307/2669533; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Portier Francois, 2018, ARXIV180101797; Richard JF, 2007, J ECONOMETRICS, V141, P1385, DOI 10.1016/j.jeconom.2007.02.007; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; van der Vaart A.W, 1998, CAMBRIDGE SERIES STA, V3; Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498; Zhang P, 1996, J AM STAT ASSOC, V91, P1245, DOI 10.2307/2291743; Zhao PL, 2015, PR MACH LEARN RES, V37, P1	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303016
C	Dhouib, S; Redko, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dhouib, Sofien; Redko, Ievgen			Revisiting (epsilon, gamma, tau)-similarity learning for domain adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Similarity learning is an active research area in machine learning that tackles the problem of finding a similarity function tailored to an observable data sample in order to achieve efficient classification. This learning scenario has been generally formalized by the means of a (epsilon, gamma, tau)-good similarity learning framework in the context of supervised classification and has been shown to have strong theoretical guarantees. In this paper, we propose to extend the theoretical analysis of similarity learning to the domain adaptation setting, a particular situation occurring when the similarity is learned and then deployed on samples following different probability distributions. We give a new definition of an (epsilon, gamma)-good similarity for domain adaptation and prove several results quantifying the performance of a similarity function on a target domain after it has been trained on a source domain. We particularly show that if the source distribution dominates the target one, then principally new domain adaptation learning bounds can be proved.	[Dhouib, Sofien] Univ Claude Bernard Lyon 1, Univ Lyon, UJM St Etienne, INSA Lyon,CNRS,Inserm,CREATIS,UMR 5220,U1206, F-69100 Lyon, France; [Redko, Ievgen] Univ Lyon, UJM St Etienne, Inst Opt, CNRS,Grad Sch,Lab Hubert Curien,UMR 5516, F-42023 St Etienne, France; [Redko, Ievgen] CREATIS, Lyon, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Institut National de la Sante et de la Recherche Medicale (Inserm); Institut National des Sciences Appliquees de Lyon - INSA Lyon; UDICE-French Research Universities; Universite Claude Bernard Lyon 1; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Jean Monnet; Institut National de la Sante et de la Recherche Medicale (Inserm); Institut National des Sciences Appliquees de Lyon - INSA Lyon	Dhouib, S (corresponding author), Univ Claude Bernard Lyon 1, Univ Lyon, UJM St Etienne, INSA Lyon,CNRS,Inserm,CREATIS,UMR 5220,U1206, F-69100 Lyon, France.	sofiane.dhouib@creatis.insa-lyon.fr; ieygen.redko@uniy-st-etienne.fr			CNRS from the Defi Imag'In	CNRS from the Defi Imag'In	This work benefited from the support provided by the CNRS funding from the Defi Imag'In.	Balcan M.-F., 2008, PROC 21 ANN C LEARN, P287; Balcan MF, 2008, MACH LEARN, V72, P89, DOI 10.1007/s10994-008-5059-5; Bellet A., 2013, ARXIV; Bellet A., 2012, ICML; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cao B, 2011, P 22 INT JOINT C ART, P1204; Cortes Corinna, 2011, ALT; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107; Germain P, 2016, PR MACH LEARN RES, V48; Guo ZC, 2014, NEURAL COMPUT, V26, P497, DOI 10.1162/NECO_a_00556; Irina Nicolae, 2015, ICONIP, P10; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Mansour Y., 2009, COLT; Mansour Y., 2009, P 25 C UNC ART INT, P367; Margolis A., 2011, LIT REV DOMAIN ADAPT; Morvant E, 2012, KNOWL INF SYST, V33, P309, DOI 10.1007/s10115-012-0516-7; Nicolae MI, 2015, LECT NOTES ARTIF INT, V9284, P594, DOI 10.1007/978-3-319-23528-8_37; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Perrot M, 2015, PR MACH LEARN RES, V37, P1708; Weiss Karl, 2016, J BIG DATA, V3; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001091
C	Diakonikolas, I; Kane, DM; Stewart, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Diakonikolas, Ilias; Kane, Daniel M.; Stewart, Alistair			Sharp Bounds for Generalized Uniformity Testing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of generalized uniformity testing of a discrete probability distribution: Given samples from a probability distribution p over an unknown size discrete domain Omega, we want to distinguish, with probability at least 2/3, between the case that p is uniform on some subset of Omega versus epsilon-far, in total variation distance, from any such uniform distribution. We establish tight bounds on the sample complexity of generalized uniformity testing. In more detail, we present a computationally efficient tester whose sample complexity is optimal, within constant factors, and a matching worst-case information-theoretic lower bound. Specifically, we show that the sample complexity of generalized uniformity testing is Theta (1/(epsilon(4/3)parallel to p parallel to(3)) + 1/(epsilon(2)parallel to p parallel to(2))).	[Diakonikolas, Ilias; Stewart, Alistair] Univ Southern Calif, Los Angeles, CA 90089 USA; [Kane, Daniel M.] Univ Calif San Diego, La Jolla, CA 92093 USA	University of Southern California; University of California System; University of California San Diego	Diakonikolas, I (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	diakonik@usc.edu; dakane@ucsd.edu; stewart.al@gmail.com						Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435; Acharya  Jayadev, 2015, ADV NEURAL INFORM PR, P3591; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Batu T., 2017, CORR; Batu T., 2004, P 36 ANN ACM S THEOR, P381; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Canonne C. L., 2016, 33 S THEOR ASP COMP; Canonne CL, 2018, ACM S THEORY COMPUT, P735, DOI 10.1145/3188745.3188756; Canonne Clement L., 2015, ELECT C COMPUTATIONA, V22, P63; Canonne Clement L., 2017, CORR; Canonne Clement L., 2017, P 30 C LEARN THEOR C, P370; Chan S., 2014, P 25 ANN ACM SIAM S, P1193, DOI [10.1137/1.9781611973402.88, DOI 10.1137/1.9781611973402.88]; Daskalakis C., 2016, CORR; Daskalakis C, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1833; Daskalakis Constantinos, 2017, P 30 C LEARNING THEO, P697; Diakonikolas I., 2016, ELECT C COMPUTATIONA, V23, P178; Diakonikolas I., 2017, 44 INT C AUT LANG PR, DOI 10.4230/LIPIcs.ICALP.2017.8; Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78; Diakonikolas I, 2015, ANN IEEE SYMP FOUND, P1183, DOI 10.1109/FOCS.2015.76; Diakonikolas Ilias, 2015, P TWENTYSIXTH ANN AC, P1841, DOI [10.1137/1.9781611973730.123, DOI 10.1137/1.9781611973730.123]; Diakonikolas Ilias, 2017, CORR; Diakonikolas Ilias, 2018, 45 INT C 2018; Goldreich O., 2000, TR00020 EL C COMP CO; Goldreich O., 2016, ECCC, V23; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987; Rubinfeld R., 2012, XRDS CROSSROADS FAL, V19, P24, DOI DOI 10.1145/2331042.2331052; Valiant G., 2014, FOCS; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000068
C	Ding, YX; Zhou, ZH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ding, Yao-Xiang; Zhou, Zhi-Hua			Preference Based Adaptation for Learning Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In many real-world learning tasks, it is hard to directly optimize the true performance measures, meanwhile choosing the right surrogate objectives is also difficult. Under this situation, it is desirable to incorporate an optimization of objective process into the learning loop based on weak modeling of the relationship between the true measure and the objective. In this work, we discuss the task of objective adaptation, in which the learner iteratively adapts the learning objective to the underlying true objective based on the preference feedback from an oracle. We show that when the objective can be linearly parameterized, this preference based learning problem can be solved by utilizing the dueling bandit model. A novel sampling based algorithm (DLM)-M-2 is proposed to learn the optimal parameter, which enjoys strong theoretical guarantees and efficient empirical performance. To avoid learning a hypothesis from scratch after each objective function update, a boosting based hypothesis adaptation approach is proposed to efficiently adapt any pre-learned element hypotheses to the current objective. We apply the overall approach to multi-label learning, and show that the proposed approach achieves significant performance under various multi-label performance measures.	[Ding, Yao-Xiang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Ding, YX (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	dingyx@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn			National Key R&D Program of China [2018YFB1004300]; NSFC [61751306]; Collaborative Innovation Center of Novel Software Technology and Industrialization; Outstanding PhD Candidate Program of Nanjing University	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Center of Novel Software Technology and Industrialization; Outstanding PhD Candidate Program of Nanjing University	This research is supported by National Key R&D Program of China (2018YFB1004300), NSFC (61751306) and Collaborative Innovation Center of Novel Software Technology and Industrialization. Yao-Xiang Ding is supported by the Outstanding PhD Candidate Program of Nanjing University. The Authors would like to thank the anonymous reviewers for constructive suggestions, as well as Lijun Zhang, Ming Pang, Xi-Zhu Wu and Yichi Xiao for helpful discussions.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abeille M, 2017, PR MACH LEARN RES, V54, P176; Agarwal A., 2014, COLT, V35, P726; Chapelle O., 2010, P 16 ACM SIGKDD INT, P1189, DOI [10.1145/1835804.1835953, DOI 10.1145/1835804.1835953]; Deb Kalyanmoy, 2014, SEARCH METHODOLOGIES, V4, P5, DOI DOI 10.1007/978-1-4614-6940-7_15; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Kumagai Wataru, 2017, P ADV NEUR INF PROC, P1488; Li N, 2013, IEEE T PATTERN ANAL, V35, P1370, DOI 10.1109/TPAMI.2012.172; Rosset S, 2004, J MACH LEARN RES, V5, P941; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Wu XZ, 2017, PR MACH LEARN RES, V70; Yue Y., 2009, ICML, P1201; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zhang LJ, 2016, PR MACH LEARN RES, V48; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002038
C	Djolonga, J; Jegelka, S; Krause, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Djolonga, Josip; Jegelka, Stefanie; Krause, Andreas			Provable Variational Inference for Constrained Log-Submodular Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				THEOREMS; TREES	Submodular maximization problems appear in several areas of machine learning and data science, as many useful modelling concepts such as diversity and coverage satisfy this natural diminishing returns property. Because the data defining these functions, as well as the decisions made with the computed solutions, are subject to statistical noise and randomness, it is arguably necessary to go beyond computing a single approximate optimum and quantify its inherent uncertainty. To this end, we define a rich class of probabilistic models associated with constrained submodular maximization problems. These capture log-submodular dependencies of arbitrary order between the variables, but also satisfy hard combinatorial constraints. Namely, the variables are assumed to take on one of - possibly exponentially many - set of states, which form the bases of a matroid. To perform inference in these models we design novel variational inference algorithms, which carefully leverage the combinatorial and probabilistic properties of these objects. In addition to providing completely tractable and well-understood variational approximations, our approach results in the minimization of a convex upper bound on the log-partition function. The bound can be efficiently evaluated using greedy algorithms and optimized using any first-order method. Moreover, for the case of facility location and weighted coverage functions, we prove the first constant factor guarantee in this setting-an efficiently certifiable e/(e - 1) approximation of the log-partition function. Finally, we empirically demonstrate the effectiveness of our approach on several instances.	[Djolonga, Josip; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; Massachusetts Institute of Technology (MIT)	Djolonga, J (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	josipd@inf.ethz.ch; stefje@csail.mit.edu; krausea@ethz.ch		Krause, Andreas/0000-0001-7260-9673	ERC [StG 307036]; Google European PhD Fellowship; NSF CAREER award [1553284]	ERC(European Research Council (ERC)European Commission); Google European PhD Fellowship(Google Incorporated); NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	The research was partially supported by ERC StG 307036, Google European PhD Fellowship, and NSF CAREER award 1553284.	Agrawal S, 2010, PROC APPL MATH, V135, P1087; Bertsekas D, 1999, NONLINEAR PROGRAMMIN; Borcea J, 2009, J AM MATH SOC, V22, P521; Bouchard-Cote A., 2010, NEURAL INFORM PROCES; BURTON R, 1993, ANN PROBAB, V21, P1329, DOI 10.1214/aop/1176989121; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Celis LE, 2018, PR MACH LEARN RES, V80; Chekuri C, 2010, ANN IEEE SYMP FOUND, P575, DOI 10.1109/FOCS.2010.60; DJOLONGA J., 2015, INT C MACH LEARN ICM; Djolonga J., 2016, NEURAL INFORM PROCES; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; DRESS AWM, 1995, APPL MATH LETT, V8, P77, DOI 10.1016/0893-9659(95)00070-7; Edmonds J., 1971, MATH PROGRAM, V1, P127, DOI [10.1007/BF01584082, DOI 10.1007/BF01584082, 10.1007/bf01584082]; Fujishige S, 2003, MATH OPER RES, V28, P463, DOI 10.1287/moor.28.3.463.16393; Gomes R., 2010, P 27 INT C MACHINE L, P391; Gomez-Rodriguez M., 2012, ACM T KNOWLEDGE DISC; Gotovos A., 2015, NEURAL INFORM PROCES; Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014; Hazan Tamir, 2012, ARXIV12066410; Jerrum M., 1998, MATH FDN MARKOV CHAI, P116; Karimi M.R., 2017, NIPS, P6856; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Koo T., 2007, EMNLP; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kulesza A., 2012, FDN TRENDS MACHINE L, V5, P2; Leme RP, 2017, GAME ECON BEHAV, V106, P294, DOI 10.1016/j.geb.2017.10.016; Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420; Li C., 2016, ADV NEURAL INFORM PR, P4188; Lyons R, 2003, PUBL MATH-PARIS, P167; MAURER SB, 1976, SIAM J APPL MATH, V30, P143, DOI 10.1137/0130017; Minka T., 2005, TECH REP; Murota K., 2003, DISCRETE CONVEX ANAL; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; OXLEY J. G., 2006, MATROID THEORY, V3; Papandreou G, 2011, IEEE I CONF COMP VIS, P193, DOI 10.1109/ICCV.2011.6126242; Rebeschini P., 2015, 28 C LEARN THEOR COL; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Risteski A., 2016, C LEARN THEOR; Shioura A, 2009, DISCRET MATH ALGORIT, V1, P1, DOI 10.1142/S1793830909000063; Smith D. A., 2008, P C EMP METH NAT LAN; Swersky K., 2012, ADV NEURAL INFORM PR, P3293; Tarlow D, 2012, ARXIV12104899; Tschiatschek S., 2016, INT C ART INT STAT A; van Erven T, 2012, ARXIV12062459; Vondrak J., 2007, THESIS; Wainwright MJ, 2006, IEEE T SIGNAL PROCES, V54, P2099, DOI 10.1109/TSP.2006.874409; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Zhang J, 2015, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2015.216	51	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302069
C	Dong, CS; Chen, YR; Zeng, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dong, Chaosheng; Chen, Yiran; Zeng, Bo			Generalized Inverse Optimization through Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Inverse optimization is a powerful paradigm for learning preferences and restrictions that explain the behavior of a decision maker, based on a set of external signal and the corresponding decision pairs. However, most inverse optimization algorithms are designed specifically in batch setting, where all the data is available in advance. As a consequence, there has been rare use of these methods in an online setting suitable for real-time applications. In this paper, we propose a general framework for inverse optimization through online learning. Specifically, we develop an online learning algorithm that uses an implicit update rule which can handle noisy data. Moreover, under additional regularity assumptions in terms of the data and the model, we prove that our algorithm converges at a rate of O(1/root T) and is statistically consistent. In our experiments, we show the online learning approach can learn the parameters with great accuracy and is very robust to noises, and achieves a dramatic improvement in computational efficacy over the batch learning approach.	[Dong, Chaosheng; Zeng, Bo] Univ Pittsburgh, Dept Ind Engn, Pittsburgh, PA 15260 USA; [Chen, Yiran] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Duke University	Dong, CS (corresponding author), Univ Pittsburgh, Dept Ind Engn, Pittsburgh, PA 15260 USA.	chaosheng@pitt.edu; yiran.chen@duke.edu; bzeng@pitt.edu			National Science Foundation [CMMI-1642514]; NSF at the Pittsburgh Supercomputing Center (PSC) [ACI-1445606]	National Science Foundation(National Science Foundation (NSF)); NSF at the Pittsburgh Supercomputing Center (PSC)	This work was partially supported by CMMI-1642514 from the National Science Foundation. This work used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).	Aggarwal CC, 2016, RECOMMENDER SYSTEMS, P1; Ahuja RK, 2001, OPER RES, V49, P771, DOI 10.1287/opre.49.5.771.10607; [Anonymous], 2006, RECENT ADV OPTIMIZAT; Aswani  Anil, 2018, OPERATIONS RES; BARMANN A, 2017, ICML; Bertsimas D, 2015, MATH PROGRAM, V153, P595, DOI 10.1007/s10107-014-0819-4; Bertsimas D, 2012, OPER RES, V60, P1389, DOI 10.1287/opre.1120.1115; Bezanson J, 2017, SIAM REV, V59, P65, DOI 10.1137/141000671; Bonnans J., 2013, PERTURBATION ANAL OP; Bonnans JF, 1998, SIAM REV, V40, P228, DOI 10.1137/S0036144596302644; Chan T H, 2018, MANAGEMENT SCI; Chan TCY, 2014, OPER RES, V62, P680, DOI 10.1287/opre.2014.1267; Cheng L., 2007, NIPS; Dong Chaosheng, 2018, ARXIV180800935; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Esfahani P. M., 2017, MATH PROGRAM, V171, P1; Iyengar G, 2005, OPER RES LETT, V33, P319, DOI 10.1016/j.orl.2004.04.007; Keshavarz A, 2011, 2011 IEEE INTERNATIONAL SYMPOSIUM ON INTELLIGENT CONTROL (ISIC), P613, DOI 10.1109/ISIC.2011.6045410; Kulis B., 2010, ICML; Mas-Collell Andrew., 1995, MICROECONOMIC THEORY; Nystrom N. A., 2015, P 2015 XSEDE C SCI A; Schaefer AJ, 2009, OPTIM LETT, V3, P483, DOI 10.1007/s11590-009-0131-z; Wang LZ, 2009, OPER RES LETT, V37, P114, DOI 10.1016/j.orl.2008.12.001; Wang T, 2017, J MACH LEARN RES, V18, P1; YANG H, 1992, TRANSPORT RES B-METH, V26, P417, DOI 10.1016/0191-2615(92)90008-K	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300009
C	Draief, M; Kutzkov, K; Scaman, K; Vojnovic, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Draief, Moez; Kutzkov, Konstantin; Scaman, Kevin; Vojnovic, Milan			KONG: Kernels for ordered-neighborhood graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present novel graph kernels for graphs with node and edge labels that have ordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with ordered neighborhoods are a natural data representation for evolving graphs where edges are created over time, which induces an order. Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. We obtain precise bounds for the approximation accuracy and computational complexity of the proposed approaches and demonstrate their applicability on real datasets. In particular, our experiments demonstrate that neighborhood ordering results in more informative features. For the special case of general graphs, i.e., graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs.	[Draief, Moez; Scaman, Kevin] Huawei Noahs Ark Lab, Hong Kong, Peoples R China; [Kutzkov, Konstantin; Vojnovic, Milan] London Sch Econ, London, England	Huawei Technologies; University of London; London School Economics & Political Science	Kutzkov, K (corresponding author), London Sch Econ, London, England.	moez.draief@huawei.com; kutzkov@gmail.com; kevin.scaman@huawei.com; m.vojnovic@lse.ac.uk			Huawei Technologies	Huawei Technologies(Huawei Technologies)	The work has been supported by a research collaboration grant funded by Huawei Technologies.	Borgwardt K. M., 2005, 5 IEEE INT C DAT MIN, DOI DOI 10.1109/ICDM.2005.132; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; Da San Martino G., 2012, SIAM SDM, P975, DOI DOI 10.1137/1.9781611972825.84; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Feigenbaum J, 2005, THEOR COMPUT SCI, V348, P207, DOI 10.1016/j.tcs.2005.09.013; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Haussler D, 1999, CONVOLUTION KERNELS, P95; Helma C, 2001, BIOINFORMATICS, V17, P107, DOI 10.1093/bioinformatics/17.1.107; Joachims T., 2006, P 12 ACM SIGKDD INT, V06, P217, DOI DOI 10.1145/1150402.1150429; Kersting Kristian, 2016, BENCHMARK DATA SETS; Kriege N, 2014, IEEE DATA MINING, P881, DOI 10.1109/ICDM.2014.129; Kriege Nils M., 2017, ABS170300676 CORR; Le Quoc V., 2013, INT C MACH LEARN, P244; Leslie C., 2002, P PSB, V575, P564; Manzoor E, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1035, DOI 10.1145/2939672.2939783; Martino GDS, 2013, IJCAI 2013, P1294; Niepert M, 2016, PR MACH LEARN RES, V48; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rual JF, 2005, NATURE, V437, P1173, DOI 10.1038/nature04209; Schomburg I, 2004, NUCLEIC ACIDS RES, V32, pD431, DOI 10.1093/nar/gkh081; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Wale N, 2006, IEEE DATA MINING, P678; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304009
C	Dunner, C; Parnell, T; Sarigiannis, D; Ioannou, N; Anghel, A; Ravi, G; Kandasamy, M; Pozidis, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dunner, Celestine; Parnell, Thomas; Sarigiannis, Dimitrios; Ioannou, Nikolas; Anghel, Andreea; Ravi, Gummadi; Kandasamy, Madhusudanan; Pozidis, Haralampos			Snap ML: A Hierarchical Framework for Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We describe a new software framework for fast training of generalized linear models. The framework, named Snap Machine Learning (Snap ML), combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems. We prove theoretically that such a hierarchical system can accelerate training in distributed environments where intra-node communication is cheaper than inter-node communication. Additionally, we provide a review of the implementation of Snap ML in terms of GPU acceleration, pipelining, communication patterns and software architecture, highlighting aspects that were critical for achieving high performance. We evaluate the performance of Snap ML in both single-node and multi-node environments, quantifying the benefit of the hierarchical scheme and the data streaming functionality, and comparing with other widely-used machine learning software frameworks. Finally, we present a logistic regression benchmark on the Criteo Terabyte Click Logs dataset and show that Snap ML achieves the same test loss an order of magnitude faster than any of the previously reported results, including those obtained using TensorFlow and scikit-learn.	[Dunner, Celestine; Parnell, Thomas; Sarigiannis, Dimitrios; Ioannou, Nikolas; Anghel, Andreea; Pozidis, Haralampos] IBM Res, Zurich, Switzerland; [Ravi, Gummadi; Kandasamy, Madhusudanan] IBM Syst, Bangalore, Karnataka, India	International Business Machines (IBM)	Dunner, C (corresponding author), IBM Res, Zurich, Switzerland.	cdu@zurich.ibm.com; tpa@zurich.ibm.com; rig@zurich.ibm.com; nio@zurich.ibm.com; aan@zurich.ibm.com; ravigumm@in.ibm.com; madhusudanan@in.ibm.com; hap@zurich.ibm.com						Abadi M, 2015, P 12 USENIX S OPERAT; Barthelemy Dagenais, 2018, PY4J BRIDGE PYTHON J; Chang C.-C., 2011, ACM T INTELLIGENT SY; Criteo Labs, 2015, CRIT REL IND LARG EV; Criteo Labs, 2018, LEARN CLICK THROUGH; Dunner  Celestine, 2017, ADV NEURAL INFORM PR, V30, P4261; Dunner  Celestine, 2017, P IEEE INT C BIG DAT, P99; Marsaglia G., 2003, J STAT SOFTW, V8, P1, DOI [10.18637/jss.v008.i18, DOI 10.18637/JSS.V008.I18]; Meng X., 2016, J MACH LEARN RES, V17, P1235, DOI DOI 10.1145/2882903.2912565; Meza Gonzalo Gasca, 2017, SAMPLES GOOGLE CLOUD; NVIDIA, 2018, THRUST; Parnell T, 2017, IEEE SYM PARA DISTR, P419, DOI 10.1109/IPDPSW.2017.140; Parnell  Thomas, 2018, FUTURE GENERATION CO; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rambler Digital Solutions, 2017, CRIT 1TB BENCHM; Smith V, 2018, J MACH LEARN RES, V18; Stack Overflow, 2016, US LARG DAT TENS; Sterbenz  Andreas, 2017, USING GOOGLE CLOUD M; Yu HF, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086743; Yu HF, 2011, MACH LEARN, V85, P41, DOI 10.1007/s10994-010-5221-8; Zhang H, 2016, IEEE DATA MINING, P619, DOI [10.1109/ICDM.2016.0073, 10.1109/ICDM.2016.171]	22	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300024
C	Dwivedi, R; Ho, N; Khamaru, K; Wainwright, MJ; Jordan, MI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dwivedi, Raaz; Ho, Nhat; Khamaru, Koulik; Wainwright, Martin J.; Jordan, Michael I.			Theoretical guarantees for the EM algorithm when applied to mis-specified Gaussian mixture models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVERGENCE PROPERTIES; FINITE; IDENTIFIABILITY	Recent years have witnessed substantial progress in understanding the behavior of EM for mixture models that are correctly specified. Given that model mis-specification is common in practice, it is important to understand EM in this more general setting. We provide non-asymptotic guarantees for the population and sample-based EM algorithms when used to estimate parameters of certain mis-specified Gaussian mixture models. Due to mis-specification, the EM iterates no longer converge to the true model and instead converge to the projection of the true model onto the fitted model class. We provide two classes of theoretical guarantees: (a) a characterization of the bias introduced due to the mis-specification; and (b) guarantees of geometric convergence of the population EM to the model projection given a suitable initialization. This geometric convergence rate for population EM implies that the EM algorithm based on n samples converges to an estimate with 1/root n accuracy. We validate our theoretical findings in different cases via several numerical examples.	[Dwivedi, Raaz; Ho, Nhat; Khamaru, Koulik; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Wainwright, Martin J.] Univ Calif Berkeley, Voleon Grp, Berkeley, CA USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley	Dwivedi, R (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	raaz.rsk@berkeley.edu; minhnhat@berkeley.edu; koulik@berkeley.edu; wainwrig@berkeley.edu; jordan@berkeley.edu	Jordan, Michael I/C-5253-2013; Dwivedi, Raaz/AAZ-2028-2020	Jordan, Michael/0000-0001-8935-817X				Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435; Cai T. T., ANN STAT; DASKALAKIS C., 2017, P 2017 C LEARN THEOR; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Hao B., J MACHINE LEARNING R; Heinrich P, 2018, ANN STAT, V46, P2844, DOI 10.1214/17-AOS1641; Jin C., 2016, ADV NEURAL INFORM PR, V29; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Ma JW, 2000, NEURAL COMPUT, V12, P2881, DOI 10.1162/089976600300014764; TEICHER H, 1963, ANN MATH STAT, V34, P1265, DOI 10.1214/aoms/1177703862; Vershynin R., ARXIV10113027V7; Villani C., 2008, OPTIMAL TRANSPORT OL; Wang Z., 2015, ADV NEURAL INFORM PR, V28; Wellner J.A., 2000, WEAK CONVERGENCE EMP; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Xu J., 2016, ADV NEURAL INFORM PR, V29; Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129; Nguyen X, 2013, ANN STAT, V41, P370, DOI 10.1214/12-AOS1065; Yan BW, 2017, ADV NEUR IN, V30; Yi X., 2015, ADV NEURAL INFORM PR, V28	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004026
C	Elsayed, GF; Krishnan, D; Mobahi, H; Regan, K; Bengio, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Elsayed, Gamaleldin F.; Krishnan, Dilip; Mobahi, Hossein; Regan, Kevin; Bengio, Samy			Large Margin Deep Networks for Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a formulation of deep learning that aims at producing a large margin classifier. The notion of margin, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classification and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any l(p) norm (p > 1) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classification loss functions. Specifically, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques such as weight decay, dropout, and batch norm.(2 )	[Elsayed, Gamaleldin F.; Krishnan, Dilip; Mobahi, Hossein; Regan, Kevin; Bengio, Samy] Google Res, Salt Lake City, UT 84102 USA	Google Incorporated	Elsayed, GF (corresponding author), Google Res, Salt Lake City, UT 84102 USA.	gamaleldin@google.com; dilipkay@google.com; hmobahi@google.com; kevinregan@google.com; bengio@google.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Athalye Anish, 2017, ARXIV PREPRINT ARXIV; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Boyd S, 2004, CONVEX OPTIMIZATION; Cisse M, 2017, PR MACH LEARN RES, V70; Cortes C., 1995, MACH LEARN, P273, DOI [10.1023/A:1022627411411, DOI 10.1007/BF00994018]; Drucker H, 1997, ADV NEUR IN, V9, P155; Frossard P, 2016, ARXIV161008401; Gal Y., 2017, ARXIV170302910; Goodfellow I. J., 2014, ARXIV14126572; Guo Chuan, 2017, ARXIV171100117; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; Hinton G., NEURAL NETWORKS MACH; Hosseini H., 2017, ARXIV170405051; Huang R., 2015, ARXIV PREPRINT ARXIV; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin Alexey, 2016, ARXIV E PRINTS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liang XZ, 2017, LECT NOTES COMPUT SC, V10635, P413, DOI 10.1007/978-3-319-70096-0_43; Liu WY, 2016, PR MACH LEARN RES, V48; Madry A., 2018, ARXIV PREPRINT ARXIV; Matyasko A, 2017, IEEE IJCNN, P300, DOI 10.1109/IJCNN.2017.7965869; Papernot N., 2016, ARXIV160202697; Rabinovich S. E, 2014, P 3 INT C LEARN REPR; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Sokolic Jure, 2016, CORR; Srebro N., 2017, ARXIV171010345; Sukhbaatar Sainbayar, 2014, ICLR; Szegedy C., 2013, CORR; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tieleman T, 2012, COURSERA NEURAL NETW, V4, P26; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vinyals Oriol, 2016, ARXIV160604080, P3630; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; [No title captured]	37	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300078
C	Erdogdu, MA; Mackey, L; Shamir, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Erdogdu, Murat A.; Mackey, Lester; Shamir, Ohad			Global Non-convex Optimization with Discretized Diffusions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVERGENCE	An Euler discretization of the Langevin diffusion is known to converge to the global minimizers of certain convex and non-convex optimization problems. We show that this property holds for any suitably smooth diffusion and that different diffusions are suitable for optimizing different classes of convex and non-convex functions. This allows us to design diffusions suitable for globally optimizing convex and non-convex functions not covered by the existing Langevin theory. Our non-asymptotic analysis delivers computable optimization and integration error bounds based on easily accessed properties of the objective and chosen diffusion. Central to our approach are new explicit Stein factor bounds on the solutions of Poisson equations. We complement these results with improved optimization guarantees for targets other than the standard Gibbs measure.	[Erdogdu, Murat A.] Univ Toronto, Toronto, ON, Canada; [Erdogdu, Murat A.] Vector Inst, Toronto, ON, Canada; [Mackey, Lester] Microsoft Res, Montreal, PQ, Canada; [Shamir, Ohad] Weizmann Inst Sci, Rehovot, Israel	University of Toronto; Weizmann Institute of Science	Erdogdu, MA (corresponding author), Univ Toronto, Toronto, ON, Canada.; Erdogdu, MA (corresponding author), Vector Inst, Toronto, ON, Canada.	erdogdu@cs.toronto.edu; lmackey@microsoft.com; ohad.shamir@weizmann.ac.il						Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Cerrai Sandra, 2001, 2 ORDER PDES FINITE, V1762; Chen C., 2015, NIPS; Cheng X., 2018, ARXIV180501648; Dalalyan AS, 2012, J COMPUT SYST SCI, V78, P1423, DOI 10.1016/j.jcss.2011.12.023; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238; Dwivedi R., 2018, ARXIV180102309; Eberle A, 2016, PROBAB THEORY REL, V166, P851, DOI 10.1007/s00440-015-0673-1; Erdogdu M. A., 2019, MULTIVARIATE S UNPUB; Gao X., 2018, ARXIV180904618; GELFAND SB, 1991, SIAM J CONTROL OPTIM, V29, P999, DOI 10.1137/0329055; Gorham J., 2016, ARXIV161106972; Hartley R., 2004, ROBOTICA; Jameson GJO, 2013, AM MATH MON, V120, P936, DOI 10.4169/amer.math.monthly.120.10.936; Khasminskii, 2012, STOCHASTIC STABILITY; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; Mathai A.M., 1992, QUADRATIC FORMS RAND; Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3; Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527; Pardoux E, 2001, ANN PROBAB, V29, P1061; Raginsky Maxim, 2017, ARXIV170203849; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Wang F., 2016, ARXIV160804471; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xu P., 2018, ADV NEURAL INFORM PR, P3126; Zwillinger D., 2014, TABLE INTEGRALS SERI, V8th, P1014, DOI DOI 10.1016/B978-0-12-384933-5.00002-3; [No title captured]	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004025
C	Evans, TW; Nair, PB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Evans, Trefor W.; Nair, Prasanth B.			Discretely Relaxing Continuous Variables for tractable Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed "DIRECT" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient estimates), eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition, our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. The DIRECT approach is not practical for all likelihoods, however, we identify a popular model structure which is practical, and demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered in the numerical studies require over 10(2352) log-likelihood evaluations, we train on datasets with over two-million points in just seconds.	[Evans, Trefor W.; Nair, Prasanth B.] Univ Toronto, Toronto, ON, Canada	University of Toronto	Evans, TW (corresponding author), Univ Toronto, Toronto, ON, Canada.	trefor.evans@mail.utoronto.ca; pbn@utias.utoronto.ca			NSERC Discovery Grant; Canada Research Chairs program	NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada Research Chairs program(Canada Research Chairs)	Research funded by an NSERC Discovery Grant and the Canada Research Chairs program.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2017, PROC INT C LEARN REP; Bishop C.M, 2006, PATTERN RECOGN; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; GOODMAN LA, 1974, BIOMETRIKA, V61, P215, DOI 10.2307/2334349; HAN S., 2015, ARXIV151000149; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hubara I, 2016, ADV NEUR IN, V29; Jacob Benoit, 2017, ARXIV171205877; Jang Eric, 2017, P 5 INT C LEARN REPR; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma DP, 2 INT C LEARN REPR I, P1; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Lawson J., 2017, ADV NEURAL INFORM PR, P2627; Lazarsfeld P.F., 1968, LATENT STRUCTURE ANA; Li Fengfu, 2016, ARXIV; Maddison Chris J, 2016, ARXIV161100712; Nielsen F, 2016, ARXIV160605850; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Thrun S., 2005, PROBABILISTIC ROBOTI; Tran Dustin, 2016, ARXIV161009787; Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Welling M, 2017, P 31 C NEUR INF PROC, P3288; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; [No title captured]; [No title captured]; [No title captured]	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005008
C	Evron, I; Moroshko, E; Crammer, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Evron, Itay; Moroshko, Edward; Crammer, Koby			Efficient Loss-Based Decoding on Graphs for Extreme Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In extreme classification problems, learning algorithms are required to map instances to labels from an extremely large label set. We build on a recent extreme classification framework with logarithmic time and space [19], and on a general approach for error correcting output coding (ECOC) with loss-based decoding [1], and introduce a flexible and efficient approach accompanied by theoretical bounds. Our framework employs output codes induced by graphs, for which we show how to perform efficient loss-based decoding to potentially improve accuracy. In addition, our framework offers a tradeoff between accuracy, model size and prediction time. We show how to find the sweet spot of this tradeoff using only the training data. Our experimental study demonstrates the validity of our assumptions and claims, and shows that our method is competitive with state-of-the-art algorithms.	[Evron, Itay] Technion, Comp Sci Dept, Haifa, Israel; [Moroshko, Edward; Crammer, Koby] Technion, Elect Engn Dept, Haifa, Israel	Technion Israel Institute of Technology; Technion Israel Institute of Technology	Evron, I (corresponding author), Technion, Comp Sci Dept, Haifa, Israel.	evron.itay@gmail.com; edward.moroshko@gmail.com; koby@ee.technion.ac.il			Israel Science Foundation [2030/16]	Israel Science Foundation(Israel Science Foundation)	We would like to thank Eyal Bairey for the fruitful discussions. This research was supported in part by The Israel Science Foundation, grant No. 2030/16.	Allwein E. L., 2000, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133; Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741; Bengio Samy, 2010, ADV NEURAL INFORM PR, V1, P163, DOI [10.5555/2997189.2997208, DOI 10.5555/2997189.2997208]; Beygelzimer A., 2009, P 25 C UNC ART INT, P51; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Choromanska A. E., 2015, ADV NEURAL INFORM PR, P55; Cisse M., 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P506, DOI 10.1007/978-3-642-33460-3_38; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; Crammer K, 2013, MACH LEARN, V91, P155, DOI 10.1007/s10994-013-5327-x; Daume H, 2017, PR MACH LEARN RES, V70; Dembczyski K, 2016, P 2016 EUR C MACH LE, P511, DOI DOI 10.1007/978-3-319-46227-132; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; Escalera S, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P117; Furnkranz J, 2002, J MACH LEARN RES, V2, P721, DOI 10.1162/153244302320884605; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Jasinska Kalina, 2016, ARXIV161101964; Jernite Y, 2017, PR MACH LEARN RES, V70; Kibriya AM, 2007, LECT NOTES ARTIF INT, V4702, P140; Li SR, 2015, LECT NOTES COMPUT SC, V9371, P259, DOI 10.1007/978-3-319-25087-8_25; Lin S., 2004, ERROR CONTROL CODING; Liu MX, 2016, IEEE T PATTERN ANAL, V38, P2335, DOI 10.1109/TPAMI.2015.2430325; Mesterharm Chris, 1999, ADV NEURAL INFORM PR; Morin F., 2005, PROC INT WORKSHOP AR, P246; Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231; Prabhu Y., 2018, P 2018 WORLD WID WEB; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Schapire R. E., 2013, EMPIRICAL INFERENCE, P37, DOI DOI 10.1007/978-3-642-41136-6_5; Tagami Y, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P455, DOI 10.1145/3097983.3097987; VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010; Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219; Weston Jason, 2011, 22 INT JOINT C ART I; Yen IEH, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P545, DOI 10.1145/3097983.3098083; Yen IEH, 2016, PR MACH LEARN RES, V48; Yu H.-F., 2014, INT C MACH LEARN, P593; Zhao Bin, 2013, INT J COMPUT VISION, V119, P60	37	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001075
C	Farina, G; Gatti, N; Sandholm, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Farina, Gabriele; Gatti, Nicola; Sandholm, Tuomas			Practical exact algorithm for trembling-hand equilibrium refinements in games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Nash equilibrium strategies have the known weakness that they do not prescribe rational play in situations that are reached with zero probability according to the strategies themselves, for example, if players have made mistakes. Trembling-hand refinements-such as extensive-form perfect equilibria and quasi-perfect equilibria-remedy this problem in sound ways. Despite their appeal, they have not received attention in practice since no known algorithm for computing them scales beyond toy instances. In this paper, we design an exact polynomial-time algorithm for finding trembling-hand equilibria in zero-sum extensive-form games. It is several orders of magnitude faster than the best prior ones, numerically stable, and quickly solves game instances with tens of thousands of nodes in the game tree. It enables, for the first time, the use of trembling-hand refinements in practice.	[Farina, Gabriele; Sandholm, Tuomas] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; [Gatti, Nicola] Politecn Milan, DEIB, Milan, Italy	Carnegie Mellon University; Polytechnic University of Milan	Farina, G (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	gfarina@cs.cmu.edu; nicola.gatti@polimi.it; sandholm@cs.cmu.edu	Gatti, Nicola/AFS-7062-2022	Gatti, Nicola/0000-0001-7349-3932	National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]	National Science Foundation(National Science Foundation (NSF)); ARO	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082.	[Anonymous], 2017, P ADV NEUR INF PROC; [Anonymous], 2006, LINEAR PROGRAMMING; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Brown N., 2017, SCIENCE; Brown N, 2017, PR MACH LEARN RES, V70; Cermak J, 2014, FRONT ARTIF INTEL AP, V263, P201, DOI 10.3233/978-1-61499-419-0-201; Farina Gabriele, 2017, AAAI C ART INT AAAI; Farina Gabriele, 2017, INT C MACH LEARN ICM; Ganzfried Sam, INT C AUT AG MULT SY; GLPK, 2017, GNU LIN PROGR KIT VE; Hillas John, 2002, HDB GAME THEORY EC A; Koller Daphne, 1996, GAMES EC BEHAV, V14; KREPS DM, 1982, ECONOMETRICA, V50, P863, DOI 10.2307/1912767; Kroer Christian, 2017, P INT JOINT C ART IN; Kuhn H. W., 1950, CONTRIBUTIONS THEORY, V1, P97; Miltersen PB, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P107, DOI 10.1145/1109557.1109570; Miltersen Peter Bro, 2010, EC THEORY, V42; Myerson R. B., 1978, International Journal of Game Theory, V7, P73, DOI 10.1007/BF01753236; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Romanovskii I., 1962, SOVIET MATH, V3; ROSS SM, 1971, J APPL PROBAB, V8, P621, DOI 10.2307/3212187; Selten Reinhard, 1975, INT J GAME THEORY; Shoham Y., 2008, MULTIAGENT SYSTEMS A, DOI DOI 10.1017/CBO9780511811654; Southey F., 2005, P 21 ANN C UNC ART I; van Damme E., 1987, STABILITY PERFECTION; van Damme Eric, 1984, INT J GAME THEORY; von Stengel Bernhard, 1996, GAMES EC BEHAV, V14	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305008
C	Farina, G; Celli, A; Gatti, N; Sandholm, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Farina, Gabriele; Celli, Andrea; Gatti, Nicola; Sandholm, Tuomas			Ex ante coordination and collusion in zero-sum multi-player extensive-form games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EFFICIENT COMPUTATION; IMPERFECT RECALL; EQUILIBRIA	Recent milestones in equilibrium computation, such as the success of Libratus, show that it is possible to compute strong solutions to two-player zero-sum games in theory and practice. This is not the case for games with more than two players, which remain one of the main open challenges in computational game theory. This paper focuses on zero-sum games where a team of players faces an opponent, as is the case, for example, in Bridge, collusion in poker, and many non-recreational applications such as war, where the colluders do not have time or means of communicating during battle, collusion in bidding, where communication during the auction is illegal, and coordinated swindling in public. The possibility for the team members to communicate before game play-that is, coordinate their strategies ex ante-makes the use of behavioral strategies unsatisfactory. The reasons for this are closely related to the fact that the team can be represented as a single player with imperfect recall. We propose a new game representation, the realization form, that generalizes the sequence form but can also be applied to imperfect-recall games. Then, we use it to derive an auxiliary game that is equivalent to the original one. It provides a sound way to map the problem of finding an optimal ex-antecoordinated strategy for the team to the well-understood Nash equilibrium-finding problem in a (larger) two-player zero-sum perfect-recall game. By reasoning over the auxiliary game, we devise an anytime algorithm, fictitious team-play, that is guaranteed to converge to an optimal coordinated strategy for the team against an optimal opponent, and that is dramatically faster than the prior state-of-the-art algorithm for this problem.	[Farina, Gabriele; Sandholm, Tuomas] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Celli, Andrea; Gatti, Nicola] Politecn Milan, DEIB, Milan, Italy	Carnegie Mellon University; Polytechnic University of Milan	Farina, G (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.	gfarina@cs.cmu.edu; andrea.celli@polimi.it; nicola.gatti@polimi.it; sandholm@cs.cmu.edu	Gatti, Nicola/AFS-7062-2022	Gatti, Nicola/0000-0001-7349-3932; CELLI, ANDREA/0000-0002-2046-4019	National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]	National Science Foundation(National Science Foundation (NSF)); ARO	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082.	[Anonymous], 2017, P ADV NEUR INF PROC; Basilico N, 2017, AAAI CONF ARTIF INTE, P356; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Brown N., 2017, SCIENCE; Brown N., 2015, INT C AUT AG MULT SY; Celli Andrea, 2018, AAAI C ART INT AAAI; Cermak J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P936; Cermak J, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P902; Cermak J, 2018, INT J APPROX REASON, V93, P290, DOI 10.1016/j.ijar.2017.11.010; Ganzfried Sam, 2014, AAAI C ART INT AAAI; Heinrich J, 2015, PR MACH LEARN RES, V37, P805; Koller D, 1996, GAME ECON BEHAV, V14, P247, DOI 10.1006/game.1996.0051; Kroer C., 2016, P ACM C EC COMP EC; Lanctot  Marc, 2012, INT C MACH LEARN ICM; Maschler M., 2013, GAME THEORY, DOI 10.1017/9781108636049; McMahan H Brendan, 2003, P 20 INT C MACH LEAR, P536; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Piccione M, 1997, GAME ECON BEHAV, V20, P3, DOI 10.1006/game.1997.0536; Shoham Y., 2008, MULTIAGENT SYSTEMS A, DOI DOI 10.1017/CBO9780511811654; Southey F., 2005, P 21 ANN C UNC ART I; Tawarmalani M, 2005, MATH PROGRAM, V103, P225, DOI 10.1007/s10107-005-0581-8; von Stengel B, 2008, MATH OPER RES, V33, P1002, DOI 10.1287/moor.1080.0340; vonStengel B, 1997, GAME ECON BEHAV, V21, P309, DOI 10.1006/game.1997.0527; vonStengel B, 1996, GAME ECON BEHAV, V14, P220, DOI 10.1006/game.1996.0050; Waugh Kevin, 2009, S ABSTR REF APPR SAR; Wichardt PC, 2008, GAME ECON BEHAV, V63, P366, DOI 10.1016/j.geb.2007.08.007	27	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004022
C	Fathony, R; Rezaei, A; Bashiri, MA; Zhang, XH; Ziebart, BD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fathony, Rizal; Rezaei, Ashkan; Bashiri, Mohammad Ali; Zhang, Xinhua; Ziebart, Brian D.			Distributionally Robust Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION	In many structured prediction problems, complex relationships between variables are compactly defined using graphical structures. The most prevalent graphical prediction methods-probabilistic graphical models and large margin methods-have their own distinct strengths but also possess significant drawbacks. Conditional random fields (CRFs) are Fisher consistent, but they do not permit integration of customized loss metrics into their learning process. Large-margin models, such as structured support vector machines (SSVMs), have the flexibility to incorporate customized loss metrics, but lack Fisher consistency guarantees. We present adversarial graphical models (AGM), a distributionally robust approach for constructing a predictor that performs robustly for a class of data distributions defined using a graphical structure. Our approach enjoys both the flexibility of incorporating customized loss metrics into its design as well as the statistical guarantee of Fisher consistency. We present exact learning and prediction algorithms for AGM with time complexity similar to existing graphical models and show the practical benefits of our approach with experiments.	[Fathony, Rizal; Rezaei, Ashkan; Bashiri, Mohammad Ali; Zhang, Xinhua; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Fathony, R (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	rfatho2@uic.edu; arezae4@uic.edu; mbashi4@uic.edu; zhangx@uic.edu; bziebart@uic.edu	Fathony, Rizal/AAB-5588-2022		National Science Foundation [1652530]; Future of Life Institute FLI-RFP-AI1 program	National Science Foundation(National Science Foundation (NSF)); Future of Life Institute FLI-RFP-AI1 program	This work was supported, in part, by the National Science Foundation under Grant No. 1652530, and by the Future of Life Institute (futureoflife.org) FLI-RFP-AI1 program.	[Anonymous], 1998, STAT LEARNING THEORY; Asif Kaiser, 2015, P C UNC ART INT; Behpour S, 2018, AAAI C ART INT; Boyd S., 2008, NOTES EE364B; Carreras Xavier, 2005, P 9 C COMP NAT LANG, P152; Chen RD, 2018, J MACH LEARN RES, V19; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Cohn T., 2005, P 9 C COMP NAT LANG, P169; COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D; Cowell R.G., 2006, PROBABILISTIC NETWOR; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741; Duchi J., 2008, PROC 25 INT C MACH L, P272; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Fathony R., 2017, ADV NEURAL INFORM PR, P563; Fathony Rizal, 2016, ADV NEURAL INFORM PR, P559; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; Hashimoto Tatsunori, 2018, P 35 INT C MACH LEAR, P2; Hatori Jun, 2008, COL 2008 COMP VOL CO, P43; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kim M, 2010, LECT NOTES COMPUT SC, V6313, P649; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Li J., 2016, IJCAI, P1690; Li S, 2009, MARKOV RANDOM FIELD; Liu, 2007, INT C ART INT STAT, V2, P291; Livni R., 2012, JMLR W CP, P722; Manning CD, 1999, FDN STAT NATURAL LAN; McMahan H Brendan, 2003, P 20 INT C MACH LEAR, P536; Namkoong H, 2016, ADV NEURAL INFORM PR, P2208; Namkoong Hongseok, 2017, NEURIPS, P2971; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Osokin A., 2017, ADV NEURAL INFORM PR, P302; Pearl Judea, 1985, P 7 C COGN SCI SOC 1; Sadeghian A, 2016, P WORKSH LEG TEXT DO, P70; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219; Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013; Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464; Tewari A, 2007, J MACH LEARN RES, V8, P1007; TOPSOE F, 1979, KYBERNETIKA, V15, P8; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Von Neumann J, 1945, B AM MATH SOC, V51, P498, DOI [DOI 10.1090/S0002-9904-1945-08391-8, 10.1090/S0002-9904-1945-08391-8]; Wang Hong, 2015, ADV NEURAL INFORM PR; Xue Nianwen, 2004, P 2004 C EMP METH NA; Zhang T, 2004, J MACH LEARN RES, V5, P1225	48	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002085
C	Feizi, S; Javadi, H; Zhang, J; Tse, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Feizi, Soheil; Javadi, Hamid; Zhang, Jesse; Tse, David			Porcupine Neural Networks: Approximating Neural Network Landscapes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes analyzing their performance challenging. In this paper, we take another approach to this problem by constraining the network such that the corresponding optimization landscape has good theoretical properties without significantly compromising performance. In particular, for two-layer neural networks we introduce Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN.	[Feizi, Soheil] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Javadi, Hamid] Rice Univ, Dept Elect & Comp Engn, Houston, TX 77251 USA; [Zhang, Jesse; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	University System of Maryland; University of Maryland College Park; Rice University; Stanford University	Feizi, S (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	sfeizi@cs.umd.edu; hrhakim@rice.edu; jessez@stanford.edu; dntse@stanford.edu			Center for Science of Information (CSoI), an NSF Science and Technology Center [CCF-0939370]	Center for Science of Information (CSoI), an NSF Science and Technology Center	This work supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.	Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Cheng XY, 2013, RANDOM MATRICES-THEO, V2, DOI 10.1142/S201032631350010X; Cho Y., 2009, NIPS, P342; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Daniely A., 2017, ARXIV170307872; Do Y, 2012, ARXIV12063763; El Karoui N, 2010, ANN STAT, V38, P1, DOI 10.1214/08-AOS648; Fan Zhou, 2015, ARXIV150705343; Ge R., 2017, ARXIV PREPRINT ARXIV; Giryes R, 2017, ARXIV171204741; Hazan E., 2015, ADV NEURAL INFORM PR, P1594; Heinemann U, 2016, JMLR WORKSH CONF PRO, V51, P1159; Janzamin M., 2015, ARXIV150608473; Kakade S., 2011, ADV NEURAL INFORM PR; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li H., 2017, ARXIV171209913; LI Y., 2017, ARXIV170509886; Mei S., 2016, ARXIV160706534; Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382; Nguyen Q., 2017, ARXIV170408045; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rivest R.L., 1989, NIPS, P494; Soltanolkotabi M., 2017, ARXIV170704926; Soltanolkotabi Mahdi, 2017, ARXIV170504591; Soudry D., 2016, ARXIV PREPRINT ARXIV; Tian Y., 2016, SYMMETRY BREAKING CO; Tian Yuandong, 2017, ARXIV170300560; Yun C., 2017, ARXIV170702444; Zhang Qiuyi, 2017, ARXIV170200458; Zhong  Kai, 2017, ARXIV170603175	34	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304081
C	Fichtenberger, H; Rohde, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fichtenberger, Hendrik; Rohde, Dennis			A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHM; WEIGHT	In the k-nearest neighborhood model (k-NN), we are given a set of points P, and we shall answer queries q by returning the k nearest neighbors of q in P according to some metric. This concept is crucial in many areas of data analysis and data processing, e.g., computer vision, document retrieval and machine learning. Many k-NN algorithms have been published and implemented, but often the relation between parameters and accuracy of the computed k-NN is not explicit. We study property testing of k-NN graphs in theory and evaluate it empirically: given a point set P subset of R-delta and a directed graph G = (P, E), is G a k-NN graph, i.e., every point p is an element of P has outgoing edges to its k nearest neighbors, or is it epsilon-far from being a k-NN graph? Here, is an element of-far means that one has to change more than an is an element of-fraction of the edges in order to make G a k-NN graph. We develop a randomized algorithm with one-sided error that decides this question, i.e., a property tester for the k-NN property, with complexity O(root nk(2)/epsilon(2)) measured in terms of the number of vertices and edges it inspects, and we prove a lower bound of Omega(root n/epsilon k). We evaluate our tester empirically on the k-NN models computed by various algorithms and show that it can be used to detect k-NN models with bad accuracy in significantly less time than the building time of the k-NN model.	[Fichtenberger, Hendrik; Rohde, Dennis] TU Dortmund, Dortmund, Germany	Dortmund University of Technology	Fichtenberger, H (corresponding author), TU Dortmund, Dortmund, Germany.	hendrik.fichtenberger@tu-dortmund.de; dennis.rohde@cs.tu-dortmund.de		Rohde, Dennis/0000-0001-8984-1962	European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) / ERC grant [307696]	European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) / ERC grant(European Research Council (ERC))	The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) / ERC grant agreement no 307696. We thank the anonymous reviewers for their comments and questions, which we addressed by adding Lemma 12 and Lemma 15 most notably.	Ann Arbor Algorithms, 2018, KGRAPH LIB K NEAR NE; Aumuller M, 2017, LECT NOTES COMPUT SC, V10609, P34, DOI 10.1007/978-3-319-68474-1_3; Ben-Zwi O, 2007, INFORM PROCESS LETT, V102, P219, DOI 10.1016/j.ipl.2006.12.011; Bernhardsson Erik, 2018, HFICHTENBERGER ANN B, DOI [10.5281/zenodo.1463824, DOI 10.5281/ZENODO.1463824]; Bernhardsson Erik, 2018, ANN BENCHMARKS ALGOS; Bernhardsson Erik, 2018, ANN BENCHMARKS BENCH; Boytsov L, 2013, LECT NOTES COMPUT SC, V8199, P280, DOI 10.1007/978-3-642-41062-8_28; CALLAHAN PB, 1995, J ASSOC COMPUT MACH, V42, P67, DOI 10.1145/200836.200853; Chen J, 2009, J MACH LEARN RES, V10, P1989; Connor M, 2010, IEEE T VIS COMPUT GR, V16, P599, DOI 10.1109/TVCG.2010.9; Czumaj A, 2005, SIAM J COMPUT, V35, P91, DOI 10.1137/S0097539703435297; CZUMAJ A, 2000, P 8 ESA, P155; Czumaj A, 2009, SIAM J COMPUT, V39, P904, DOI 10.1137/060672121; Czumaj A, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1367064.1367071; Dasarathy B.V, 1991, NEAREST NEIGHBOUR NN; Fichtenberger Hendrik, THEORY BASED EVALUAT; Fichtenberger Hendrik, 2018, HFICHTENBERGER KNN T, DOI [10.5281/zenodo.1463804, DOI 10.5281/ZENODO.1463804]; FRIEDMAN JH, 1975, IEEE T COMPUT, V24, P1000, DOI 10.1109/T-C.1975.224110; Fu Xiping, 2018, UCI MACHINE LEARNING; FUKUNAGA K, 1975, IEEE T COMPUT, VC 24, P750, DOI 10.1109/T-C.1975.224297; Goldreich O, 1998, J ACM, V45, P653, DOI 10.1145/285055.285060; Hellweg F, 2010, LECT NOTES COMPUT SC, V6390, P306, DOI 10.1007/978-3-642-16367-8_24; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Kabatyanskii G. A., 1978, Problems of Information Transmission, V14, P1; LeCun Y., 2018, MNIST HANDWRITTEN DI; Maillo J, 2017, KNOWL-BASED SYST, V117, P3, DOI 10.1016/j.knosys.2016.06.012; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Naidan Bilegsaikhan, 2018, NONMETRIC SPACE LIB; Parnas Michal, 2001, 33 ANN ACM S THEOR C, P276, DOI [10.1145/380752.380811, DOI 10.1145/380752.380811]; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rubinfeld R, 1996, SIAM J COMPUT, V25, P252, DOI 10.1137/S0097539793255151; Shakhnarovich G., 2006, NEURAL INFORM PROCES; WYNER AD, 1965, BELL SYST TECH J, V44, P1061, DOI 10.1002/j.1538-7305.1965.tb04170.x; Zalando Research, 2018, FASH MNIST MNIST LIK; ZEGER K, 1994, IEEE T INFORM THEORY, V40, P1647, DOI 10.1109/18.333884; Zhang SC, 2018, IEEE T NEUR NET LEAR, V29, P1774, DOI 10.1109/TNNLS.2017.2673241	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001030
C	Finocchiaro, J; Frongillo, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Finocchiaro, Jessica; Frongillo, Rafael			Convex Elicitation of Continuous Real Properties	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CLASSIFICATION	A property or statistic of a distribution is said to be elicitable if it can be expressed as the minimizer of some loss function in expectation. Recent work shows that continuous real-valued properties are elicitable if and only if they are identifiable, meaning the set of distributions with the same property value can be described by linear constraints. From a practical standpoint, one may ask for which such properties do there exist convex loss functions. In this paper, in a finite-outcome setting, we show that in fact essentially every elicitable real-valued property can be elicited by a convex loss function. Our proof is constructive, and leads to convex loss functions for new properties.	[Finocchiaro, Jessica; Frongillo, Rafael] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Finocchiaro, J (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.	jessica.finocchiaro@colorado.edu; raf@colorado.edu			National Science Foundation [CCF-1657598]	National Science Foundation(National Science Foundation (NSF))	We would like to thank Bo Waggoner and Arpit Agarwal for their insights and the discussion which led to this project, and we thank Krisztina Dearborn for consultation on analysis results. Additionally, we would like to thank our reviewers for their feedback and suggestions. This project was funded by National Science Foundation Grant CCF-1657598.	ABBOTT S, 2001, UNDERSTANDING ANAL; Abernethy J.D., 2011, ADV NEURAL INFORM PR, P2600; Agarwal A., 2015, JMLR WORKSHOP C P, V40, P1; Apostol T. M., 1974, MATH ANAL; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Buja A., 2005, LOSS FUNCTIONS BINAR; Fissler T, 2016, ANN STAT, V44, P1680, DOI 10.1214/16-AOS1439; Frongillo R., 2015, ADV NEURAL INFORM PR, V29; FRONGILLO R., 2015, JMLR WORKSHOP C P, V40, P1; Frongillo R., 2017, PREPRINT; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Lambert N.S., 2018, ELICITATION EVALUATI; Lambert N, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P109; Lambert N, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P129; Lambert NS, 2015, J ECON THEORY, V156, P389, DOI 10.1016/j.jet.2014.03.012; Pouso R., 2012, ARXIV12031462; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Rockafellar R.T., 1997, PRINCETON MATH SERIE, V28; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; STEINWART I, 2014, J MACHINE LEARNING R, V35, P482; Steinwart I., 2008, SUPPORT VECTOR MACHI; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Williamson RC, 2016, J MACH LEARN RES, V17, P1; Witkowski J., 2018, P 32 AAAI C ART INT	25	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005002
C	Friedman, T; Van den Broeck, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Friedman, Tal; Van den Broeck, Guy			Approximate Knowledge Compilation by Online Collapsed Importance Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE	We introduce collapsed compilation, a novel approximate inference algorithm for discrete probabilistic graphical models. It is a collapsed sampling algorithm that incrementally selects which variable to sample next based on the partial compilation obtained so far. This online collapsing, together with knowledge compilation inference on the remaining variables, naturally exploits local structure and context-specific independence in the distribution. These properties are used implicitly in exact inference, but are difficult to harness for approximate inference. Moreover, by having a partially compiled circuit available during sampling, collapsed compilation has access to a highly effective proposal distribution for importance sampling. Our experimental evaluation shows that collapsed compilation performs well on standard benchmarks. In particular, when the amount of exact inference is equally limited, collapsed compilation is competitive with the state of the art, and outperforms it on several benchmarks.	[Friedman, Tal; Van den Broeck, Guy] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Friedman, T (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	tal@cs.ucla.edu; guyvdb@cs.ucla.edu			NSF [IIS-1657613, IIS-1633857, CCF-1837129]; DARPA XAI [N66001-17-2-4032]	NSF(National Science Foundation (NSF)); DARPA XAI	We thank Jonas Vlasselaer and Wannes Meert for initial discussions. Additionally, we thank Arthur Choi, Yujia Shen, Steven Holtzen, and YooJung Choi for helpful feedback. This work is partially supported by a gift from Intel, NSF grants #IIS-1657613, #IIS-1633857, #CCF-1837129, and DARPA XAI grant #N66001-17-2-4032.	Bekker J., 2015, ADV NEURAL INFORM PR, P2242; Bidyuk B, 2007, J ARTIF INTELL RES, V28, P1, DOI 10.1613/jair.2149; Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115; Chakraborty S, 2014, AAAI CONF ARTIF INTE, P1722; Chan Hei, 2006, P 22 C UNC ART INT, P63; Chavira M, 2006, INT J APPROX REASON, V42, P4, DOI 10.1016/j.ijar.2005.10.001; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Choi A., 2006, P NAT C ART INT, P1107; Choi A., 2017, ICML; Choi Arthur, 2005, P 21 C UNC ART INT U, P128; Choi Arthur, 2013, ECSQARU; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Darwiche A, 2001, J ACM, V48, P608, DOI 10.1145/502090.502091; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Darwiche Adnan, 1999, P IJCAI; Darwiche Adnan, 2011, P IJCAI; De Raedt L, 2015, MACH LEARN, V100, P5, DOI 10.1007/s10994-015-5494-z; Dechter Rina, 2002, P 18 C UNC ART INT, P128; Ermon Stefano, 2013, ADV NEURAL INFORM PR, P2085; Fierens D, 2015, THEOR PRACT LOG PROG, V15, P358, DOI 10.1017/S1471068414000076; Gogate V., 2007, P 11 C ART INT STAT, P147; Gogate V, 2011, ARTIF INTELL, V175, P694, DOI 10.1016/j.artint.2010.10.009; Koller D., 2009, PROBABILISTIC GRAPHI; Lowd Daniel, 2010, NIPS, P1477; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Tokdar ST, 2010, WILEY INTERDISCIP RE, V2, P54, DOI 10.1002/wics.56; Van den Broeck, 2015, P 29 C ART INT AAAI; Van den Broeck G., 2013, THESIS; Van den Broeck G, 2015, FOUND TRENDS DATABAS, V7, pI, DOI 10.1561/1900000052; Vlasselaer J, 2016, ARTIF INTELL, V232, P43, DOI 10.1016/j.artint.2015.12.001	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002056
C	Friesen, AL; Domingos, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Friesen, Abram L.; Domingos, Pedro			Submodular Field Grammars: Representation, Inference, and Application to Image Parsing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ENERGY MINIMIZATION	Natural scenes contain many layers of part-subpart structure, and distributions over them are thus naturally represented by stochastic image grammars, with one production per decomposition of a part. Unfortunately, in contrast to language grammars, where the number of possible split points for a production A -> BC is linear in the length of A, in an image there are an exponential number of ways to split a region into subregions. This makes parsing intractable and requires image grammars to be severely restricted in practice, for example by allowing only rectangular regions. In this paper, we address this problem by associating with each production a submodular Markov random field whose labels are the subparts and whose labeling segments the current object into these subparts. We call the resulting model a submodular field grammar (SFG). Finding the MAP split of a region into subregions is now tractable, and by exploiting this we develop an efficient approximate algorithm for MAP parsing of images with SFGs. Empirically, we show promising improvements in accuracy when using SFGs for scene understanding, and demonstrate exponential improvements in inference time compared to traditional methods, while returning comparable minima.	[Friesen, Abram L.; Domingos, Pedro] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Friesen, AL (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.	afriesen@cs.washington.edu; pedrod@cs.washington.edu			ONR [N00014-16-1-2697]; AFRL [FA8750-13-2-0019]	ONR(Office of Naval Research); AFRL(United States Department of DefenseUS Air Force Research Laboratory)	AF would like to thank Robert Gens, Rahul Kidambi, and Gena Barnabee for useful discussions, insights, and assistance with this document. The DGX-1 used for this research was donated by NVIDIA. This research was partly funded by ONR grant N00014-16-1-2697 and AFRL contract FA8750-13-2-0019. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ONR, AFRL, or the United States Government.	Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chandrasekaran V., 2008, P 24 C UNC ART INT, P70; Chen L.-C., 2015, COMPUT SCI; Chen Liang- Chieh, 2016, ARXIV160600915CSCV; Delong A, 2012, INT J COMPUT VISION, V100, P38, DOI 10.1007/s11263-012-0531-x; Gens R., 2013, 30 INT C MACHINE LEA, P873; Gould S., 2009, P IEEE INT C COMP VI; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; Hopcroft John E., 1979, INTRO AUTOMATA THEOR; IVANESCU PL, 1965, OPER RES, V13, P388, DOI 10.1287/opre.13.3.388; Jurafsky Daniel, 2000, SPEECH LANGUAGE PROC; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Kumar M. P., 2009, P 25 C UNC ART INT, P313; Lempitsky V. S., 2011, NEURAL INFORM PROCES; Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143; Poon Hoifung, 2011, P 20 C UNC ART INT U, P337; Russell Chris, 2010, 26 C UNC ART INT; Sharma A., 2014, ADV NEURAL INFORM PR; Shotton J., 2006, P EUR C COMP VIS ECC, V3951; Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1; Socher R., 2011, P 28 INT C INT C MAC, P129; Zhao Y., 2011, NIPS; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304033
C	Frongillo, R; Waggoner, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Frongillo, Rafael; Waggoner, Bo			Bounded-Loss Private Prediction Markets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Prior work has investigated variations of prediction markets that preserve participants' (differential) privacy, which formed the basis of useful mechanisms for purchasing data for machine learning objectives. Such markets required potentially unlimited financial subsidy, however, making them impractical. In this work, we design an adaptively-growing prediction market with a bounded financial subsidy, while achieving privacy, incentives to produce accurate predictions, and precision in the sense that market prices are not heavily impacted by the added privacy-preserving noise. We briefly discuss how our mechanism can extend to the data-purchasing setting, and its relationship to traditional learning algorithms.	[Waggoner, Bo] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Waggoner, B (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	raf@colorado.edu; bwag@colorado.edu		Waggoner, Bo/0000-0002-1366-1065; Frongillo, Rafael/0000-0002-0170-7572				Abernethy J., 2014, P 15 ACM C EC COMP, P395; Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12; Abernethy J. D., 2012, C LEARN THEOR INT MA, P1; Abernethy J.D., 2011, ADV NEURAL INFORM PR, P2600; Abernethy Jacob D., 2014, P 15 ACM C EC COMP, P413, DOI [10.1145/2600057.2602900, DOI 10.1145/2600057.2602900]; Banerjee A, 2005, IEEE T INFORM THEORY, V51, P2664, DOI 10.1109/TIT.2005.850145; Cummings R, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P143, DOI 10.1145/2940716.2940721; FRONGILLO R., 2012, ADV NEURAL INFORM PR, V25, P3275; Frongillo Rafael, 2015, ADV NEURAL INFORM PR, P3016; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; Talagrand Michel, 2014, UPPER LOWER BOUNDS S, V60; Waggoner B., 2015, ADV NEURAL INFORM PR, V29, P3492	12	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005005
C	Gaillard, P; Wintenberger, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gaillard, Pierre; Wintenberger, Olivier			Efficient online algorithms for fast-rate regret bounds under sparsity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of online convex optimization in two different settings: arbitrary and i.i.d. sequence of convex loss functions. In both settings, we provide efficient algorithms whose cumulative excess risks are controlled with fast-rate sparse bounds. First, the excess risks bounds depend on the sparsity of the objective rather than on the dimension of the parameters space. Second, their rates are faster than the slow-rate (1)/root T under additional convexity assumptions on the loss functions. In the adversarial setting, we develop an algorithm BOA+ whose cumulative excess risks is controlled by several bounds with different trade-offs between sparsity and rate for strongly convex loss functions. In the i.i.d. setting under the Lojasiewicz's assumption, we establish new risk bounds that are sparse with a rate adaptive to the convexity of the risk (ranging from a rate (1)/root T- for general convex risk to (1)/T for strongly convex risk). These results generalize previous works on sparse online learning under weak assumptions on the risk.	[Gaillard, Pierre] PSL Res Univ, ENS, INRIA, Paris, France; [Wintenberger, Olivier] Sorbonne Univ, CNRS, LPSM, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Gaillard, P (corresponding author), PSL Res Univ, ENS, INRIA, Paris, France.	pierre.gaillard@inria.fr; olivier.wintenberger@upmc.fr						Agarwal A., 2012, P ADV NEUR INF PROC, P1538, DOI DOI 10.1109/CISS.2014.6814157; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Audibert J.-Y., 2008, ADV NEURAL INFORM PR, V20, P41; Bunea F, 2007, ELECTRON J STAT, V1, P169, DOI 10.1214/07-EJS008; Catoni O., 1999, TECH REPOT, P510; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Duchi J. C., 2010, COLT, P14; Foster D. P., 2016, P 29 C LEARNING THEO, P960; Gaillard P., 2017, 20 INT C ART INT STA; Gerchinovitz S., 2011, THESIS; Gerchinovitz S, 2013, J MACH LEARN RES, V14, P729; Giraud C., 2014, INTRO HIGH DIMENSION; Jordan M.I., 2014, C LEARN THEOR, P921; Kale S., 2017, ARXIV170604690; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Koolen Wouter M, 2015, COLT, P1155; Koolen Wouter M., 2016, ADV NEURAL INFORM PR, V29, P4457; Langford J, 2009, J MACH LEARN RES, V10, P777; LOJASIEWICZ S, 1993, ANN I FOURIER, V43, P1575; Lojasiewicz S., 1963, EQUATIONS DERIVEES P, P87, DOI DOI 10.1006/JDEQ.1997.3393; Mehta NA, 2017, PR MACH LEARN RES, V54, P1085; Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854; Roulet V., 2017, ADV NEURAL INFORM PR, V30, P1119; Steinhardt J., 2014, ARXIV14124182; Steinwart I, 2011, BERNOULLI, V17, P211, DOI 10.3150/10-BEJ267; van Erven T, 2015, J MACH LEARN RES, V16, P1793; Vovk V. G., 1990, P COMP LEARN THEOR 1; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Wintenberger O, 2017, MACH LEARN, V106, P119, DOI 10.1007/s10994-016-5592-6; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Yang YH, 2004, ECONOMET THEOR, V20, P176, DOI 10.1017/S0266466604201086	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001056
C	Gallo, N; Ihler, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gallo, Nicholas; Ihler, Alexander			Lifted Weighted Mini-Bucket	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many graphical models, such as Markov Logic Networks (MLNs) with evidence, possess highly symmetric substructures but no exact symmetries. Unfortunately, there are few principled methods that exploit these symmetric substructures to perform efficient approximate inference. In this paper, we present a lifted variant of the Weighted Mini-Bucket elimination algorithm which provides a principled way to (i) exploit the highly symmetric substructure of MLN models, and (ii) incorporate high-order inference terms which are necessary for high quality approximate inference. Our method has significant control over the accuracy-time trade-off of the approximation, allowing us to generate any-time approximations. Experimental results demonstrate the utility of this class of approximations, especially in models with strong repulsive potentials.	[Gallo, Nicholas; Ihler, Alexander] Univ Calif Irvine, Irvine, CA 92637 USA	University of California System; University of California Irvine	Gallo, N (corresponding author), Univ Calif Irvine, Irvine, CA 92637 USA.	ngallo1@uci.edu; ihler@ics.uci.edu		Ihler, Alexander/0000-0002-4331-1015	NSF [IIS-1526842, IIS-1254071]; United States Air Force [FA9453-16-C-0508]; DARPA [W911NF-18-C-0015]	NSF(National Science Foundation (NSF)); United States Air Force(United States Department of Defense); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work is sponsored in part by NSF grants IIS-1526842, IIS-1254071, by the United States Air Force under Contract No. FA9453-16-C-0508, and DARPA Contract No. W911NF-18-C-0015.	Berkholz C, 2013, LECT NOTES COMPUT SC, V8125, P145, DOI 10.1007/978-3-642-40450-4_13; Bui H. B, 2012, AAAI; Bui H. H, 2014, ARXIV14064200; Bui Hung Hai, 2012, ARXIV12074814; Darwiche A., 2013, ADV NEURAL INFORM PR, V26, P2868; Dechter R, 1999, ARTIF INTELL, V113, P41, DOI 10.1016/S0004-3702(99)00059-4; Dechter R, 1997, P 13 C UNC ART INT, P132; Forouzan S, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P268; Gallo N., 2018, AAAI; Liu Q., 2011, P 28 INT C MACH LEAR, P849; Mladenov M., 2012, JMLR W CP, V22, P788; Mladenov M, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P602; Mladenov M, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P603; Mladenov M, 2014, JMLR WORKSH CONF PRO, V33, P623; Poole D., 2003, P INT JOINT C ART IN, P985; Sen P, 2009, P 25 C UNC ART INT, P496; Singla P., 2008, P 23 AAAI C ART INT, V8, P1094; Singla P, 2014, AAAI CONF ARTIF INTE, P2497; Smith D., 2016, ARXIV160609637; Taghipour N., 2012, JMLR, V22, P1194; V d Broeck G, 2014, ARXIV14120315; Venugopal Deepak, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P258, DOI 10.1007/978-3-662-44845-8_17; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004084
C	Gamarnik, D; Zadik, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gamarnik, David; Zadik, Ilias			High Dimensional Linear Regression using Lattice Basis Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFORMATION; RECOVERY	We consider a high dimensional linear regression problem where the goal is to efficiently recover an unknown vector beta* from n noisy linear observations Y = X beta* + W is an element of R-n, for known X is an element of R-nxp and unknown W is an element of R-n. Unlike most of the literature on this model we make no sparsity assumption on beta*. Instead we adopt a regularization based on assuming that the underlying vectors beta* have rational entries with the same denominator Q is an element of Z(>0). We call this Q-rationality assumption. We propose a new polynomial-time algorithm for this task which is based on the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm. We establish that under the Q-rationality assumption, our algorithm recovers exactly the vector beta* for a large class of distributions for the iid entries of X and non-zero noise W. We prove that it is successful under small noise, even when the learner has access to only one observation (n = 1). Furthermore, we prove that in the case of the Gaussian white noise for W, n = o (p/ log p) and Q sufficiently large, our algorithm tolerates a nearly optimal information-theoretic level of the noise.	[Gamarnik, David] MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Zadik, Ilias] MIT, Operat Res Ctr, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Gamarnik, D (corresponding author), MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	gamarnik@mit.edu; izadik@mit.edu						Bora A, 2017, PR MACH LEARN RES, V70; Borno M. A., 2011, REDUCTION SOLVING SO; Brunel L., 1999, P 1999 IEEE INF THEO; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Cover T.M, 2006, WILEY SERIES TELECOM; Donoho D. L., 2006, COUNTING FACES RANDO; Donoho D, 2009, PHILOS T R SOC A, V367, P4273, DOI 10.1098/rsta.2009.0152; Donoho DL, 2013, IEEE T INFORM THEORY, V59, P7434, DOI 10.1109/TIT.2013.2274513; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Donoho DL, 2005, P NATL ACAD SCI USA, V102, P9452, DOI 10.1073/pnas.0502258102; Erdos P., 1985, ACTA ARITH, V5, P524; Foucart S., 2013, MATH INTRO COMPRESSI, P1, DOI DOI 10.1007/978-0-8176-4948-7; FRIEZE AM, 1986, SIAM J COMPUT, V15, P536, DOI 10.1137/0215038; GAMARNIK D., 2017, SPARSE HIGH DIMENSIO; Gamarnik D., 2017, C LEARN THEOR; Hardy G. H., 1975, INTRO THEORY NUMBERS; Hassibi A., 1998, IEEE T SIGNAL PROCES; Hassibi B., 2002, 2002 IEEE INT C AC S; LAGARIAS JC, 1985, J ACM, V32, P229, DOI 10.1145/2455.2461; Lempel A., 1979, Computing Surveys, V11, P285, DOI 10.1145/356789.356792; LENSTRA AK, 1982, MATH ANN, V261, P515, DOI 10.1007/BF01457454; MERKLE RC, 1978, IEEE T INFORM THEORY, V24, P525, DOI 10.1109/TIT.1978.1055927; Shamir A., 1982, 23rd Annual Symposium on Foundations of Computer Science, P145, DOI 10.1109/SFCS.1982.5; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301080
C	Gao, R; Xie, LY; Xie, Y; Xu, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gao, Rui; Xie, Liyan; Xie, Yao; Xu, Huan			Robust Hypothesis Testing Using Wasserstein Uncertainty Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We develop a novel computationally efficient and general framework for robust hypothesis testing. The new framework features a new way to construct uncertainty sets under the null and the alternative distributions, which are sets centered around the empirical distribution defined via Wasserstein metric, thus our approach is data-driven and free of distributional assumptions. We develop a convex safe approximation of the minimax formulation and show that such approximation renders a nearly-optimal detector among the family of all possible tests. By exploiting the structure of the least favorable distribution, we also develop a tractable reformulation of such approximation, with complexity independent of the dimension of observation space and can be nearly sample-size-independent in general. Real-data example using human activity data demonstrated the excellent performance of the new robust detector.	[Gao, Rui; Xie, Liyan; Xie, Yao; Xu, Huan] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Gao, R (corresponding author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	rgao32@gatech.edu; lxie49@gatech.edu; yao.xie@isye.gatech.edu; huan.xu@isye.gatech.edu						[Anonymous], 2001, LECT MODERN CONVEX O; [Anonymous], 2008, PRINCIPLES SIGNAL DE; BLANCHET J, 2016, ARXIV160401446; Bottou L., 2017, ARXIV170107875STATML; Cao Yang, 2017, INT S INF THEOR ISIT; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Gao R., 2016, MATH OPER RES; Goldenshluger A, 2015, ELECTRON J STAT, V9, P1645, DOI 10.1214/15-EJS1054; Gul G, 2017, IEEE T INFORM THEORY, V63, P5572, DOI 10.1109/TIT.2017.2693198; Gulrajani I., 2017, INT C NEURAL INF PRO; Hellinger E, 1909, J REINE ANGEW MATH, V136, P210, DOI 10.1515/crll.1909.136.210; Huber P., 1981, ROBUST STAT; HUBER PJ, 1965, ANN MATH STAT, V36, P1753, DOI 10.1214/aoms/1177699803; Juditsky A, 2016, ELECTRON J STAT, V10, P2204, DOI 10.1214/16-EJS1170; Kwapisz Jennifer R., 2011, SIGKDD EXPLOR NEWSL, V12, P74, DOI [10.1145/1964897.1964918, DOI 10.1145/1964897.1964918]; Levy BC, 2009, IEEE T INFORM THEORY, V55, P413, DOI 10.1109/TIT.2008.2008128; Lockhart J. W., 2011, P 5 INT WORKSH KNOWL, P25, DOI [DOI 10.1145/2003653.2003656, 10.1145/2003653]; Manning CD, 1999, FDN STAT NATURAL LAN; Montgomery D.C., 2009, INTRO STAT QUALITY C; Nemirovski A, 2006, SIAM J OPTIMIZ, V17, P969, DOI 10.1137/050622328; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Ramdas A, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19020047; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sinha Aman, 2017, ARXIV PREPRINT ARXIV; Topsoe F, 2000, IEEE T INFORM THEORY, V46, P1602, DOI 10.1109/18.850703; Weiss GM, 2012, AAAI WORKSH ACT CONT, P98	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002045
C	Garg, VK; Dekel, O; Xiao, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Garg, Vikas K.; Dekel, Ofer; Xiao, Lin			Learning SMaLL Predictors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DNF; ALGORITHM; CLASSIFICATION; SELECTION; REGRESSION	We introduce a new framework for learning in severely resource-constrained settings. Our technique delicately amalgamates the representational richness of multiple linear predictors with the sparsity of Boolean relaxations, and thereby yields classifiers that are compact, interpretable, and accurate. We provide a rigorous formalism of the learning problem, and establish fast convergence of the ensuing algorithm via relaxation to a minimax saddle point objective. We supplement the theoretical foundations of our work with an extensive empirical evaluation.	[Garg, Vikas K.] MIT, CSAIL, Cambridge, MA 02139 USA; [Dekel, Ofer; Xiao, Lin] Microsoft Res, Redmond, WA USA	Massachusetts Institute of Technology (MIT); Microsoft	Garg, VK (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	vgarg@csail.mit.edu; oferd@microsoft.com; lin.xiao@microsoft.com						Aiolli F, 2005, J MACH LEARN RES, V6, P817; Bertsimas D, 2007, OPER RES, V55, P252, DOI 10.1287/opre.1060.0360; Boyd S, 2004, CONVEX OPTIMIZATION; BRUCKER P, 1984, OPER RES LETT, V3, P163, DOI 10.1016/0167-6377(84)90010-5; Bshouty N. H., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P286, DOI 10.1145/307400.307472; Bshouty NH, 2005, J COMPUT SYST SCI, V71, P250, DOI 10.1016/j.jcss.2004.10.010; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Cord O, 2001, GENETIC FUZZY SYSTEM, V19; Dekel O., 2007, ADV NEURAL INFORM PR, P345; Dekel O, 2008, SIAM J COMPUT, V37, P1342, DOI 10.1137/060666998; Duchi J., 2008, PROC 25 INT C MACH L, P272; Feldman V., 2012, C LEARN THEOR COLT; Garg V. K., 2018, UAI; Gupta C, 2017, PR MACH LEARN RES, V70; Han S., 2016, ICLR; Hauser JR, 2010, J MARKETING RES, V47, P485, DOI 10.1509/jmkr.47.3.485; Hubara I, 2016, ADV NEUR IN, V29; Jackson JC, 1997, J COMPUT SYST SCI, V55, P414, DOI 10.1006/jcss.1997.1533; Jalali A, 2017, SIAM J OPTIMIZ, V27, P2634, DOI 10.1137/16M1087424; Juditsky A, 2012, OPTIMIZATION FOR MACHINE LEARNING, P149; Khot S, 2008, ANN IEEE SYMP FOUND, P231, DOI 10.1109/FOCS.2008.37; Klivans AR, 2004, J COMPUT SYST SCI, V68, P303, DOI 10.1016/j.jcss.2003.07.007; Kumar A, 2017, PR MACH LEARN RES, V70; Kusner MJ, 2014, PR MACH LEARN RES, V32, P622; MANSOUR Y, 1995, J COMPUT SYST SCI, V50, P543, DOI 10.1006/jcss.1995.1043; Nakkiran P, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1473; Nan F, 2016, ADV NEUR IN, V29; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748; Pilanci M, 2015, MATH PROGRAM, V151, P63, DOI 10.1007/s10107-015-0894-1; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Sakai Y, 2000, THEOR COMPUT SYST, V33, P17, DOI 10.1007/s002249910002; Servedio RA, 2004, INFORM COMPUT, V193, P57, DOI 10.1016/j.ic.2004.04.003; Tan MK, 2014, J MACH LEARN RES, V15, P1371; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Verbeurgt K., 1998, P 9 C ALG LEARN THEO, P385; Wang T, 2017, J MACH LEARN RES, V18, P1; Zhong K, 2017, PR MACH LEARN RES, V54, P1255; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003066
C	Ge, R; Lee, H; Risteski, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ge, Rong; Lee, Holden; Risteski, Andrej			Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REVERSIBLE DIFFUSION-PROCESSES; METASTABILITY; ASYMPTOTICS	A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). One prevalent example of this is sampling posteriors in parametric distributions, such as latent-variable generative models. However sampling (even very approximately) can be #P-hard. Classical results (going back to [BE85]) on sampling focus on log-concave distributions, and show a natural Markov chain called Langevin diffusion mixes in polynomial time. However, all log-concave distributions are uni-modal, while in practice it is very common for the distribution of interest to have multiple modes. In this case, Langevin diffusion suffers from torpid mixing. We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for a mixture of (strongly) log-concave distributions of the same shape. In particular, our technique applies to the canonical multi-modal distribution: a mixture of gaussians (of equal variance). Our algorithm efficiently samples from these distributions given only access to the gradient of the log-pdf. To the best of our knowledge, this is the first result that proves fast mixing for multimodal distributions in this setting. For the analysis, we introduce novel techniques for proving spectral gaps based on decomposing the action of the generator of the diffusion. Previous approaches rely on decomposing the state space as a partition of sets, while our approach can be thought of as decomposing the stationary measure as a mixture of distributions (a "soft partition"). Additional materials for the paper can be found at http : //tiny. cc/glr17. Note that the proof and results have been improved and generalized from the precursor at http: //www.arxiv. org/abs/1710 . 02736. See Section ?? for a comparison.	[Ge, Rong] Duke Univ, Dept Comp Sci, Durham, NC 27706 USA; [Lee, Holden] Princeton Univ, Dept Math, Princeton, NJ 08544 USA; [Risteski, Andrej] MIT, Appl Math, Cambridge, MA 02139 USA; [Risteski, Andrej] IDSS, Cambridge, MA USA	Duke University; Princeton University; Massachusetts Institute of Technology (MIT)	Ge, R (corresponding author), Duke Univ, Dept Comp Sci, Durham, NC 27706 USA.	rongge@cs.duke.edu; holdenl@princeton.edu; risteski@mit.edu						Bakry D., 1985, LECT NOTES MATH, P177, DOI DOI 10.1007/BFB0075847; Bakry D., 2013, ANAL GEOMETRY MARKOV, V348; Bakry D, 2008, ELECTRON COMMUN PROB, V13, P60, DOI 10.1214/ECP.v13-1352; BHATTACHARYA RN, 1978, ANN PROBAB, V6, P541, DOI 10.1214/aop/1176995476; Bovier A, 2005, J EUR MATH SOC, V7, P69; Bovier A, 2002, COMMUN MATH PHYS, V228, P219, DOI 10.1007/s002200200609; Bubeck S, 2018, DISCRETE COMPUT GEOM, V59, P757, DOI 10.1007/s00454-018-9992-1; Cheng X., 2017, ARXIV170703663; Dalalyan A. S., 2017, ARXIV170404752; Dalalyan A. S., 2017, ARXIV171000095; Dalalyan Arnak S, 2016, J ROYAL STAT SOC B; Durmus  A., 2016, HIGH DIMENSIONAL BAY; Durmus  Alain, 2018, ARXIV180209188; Dwivedi Raaz, 2018, P 2018 C LEARN THEOR; Giraud F, 2017, BERNOULLI, V23, P670, DOI 10.3150/14-BEJ680; Kingma DP, 2 INT C LEARN REPR I, P1; Liang FM, 2005, PHYSICA A, V356, P468, DOI 10.1016/j.physa.2005.04.007; LOVASZ L, 1993, RANDOM STRUCT ALGOR, V4, P359, DOI 10.1002/rsa.3240040402; Madras N, 2002, ANN APPL PROBAB, V12, P581; Mangoubi O., 2017, ARXIV170807114; Mangoubi Oren, 2017, ARXIV171102621; MARINARI E, 1992, EUROPHYS LETT, V19, P451, DOI 10.1209/0295-5075/19/6/002; Neal RM, 1996, STAT COMPUT, V6, P353, DOI 10.1007/BF00143556; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Park S, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.016703; Paulin Daniel, 2015, ARXIV150908775; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Schweizer Nikolaus, 2012, NONASYMPTOTIC ERROR; Sontag D., 2011, ADV NEURAL INFORM PR, P1008; Vempala  Santosh, 2005, COMBINATORIAL COMPUT, V52, P2; Woodard DB, 2009, ANN APPL PROBAB, V19, P617, DOI 10.1214/08-AAP555; Zheng ZR, 2003, STOCH PROC APPL, V104, P131, DOI 10.1016/S0304-4149(02)00232-6	35	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002040
C	Gopakumar, S; Gupta, S; Rana, S; Nguyen, V; Venkatesh, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gopakumar, Shivapratap; Gupta, Sunil; Rana, Santu; Vu Nguyen; Venkatesh, Svetha			Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EFFICIENT GLOBAL OPTIMIZATION; SEARCH	We introduce algorithmic assurance, the problem of testing whether machine learning algorithms are conforming to their intended design goal. We address this problem by proposing an efficient framework for algorithmic testing. To provide assurance, we need to efficiently discover scenarios where an algorithm decision deviates maximally from its intended gold standard. We mathematically formulate this task as an optimisation problem of an expensive, black-box function. We use an active learning approach based on Bayesian optimisation to solve this optimisation problem. We extend this framework to algorithms with vector-valued outputs by making appropriate modification in Bayesian optimisation via the EXP3 algorithm. We theoretically analyse our methods for convergence. Using two real-world applications, we demonstrate the efficiency of our methods. The significance of our problem formulation and initial solutions is that it will serve as the foundation in assuring humans about machines making complex decisions.	[Gopakumar, Shivapratap; Gupta, Sunil; Rana, Santu; Vu Nguyen; Venkatesh, Svetha] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia	Deakin University	Gupta, S (corresponding author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.	sunil.gupta@deakin.edu.au	Nguyen, Vu/HGV-1806-2022; Rana, Santu/R-2992-2019; Nguyen, Vu/AAQ-5062-2020	Rana, Santu/0000-0003-2247-850X; Nguyen, Vu/0000-0002-0294-4561; gupta, sunil/0000-0002-4669-9940; Gopakumar, Shivapratap/0000-0002-7986-7752; venkatesh, svetha/0000-0001-8675-6631	Australian Government through the Australian Research Council (ARC); ARC Australian Laureate Fellowship [FL170100006]	Australian Government through the Australian Research Council (ARC)(Australian Research Council); ARC Australian Laureate Fellowship(Australian Research Council)	This research was partially funded by the Australian Government through the Australian Research Council (ARC). Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).	Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bull AD, 2011, J MACH LEARN RES, V12, P2879; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hoffman M., 2011, P 27 C UNCERTAINTY A, P327; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Nogueira J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1967, DOI 10.1109/IROS.2016.7759310; Rana S, 2017, PR MACH LEARN RES, V70; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; Seldin Y., 2012, EUROPEAN WORKSHOP RE, P103; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Nguyen TD, 2017, LECT NOTES ARTIF INT, V10235, P578, DOI 10.1007/978-3-319-57529-2_45	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000001
C	Gottlieb, LA; Kaufman, E; Kontorovich, A; Nivasch, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gottlieb, Lee-Ad; Kaufman, Eran; Kontorovich, Aryeh; Nivasch, Gabriel			Learning convex polytopes with margin	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INTERSECTIONS; COMPLEXITY; APPROXIMABILITY; HALFSPACES; HARDNESS	We present an improved algorithm for properly learning convex polytopes in the realizable PAC setting from data with a margin. Our learning algorithm constructs a consistent polytope as an intersection of about t log t halfspaces with margins in time polynomial in t (where t is the number of halfspaces forming an optimal polytope). We also identify distinct generalizations of the notion of margin from hyperplanes to polytopes and investigate how they relate geometrically; this result may be of interest beyond the learning setting.	[Gottlieb, Lee-Ad; Kaufman, Eran; Nivasch, Gabriel] Ariel Univ, Ariel, Israel; [Kontorovich, Aryeh] Ben Gurion Univ Negev, Beer Sheva, Israel	Ariel University; Ben Gurion University	Gottlieb, LA (corresponding author), Ariel Univ, Ariel, Israel.	leead@ariel.ac.il; erankfmn@gmail.com; karyeh@bgu.sc.il; gabrieln@ariel.ac.il	Kontorovich, Aryeh/AAB-4744-2020; Kontorovich, Aryeh/X-9225-2019	Kontorovich, Aryeh/0000-0001-8038-8671; Gottlieb, Lee-Ad/0000-0003-3355-351X	Israel Science Foundation [755/15]	Israel Science Foundation(Israel Science Foundation)	We thank Sasho Nikolov, Bernd Gartner and David Eppstein for helpful discussions. L. Gottlieb and A. Kontorovich were supported in part by the Israel Science Foundation (grant No. 755/15).	Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; AMALDI E, 1995, THEOR COMPUT SCI, V147, P181, DOI 10.1016/0304-3975(94)00254-G; Amaldi E, 1998, THEOR COMPUT SCI, V209, P237, DOI 10.1016/S0304-3975(97)00115-1; Anderson J., 2013, C LEARNING THEORY, P1020; Angluin D., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P351, DOI 10.1145/129712.129746; [Anonymous], 2002, GRAD TEXT M; Anthony Martin, 1999, NEURAL NETWORK LEARN, DOI [10.1017/CBO9780511624216, DOI 10.1017/CBO9780511624216]; Arriaga RI, 2006, MACH LEARN, V63, P161, DOI 10.1007/s10994-006-6265-7; Barasz M., 2010, INNOVATIONS COMPUTER, P42; Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43; Ben-David S, 2003, J COMPUT SYST SCI, V66, P496, DOI 10.1016/S0022-0000(03)00038-2; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Chvatal Vasek, NOTES KHACHIYAN KALA; Cristianini N., 2000, INTRO SUPPORT VECTOR; Goel S., 2018, ARXIV170906010V4; Hanneke S, 2017, OPTIMALITY SVM NOVEL; Hegedus T., 1994, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, COLT 94, P228, DOI 10.1145/180139.181124; Hellerstein L, 2007, THEOR COMPUT SCI, V384, P66, DOI 10.1016/j.tcs.2007.05.018; HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011; Jain S, 2003, J COMPUT SYST SCI, V67, P546, DOI 10.1016/S0022-0000(03)00067-9; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Kane D., 2013, P 26 ANN C LEARN THE, P522; Kantchelian A, 2014, ADV NEUR IN, V27; KEARNS MJ, 1997, INTRO COMPUTATIONAL; Khot S, 2011, J COMPUT SYST SCI, V77, P129, DOI 10.1016/j.jcss.2010.06.010; Klivans AR, 2008, J COMPUT SYST SCI, V74, P35, DOI 10.1016/j.jcss.2007.04.012; Klivans AR, 2009, J COMPUT SYST SCI, V75, P2, DOI 10.1016/j.jcss.2008.07.008; Kwek S, 1998, ALGORITHMICA, V22, P53, DOI 10.1007/PL00013834; LONG PM, 1994, INFORM COMPUT, V113, P230, DOI 10.1006/inco.1994.1071; MEGIDDO N, 1988, DISCRETE COMPUT GEOM, V3, P325, DOI 10.1007/BF02187916; Nikolov Aleksandar, 2018, THEORETICAL COMPUTER; Rademacher Luis, 2009, COLT 2009; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vershynin Roman, 2010, CORR; Zuckerman D., 2007, THEOR COMPUT, V3, P103, DOI [10.4086/toc.2007.v003a006, DOI 10.4086/TOC.2007.V003A006, DOI 10.4086/T0C.2007.V003A006]	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000023
C	Greenberg, CS; Monath, N; Kobren, A; Flaherty, P; McGregor, A; McCallum, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Greenberg, Craig S.; Monath, Nicholas; Kobren, Ari; Flaherty, Patrick; McGregor, Andrew; McCallum, Andrew			Compact Representation of Uncertainty in Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					For many classic structured prediction problems, probability distributions over the dependent variables can be efficiently computed using widely-known algorithms and data structures (such as forward-backward, and its corresponding trellis for exact probability distributions in Markov models). However, we know of no previous work studying efficient representations of exact distributions over clusterings. This paper presents definitions and proofs for a dynamic-programming inference procedure that computes the partition function, the marginal probability of a cluster, and the MAP clustering-all exactly. Rather than the Nth Bell number, these exact solutions take time and space proportional to the substantially smaller powerset of N. Indeed, we improve upon the time complexity of the algorithm introduced by Kohonen and Corander [11] for this problem by a factor of N. While still large, this previously unknown result is intellectually interesting in its own right, makes feasible exact inference for important real-world small data applications (such as medicine), and provides a natural stepping stone towards sparse-trellis approximations that enable further scalability (which we also explore). In experiments, we demonstrate the superiority of our approach over approximate methods in analyzing real-world gene expression data used in cancer treatment.	[Greenberg, Craig S.; Monath, Nicholas; Kobren, Ari; McGregor, Andrew; McCallum, Andrew] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Greenberg, Craig S.] NIST, Gaithersburg, MD 20899 USA; [Flaherty, Patrick] Univ Massachusetts, Dept Math & Stat, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst; National Institute of Standards & Technology (NIST) - USA; University of Massachusetts System; University of Massachusetts Amherst	Greenberg, CS (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.; Greenberg, CS (corresponding author), NIST, Gaithersburg, MD 20899 USA.	csgreenberg@cs.umass.edu; nmonath@cs.umass.edu; akobren@cs.umass.edu; flaherty@math.umass.edu; mcgregor@cs.umass.edu; mccallum@cs.umass.edu			Center for Intelligent Information Retrieval; DARPA [FA8750-13-2-0020]; National Science Foundation Graduate Research Fellowship [NSF-1451512]; National Science Foundation [1637536]	Center for Intelligent Information Retrieval; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF)); National Science Foundation(National Science Foundation (NSF))	This work was supported in part by the Center for Intelligent Information Retrieval, in part by DARPA under agreement number FA8750-13-2-0020, in part by the National Science Foundation Graduate Research Fellowship under Grant No. NSF-1451512 and in part by the National Science Foundation Grant 1637536. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.	Bansal N., 2004, MACHINE LEARNING; Bell E. T., 1934, ANN MATH; Blundell Charles, 2010, C UNC ART INT; Cancer Genome Atlas Network, 2012, NATURE; Dechter Rina, 1999, BUCKET ELIMINATION U; Geman D., 1984, IEEE T PATTERN ANAL; Heller K. A., 2005, INT C MACH LEARN; HUBERT L. J., 2001, COMBINATORIAL DATA A; Jensen Robert E., 1969, OPERATIONS RES; Kappes J.H., 2015, SSVM; Kohonen J., 2016, COMMUNICATIONS STAT; LeCun Y., 2006, PREDICTING STRUCTURE; Lehmann Brian D, 2014, J PATHOLOGY; Lovasz Laszlo, 1993, COMBINATORIAL PROBLE, V2nd; Papandreou G., 2011, INT C COMP VIS; Poole David, 1996, J ARTIFICIAL INTELLI; Reddy D.R., 1977, SPEECH UNDERSTANDING; Rouzier Roman, 2005, CLIN CANC RES; Saddiki Hachem, 2014, BIOINFORMATICS; Sorlie Therese, 2003, P NATL ACAD SCI; Sorlie Therese, 2001, P NATL ACAD SCI; Van Os BJ, 2004, J CLASSIFICATION; Yersal Ozlem, 2014, WORLD J CLIN ONCOLOG; Zanella Giacomo, 2016, ADV NEURAL INFORM PR	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003021
C	Grill, JB; Valko, M; Munos, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Grill, Jean-Bastien; Valko, Michal; Munos, Remi			Optimistic Optimization of a Brownian	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				APPROXIMATION	We address the problem of optimizing a Brownian motion. We consider a (random) realization W of a Brownian motion with input space in [0, 1]. Given W, our goal is to return an epsilon-approximation of its maximum using the smallest possible number of function evaluations, the sample complexity of the algorithm. We provide an algorithm with sample complexity of order log(2 )(1/epsilon). This improves over previous results of Al-Mharmah and Calvin (1996) and Calvin et al. (2017) which provided only polynomial rates. Our algorithm is adaptive-each query depends on previous values-and is an instance of the optimism-in-the-face-of-uncertainty principle.	[Grill, Jean-Bastien] INRIA Lille, SequeL Team, Lille, France; [Valko, Michal] Nord Europe, Lille, France; [Munos, Remi] DeepMind Paris, Paris, France		Grill, JB (corresponding author), INRIA Lille, SequeL Team, Lille, France.	jbgrill@google.com; michal.valko@inria.fr; munos@google.com			European CHIST-ERA project DELTA; French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council; Inria; Otto-von-Guericke-Universitat Magdeburg associated-team north-European project Allocate; French National Research Agency project ExTra-Learn [ANR-14-CE24-0010-01]; French National Research Agency project BoB [ANR-16CE23-0003]; FMJH Program PGMO; Criteo, a doctoral grant of Ecole Normale Superieure	European CHIST-ERA project DELTA; French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council(Region Hauts-de-France); Inria; Otto-von-Guericke-Universitat Magdeburg associated-team north-European project Allocate; French National Research Agency project ExTra-Learn(French National Research Agency (ANR)); French National Research Agency project BoB(French National Research Agency (ANR)); FMJH Program PGMO; Criteo, a doctoral grant of Ecole Normale Superieure	This research was supported by European CHIST-ERA project DELTA, French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, Inria and Otto-von-Guericke-Universitat Magdeburg associated-team north-European project Allocate, French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n. ANR-16CE23-0003), FMJH Program PGMO with the support to this program from Criteo, a doctoral grant of Ecole Normale Superieure, and Maryse & Michel Grill.	AlMharmah H, 1996, J GLOBAL OPTIM, V8, P81; Basu Kinjal, 2018, ARXIV170506808; Calvin JM, 2017, J COMPLEXITY, V39, P17, DOI 10.1016/j.jco.2016.11.002; Chapelle O., 2011, NEURAL INFORM PROCES; Denisov I., 1983, TEOR VEROYATNOST PRI, V28, P785; DURRETT RT, 1977, ANN PROBAB, V5, P117, DOI 10.1214/aop/1176995895; Hefter M, 2017, COMMUN MATH SCI, V15, P2121, DOI 10.4310/CMS.2017.v15.n8.a2; Munos R., 2011, NEURAL INFORM PROCES; Russo Daniel, 2018, FDN TRENDS MACHINE L; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	10	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303004
C	Grover, A; Achim, T; Ermon, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Grover, Aditya; Achim, Tudor; Ermon, Stefano			Streamlining Variational Inference for Constraint Satisfaction Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RANDOM K-SAT; SURVEY PROPAGATION; ALGORITHM; THRESHOLD	Several algorithms for solving constraint satisfaction problems are based on survey propagation, a variational inference scheme used to obtain approximate marginal probability estimates for variable assignments. These marginals correspond to how frequently each variable is set to true among satisfying assignments, and are used to inform branching decisions during search; however, marginal estimates obtained via survey propagation are approximate and can be self-contradictory. We introduce a more general branching strategy based on streamlining constraints, which sidestep hard assignments to variables. We show that streamlined solvers consistently outperform decimation-based solvers on random k-SAT instances for several problem sizes, shrinking the gap between empirical performance and theoretical limits of satisfiability by 16.3% on average for k = 3, 4, 5, 6.	[Grover, Aditya; Achim, Tudor; Ermon, Stefano] Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA	Stanford University	Grover, A (corresponding author), Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA.	adityag@cs.stanford.edu; tachim@cs.stanford.edu; ermon@cs.stanford.edu		Achim, Tudor/0000-0001-5577-3383	NSF [1651565, 1522054, 1733686]; Microsoft Research PhD Fellowship; Stanford Data Science Scholarship; FLI	NSF(National Science Foundation (NSF)); Microsoft Research PhD Fellowship(Microsoft); Stanford Data Science Scholarship; FLI	This research was supported by NSF (#1651565, #1522054, #1733686) and FLI. AG is supported by a Microsoft Research PhD Fellowship and a Stanford Data Science Scholarship. We are grateful to Neal Jean for helpful comments on early drafts.	Achim T, 2016, PR MACH LEARN RES, V48; Achlioptas D, 2004, J AM MATH SOC, V17, P947, DOI 10.1090/S0894-0347-04-00464-3; Achlioptas D, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P22; Banko M., 2007, INT JOINT C ART INT; Biere A, 2009, FRONT ARTIF INTEL AP, V185, P457, DOI 10.3233/978-1-58603-929-5-457; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Braunstein A, 2005, RANDOM STRUCT ALGOR, V27, P201, DOI 10.1002/rsa.20057; Chen M, 2017, IEEE T AUTOMAT CONTR, V62, P1451, DOI 10.1109/TAC.2016.2577619; Chen M, 2014, P AMER CONTR CONF; Coja-Oghlan A, 2016, ADV MATH, V288, P985, DOI 10.1016/j.aim.2015.11.007; Comes C, 2004, LECT NOTES COMPUT SC, V3258, P274; Daude H, 2008, LECT NOTES COMPUT SC, V4957, P12, DOI 10.1007/978-3-540-78773-0_2; Ding J, 2015, S THEOR COMP; Do MB, 2001, ARTIF INTELL, V132, P151, DOI 10.1016/S0004-3702(01)00128-X; Ermon S, 2014, PR MACH LEARN RES, V32; Euzenat J., 2007, ONTOLOGY MATCHING, V333; Gableske O, 2013, PRAGMATICS SAT POS; Gomes C P, 2007, AAAI C ART INT; Grover A, 2016, ADV NEURAL INFORM PR; Hromkovic Juraj, 2013, ALGORITHMICS HARD PR; Kroc L, 2009, S APPL COMP; Maneva E, 2007, J ACM, V54, DOI 10.1145/1255443.1255445; Marino R, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12996; Mezard M, 2002, SCIENCE, V297, P812, DOI 10.1126/science.1073287; Mitchell D, 1992, AAAI C ART INT; Molloy M, 2003, SIAM J COMPUT, V32, P935, DOI 10.1137/S0097539700368667; Nuijten W, 1994, THESIS; Pittel B, 2016, COMB PROBAB COMPUT, V25, P236, DOI 10.1017/S0963548315000097; Ren H, 2018, INT JOINT C ART INT; Selman B, 1994, AAAI C ART INT; Singla P, 2006, INT C DAT MIN; Stewart R, 2017, AAAI CONF ARTIF INTE, P2576; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wei W, 2004, AAAI C ART INT; Zhao  S., 2016, AAAI C ART INT; Zhou Z, 2014, AM CONTR C; Zhou Z, 2017, ADV NEURAL INFORM PR; Zhou ZY, 2018, AUTOMATICA, V89, P28, DOI 10.1016/j.automatica.2017.11.035	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005016
C	Guo, DL; Yu, AJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Guo, Dalin; Yu, Angela J.			Why so gloomy? A Bayesian explanation of human pessimism bias in the multi-armed bandit task	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					How humans make repeated choices among options with imperfectly known reward outcomes is an important problem in psychology and neuroscience. This is often studied using multi-armed bandits, which is also frequently studied in machine learning. We present data from a human stationary bandit experiment, in which we vary the average abundance and variability of reward availability (mean and variance of the reward rate distribution). Surprisingly, we find subjects significantly underestimate prior mean of reward rates - based on their self-report on their reward expectation of non-chosen arms at the end of a game. Previously, human learning in the bandit task was found to be well captured by a Bayesian ideal learning model, the Dynamic Belief Model (DBM), albeit under an incorrect generative assumption of the temporal structure - humans assume reward rates can change over time even though they are truly fixed. We find that the "pessimism bias" in the bandit task is well captured by the prior mean of DBM when fitted to human choices; but it is poorly captured by the prior mean of the Fixed Belief Model (FBM), an alternative Bayesian model that (correctly) assumes reward rates to be constants. This pessimism bias is also incompletely captured by a simple reinforcement learning model (RL) commonly used in neuroscience and psychology, in terms of fitted initial Q-values. While it seems sub-optimal, and thus mysterious, that humans have an underestimated prior reward expectation, our simulations show that an underestimated prior mean helps to maximize long-term gain, if the observer assumes volatility when reward rates are stable, and utilizes a softmax decision policy instead of the optimal one (obtainable by dynamic programming). This raises the intriguing possibility that the brain underestimates reward rates to compensate for the incorrect non-stationarity assumption in the generative model and a simplified decision policy.	[Guo, Dalin; Yu, Angela J.] Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Guo, DL (corresponding author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.	dag082@ucsd.edu; ajyu@ucsd.edu	Guo, Dalin/ABD-2161-2020		NSF CRCNS grant [BCS-1309346]	NSF CRCNS grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank Shunan Zhang, Henry Qiu, Alvita Tran, Joseph Schilz, and numerous undergraduate research assistants who helped in the data collection. We thank Samer Sabri for helpful input with the writing. This work was in part funded by an NSF CRCNS grant (BCS-1309346) to AJY.	Averbeck BB, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004164; Bateson M, 2016, CURR OPIN BEHAV SCI, V12, P115, DOI 10.1016/j.cobeha.2016.09.013; Behrens TEJ, 2007, NAT NEUROSCI, V10, P1214, DOI 10.1038/nn1954; BERK RA, 1983, AM SOCIOL REV, V48, P386, DOI 10.2307/2095230; Cohen JD, 2007, PHILOS T R SOC B, V362, P933, DOI 10.1098/rstb.2007.2098; Daw ND, 2006, NATURE, V441, P876, DOI 10.1038/nature04766; Dezza IC, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-17237-w; Frazier P, 2008, ADV NEURAL INFORM PR; Gershman SJ, 2018, COGNITION, V173, P34, DOI 10.1016/j.cognition.2017.12.014; Gershman SJ, 2015, TOP COGN SCI, V7, P391, DOI 10.1111/tops.12138; Harle KM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186473; Harle KM, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01910; Ide JS, 2013, J NEUROSCI, V33, P2039, DOI 10.1523/JNEUROSCI.2201-12.2013; Montague PR, 1996, J NEUROSCI, V16, P1936, DOI 10.1523/jneurosci.16-05-01936.1996; Nickerson RS, 2002, PSYCHOL REV, V109, P330, DOI 10.1037//0033-295X.109.2.330; Rescorla RA., 1972, CLASSICAL CONDITION, pp. 64, DOI DOI 10.1101/GR.110528.110; RYALI CK, 2018, ADV NEURAL INFORM PR; Schonberg T, 2007, J NEUROSCI, V27, P12860, DOI 10.1523/JNEUROSCI.2496-07.2007; Schultz W, 1998, J NEUROPHYSIOL, V80, P1, DOI 10.1152/jn.1998.80.1.1; Sharot T, 2011, CURR BIOL, V21, pR941, DOI 10.1016/j.cub.2011.10.030; Shenoy P., 2010, ADV NEURAL INFORM PR, V23, P2146; Speekenbrink M, 2015, TOP COGN SCI, V7, P351, DOI 10.1111/tops.12145; Stankevicius A, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003605; Steyvers M, 2009, J MATH PSYCHOL, V53, P168, DOI 10.1016/j.jmp.2008.11.002; Yu A J., 2014, DECISION, V1, P275, DOI [10.1037/dec0000013, DOI 10.1037/DEC0000013]; Yu AJ, 2009, ADV NEURAL INF PROCE, V21, P1873, DOI DOI 10.1371/JOURNAL.PONE.0099909; Zhang  S, 2013, ADV NEURAL INFORM PR, V26	27	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305021
C	Gupta, A; Mendonca, R; Liu, YX; Abbeel, P; Levine, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gupta, Abhishek; Mendonca, Russell; Liu, YuXuan; Abbeel, Pieter; Levine, Sergey			Meta-Reinforcement Learning of Structured Exploration Strategies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm - model agnostic exploration with structured noise (MAESN) - to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.	[Gupta, Abhishek; Mendonca, Russell; Liu, YuXuan; Abbeel, Pieter; Levine, Sergey] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Gupta, A (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	abhigupta@eecs.berkeley.edu; russellm@berkeley.edu; yuxuanliu@berkeley.edu; pabbeel@eecs.berkeley.edu; svlevine@eecs.berkeley.edu			National Science Foundation [IIS-1651843, IIS-1614653]; ONR PECASE award; ONR Young Investigator Program	National Science Foundation(National Science Foundation (NSF)); ONR PECASE award; ONR Young Investigator Program	The authors would like to thank Chelsea Finn, Gregory Kahn, Ignasi Clavera for thoughtful discussions and Justin Fu, Marvin Zhang for comments on an early version of the paper. This work was supported by a National Science Foundation Graduate Research Fellowship for Abhishek Gupta, ONR PECASE award for Pieter Abbeel, and the National Science Foundation through IIS-1651843 and IIS-1614653, as well as an ONR Young Investigator Program award for Sergey Levine.	Andrychowicz M., 2016, NIPS; [Anonymous], 2016, CORR; Bellemare M, 2016, NIPS; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Duan Y., 2016, CORR; Finn Chelsea, 2017, ICML; Florensa C., 2017, CORR; Fortunato M., 2017, CORR; Hausman K., 2018, P INT C LEARN REPR I; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Houthooft R., 2016, NIPS; J Zico Kolter, 2007, RSS; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Levine S, 2016, J MACH LEARN RES, V17; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li Z., 2017, FSSD FEATURE FUSION; Lopes M., 2012, NIPS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Osband I., 2016, NIPS; Plappert M., 2017, CORR; Rajeswaran Aravind, 2017, CORR; Ravi S., 2017, INT C LEARN REPR; Santoro A., 2016, ICML; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Schulman J., 2015, ICML; Singh Satinder, 2004, NIPS; Stadie Bradly C., 2015, CORR; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Thrun S, 1998, LEARNING TO LEARN, P3; Vinyals O., 2016, NIPS; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	31	0	0	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305033
C	Gupta, MR; Bahri, D; Cotter, A; Canini, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gupta, Maya R.; Bahri, Dara; Cotter, Andrew; Canini, Kevin			Diminishing Returns Shape Constraints for Interpretability and Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs. We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of concavity/convexity and monotonicity constraints on any subsets of features, and compare to new shape-constrained neural networks. We demonstrate on real-world examples that these shape constrained models can provide tuning-free regularization and improve model understandability.	[Gupta, Maya R.; Bahri, Dara; Cotter, Andrew; Canini, Kevin] Google AI, 1600 Charleston Rd, Mountain View, CA 94043 USA		Gupta, MR (corresponding author), Google AI, 1600 Charleston Rd, Mountain View, CA 94043 USA.	mayagupta@google.com; dbahri@google.com; acotter@google.com; canini@google.com						Amos B., 2017, P ICML; ARCHER NP, 1993, DECISION SCI, V24, P60, DOI 10.1111/j.1540-5915.1993.tb00462.x; Barlow R. E., 1972, STAT INFERENCE ORDER; Boyd  S., 2008, CONVEX OPTIMIZATION; Canini K., 2016, ADV NEURAL INFORM PR; Chen Y., 2016, J ROYAL STAT SOC B; Cotter A., 2016, C LEARN THEOR, P729; Daniels H, 2010, IEEE T NEURAL NETWOR, V21, P906, DOI 10.1109/TNN.2010.2044803; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Dugas C., 2009, J MACHINE LEARNING R; Fard Mahdi Milani, 2016, ADV NEURAL INFORM PR; Garcia E. K., 2009, ADV NEURAL INFORM PR; Garcia E, 2012, IEEE T IMAGE PROCESS, V21, P4128, DOI 10.1109/TIP.2012.2200902; Groeneboom P., 2014, NONPARAMETRIC ESTIMA; Gupta M, 2016, J MACH LEARN RES, V17; Hastie T.J., 1990, GEN ADDITIVE MODELS, V43; Kahneman D, 2000, CHOICES VALUES FRAME; Kay H, 2000, AICHE J, V46, P2426, DOI 10.1002/aic.690461211; Kim J., 2004, P IEEE INT C COMP AI; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Magnani A., 2009, OPTIMIZATION ENG; Minin A, 2010, NEURAL NETWORKS, V23, P471, DOI 10.1016/j.neunet.2009.09.002; Patton F. L., 1926, DIMINISHING RETURNS; Pya N., 2015, STAT COMPUTING; Qu YJ, 2011, IEEE T NEURAL NETWOR, V22, P2447, DOI 10.1109/TNN.2011.2167348; Shingo S., 1986, ZERO QUALITY CONTROL; Sill J., 1998, ADV NEURAL INFORM PR; Smith A., 1817, PRINCIPLES POLITICAL; Smith Adam, 1981, INQUIRY NATURE CAUSE; You S., 2017, ADV NEURAL INFORM PR	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001038
C	Gupta, N; Sidford, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gupta, Neha; Sidford, Aaron			Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix A is an element of R-nxd where every row a is an element of R-d has parallel to a parallel to(2)(2) <= L and numerical sparsity at most s, i.e. parallel to a parallel to(2)(1)/parallel to a parallel to(2)(2) <= s, we provide faster algorithms for these problems in many parameter settings. For top eigenvector computation, we obtain a running time of (O) over tilde (nd + r(s + root rs)/gap(2)) where gap > 0 is the relative gap between the top two eigenvectors of A(inverted perpendicular) A and r is the stable rank of A. This running time improves upon the previous best unaccelerated running time of O(nd + rd/gap(2)) as r <= d and s <= d. For regression, we obtain a running time of (O) over tilde (nd (nL/mu) root snL/mu) where > 0 is the smallest eigenvalue of A(inverted perpendicular)A. This running time improves upon the previous best unaccelerated running time of (O) over tilde (nd + nLd/mu). This result expands the regimes where regression can be solved in nearly linear time from when L/mu = (O) over tilde (1) to when L/mu = (O) over tilde (d(2/3) / (sn)(1/3)). Furthermore, we obtain similar improvements even when row norms and numerical sparsities are non-uniform and we show how to achieve even faster running times by accelerating using approximate proximal point [9] / catalyst [15]. Our running times depend only on the size of the input and natural numerical measures of the matrix, i.e. eigenvalues and l(p) norms, making progress on a key open problem regarding optimal running times for efficient large-scale learning.	[Gupta, Neha] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Sidford, Aaron] Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA	Stanford University; Stanford University	Gupta, N (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	nehagupta@cs.stanford.edu; sidford@stanford.edu						Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097; Achlioptas Dimitris, 2013, ADV NEURAL INFORM PR, P1565; Agarwal Naman, 2017, ARXIV171108426; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Arora S, 2006, LECT NOTES COMPUT SC, V4110, P272; Bottou U, 2004, ADV NEUR IN, V16, P217; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Drineas P, 2011, INFORM PROCESS LETT, V111, P385, DOI 10.1016/j.ipl.2011.01.010; Frostig R, 2015, PR MACH LEARN RES, V37, P2540; Garber D, 2016, PR MACH LEARN RES, V48; Gittens Alex, 2009, ARXIV09114108; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Konecny J, 2017, OPTIM METHOD SOFTW, V32, P993, DOI 10.1080/10556788.2017.1298596; Kundu A., 2014, ARXIV14040320; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Musco C, 2015, ADV NEUR IN, V28; Musco Cameron, 2017, ARXIV170404163; Nguyen N. H, 2010, ARXIV10054732; Nguyen NH, 2009, MATRIX SPARSIFICATIO; Shamir O, 2015, PR MACH LEARN RES, V37, P144	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305030
C	Gur, Y; Momeni, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gur, Yonatan; Momeni, Ahmadreza			Adaptive Learning with Unknown Information Flows	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					An agent facing sequential decisions that are characterized by partial feedback needs to strike a balance between maximizing immediate payoffs based on available information, and acquiring new information that may be essential for maximizing future payoffs. This trade-off is captured by the multi-armed bandit (MAB) framework that has been studied and applied when at each time epoch payoff observations are collected on the actions that are selected at that epoch. In this paper we introduce a new, generalized MAB formulation in which additional information on each arm may appear arbitrarily throughout the decision horizon, and study the impact of such information flows on the achievable performance and the design of efficient decision-making policies. By obtaining matching lower and upper bounds, we characterize the (regret) complexity of this family of MAB problems as a function of the information flows. We introduce an adaptive exploration policy that, without any prior knowledge of the information arrival process, attains the best performance (in terms of regret rate) that is achievable when the information arrival process is a priori known. Our policy uses dynamically customized virtual time indexes to endogenously control the exploration rate based on the realized information arrival process.	[Gur, Yonatan] Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA; [Momeni, Ahmadreza] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Stanford University; Stanford University	Gur, Y (corresponding author), Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA.	ygur@stanford.edu; amomenis@stanford.edu	Momeni, Ahmadreza/Q-1837-2018	Momeni, Ahmadreza/0000-0002-0575-7016				Agrawal S., 2013, ARTIF INTELL, P99; Audibert J.-Y., 2010, COLT 23 C LEARN THEO, P13; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bastani H., 2017, ARXIV170409011; Bastani H., 2015, PREPRINT; Besbes O., 2014, ADV NEURAL INFORM PR, P199; Bubeck S., 2013, P 26 ANN C LEARN THE, P122; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Goldenshluger A., 2013, STOCHASTIC SYSTEMS, V3, P230, DOI [10.1287/11-SSY032, DOI 10.1287/11-SSY032]; Jadbabaie A, 2015, JMLR WORKSH CONF PRO, V38, P398; Komiyama J., 2013, P AS C MACH LEARN NO, V29, P100; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Langford J., 2008, ADV NEURAL INFORM PR, P817; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Sani A., 2014, ADV NEURAL INFORM PR, V27, P810; Seldin Y, 2014, PR MACH LEARN RES, V32, P1287; Shah V., 2018, ARXIV180205693; Strehl A. L., 2006, P 23 INT C MACH LEAR, P889; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Traca S., 2015, ARXIV150505629; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Wang CC, 2005, IEEE T AUTOMAT CONTR, V50, P338, DOI 10.1109/TAC.2005.844079	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002006
C	Halloran, JT; Rocke, DM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Halloran, John T.; Rocke, David M.			Learning Concave Conditional Likelihood Models for Improved Analysis of Tandem Mass Spectra	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FALSE DISCOVERY RATE; PEPTIDE IDENTIFICATION	The most widely used technology to identify the proteins present in a complex biological sample is tandem mass spectrometry, which quickly produces a large collection of spectra representative of the peptides (i.e., protein subsequences) present in the original sample. In this work, we greatly expand the parameter learning capabilities of a dynamic Bayesian network (DBN) peptide-scoring algorithm, Didea [25], by deriving emission distributions for which its conditional log-likelihood scoring function remains concave. We show that this class of emission distributions, called Convex Virtual Emissions (CVEs), naturally generalizes the log-sum-exp function while rendering both maximum likelihood estimation and conditional maximum likelihood estimation concave for a wide range of Bayesian networks. Utilizing CVEs in Didea allows efficient learning of a large number of parameters while ensuring global convergence, in stark contrast to Didea's previous parameter learning framework (which could only learn a single parameter using a costly grid search) and other trainable models [12, 13, 14] (which only ensure convergence to local optima). The newly trained scoring function substantially outperforms the state-of-the-art in both scoring function accuracy and downstream Fisher kernel analysis. Furthermore, we significantly improve Didea's runtime performance through successive optimizations to its message passing schedule and derive explicit connections between Didea's new concave score and related MS/MS scoring functions.	[Halloran, John T.; Rocke, David M.] Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA	University of California System; University of California Davis	Halloran, JT (corresponding author), Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA.	jthalloran@ucdavis.edu; dmrocke@ucdavis.edu			National Center for Advancing Translational Sciences (NCATS), National Institutes of Health [UL1 TR001860]	National Center for Advancing Translational Sciences (NCATS), National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Advancing Translational Sciences (NCATS))	This work was supported by the National Center for Advancing Translational Sciences (NCATS), National Institutes of Health, through grant UL1 TR001860.	BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Craig R, 2004, BIOINFORMATICS, V20, P1466, DOI 10.1093/bioinformatics/bth092; Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deng L., 2006, SYNTH LECT SPEECH AU, V2, P1; Diament BJ, 2011, J PROTEOME RES, V10, P3871, DOI 10.1021/pr101196n; Eng JK, 2008, J PROTEOME RES, V7, P4598, DOI 10.1021/pr800420s; Eng JK, 2013, PROTEOMICS, V13, P22, DOI 10.1002/pmic.201200439; ENG JK, 1994, J AM SOC MASS SPECTR, V5, P976, DOI 10.1016/1044-0305(94)80016-2; Geer LY, 2004, J PROTEOME RES, V3, P958, DOI 10.1021/pr0499491; Halloran JT, 2018, METHODS MOL BIOL, V1807, P163, DOI 10.1007/978-1-4939-8561-6_12; Halloran JT, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P320; Halloran JT, 2016, J PROTEOME RES, V15, P2749, DOI 10.1021/acs.jproteome.6b00290; Halloran John T., 2018, ADV NEURAL INFORM PR; Halloran John T, 2017, ADV NEURAL INF PROCE, P5728; Jeffry Howbert  J, 2014, MOL CELLULAR PROTEOM, pmcp; Kall L, 2007, NAT METHODS, V4, P923, DOI 10.1038/NMETH1113; Keich U, 2015, J PROTEOME RES, V14, P3148, DOI 10.1021/acs.jproteome.5b00081; Kim S., 2014, NATURE COMMUNICATION, V5; Klammer AA, 2009, J PROTEOME RES, V8, P2106, DOI 10.1021/pr8011107; McIlwain  Sean, 2014, J PROTEOME RES; Murphy KP, 2002, ADV NEUR IN, V14, P833; Park CY, 2008, J PROTEOME RES, V7, P3022, DOI 10.1021/pr800127y; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Singh AP, 2012, UNCERTAINTY ARTIFICI; Spivak M, 2012, MOL CELLULAR PROTEOM, V11; Spivak M, 2009, J PROTEOME RES, V8, P3737, DOI 10.1021/pr801109k; Wenger C. D, 2013, J PROTEOME RES	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305044
C	Han, I; Avron, H; Shin, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Han, Insu; Avron, Haim; Shin, Jinwoo			Stochastic Chebyshev Gradient Descent for Spectral Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LOW-RANK; MATRIX	A large class of machine learning techniques requires the solution of optimization problems involving spectral functions of parametric matrices, e.g. log-determinant and nuclear norm. Unfortunately, computing the gradient of a spectral function is generally of cubic complexity, as such gradient descent methods are rather expensive for optimizing objectives involving the spectral function. Thus, one naturally turns to stochastic gradient methods in hope that they will provide a way to reduce or altogether avoid the computation of full gradients. However, here a new challenge appears: there is no straightforward way to compute unbiased stochastic gradients for spectral functions. In this paper, we develop unbiased stochastic gradients for spectral-sums, an important subclass of spectral functions. Our unbiased stochastic gradients are based on combining randomized trace estimators with stochastic truncation of the Chebyshev expansions. A careful design of the truncation distribution allows us to offer distributions that are variance-optimal, which is crucial for fast and stable convergence of stochastic gradient methods. We further leverage our proposed stochastic gradients to devise stochastic methods for objective functions involving spectral-sums, and rigorously analyze their convergence rate. The utility of our methods is demonstrated in numerical experiments.	[Han, Insu; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Avron, Haim] Tel Aviv Univ, Dept Appl Math, Tel Aviv, Israel; [Shin, Jinwoo] AItrics, Seoul, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Tel Aviv University	Han, I (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	insu.han@kaist.ac.kr; haimav@post.tau.ac.il; jinwoos@kaist.ac.kr			National Research Foundation of Korea(NRF) - Korea government(MS IT) [2018RIA5A1059921]; Israel Science Foundation [1272/17]	National Research Foundation of Korea(NRF) - Korea government(MS IT); Israel Science Foundation(Israel Science Foundation)	This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MS IT) (2018RIA5A1059921). Haim Avron acknowledges the support of the Israel Science Foundation (grant no. 1272/17)	Adams R. P., 2018, ARXIV180203451; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; [Anonymous], 2017, P 32 INT C NEUR INF; Avron H, 2011, J ACM, V58, DOI 10.1145/1944345.1944349; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Broniatowski Michel, 2014, ARXIV14035113; Budincsevity N, 2016, WEATHER SZEGED 2006; DAVIDSON ER, 1975, J COMPUT PHYS, V17, P87, DOI 10.1016/0021-9991(75)90065-0; Friedlander MP, 2016, SIAM J SCI COMPUT, V38, pA1616, DOI 10.1137/15M1034283; Garber D., 2015, ARXIV150905647; Han I, 2015, PR MACH LEARN RES, V37, P908; Han I, 2017, SIAM J SCI COMPUT, V39, pA1558, DOI 10.1137/16M1078148; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; HUTCHINSON MF, 1989, COMMUN STAT SIMULAT, V18, P1059, DOI 10.1080/03610918908812806; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Koltchinskii V, 2015, J MACH LEARN RES, V16, P1757; Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68; Lewis AS, 1996, MATH OPER RES, V21, P576, DOI 10.1287/moor.21.3.576; Lu CY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2014.2380155; Mason J.C., 2002, CHEBYSHEV POLYNOMIAL; Mohan K, 2012, J MACH LEARN RES, V13, P3441; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roosta-Khorasani F, 2015, FOUND COMPUT MATH, V15, P1187, DOI 10.1007/s10208-014-9220-1; Trefethen LN, 2013, APPROXIMATION THEORY AND APPROXIMATION PRACTICE, P1; Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974; Vinck M, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.051139; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001090
C	Hansen, SS; Sprechmann, P; Pritzel, A; Barreto, A; Blundell, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hansen, Steven S.; Sprechmann, Pablo; Pritzel, Alexander; Barreto, Andre; Blundell, Charles			Fast deep reinforcement learning using online adjustments from the past	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MEMORY	We propose Ephemeral Value Adjusments (EVA): a means of allowing deep reinforcement learning agents to rapidly adapt to experience in their replay buffer. EVA shifts the value predicted by a neural network with an estimate of the value function found by planning over experience tuples from the replay buffer near the current state. EVA combines a number of recent ideas around combining episodic memory-like structures into reinforcement learning agents: slot-based storage, content-based retrieval, and memory-based planning. We show that EVA is performant on a demonstration task and Atari games.	[Hansen, Steven S.; Sprechmann, Pablo; Pritzel, Alexander; Barreto, Andre; Blundell, Charles] DeepMind, London, England		Hansen, SS (corresponding author), DeepMind, London, England.	stevenhansen@google.com; psprechmann@google.com; apritzel@google.com; andrebarreto@google.com; cblundell@google.com	Barreto, André M S/J-5063-2013	Pritzel, Alexander/0000-0002-4233-9040				Bakker B., 2003, P 2003 IEEE RSJ INT, V1, P430, DOI DOI 10.1109/IROS.2003.1250667; Barth-Maron G., 2018, P INT C LEARN REPR V; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Blundell C., 2016, ARXIV160604460; Brea Johanni, 2017, ARXIV171106677; Dayan P, 2008, COGN AFFECT BEHAV NE, V8, P429, DOI 10.3758/CABN.8.4.429; Espeholt L, 2018, PR MACH LEARN RES, V80; Gabel T, 2005, LECT NOTES ARTIF INT, V3620, P206, DOI 10.1007/11536406_18; Gershman SJ, 2017, ANNU REV PSYCHOL, V68, P101, DOI 10.1146/annurev-psych-122414-033625; Grave Edouard, 2016, ARXIV161204426; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Hausknecht Matthew, 2015, 2015 AAAI FALL S SER; Kaiser Lukasz, 2016, LEARNING REMEMBER RA; Koltun, 2018, ICLR, P1; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; Munos Remi, 1998, NIPS, P1024; Nishio Daichi, 2018, ARXIV180101968; Oh J, 2016, PR MACH LEARN RES, V48; Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829; Pritzel A, 2017, PR MACH LEARN RES, V70; Santamaria JC, 1997, ADAPT BEHAV, V6, P163, DOI 10.1177/105971239700600201; Sarkin Jain Mika, 2018, ICLR 2018 WORKSH; Schaul T, 2015, P INT C LEARN REPR; Silver D., 2008, P 25 INT C MACH LEAR, P968, DOI DOI 10.1145/1390156.1390278; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sprechmann Pablo, 2018, INT C LEARN REPR; Stepleton T., 2017, PYCOLAB GAME ENGINE; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Wayne G., 2018, UNSUPERVISED PREDICT; Xiao CJ, 2018, AAAI CONF ARTIF INTE, P1455	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005017
C	Hao, Y; Orlitsky, A; Suresh, AT; Wu, YH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hao, Yi; Orlitsky, Alon; Suresh, Ananda T.; Wu, Yihong			Data Amplification: A Unified and Competitive Approach to Property Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NONPARAMETRIC-ESTIMATION; MINIMAX ESTIMATION; INFORMATION; ENTROPY; NUMBER	Estimating properties of discrete distributions is a fundamental problem in statistical learning. We design the first unified, linear-time, competitive, property estimator that for a wide class of properties and for all underlying distributions uses just 2n samples to achieve the performance attained by the empirical estimator with n root log n samples. This provides off-the-shelf, distribution-independent, "amplification" of the amount of data available relative to common-practice estimators. We illustrate the estimator's practical advantages by comparing it to existing estimators for a wide variety of properties and distributions. In most cases, its performance with n samples is even as good as that of the empirical estimator with n log n samples, and for essentially all properties, its performance is comparable to that of the best existing estimator designed specifically for that property.	[Hao, Yi; Orlitsky, Alon] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA; [Suresh, Ananda T.] Google Res, New York, NY 10011 USA; [Wu, Yihong] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06511 USA	University of California System; University of California San Diego; Google Incorporated; Yale University	Hao, Y (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.	yih179@eng.ucsd.edu; alon@eng.ucsd.edu; theertha@google.com; yihong.wu@yale.edu						Acharya J, 2017, PR MACH LEARN RES, V70; Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435; Batu T, 2017, ANN IEEE SYMP FOUND, P880, DOI 10.1109/FOCS.2017.86; Bustamante J, 2017, BERNSTEIN OPERATORS; Canonne C. L., 2017, SURVEY DISTRIBUTION; CARLTON AG, 1969, PSYCHOL BULL, V71, P108, DOI 10.1037/h0026857; CHAO A, 1984, SCAND J STAT, V11, P265; Chao A., 2005, ENCY STAT SCI; CHUNG F. R, 2017, COMPLEX GRAPHS NETWO; Colwell RK, 2012, J PLANT ECOL, V5, P3, DOI 10.1093/jpe/rtr044; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Haas P. J., 1995, VLDB '95. Proceedings of the 21st International Conference on Very Large Data Bases, P311; HAN Y, 2018, ARXIV180208405; Hao Y, 2018, IEEE INT SYMP INFO, P1076; Ionita-Laza I, 2009, P NATL ACAD SCI USA, V106, P5008, DOI 10.1073/pnas.0807815106; Jiao JT, 2016, IEEE INT SYMP INFO, P750; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Korneichuk N.P., 1991, EXACT CONSTANTS APPR, V38; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Loh WY, 2011, WIRES DATA MIN KNOWL, V1, P14, DOI 10.1002/widm.8; MCNEIL DR, 1973, J AM STAT ASSOC, V68, P92, DOI 10.2307/2284147; Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113; Orlitsky Alon, 2015, C LEARN THEOR, P1066; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Renyi A., 1961, MEASURES ENTROPY INF; Sarndal C. E., 2003, MODEL ASSISTED SURVE; SMITH EP, 1984, BIOMETRICS, V40, P119, DOI 10.2307/2530750; Timan AF, 2014, THEORY APPROXIMATION; Valiant G, 2017, J ACM, V64, DOI 10.1145/3125643; Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; [No title captured]	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003039
C	Harris, DG; Li, S; Pensyl, T; Srinivasan, A; Trinh, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Harris, David G.; Li, Shi; Pensyl, Thomas; Srinivasan, Aravind; Khoa Trinh			Approximation algorithms for stochastic clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA			clustering; k-center; k-median; lottery; approximation algorithms	RACE	We consider stochastic settings for clustering, and develop provably-good (approximation) algorithms for a number of these notions. These algorithms allow one to obtain better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including providing fairer clustering and clustering which has better long-term behavior for each user. In particular, they ensure that every user is guaranteed to get good service (on average). We also complement some of these with impossibility results.	[Harris, David G.; Srinivasan, Aravind] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Li, Shi] SUNY Buffalo, Buffalo, NY USA; [Pensyl, Thomas] Bandwidth Inc, Raleigh, NC USA; [Srinivasan, Aravind] Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA; [Khoa Trinh] Google, Mountain View, CA 94043 USA	University System of Maryland; University of Maryland College Park; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; University System of Maryland; University of Maryland College Park; Google Incorporated	Harris, DG (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	davidgharris29@gmail.com; shil@buffalo.edu; tpensyl@bandwidth.com; srin@cs.umd.edu; khoatrinh@google.com			NSF [CNS-1010789, CCF-1422569, CCF-1749864, CCF-1566356, CCF-1717134]; Adobe, Inc.	NSF(National Science Foundation (NSF)); Adobe, Inc.	Research supported in part by NSF Awards CNS-1010789, CCF-1422569 and CCF-1749864, CCF-1566356, CCF-1717134 and by research awards from Adobe, Inc.	Ahmadian S., 2016, CORR; Alipour S, 2018, PODS'18: PROCEEDINGS OF THE 37TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P425, DOI 10.1145/3196959.3196969; Ayres I, 2015, RAND J ECON, V46, P891, DOI 10.1111/1756-2171.12115; BADGER E, 2016, WASHINGTON POST; Bertrand M, 2004, AM ECON REV, V94, P991, DOI 10.1257/0002828042002561; Byrka J, 2017, ACM T ALGORITHMS, V13, DOI 10.1145/2981561; Charikar M, 2012, LECT NOTES COMPUT SC, V7391, P194, DOI 10.1007/978-3-642-31594-7_17; Datta Amit, 2015, Proceedings on Privacy Enhancing Technologies, V1, P92, DOI 10.1515/popets-2015-0007; Guha S, 1999, J ALGORITHMS, V31, P228, DOI 10.1006/jagm.1998.0993; Harris D. G., 2017, LIPICS LEIBNIZ INT P, V81; Harris David G., 2017, ABS170906995 ARXIV; HOCHBAUM DS, 1986, J ACM, V33, P533, DOI 10.1145/5925.5933; Huang LX, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P110; Ibarra I. A., 2018, AM EC ASS PAPERS P, V1; Jain K., 2002, P 34 ANN ACM S THEOR, P731, DOI [DOI 10.1145/509907.510012, 10.1145/509907.510012, DOI 10.1016/J.0RL.2006.03.0]; Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003; Kempe D., 2015, THEORY COMPUT, V11, P105, DOI [DOI 10.4086/T0C.2015.V011A004, 10.4086/toc.2015.v011a004, DOI 10.4086/TOC.2015.V011A004]; Krishnaswamy R, 2018, ACM S THEORY COMPUT, P646, DOI 10.1145/3188745.3188882; Lanier J., 2014, WHO OWNS FUTURE; Moshkovitz D., 2015, THEORY COMPUT, V11, P221, DOI [DOI 10.4086/TOC.2015.V011A007, 10.4086/toc.2015.v011a007]; Schulman KA, 1999, NEW ENGL J MED, V340, P618, DOI 10.1056/NEJM199902253400806; Srinivasan A, 2001, ANN IEEE SYMP FOUND, P588, DOI 10.1109/SFCS.2001.959935	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000053
C	Hassidim, A; Singer, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hassidim, Avinatan; Singer, Yaron			Optimization for Approximate Submodularity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MAXIMIZATION; ALGORITHMS; QUERIES	We consider the problem of maximizing a submodular function when given access to its approximate version. Submodular functions are heavily studied in a wide variety of disciplines since they are used to model many real world phenomena and are amenable to optimization. There are many cases however in which the phenomena we observe is only approximately submodular and the optimization guarantees cease to hold. In this paper we describe a technique that yields strong guarantees for maximization of monotone submodular functions from approximate surrogates under cardinality and intersection of matroid constraints. In particular, we show tight guarantees for maximization under a cardinality constraint and 1/(1 + P) approximation under intersection of P matroids.	[Hassidim, Avinatan] Bar Ilan Univ, Ramat Gan, Israel; [Hassidim, Avinatan] Google, Ramat Gan, Israel; [Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA	Bar Ilan University; Google Incorporated; Harvard University	Hassidim, A (corresponding author), Bar Ilan Univ, Ramat Gan, Israel.; Hassidim, A (corresponding author), Google, Ramat Gan, Israel.	avinatan@cs.biu.ac.il; yaron@seas.harvard.edu			BSF [2014389]; NSF [CCF 1452961, CCF 1301976, 1540428]; Google Research award; Facebook research award;  [1394/16]	BSF(US-Israel Binational Science Foundation); NSF(National Science Foundation (NSF)); Google Research award(Google Incorporated); Facebook research award(Facebook Inc); 	A.H. is supported by 1394/16 and by a BSF grant. Y.S. is supported by NSF grant CAREER CCF 1452961, NSF CCF 1301976, BSF grant 2014389, NSF USICCS proposal 1540428, a Google Research award, and a Facebook research award.	Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2; Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753; Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Balcan MF, 2011, ACM S THEORY COMPUT, P793; Bshouty NH, 2002, J MACH LEARN RES, V2, P359, DOI 10.1162/153244302760200669; Buchbinder N., 2014, P 25 ANN ACM SIAM S, P1433; Buchbinder N, 2012, ANN IEEE SYMP FOUND, P649, DOI 10.1109/FOCS.2012.73; Buchfuhrer D., 2010, P 11 ACM EC, P33; Buchfuhrer D, 2010, PROC APPL MATH, V135, P518; Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182; Chambers CP, 2016, ECONOMETRIC SOC MONO; Chekuri C, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P201, DOI 10.1145/2688073.2688086; Chekuri C, 2011, ACM S THEORY COMPUT, P783; Chekuri C, 2011, ANN IEEE SYMP FOUND, P807, DOI 10.1109/FOCS.2011.34; Chen  Lin, 2018, P 35 INT C MACH LEAR, P813; Djolonga  J., 2014, ADV NEURAL INFORM PR; Dobzinski S., 2005, P 37 ANN ACM S THEOR, P610; Dobzinski S., 2008, FOCS; Dobzinski S, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1064, DOI 10.1145/1109557.1109675; Dobzinski S, 2011, ACM S THEORY COMPUT, P129; Dobzinski Shahar, 2012, P 13 ACM C EL COMM E, P405; Dughmi S, 2011, ACM S THEORY COMPUT, P149; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Feige U, 2006, ANN IEEE SYMP FOUND, P667; Feige U, 2015, AAAI CONF ARTIF INTE, P872; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman V, 2009, J MACH LEARN RES, V10, P163; Fisher M. L., 1978, ANAL APPROXIMATIONS; Goldman S. A., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory; Golovin  D., 2010, IPSN; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gomes  R., 2010, INT C MACH LEARN ICM; Gomez Rodriguez  M., 2011, ACM TKDD, V5; Hassani Hamed, 2017, ADV NEURAL INFORM PR, P5843; Hassidim A., 2017, SUBMODULAR OPTIMIZAT; Horel T, 2016, ADV NEUR IN, V29; Jackson J., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P42, DOI 10.1109/SFCS.1994.365706; Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589; Jegelka  S., 2011, INT C MACH LEARN ICM; Kempe D., 2003, ACM SIGKDD C KNOWL D; Khot S, 2005, LECT NOTES COMPUT SC, V3828, P92; Kohli P., 2013, IEEE C COMP VIS PATT; Krause A., 2007, INT C MACH LEARN ICM; Kumar R., 2013, SPAA; Lee J, 2009, ACM S THEORY COMPUT, P323; Leskovec J., 2007, ACM SIGKDD C KNOWL D; Li  Qiang, 2017, ADV NEURAL INFORM PR, V30, P3804; Lin H, 2011, ACL HLT; Lin  H., 2011, P INTERSPEECH; Lucier Brendan, 2013, Web and Internet Economics. 9th International Conference, WINE 2013. Proceedings: LNCS 8289, P347, DOI 10.1007/978-3-642-45046-4_28; Mirrokni V, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P70; Mokhtari A., 2018, C ART INT STAT, V84, P1886; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; Nemhauser G. L., 1978, MATH PROGRAMMING STU, V8; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Papadimitriou C, 2011, ACM S THEORY COMPUT, P119; Papadimitriou C, 2008, ANN IEEE SYMP FOUND, P250, DOI 10.1109/FOCS.2008.54; Qian  Chao, 2017, ADV NEURAL INFORM PR, V30, P3563; Schapira M, 2008, LECT NOTES COMPUT SC, V5385, P351, DOI 10.1007/978-3-540-92185-1_41; Shamir E., 1995, Computational Learning Theory. Second European Conference, EuroCOLT '95. Proceedings, P357; Streeter Matthew, 2009, ADV NEURAL INFORM PR; Vondrak J, 2008, ACM S THEORY COMPUT, P67	62	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300037
C	Hazan, E; Hu, W; Li, YZ; Li, ZY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hazan, Elad; Hu, Wei; Li, Yuanzhi; Li, Zhiyuan			Online Improper Learning with an Approximation Oracle	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	We study the following question: given an efficient approximation algorithm for an optimization problem, can we learn efficiently in the same setting? We give a formal affirmative answer to this question in the form of a reduction from online learning to offline approximate optimization using an efficient algorithm that guarantees near optimal regret. The algorithm is efficient in terms of the number of oracle calls to a given approximation oracle - it makes only logarithmically many such calls per iteration. This resolves an open question by Kalai and Vempala, and by Garber. Furthermore, our result applies to the more general improper learning problems.	[Hazan, Elad; Hu, Wei; Li, Zhiyuan] Princeton Univ, Princeton, NJ 08544 USA; [Hazan, Elad] Google AI Princeton, Princeton, NJ USA; [Li, Yuanzhi] Stanford Univ, Stanford, CA 94305 USA	Princeton University; Stanford University	Hazan, E (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.; Hazan, E (corresponding author), Google AI Princeton, Princeton, NJ USA.	ehazan@cs.princeton.edu; huwei@cs.princeton.edu; yuanzhil@stanford.edu; zhiyuanli@cs.princeton.edu	Li, Yuan/GXV-1310-2022; li, zhiyuan/HGD-9581-2022	Hazan, Elad/0000-0002-1566-3216				[Anonymous], 2016, FDN TRENDS IN OPTIMI; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Balcan M.-F., 2006, P 7 ACM C EL COMM, P29; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; Dudik M., 2016, ARXIV161101688; Fujita T, 2013, LECT NOTES ARTIF INT, V8139, P68; Garber Dan, 2017, ADV NEURAL INFORM PR, P627; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Hazan E, 2016, ACM S THEORY COMPUT, P128, DOI 10.1145/2897518.2897536; Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; PREKOPA A, 1973, ACTA SCI MATH, V34, P334; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000018
C	Hoffman, MD; Johnson, MJ; Tran, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hoffman, Matthew D.; Johnson, Matthew J.; Tran, Dustin			Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python-embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies.(1)	[Hoffman, Matthew D.] Google AI, Mountain View, CA 94043 USA; [Johnson, Matthew J.; Tran, Dustin] Google Brain, Mountain View, CA USA	Google Incorporated	Hoffman, MD (corresponding author), Google AI, Mountain View, CA 94043 USA.	mhoffman@google.com; mattjj@google.com; trandustin@google.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Baader F., 1999, TERM REWRITING ALL; Blei D.M., 2005, NIPS; Carette Jacques, 2016, Practical Aspects of Declarative Languages. 18th International Symposium, PADL 2016. Proceedings: LNCS 9585, P135, DOI 10.1007/978-3-319-28228-2_9; Cook SR, 2006, J COMPUT GRAPH STAT, V15, P675, DOI 10.1198/106186006X136976; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Diehl S., 2013, PYREWRITE PYTHON TER; Gehr T, 2016, LECT NOTES COMPUT SC, V9779, P62, DOI 10.1007/978-3-319-41528-4_4; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Goodman N. D., 2014, DESIGN IMPLEMENTATIO; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jaakkola T.S., 1996, 6 INT WORKSHOP ARTIF, P4; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Khan M. E, 2016, C UNC ART INT UAI; Khan M. E., 2017, ARTIFICIAL INTELLIGE; Khan M. E., 2015, ADV NEURAL INFORM PR, P3402; Koller D., 2009, PROBABILISTIC GRAPHI; Kucukelbir Alp, 2016, ARXIV160300788; Maclaurin D, 2014, AUTOGRAD REVERSE MOD; Murray L. M, 2018, ARTIFICIAL INTELLIGE; Narayanan Praveen, 2016, Functional and Logic Programming. 13th International Symposium, FLOPS 2016. Proceedings: LNCS 9613, P62, DOI 10.1007/978-3-319-29604-3_5; Narayanan P., 2017, P ACM PROGRAMMING LA, V1, P11; Radul A., 2013, RULES EXTENSIBLE PAT; Rozenberg G., 1997, HDB GRAPH GRAMMARS C, V1; Spiegelhalter D.J., 1995, BUGS BAYESIAN INFERE; Sussman G. J, 2018, SCMUTILS; Tran D., 2018, NEURAL INFORM PROCES; Tristan J.-B, 2014, NEURAL INFORM PROCES; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Winn J, 2005, J MACH LEARN RES, V6, P661	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005031
C	Hu, SB; Chen, ZT; Nia, VP; Chan, LW; Geng, YH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hu, Shoubo; Chen, Zhitang; Nia, Vahid Partovi; Chan, Laiwan; Geng, Yanhui			Causal Inference and Mechanism Clustering of A Mixture of Additive Noise Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The inference of the causal relationship between a pair of observed variables is a fundamental problem in science, and most existing approaches are based on one single causal model. In practice, however, observations are often collected from multiple sources with heterogeneous causal models due to certain uncontrollable factors, which renders causal analysis results obtained by a single model skeptical. In this paper, we generalize the Additive Noise Model (ANM) to a mixture model, which consists of a finite number of ANMs, and provide the condition of its causal identifiability. To conduct model estimation, we propose Gaussian Process Partially Observable Model (GPPOM), and incorporate independence enforcement into it to learn latent parameter associated with each observation. Causal inference and clustering according to the underlying generating mechanisms of the mixture model are addressed in this work. Experiments on synthetic and real data demonstrate the effectiveness of our proposed approach.	[Hu, Shoubo; Chan, Laiwan] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Chen, Zhitang; Nia, Vahid Partovi] Huawei Noahs Ark Lab, Hong Kong, Peoples R China; [Geng, Yanhui] Huawei Montreal Res Ctr, Montreal, PQ, Canada	Chinese University of Hong Kong; Huawei Technologies; Huawei Technologies	Hu, SB (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	sbhu@cse.cuhk.edu.hk; chenzhitang2@huawei.com; vahid.partovinia@huawei.com; lwchan@cse.cuhk.edu.hk; geng.yanhui@huawei.com			Hong Kong Research Grants Council	Hong Kong Research Grants Council(Hong Kong Research Grants Council)	This work is partially supported by the Hong Kong Research Grants Council.	Daniusis P., 2012, ARXIV12033475; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Gretton A, 2005, AISTATS, P112; Hoyer P. O., 2009, ADV NEURAL INFORM PR, V21, P689; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Janzing D, 2010, IEEE T INFORM THEORY, V56, P5168, DOI 10.1109/TIT.2010.2060095; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Liu FR, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2700477; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5; Mooij JM, 2016, J MACH LEARN RES, V17; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Williams CKI, 1998, NATO ADV SCI I D-BEH, V89, P599; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647; Zhang K., 2015, ARXIV150908056	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305024
C	Nguyen, HL; Zakynthinou, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huy Le Nguyen; Zakynthinou, Lydia			Improved Algorithms for Collaborative PAC Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODEL	We study a recent model of collaborative PAC learning where k players with k different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small error on all tasks, there is a collaborative algorithm that finds a single classifier for all tasks and has O((ln(k))(2)) times the worst-case sample complexity for learning a single task. In this work, we design new algorithms for both the realizable and the non-realizable setting, having sample complexity only O(ln(k)) times the worst-case sample complexity for learning a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters are even better than previous algorithms that are allowed to output different classifiers for different tasks.	[Huy Le Nguyen; Zakynthinou, Lydia] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA	Northeastern University	Nguyen, HL (corresponding author), Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.	hu.nguyen@northeastern.edu; zakynthinou.1@northeastern.edu	Nguyen, Huy/AFN-7027-2022; Nguyen, Huy/GWQ-6433-2022		NSF CAREER [1750716]; Northeastern University's College of Computer and Information Science	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Northeastern University's College of Computer and Information Science	We thank the anonymous reviewers for their helpful remarks and for pointing us to the idea of slightly modifying the algorithms in the non-realizable setting so that the optimal error is unknown. This work was partially supported by NSF CAREER 1750716 and a Graduate fellowship from Northeastern University's College of Computer and Information Science.	[Anonymous], 2018, ARXIVABS180904684; Balcan Maria-Florina, 2012, P 25 C COMP LEARN TH; Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Blum A., 2017, ADV NEURAL INFORM PR, P2389; Dekel O., 2011, P 28 INT C MACH LEAR, P713; Finn C, 2017, PR MACH LEARN RES, V70; Koufogiannakis C, 2014, ALGORITHMICA, V70, P648, DOI 10.1007/s00453-013-9771-6; Mansour Y., 2009, P COLT, P19; Mitzenmacher M., 2017, PROBABILITY COMPUTIN; Rostamizadeh A., 2009, ADV NEURAL INFORM PR, P1041; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Wang JW, 2016, AER ADV ENG RES, V79, P751	14	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002020
C	Jaini, P; Poupart, P; Yu, YL		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jaini, Priyank; Poupart, Pascal; Yu, Yaoliang			Deep Homogeneous Mixture Models: Representation, Separation, and Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE	At their core, many unsupervised learning models provide a compact representation of homogeneous density mixtures, but their similarities and differences are not always clearly understood. In this work, we formally establish the relationships among latent tree graphical models (including special cases such as hidden Markov models and tensorial mixture models), hierarchical tensor formats and sum-product networks. Based on this connection, we then give a unified treatment of exponential separation in exact representation size between deep mixture architectures and shallow ones. In contrast, for approximate representation, we show that the conditional gradient algorithm can approximate any homogeneous mixture within epsilon accuracy by combining O(1/epsilon(2)) "shallow" architectures, where the hidden constant may decrease (exponentially) with respect to the depth. Our experiments on both synthetic and real datasets confirm the benefits of depth in density estimation.	[Jaini, Priyank; Yu, Yaoliang] Univ Waterloo, Dept Comp Sci, Waterloo, ON, Canada; [Jaini, Priyank; Yu, Yaoliang] Univ Waterloo, Waterloo AI Inst, Waterloo, ON, Canada; [Poupart, Pascal] Univ Waterloo, Vector Inst, Waterloo, ON, Canada; [Poupart, Pascal] Waterloo AI Inst, Waterloo, ON, Canada	University of Waterloo; University of Waterloo; University of Waterloo; University of Waterloo	Jaini, P (corresponding author), Univ Waterloo, Dept Comp Sci, Waterloo, ON, Canada.; Jaini, P (corresponding author), Univ Waterloo, Waterloo AI Inst, Waterloo, ON, Canada.	pjaini@uwaterloo.ca; ppoupart@uwaterloo.ca; yaoliang.yu@uwaterloo.ca			NSERC discovery program	NSERC discovery program(Natural Sciences and Engineering Research Council of Canada (NSERC))	The authors gratefully acknowledge support from the NSERC discovery program.	Alon N, 2009, COMB PROBAB COMPUT, V18, P3, DOI 10.1017/S0963548307008917; Anandkumar A., 2012, ADV NEURAL INFORM PR; [Anonymous], ARXIV161103974; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Bengio, 2011, ADV NEURAL INFORM PR, P666, DOI DOI 10.5555/2986459.2986534; Caron R., 2005, TECHNICAL REPORT; Choi MJ, 2011, J MACH LEARN RES, V12, P1771; Cohen N., 2016, C LEARNING THEORY, V49, P698; Cohen N, 2016, PR MACH LEARN RES, V48; Cohen Nadav, 2017, ARXIV170502302V4; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Dinh Laurent, 2014, ARXIV14108516; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Hackbusch W., 2012, TENSOR SPACES NUMERI; Ishteva M, 2015, LECT NOTES COMPUT SC, V9237, P49, DOI 10.1007/978-3-319-22482-4_6; Le Song, 2013, ICML; LeCun Y, 2004, PROC CVPR IEEE, P97; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li JQ, 2000, ADV NEUR IN, V12, P279; Martens James, 2014, NEW INSIGHTS PERSPEC; McLachlan GJ, 2004, FINITE MIXTURE MODEL, DOI [10.1002/0471721182, DOI 10.1002/0471721182]; Meila M, 2001, J MACH LEARN RES, V1, P1, DOI 10.1162/153244301753344605; Mourad R, 2013, J ARTIF INTELL RES, V47, P157, DOI 10.1613/jair.3879; Peharz R, 2017, IEEE T PATTERN ANAL, V39, P2030, DOI 10.1109/TPAMI.2016.2618381; Poon H., 2011, P 27 C UNC ART INT, P337, DOI DOI 10.1109/ICCVW.2011; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Sharir Or, 2018, ARXIV161004167V5; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Zhao H, 2015, PR MACH LEARN RES, V37, P116	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001066
C	Jalalzai, H; Clemencon, S; Sabourin, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jalalzai, Hamid; Clemencon, Stephan; Sabourin, Anne			On Binary Classification in Extreme Regions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In pattern recognition, a random label Y is to be predicted based upon observing a random vector X valued in R(d )with d >= 1 by means of a classification rule with minimum probability of error. In a wide variety of applications, ranging from finance/insurance to environmental sciences through teletraffic data analysis for instance, extreme (i.e. very large) observations X are of crucial importance, while contributing in a negligible manner to the (empirical) error however, simply because of their rarity. As a consequence, empirical risk minimizers generally perform very poorly in extreme regions. It is the purpose of this paper to develop a general framework for classification in the extremes. Precisely, under non-parametric heavy-tail assumptions for the class distributions, we prove that a natural and asymptotic notion of risk, accounting for predictive performance in extreme regions of the input space, can be defined and show that minimizers of an empirical version of a non-asymptotic approximant of this dedicated risk, based on a fraction of the largest observations, lead to classification rules with good generalization capacity, by means of maximal deviation inequalities in low probability regions. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.	[Jalalzai, Hamid; Clemencon, Stephan; Sabourin, Anne] Univ Paris Saclay, LTCI Telecom ParisTech, F-75013 Paris, France	UDICE-French Research Universities; Universite Paris Saclay	Jalalzai, H (corresponding author), Univ Paris Saclay, LTCI Telecom ParisTech, F-75013 Paris, France.	hamid.jalalzai@telecom-paristech.fr; stephan.clemencon@telecom-paristech.fr; anne.sabourin@telecom-paristech.fr		Sabourin, Anne/0000-0002-5096-9157				Brownlees C, 2015, ANN STAT, V43, P2507, DOI 10.1214/15-AOS1350; Cai JJ, 2011, ANN STAT, V39, P1803, DOI 10.1214/11-AOS891; Carpentier A, 2014, ADV NEUR IN, V27; DEHAAN L, 1987, STOCH PROC APPL, V25, P83, DOI 10.1016/0304-4149(87)90191-8; Devroye L., 1996, APPL MATH STOCHASTIC; Goix N, 2016, JMLR WORKSH CONF PRO, V51, P75; Goix N, 2017, J MULTIVARIATE ANAL, V161, P12, DOI 10.1016/j.jmva.2017.06.010; Mendelson S, 2018, PROBAB THEORY REL, V171, P459, DOI 10.1007/s00440-017-0784-y; NAKAI K, 1992, GENOMICS, V14, P897, DOI 10.1016/S0888-7543(05)80111-9; Ohannessian M. I., 2012, J MACH LEARN RES TRA, P21; Resnick S. I., 2008, SPRINGER SERIES OPER; Roos T., 2006, ADV NEURAL INFORM PR, P1129; Stephenson A., 2003, EXTREMES, V6, P49, DOI DOI 10.1023/A:1026277229992	13	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303012
C	Jamieson, K; Jain, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jamieson, Kevin; Jain, Lalit			A Bandit Approach to Multiple Testing with False Discovery Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTIARMED BANDIT; COMPLEXITY; BOUNDS	We propose an adaptive sampling approach for multiple testing which aims to maximize statistical power while ensuring anytime false discovery control. We consider n distributions whose means are partitioned by whether they are below or equal to a baseline (nulls), versus above the baseline (actual positives). In addition, each distribution can be sequentially and repeatedly sampled. Inspired by the multi-armed bandit literature, we provide an algorithm that takes as few samples as possible to exceed a target true positive proportion (i.e. proportion of actual positives discovered) while giving anytime control of the false discovery proportion (nulls predicted as actual positives). Our sample complexity results match known information theoretic lower bounds and through simulations we show a substantial performance improvement over uniform sampling and an adaptive elimination style algorithm. Given the simplicity of the approach, and its sample efficiency, the method has promise for wide adoption in the biological sciences, clinical testing for drug discovery, and online A/B/n testing problems.	[Jamieson, Kevin; Jain, Lalit] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA; [Jamieson, Kevin] Optimizely, San Francisco, CA 94105 USA	University of Washington; University of Washington Seattle	Jamieson, K (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.; Jamieson, K (corresponding author), Optimizely, San Francisco, CA 94105 USA.	jamieson@cs.washington.edu; lalitj@cs.washington.edu						Balsubramani A., 2014, ARXIV E PRINTS; Benjamini Y, 2001, ANN STAT, V29, P1165; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Cao T., 2017, ARXIV171108018; Castro RM, 2014, BERNOULLI, V20, P2217, DOI 10.3150/13-BEJ555; Chen L., 2017, P 30 COLT, V65, P535; Chen LJ, 2017, PR MACH LEARN RES, V54, P101; Chen S., 2014, P ADV NEUR INF PROC, P379; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Hao LH, 2008, NATURE, V454, P890, DOI 10.1038/nature07151; Hartman P, 1941, AM J MATH, V63, P169, DOI 10.2307/2371287; Haupt J, 2011, IEEE T INFORM THEORY, V57, P6222, DOI 10.1109/TIT.2011.2162269; Heckel Reinhard, 2018, P INT C ART INT STAT, P1057; Jamieson K., 2014, C LEARN THEOR, P423; Johari Ramesh, 2015, ARXIV151204922, V2015; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Kano Hideaki, 2017, ARXIV171006360; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Kaufmann E, 2016, J MACH LEARN RES, V17; Locatelli A, 2016, PR MACH LEARN RES, V48; Malloy ML, 2014, IEEE T INFORM THEORY, V60, P7862, DOI 10.1109/TIT.2014.2363846; Optimizely, 2018, ACC EXP MACH LEARN; Rabinovich Maxim, 2017, ARXIV170505391; Raginsky M., 2011, NIPS, P1026; Ramdas A., 2017, ARXIV E PRINTS; Rocklin GJ, 2017, SCIENCE, V357, P168, DOI 10.1126/science.aan0693; Simchowitz M., 2017, C LEARN THEOR, V65, P1794; Tanczos Ervin, 2017, ADV NEURAL INFORM PR, V30, P5896; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Yang F, 2017, ADV NEURAL INFORM PR, V30, P5959	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303064
C	Jetley, S; Lord, NA; Torr, PHS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jetley, Saumya; Lord, Nicholas A.; Torr, Philip H. S.			With Friends Like These, Who Needs Adversaries?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORKS	The vulnerability of deep image classification networks to adversarial attack is now well known, but less well understood. Via a novel experimental analysis, we illustrate some facts about deep convolutional networks for image classification that shed new light on their behaviour and how it connects to the problem of adversaries. In short, the celebrated performance of these networks and their vulnerability to adversarial attack are simply two sides of the same coin: the input image-space directions along which the networks are most vulnerable to attack are the same directions which they use to achieve their classification performance in the first place. We develop this result in two main steps. The first uncovers the fact that classes tend to be associated with specific image-space directions. This is shown by an examination of the class-score outputs of nets as functions of 1D movements along these directions. This provides a novel perspective on the existence of universal adversarial perturbations. The second is a clear demonstration of the tight coupling between classification performance and vulnerability to adversarial attack within the spaces spanned by these directions. Thus, our analysis resolves the apparent contradiction between accuracy and vulnerability. It provides a new perspective on much of the prior art and reveals profound implications for efforts to construct neural nets that are both accurate and robust to adversarial attack.(1)	[Jetley, Saumya; Lord, Nicholas A.; Torr, Philip H. S.] Univ Oxford, Dept Engn Sci, Oxford, England; [Lord, Nicholas A.; Torr, Philip H. S.] FiveAI Ltd, Oxford Res Grp, Oxford, England	University of Oxford	Jetley, S (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	sjetley@robots.ox.ac.uk; nicklord@robots.ox.ac.uk; phst@robots.ox.ac.uk			ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC [Seebibyte EP/M013774/1]; EPSRC/MURI [EP/N019474/1]	ERC(European Research Council (ERC)European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We would also like to acknowledge the Royal Academy of Engineering, FiveAI, and extend our thanks to Seyed-Mohsen Moosavi-Dezfooli for providing his research code for curvature analysis of decision boundaries of DCNs.	Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], INT C LEARN REPR; [Anonymous], 2017, 2017 COMP ADV ATT DE; [Anonymous], 2018, INT C LEARN REPR; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Das N, 2017, ABS170502900 CORR; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; Fawzi A, 2017, IEEE SIGNAL PROC MAG, V34, P50, DOI 10.1109/MSP.2017.2740965; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Fawzi Alhussein, 2017, ARXIV170509552; Gao J, 2017, INT C LEARN REPR; Goodfellow I. J., 2015, P ICLR; Lu JJ, 2017, IEEE I CONF COMP VIS, P446, DOI 10.1109/ICCV.2017.56; Madry Aleksander, 2018, ICLR; Maharaj A. V., 2015, IMPROVING ADVERSARIA; Metzen J.H., 2017, ICLR; Moosavi-Dezfooli S. M, 2016, P 2016 IEEE C COMP V; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2018, INT C LEARN REPR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sabour S, 2016, INT C LEARN REPR; Simonyan K., 2013, DEEP INSIDE CONVOLUT; Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Tanay T., 2016, ARXIV PREPRINT ARXIV; Wang B, 2016, ARXIV161200334; Xie C, 2017, ABS171101991 CORR; Zhao Q, 2016, ABS160305145 CORR	28	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005034
C	Jia, B; Ray, S; Safavi, S; Bento, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jia, Bei; Ray, Surjyendu; Safavi, Sam; Bento, Jose			Efficient Projection onto the Perfect Phylogeny Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE; HETEROGENEITY; ALGORITHM; SIMPLEX; HISTORY	Several algorithms build on the perfect phylogeny model to infer evolutionary trees. This problem is particularly hard when evolutionary trees are inferred from the fraction of genomes that have mutations in different positions, across different samples. Existing algorithms might do extensive searches over the space of possible trees. At the center of these algorithms is a projection problem that assigns a fitness cost to phylogenetic trees. In order to perform a wide search over the space of the trees, it is critical to solve this projection problem fast. In this paper, we use Moreau's decomposition for proximal operators, and a tree reduction scheme, to develop a new algorithm to compute this projection. Our algorithm terminates with an exact solution in a finite number of steps, and is extremely fast. In particular, it can search over all evolutionary trees with fewer than 11 nodes, a size relevant for several biological problems (more than 2 billion trees) in about 2 hours.	[Jia, Bei; Ray, Surjyendu; Safavi, Sam; Bento, Jose] Boston Coll, Chestnut Hill, MA 02167 USA; [Jia, Bei] Element AI, Montreal, PQ, Canada	Boston College	Jia, B (corresponding author), Boston Coll, Chestnut Hill, MA 02167 USA.; Jia, B (corresponding author), Element AI, Montreal, PQ, Canada.	jiabe@bc.edu; raysc@bc.edu; safavisa@bc.edu; jose.bento@bc.edu			NVIDIA hardware grant [NIH/1U01AI124302, NSF/IIS-1741129]	NVIDIA hardware grant	This work was partially funded by NIH/1U01AI124302, NSF/IIS-1741129, and a NVIDIA hardware grant.	[Anonymous], 2009, 26 ANN INT C MACH LE, DOI DOI 10.1145/1553374.1553459; Bento J., 2013, ADV NEURAL INFORM PR, P521; Bento J, 2015, AAAI CONF ARTIF INTE, P3657; Bonizzoni Paola, 2014, DISCRETE TOPOLOGICAL, P67, DOI DOI 10.1007/978-3-642-40193-0_4.; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6; Deshwar AG, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0602-8; Ding ZH, 2006, J COMPUT BIOL, V13, P522, DOI 10.1089/cmb.2006.13.522; Duchi J., 2008, PROC 25 INT C MACH L, P272; El-Kebir M, 2016, ARXIV160402605; El-Kebir M, 2016, CELL SYST, V3, P43, DOI 10.1016/j.cels.2016.07.004; El-Kebir M, 2015, BIOINFORMATICS, V31, P62, DOI 10.1093/bioinformatics/btv261; Fernandez-Baca D, 2001, COMB OPT (SER), V11, P203; Franca G., 2017, ARXIV171000889; Franca G, 2016, IEEE INT SYMP INFO, P2104, DOI 10.1109/ISIT.2016.7541670; Garey M.R., 2002, COMPUTERS INTRACTABI, V29; Ghahramani Z, 2010, ADV NEURAL INF PROCE, V2010, P19; Gong PH, 2011, NEUROCOMPUTING, V74, P2754, DOI 10.1016/j.neucom.2011.02.019; GUSFIELD D, 1991, NETWORKS, V21, P19, DOI 10.1002/net.3230210104; Hajirasouliha I, 2014, BIOINFORMATICS, V30, P78, DOI 10.1093/bioinformatics/btu284; Hao N, 2016, IEEE SYM PARA DISTR, P835, DOI 10.1109/IPDPSW.2016.162; HUDSON RR, 1983, THEOR POPUL BIOL, V23, P183, DOI 10.1016/0040-5809(83)90013-8; Jiang YC, 2016, P NATL ACAD SCI USA, V113, pE5528, DOI 10.1073/pnas.1522203113; Jiao W, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-35; KIMURA M, 1969, GENETICS, V61, P893; Malikic S, 2015, BIOINFORMATICS, V31, P1349, DOI 10.1093/bioinformatics/btv003; Mathy Charles JM, SPARTA FAST GLOBAL P; MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486; MOREAU JJ, 1962, CR HEBD ACAD SCI, V255, P238; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Popic V, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0647-8; Prufer H., 1918, ARCH MATH PHYS, V27, P742; Satas G, 2017, BIOINFORMATICS, V33, pI152, DOI 10.1093/bioinformatics/btx270; Schuh A, 2012, BLOOD, V120, P4191, DOI 10.1182/blood-2012-05-433540; Yang Laurence, 2018, ARXIV180704245; Zoran Daniel, 2014, ADV NEURAL INFORM PR, P226	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304014
C	Jiang, F; Yin, GS; Francesca, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Fei; Yin, Guosheng; Francesca, Dominici			Bayesian Model Selection Approach to Boundary Detection with Non-Local Priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CHANGE-POINT	Based on non-local prior distributions, we propose a Bayesian model selection (BMS) procedure for boundary detection in a sequence of data with multiple systematic mean changes. The BMS method can effectively suppress the non-boundary spike points with large instantaneous changes. We speed up the algorithm by reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and locations of the change points under various prior distributions. Extensive simulation studies are conducted to compare the BMS with existing methods, and our approach is illustrated with application to the magnetic resonance imaging guided radiation therapy data.	[Jiang, Fei; Yin, Guosheng] Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong, Peoples R China; [Francesca, Dominici] Harvard Univ, Harvard TH Chan Sch Publ Hlth, Cambridge, MA 02138 USA	University of Hong Kong; Harvard University; Harvard T.H. Chan School of Public Health	Jiang, F (corresponding author), Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong, Peoples R China.	feijiang@hku.hk; gyin@hku.hk; fdominic@hsph.harvard.edu	jiang, fei/V-4955-2019; Yin, Guosheng/D-3214-2009	Yin, Guosheng/0000-0003-3276-1392	Research Grants Council of Hong Kong [27304117, 17326316]	Research Grants Council of Hong Kong(Hong Kong Research Grants Council)	The authors would like to thank Dr. Zhou Shouhao from Department of Biostatistics, M.D. Anderson Cancer Center for providing the data. The research is partially supported by grants from the Research Grants Council of Hong Kong (grant number 27304117 for Jiang and 17326316 for Yin).	Baranowski R., 2016, ARXIV160900293; Bertolino F, 2000, J ROY STAT SOC D-STA, V49, P503; Conigliani C, 2000, CAN J STAT, V28, P343, DOI 10.2307/3315983; De Santis F, 2001, J STAT PLAN INFER, V97, P305, DOI 10.1016/S0378-3758(00)00240-8; Du C, 2016, J AM STAT ASSOC, V111, P314, DOI 10.1080/01621459.2015.1006365; Eiauer P, 1978, TECHNOMETRICS, V20, P431; Fryzlewicz P, 2014, ANN STAT, V42, P2243, DOI 10.1214/14-AOS1245; Glaz J., 2001, SCAN STAT; Jeffreys H., 1998, THEORY PROBABILITY; Jiang Fei, 2018, BAYESIAN MODEL SELEC; Johnson VE, 2010, J R STAT SOC B, V72, P143, DOI 10.1111/j.1467-9868.2009.00730.x; Killick R, 2012, J AM STAT ASSOC, V107, P1590, DOI 10.1080/01621459.2012.737745; KIRCH C., 2014, PREPRINT; Lavielle M, 2000, BERNOULLI, V6, P845, DOI 10.2307/3318759; Preuss P, 2015, J AM STAT ASSOC, V110, P654, DOI 10.1080/01621459.2014.920613; Walker SG, 2004, STAT SCI, V19, P111, DOI 10.1214/088342304000000134; Yau CY, 2016, J R STAT SOC B, V78, P895, DOI 10.1111/rssb.12139	19	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302002
C	Jiang, N; Kulesza, A; Singh, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Nan; Kulesza, Alex; Singh, Satinder			Completing State Representations using Spectral Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A central problem in dynamical system modeling is state discovery-that is, finding a compact summary of the past that captures the information needed to predict the future. Predictive State Representations (PSRs) enable clever spectral methods for state discovery; however, while consistent in the limit of infinite data, these methods often suffer from poor performance in the low data regime. In this paper we develop a novel algorithm for incorporating domain knowledge, in the form of an imperfect state representation, as side information to speed spectral learning for PSRs. We prove theoretical results characterizing the relevance of a user-provided state representation, and design spectral algorithms that can take advantage of a relevant representation. Our algorithm utilizes principal angles to extract the relevant components of the representation, and is robust to mis-specification. Empirical evaluation on synthetic HMMs, an aircraft identification domain, and a gene splice dataset shows that, even with weak domain knowledge, the algorithm can significantly outperform standard PSR learning.	[Jiang, Nan] UIUC, Urbana, IL 61801 USA; [Kulesza, Alex] Google Res, New York, NY USA; [Singh, Satinder] Univ Michigan, Ann Arbor, MI 48109 USA	University of Illinois System; University of Illinois Urbana-Champaign; Google Incorporated; University of Michigan System; University of Michigan	Jiang, N (corresponding author), UIUC, Urbana, IL 61801 USA.	nanjiang@illinois.edu; kulesza@google.com; baveja@umich.edu			NSF [IIS 1319365]	NSF(National Science Foundation (NSF))	This work was supported by NSF grant IIS 1319365. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsors.	BJORCK A, 1973, MATH COMPUT, V27, P579, DOI 10.2307/2005662; Boots Byron, 2010, P 9 INT C AUT AG MUL, P1369; Cassandra AR, 1998, EXACT APPROXIMATE AL; Denis F, 2014, PR MACH LEARN RES, V32; Dheeru D., 2019, UCI MACHINE LEARNING; Hefny A., 2015, ADV NEURAL INFORM PR, V28, P1963; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; James MR, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P734; James Michael R., 2004, P 21 INT C MACH LEAR, P53; Jiang N, 2016, AAAI CONF ARTIF INTE, P1709; Kulesza A, 2015, JMLR WORKSH CONF PRO, V38, P517; Kulesza A, 2015, AAAI CONF ARTIF INTE, P2715; Littman M. L., 2001, NIPS; Liu YL, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1259; Oh J, 2016, PR MACH LEARN RES, V48; Ong Sylvie CW, 2013, AAAI; Rosencrantz M., 2004, P 21 INT C MACH LEAR; Shaban A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P792; Siddiqi S., 2010, J MACHINE LEARNING R, V9, P741; Singh S., 2004, P 20 C UNCERTAINTY A, P512	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304035
C	Johnson, DD; Gorelik, D; Mawhorter, R; Suver, K; Gu, WQ; Xing, S; Gabriel, C; Sankhagowit, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Johnson, Daniel D.; Gorelik, Daniel; Mawhorter, Ross; Suver, Kyle; Gu, Weiqing; Xing, Steven; Gabriel, Cody; Sankhagowit, Peter			Latent Gaussian Activity Propagation: Using Smoothness and Structure to Separate and Localize Sounds in Large Noisy Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BLIND SEPARATION; SIGNALS	We present an approach for simultaneously separating and localizing multiple sound sources using recorded microphone data. Inspired by topic models, our approach is based on a probabilistic model of inter-microphone phase differences, and poses separation and localization as a Bayesian inference problem. We assume sound activity is locally smooth across time, frequency, and location, and use the known position of the microphones to obtain a consistent separation. We compare the performance of our method against existing algorithms on simulated anechoic voice data and find that it obtains high performance across a variety of input conditions.	[Johnson, Daniel D.; Gorelik, Daniel; Mawhorter, Ross; Suver, Kyle; Gu, Weiqing] Harvey Mudd Coll, Dept Math, Claremont, CA 91711 USA; [Xing, Steven; Gabriel, Cody; Sankhagowit, Peter] Intel Corp, Hillsboro, OR 97124 USA	Claremont Colleges; Harvey Mudd College; Intel Corporation	Johnson, DD (corresponding author), Harvey Mudd Coll, Dept Math, Claremont, CA 91711 USA.	ddjohnson@hmc.edu; dgorelik@hmc.edu; rmawhorter@hmc.edu; ksuver@hmc.edu; gu@hmc.edu; steven.xing@intel.com; cody.gabriel@intel.com; peter.sankhagowit@intel.com		Johnson, Daniel D./0000-0002-2594-8442				Aarabi P, 2002, IEEE T SYST MAN CY C, V32, P474, DOI 10.1109/TSMCB.2002.804369; Bagchi D, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P496, DOI 10.1109/ASRU.2015.7404836; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Dietz L., 2010, TECH REP; Dorfan Y, 2015, EUR SIGNAL PR CONF, P1256, DOI 10.1109/EUSIPCO.2015.7362585; Emiya V, 2011, IEEE T AUDIO SPEECH, V19, P2046, DOI 10.1109/TASL.2011.2109381; Hyvarinen A, 2004, INDEPENDENT COMPONEN, V46; Jourjine A, 2000, INT CONF ACOUST SPEE, P2985, DOI 10.1109/ICASSP.2000.861162; Lewis Jerad, 2012, ANALOG DEVICES; Mandel MI, 2015, EUR SIGNAL PR CONF, P2028, DOI 10.1109/EUSIPCO.2015.7362740; Mandel Michael I, 2009, IEEE T AUDIO SPEECH, V17; Oldfield R, 2015, MULTIMED TOOLS APPL, V74, P2717, DOI 10.1007/s11042-013-1472-2; Rickard S, 2007, SIGNALS COMMUN TECHN, P217, DOI 10.1007/978-1-4020-6479-1_8; Srinivasan S, 2006, SPEECH COMMUN, V48, P1486, DOI 10.1016/j.specom.2006.09.003; Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005; Virtanen T, 2007, IEEE T AUDIO SPEECH, V15, P1066, DOI 10.1109/TASL.2006.885253; Wang D., 2017, ARXIV170807524; Yeredor A., 2001, P ICA, P522	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303046
C	Koide, S; Kawano, K; Kutsuna, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Koide, Satoshi; Kawano, Keisuke; Kutsuna, Takuro			Neural Edit Operations for Biological Sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORKS; SEARCH	The evolution of biological sequences, such as proteins or DNAs, is driven by the three basic edit operations: substitution, insertion, and deletion. Motivated by the recent progress of neural network models for biological tasks, we implement two neural network architectures that can treat such edit operations. The first proposal is the edit invariant neural networks, based on differentiable Needleman-Wunsch algorithms. The second is the use of deep CNNs with concatenations. Our analysis shows that CNNs can recognize regular expressions without Kleene star, and that deeper CNNs can recognize more complex regular expressions including the insertion/deletion of characters. The experimental results for the protein secondary structure prediction task suggest the importance of insertion/deletion. The test accuracy on the widely-used CB513 dataset is 71.5%, which is 1.2-points better than the current best result on non-ensemble models.	[Koide, Satoshi; Kawano, Keisuke; Kutsuna, Takuro] Toyota Cent R&D Labs, Nagakute, Aichi, Japan	Toyota Central R&D Labs Inc	Koide, S (corresponding author), Toyota Cent R&D Labs, Nagakute, Aichi, Japan.	koide@mosk.tytlabs.co.jp; kawano@mosk.tytlabs.co.jp; kutsuna@mosk.tytlabs.co.jp	Kutsuna, Takuro/Q-5626-2019	Kutsuna, Takuro/0000-0001-6965-1512				ALTSCHUL SF, 1990, J MOL BIOL, V215, P403, DOI 10.1006/jmbi.1990.9999; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Busia  A., 2017, ABS170203865 CORR; Cuturi M, 2017, PR MACH LEARN RES, V70; Di Lena P, 2012, BIOINFORMATICS, V28, P2449, DOI 10.1093/bioinformatics/bts475; Forcada ML, 2001, LECT NOTES ARTIF INT, V2036, P480; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Gers FA, 2001, IEEE T NEURAL NETWOR, V12, P1333, DOI 10.1109/72.963769; Golkov  V., 2017, P NIPS 16, P4222; Graves A., 2006, P INT C MACH LEARN I; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kelley DR, 2016, GENOME RES, V26, P990, DOI 10.1101/gr.200535.115; Li Z., 2016, PROTEIN SECONDARY ST, P2560; Lin  Z., 2016, P AAAI; Magnan CN, 2014, BIOINFORMATICS, V30, P2592, DOI 10.1093/bioinformatics/btu352; Minsky M.L., 1967, COMPUTATION; NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4; Qi YF, 2012, PLOS ONE, V7, DOI [10.1371/journal.pone.0031539, 10.1371/journal.pone.0033859]; Saigo H, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-246; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5; Soren Kaae Sonderby O. W., 2016, ARXIV14127828; Wang S, 2016, SCI REP-UK, V6, DOI 10.1038/srep18962; Worrall D. E, 2017, P IEEE C COMP VIS PA, P5028; Zhou J, 2014, INT CONF MACH LEARN, P71, DOI 10.1109/ICMLC.2014.7009094	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305001
C	Kong, WH; Valiant, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kong, Weihao; Valiant, Gregory			Estimating Learnability in the Sublinear Data Regime	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SIGNAL-TO-NOISE; EIGENVALUES	We consider the problem of estimating how well a model class is capable of fitting a distribution of labeled data. We show that it is possible to accurately estimate this "learnability" even when given an amount of data that is too small to reliably learn any accurate model. Our first result applies to the setting where the data is drawn from a d-dimensional distribution with isotropic covariance, and the label of each datapoint is an arbitrary noisy function of the datapoint. In this setting, we show that with O(root d) samples, one can accurately estimate the fraction of the variance of the label that can be explained via the best linear function of the data. For comparison, even if the labels are noiseless linear functions of the data, a sample size linear in the dimension, d, is required to learn any function correlated with the underlying model. Our estimation approach also applies to the setting where the data distribution has an (unknown) arbitrary covariance matrix, allowing these techniques to be applied to settings where the model class consists of a linear function applied to a nonlinear embedding of the data. In this setting we give a consistent estimator of the fraction of explainable variance that uses o(d) samples. Finally, our techniques also extend to the setting of binary classification, where we obtain analogous results under the logistic model, for estimating the classification accuracy of the best linear classifier. We demonstrate the practical viability of our approaches on synthetic and real data. This ability to estimate the explanatory value of a set of features (or dataset), even in the regime in which there is too little data to realize that explanatory value, may be relevant to the scientific and industrial settings for which data collection is expensive and there are many potentially relevant feature sets that could be collected.	[Kong, Weihao; Valiant, Gregory] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Kong, WH (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	whkong@stanford.edu; gvaliant@cs.stanford.edu			NSF [CCF-1704417]; ONR Young Investigator Award; Sloan Research Fellowship	NSF(National Science Foundation (NSF)); ONR Young Investigator Award; Sloan Research Fellowship(Alfred P. Sloan Foundation)	This work was supported by NSF award CCF-1704417, an ONR Young Investigator Award, and a Sloan Research Fellowship.	BAI ZD, 1988, THEOR PROBAB APPL+, V32, P490, DOI 10.1137/1132067; Bayati M., 2013, ADV NEURAL INFORM PR, V26, P944; Bellare M, 1996, IEEE T INFORM THEORY, V42, P1781, DOI 10.1109/18.556674; Ben-Sasson E., 2003, P 35 ANN ACM S THEOR, P612; Chen X, 2014, ANN IEEE SYMP FOUND, P286, DOI 10.1109/FOCS.2014.38; Dicker LH, 2014, BIOMETRIKA, V101, P269, DOI 10.1093/biomet/ast065; Fan JQ, 2012, J R STAT SOC B, V74, P37, DOI 10.1111/j.1467-9868.2011.01005.x; Goldreich O, 1998, ANN IEEE SYMP FOUND, P426, DOI 10.1109/SFCS.1998.743493; Guo Zijian, 2017, J AM STAT ASS; Janson L, 2017, J R STAT SOC B, V79, P1037, DOI 10.1111/rssb.12203; Kong WH, 2017, ANN STAT, V45, P2218, DOI 10.1214/16-AOS1525; Stadler N, 2010, TEST-SPAIN, V19, P209, DOI 10.1007/s11749-010-0197-z; Verzelen N, 2018, BERNOULLI, V24, P3683, DOI 10.3150/17-BEJ975; Verzelen N, 2010, ANN STAT, V38, P704, DOI 10.1214/08-AOS629; Wencheko E, 2000, STAT PAP, V41, P327, DOI 10.1007/BF02925926; YIN YQ, 1983, J MULTIVARIATE ANAL, V13, P489, DOI 10.1016/0047-259X(83)90035-0	17	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305047
C	Kovalev, D; Gorbunov, E; Gasanov, E; Richtarik, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kovalev, Dmitry; Gorbunov, Eduard; Gasanov, Elnur; Richtarik, Peter			Stochastic Spectral and Conjugate Descent Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EFFICIENCY	The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.	[Kovalev, Dmitry; Gorbunov, Eduard; Gasanov, Elnur; Richtarik, Peter] Moscow Inst Phys & Technol, Dolgoprudnyi, Russia; [Kovalev, Dmitry; Gasanov, Elnur; Richtarik, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia; [Richtarik, Peter] Univ Edinburgh, Edinburgh, Midlothian, Scotland	Moscow Institute of Physics & Technology; King Abdullah University of Science & Technology; University of Edinburgh	Kovalev, D (corresponding author), Moscow Inst Phys & Technol, Dolgoprudnyi, Russia.; Kovalev, D (corresponding author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.		Gorbunov, Eduard/U-1740-2019; Richtarik, Peter/O-5797-2018	Gorbunov, Eduard/0000-0002-3370-4130; Kovalev, Dmitry/0000-0003-1467-2994				Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141; Birgin EG, 2014, J STAT SOFTW, V60, DOI 10.18637/jss.v060.i03; Csiba D., 2015, J MACH LEARN RES, V37, P674; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Gower R.M., 2015, ARXIV151206890; Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487; Lee C.P., 2016, ARXIV160708320; Lee Yin Tat, 2013, FOCS; Leventhal D, 2010, MATH OPER RES, V35, P641, DOI 10.1287/moor.1100.0456; Loizou N., 2017, ARXIV171209677; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182; Nutini J., 2015, ICML; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Qu Z, 2016, OPTIM METHOD SOFTW, V31, P829, DOI 10.1080/10556788.2016.1190360; RICHTARIK P, 2017, ARXIV170601108; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; Richtarik P, 2016, OPTIM LETT, V10, P1233, DOI 10.1007/s11590-015-0916-1; Tu S., 2017, ICML	21	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303036
C	Krijthe, JH; Loog, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Krijthe, Jesse H.; Loog, Marco			The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CLASSIFICATION	Consider a classification problem where we have both labeled and unlabeled data available. We show that for linear classifiers defined by convex margin-based surrogate losses that are decreasing, it is impossible to construct any semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss on the labeled and unlabeled data. For convex margin-based loss functions that also increase, we demonstrate safe improvements are possible.	[Krijthe, Jesse H.] Radboud Univ Nijmegen, Nijmegen, Netherlands; [Loog, Marco] Delft Univ Technol, Delft, Netherlands; [Loog, Marco] Univ Copenhagen, Copenhagen, Denmark	Radboud University Nijmegen; Delft University of Technology; University of Copenhagen	Krijthe, JH (corresponding author), Radboud Univ Nijmegen, Nijmegen, Netherlands.	jkrijthe@gmail.com; m.loog@tudelft.nl			Project P23 of the Dutch COMMIT research programme	Project P23 of the Dutch COMMIT research programme	We thank Alexander Mey for his constructive feedback on an earlier version of this manuscript. This work was funded by Project P23 of the Dutch COMMIT research programme.	Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Ben-David S., 2008, C LEARN THEOR COLT; Cozman F, 2006, SEMISUPERVISED LEARN, P56; diaeresis> M., 2013, STACS, V20, P185; Elworthy David, 1994, P 4 C APPL NAT LANG, P53; Fung G., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P77, DOI 10.1145/502512.502527; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Kawakita M, 2014, NEURAL NETWORKS, V53, P146, DOI 10.1016/j.neunet.2014.01.016; Krijthe JH, 2017, MACH LEARN, V106, P993, DOI 10.1007/s10994-017-5626-8; Krijthe JH, 2017, PATTERN RECOGN, V63, P115, DOI 10.1016/j.patcog.2016.09.009; Loog M, 2016, IEEE T PATTERN ANAL, V38, P462, DOI 10.1109/TPAMI.2015.2452921; Loog M, 2010, LECT NOTES ARTIF INT, V6322, P291, DOI 10.1007/978-3-642-15883-4_19; Poggio Tomaso, 2003, NOTICES AM MATH SOC, V50, P537; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rifkin R, 2003, NATO SCI SERIES 3, V190, P131, DOI DOI 10.1016/S0072-9752(06)80038-2; Seeger M., 2001, TECHNICAL REPORT; Shi M, 2011, BIOINFORMATICS, V27, P3017, DOI 10.1093/bioinformatics/btr502; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Sokolovska N., 2008, P 25 INT C MACH LEAR, P984; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Weston J, 2005, BIOINFORMATICS, V21, P3241, DOI 10.1093/bioinformatics/bti497	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301075
C	Kumar, R; Purohit, M; Svitkina, Z		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kumar, Ravi; Purohit, Manish; Svitkina, Zoya			Improving Online Algorithms via ML Predictions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this work we study the problem of using machine-learned predictions to improve the performance of online algorithms. We consider two classical problems, ski rental and non-clairvoyant job scheduling, and obtain new online algorithms that use predictions to make their decisions. These algorithms are oblivious to the performance of the predictor, improve with better predictions, but do not degrade much if the predictions are poor.	[Kumar, Ravi; Purohit, Manish; Svitkina, Zoya] Google, Mountain View, CA 94043 USA	Google Incorporated	Kumar, R (corresponding author), Google, Mountain View, CA 94043 USA.	ravi.k53@gmail.com; mpurohit@google.com; zoya@cs.cornell.edu	Purohit, Manish/ABD-3458-2021					Bansal N, 2004, ALGORITHMICA, V40, P305, DOI 10.1007/s00453-004-1115-0; Bansal Nikhil, 2001, P JOINT INT C MEAS M, P279, DOI [10.1145/378420.378792, 10.1145/384268.378792, DOI 10.1145/384268.378792]; Becchetti Luca, 2001, P 33 ANN ACM S THEOR, P94; Bent Russell, 2009, ONLINE STOCHASTIC CO; Borodin A., 1998, ONLINE COMPUTATION C; Bubeck S., 2012, COLT; Crovella ME, 1997, IEEE ACM T NETWORK, V5, P835, DOI 10.1109/90.650143; HarcholBalter M, 1997, ACM T COMPUT SYST, V15, P253, DOI 10.1145/263326.263344; Im S, 2018, J ACM, V65, DOI 10.1145/3136754; Im SJ, 2014, ANN IEEE SYMP FOUND, P531, DOI 10.1109/FOCS.2014.63; KARLIN AR, 1994, ALGORITHMICA, V11, P542, DOI 10.1007/BF01189993; Karlin AR, 2003, ALGORITHMICA, V36, P209, DOI 10.1007/s00453-003-1013-x; KARLIN AR, 1988, ALGORITHMICA, V3, P79, DOI 10.1007/BF01762111; Khanafer A, 2013, IEEE INFOCOM SER, P1492; Kodialam Rohan, 2014, SIAM UNDERGRADUATE R, V7, P233; Kouvelis P., 2013, ROBUST DISCRETE OPTI, V14; Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909; Lykouris T, 2018, PR MACH LEARN RES, V80; Mahdian M, 2012, ACM T ALGORITHMS, V8, DOI 10.1145/2071379.2071381; Meyerson A, 2005, ANN IEEE SYMP FOUND, P274, DOI 10.1109/SFCS.2005.72; Mirrokni Vahab S, 2012, P TWENTYTHIRD ANN AC, P1690; MOTWANI R, 1994, THEOR COMPUT SCI, V130, P17, DOI 10.1016/0304-3975(94)90151-1; Munoz Andres, 2017, ADV NEURAL INFORM PR, V30, P1858	23	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004024
C	Kumaraswamy, R; Schlegel, M; White, A; White, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kumaraswamy, Raksha; Schlegel, Matthew; White, Adam; White, Martha			Context-Dependent Upper-Confidence Bounds for Directed Exploration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Directed exploration strategies for reinforcement learning are critical for learning an optimal policy in a minimal number of interactions with the environment. Many algorithms use optimism to direct exploration, either through visitation estimates or upper-confidence bounds, as opposed to data-inefficient strategies like epsilon-greedy that use random, undirected exploration. Most data-efficient exploration methods require significant computation, typically relying on a learned model to guide exploration. Least-squares methods have the potential to provide some of the data-efficiency benefits of model-based approaches-because they summarize past interactions-with the computation closer to that of model-free approaches. In this work, we provide a novel, computationally efficient, incremental exploration strategy, leveraging this property of least-squares temporal difference learning (LSTD). We derive upper-confidence bounds on the action-values learned by LSTD, with context-dependent (or state-dependent) noise variance. Such context-dependent noise focuses exploration on a subset of variable states, and allows for reduced exploration in other states. We empirically demonstrate that our algorithm can converge more quickly than other incremental exploration strategies using confidence estimates on action-values.	[Kumaraswamy, Raksha; Schlegel, Matthew; White, Adam; White, Martha] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [White, Adam] DeepMind, London, England	University of Alberta	Kumaraswamy, R (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	kumarasw@ualberta.ca; mkschleg@ualberta.ca; adamwhite@google.com; whitem@ualberta.ca	White, Martha/AAF-7066-2020	White, Martha/0000-0002-5356-2950				Abbasi-Yadkori Y., 2014, UNCERTAINTY ARTIFICI; Auer P., 2006, ADV NEURAL INFORM PR; Bartlett P. L., 2009, C UNC ART INT; Boyan JA, 2002, MACH LEARN, V49, P233, DOI 10.1023/A:1017936530646; Brafman R., 2003, J MACHINE LEARNING R; Chu Wei, 2011, INT C ART INT STAT; Grande Robert, 2014, INT C MACH LEARN; Jaksch Thomas, 2010, J MACHINE LEARNING R; Jong N., 2007, ABSTRACTION REFORMUL; Jung T., 2010, MACHINE LEARNING ECM; Kaelbling L.P., 1993, LEARNING EMBEDDED SY; Kaelbling Leslie Pack, 1996, J ARTIFICIAL INTELLI; Kakade S., 2003, INT C MACH LEARN; Kawaguchi K., 2016, AAAI C ART INT; Kearns M. J., 2002, MACHINE LEARNING; Lagoudakis M., 2003, J MACHINE LEARNING R; Levine N., 2017, ADV NEURAL INFORM PR, P3138; Li L., 2009, INT C AUT AG MULT SY; Li L., 2010, WORLD WID WEB C; Lorincz A., 2008, INT C MACH LEARN; Martin J., 2017, INT JOINT C ART INT; Meuleau N., 1999, MACHINE LEARNING; MEYER CD, 1973, SIAM J APPL MATH, V24, P315; Miller K. S., 1981, MATH MAG, V54, P67, DOI DOI 10.1080/0025570X.1981.11976898; Nouri A., 2009, ADV NEURAL INFORM PR; Ortner Ronald, 2012, ADV NEURAL INFORM PR, V25, P1763; Osband I., 2017, INT C MACH LEARN; Osband I., 2016, ADV NEURAL INFORM PR; Osband Ian, 2013, ADV NEURAL INFORM PR; Osband Ian, 2016, INT C MACH LEARN; Ostrovski G., 2017, INT C MACH LEARN; Pazis J., 2013, AAAI C ART INT; Plappert M., 2017, PARAMETER SPACE NOIS; Singh S. P., 2000, MACHINE LEARNING; Strehl A., 2004, INT C MACH LEARN; Strehl Alexander L., 2006, INT C MACH LEARN; Sutton R, 2008, C UNC ART INT; Sutton R. S., 1988, MACHINE LEARNING; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szepesvari C., 2010, INT C MACH LEARN; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; van Seijen, 2015, INT C MACH LEARN; White M., 2017, INT C MACH LEARN; Wiering M. A., 1998, SIMULATION ADAPTIVE	47	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304076
C	Laue, S; Mitterreiter, M; Giesen, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Laue, Soeren; Mitterreiter, Matthias; Giesen, Joachim			Computing Higher Order Derivatives of Matrix and Tensor Expressions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup of up to two orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives on CPUs and a speedup of about three orders of magnitude on GPUs.	[Laue, Soeren; Mitterreiter, Matthias; Giesen, Joachim] Friedrich Schiller Univ Jena, Jena, Germany	Friedrich Schiller University of Jena	Laue, S (corresponding author), Friedrich Schiller Univ Jena, Jena, Germany.	soeren.laue@uni-jena.de; matthias.mitterreiter@uni-jena.de; joachim.giesen@uni-jena.de			Deutsche Forschungsgemeinschaft (DFG) [LA 2971/1-1]; AWS Cloud Credits for Research program	Deutsche Forschungsgemeinschaft (DFG)(German Research Foundation (DFG)); AWS Cloud Credits for Research program	Soren Laue has been funded by Deutsche Forschungsgemeinschaft (DFG) under grant LA 2971/1-1. This work has also been supported by the AWS Cloud Credits for Research program and by a gift from Google.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Baydin AG, 2018, J MACH LEARN RES, V18; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Broomhead D. S., 1988, Complex Systems, V2, P321; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; COX DR, 1958, J R STAT SOC B, V20, P215; D Maclaurin, 2015, ICML AUTOML WORKSH; Gebremedhin AH, 2009, INFORMS J COMPUT, V21, P209, DOI 10.1287/ijoc.1080.0286; Giles M.B., 2008, ADV AUTOMATIC DIFFER, V64, P35, DOI [10.1007/978-3-540-68942-34, DOI 10.1007/978-3-540-68942-34]; Hascoet L, 2013, ACM T MATH SOFTWARE, V39, DOI 10.1145/2450153.2450158; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Magnus J. R., 2007, MATRIX DIFFERENTIAL; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Peters J, 2017, ADAPT COMPUT MACH LE; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ricci M., 1900, MATH ANN, V54, P125, DOI DOI 10.1007/BF01454201; Seeger M., 2017, ARXIV171008717; Theano Development Team, 2016, ARXIV E PRINTS; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Walther A, 2012, CH CRC COMP SCI SER, P181	25	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302074
C	Lee, J; Raginsky, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Jaeho; Raginsky, Maxim			Minimax statistical learning with Wasserstein distances	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove generalization bounds that involve the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for transport-based domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.	[Lee, Jaeho] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Lee, J (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.	jlee620@illinois.edu; maxim@illinois.edu			NSF [CIF-1527 388, CIF-1302438]; NSF CAREER award [1254041]	NSF(National Science Foundation (NSF)); NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported in part by NSF grant nos. CIF-1527 388 and CIF-1302438, and in part by the NSF CAREER award 1254041.	Ambrosio L., 2008, LECT MATH ETH ZURICH, VSecond; Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blanchet J., 2016, 161005627V2 ARXIV; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; Duchi J. C., 2016, 161003425 ARXIV; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Gao R, 2016, 160402199 ARXIV; Goodfellow I. J., 2014, 14126572V3 ARXIV; Koltchinskii V, 2002, ANN STAT, V30, P1; Mansour Yishay, 2009, PROC 22 ANN C LEARN; Sinha A., 2018, ICLR; Talagrand M., 2014, UPPER LOWER BOUNDS S; Vapnik V.N, 1998, STAT LEARNING THEORY; Villani C., 2003, TOPICS OPTIMAL TRANS	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302068
C	Leung, D; Drton, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Leung, Dennis; Drton, Mathias			Algebraic tests of general Gaussian latent tree models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BOOTSTRAP	We consider general Gaussian latent tree models in which the observed variables are not restricted to be leaves of the tree. Extending related recent work, we give a full semi-algebraic description of the set of covariance matrices of any such model. In other words, we find polynomial constraints that characterize when a matrix is the covariance matrix of a distribution in a given latent tree model. However, leveraging these constraints to test a given such model is often complicated by the number of constraints being large and by singularities of individual polynomials, which may invalidate standard approximations to relevant probability distributions. Illustrating with the star tree, we propose a new testing methodology that circumvents singularity issues by trading off some statistical estimation efficiency and handles cases with many constraints through recent advances on Gaussian approximation for maxima of sums of high-dimensional random vectors. Our test avoids the need to maximize the possibly multimodal likelihood function of such models and is applicable to models with larger number of variables. These points are illustrated in numerical experiments.	[Leung, Dennis] Univ Southern Calif, Dept Data Sci & Operat, Los Angeles, CA 90089 USA; [Drton, Mathias] Univ Washington, Dept Stat, Seattle, WA 98195 USA; [Drton, Mathias] Univ Copenhagen, Dept Math Sci, Copenhagen, Denmark	University of Southern California; University of Washington; University of Washington Seattle; University of Copenhagen	Leung, D (corresponding author), Univ Southern Calif, Dept Data Sci & Operat, Los Angeles, CA 90089 USA.	dmhleung@uw.edu; md5@uw.edu		Drton, Mathias/0000-0001-5614-3025				Anderson TW, 1958, INTRO MULTIVARIATE S; BEKKER PA, 1987, PSYCHOMETRIKA, V52, P125, DOI 10.1007/BF02293960; BOLLEN KA, 1993, SOCIOL METHODOL, V23, P147, DOI 10.2307/271009; Buhlmann P, 2002, STAT SCI, V17, P52, DOI 10.1214/ss/1023798998; Chernozhukov V., 2013, ARXIV13127614; Choi MJ, 2011, J MACH LEARN RES, V12, P1771; Drton M., 2009, OBERWOLFACH SEMINARS, V39; Drton M, 2008, ANN STAT, V36, P2261, DOI 10.1214/07-AOS522; Drton M, 2007, PROBAB THEORY REL, V138, P463, DOI 10.1007/s00440-006-0033-2; Drton M, 2016, BERNOULLI, V22, P38, DOI 10.3150/14-BEJ620; Drton M, 2009, ANN STAT, V37, P979, DOI 10.1214/07-AOS571; HALL P, 1995, BIOMETRIKA, V82, P561; Lahiri S. N., 2003, RESAMPLING METHODS D, P115; Mourad R, 2013, J ARTIF INTELL RES, V47, P157, DOI 10.1613/jair.3879; Semple C, 2003, OXFORD LECT SERIES M, V24; Shiers N, 2016, BIOMETRIKA, V103, P531, DOI 10.1093/biomet/asw032; Spearman C, 1904, AM J PSYCHOL, V15, P201, DOI 10.2307/1412107; Wishart J, 1928, B J PSYCHOL-GEN SECT, V19, P180, DOI 10.1111/j.2044-8295.1928.tb00508.x; Zhang DN, 2017, ANN STAT, V45, P1895, DOI 10.1214/16-AOS1512	20	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000077
C	Levin, R; Sevekari, A; Woodruff, DP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Levin, Roie; Sevekari, Anish; Woodruff, David P.			Robust Subspace Approximation in a Stream	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study robust subspace estimation in the streaming and distributed settings. Given a set of n data points {a(i)}(i=1)(n) in R-d and an integer k, we wish to find a linear subspace S of dimension k for which Sigma(i) M(dist(S; a(i))) is minimized, where dist(S; x) := min(y is an element of S) parallel to x - y parallel to(2), and M( is some loss function. When M is the identity function, S gives a subspace that is more robust to outliers than that provided by the truncated SVD. Though the problem is NP-hard, it is approximable within a (1 + epsilon) factor in polynomial time when k and epsilon are constant. We give the first sublinear approximation algorithm for this problem in the turnstile streaming and arbitrary partition distributed models, achieving the same time guarantees as in the offline case. Our algorithm is the fi rst based entirely on oblivious dimensionality reduction, and significantly simplifies prior methods for this problem, which held in neither the streaming nor distributed models.	[Levin, Roie; Woodruff, David P.] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Sevekari, Anish] Carnegie Mellon Univ, Dept Math Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Levin, R (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.	roiel@cs.cmu.edu; asevekar@andrew.cmu.edu; dwoodruf@cs.cmu.edu			National Science Foundation [CCF-1815840]	National Science Foundation(National Science Foundation (NSF))	We would like to thank Ainesh Bakshi for many helpful discussions. D. Woodruff thanks partial support from the National Science Foundation under Grant No. CCF-1815840. Part of this work was also done while D. Woodruff was visiting the Simons Institute for the Theory of Computing.	Backurs Arturs, 2016, SODA; Basu Saugata, 1994, J ACM; Clarkson K., 2015, SODA; Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; Deshpande A, 2007, ACM S THEORY COMPUT, P641, DOI 10.1145/1250790.1250884; Deshpande Amit, 2011, SODA; Deshpande Amit, 2007, STOC; Ding C., 2006, ICML; FELDMAN D, 2010, SODA; Feldman D, 2010, PROC APPL MATH, V135, P630; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Guruswami V, 2016, ACM T ALGORITHMS, V12, DOI 10.1145/2737729; Indyk P, 2001, ANN IEEE SYMP FOUND, P10, DOI 10.1109/SFCS.2001.959878; Kannan R., 2014, P 27 C LEARN THEOR C, P1040; Langberg M, 2011, STOC; Monemizadeh Morteza, 2010, SODA; Muthukrishnan S., 2005, FDN TRENDS THEORETIC, V1; Shyamalkumar ND, 2012, DISCRETE COMPUT GEOM, V47, P44, DOI 10.1007/s00454-011-9384-2; Song Zhao, 2016, CORR; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005028
C	Li, J; Liu, Y; Yin, R; Zhang, H; Ding, LZ; Wang, WP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Jian; Liu, Yong; Yin, Rong; Zhang, Hua; Ding, Lizhong; Wang, Weiping			Multi-Class Learning: From Theory to Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we study the generalization performance of multi-class classification and obtain a shaper data-dependent generalization error bound with fast convergence rate, substantially improving the state-of-art bounds in the existing data-dependent generalization analysis. The theoretical analysis motivates us to devise two effective multi-class kernel learning algorithms with statistical guarantees. Experimental results show that our proposed methods can significantly outperform the existing multi-class classification methods.	[Li, Jian; Liu, Yong; Yin, Rong; Zhang, Hua; Wang, Weiping] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China; [Li, Jian; Yin, Rong] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China; [Wang, Weiping] Natl Engn Res Ctr Informat Secur, Shanghai, Peoples R China; [Wang, Weiping] Natl Engn Lab Informat Secur Technol, Shanghai, Peoples R China; [Ding, Lizhong] IIAI, Abu Dhabi, U Arab Emirates	Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Liu, Y (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.	lijian9026@iie.ac.cn; liuyong@iie.ac.cn; yinrong@iie.ac.cn; lizhong.ding@inceptioniai.org; wangweiping@iie.ac.cn		Li, Jian/0000-0003-4977-1802	National Natural Science Foundation of China [61703396, 61673293, 61602467]; National Key Research and Development Program of China [2016YFB1000604]; Science and Technology Project of Beijing [Z181100002718004]; Excellent Talent Introduction of Institute of Information Engineering of CAS [Y7Z0111107]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; Science and Technology Project of Beijing; Excellent Talent Introduction of Institute of Information Engineering of CAS	This work is supported in part by the National Natural Science Foundation of China (No.61703396, No.61673293, No.61602467), the National Key Research and Development Program of China (No.2016YFB1000604), the Science and Technology Project of Beijing (No.Z181100002718004) and the Excellent Talent Introduction of Institute of Information Engineering of CAS (Y7Z0111107).	Allwein E. L., 2000, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133; [Anonymous], 2015, ADV NEURAL INFORM PR; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; BOTTOU L, 1994, INT C PATT RECOG, P77, DOI 10.1109/ICPR.1994.576879; Cortes C., 2013, ADV NEURAL INFORM PR, P2760; Cortes Corinna, 2013, INT C MACHINE LEARNI, P46, DOI DOI 10.1007/978-1-4471-4351-2_3; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; D. McAllester, 2013, ARXIV 13; Daniely A., 2014, COLT, P287; Franc  V., 2005, THESIS; Guermeur Y, 2002, PATTERN ANAL APPL, V5, P168, DOI 10.1007/s100440200015; Hardt M, 2016, PR MACH LEARN RES, V48; Hill SI, 2007, J ARTIF INTELL RES, V30, P525, DOI 10.1613/jair.2251; Knerr S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P41; Koltchinskii V, 2002, ANN STAT, V30, P1; Koltchinskii V, 2001, ADV NEUR IN, V13, P245; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Kuznetsov V., 2014, ADV NEURAL INFORM PR, P2501; Liu Y., 2011, P 20 ACM INT C INF K, P2205; Liu Y, 2017, AAAI CONF ARTIF INTE, P2280; Liu Y, 2015, AAAI CONF ARTIF INTE, P2814; Liu Y, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2189, DOI 10.1145/2505515.2505584; Liu Y, 2014, ADV INTEL SYS RES, V100, P324; Maximov Yu, 2016, Pattern Recognition and Image Analysis, V26, P673, DOI 10.1134/S105466181604009X; Moh  M., 2012, FDN MACHINE LEARNING; Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1023/A:1022605311895; Orabona F, 2010, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2010.5540137; Orabona Francesco, 2011, P 28 INT C MACH LEAR, P249; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; Vapnik V., 2000, NATURE STAT LEARNING; Xu C, 2016, IEEE T IMAGE PROCESS, V25, P1495, DOI 10.1109/TIP.2016.2524207; Yong Liu, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P290, DOI 10.1007/978-3-662-44851-9_19; Yousefi  N., 2016, ARXIV160205916; Zhang T, 2004, J MACH LEARN RES, V5, P1225; Zien A., 2007, P 24 INT C MACH LEAR, V1, P1191, DOI DOI 10.1145/1273496.1273646	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301056
C	Li, P; Milenkovic, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Pan; Milenkovic, Olgica			Revisiting Decomposable Submodular Function Minimization with Incidence Relations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a new approach to decomposable submodular function minimization (DSFM) that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions, and when properly utilized, they allow for improving the convergence rates of DSFM solvers. Our main results include the precise parametrization of the DSFM problem based on incidence relations, the development of new scalable alternative projections and parallel coordinate descent methods and an accompanying rigorous analysis of their convergence rates.	[Li, Pan; Milenkovic, Olgica] UIUC, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Li, P (corresponding author), UIUC, Champaign, IL 61820 USA.	panli2@illinois.edu; milenkov@illinois.edu			NSF [CCF 15-27636]; NSF Purdue [4101-38050]; NFT STC center Science of Information	NSF(National Science Foundation (NSF)); NSF Purdue; NFT STC center Science of Information	The authors gratefully acknowledge many useful suggestions by the reviewers. This work was supported in part by the NSF grant CCF 15-27636, the NSF Purdue 4101-38050 and the NFT STC center Science of Information.	Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47; [Anonymous], 2017, P NEURAL INFORM PROC; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Chakrabarty D., 2014, ADV NEURAL INFORM PR, V1, P802, DOI 10.48550/arXiv.1411.0095; Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9; Chekuri C, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1085; Djolonga J., 2015, ICML, P1804; Ene A, 2017, ADV NEURAL INFORM PR, P2874, DOI [10.5555/3294996.3295047, DOI 10.5555/3294996.3295047]; Ene A, 2015, PR MACH LEARN RES, V37, P787; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Fujishige S., 1992, JPN J IND APPL MATH, V9, P369, DOI 10.1007/BF03167272; Hein M., 2013, P ADV NEUR INF PROC, V26; Jegelka S., 2013, ADV NEURAL INFO PROC, P1313; KARGER DR, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P21; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Kierstead HA, 2010, COMBINATORICA, V30, P217, DOI 10.1007/s00493-010-2483-5; Kipf TN, 2016, P INT C LEARN REPR; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025; Krause A., 2007, AAAI C ART INT; Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Li P., 2018, P 35 INT C MACHINE L, P3014; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; MEYER W, 1973, AM MATH MON, V80, P920, DOI 10.2307/2319405; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nishihara R., 2014, ADV NEURAL INFO PROC, P640; Stobbe P., 2010, ADV NEURAL INFO PROC, P2208; Szemeredi E., 1970, COMBINATORIAL THEORY, VII, P601; Wei K, 2015, PR MACH LEARN RES, V37, P1954; WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381; Yadati N, 2018, ARXIV180902589; ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302026
C	Li, S; Xiao, S; Zhu, SX; Du, N; Xie, Y; Song, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Shuang; Xiao, Shuai; Zhu, Shixiang; Du, Nan; Xie, Yao; Song, Le			Learning Temporal Point Processes via Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Social goods, such as healthcare, smart city, and information networks, often produce ordered event data in continuous time. The generative processes of these event data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE, we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting, we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data.	[Li, Shuang; Zhu, Shixiang; Xie, Yao; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Xiao, Shuai; Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China; [Du, Nan] Google Brain, Mountain View, CA USA	University System of Georgia; Georgia Institute of Technology; Google Incorporated	Li, S (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	sli370@gatech.edu; yao.xie@isye.gatech.edu; lsong@cc.gatech.edu	Xiao, Shuai/AAQ-5932-2020		NSF [CCF-1442635, CMMI-1538746, DMS-1830210, IIS-1218749, IIS-1639792 EAGER, CNS-1704701, CCF-1836822, IIS-1841351 EAGER]; NSF CAREER Award [CCF-1650913]; Atlanta Police Foundation fund; S.F. Express fund; NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; Siemens	NSF(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Atlanta Police Foundation fund; S.F. Express fund; NIH BIGDATA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); Intel ISTC; NVIDIA; Amazon AWS; Siemens(Siemens AG)	This project was supported in part by NSF grants CCF-1442635, CMMI-1538746, DMS-1830210, NSF CAREER Award CCF-1650913, Atlanta Police Foundation fund, and an S.F. Express fund awarded to Yao Xie. This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS, NSF CCF-1836822, NSF IIS-1841351 EAGER, and Siemens awarded to Le Song.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; [Anonymous], 2010, PROC 16 ACM SIGKDD I, DOI DOI 10.1145/1835804.1835933; [Anonymous], 2003, SOBOLEV SPACES; Ba J., 2017, P 3 INT C LEARN REPR; Berlinet A., 2011, REPRODUCING KERNEL H; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Daley D., 2007, INTRO THEORY POINT P; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147; Dziugaite G.K., 2015, ARXIV150503906; Farajtabar M, 2015, ADV NEURAL INFORM PR, P1954; Grandell Jan, 2006, DOUBLY STOCHASTIC PO, V529; Gretton A, 2012, J MACH LEARN RES, V13, P723; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kim Beomjoon, 2013, ROBOTICS SCI SYSTEMS; Kingman J. F. C., 1993, POISSON PROCESSES; Mei H., 2017, ADV NEURAL INFORM PR, P6754; Omi T, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.012303; Pfanzagl Johann, 2011, PARAMETRIC STAT THEO; Scholkopf B., 2001, LEARNING KERNELS SUP; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sriperumbudur B. K., 2010, P 13 INT C ART INT S, V9, P773; Sutton R. S., 1998, REINFORCEMENT LEARNI, V1; Xiao S, 2017, ADV NEUR IN, V30; Ziebart B. D., 2008, AAAI, V8, P1433	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005037
C	Li, YJ; Bresler, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Yanjun; Bresler, Yoram			Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SUBSPACE METHODS; RECONSTRUCTION; IDENTIFICATION; REPRESENTATION; ALGORITHM	Multichannel blind deconvolution is the problem of recovering an unknown signal f and multiple unknown channels x(i) from convolutional measurements y(i) = x(i)circle star f (i = 1, 2, .. . , N). We consider the case where the x(i)'s are sparse, and convolution with f is invertible. Our nonconvex optimization formulation solves for a filter h on the unit sphere that produces sparse output y(i)circle star h. Under some technical assumptions, we show that all local minima of the objective function correspond to the inverse filter of f up to an inherent sign and shift ambiguity, and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of f and x(i) using a simple manifold gradient descent algorithm with random initialization. Our theoretical findings are complemented by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods.	[Li, Yanjun; Bresler, Yoram] Univ Illinois, CSL, Champaign, IL 61820 USA; [Li, Yanjun; Bresler, Yoram] Univ Illinois, Dept ECE, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Li, YJ (corresponding author), Univ Illinois, CSL, Champaign, IL 61820 USA.; Li, YJ (corresponding author), Univ Illinois, Dept ECE, Champaign, IL 61820 USA.	yli145@illinois.edu; ybresler@illinois.edu		Bresler, Yoram/0000-0002-9738-1094	National Science Foundation (NSF) [IIS 14-47879]	National Science Foundation (NSF)(National Science Foundation (NSF))	This work was supported in part by the National Science Foundation (NSF) under Grant IIS 14-47879. The authors would like to thank Ju Sun for helpful discussions about this paper. The manuscript benefited from constructive comments by the anonymous reviewers.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; Allen-Zhu Z., 2017, ARXIV170200763; ALLEN- ZHU Z., 2017, ARXIV170808694; [Anonymous], ARXIV171003309; Balzano L, 2007, PROCEEDINGS OF THE SIXTH INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P79, DOI 10.1109/IPSN.2007.4379667; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Berger CR, 2010, IEEE T SIGNAL PROCES, V58, P1708, DOI 10.1109/TSP.2009.2038424; Betzig E, 2006, SCIENCE, V313, P1642, DOI 10.1126/science.1127344; Bilen C, 2014, IEEE T SIGNAL PROCES, V62, P4847, DOI 10.1109/TSP.2014.2342651; Boumal N., 2016, ARXIV160508101; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chi YJ, 2016, IEEE J-STSP, V10, P782, DOI 10.1109/JSTSP.2016.2543462; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Eldar Yonina C, 2017, ARXIV170703378; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Gitelman DR, 2003, NEUROIMAGE, V19, P200, DOI 10.1016/S1053-8119(03)00058-2; Goldfarb D, 2017, COMPUT OPTIM APPL, V68, P479, DOI 10.1007/s10589-017-9925-6; GURELLI MI, 1995, IEEE T SIGNAL PROCES, V43, P134, DOI 10.1109/78.365293; Jin C, 2017, PR MACH LEARN RES, V70; Kaaresen KF, 1998, GEOPHYSICS, V63, P2093, DOI 10.1190/1.1444503; Lee J. D., 2016, C LEARN THEOR, P1246; Lee Jason D, 2017, ARXIV171007406; Lee K., 2017, ARXIV170804343; Lee K, 2017, IEEE T INFORM THEORY, V63, P802, DOI 10.1109/TIT.2016.2636204; Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148; Li X., 2016, ARXIV160604933; Li  Y., 2018, IEEE T INFORM THEORY; Li YJ, 2017, IEEE T INFORM THEORY, V63, P4619, DOI 10.1109/TIT.2017.2689779; Li YJ, 2017, IEEE T INFORM THEORY, V63, P822, DOI 10.1109/TIT.2016.2637933; Li YJ, 2016, IEEE T SIGNAL PROCES, V64, P5549, DOI 10.1109/TSP.2016.2598311; Li YJ, 2016, IEEE T INFORM THEORY, V62, P4266, DOI 10.1109/TIT.2016.2569578; Ling  S., 2016, SELF CALIBRATION VIA; Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002; Mei S., 2016, ARXIV160706534; MOULINES E, 1995, IEEE T SIGNAL PROCES, V43, P516, DOI 10.1109/78.348133; Mukamel EA, 2012, BIOPHYS J, V102, P2391, DOI 10.1016/j.bpj.2012.03.070; Panageas I., 2016, ARXIV160500405; Paulraj A., 1985, ICASSP 85. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No. 85CH2118-8), P640; Rust MJ, 2006, NAT METHODS, V3, P793, DOI 10.1038/nmeth929; Sabra KG, 2004, J ACOUST SOC AM, V116, P262, DOI 10.1121/1.1751151; Sarder P, 2006, IEEE SIGNAL PROC MAG, V23, P32, DOI 10.1109/MSP.2006.1628876; She HJ, 2015, MAGN RESON IMAGING, V33, P1106, DOI 10.1016/j.mri.2015.06.008; Strohmer T, 2002, LINEAR ALGEBRA APPL, V343, P321, DOI 10.1016/S0024-3795(01)00243-9; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Tian N, 2017, J ACOUST SOC AM, V141, P3337, DOI 10.1121/1.4983311; Tong L, 1998, P IEEE, V86, P1951, DOI 10.1109/5.720247; TONG L, 1991, CONFERENCE RECORD OF THE TWENTY-FIFTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P856, DOI 10.1109/ACSSC.1991.186568; Wang LM, 2016, IEEE SIGNAL PROC LET, V23, P1384, DOI 10.1109/LSP.2016.2599104; Wylie M. P., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P281, DOI 10.1109/ICASSP.1993.319110; Xu GH, 1995, IEEE T SIGNAL PROCES, V43, P2982, DOI 10.1109/78.476442; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Zhang HC, 2013, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2013.140; Zhang Y., 2017, P 10 NIPS WORKSH OPT; Zhang Y., 2017, P IEEE C COMPUTER VI, P4894	59	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301015
C	Liang, XD; Hu, ZT; Zhang, H; Lin, L; Xing, EP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liang, Xiaodan; Hu, Zhiting; Zhang, Hao; Lin, Liang; Xing, Eric P.			Symbolic Graph Reasoning Meets Convolutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Beyond local convolution networks, we explore how to harness various external human knowledge for endowing the networks with the capability of semantic global reasoning. Rather than using separate graphical models (e.g. CRF) or constraints for modeling broader dependencies, we propose a new Symbolic Graph Reasoning (SGR) layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features. The SGR layer can be injected between any convolution layers and instantiated with distinct prior graphs. Extensive experiments show incorporating SGR significantly improves plain ConvNets on three semantic segmentation tasks and one image classification task. More analyses show the SGR layer learns shared symbolic representations for domains/datasets with the different label set given a universal knowledge graph, demonstrating its superior generalization capability.	[Liang, Xiaodan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China; [Hu, Zhiting; Zhang, Hao] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China; [Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA	Sun Yat Sen University; Carnegie Mellon University; Sun Yat Sen University	Liang, XD (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.	xdliang328@gmail.com; zhitingh@cs.cmu.edu; hao@cs.cmu.edu; linliang@ieee.org; epxing@cs.cmu.edu			National Key Research and Development Program of China [2018YFC0830103]; National High Level Talents Special Support Plan (Ten Thousand Talents Program); National Natural Science Foundation of China (NSFC) [61622214, 61836012]	National Key Research and Development Program of China; National High Level Talents Special Support Plan (Ten Thousand Talents Program); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key Research and Development Program of China under Grant No. 2018YFC0830103, in part by National High Level Talents Special Support Plan (Ten Thousand Talents Program), and in part by National Natural Science Foundation of China (NSFC) under Grant No. 61622214, and 61836012.	[Anonymous], 2018, CVPR; [Anonymous], 2018, ICLR; Arnab A, 2016, LECT NOTES COMPUT SC, V9906, P524, DOI 10.1007/978-3-319-46475-6_33; Badrinarayanan V., 2015, CVPR; Barriuso A., 2016, ARXIV160805442; BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X; Caesar H., 2016, ARXIV161203716; Chandra S., 2017, ICCV; Chen LH, 2016, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2016, VOL 5A; Chen X., 2018, CVPR; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Frome Andrea, 2013, NEURIPS; Gilmer Justin, 2017, ARXIV170401212; He K., 2016, ECCV; Hu H., 2017, CORRABS170309891; Huang G., 2017, CVPR; Joulin Armand, 2016, ARXIV161203651; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kipf Thomas N., 2017, INT C LEARNING REPRE; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lafferty John, 2001, P INT C MACH LEARN J; Lao Ni, 2011, P C EMP METH NAT LAN, P529, DOI DOI 10.5555/2145432.2145494; Liang X., 2017, CVPR; Liang X, 2016, ECCV; Lin G, 2017, CVPR; LIN GS, 2016, PROC CVPR IEEE, P3194, DOI DOI 10.1109/CVPR.2016.348; Liu W., 2015, COMPUTER SCI; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Marino K., 2016, ARXIV161204844; Mitchell T, 2015, AAAI CONF ARTIF INTE, P2302; Mottaghi R., 2014, CVPR; NEWELL A, 1980, COGNITIVE SCI, V4, P135, DOI 10.1016/S0364-0213(80)80015-2; Niepert M, 2016, PR MACH LEARN RES, V48; Ordonez V, 2013, IEEE I CONF COMP VIS, P2768, DOI 10.1109/ICCV.2013.344; Redmon Joseph, 2017, CVPR; Sabour S., 2017, NIPS; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schwing A. G., 2015, ARXIV150302351 2015A; Shuai B., 2017, TPAMI; Wu Z, 2016, CORR, P1; Wu Zifeng, 2016, ARXIV160506885; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu Fisher, 2016, MULTISCALE CONTEXT A; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhao H., 2017, ICCV; Zhao H, 2017, P IEEE C COMPUTER VI; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhou H., 2018, CVPR; Zhu Yuke, 2015, ARXIV150705670	52	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301081
C	Littwin, E; Wolf, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Littwin, Etai; Wolf, Lior			Regularizing by the Variance of the Activations' Sample-Variances	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Normalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations, we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. As we prove, this encourages the activations to be distributed around a few distinct modes. We also show that if the inputs are from a mixture of two Gaussians, the new loss would either join the two together, or separate between them optimally in the LDA sense, depending on the prior probabilities. Finally, we are able to link the new regularization term to the batchnorm method, which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks.	[Littwin, Etai; Wolf, Lior] Tel Aviv Univ, Tel Aviv, Israel; [Wolf, Lior] Facebook AI Res, Tel Aviv, Israel	Tel Aviv University; Facebook Inc	Littwin, E (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.				European Research Council (ERC) under the European Union [ERC CoG 725974]	European Research Council (ERC) under the European Union(European Research Council (ERC))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant ERC CoG 725974).	Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Djork-Arn, ICLR 2016; Estep  Donald, 2006, ERROR ESTIMATION ADA; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton, 2016, ARXIV PREPRINT ARXIV; Huang Gao, 2017, ARXIV PREPRINT ARXIV; Ioffe Sergey, 2017, NEURIPS; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Kingma D. P., 2013, AUTO ENCODING VARIAT; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lyu SW, 2008, PROC CVPR IEEE, P3721; Rolfe J. T., 2016, ARXIV160902200; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Scardapane S, 2017, NEUROCOMPUTING, V241, P81, DOI 10.1016/j.neucom.2017.02.029; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Ulyanov D., 2016, ARXIV160708022; Wen W, 2016, ADV NEUR IN, V29; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Yoon J, 2017, PR MACH LEARN RES, V70	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302015
C	Liu, MR; Zhang, XX; Zhang, LJ; Jin, R; Yang, TB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Mingrui; Zhang, Xiaoxuan; Zhang, Lijun; Jin, Rong; Yang, Tianbao			Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have recently received increasing attention for developing optimization algorithms with fast convergence. However, the studies of EBC in statistical learning are hitherto still limited. The main contributions of this paper are two-fold. First, we develop fast and intermediate rates of empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous, and smooth convex random functions. Second, we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization with Lipschitz continuous random functions, which requires only one pass of n samples and adapts to EBC. For both approaches, the convergence rates span a full spectrum between (O) over tilde (1/root n ) and (O) over tilde (1/n) depending on the power constant in EBC, and could be even faster than O (1/n) in special cases for ERM. Moreover, these convergence rates are automatically adaptive without using any knowledge of EBC.	[Liu, Mingrui; Zhang, Xiaoxuan; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Zhang, Lijun] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China; [Jin, Rong] Alibaba Grp, Machine Intelligence Technol, Bellevue, WA 98004 USA	University of Iowa; Nanjing University; Alibaba Group	Liu, MR (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	mingrui-liu@uiowa.edu; zljzju@gmail.com; tianbao-yang@uiowa.edu		Liu, Mingrui/0000-0002-5181-3429	National Science Foundation [IIS-1545995, 2017QNRC001]	National Science Foundation(National Science Foundation (NSF))	The authors thank the anonymous reviewers for their helpful comments. M. Liu and T. Yang are partially supported by National Science Foundation (IIS-1545995). L. Zhang is partially supported by YESS (2017QNRC001). We thank Nishant A. Mehta for pointing out the work [12] for the proof of Theorem 1.	[Anonymous], 2012, ICML; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bartlett Peter L., 2006, PROBABILITY THEORY R; Bartlett Peter L., 2005, ANN STAT; Bolte Jerome, 2015, CORR; BURKE JV, 1993, SIAM J CONTROL OPTIM, V31, P1340, DOI 10.1137/0331063; DRUSVYATSKIY D., 2016, ARXIV160206661; Duchi J. C., 2010, COLT, P14; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Feldman Vitaly, 2016, NIPS; Garber Dan, 2016, ICML; Grunwald Peter D., 2016, CORR; Hazan E., 2007, MACHINE LEARNING; Hazan Elad, 2011, COLT; Juditsky Anatoli, 2014, STOCH SYST; Kakade Sham M., 2008, NIPS; Karimi H., 2016, ECML PKDD; Kim Sujin, 2015, GUIDE SAMPLE AVERAGE, P207; Koltchinskii Vladimir, 2006, ANN STAT; Koolen Wouter M., 2016, NIPS; Koren Tomer, 2015, NIPS; Lee WS, 1998, IEEE T INFORM THEORY, V44, P1974, DOI 10.1109/18.705577; Li G., 2016, CORR; Li Guoyin, 2013, MATH PROGRAM; Lin X, 2017, CORR; Mammen E, 1999, ANN STAT, V27, P1808; Mehta Nishant A., 2017, AISTATS; Mehta Nishant A., 2014, NIPS; Necoara I., 2015, CORR; Nemirovski Arkadi, 2009, SIAM J OPTIMIZATION; Nesterov Y., 2018, APPL OPTIMIZATION; Pang Jong-Shi, 1997, MATH PROGRAM; Ramdas A., 2013, ICML; Rockafellar R. T., 1970, CONVEX ANAL; Shalev-Shwartz S., 2007, ICML; Shalev-Shwartz S., 2009, P 22 C LEARN THEOR C, P1; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Shalev-Shwartz Shai, 2017, J MACH LEARN RES, V18, P8245; Shamir Ohad, 2013, ICML; Shapiro A, 2014, LECT STOCHASTIC PROG; Smale Steve, 2007, CONSTRUCTIVE APPROXI; Srebro Nathan, 2010, NIPS; Srebro Nathan, 2010, ARXIV E PRINTS; Sridharan Karthik, 2008, NIPS; Tsybakov AB, 2004, ANN STAT, V32, P135; van Erven Tim, 2016, NIPS; van Erven Tim, 2015, JMLR; Vapnik V.N, 1998, STAT LEARNING THEORY; Xu Y., 2016, CORR; Xu Y, 2017, PR MACH LEARN RES, V70; Yang T.-J., 2016, CORR; Yang Tianbao, 2018, AISTATS, P445; Yang W. H., 2009, SIAM J OPTIMIZATION; Zhang Han, 2016, CORR; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	55	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304067
C	Liu, MR; Li, Z; Wang, XY; Yi, JF; Yang, TB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Mingrui; Li, Zhe; Wang, Xiaoyu; Yi, Jinfeng; Yang, Tianbao			Adaptive Negative Curvature Descent with Applications in Non-convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima In existing studies, NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (e.g., epsilon(2) << 1) in order to achieve a sufficiently accurate second-order stationary solution (i.e., lambda(min)(del(2)f(x)) >= -epsilon(2)). One issue with this approach is that the target precision epsilon(2) is usually set to be very small in order to find a high quality solution, which increases the complexity for computing a negative curvature. To address this issue, we propose an adaptive NCD to allow an adaptive error dependent on the current gradient's magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization, and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity.	[Liu, Mingrui; Li, Zhe; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Wang, Xiaoyu] Intellifusion, Parlin, NJ USA; [Yi, Jinfeng] JD AI Res, Stanford, CA USA	University of Iowa	Liu, MR (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	mingrui-liu@uiowa.edu; tianbao-yang@uiowa.edu		Liu, Mingrui/0000-0002-5181-3429; Wang, Xiaoyu/0000-0002-6431-8822	National Science Foundation [IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	We thank the anonymous reviewers for their helpful comments. M. Liu, T. Yang are partially supported by National Science Foundation (IIS-1545995).	Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; Carmon Yair, 2016, CORR; Cartis C, 2011, MATH PROGRAM, V130, P295, DOI 10.1007/s10107-009-0337-y; Cartis C, 2011, MATH PROGRAM, V127, P245, DOI 10.1007/s10107-009-0286-5; Curtis Frank E., 2017, CORR; Fan R. E., 2011, LIBSVM DATA CLASSIFI; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lei LH, 2017, ADV NEUR IN, V30; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Reddi SJ, 2016, IEEE DECIS CONTR P, P1971, DOI 10.1109/CDC.2016.7798553; Reddi Sashank J., 2017, CORR; Royer Clement W., 2017, CORR; Xu P., 2017, CORR; Xu Y., 2017, CORR; Zeyuan Allen-Zhu, 2017, CORR; Zhang Y., 2017, C LEARN THEOR PMLR, P1980	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304083
C	Liu, Q; Li, LH; Tang, ZY; Zhou, DY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Qiang; Li, Lihong; Tang, Ziyang; Zhou, Dengyong			Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider off-policy estimation of the expected reward of a target policy using samples collected by a different behavior policy. Importance sampling (IS) has been a key technique for deriving (nearly) unbiased estimators, but is known to suffer from an excessively high variance in long-horizon problems. In the extreme case of infinite-horizon problems, the variance of an IS-based estimator may even be unbounded. In this paper, we propose a new off-policy estimator that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance faced by existing methods. Our key contribution is a novel approach to estimating the density ratio of two stationary state distributions, with trajectories sampled from only the behavior distribution. We develop a mini-max loss function for the estimation problem, and derive a closed-form solution for the case of RKHS. We support our method with both theoretical and empirical analyses.	[Liu, Qiang; Tang, Ziyang] Univ Texas Austin, Austin, TX 78712 USA; [Li, Lihong; Zhou, Dengyong] Google Brain, Kirkland, WA 98033 USA	University of Texas System; University of Texas Austin; Google Incorporated	Liu, Q (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	lqiang@cs.utexas.edu; lihong@google.com; ztang@cs.utexas.edu; dennyzhou@google.com	, Gustavo/ABC-1706-2022; Camps-Valls, Gustavo/A-2532-2011	Camps-Valls, Gustavo/0000-0003-1683-2138	NSF [CRII 1830161]; Google Cloud	NSF(National Science Foundation (NSF)); Google Cloud(Google Incorporated)	This work is supported in part by NSF CRII 1830161. We would like to acknowledge Google Cloud for their support.	Asmussen  Soren, 2007, PROBABILITY THEORY S; Bellman RE, 1957, DYNAMIC PROGRAMMING; Berlinet A., 2011, REPRODUCING KERNEL H; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Chapelle O, 2014, ACM T INTEL SYST TEC, V5, P1, DOI [10.1145/2532128, DOI 10.1145/2532128]; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Guo  Zhaohan, 2017, ADV NEURAL INFORM PR, P2489; Hallak A, 2016, AAAI CONF ARTIF INTE, P1631; Hallak Assaf, 2017, P 34 INT C MACH LEAR, V70, P1372; Hirano K, 2003, ECONOMETRICA, V71, P1161, DOI 10.1111/1468-0262.00442; Jiang N, 2016, PR MACH LEARN RES, V48; Krajzewicz D., 2012, INT J ADV SYST MEASU, V5; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Li L., 2011, PROC 4 ACM INT C WEB, P297, DOI DOI 10.1145/1935826.1935878; Li LH, 2015, JMLR WORKSH CONF PRO, V38, P608; Li Lihong, 2015, P INT WORLD WID WEB, P929, DOI DOI 10.1145/2740908.2742562; Liu H., 2018, P 6 INT C LEARN REPR; Liu J. S., 2001, SPRINGER SERIES STAT; Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Murphy SA, 2001, J AM STAT ASSOC, V96, P1410, DOI 10.1198/016214501753382327; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Peres Yuval, 2017, MARKOV CHAINS MIXING, V107; Precup D., 2001, P 18 INT C MACH LEAR, P417; Precup D., 2000, INT C MACH LEARN, P759; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Scholkopf B., 2001, LEARNING KERNELS SUP; Serfling R. J., 2009, APPROXIMATION THEORE, V162; Strehl A, 2010, ADV NEURAL INFORM PR, V2, P2217; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS., 2016, J MACH LEARN RES, V17, P2603; Tang L, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1587, DOI 10.1145/2505515.2514700; Thomas P., 2016, INT C MACH LEARN, P2139; Thomas PS, 2017, AAAI CONF ARTIF INTE, P4740; Van der Pol E., 2016, NIPS WORKSH LEARN IN; Wang Y. -X., 2017, P 34 INT C MACH LEAR, P3589	42	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305038
C	Liu, TY; Li, SY; Shi, JP; Zhou, EL; Zhao, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Tianyi; Li, Shiyang; Shi, Jianping; Zhou, Enlu; Zhao, Tuo			Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) have been widely used in distributed machine learning, e.g., training large collaborative filtering systems and deep neural networks. Due to current technical limit, however, establishing convergence properties of Async-MSGD for these highly complicated nonoconvex problems is generally infeasible. Therefore, we propose to analyze the algorithm through a simpler but nontrivial nonconvex problems - streaming PCA. This allows us to make progress toward understanding Aync-MSGD and gaining new insights for more general problems. Specifically, by exploiting the diffusion approximation of stochastic optimization, we establish the asymptotic rate of convergence of Async-MSGD for streaming PCA. Our results indicate a fundamental tradeoff between asynchrony and momentum: To ensure convergence and acceleration through asynchrony, we have to reduce the momentum (compared with Sync-MSGD). To the best of our knowledge, this is the first theoretical attempt on understanding Async-MSGD for distributed nonconvex stochastic optimization. Numerical experiments on both streaming PCA and training deep neural networks are provided to support our findings for Async-MSGD.	[Liu, Tianyi; Zhou, Enlu; Zhao, Tuo] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; [Li, Shiyang] Harbin Inst Technol, Harbin, Heilongjiang, Peoples R China; [Shi, Jianping] Sensetime Grp Ltd, Hong Kong, Peoples R China	University System of Georgia; Georgia Institute of Technology; Harbin Institute of Technology	Liu, TY (corresponding author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	tliu341@gatech.edu; lsydevin@gmail.com; shijianping@sensetime.com; enlu.zhou@isye.gatech.edu; tuo.zhao@isye.gatech.edu	Liu, Tianyi/GLU-4923-2022					[Anonymous], 2016, ARXIV161209296; Chen J., 2016, ABS160400981 CORR; CHEN Z., 2017, ARXIV170208134; Ge R., 2016, NEURIPS, P2973; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Lian X., 2016, ADV NEURAL INFORM PR, P3054; LIU T., 2018, ARXIV180205155; Liu WY, 2017, ADV NEUR IN, V30; Mitliagkas I, 2016, ANN ALLERTON CONF, P997, DOI 10.1109/ALLERTON.2016.7852343; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Salakhutdinov R., 2007, P 24 INT C MACHINE L, V227, P791, DOI [DOI 10.1145/1273496.1273596, 10.1145/1273496.1273596]; SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0; SUN J., 2016, INF THEOR ISIT 2016; ZHANG J., 2018, TRAINING, V1, P2; Zhang W., 2015, ARXIV PREPRINT ARXIV	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303066
C	Liu, Y; Gottesman, O; Raghu, A; Komorowski, M; Faisal, A; Doshi-Velez, F; Brunskill, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Yao; Gottesman, Omer; Raghu, Aniruddh; Komorowski, Matthieu; Faisal, Aldo; Doshi-Velez, Finale; Brunskill, Emma			Representation Balancing MDPs for Off-Policy Policy Evaluation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in common synthetic benchmarks and a HIV treatment simulation domain.	[Liu, Yao; Brunskill, Emma] Stanford Univ, Stanford, CA 94305 USA; [Gottesman, Omer; Doshi-Velez, Finale] Harvard Univ, Cambridge, MA 02138 USA; [Raghu, Aniruddh] Univ Cambridge, Cambridge, England; [Komorowski, Matthieu; Faisal, Aldo] Imperial Coll London, London, England	Stanford University; Harvard University; University of Cambridge; Imperial College London	Liu, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yaoliu@stanford.edu; gottesman@fas.harvard.edu; aniruddhraghu@gmail.com; matthieu.komorowski@gmail.com; a.faisal@imperial.ac.uk; finale@seas.harvard.edu; ebrun@cs.stanford.edu	Liu, Yao/ABZ-7322-2022	Liu, Yao/0000-0002-9010-5388	Harvard Data Science Initiative; Siemens; NSF CAREER grant	Harvard Data Science Initiative; Siemens(Siemens AG); NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported in part by the Harvard Data Science Initiative, Siemens, and a NSF CAREER grant.	Alaa Ahmed M., 2017, ADV NEURAL INFORM PR, V30, P3424; Atan Onur, 2018, ARXIV180208679; Brockman G., 2016, OPENAI GYM; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Ernst D, 2006, IEEE DECIS CONTR P, P669; Farajtabar M, 2018, P 35 INT C MACH LEAR, P1447; Guo ZD, 2017, ADV NEUR IN, V30; Hanna JP, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P538; Jiang N, 2016, PR MACH LEARN RES, V48; Johansson F. D., 2018, ARXIV180208598; Johansson FD, 2016, PR MACH LEARN RES, V48; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077; Precup D., 2000, INT C MACH LEARN, P759; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Schulam P, 2017, ADV NEUR IN, V30; Sekhon J. S., 2017, ARXIV170603461; Shalit U, 2017, PR MACH LEARN RES, V70; Sriperumbudur B. K., 2009, ARXIV PREPRINT ARXIV; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Thomas P., 2016, INT C MACH LEARN, P2139; Thomas P. S., 2015, AAAI; Wager Stefan, 2017, J AM STAT ASS; Yoon Jinsung, 2018, ICLR	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302064
C	Madras, D; Pitassi, T; Zemel, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Madras, David; Pitassi, Toniann; Zemel, Richard			Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing learning to defer, which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system.	[Madras, David; Pitassi, Toniann; Zemel, Richard] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Toronto	Madras, D (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	madras@cs.toronto.edu; toni@cs.toronto.edu; zemel@cs.toronto.edu						Attenberg Josh, 2011, HUMAN COMPUTATION, V11; Ba J., 2017, P 3 INT C LEARN REPR; Bechavod Yahav, 2017, ARXIV170700044CSSTAT; Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; Bower Amanda, 2017, ARXIV170700391CSSTAT; Busenitz LW, 1997, J BUS VENTURING, V12, P9, DOI 10.1016/S0883-9026(96)00003-1; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Chow C., 1957, IEEE T C; Chow C., 1970, IEEE T C; Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5; Danziger S, 2011, P NATL ACAD SCI USA, V108, P6889, DOI 10.1073/pnas.1018033108; DAWES RM, 1989, SCIENCE, V243, P1668, DOI 10.1126/science.2648573; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Fischer Lydia, 2016, PROBABILISTIC CLASSI; Grgic-Hlaca Nina, 2017, ARXIV170610208CSSTAT; Guo CA, 2017, PR MACH LEARN RES, V70; Hadfield-Menell D, 2016, ADV NEURAL INFORM PR, V29, P3909; Hannah-Moffat K, 2013, JUSTICE Q, V30, P270, DOI 10.1080/07418825.2012.682603; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Joseph Matthew, 2016, NIPS, P325; Kamiran F., 2009, COMPUTER CONTROL COM, P1, DOI 10.1109/IC4.2009.4909197; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Kirchner Lauren, 2016, WE ANAL COMPAS RECID; Kleinberg Jon, 2016, ARXIV160905807CSSTAT; Maddison Chris J, 2016, ARXIV161100712; Menon Aditya Krishna, 2018, P C FAIRNESS ACCOUNT, P107; Pleiss G., 2017, ADV NEURAL INFORM PR, P5680; Selbst Andrew D, 2018, ACM C FAIRN ACC TRAN; Varshney KR, 2017, BIG DATA-US, V5, P246, DOI 10.1089/big.2016.0051; Xin Wang, 2017, ARXIV170600885CS; Zemel R., 2013, P INT C MACH LEARN, P325	35	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000063
C	Malek, A; Bartlett, PL		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Malek, Alan; Bartlett, Peter L.			Horizon-Independent Minimax Linear Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LOSS BOUNDS; PREDICTION; GRADIENT	We consider online linear regression: at each round, an adversary reveals a covariate vector, the learner predicts a real value, the adversary reveals a label, and the learner suffers the squared prediction error. The aim is to minimize the difference between the cumulative loss and that of the linear predictor that is best in hindsight. Previous work demonstrated that the minimax optimal strategy is easy to compute recursively from the end of the game; this requires the entire sequence of covariate vectors in advance. We show that, once provided with a measure of the scale of the problem, we can invert the recursion and play the minimax strategy without knowing the future covariates. Further, we show that this forward recursion remains optimal even against adaptively chosen labels and covariates, provided that the adversary adheres to a set of constraints that prevent misrepresentation of the scale of the problem. This strategy is horizon-independent in that the regret and minimax strategies depend on the size of the constraint set and not on the time-horizon, and hence it incurs no more regret than the optimal strategy that knows in advance the number of rounds of the game. We also provide an interpretation of the minimax algorithm as a follow-the-regularized-leader strategy with a data-dependent regularizer and obtain an explicit expression for the minimax regret.	[Malek, Alan] MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Bartlett, Peter L.] Univ Calif Berkeley, Dept EECS & Stat, Berkeley, CA 94720 USA	Massachusetts Institute of Technology (MIT); University of California System; University of California Berkeley	Malek, A (corresponding author), MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	amalek@mit.edu; bartlett@cs.berkeley.edu			NSF [IIS-1619362]	NSF(National Science Foundation (NSF))	We gratefully acknowledge the support of the NSF through grant IIS-1619362.	[Anonymous], 1997, MATRIX ALGEBRA STAT; Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; BARTLETT P. L., 2015, C LEARN THEORY, P226; CesaBianchi N, 1996, IEEE T NEURAL NETWOR, V7, P604, DOI 10.1109/72.501719; Forster J, 1999, LECT NOTES COMPUT SC, V1684, P269; FOSTER DP, 1991, ANN STAT, V19, P1084, DOI 10.1214/aos/1176348140; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Koolen WM, 2014, ADV NEURAL INFORM PR, P3230; Koolen Wouter M., 2015, ADV NEURAL INFORM PR, V28, P2548; Moroshko E, 2014, THEOR COMPUT SCI, V558, P107, DOI 10.1016/j.tcs.2014.09.028; TAKIMOTO EIJI, 2000, 13 COLT, P100; Vovk V, 1998, ADV NEUR IN, V10, P364; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305029
C	Mandal, A; Jiang, H; Shrivastava, A; Sarkar, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mandal, Ankush; Jiang, He; Shrivastava, Anshumali; Sarkar, Vivek			Topkapi: Parallel and Fast Sketches for Finding Top-K Frequent Elements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Identifying the top-K frequent items in a collection or data stream is one of the most common and important operations in large data processing systems. As a result, several solutions have been proposed to solve this problem approximately. We observe that the existing algorithms, although theoretically sound, are suboptimal from the performance perspective because of their limitations in exploiting parallelism in modern distributed compute settings. In particular, for identifying top-K frequent items, Count-Min Sketch (CMS) has an excellent update time, but lacks the important property of reducibility which is needed for exploiting available massive data parallelism. On the other end, the popular Frequent algorithm (FA) leads to reducible summaries but its update costs are significant. In this paper, we present Topkapi, a fast and parallel algorithm for finding top-K frequent items, which gives the best of both worlds, i.e., it is reducible and has fast update time similar to CMS. Topkapi possesses strong theoretical guarantees and leads to significant performance gains due to increased parallelism, relative to past work.	[Mandal, Ankush; Sarkar, Vivek] Georgia Inst Technol, Sch Comp Sci, Atlanta, GA 30332 USA; [Jiang, He; Shrivastava, Anshumali] Rice Univ, Dept Comp Sci, Houston, TX USA	University System of Georgia; Georgia Institute of Technology; Rice University	Mandal, A (corresponding author), Georgia Inst Technol, Sch Comp Sci, Atlanta, GA 30332 USA.	ankush@gatech.edu; cary.jiang@rice.edu; anshumali@rice.edu; vsarkar@gatech.edu			AFOSR-YIP [FA9550-18-1-0152]; BRC grant for Randomized Numerical Linear Algebra; Amazon Research Award; Data Analysis and Visualization Cyberinfrastructure - NSF [OCI-0959097]; Rice University;  [NSF-1629459];  [NSF-1652131];  [NSF-1838177]	AFOSR-YIP(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); BRC grant for Randomized Numerical Linear Algebra; Amazon Research Award; Data Analysis and Visualization Cyberinfrastructure - NSF(National Science Foundation (NSF)); Rice University; ; ; 	This work was supported in part by NSF-1629459, NSF-1652131, NSF-1838177, AFOSR-YIP FA9550-18-1-0152, BRC grant for Randomized Numerical Linear Algebra, Amazon Research Award, and Data Analysis and Visualization Cyberinfrastructure funded by NSF under grant OCI-0959097 and Rice University.	Agarwal P. K., 2012, P 31 ACM SIGMOD SIGA, P23, DOI DOI 10.1145/2213556.2213562; Berinde R, 2010, ACM T DATABASE SYST, V35, DOI 10.1145/1862919.1862923; Cafaro M, 2017, 2017 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING & SIMULATION (HPCS), P707, DOI 10.1109/HPCS.2017.108; Cafaro M, 2016, INFORM SCIENCES, V329, P1, DOI 10.1016/j.ins.2015.09.003; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Cormode G, 2010, VLDB J, V19, P3, DOI 10.1007/s00778-009-0172-z; Demaine ED, 2002, LECT NOTES COMPUT SC, V2461, P348; Karp RM, 2003, ACM T DATABASE SYST, V28, P51, DOI 10.1145/762471.762473; McMahan HB, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1222; Metwally A, 2005, LECT NOTES COMPUT SC, V3363, P398, DOI 10.1007/978-3-540-30570-5_27; Metwally A, 2006, ACM T DATABASE SYST, V31, P1095, DOI 10.1145/1166074.1166084; Roy P., 2012, P 18 ACM SIGKDD INT, P1451; Shrivastava A, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1417, DOI 10.1145/2882903.2882946; Singh Manku G., 2002, Proceedings of the Twenty-eighth International Conference on Very Large Data Bases, P346; Yang X, 2016, INT C INTEL HUM MACH, P225, DOI 10.1109/IHMSC.2016.123; Yi K, 2013, ALGORITHMICA, V65, P206, DOI 10.1007/s00453-011-9584-4	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005048
C	Marom, O; Rosman, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Marom, Ofir; Rosman, Benjamin			Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Object-oriented representations in reinforcement learning have shown promise in transfer learning, with previous research introducing a propositional object-oriented framework that has provably efficient learning bounds with respect to sample complexity. However, this framework has limitations in terms of the classes of tasks it can efficiently learn. In this paper we introduce a novel deictic object-oriented framework that has provably efficient learning bounds and can solve a broader range of tasks. Additionally, we show that this framework is capable of zero-shot transfer of transition dynamics across tasks and demonstrate this empirically for the Taxi and Sokoban domains.	[Marom, Ofir; Rosman, Benjamin] Univ Witwatersrand, Johannesburg, South Africa; [Rosman, Benjamin] CSIR, Pretoria, South Africa	University of Witwatersrand; Council for Scientific & Industrial Research (CSIR) - South Africa	Marom, O (corresponding author), Univ Witwatersrand, Johannesburg, South Africa.		Rosman, Benjamin/ABF-3933-2020	Rosman, Benjamin/0000-0002-0284-4114	Google Travel Grant; Google Conference Grant	Google Travel Grant(Google Incorporated); Google Conference Grant(Google Incorporated)	The authors with to thank Google Travel and Conference Grants for their support. The authors also wish to thank the anonymous reviewers for their thorough feedback and helpful comments.	[Anonymous], 2003, IJCAI; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Chen JM, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P3056, DOI 10.1109/ICMLC.2008.4620932; Dietterich T., 1998, P 15 INT C MACH LEAR; Diuk C., 2010, THESIS; Diuk C., 2008, ICML, DOI [10.1145/1390156.1390187, DOI 10.1145/1390156.1390187]; Dzeroski S, 2001, MACH LEARN, V43, P7, DOI 10.1023/A:1007694015589; Finney S., 2002, TECHNICAL REPORT; Kansky K, 2017, PR MACH LEARN RES, V70; Konidaris G, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P895; RAVINDRAN B, 2003, P 12 YAL WORKSH AD L, P109; Scholz J, 2014, PR MACH LEARN RES, V32, P1089; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Taylor ME, 2009, J MACH LEARN RES, V10, P1633	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302031
C	Maron, H; Lipman, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Maron, Haggai; Lipman, Yaron			(Probably) Concave Graph Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we address the graph matching problem. Following the recent works of Zaslayskiy et al. (2009); Vestner et al. (2017) we analyze and generalize the idea of concave relaxations. We introduce the concepts of conditionally concave and probably conditionally concave energies on polytopes and show that they encapsulate many instances of the graph matching problem, including matching Euclidean graphs and graphs on surfaces. We further prove that local minima of probably conditionally concave energies on general matching polytopes (e.g., doubly stochastic) are with high probability extreme points of the matching polytope (e.g., permutations).	[Maron, Haggai; Lipman, Yaron] Weizmann Inst Sci, Rehovot, Israel	Weizmann Institute of Science	Maron, H (corresponding author), Weizmann Inst Sci, Rehovot, Israel.	haggai.maron@weizmann.ac.il; yaron.lipman@weizmann.ac.il			European Research Council (ERC Consolidator Grant) [771136]; Israel Science Foundation [1830/17]	European Research Council (ERC Consolidator Grant)(European Research Council (ERC)); Israel Science Foundation(Israel Science Foundation)	The authors would like to thank Boaz Nadler, Omri Sarig, Vova Kim and Uri Bader for their helpful remarks and suggestions. This research was supported in part by the European Research Council (ERC Consolidator Grant, "LiftMatch" 771136) and the Israel Science Foundation (Grant No. 1830/17). The authors would also like to thank Tomer Stern and Eli Zelzer for the bone scans.	Aflalo Y, 2015, P NATL ACAD SCI USA, V112, P2942, DOI 10.1073/pnas.1401651112; ALMOHAMAD HA, 1993, IEEE T PATTERN ANAL, V15, P522, DOI 10.1109/34.211474; Berg AC, 2005, PROC CVPR IEEE, P26; Bernard F, 2017, ARXIV171110733; Bernard F, 2016, PROC SPIE, V9784, DOI 10.1117/12.2206024; Bogomolny E, 2007, ARXIV07102063; Boyarski A, 2017, ARXIV170708991; Burkard R.E., 1998, HDB COMBINATORIAL OP; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Cour T., 2007, P ADV NEURAL INFORM, P313; Dym N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130826; Fiori M, 2015, INF INFERENCE, V4, P63, DOI 10.1093/imaiai/iav002; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Funkhouser T., 2006, PROC EUROGRAPHICS S, P131; Giorgi D., 2007, SHREC COMPETITION, V8; Guo YR, 2013, IEEE T MED IMAGING, V32, P268, DOI 10.1109/TMI.2012.2223710; Huet B, 1999, ADV NEUR IN, V11, P896; IMHOF JP, 1961, BIOMETRIKA, V48, P419, DOI 10.1093/biomet/48.3-4.419; Kezurer I, 2015, COMPUT GRAPH FORUM, V34, P115, DOI 10.1111/cgf.12701; Kim J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966403; Kruskal JB, 1978, MULTIDIMENSIONAL SCA, V11; Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032; Lyzinski V, 2016, IEEE T PATTERN ANAL; Rodola E, 2013, IEEE I CONF COMP VIS, P1169, DOI 10.1109/ICCV.2013.149; Rudelson M, 2013, ELECT COMMUNICATIONS, V18; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778; Vestner M, 2017, PROC CVPR IEEE, P6681, DOI 10.1109/CVPR.2017.707; Vogelstein JT, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121002; Wendland H., 2004, SCATTERED DATA APPRO, V17; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300038
C	Mattei, PA; Frellsen, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mattei, Pierre-Alexandre; Frellsen, Jes			Leveraging the Exact Likelihood of Deep Latent Variable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MAXIMUM-LIKELIHOOD; CONSISTENCY; INFERENCE; NETWORKS	Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a DLVM. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs.	[Mattei, Pierre-Alexandre; Frellsen, Jes] IT Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark	IT University Copenhagen	Mattei, PA (corresponding author), IT Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	pima@itu.dk; jefr@itu.dk	Frellsen, Jes/K-4337-2014	Frellsen, Jes/0000-0001-9224-1271				Arora Sanjeev, 2018, ICLR; Bartholomew D.J., 2011, LATENT VARIABLE MODE, V904; Biernacki C., 2011, PUB IRMA LILLE, V71; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Bowman S., 2016, P CONLL; Burda Yuri, 2016, INT C LEARN REPR; Chen JH, 2017, STAT SCI, V32, P47, DOI 10.1214/16-STS578; Cremer C., 2017, INT C LEARN REPR WOR; Cremer C., 2018, P 35 INT C MACH LEAR; Dai B, 2018, J MACH LEARN RES, V19; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2732; Dinh Laurent., 2017, INT C LEARN REPR; Du CL, 2019, IEEE T SYST MAN CY-S, V49, P364, DOI 10.1109/TSMC.2018.2815032; Gelman A., 1993, COMPUTING SCI STAT, P433; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gomez-Bombarelli Rafael, 2018, ACS CENTRAL SCI; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gronbech C. H., 2018, BIORXIV; Grosse R. B., 2015, ARXIV151102543; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; HATHAWAY RJ, 1985, ANN STAT, V13, P795, DOI 10.1214/aos/1176349557; Heckerman D, 2001, J MACH LEARN RES, V1, P49, DOI 10.1162/153244301753344614; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Jimenez Rezende D., 2016, ADV NEURAL INFORM PR, P4996; Jolliffe IT, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0202; KIEFER J, 1956, ANN MATH STAT, V27, P887, DOI 10.1214/aoms/1177728066; Kingma D. P, 2017, P INT C LEARN REPR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kusner MJ, 2017, PR MACH LEARN RES, V70; LECAM L, 1990, INT STAT REV, V58, P153; LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5; Li C., 2016, P 33 INT C MACH LEAR, P1177; LINDSAY BG, 1983, ANN STAT, V11, P86, DOI 10.1214/aos/1176346059; LINDSAY BG, 1995, REGIONAL C SERIES PR, V5; Lucas T., 2018, INT C MACH LEARN; Maaloe L, 2016, PR MACH LEARN RES, V48; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Pu Y., 2016, ADV NEURAL INF PROCE, P2352; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende D., 2015, ICML, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rezende Danilo Jimenez, 2018, ARXIV181000597; Richardson E, 2018, ADV NEURAL INFORM PR; Roeder G., 2017, ADV NEURAL INFORM PR, P6928; Sonderby CK, 2016, ADV NEUR IN, V29; Takahashi H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2696; Tikhonov A.N., 1977, SOLUTION ILL POSED P; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tomczak JM, 2018, PR MACH LEARN RES, V84; van de Geer S, 2003, COMPUT STAT DATA AN, V41, P453, DOI 10.1016/S0167-9473(02)00188-3; VANDERVAART AW, 1992, J MULTIVARIATE ANAL, V43, P133, DOI 10.1016/0047-259X(92)90113-T; Wang Y, 2007, J R STAT SOC B, V69, P185, DOI 10.1111/j.1467-9868.2007.00583.x; Welling M, 2014, AUTOENCODING VARIATI; Zhao S., 2017, INT C MACH LEARN PML, P4091	57	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303082
C	Matyasko, A; Chau, LP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Matyasko, Alexander; Chau, Lap-Pui			Improved Network Robustness with Adversary Critic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Ideally, what confuses neural network should be confusing to humans. However, recent experiments have shown that small, imperceptible perturbations can change the network prediction. To address this gap in perception, we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN), where the adversarial attack on classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments, we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally, we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing.	[Matyasko, Alexander; Chau, Lap-Pui] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Matyasko, A (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.	aliaksan001@ntu.edu.sg; elpchau@ntu.edu.sg	Chau, Lap-Pui/A-5149-2011		National Research Foundation, Singapore; Infocomm Media Development Authority, Singapore	National Research Foundation, Singapore(National Research Foundation, Singapore); Infocomm Media Development Authority, Singapore	This work was carried out at the Rapid-Rich Object Search (ROSE) Lab at Nanyang Technological University (NTU), Singapore. The ROSE Lab is supported by the National Research Foundation, Singapore, and the Infocomm Media Development Authority, Singapore. We thank NVIDIA Corporation for the donation of the GeForce Titan X and GeForce Titan X (Pascal) used in this research. We also thank all the anonymous reviewers for their valuable comments and suggestions.	Baluja S, 2018, AAAI CONF ARTIF INTE, P2687; Bengio  Yoshua, 2015, ARXIV13083432; Bertsimas D, 2018, MATH PROGRAM, V167, P235, DOI 10.1007/s10107-017-1125-8; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009; Chongxuan L., 2017, ADV NEURAL INFORM PR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Elsayed G., 2018, ADV NEURAL INFORM PR, P842; Feinman R., 2017, ARXIV PREPRINT ARXIV; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Kabkab M., 2018, P ICLR; Kingma D.P, P 3 INT C LEARNING R; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Lee H., 2017, GENERATIVE ADVERSARI; Madry A., 2018, P ICLR VANC BC CAN; Matyasko  Alexander, 2017, IJCNN; Metzen J.H., 2017, ICLR; Miyato T., 2015, ARXIV PREPRINT ARXIV; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Negahban Sahand, 2015, ARXIV PREPRINT ARXIV; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; The Tensorflow Development Team, 2015, ARXIV160304467 TENS; Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131; Xu Huan, 2009, J MACHINE LEARNING R; Xu  Huan, 2009, NIPS; Zheng Stephan, 2016, P CVPR; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005018
C	Meila, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Meila, Marina			How to tell when a clustering is (approximately) correct using convex relaxations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				AUGMENTED LAGRANGIAN METHOD	We introduce the Sublevel Set (SS) method, a generic method to obtain sufficient guarantees of near-optimality and uniqueness (up to small perturbations) for a clustering. This method can be instantiated for a variety of clustering loss functions for which convex relaxations exist. Obtaining the guarantees in practice amounts to solving a convex optimization. We demonstrate the applicability of this method by obtaining distribution free guarantees for K-means clustering on realistic data sets.	[Meila, Marina] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Meila, M (corresponding author), Univ Washington, Seattle, WA 98195 USA.	mmp@stat.washington.edu			NSF DMS PD [08-1269]; NSF [IIS-0313339]	NSF DMS PD; NSF(National Science Foundation (NSF))	The author gratefully acknowledges Maryam Fazel for her early interest in this work, for reading a previous version of this paper and for many discussions; the Simons Institute for the Theory of Computing where part of this research was performed, and partial support from the NSF DMS PD 08-1269 and NSF IIS-0313339 awards.	Abbe E., 2016, ADV NEURAL INFORM PR, P1334; Abbe E., 2015, ARXIV150300609; Achlioptas D, 2005, LECT NOTES COMPUT SC, V3559, P458, DOI 10.1007/11503415_31; Ahmadian Sara, 2016, LIPICS, V55; Awasthi P., 2014, ARXIV E PRINTS; Awasthi P, 2014, PR MACH LEARN RES, V32, P550; Awasthi P, 2012, INFORM PROCESS LETT, V112, P49, DOI 10.1016/j.ipl.2011.10.006; Awasthi Pranjal, 2010, ARXIV10093594; Awasthi Pranjal, 2015, ARXIV150203316, V34, P754; Bach F., 2007, P NIPS, P49; Balakrishnan Sivaraman, 2014, ABS14082156 CORR; Balcan MF, 2016, SIAM J COMPUT, V45, P102, DOI 10.1137/140981575; Balcan MF, 2014, J MACH LEARN RES, V15, P3831; Balcan Maria- Florina, 2016, 43 INT C AUT LANG PR; Ben- David Shai, 2015, ARXIV150100437; Bilu Y., 2009, ABS09063162 CORR; Bubeck S, 2012, ESAIM-PROBAB STAT, V16, P436, DOI 10.1051/ps/2012013; Charikar M., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P378, DOI 10.1109/SFFCS.1999.814609; Charikar Moses, 2016, 160909548 ARXIV; Chen Y., 2014, ARXIV14021267; Chmiela S, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1603015; Cole Sam, 2015, ABS150300423 CORR; Dasgupta S, 2007, J MACH LEARN RES, V8, P203; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; Dasgupta Sanjoy, 2014, UAI 00, P143; Deshpande Y., 2015, ARXIV E PRINTS; Ding C., 2004, P INT MACH LEARN C I; Hazan Elad, 2016, ABS161001132 CORR; Hein M., 2011, ADV NEURAL INFORM PR, V24, P2366; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Iguchi T., 2015, ARXIV E PRINTS; Iguchi T., 2015, PROBABLY CERTIFIABLY; Jalali A., 2016, P NIPS 2016 DEC; KANNAN R, 2000, P 41 S FDN COMP SCI; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; Lee J. R., 2014, ARXIV11111055; Meila M., 2006, P 23 INT C MACH LEAR, P625, DOI DOI 10.1145/1143844.1143923; Meila M., 2005, P ART INT STAT WORKS; Meila M, 2012, MACH LEARN, V86, P369, DOI 10.1007/s10994-011-5267-2; Peng Jiming, 2007, SIAM J OPTIMIZATION; PENG R., 2015, PARTITIONING WELL CL, V40, P1; Qin T., 2013, ADV NEURAL INFORM PR; Rangapuram Syama Sundar, 2014, ADV NEURAL INFORM PR, V27, P3131; Roy Aurko, 2016, ADV NEURAL INFORM PR; Swamy Chaitanya, 2004, P S DISCRETE ALGORIT, P526; Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jcss.2003.11.008; Vinayak R. Korlakai, 2014, ADV NEURAL INFORM PR, P2996; Wan Yali, 2015, ADV NEURAL INFORM PR; Xing Eric P., 2003, UCBCSD031265 EECS DE; Yang LQ, 2015, MATH PROGRAM COMPUT, V7, P331, DOI 10.1007/s12532-015-0082-6; Zhao XY, 2010, SIAM J OPTIMIZ, V20, P1737, DOI 10.1137/080718206	52	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001092
C	Mhammedi, Z; Williamson, RC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mhammedi, Zakaria; Williamson, Robert C.			Constant Regret, Generalized Mixability, and Mirror Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PREDICTION	We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and for the right choice of loss function and "mixing" algorithm, it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example, a constant regret can be achieved for mixable losses using the aggregating algorithm. The Generalized Aggregating Algorithm (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the aggregating algorithm when using the Shannon entropy S. For a given entropy Phi, losses for which a constant regret is possible using the GAA are called Phi-mixable. Which losses are Phi-mixable was previously left as an open question. We fully characterize Phi-mixability and answer other open questions posed by [6]. We show that the Shannon entropy S is fundamental in nature when it comes to mixability; any Phi-mixable loss is necessarily S-mixable, and the lowest worst-case regret of the GAA is achieved using the Shannon entropy. Finally, by leveraging the connection between the mirror descent algorithm and the update step of the GAA, we suggest a new adaptive generalized aggregating algorithm and analyze its performance in terms of the regret bound.	[Mhammedi, Zakaria; Williamson, Robert C.] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia; [Mhammedi, Zakaria; Williamson, Robert C.] DATA61, Sydney, NSW, Australia	Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Mhammedi, Z (corresponding author), Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia.; Mhammedi, Z (corresponding author), DATA61, Sydney, NSW, Australia.	zak.mhammedi@anu.edu.au; bob.williamson@anu.edu.au			Australian Research Council [DATA61]	Australian Research Council(Australian Research Council)	This work was supported by the Australian Research Council and DATA61.	Chernov A, 2010, THEOR COMPUT SCI, V411, P2647, DOI 10.1016/j.tcs.2010.04.003; Hiriart-Urruty J. B., 2001, FUNDAMENTALS CONVEX; Kalnishkan Y, 2004, THEOR COMPUT SCI, V313, P195, DOI 10.1016/j.tcs.2003.11.005; Kamalaruban P., 2015, C LEARN THEOR, P1035; Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8; Reid M.D., 2015, C LEARN THEOR, P1501; Rockafellar R. T., 1997, CONVEX ANAL; Steinhardt Jacob., 2014, ICML, P1593; van Erven T, 2012, J MACH LEARN RES, V13, P1639; Vovk V, 2001, INT STAT REV, V69, P213; Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556; Vovk V, 2009, J MACH LEARN RES, V10, P2445; Williamson R. C., 2014, C LEARN THEOR COLT, V35, P1078; Williamson Robert C., 2016, J MACHINE LEARNING R, V17	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002001
C	Michoel, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Michoel, Tom			Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				VARIABLE SELECTION; REGRESSION; REGULARIZATION; MODELS	The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve regression shrinkage and variable selection, allowing the inference of robust models from large data sets. However, there has been limited success in deriving estimates for the full posterior distribution of regression coefficients in these models, due to a need to evaluate analytically intractable partition function integrals. Here, the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over "regression frequencies". This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients, and allows for Bayesian inference of much higher dimensional models than previously possible.	[Michoel, Tom] Univ Edinburgh, Roslin Inst, Edinburgh, Midlothian, Scotland; [Michoel, Tom] Univ Bergen, Computat Biol Unit, Dept Informat, Bergen, Norway	UK Research & Innovation (UKRI); Biotechnology and Biological Sciences Research Council (BBSRC); Roslin Institute; University of Edinburgh; University of Bergen	Michoel, T (corresponding author), Univ Edinburgh, Roslin Inst, Edinburgh, Midlothian, Scotland.; Michoel, T (corresponding author), Univ Bergen, Computat Biol Unit, Dept Informat, Bergen, Norway.	tom.michoel@uib.no			BBSRC [BB/J004235/1, BB/M020053/1]	BBSRC(UK Research & Innovation (UKRI)Biotechnology and Biological Sciences Research Council (BBSRC))	This research was supported by the BBSRC (grant numbers BB/J004235/1 and BB/M020053/1).	Barretina J, 2012, NATURE, V483, P603, DOI 10.1038/nature11003; Boyd S, 2004, CONVEX OPTIMIZATION; DANIELS HE, 1954, ANN MATH STAT, V25, P631, DOI 10.1214/aoms/1177728652; Drton M, 2017, J R STAT SOC B, V79, P323, DOI 10.1111/rssb.12187; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Friedman J, 2001, SPRINGER SERIES STAT, V10; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Hans C, 2011, J AM STAT ASSOC, V106, P1383, DOI 10.1198/jasa.2011.tm09241; Hans C, 2009, BIOMETRIKA, V96, P835, DOI 10.1093/biomet/asp047; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; KASS RE, 1989, J AM STAT ASSOC, V84, P717, DOI 10.2307/2289653; Lang  Serge, 2002, GRADUATE TEXTS MATH, V103; Lee JS, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/11/P11002; Li Q, 2010, BAYESIAN ANAL, V5, P151, DOI 10.1214/10-BA506; Litvinov Grigory Lazarevich, 2005, MATH0507014 ARXIV; Liu J.S., 2004, MONTE CARLO STRATEGI; Makalic E., 2016, ARXIV161106649; Mallick Himel, 2013, J Biom Biostat, V1, P005; Michoel T, 2016, COMPUT STAT DATA AN, V97, P60, DOI 10.1016/j.csda.2015.11.009; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Park T, 2008, J AM STAT ASSOC, V103, P681, DOI 10.1198/016214508000000337; Rajaratnam B, 2015, ARXIV150800947; Rajaratnam  Bala, 2015, ARXIV150903697; Rakitsch B, 2013, BIOINFORMATICS, V29, P206, DOI 10.1093/bioinformatics/bts669; Rockafellar R. T., 1970, CONVEX ANAL; Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x; Schneidemann  Volker, 2005, INTRO COMPLEX ANAL S; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; van Zon R, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.056121; Watanabe S, 2013, J MACH LEARN RES, V14, P867; Wong R., 2001, ASYMPTOTIC APPROXIMA, V34; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	36	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302076
C	Mindermann, S; Armstrong, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mindermann, Soren; Armstrong, Stuart			Occam's razor is insufficient to infer the preferences of irrational agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RATIONALITY	Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple 'normative' assumptions, which cannot be deduced exclusively from observations.	[Mindermann, Soren] Univ Toronto, Vector Inst, Toronto, ON, Canada; [Armstrong, Stuart] Univ Oxford, Future Humanity Inst, Oxford, England; [Mindermann, Soren] Future Human Inst, Oxford, England; [Armstrong, Stuart] Machine Intelligence Res Inst, Berkeley, CA USA	University of Toronto; University of Oxford	Mindermann, S (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	soeren.mindermann@gmail.com; stuart.armstrong@philosophy.ox.ac.uk			Alexander Tamas programme on AI safety research; Leverhulme Trust; Machine Intelligence Research Institute	Alexander Tamas programme on AI safety research; Leverhulme Trust(Leverhulme Trust); Machine Intelligence Research Institute	We wish to thank Laurent Orseau, Xavier O'Rourke, Jan Leike, Shane Legg, Nick Bostrom, Owain Evans, Jelena Luketina, Tom Everrit, Jessica Taylor, Paul Christiano, Eliezer Yudkowsky, Stuart Russell, Dylan Hadfield-Menell, and Anders Sandberg, Adam Gleave, Rohin Shah, among many others. This work was supported by the Alexander Tamas programme on AI safety research, the Leverhulme Trust, and the Machine Intelligence Research Institute.	Abbeel Pieter, 2004, APPRENTICESHIP LEARN; Allender Eric., 2001, P LECT NOTES COMPUTE, V2245, P1, DOI [10.1007/3-540-45294-X_1, DOI 10.1007/3-540-45294-X_1]; Amin Kareem, 2016, RESOLVING UNIDENTIFI; Amodei D., 2016, CONCRETE PROBLEMS SA; [Anonymous], [No title captured]; [Anonymous], 2011, THINKING FAST SLOW; [Anonymous], 2019, INT C LEARN RE UNPUB; [Anonymous], [No title captured]; Ariely Dan, 2004, PSYCHOL EC DECISIONS, V2, P131; Bostrom N., 2014, SUPERINTELLIGENCE PA; Boularias Abdeslam, 2011, RELATIVE ENTROPY INV; Bruck J., 1999, EUROPEAN J ARCHAEOLO, V2, P313, DOI [10.1179/eja.1999.2.3.313, DOI 10.1179/EJA.1999.2.3.313]; Calude C., 2002, INFORM RANDOMNESS AL; Cundy Chris, 2018, ARXIV180705037; Evans Owain, 2015, 30 AAAI C ART INT; Evans Owain, 2015, NIPS WORKSH BOUND OP, P16; Everitt T, 2016, LECT NOTES ARTIF INT, V9782, P12, DOI 10.1007/978-3-319-41649-6_2; Everitt T, 2014, 2014 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC), P167, DOI 10.1109/CEC.2014.6900546; Everitt Tom, 2017, REINFORCEMENT LEARNI; Glimcher PW, 2009, NEUROECONOMICS: DECISION MAKING AND THE BRAIN, P1; Griffiths TL, 2015, TOP COGN SCI, V7, P217, DOI 10.1111/tops.12142; Hadfield-Menell Dylan, 2016, ARXIV160603137CS; Hadfield-Menell Dylan, 2017, ADV NEURAL INFORM PR, P6749; Hula A, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004254; Hume David, 1952, TREATISE HUMAN NATUR, V2; KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185; Kahneman D, 2016, HARVARD BUS REV, V94, P38; Kolmogorov A. N., 1968, International Journal of Computer Mathematics, V2, P157, DOI 10.1080/00207166808803030; Leike J, 2017, ARXIV171109883; LEVIN LA, 1984, INFORM CONTROL, V61, P15, DOI 10.1016/S0019-9958(84)80060-1; Lewis RL, 2014, TOP COGN SCI, V6, P279, DOI 10.1111/tops.12086; MILLER JG, 1984, J PERS SOC PSYCHOL, V46, P961; Milli Smitha, 2017, ARXIV170509990; Ming L. I., 2014, ALGORITHMS COMPLEXIT, V1, P187; Minsky Marvin, 1984, AFTERWORD VERN UNPUB; Moravec H, 1988, MIND CHILDREN FUTURE; Muehlhauser L., 2012, SINGULARITY HYPOSTHE, P101, DOI [10.1007/978-3-642-32560-1_6, DOI 10.1007/978-3-642-32560-1_6]; Muller M, 2010, THEOR COMPUT SCI, V411, P113, DOI 10.1016/j.tcs.2009.09.017; Nisbett RE, 2001, PSYCHOL REV, V108, P291, DOI 10.1037//0033-295X.108.2.291; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Rawls J., 1999, THEORY JUSTICE; Russell S, 2016, SYNTH LIBR, V376, P7, DOI 10.1007/978-3-319-26485-1_2; Russell S, 2015, AI MAG, V36, P105, DOI 10.1609/aimag.v36i4.2577; Samuelson PA, 1948, ECONOMICA-NEW SER, V15, P243, DOI 10.2307/2549561; Schmidhuber J., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P216; SCHUMAN H, 1983, AM SOCIOL REV, V48, P112, DOI 10.2307/2095149; Scopelliti I, 2015, MANAGE SCI, V61, P2468, DOI 10.1287/mnsc.2014.2096; Shiarlis Kyriacos, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1541, DOI 10.1109/ICRA.2017.7989184; SLOVIC P, 1974, BEHAV SCI, V19, P368, DOI 10.1002/bs.3830190603; Tversky A, 1975, UTILITY PROBABILITY, P141, DOI DOI 10.1007/978-94-010-1834-0_8; Wolpert D. H., 1997, IEEE T EVOLUT COMPUT, V1, P1; Yudkowsky Eliezer, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P388, DOI 10.1007/978-3-642-22887-2_48; Ziebart B. D., 2008, AAAI, V8, P1433	54	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000013
C	Morais, MJ; Pillow, JW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Morais, Michael J.; Pillow, Jonathan W.			Power-law efficient neural codes provide general link between perceptual bias and discriminability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FISHER INFORMATION; MUTUAL INFORMATION	Recent work in theoretical neuroscience has shown that efficient neural codes, which allocate neural resources to maximize the mutual information between stimuli and neural responses, give rise to a lawful relationship between perceptual bias and discriminability in psychophysical measurements (Wei & Stocker 2017, [1]). Here we generalize these results to show that the same law arises under a much larger family of optimal neural codes, which we call power-law efficient codes. These codes provide a unifying framework for understanding the relationship between perceptual bias and discriminability, and how it depends on the allocation of neural resources. Specifically, we show that the same lawful relationship between bias and discriminability arises whenever Fisher information is allocated proportional to any power of the prior distribution. This family includes neural codes that are optimal for minimizing Lp error for any p, indicating that the lawful relationship observed in human psychophysical data does not require information-theoretically optimal neural codes. Furthermore, we derive the exact constant of proportionality governing the relationship between bias and discriminability for different choices of power law exponent q, which includes information-theoretic (q = 2) as well as "discrimax" (q = 1/2) neural codes, and different choices of decoder. As a bonus, our framework provides new insights into "anti-Bayesian" perceptual biases, in which percepts are biased away from the center of mass of the prior. We derive an explicit formula that clarifies precisely which combinations of neural encoder and decoder can give rise to such biases.	[Morais, Michael J.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA	Princeton University; Princeton University	Morais, MJ (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	mjmorais@princeton.edu; pillow@princeton.edu			NSF Graduate Research Fellowship; McKnight Foundation; NSF CAREER Award [IIS-1150186]; Simons Collaboration on the Global Brain [SCGB AWD1004351]	NSF Graduate Research Fellowship(National Science Foundation (NSF)); McKnight Foundation; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Simons Collaboration on the Global Brain	We thank David Zoltowski and Nicholas Roy for helpful comments. MJM was supported by an NSF Graduate Research Fellowship; JWP was supported by grants from the McKnight Foundation, Simons Collaboration on the Global Brain (SCGB AWD1004351) and NSF CAREER Award (IIS-1150186).	BARRETT HH, 1993, P NATL ACAD SCI USA, V90, P9758, DOI 10.1073/pnas.90.21.9758; Bercher JF, 2010, AIP CONF PROC, V1305, P208; Bercher JF, 2009, PHYS LETT A, V373, P3235, DOI 10.1016/j.physleta.2009.07.015; Bercher Jean-Francois, 2012, J PHYS A, V45, P255; Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115; CAMPBELL LL, 1965, INFORM CONTROL, V8, P423, DOI 10.1016/S0019-9958(65)90332-3; Fechner Gustav Theodor, 1966, ELEMENTS PSYCHOPHYSI, V1; Ganguli D, 2016, ARXIV160300058; Ganguli Deep, 2010, Adv Neural Inf Process Syst, V2010, P658; Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638; Pillow JW, 2015, NAT NEUROSCI, V18, P1435, DOI 10.1038/nn.4122; Series P, 2009, NEURAL COMPUT, V21, P3271, DOI 10.1162/neco.2009.09-08-869; STEVENS SS, 1957, PSYCHOL REV, V64, P153, DOI 10.1037/h0046162; Stocker AA, 2006, NAT NEUROSCI, V9, P578, DOI 10.1038/nn1669; Van Trees H.L., 2004, DETECTION ESTIMATION; Wang Z, 2016, NEURAL COMPUT, V28, P2656, DOI 10.1162/NECO_a_00900; Wei XX, 2017, P NATL ACAD SCI USA, V114, P10244, DOI 10.1073/pnas.1619153114; Wei XX, 2016, NEURAL COMPUT, V28, P305, DOI 10.1162/NECO_a_00804; Wei XX, 2015, NAT NEUROSCI, V18, P1509, DOI 10.1038/nn.4105; Yaeli S, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00130	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305011
C	Moran, O; Caramazza, P; Faccio, D; Murray-Smith, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Moran, Oisin; Caramazza, Piergiorgio; Faccio, Daniele; Murray-Smith, Roderick			Deep, complex, invertible networks for inversion of transmission effects in multimode optical fibres	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LIGHT	We use complex-weighted, deep networks to invert the effects of multimode optical fibre distortion of a coherent input image. We generated experimental data based on collections of optical fibre responses to greyscale input images generated with coherent light, by measuring only image amplitude (not amplitude and phase as is typical) at the output of 1 m and 10 m long, 105 mu m diameter multimode fibre. This data is made available as the Optical fibre inverse problem Benchmark collection. The experimental data is used to train complex-weighted models with a range of regularisation approaches. A unitary regularisation approach for complex-weighted networks is proposed which performs well in robustly inverting the fibre transmission matrix, which is compatible with the physical theory. A benefit of the unitary constraint is that it allows us to learn a forward unitary model and analytically invert it to solve the inverse problem. We demonstrate this approach, and outline how it has the potential to improve performance by incorporating knowledge of the phase shift induced by the spatial light modulator.	[Moran, Oisin; Murray-Smith, Roderick] Univ Glasgow, Sch Comp Sci, Glasgow, Lanark, Scotland; [Caramazza, Piergiorgio; Faccio, Daniele] Univ Glasgow, Sch Phys & Astron, Glasgow, Lanark, Scotland	University of Glasgow; University of Glasgow	Murray-Smith, R (corresponding author), Univ Glasgow, Sch Comp Sci, Glasgow, Lanark, Scotland.	oisin@inscribe.ai; piergiorgio.caramazza@gmail.com; Daniele.Faccio@glasgow.ac.uk; Roderick.Murray-Smith@glasgow.ac.uk	Murray-Smith, Roderick/AAF-9987-2020	Murray-Smith, Roderick/0000-0003-4228-7962	Engineering and Physical Sciences Research Council on the QuantIC EPSRC [EP/M01326X/1]; EPSRC [EP/R018634/1]	Engineering and Physical Sciences Research Council on the QuantIC EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We acknowledge funding from the Engineering and Physical Sciences Research Council on the QuantIC EPSRC grant EP/M01326X/1. R. Murray-Smith also acknowledges the support of the EPSRC Closed-loop data science grant EP/R018634/1. We would like to thank Francesco Tonolini and John Williamson for useful discussions and Maya Levitsky for discussions and support on simulation experiments.	Abadi M, 2015, P 12 USENIX S OPERAT; Ardizzone L., 2018, ARXIV E PRINTS; Arjovsky M, 2016, PR MACH LEARN RES, V48; Borhani N, 2018, OPTICA, V5, P960, DOI 10.1364/OPTICA.5.000960; Brock Andrew, 2016, ARXIV160804236; Cabrera JBD, 1999, IEEE T AUTOMAT CONTR, V44, P2007, DOI 10.1109/9.802910; Choi Y, 2012, PHYS REV LETT, V109, DOI 10.1103/PhysRevLett.109.203901; Cizmar T, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms2024; Cizmar T, 2011, OPT EXPRESS, V19, P18871, DOI 10.1364/OE.19.018871; Dinh L, 2016, ARXIV E PRINTS; Fan P., 2018, ARXIV180709351; Grathwohl W, 2018, ARXIV E PRINTS; Guberman N, 2016, ARXIV160209046; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Higham CF, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-20521-y; Hirose A, 2012, STUD COMPUT INTELL, V400, P1, DOI 10.1007/978-3-64227632-3; Hirose A., 2003, COMPLEX VALUED NEURA, P1; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mahalati RN, 2013, OPT EXPRESS, V21, P1656, DOI 10.1364/OE.21.001656; Muybridge Eadweard, 1955, HUMAN FIGURE MOTION, V11; Papadopoulos IN, 2012, OPT EXPRESS, V20, P10583, DOI 10.1364/OE.20.010583; Ploschner M, 2015, NAT PHOTONICS, V9, P529, DOI [10.1038/nphoton.2015.112, 10.1038/NPHOTON.2015.112]; Rahmani B, 2018, LIGHT-SCI APPL, V7, DOI 10.1038/s41377-018-0074-1; Seibel EJ, 2006, PROC SPIE, V6083, DOI 10.1117/12.648030; Shang WL, 2016, PR MACH LEARN RES, V48; Stasio Nicolino, 2017, THESIS; Trabelsi C., 2018, INT C LEARN REPR VAN; Tygert M, 2016, NEURAL COMPUT, V28, P815, DOI 10.1162/NECO_a_00824; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5; Xiao H., 2017, FASHION MNIST NOVEL; Zhao Han, 2015, CORR	33	0	0	1	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303029
C	Mrowca, D; Zhuang, CX; Wang, E; Haber, N; Li, FF; Tenenbaum, JB; Yamins, DLK		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mrowca, Damian; Zhuang, Chengxu; Wang, Elias; Haber, Nick; Li Fei-Fei; Tenenbaum, Joshua B.; Yamins, Daniel L. K.			Flexible Neural Representation for Physics Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Humans have a remarkable capacity to understand the physical dynamics of objects in their environment, flexibly capturing complex structures and interactions at multiple levels of detail. Inspired by this ability, we propose a hierarchical particle-based object representation that covers a wide variety of types of three-dimensional objects, including both arbitrary rigid geometrical shapes and deformable materials. We then describe the Hierarchical Relation Network (HRN), an end-to-end differentiable neural network based on hierarchical graph convolution, that learns to predict physical dynamics in this representation. Compared to other neural network baselines, the HRN accurately handles complex collisions and nonrigid deformations, generating plausible dynamics predictions at long time scales in novel settings, and scaling to large scene configurations. These results demonstrate an architecture with the potential to form the basis of next-generation physics predictors for use in computer vision, robotics, and quantitative cognitive science.	[Mrowca, Damian; Li Fei-Fei; Yamins, Daniel L. K.] Dept Comp Sci, Stanford, CA 94305 USA; [Zhuang, Chengxu; Haber, Nick; Yamins, Daniel L. K.] Dept Psychol, Stanford, CA 94305 USA; [Wang, Elias] Dept Elect Engn, Stanford, CA 94305 USA; [Haber, Nick] Dept Pediat, Stanford, CA 94305 USA; [Haber, Nick] Dept Biomed Data Sci, Stanford, CA 94305 USA; [Yamins, Daniel L. K.] Wu Tsai Neurosci Inst, Stanford, CA 94305 USA; [Tenenbaum, Joshua B.] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA; [Tenenbaum, Joshua B.] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Mrowca, D (corresponding author), Dept Comp Sci, Stanford, CA 94305 USA.	mrowca@stanford.edu; chengxuz@stanford.edu; eliwang@stanford.edu			James S. McDonnell Foundation; Simons Foundation; Sloan Foundation; Berry Foundation postdoctoral fellowship; NVIDIA Corporation; ONR - MURI [N00014-16-1-2127, 1015 G TA275]	James S. McDonnell Foundation; Simons Foundation; Sloan Foundation(Alfred P. Sloan Foundation); Berry Foundation postdoctoral fellowship; NVIDIA Corporation; ONR - MURI(MURIOffice of Naval Research)	We thank Viktor Reutskyy, Miles Macklin, Mike Skolones and Rev Lebaredian for helpful discussions and their support with integrating NVIDIA FleX into our simulation environment. This work was supported by grants from the James S. McDonnell Foundation, Simons Foundation, and Sloan Foundation (DLKY), a Berry Foundation postdoctoral fellowship (NH), the NVIDIA Corporation, ONR - MURI (Stanford Lead) N00014-16-1-2127 and ONR - MURI (UCLA Lead) 1015 G TA275.	Agrawal P., 2016, P ADV NEURAL INFORM, P5074; [Anonymous], 2015, DEEP CONVOLUTIONAL N; [Anonymous], 2017, ARXIV PREPRINT ARXIV; Baraff D, 2001, SIGGRAPH COURSE NOTE, V2, P2; Bates C, 2015, COGSCI; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Battaglia Peter W, 2016, ARXIV161200222; Bender J., 2015, EUROGRAPHICS TUTORIA; Brand M, 1997, COMPUT VIS IMAGE UND, V65, P192, DOI 10.1006/cviu.1996.0572; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J., 2014, INT C LEARNING REPRE; Byravan Arunkumar, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P173, DOI 10.1109/ICRA.2017.7989023; Chang Michael B, 2016, ARXIV161200341; Coumans E., 2010, OPEN SOURCE SOFTWARE; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Duvenaud David K, 2015, P NIPS; Fan H., 2017, P CVPR HON HAW 21 26, V2, P6; Fragkiadaki K., 2015, ARXIV151107404; Gershman S.J., 2017, BEHAV BRAIN SCI, V40; Grzeszczuk R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P9, DOI 10.1145/280814.280816; Haber N., 2018, ARXIV180207442; Hamrick J., 2011, C COG SC, P1545; Hegarty M, 2004, TRENDS COGN SCI, V8, P280, DOI 10.1016/j.tics.2004.04.001; Kipf T, 2018, INT C MACH LEARN PML, P2688; Kipf Thomas N, 2016, 5 INT C LEARN REPR I; Kulkarni T. D., 2014, ARXIV14071339; Kulkarni TD, 2015, ADV NEUR IN, V28; Lerer A., 2016, ARXIV160301312; Li Wenbin, 2016, ARXIV160400066; Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601152, 10.1145/280/109/2601152]; MCCLOSKEY M, 1980, SCIENCE, V210, P1139, DOI 10.1126/science.210.4474.1139; Mottaghi R, 2016, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2016.383; Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17; Piloto L., 2018, ARXIV180401128; Qi C.R., 2017, P 31 ANN C NEUR INF, P4; Qi Charles Ruizhongtai, 2017, P ADV NEUR INF PROC, P5099; Riochet Ronan, 2018, ARXIV180307616; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Smith KA, 2013, TOP COGN SCI, V5, P185, DOI 10.1111/tops.12009; SPELKE ES, 1992, PSYCHOL REV, V99, P605, DOI 10.1037/0033-295X.99.4.605; SPELKE ES, 1990, COGNITIVE SCI, V14, P29, DOI 10.1207/s15516709cog1401_3; Sutskever I, 2009, ADV NEURAL INFORM PR, P1593; Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788; Tran D, 2016, IEEE COMPUT SOC CONF, P402, DOI 10.1109/CVPRW.2016.57; Ullman T. D., 2014, P 36 ANN C COGN SCI, P1640; Wang Z., 2018, ARXIV180500328; Watters N, 2017, ARXIV170601433; Whitney William F, 2016, ARXIV160206822; Wu J., 2016, PROC BRIT MACH VIS C, V2, P7; Wu J., 2015, ADV NEURAL INFORM PR; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22	54	0	0	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003036
C	Murata, T; Suzuki, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Murata, Tomoya; Suzuki, Taiji			Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method for Stochastic Sparse Linear Regression with Limited Attribute Observation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ONLINE	We develop new stochastic gradient methods for efficiently solving sparse linear regression in a partial attribute observation setting, where learners are only allowed to observe a fixed number of actively chosen attributes per example at training and prediction times. It is shown that the methods achieve essentially a sample complexity of O(1/epsilon) to attain an error of epsilon under a variant of restricted eigenvalue condition, and the rate has better dependency on the problem dimension than existing methods. Particularly, if the smallest magnitude of the non-zero components of the optimal solution is not too small, the rate of our proposed Hybrid algorithm can be boosted to near the minimax optimal sample complexity of full information algorithms. The core ideas are (i) efficient construction of an unbiased gradient estimator by the iterative usage of the hard thresholding operator for configuring an exploration algorithm; and (ii) an adaptive combination of the exploration and an exploitation algorithms for quickly identifying the support of the optimum and efficiently searching the optimal parameter in its support. Experimental results are presented to validate our theoretical findings and the superiority of our proposed methods.	[Murata, Tomoya] NTT DATA Math Syst Inc, Tokyo, Japan; [Suzuki, Taiji] Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Math Informat, Tokyo, Japan; [Suzuki, Taiji] RIKEN, Ctr Adv Intelligence Project, Tokyo, Japan	University of Tokyo; RIKEN	Murata, T (corresponding author), NTT DATA Math Syst Inc, Tokyo, Japan.	murata@msi.co.jp; taiji@mist.i.u-tokyo.ac.jp			MEXT Kakenhi [25730013, 25120012, 26280009, 15H05707, 18H03201]; Japan Digital Design; JST-CREST	MEXT Kakenhi(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Japan Digital Design; JST-CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	TS was partially supported by MEXT Kakenhi (25730013, 25120012, 26280009, 15H05707 and 18H03201), Japan Digital Design, and JST-CREST.	Ben-David S., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P287, DOI 10.1145/168304.168353; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Cesa-Bianchi N, 2011, J MACH LEARN RES, V12, P2857; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Foster D. P., 2016, P 29 C LEARNING THEO, P960; Hazan  E., 2012, ARXIV12064678; Ito  S., 2017, ADV NEURAL INFORM PR, P4102; Kale  S., 2017, P 34 INT C MACH LEAR, V70, P1780; Kar P., 2014, ADV NEURAL INFORM PR, P685; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Nguyen N, 2017, IEEE T INFORM THEORY, V63, P6869, DOI 10.1109/TIT.2017.2749330; Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305034
C	Mussmann, S; Liang, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mussmann, Stephen; Liang, Percy			Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CLASSIFICATION	Uncertainty sampling, a popular active learning algorithm, is used to reduce the amount of data required to learn a classifier, but it has been observed in practice to converge to different parameters depending on the initialization and sometimes to even better parameters than standard training on all the data. In this work, we give a theoretical explanation of this phenomenon, showing that uncertainty sampling on a convex (e.g., logistic) loss can be interpreted as performing a preconditioned stochastic gradient step on the population zero-one loss. Experiments on synthetic and real datasets support this connection.	[Mussmann, Stephen; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Mussmann, S (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	mussmann@stanford.edu; pliang@cs.stanford.edu			NSF Graduate Fellowship	NSF Graduate Fellowship(National Science Foundation (NSF))	This research was supported by an NSF Graduate Fellowship to the first author.	[Anonymous], 2013, P INT C MACH LEARN I; Bach F. R., 2007, ADV NEURAL INFORM PR, V19, P65; Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Beygelzimer A., 2009, P 26 ANN INT C MACH, P49; Bordes A, 2005, J MACH LEARN RES, V6, P1579; Chang H.-S., 2017, ADV NEURAL INFORM PR, P1003; Dasgupta S., 2008, P 25 INT C MACH LEAR, P208, DOI DOI 10.1145/1390156.1390183; Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094; Guillory Andrew, 2009, ARTIF INTELL, P201; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; Hoveijn I., 2007, ARXIV07120915; Klein S, 2011, LECT NOTES COMPUT SC, V6892, P549, DOI 10.1007/978-3-642-23629-7_67; Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3; Li XL, 2018, IEEE T NEUR NET LEAR, V29, P1454, DOI 10.1109/TNNLS.2017.2672978; Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Ramdas A, 2013, LECT NOTES ARTIF INT, V8139, P339; Schohn G., 2000, P 17 INT C MACH LEAR, P839; Schutze H., 2006, P 15 ACM INT C INFOR, P662; Settles B., 2010, ACTIVE LEARNING LIT, V1648; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Wu YC, 2007, J AM STAT ASSOC, V102, P974, DOI 10.1198/016214507000000617; Yang Y., 2016, ARXIV161108618	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001049
C	Muzellec, B; Cuturi, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Muzellec, Boris; Cuturi, Marco			Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DIMENSIONALITY; DISTANCE; PRODUCT	Embedding complex objects as vectors in low dimensional spaces is a longstanding problem in machine learning. We propose in this work an extension of that approach, which consists in embedding objects as elliptical probability distributions, namely distributions whose densities have elliptical level sets. We endow these measures with the 2-Wasserstein metric, with two important benefits: (i) For such measures, the squared 2-Wasserstein metric has a closed form, equal to a weighted sum of the squared Euclidean distance between means and the squared Bures metric between covariance matrices. The latter is a Riemannian metric between positive semi-definite matrices, which turns out to be Euclidean on a suitable factor representation of such matrices, which is valid on the entire geodesic between these matrices. (ii) The 2-Wasserstein distance boils down to the usual Euclidean metric when comparing Diracs, and therefore provides a natural framework to extend point embeddings. We show that for these reasons Wasserstein elliptical embeddings are more intuitive and yield tools that are better behaved numerically than the alternative choice of Gaussian embeddings with the Kullback-Leibler divergence. In particular, and unlike previous work based on the KL geometry, we learn elliptical distributions that are not necessarily diagonal. We demonstrate the advantages of elliptical embeddings by using them for visualization, to compute embeddings of words, and to reflect entailment or hypernymy.	[Muzellec, Boris; Cuturi, Marco] ENSAE, CREST, Palaiseau, France; [Cuturi, Marco] Google Brain, Mountain View, CA USA	Institut Polytechnique de Paris; Google Incorporated	Muzellec, B (corresponding author), ENSAE, CREST, Palaiseau, France.	boris.muzellec@ensae.fr; cuturi@google.com						Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Ambrosio L., 2006, LECT MATH; [Anonymous], 2014, P INT C LEARN REPR; Badoiu M, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P512; Baroni M, 2009, LANG RESOUR EVAL, V43, P209, DOI 10.1007/s10579-009-9081-4; Baroni Marco, 2012, P 13 C EUR CHAPT ASS, P23, DOI DOI 10.1111/J.1551-6709.2009.01068.X; Bhatia R., 2018, EXPO MATH; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; BORG I., 2005, MODERN MULTIDIMENSIO, P207; BRENIER Y, 1987, CR ACAD SCI I-MATH, V305, P805; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Bruni E, 2014, J ARTIF INTELL RES, V49, P1, DOI 10.1613/jair.4135; BURES D, 1969, T AM MATH SOC, V135, P199, DOI 10.2307/1995012; CAMBANIS S, 1981, J MULTIVARIATE ANAL, V11, P368, DOI 10.1016/0047-259X(81)90082-8; de Leeuw J, 1977, RECENT DEV STAT; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fakcharoenphol J., 2003, P 35 ANN ACM S THEOR, P448, DOI DOI 10.1145/780542.780608; Fang K.-T., 1990, SYMMETRIC MULTIVARIA; Finkelstein L, 2002, ACM T INFORM SYST, V20, P116, DOI 10.1145/503104.503110; GELBRICH M, 1990, MATH NACHR, V147, P185, DOI 10.1002/mana.19901470121; Globerson A, 2007, J MACH LEARN RES, V8, P2265; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Halawi Guy, 2012, P 18 ACM SIGKDD INT, P1406, DOI [10.1145/371920.372094, DOI 10.1145/371920.372094, 10.1145/2339530.2339751, DOI 10.1145/2339530.2339751]; Higham NJ, 2008, FUNCTIONS MATRICES T; Hill F, 2015, COMPUT LINGUIST, V41, P665, DOI 10.1162/COLI_a_00237; Hinton G., 2003, ADV NEURAL INFORM PR, P857; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Lee JM., 1997, GRADUATE TEXTS MATH, P155, DOI 10.1007/0-387-22726-1_9; Luong Minh-Thang, 2013, P 13 ANN C NAT LANG; Malago L, 2018, ARXIV180109269; Maron Y., 2010, P INT C NEUR INF PRO, P1567; Mikolov T., 2013, INT C LEARN REPRESEN, DOI 10.1109/TCBB.2021.3077905; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; MILLER GA, 1991, LANG COGNITIVE PROC, V6, P1, DOI 10.1080/01690969108406936; Nickel M, 2017, ADV NEUR IN, V30; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; Peyre G., 2018, ARXIV180300567; Radinsky Kira, 2011, P 20 INT C WORLD WID, P337, DOI [10.1145/1963405.1963455, DOI 10.1145/1963405.1963455]; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; RUBENSTEIN H, 1965, COMMUN ACM, V8, P627, DOI 10.1145/365628.365657; Singh S. P., 2018, ARXIV180809663; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Ushakov NG., 2011, SELECTED TOPICS CHAR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vilnis L., 2015, P 2015 INT C LEARN R; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Welling M, 2014, AUTOENCODING VARIATI; Yang D, 2005, P 28 AUSTR C COMP SC, V38, P315	54	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004076
C	Negrinho, R; Gormley, MR; Gordon, GJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Negrinho, Renato; Gormley, Matthew R.; Gordon, Geoffrey J.			Learning Beam Search Policies via Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Beam search is widely used for approximate decoding in structured prediction problems. Models often use a beam at test time but ignore its existence at train time, and therefore do not explicitly learn how to use the beam. We develop an unifying meta-algorithm for learning beam search policies using imitation learning. In our setting, the beam is part of the model, and not just an artifact of approximate decoding. Our meta-algorithm captures existing learning algorithms and suggests new ones. It also lets us show novel no-regret guarantees for learning beam search policies.	[Negrinho, Renato; Gormley, Matthew R.; Gordon, Geoffrey J.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Gordon, Geoffrey J.] Microsoft Res, Washington, DC USA	Carnegie Mellon University; Microsoft	Negrinho, R (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	negrinho@cs.cmu.edu; mgormley@cs.cmu.edu; ggordon@cs.cmu.edu						Andor D., 2016, ACL; Bengio Samy, 2015, NEURIPS; Beygelzimer Alina, 2008, PERFORMANCE MODELING; Cesa-Bianchi Nicolo, 2004, IEEE T INFORM THEORY; Chang Kai-Wei, 2015, ICML; Collins Michael, 2004, ACL; Daume III H., 2009, MACHINE LEARNING; Daume III H., 2005, ICML; Gimpel Kevin, 2010, ACL; Goyal Kartik, 2018, AAAI; Graves A., 2013, ICASSP; Guestrin Carlos, 2003, NEURIPS; Hazan E., 2016, FDN TRENDS OPTIMIZAT; Kalai Adam, 2005, J COMPUTER SYSTEM SC; Kingma D.P., 2015, INT C LEARN REPR, P1; Liang Huang, 2012, NAACL; Papineni K., 2002, P ANN M ASS COMP LIN; Ross S., 2011, P INT C ARTIFICIAL I, P627; Ross S., 2014, ARXIV14065979; Sutskever Ilya, 2014, NEURIPS; Vinyals O., 2015, CVPR; Weiss David, 2015, ACL; Wiseman S., 2016, ACL; Xu Yuehua, 2007, ICML; Zinkevich M, 2003, ICML	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005025
C	Niazadeh, R; Roughgarden, T; Wang, JR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Niazadeh, Rad; Roughgarden, Tim; Wang, Joshua R.			Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper we study the fundamental problems of maximizing abcontinuous non-monotone submodular function over a hypercube, with and without coordinatewise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first 1/2-approximation algorithm for continuous submodular function maximization; the approximation factor of 1/2 is the best possible for algorithms that use only polynomially many queries. For the special case of DR-submodular maximization, i.e., when the submodular functions is also coordinate-wise concave along all coordinates, we provide a faster 1/2 -approximation algorithm that runs in almost linear time. Both of these results improve upon prior work [Bjan et al., 2017a,b, Soma and Yoshida, 2017, Buchbinder et al., 2012, 2015]. Our first algorithm is a single-pass algorithm that uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm is a faster single-pass algorithm that exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications.	[Niazadeh, Rad; Roughgarden, Tim] Stanford Univ, Dept Comp Sci, Stanford, CA 95130 USA; [Wang, Joshua R.] Google, Mountain View, CA 94043 USA	Stanford University; Google Incorporated	Niazadeh, R (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 95130 USA.	rad@cs.stanford.edu; tim@cs.stanford.edu; joshuawang@google.com			Stanford Computer Science Motwani Fellowship; Google Faculty Grant; Guggenheim Fellowship; NSF [CCF-1524062, CCF-1813188]	Stanford Computer Science Motwani Fellowship; Google Faculty Grant(Google Incorporated); Guggenheim Fellowship; NSF(National Science Foundation (NSF))	Rad Niazadeh was supported by Stanford Computer Science Motwani Fellowship.; Tim Roughgarden was supported in part by Google Faculty Grant, Guggenheim Fellowship, and NSF Grants CCF-1524062 and CCF-1813188.	Antoniadis A, 2011, ANN I STAT MATH, V63, P585, DOI 10.1007/s10463-009-0242-4; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Bian A., 2017, ADV NEURAL INFORM PR, P486; Bian AA, 2017, PR MACH LEARN RES, V54, P111; Buchbinder N., 2016, P 27 ANN ACM SIAM S, P392; Buchbinder N, 2015, SIAM J COMPUT, V44, P1384, DOI 10.1137/130929205; Buchbinder N, 2012, ANN IEEE SYMP FOUND, P649, DOI 10.1109/FOCS.2012.73; Chen L, 2018, PROC INTERNAT C ARTI, P1896; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; Feige U, 2007, ANN IEEE SYMP FOUND, P461, DOI 10.1109/FOCS.2007.29; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Gillenwater J., 2012, ADV NEURAL INFORM PR; Gotovos Alkis, 2015, 24 INT JOINT C ART I; Graham R. L., 1972, Information Processing Letters, V1, P132, DOI 10.1016/0020-0190(72)90045-2; Hartline J., 2008, P 17 INT C WORLD WID, P189; Hassani Hamed, 2017, ADV NEURAL INFORM PR, P5843; Ito S., 2016, ADV NEURAL INFORM PR, P3855; Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096; Kim SY, 2003, COMPUT OPTIM APPL, V26, P143, DOI 10.1023/A:1025794313696; Krause A, 2014, TRACTABILITY, P71; Li C., 2016, ADV NEURAL INFORM PR, P4188; Luo ZQ, 2010, IEEE SIGNAL PROC MAG, V27, P20, DOI 10.1109/MSP.2010.936019; Mokhtari A., 2018, C ART INT STAT, V84, P1886; Roughgarden T, 2018, P C LEARN THEOR, P1307; Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989; Soma T., 2015, P ADV NEURAL INFORM, P847; Soma T, 2017, AAAI CONF ARTIF INTE, P898; Staib Matthew, 2017, ARXIV170208791; Zhang J, 2015, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2015.216	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004018
C	Nissim, K; Smith, A; Steinke, T; Stemmer, U; Ullman, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nissim, Kobbi; Smith, Adam; Steinke, Thomas; Stemmer, Uri; Ullman, Jonathan			The Limits of Post-Selection Generalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					While statistics and machine learning offers numerous methods for ensuring generalization, these methods often fail in the presence of post selection-the common practice in which the choice of analysis depends on previous interactions with the same dataset. A recent line of work has introduced powerful, general purpose algorithms that ensure a property called post hoc generalization (Cummings et al., COLT' 16), which says that no person when given the output of the algorithm should be able to find any statistic for which the data differs significantly from the population it came from. In this work we show several limitations on the power of algorithms satisfying post hoc generalization. First, we show a tight lower bound on the error of any algorithm that satisfies post hoc generalization and answers adaptively chosen statistical queries, showing a strong barrier to progress in post selection data analysis. Second, we show that post hoc generalization is not closed under composition, despite many examples of such algorithms exhibiting strong composition properties.	[Nissim, Kobbi] Georgetown Univ, Washington, DC 20057 USA; [Smith, Adam] Boston Univ, Boston, MA 02215 USA; [Steinke, Thomas] IBM Res Almaden, San Jose, CA USA; [Stemmer, Uri] Ben Gurion Univ Negev, Beer Sheva, Israel; [Ullman, Jonathan] Northeastern Univ, Boston, MA 02115 USA; [Stemmer, Uri] Weizmann Inst Sci, IL-76100 Rehovot, Israel	Georgetown University; Boston University; International Business Machines (IBM); Ben Gurion University; Northeastern University; Weizmann Institute of Science	Nissim, K (corresponding author), Georgetown Univ, Washington, DC 20057 USA.	kobbi.nissim@georgetown.edu; ads22@bu.edu; posel@thomas-steinke.net; u@uri.co.il; jullman@ccs.neu.edu	Smith, Adam/GPS-8322-2022	Stemmer, Uri/0000-0001-7584-8768	NSF [CCF-1718088, CCF-1750640, CNS-1816028, CNS-1565387, IIS-1447700, AF-1763665]; Google Faculty Award; Sloan Foundation Research Award; Koshland fellowship; Israel Science Foundation [950/16, 5219/17]	NSF(National Science Foundation (NSF)); Google Faculty Award(Google Incorporated); Sloan Foundation Research Award; Koshland fellowship; Israel Science Foundation(Israel Science Foundation)	Supported by NSF award CNS-1565387.; Supported by NSF awards IIS-1447700 and AF-1763665, a Google Faculty Award and a Sloan Foundation Research Award.; Work done while U.S. was a postdoctoral researcher at the Weizmann Institute of Science, supported by a Koshland fellowship, and by the Israel Science Foundation (grants 950/16 and 5219/17).; Supported by NSF awards CCF-1718088, CCF-1750640, and CNS-1816028, and a Google Faculty Award.	Bassily R., 2016, STOC; Berk R, 2013, ANN STAT, V41, P802, DOI 10.1214/12-AOS1077; Boneh D, 1998, IEEE T INFORM THEORY, V44, P1897, DOI 10.1109/18.705568; Buja Andreas, 2015, STAT SCI, V1460; Bun M, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1, DOI 10.1145/2591796.2591877; Bun Mark, 2017, SODA; Cummings Rachel, 2016, COLT; Dwork C., 2015, FOCS; Dwork C., 2006, TCC; Dwork Cynthia, 2015, NIPS; Dwork Cynthia, 2015, STOC; Efron B, 2014, J AM STAT ASSOC, V109, P991, DOI 10.1080/01621459.2013.823775; Fithian W, 2014, OPTIMAL INFERENCE MO; Gelman A, 2014, AM SCI, V102, P460, DOI 10.1511/2014.111.460; Hardt Moritz, 2014, FOCS; HURVICH CM, 1990, AM STAT, V44, P214, DOI 10.2307/2685338; Kearns Michael J., 1993, STOC; Potscher Benedikt M, 1991, ECONOMETRIC THEORY; Russo D., 2016, AISTATS; Steinke T., 2015, COLT; Steinke Thomas, 2017, FOCS; Tardos G, 2008, J ACM, V55, DOI 10.1145/1346330.1346335; Taylor J, 2015, P NATL ACAD SCI USA, V112, P7629, DOI 10.1073/pnas.1507583112; Wang Y, 2017, THESIS	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000086
C	Orhan, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Orhan, Emin			A Simple Cache Model for Image Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Training large-scale image recognition models is computationally expensive. This raises the question of whether there might be simple ways to improve the test performance of an already trained model without having to re-train or fine-tune it with new data. Here, we show that, surprisingly, this is indeed possible. The key observation we make is that the layers of a deep network close to the output layer contain independent, easily extractable class-relevant information that is not contained in the output layer itself. We propose to extract this extra class-relevant information using a simple key-value cache memory to improve the classification performance of the model at test time. Our cache memory is directly inspired by a similar cache model previously proposed for language modeling (Grave et al., 2017). This cache component does not require any training or fine-tuning; it can be applied to any pre-trained model and, by properly setting only two hyper-parameters, leads to significant improvements in its classification performance. Improvements are observed across several architectures and datasets. In the cache component, using features extracted from layers close to the output (but not from the output layer itself) as keys leads to the largest improvements. Concatenating features from multiple layers to form keys can further improve performance over using single-layer features as keys. The cache component also has a regularizing effect, a simple consequence of which is that it substantially increases the robustness of models against adversarial attacks.	[Orhan, Emin] Baylor Coll Med, Houston, TX 77030 USA; [Orhan, Emin] NYU, New York, NY 10003 USA	Baylor College of Medicine; New York University	Orhan, E (corresponding author), Baylor Coll Med, Houston, TX 77030 USA.; Orhan, E (corresponding author), NYU, New York, NY 10003 USA.	aeminorhan@gmail.com						Badia Adria Puigdomenech, 2017, ICML; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Goodfellow I. J., 2014, ARXIV14126572; Grave E., 2017, NIPS; Grave E, 2017, ICLR; He K., 2016, ARXIV160305027; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G., 2016, CVPR; Kaiser L., 2017, ICLR; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Moosavi-Dezfooli S-M, 2015, CVPR; Novak R., 2018, ICLR; Papernot N, 2018, ARXIV180304765; Rauber Jonas, 2017, REL MACH LEARN WILD, P6, DOI DOI 10.21105/JOSS.02607; Su J., 2017, ARXIV171008864; Wang Y., 2018, ICML; Zhang H., 2017, ICLR; Zhao Jake, 2018, ARXIV180209502	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004064
C	Park, J; Boo, Y; Choi, I; Shin, S; Sung, W		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Park, Jinhwan; Boo, Yoonho; Choi, Iksoo; Shin, Sungho; Sung, Wonyong			Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Real-time automatic speech recognition (ASR) on mobile and embedded devices has been of great interests for many years. We present real-time speech recognition on smartphones or embedded systems by employing recurrent neural network (RNN) based acoustic models, RNN based language models, and beam-search decoding. The acoustic model is end-to-end trained with connectionist temporal classification (CTC) loss. The RNN implementation on embedded devices can suffer from excessive DRAM accesses because the parameter size of a neural network usually exceeds that of the cache memory and the parameters are used only once for each time step. To remedy this problem, we employ a multi-time step parallelization approach that computes multiple output samples at a time with the parameters fetched from the DRAM. Since the number of DRAM accesses can be reduced in proportion to the number of parallelization steps, we can achieve a high processing speed. However, conventional RNNs, such as long short-term memory (LSTM) or gated recurrent unit (GRU), do not permit multi-time step parallelization. We construct an acoustic model by combining simple recurrent units (SRUs) and depth-wise 1-dimensional convolution layers for multi-time step parallelization. Both the character and word piece models are developed for acoustic modeling, and the corresponding RNN based language models are used for beam search decoding. We achieve a competitive WER for WSJ corpus using the entire model size of around 15MB and achieve real-time speed using only a single core ARM without GPU or special hardware.	[Park, Jinhwan; Boo, Yoonho; Choi, Iksoo; Shin, Sungho; Sung, Wonyong] Seoul Natl Univ, Seoul, South Korea	Seoul National University (SNU)	Park, J (corresponding author), Seoul Natl Univ, Seoul, South Korea.	bnoo@snu.ac.kr; dnsgh@snu.ac.kr; akacis@snu.ac.kr; ssh9919@snu.ac.kr; wysung@snu.ac.kr			Brain Korea 21 Plus Project; National Research Foundation of Korea (NRF) - Korea government (MSIP) [2018R1A2A1A05079504]	Brain Korea 21 Plus Project; National Research Foundation of Korea (NRF) - Korea government (MSIP)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea)	This work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2018R1A2A1A05079504).	Amodei D, 2016, PR MACH LEARN RES, V48; Balduzzi David, 2016, ARXIV160202218; Battenberg E, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P206; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chen ZH, 2017, IEEE-ACM T AUDIO SPE, V25, P90, DOI 10.1109/TASLP.2016.2625459; Chiu CC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4774; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Dauphin YN, 2017, PR MACH LEARN RES, V70; Gal Yarin, 2016, ADV NEURAL INFORM PR, P1019, DOI DOI 10.5555/3157096.3157211; Gehring J., 2017, P ICML; Graves A., 2006, P 23 INT C MACH LEAR, P369; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huggins-Daines D, 2006, INT CONF ACOUST SPEE, P185; Hwang K, 2017, INT CONF ACOUST SPEE, P5720, DOI 10.1109/ICASSP.2017.7953252; Hwang K, 2016, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2016.7472696; Kalchbrenner Nal, 2017, ICML, P2; Kingma D.P, P 3 INT C LEARNING R; Lei Tao, 2017, ARXIV170902755; Liptchinsky V., 2017, LETT BASED SPEECH RE; Martin E., 2018, INT C LEARN REPR ICL; Merity Stephen, 2017, P ICLR 2017; Miao YJ, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P167, DOI 10.1109/ASRU.2015.7404790; Narang Sharan, 2017, INT C LEARN REPR ICL; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; Prabhavalkar R, 2016, INT CONF ACOUST SPEE, P5970, DOI 10.1109/ICASSP.2016.7472823; Rao K, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P193; Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xianyi Zhang, OPENBLAS; Xu Chen, 2018, ICLR; Zeyer A, 2018, INTERSPEECH, P7	34	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005022
C	Pauwels, E; Bach, F; Vert, JP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pauwels, Edouard; Bach, Francis; Vert, Jean-Philippe			Relating Leverage Scores and Density using Regularized Christoffel Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MATRIX; APPROXIMATION; ASYMPTOTICS; REGRESSION	Statistical leverage scores emerged as a fundamental tool for matrix sketching and column sampling with applications to low rank approximation, regression, random feature learning and quadrature. Yet, the very nature of this quantity is barely understood. Borrowing ideas from the orthogonal polynomial literature, we introduce the regularized Christoffel function associated to a positive definite kernel. This uncovers a variational formulation for leverage scores for kernel methods and allows to elucidate their relationships with the chosen kernel as well as population density. Our main result quantitatively describes a decreasing relation between leverage score and population density for a broad class of kernels on Euclidean spaces. Numerical simulations support our findings.	[Pauwels, Edouard] Univ Toulouse 3, Paul Sabatier, IRIT AOC, Toulouse, France; [Bach, Francis] PSL Res Univ, Ecole Normale Super, INRIA, Paris, France; [Vert, Jean-Philippe] PSL Res Univ, CBIO Mines ParisTech, Google Brain, Paris, France	Universite de Toulouse; Universite Toulouse III - Paul Sabatier; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); UDICE-French Research Universities; PSL Research University Paris; MINES ParisTech	Pauwels, E (corresponding author), Univ Toulouse 3, Paul Sabatier, IRIT AOC, Toulouse, France.				European Research Council [SEQUOIA 724063]	European Research Council(European Research Council (ERC)European Commission)	We acknowledge support from the European Research Council (grant SEQUOIA 724063).	Abramowitz M., 1970, HDB MATH FUNCTIONS F; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Askari  A., 2018, ARXIV180606775; ASSCHE WV, 1987, ASYMPTOTICS ORTHOGON; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Bach Francis, 2013, C LEARNING THEORY, P185; BOS L, 1998, REND CIRC MAT PALERM, V2, P277; BOS L, 1994, NZ J MATH, V23, P109; Chatterjee S., 1986, STAT SCI, V1, P379, DOI [10.1214/ss/1177013622, DOI 10.1214/SS/1177013622]; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; De Vito E, 2014, APPL COMPUT HARMON A, V37, P185, DOI 10.1016/j.acha.2013.11.003; Dick J, 2013, ACTA NUMER, V22, P133, DOI 10.1017/S0962492913000044; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Dunkl C. F., 2014, ORTHOGONAL POLYNOMIA; Hardy M, 2006, ELECTRON J COMB, V13; HOAGLIN DC, 1978, AM STAT, V32, P17, DOI 10.2307/2683469; Hunter J.K., 2001, APPL ANAL; Kroo A, 2013, CAN J MATH, V65, P600, DOI 10.4153/CJM-2012-016-x; Lasserre Jean B, 2017, ARXIV170102886; Ma P, 2015, J MACH LEARN RES, V16, P861; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; MATE A, 1980, ANN MATH, V111, P145, DOI 10.2307/1971219; MATE A, 1991, ANN MATH, V134, P433, DOI 10.2307/2944352; Minsker S., 2011, SOME EXTENSIONS BERN; NEVAI P, 1986, J APPROX THEORY, V48, P3, DOI 10.1016/0021-9045(86)90016-X; Rudi A, 2017, ADV NEUR IN, V30; Rudi A, 2015, ADV NEUR IN, V28; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sriperumbudur B. K., 2010, P 13 INT C ART INT S, V9, P773; Szego  G., 1974, ORTHOGONAL POLYNOMIA; Totik V, 2000, J ANAL MATH, V81, P283, DOI 10.1007/BF02788993; VELLEMAN PF, 1981, AM STAT, V35, P234, DOI 10.2307/2683296; Wang SS, 2013, J MACH LEARN RES, V14, P2729; Xu Y, 1999, J APPROX THEORY, V99, P122, DOI 10.1006/jath.1998.3312; Xu Y, 1996, METHODS APPL ANAL, V3, P257	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301063
C	Pillutla, K; Roulet, V; Kakade, SM; Harchaoui, Z		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pillutla, Krishna; Roulet, Vincent; Kakade, Sham M.; Harchaoui, Zaid			A Smoother Way to Train Structured Prediction Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MOST PROBABLE CONFIGURATIONS; ALGORITHMS; OPTIMIZATION; MINIMIZATION	We present a framework to train a structured prediction model by performing smoothing on the inference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum margin structured prediction objective, and paves the way for the use of fast primal gradient-based optimization algorithms. We illustrate the proposed framework by developing a novel primal incremental optimization algorithm for the structural support vector machine. The proposed algorithm blends an extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study several practical variants. We present experimental results on two real-world problems, namely named entity recognition and visual object localization. The experimental results show that the proposed framework allows us to build upon efficient inference algorithms to develop large-scale optimization algorithms for structured prediction which can achieve competitive performance on the two real-world problems.	[Pillutla, Krishna] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA; Univ Washington, Dept Stat, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Pillutla, K (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.	krishnapillutla@uw.edu; vincentroulet@uw.edu; shamkakade@uw.edu; zaidharchaoui@uw.edu			NSF [CCF-1740551]; Washington Research Foundation for innovation in Data-intensive Discovery; program "Learning in Machines and Brains" of CIFAR	NSF(National Science Foundation (NSF)); Washington Research Foundation for innovation in Data-intensive Discovery; program "Learning in Machines and Brains" of CIFAR	This work was supported by NSF Award CCF-1740551, the Washington Research Foundation for innovation in Data-intensive Discovery, and the program "Learning in Machines and Brains" of CIFAR.	Allen-Zhu Z., 2017, J MACHINE LEARNING R, V18; Balamurugan P., 2016, ADV NEURAL INFORM PR, P1416; BENGIO Y, 1995, NEURAL COMPUT, V7, P1289, DOI 10.1162/neco.1995.7.6.1289; Blaschko MB, 2008, PROC CVPR IEEE, P93, DOI 10.1109/cvpr.2008.4587586; Bottou L, 1997, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.1997.609370; Bottou L., 1990, P 3 INT C NEUR INF P, P781; BURKE JV, 1985, MATH PROGRAM, V33, P260, DOI 10.1007/BF01584377; Collins M, 2008, J MACH LEARN RES, V9, P1775; Cox B, 2014, MATH PROGRAM, V148, P143, DOI 10.1007/s10107-013-0725-1; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A, 2016, ADV NEUR IN, V29; Drusvyatskiy D., 2018, MATH PROGRAMMING; Duchi J. C., 2006, P ADV NEUR INF PROC, P369; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Frostig R, 2015, PR MACH LEARN RES, V37, P2540; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Hazan T., 2010, ADV NEURAL INFORM PR, P838; Hazan T, 2016, J MACH LEARN RES, V17; He LH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P473, DOI 10.18653/v1/P17-1044; He N, 2015, ADV NEUR IN, V28; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Johnson J. K., 2008, THESIS; Jojic V., 2010, P 27 INT C MACH LEAR, P503; Jurafsky Dan, 2014, SPEECH LANGUAGE PROC; Kim Sang E.F.T., 2003, P 7 C NAT LANG LEARN; Kohli P, 2008, COMPUT VIS IMAGE UND, V112, P30, DOI 10.1016/j.cviu.2008.07.002; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lacoste-Julien S., 2013, P 30 INT C MACH LEAR, P53; Lacoste-Julien S., 2002, ARXIV12122002; Le Roux N., 2012, P ADV NEUR INF PROC, P2672; Lewis Mike, 2014, P 2014 C EMP METH NA, P990, DOI DOI 10.3115/V1/D14-1107; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Lin HZ, 2018, J MACH LEARN RES, V18; Martins AFT, 2016, PR MACH LEARN RES, V48; Mensch A, 2018, PR MACH LEARN RES, V80; Meshi O., 2010, P 27 INT C MACH LEAR, P783; Meshi O., 2012, NIPS, P3023; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Niculae V, 2018, PR MACH LEARN RES, V80; Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483; Osokin A, 2016, PR MACH LEARN RES, V48; Pillutla K., 2019, SMOOTHER WAY TRAIN S; Ratliff N. D., 2007, INT C ART INT STAT, P380; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Savchynskyy Bogdan, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1817, DOI 10.1109/CVPR.2011.5995652; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Schmidt M, 2015, JMLR WORKSH CONF PRO, V38, P819; SEROUSSI B, 1994, INT J APPROX REASON, V11, P205, DOI 10.1016/0888-613X(94)90031-0; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Song HO, 2014, PR MACH LEARN RES, V32, P1611; Taskar B, 2004, ADV NEUR IN, V16, P25; Taskar B, 2006, J MACH LEARN RES, V7, P1627; Taskar Ben, 2005, P HUM LANG TECHN C C, P73; Teo Choon Hui, 2009, J MACHINE LEARNING R, V1, P55; Tkachenko M., 2012, P KONVENS, P118; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456; Yanover C, 2004, ADV NEUR IN, V16, P289; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zhang XH, 2014, THEOR COMPUT SCI, V519, P88, DOI 10.1016/j.tcs.2013.09.026	69	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304075
C	Polson, NG; Rockova, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Polson, Nicholas G.; Rockova, Veronika			Posterior Concentration for Sparse Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LINEAR-COMBINATIONS; CONVERGENCE-RATES; NEURAL-NETWORKS; APPROXIMATION; DISTRIBUTIONS	We introduce Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables provable recovery of smooth input-output maps with unknown levels of smoothness. Indeed, we show that the posterior distribution concentrates at the near minimax rate for alpha-Holder smooth maps, performing as well as if we knew the smoothness level alpha ahead of time. Our result sheds light on architecture design for deep neural networks, namely the choice of depth, width and sparsity level. These network attributes typically depend on unknown smoothness in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside, we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.	[Polson, Nicholas G.; Rockova, Veronika] Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA	University of Chicago	Polson, NG (corresponding author), Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA.				James S. Kemper Research Fund at the University of Chicago Booth School of Business	James S. Kemper Research Fund at the University of Chicago Booth School of Business	This work was supported by the James S. Kemper Research Fund at the University of Chicago Booth School of Business. The authors would like to thank the anonymous referees and the area chair for useful feedback.	Bauer B, 2017, DEEP LEARNING REMEDY; Bengio Y., 2011, DEEP SPARSE RECTIFIE; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Castillo I, 2012, ANN STAT, V40, P2069, DOI 10.1214/12-AOS1029; Cheang GHL, 2010, J APPROX THEORY, V162, P1450, DOI 10.1016/j.jat.2010.03.004; Cheang GHL, 2000, J APPROX THEORY, V104, P183, DOI 10.1006/jath.1999.3441; Coram M, 2006, ANN STAT, V34, P1233, DOI 10.1214/009053606000000236; Dinh R, 2017, SHARP MINIMA CAN GEN; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228; Ghosal S, 2007, ANN STAT, V35, P192, DOI 10.1214/009053606000001172; Ghosh S, 2017, ADV NEURAL INFORM PR; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Ismailov VE, 2017, ST PETERSB MATH J+, V28, P741, DOI 10.1090/spmj/1471; Kaelbling L. P., 2017, GEN DEEP LEARNING; Kainen PC, 2007, J APPROX THEORY, V147, P1, DOI 10.1016/j.jat.2006.12.009; Kainen PC, 2003, J APPROX THEORY, V122, P151, DOI 10.1016/S0021-9045(03)00072-8; Klusowki J. M, 2017, MINIMAX LOWER BOUNDS; Klusowki J. M, 2016, RISK BOUNDS HIGH DIM; Kolmogorov Andrei Nikolaevich, 1963, AM MATH SOC TRANSL, V2, P55, DOI [10.1090/trans2/028/04, DOI 10.1090/TRANS2/028/04]; Kurkova V, 1997, NEURAL NETWORKS, V10, P1061, DOI 10.1016/S0893-6080(97)00028-2; LECAM L, 1973, ANN STAT, V1, P38, DOI 10.1214/aos/1193342380; Lee HKH, 2000, NEURAL NETWORKS, V13, P629, DOI 10.1016/S0893-6080(00)00045-9; Liu, 2017, ADV NEURAL INFORM PR, V30; Liu Y, 2018, ABC BAYESIAN FORESTS; Mhaskar HN, 1996, NEURAL COMPUT, V8, P164, DOI 10.1162/neco.1996.8.1.164; Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924; Petersen P, 2017, OPTIMAL APPROXIMATIO; Petrushev PP, 1998, SIAM J MATH ANAL, V30, P155, DOI 10.1137/S0036141097322959; Poggio T, 2017, INT J AUTOM COMPUT, V14, P503, DOI 10.1007/s11633-017-1054-2; Polson NG, 2017, BAYESIAN ANAL, V12, P1275, DOI 10.1214/17-BA1082; Rockova V, 2017, POSTERIOR CONCENTRAT; Rockova V, 2018, J AM STAT ASSOC, V113, P431, DOI 10.1080/01621459.2016.1260469; Rockova V, 2014, J AM STAT ASSOC, V109, P828, DOI 10.1080/01621459.2013.869223; Schmidt-Hieber J., 2017, ARXIV170806633; Shen XT, 2001, ANN STAT, V29, P687; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Telgarsky M, 2017, NEURAL NETWORKS RATI; Telgarsky Matus, 2016, JMLR WORKSHOP C P, V49, P1517; Ullrich K., 2017, INT C LEARN REPR; van der Pas S., 2017, ADV NEURAL INFORM PR, V30, P2089; VITUSKIN AG, 1964, SOV MATH DOKL, V5, P793; Wager S., 2014, ADV NEURAL INFORM PR; Walker SG, 2007, ANN STAT, V35, P738, DOI 10.1214/009053606000001361; WONG WH, 1995, ANN STAT, V23, P339, DOI 10.1214/aos/1176324524; Xu, 2018, WEIGHTED BAYESIAN BO; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002	49	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300086
C	Pretorius, A; Van Biljon, E; Kroon, S; Kamper, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pretorius, Arnu; Van Biljon, Elan; Kroon, Steve; Kamper, Herman			Critical initialisation for deep signal propagation in noisy rectifier neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Stochastic regularisation is an important weapon in the arsenal of a deep learning practitioner. However, despite recent theoretical advances, our understanding of how noise influences signal propagation in deep neural networks remains limited. By extending recent work based on mean field theory, we develop a new framework for signal propagation in stochastic regularised neural networks. Our noisy signal propagation theory can incorporate several common noise distributions, including additive and multiplicative Gaussian noise as well as dropout. We use this framework to investigate initialisation strategies for noisy ReLU networks. We show that no critical initialisation strategy exists using additive noise, with signal propagation exploding regardless of the selected noise distribution. For multiplicative noise (e.g. dropout), we identify alternative critical initialisation strategies that depend on the second moment of the noise distribution. Simulations and experiments on real-world data confirm that our proposed initialisation is able to stably propagate signals in deep networks, while using an initialisation disregarding noise fails to do so. Furthermore, we analyse correlation dynamics between inputs. Stronger noise regularisation is shown to reduce the depth to which discriminatory information about the inputs to a noisy ReLU network is able to propagate, even when initialised at criticality. We support our theoretical predictions for these trainable depths with simulations, as well as with experiments on MNIST and CIFAR-10.(double dagger)	[Pretorius, Arnu] Stellenbosch Univ, CAIR, Comp Sci Div, CSIR, Stellenbosch, South Africa; [Van Biljon, Elan; Kroon, Steve] Stellenbosch Univ, Comp Sci Div, Stellenbosch, South Africa; [Kamper, Herman] Stellenbosch Univ, Dept Elect & Elect Engn, Stellenbosch, South Africa	Council for Scientific & Industrial Research (CSIR) - South Africa; Stellenbosch University; Stellenbosch University; Stellenbosch University	Pretorius, A (corresponding author), Stellenbosch Univ, CAIR, Comp Sci Div, CSIR, Stellenbosch, South Africa.	arnupretorius@gmail.com			Google; CSIR/SU Centre for Artificial Intelligence Research (CAIR); Science Faculty and the Postgraduate and International Office of Stellenbosch University	Google(Google Incorporated); CSIR/SU Centre for Artificial Intelligence Research (CAIR); Science Faculty and the Postgraduate and International Office of Stellenbosch University	We would like to thank the reviewers for their insightful comments which improved the quality of this work. Furthermore, we would like to thank Google, the CSIR/SU Centre for Artificial Intelligence Research (CAIR) as well as the Science Faculty and the Postgraduate and International Office of Stellenbosch University for financial support. Finally, we gratefully acknowledge the support of NVIDIA Corporation with the donation of a Titan Xp GPU used for this research.	[Anonymous], 2014, ARXIV14126558; [Anonymous], 2016, P INT C LEARN REPR; Chen M., 2018, P INT C MACH LEARN; Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346; Ge Yang, 2017, ADV NEURAL INF PROCE, V30, P7103; Glorot X, 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1177/1753193410395357; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hayou S., 2018, J FUZHOU U, V56, P1437; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Poole B, 2016, ADV NEUR IN, V29; Saxe  A.M., 2014, P INT C LEARN REPR; Schoenholz S.S., 2017, P INT C LEARN REPR; Simonyan K., 2015, ICLR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Xiao  L., 2018, P INT C MACH LEARN	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000024
C	Pumir, T; Jelassi, S; Boumal, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pumir, Thomas; Jelassi, Samy; Boumal, Nicolas			Smoothed analysis of the low-rank approach for smooth semidefinite programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION; ALGORITHM; MATRICES	We consider semidefinite programs (SDPs) of size n with equality constraints. In order to overcome scalability issues, Burer and Monteiro proposed a factorized approach based on optimizing over a matrix Y of size nxk such that X = Y Y* is the SDP variable. The advantages of such formulation are twofold: the dimension of the optimization variable is reduced, and positive semidefiniteness is naturally enforced. However, optimization in Y is non-convex. In prior work, it has been shown that, when the constraints on the factorized variable regularly define a smooth manifold, provided k is large enough, for almost all cost matrices, all second-order stationary points (SOSPs) are optimal. Importantly, in practice, one can only compute points which approximately satisfy necessary optimality conditions, leading to the question: are such points also approximately optimal? To answer it, under similar assumptions, we use smoothed analysis to show that approximate SOSPs for a randomly perturbed objective function are approximate global optima, with k scaling like the square root of the number of constraints (up to log factors). Moreover, we bound the optimality gap at the approximate solution of the perturbed problem with respect to the original problem. We particularize our results to an SDP relaxation of phase retrieval.	[Pumir, Thomas; Jelassi, Samy] Princeton Univ, ORFE Dept, Princeton, NJ 08544 USA; [Boumal, Nicolas] Princeton Univ, Dept Math, Princeton, NJ 08544 USA	Princeton University; Princeton University	Pumir, T (corresponding author), Princeton Univ, ORFE Dept, Princeton, NJ 08544 USA.	tpumir@princeton.edu; sjelassi@princeton.edu; nboumal@math.princeton.edu			NSF [DMS-1719558]	NSF(National Science Foundation (NSF))	NB is partially supported by NSF award DMS-1719558.	Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; Absil PA, 2007, FOUND COMPUT MATH, V7, P303, DOI 10.1007/s10208-005-0179-9; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Bandeira AS, 2017, MATH PROGRAM, V163, P145, DOI 10.1007/s10107-016-1059-6; BARVINOK AI, 1995, DISCRETE COMPUT GEOM, V13, P189, DOI 10.1007/BF02574037; Bhatia R, 2007, PRINC SER APPL MATH, P1; Boumal N., 2018, COMMUNICATIONS PURE; BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Boumal N, 2019, IMA J NUMER ANAL, V39, P1, DOI 10.1093/imanum/drx080; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Carson T, 2017, 2017 INTERNATIONAL CONFERENCE ON SAMPLING THEORY AND APPLICATIONS (SAMPTA), P73, DOI 10.1109/SAMPTA.2017.8024388; Garber D., 2016, ADV NEURAL INFORM PR, P874; Garber D, 2016, MATH PROGRAM, V158, P329, DOI 10.1007/s10107-015-0932-z; GOLUB GH, 1973, SIAM J NUMER ANAL, V10, P413, DOI 10.1137/0710036; Guionnet A., 2010, HDB RANDOM MATRICES, P433; Hazan E, 2008, LECT NOTES COMPUT SC, V4957, P306, DOI 10.1007/978-3-540-78773-0_27; Helmberg C, 1996, SIAM J OPTIMIZ, V6, P342, DOI 10.1137/0806020; Jain P., 2018, P MACHINE LEARNING R, V75, P3243; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Kulis B., 2007, INT C ART INT STAT, P235; Laue S., 2012, P INT C MACH LEARN, P177; Laurent M, 1996, SIAM J MATRIX ANAL A, V17, P530, DOI 10.1137/0617031; Mei S., 2017, C LEARN THEOR, P1476; Nguyen HH, 2018, J FUNCT ANAL, V275, P2197, DOI 10.1016/j.jfa.2018.06.010; Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339; RIGOLLET P., 2017, HIGH DIMENSIONAL STA; Singer A, 2011, APPL COMPUT HARMON A, V30, P20, DOI 10.1016/j.acha.2010.02.001; Spielman DA, 2004, J ACM, V51, P385, DOI 10.1145/990308.990310; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Yurtsever A, 2017, PR MACH LEARN RES, V54, P1188	34	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302030
C	Quoc, TD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Quoc Tran-Dinh			Non-Ergodic Alternating Proximal Augmented Lagrangian Algorithms with Optimal Rates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DIRECTION METHOD; CONVERGENCE; SELECTION; ADMM	We develop two new non-ergodic alternating proximal augmented Lagrangian algorithms (NEAPAL) to solve a class of nonsmooth constrained convex optimization problems. Our approach relies on a novel combination of the augmented Lagrangian framework, alternating/linearization scheme, Nesterov's acceleration techniques, and adaptive strategy for parameters. Our algorithms have several new features compared to existing methods. Firstly, they have a Nesterov's acceleration step on the primal variables compared to the dual one in several methods in the literature. Secondly, they achieve non-ergodic optimal convergence rates under standard assumptions, i.e. an O(1/k) rate without any smoothness or strong convexity-type assumption, or an O(1/k(2)) rate under only semi-strong convexity, where k is the iteration counter. Thirdly, they preserve or have better per-iteration complexity compared to existing algorithms. Fourthly, they can be implemented in a parallel fashion. Finally, all the parameters are adaptively updated without heuristic tuning. We verify our algorithms on different numerical examples and compare them with some state-of-the-art methods.	[Quoc Tran-Dinh] Univ N Carolina, Dept Stat & Operat Res, Hanes Hall 333, Chapel Hill, NC 27599 USA	University of North Carolina; University of North Carolina Chapel Hill	Quoc, TD (corresponding author), Univ N Carolina, Dept Stat & Operat Res, Hanes Hall 333, Chapel Hill, NC 27599 USA.	quoctd@email.unc.edu	Tran-Dinh, Quoc/AAX-8950-2020	Tran-Dinh, Quoc/0000-0002-1077-2579	NSF, USA [DMS-1619884]	NSF, USA(National Science Foundation (NSF))	This work is partly supported by the NSF-grant, DMS-1619884, USA.	Bertsekas DP., 1996, CONSTRAINED OPTIMIZA, V1st edn; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chen CH, 2016, MATH PROGRAM, V155, P57, DOI 10.1007/s10107-014-0826-5; Davis D., 2014, MATH OPER RES; Davis D, 2015, SIAM J OPTIMIZ, V25, P1760, DOI 10.1137/140992291; Deng W, 2017, J SCI COMPUT, V71, P712, DOI 10.1007/s10915-016-0318-2; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Esser J. E., 2010, THESIS; Ghadimi E, 2015, IEEE T AUTOMAT CONTR, V60, P644, DOI 10.1109/TAC.2014.2354892; Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219; He B., 2012, NUMER MATH, V130, P567; He B. S., 2011, OPTIMIZATION; Li Huan, 2016, ARXIV160806366; LIONS PL, 1979, SIAM J NUMER ANAL, V16, P964, DOI 10.1137/0716071; Monteiro RDC, 2013, SIAM J OPTIMIZ, V23, P475, DOI 10.1137/110849468; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Nishihara R, 2015, PR MACH LEARN RES, V37, P343; Ouyang YY, 2015, SIAM J IMAGING SCI, V8, P644, DOI 10.1137/14095697X; Dinh QT, 2013, COMPUT OPTIM APPL, V55, P75, DOI 10.1007/s10589-012-9515-6; Tran-Dinh Q, 2018, SIAM J OPTIMIZ, V28, P96, DOI 10.1137/16M1093094; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Shefi R, 2016, EURO J COMPUT OPTIM, V4, P27, DOI 10.1007/s13675-015-0048-5; Tseng P., 2008, SIAM J OPTIM UNPUB; Wang H., 2013, J CIRCUITS SYST, V18, P1; Wei E., 2011, IEEE T AUTOMAT CONTR, V58, P2162; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	30	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304079
C	Ren, WQ; Zhang, JW; Ma, L; Pan, JS; Cao, XC; Zuo, WM; Liu, W; Yang, MH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ren, Wenqi; Zhang, Jiawei; Ma, Lin; Pan, Jinshan; Cao, Xiaochun; Zuo, Wangmeng; Liu, Wei; Yang, Ming-Hsuan			Deep Non-Blind Deconvolution via Generalized Low-Rank Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we present a deep convolutional neural network to capture the inherent properties of image degradation, which can handle different kernels and saturated pixels in a unified framework. The proposed neural network is motivated by the low-rank property of pseudo-inverse kernels. Specifically, we first compute a generalized low-rank approximation to a large number of blur kernels, and then use separable filters to initialize the convolutional parameters in the network. Our analysis shows that the estimated decomposed matrices contain the most essential information of an input kernel, which ensures the proposed network to handle various blurs in a unified framework and generate high-quality deblurring results. Experimental results on benchmark datasets with noisy and saturated pixels demonstrate that the proposed deconvolution approach relying on generalized low-rank approximation performs favorably against the state-of-the-arts.	[Ren, Wenqi; Cao, Xiaochun] Chinese Acad Sci, IIE, Beijing, Peoples R China; [Zhang, Jiawei] SenseTime Res, Hong Kong, Peoples R China; [Ma, Lin; Liu, Wei] Tencent Al Lab, Bellevue, WA USA; [Pan, Jinshan] NJUST, Nanjing, Jiangsu, Peoples R China; [Zuo, Wangmeng] HIT, Harbin, Heilongjiang, Peoples R China; [Yang, Ming-Hsuan] UCMerced, Google Cloud, Merced, CA USA	Chinese Academy of Sciences; Nanjing University of Science & Technology	Cao, XC (corresponding author), Chinese Acad Sci, IIE, Beijing, Peoples R China.	caoxiaochun@iie.ac.cn	Pan, Jinshan/AAO-2258-2021; Zhang, Jiawei/AAF-2390-2019; Pan, Jinshan/S-3658-2019; Zuo, Wangmeng/B-3701-2008; Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019	Yang, Ming-Hsuan/0000-0003-4848-2304; 	National Key R&D Program of China [2016YFC0801004]; National Natural Science Foundation of China [61802403, U1605252, U1736219, 61650202]; Beijing Natural Science Foundation [4172068]; Open Projects Program of National Laboratory of Pattern Recognition; CCF-Tencent Open Fund; Natural Science Foundation of Jiangsu Province [BK20180471]; NSF CAREER [1149783]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Open Projects Program of National Laboratory of Pattern Recognition; CCF-Tencent Open Fund; Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work is supported in part by the National Key R&D Program of China (Grant No. 2016YFC0801004), National Natural Science Foundation of China (No. 61802403, U1605252, U1736219, 61650202), Beijing Natural Science Foundation (No. 4172068). W. Ren is supported in part by the Open Projects Program of National Laboratory of Pattern Recognition and the CCF-Tencent Open Fund. J. Pan is supported in part by the Natural Science Foundation of Jiangsu Province (No. BK20180471). M.-H. Yang is supported in part by the NSF CAREER Grant #1149783 and gifts from and NVIDIA.	[Anonymous], 2018, CVPR; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P1302, DOI 10.1109/TIP.2015.2400217; Cho S., 2011, ICCV; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Deng H, 2016, EURASIP J ADV SIG PR, DOI 10.1186/s13634-016-0318-2; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Glorot  X., 2010, ICAIS; Gong D., 2018, ARXIV PREPRINT ARXIV; Gong  D., 2016, CVPR; Jancsary J., 2012, ECCV; Jin  M., 2017, CVPR; Joshi N., 2009, CVPR; Kenig T, 2010, IEEE T PATTERN ANAL, V32, P2191, DOI 10.1109/TPAMI.2010.45; King DB, 2015, ACS SYM SER, V1214, P1; Krishnan D., 2009, NIPS; Kruse J., 2017, ICCV; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Martin D. R., 2001, ICCV; Ren W., 2017, ICCV; Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062; RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; Schmidt U., 2011, CVPR; Schmidt U., 2013, CVPR; Schmidt U., 2014, CVPR; Schmidt U, 2016, IEEE T PATTERN ANAL, V38, P677, DOI 10.1109/TPAMI.2015.2441053; Schuler Christian J, 2013, CVPR; Sun L., 2014, ECCV; Xu L., 2014, NIPS; Xu L., 2014, ECCV; Yan Y., 2017, CVPR; Ye JP, 2005, MACH LEARN, V61, P167, DOI 10.1007/s10994-005-3561-6; Zeiler Matthew D., 2010, CVPR; Zeng XH, 2015, IEEE T IMAGE PROCESS, V24, P4556, DOI 10.1109/TIP.2015.2468172; Zhang J., 2017, CVPR; Zhang K, 2017, CVPR; Zhang Kai, 2018, IEEE T IMAGE PROCESS; Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733; Zoran Daniel, 2011, ICCV	41	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300028
C	Robin, G; Wai, HT; Josse, J; Klopp, O; Moulines, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Robin, Genevieve; Wai, Hoi-To; Josse, Julie; Klopp, Olga; Moulines, Eric			Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MATRIX COMPLETION	Many applications of machine learning involve the analysis of large data frames matrices collecting heterogeneous measurements (binary, numerical, counts, etc.) across samples - with missing values. Low-rank models, as studied by Udell et al. [27], are popular in this framework for tasks such as visualization, clustering and missing value imputation. Yet, available methods with statistical guarantees and efficient optimization do not allow explicit modeling of main additive effects such as row and column, or covariate effects. In this paper, we introduce a low-rank interaction and sparse additive effects (LORIS) model which combines matrix regression on a dictionary and low-rank design, to estimate main effects and interactions simultaneously. We provide statistical guarantees in the form of upper bounds on the estimation error of both components. Then, we introduce a mixed coordinate gradient descent (MCGD) method which provably converges sub-linearly to an optimal solution and is computationally efficient for large scale data sets. We show on simulated and survey data that the method has a clear advantage over current practices.	[Robin, Genevieve; Josse, Julie; Moulines, Eric] Ecole Polytech, Ctr Math Appl, XPOP, INRIA, F-91120 Palaiseau, France; [Wai, Hoi-To] Chinese Univ Hong Kong, Dept SE & EM, Shatin, Hong Kong, Peoples R China; [Klopp, Olga] ESSEC Business Sch, CREST, ENSAE, F-95021 Cergy, France	Inria; Institut Polytechnique de Paris; Chinese University of Hong Kong; ESSEC Business School; Institut Polytechnique de Paris	Robin, G (corresponding author), Ecole Polytech, Ctr Math Appl, XPOP, INRIA, F-91120 Palaiseau, France.	genevieve.robin@polytechnique.edu; htwai@se.cuhk.edu.hk; julie.josse@polytechnique.edu; klopp@essec.edu; eric.moulines@polytechnique.edu			 [NSF CCF-BSF 1714672]		The authors would like to thank for the useful comments from three anonymous reviewers. HTW's work was supported by the grant NSF CCF-BSF 1714672.	Agresti A, 2013, CATEGORICAL DATA ANA; Beck A, 2015, SIAM J OPTIMIZ, V25, P2024, DOI 10.1137/15M1008397; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chen Y., 2015, CORR; Feuerverger A, 2012, STAT SCI, V27, P202, DOI 10.1214/11-STS368; Fithian W, 2018, STAT SCI, V33, P238, DOI 10.1214/18-STS642; Garber D., 2018, ARXIV180205581; Gidel G., 2017, OPTML 2017, P21; Hastie T, 2015, J MACH LEARN RES, V16, P3367; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; J. Pages, 2014, MULTIPLE FACTOR ANAL; Jaggi M., 2013, P 30 INT C MACHINE L, P427; KIERS HAL, 1991, PSYCHOMETRIKA, V56, P197, DOI 10.1007/BF02294458; Klopp O, 2014, BERNOULLI, V20, P282, DOI 10.3150/12-BEJ486; Kumar NK, 2017, LINEAR MULTILINEAR A, V65, P2212, DOI 10.1080/03081087.2016.1267104; Lacoste-Julien S., 2013, P 30 INT C MACH LEAR, P53; Landgraf A. J., 2015, TECHNICAL REPORT; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Liu L. T., 2018, ANN APPL STAT; Mardani M, 2013, IEEE T INFORM THEORY, V59, P5186, DOI 10.1109/TIT.2013.2257913; Mu C, 2016, SIAM J SCI COMPUT, V38, pA3291, DOI 10.1137/15M101628X; Tao M, 2011, SIAM J OPTIMIZ, V21, P57, DOI 10.1137/100781894; Udell M, 2016, FOUND TRENDS MACH LE, V9, P2, DOI 10.1561/2200000055; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Zhang X., 2018, INT C ART INT STAT, P1097; Zheng W., 2017, ARXIV171207495	29	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000004
C	Rohekar, RY; Gurwicz, Y; Nisimov, S; Koren, G; Novik, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rohekar, Raanan Y.; Gurwicz, Yaniv; Nisimov, Shami; Koren, Guy; Novik, Gal			Bayesian Structure Learning by Recursive Bootstrap	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORK STRUCTURE; STRUCTURE DISCOVERY	We address the problem of Bayesian structure learning for domains with hundreds of variables by employing non-parametric bootstrap, recursively. We propose a method that covers both model averaging and model selection in the same framework. The proposed method deals with the main weakness of constraint-based learning-sensitivity to errors in the independence tests-by a novel way of combining bootstrap with constraint-based learning. Essentially, we provide an algorithm for learning a tree, in which each node represents a scored CPDAG for a subset of variables and the level of the node corresponds to the maximal order of conditional independencies that are encoded in the graph. As higher order independencies are tested in deeper recursive calls, they benefit from more bootstrap samples, and therefore are more resistant to the curse-of-dimensionality. Moreover, the re-use of stable low order independencies allows greater computational efficiency. We also provide an algorithm for sampling CPDAGs efficiently from their posterior given the learned tree. That is, not from the full posterior, but from a reduced space of CPDAGs encoded in the learned tree. We empirically demonstrate that the proposed algorithm scales well to hundreds of variables, and learns better MAP models and more reliable causal relationships between variables, than other state-of-the-art-methods.	[Rohekar, Raanan Y.; Gurwicz, Yaniv; Nisimov, Shami; Koren, Guy; Novik, Gal] Intel AI Lab, Santa Clara, CA 95054 USA		Rohekar, RY (corresponding author), Intel AI Lab, Santa Clara, CA 95054 USA.	raanan.yehezkel@intel.com; yaniv.gurwicz@intel.com; shami.nisimov@intel.com; guy.koren@intel.com; gal.novik@intel.com						Chen EYJ, 2016, JMLR WORKSH CONF PRO, V51, P591; Chen Yetian, 2014, 28 AAAI C ART INT; Chickering D., 1995, P 5 C ART INT STAT, P112; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Eaton D., 2007, PROC 23 C UNCERTAINT, P101; Efron B., 1994, INTRO BOOTSTRAP; Friedman N, 2003, MACH LEARN, V50, P95, DOI 10.1023/A:1020249912095; Friedman N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P196; Grzegorczyk M, 2008, MACH LEARN, V71, P265, DOI 10.1007/s10994-008-5057-7; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Jaakkola T, 2010, P 13 INT C ART INT S, P358; Koivisto M, 2004, J MACH LEARN RES, V5, P549; Koivisto M., 2013, 23 INT JOINT C ART I; Lerner B., 2013, IJCAI, P1458; Murphy K, 2001, COMPUTER SCI STAT, V33, P1024; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Silander T., 2006, P 22 C UNC ART INT, P445; Spirtes P., 2000, CAUSATION PREDICTION; Su Chengwei., 2016, J MACH LEARN RES JML; Tian J, 2010, P 26 C UNC ART INT, P589; TREVES A, 1995, NEURAL COMPUT, V7, P399, DOI 10.1162/neco.1995.7.2.399; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Yehezkel R, 2009, J MACH LEARN RES, V10, P1527; Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005013
C	Rudi, A; Ciliberto, C; Marconi, GM; Rosasco, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rudi, Alessandro; Ciliberto, Carlo; Marconi, Gian Maria; Rosasco, Lorenzo			Manifold Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LEAST-SQUARES; REGRESSION	Structured prediction provides a general framework to deal with supervised problems where the outputs have semantically rich structure. While classical approaches consider finite, albeit potentially huge, output spaces, in this paper we discuss how structured prediction can be extended to a continuous scenario. Specifically, we study a structured prediction approach to manifold valued regression. We characterize a class of problems for which the considered approach is statistically consistent and study how geometric optimization can be used to compute the corresponding estimator. Promising experimental results on both simulated and real data complete our study.	[Rudi, Alessandro] PSL Res Univ, Ecole Normale Super, Dept Informat, INRIA, Paris, France; [Ciliberto, Carlo] Imperial Coll, Dept Elect & Elect Engn, London, England; [Marconi, Gian Maria; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy; [Marconi, Gian Maria; Rosasco, Lorenzo] Ist Italiano Tecnol, Genoa, Italy; [Rosasco, Lorenzo] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Ciliberto, Carlo] UCL, London, England	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Imperial College London; University of Genoa; Istituto Italiano di Tecnologia - IIT; Massachusetts Institute of Technology (MIT); University of London; University College London	Rudi, A (corresponding author), PSL Res Univ, Ecole Normale Super, Dept Informat, INRIA, Paris, France.	alessandro.rudi@inria.fr; c.ciliberto@imperial.ac.uk; gian.maria.marconi@iit.it; lrosasco@mit.edu			European Research Council [SEQUOIA 724063]; AFOSR [FA9550-17-1-0390, BAA-AFRL-AFOSR-2016-0007]; EU H2020-MSCA-RISE project NoMADS [DLV-777826]; Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]; Italian Institute of Technology; EPSRC [EP/P009069/1]	European Research Council(European Research Council (ERC)European Commission); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); EU H2020-MSCA-RISE project NoMADS; Center for Brains, Minds and Machines (CBMM) - NSF STC award; Italian Institute of Technology(Istituto Italiano di Tecnologia - IIT); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063). L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS -DLV-777826. The work of L. R. and G. M. M. is supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the Italian Institute of Technology. We gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research. This work was supported in part by EPSRC grant EP/P009069/1.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Amari S., 2007, METHODS INFORM GEOME, V191; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; BakIr G., 2007, PREDICTING STRUCTURE; Bhatia R., 2009, POSITIVE DEFINITE MA; Bicer V, 2011, LECT NOTES COMPUT SC, V6643, P47, DOI 10.1007/978-3-642-21034-1_4; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Camoriano R, 2016, JMLR WORKSH CONF PRO, V51, P1403; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Carratino L., 2018, ADV NEURAL INFORM PR; Ciliberto C., 2017, ADV NEURAL INFORM PR, P1986; Ciliberto Carlo, 2018, ARXIV180602402; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; DAUME H, 2006, PRACTICAL STRUCTURED; Diestel J., 2014, JOYS HAAR MEASURE; Djerrab  Moussab, 2018, MACH LEARN, P1; Duchi John C., 2010, P 27 INT C MACH LEAR, P327; Fletcher PT, 2013, INT J COMPUT VISION, V105, P171, DOI 10.1007/s11263-012-0591-y; Hardle W, 2007, APPL MULTIVARIATE ST, V2nd; Hauberg S, 2012, P C WORKSH NEUR INF, P2024; Hebey E., 2000, NONLINEAR ANAL MANIF, V5; Hinkle J, 2012, LECT NOTES COMPUT SC, V7574, P1, DOI 10.1007/978-3-642-33712-3_1; Kadous MW, 2005, MACH LEARN, V58, P179, DOI 10.1007/s10994-005-5826-5; Korba  Anna, 2018, ADV NEURAL INFORM PR; LE QV, 2005, NONPARAMETRIC QUANTI; Lee J. M, 2000, INTRO SMOOTH MANIFOL, DOI [10.1007/978-0-387-21752-9, DOI 10.1007/978-0-387-21752-9]; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; Luise  Giulia, 2018, ADV NEURAL INFORM PR; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Moakher M, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P285, DOI 10.1007/3-540-31272-2_17; Morris JS, 2015, ANNU REV STAT APPL, V2, P321, DOI 10.1146/annurev-statistics-010814-020413; Nickel M., 2017, ADV NEURAL INFORM PR, P6338; Nickel Maximillian, 2018, INT C MACH LEARN; Nielsen  Frank, 2017, ARXIV170400454; Nowak-Vila  Alex, 2018, ARXIV181006839; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Paassen B, 2018, NEURAL PROCESS LETT, V48, P669, DOI 10.1007/s11063-017-9684-5; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657, DOI DOI 10.5555/2969239.2969424; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3888; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Smola Alex J, 2000, SPARSE GREEDY MATRIX; SRINIVASAN A, 1999, NOTE LOCATION OPTIMA; Steinke F., 2009, ADV NEURAL INFORM PR, P1561; Steinke F, 2010, SIAM J IMAGING SCI, V3, P527, DOI 10.1137/080744189; Steinwart I., 2008, SUPPORT VECTOR MACHI; Steinwart I., 2009, P C LEARN THEOR COLT; Treves F., 2016, TOPOLOGICAL VECTOR S, V25; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Zhang H., 2016, P C LEARN THEOR, P1617	51	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000014
C	Ryali, CK; Yu, AJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ryali, Chaitanya K.; Yu, Angela J.			Beauty-in-averageness and its contextual modulations: A Bayesian statistical account	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FACES; ATTRACTIVENESS	Understanding how humans perceive the likability of high-dimensional "objects" such as faces is an important problem in both cognitive science and AI/ML. Existing models generally assume these preferences to be fixed. However, psychologists have found human assessment of facial attractiveness to be context-dependent. Specifically, the classical Beauty-in-Averageness (BiA) effect, whereby a blended face is judged to be more attractive than the originals, is significantly diminished or reversed when the original faces are recognizable, or when the blend is mixed-race/mixed-gender and the attractiveness judgment is preceded by a race/gender categorization, respectively. This "Ugliness-in-Averageness" (UiA) effect has previously been explained via a qualitative disfluency account, which posits that the negative affect associated with the difficult race or gender categorization is inadvertently interpreted by the brain as a dislike for the face itself. In contrast, we hypothesize that human preference for an object is increased when it incurs lower encoding cost, in particular when its perceived statistical typicality is high, in consonance with Barlow's seminal "efficient coding hypothesis." This statistical coding cost account explains both BiA, where facial blends generally have higher likelihood than "parent faces", and UiA, when the preceding context or task restricts face representation to a task-relevant subset of features, thus redefining statistical typicality and encoding cost within that subspace. We use simulations to show that our model provides a parsimonious, statistically grounded, and quantitative account of both BiA and UiA. We validate our model using experimental data from a gender categorization task. We also propose a novel experiment, based on model predictions, that will be able to arbitrate between the disfluency account and our statistical coding cost account of attractiveness.	[Ryali, Chaitanya K.] Univ Calif San Diego, Dept Comp Sci & Engn, 9500 Gilman Dr, La Jolla, CA 92093 USA; [Yu, Angela J.] Univ Calif San Diego, Dept Cognit Sci, 9500 Gilman Dr, La Jolla, CA 92093 USA	University of California System; University of California San Diego; University of California System; University of California San Diego	Ryali, CK (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, 9500 Gilman Dr, La Jolla, CA 92093 USA.	rckrishn@eng.ucsd.edu; ajyu@ucsd.edu			UCSD Academic Senate research award	UCSD Academic Senate research award	We thank Piotr Winkielman and Jamin Halberstadt for sharing the gender categorization data and helpful discussions, Samer Sabri for helpful input with the writing and the anonymous reviewers for helpful comments. This project was partially funded by a UCSD Academic Senate research award to AJY.	Bainbridge WA, 2013, J EXP PSYCHOL GEN, V142, P1323, DOI 10.1037/a0033872; Barlow H, 1961, POSSIBLE PRINCIPLES; Barron A, 1998, IEEE T INFORM THEORY, V44, P2743, DOI 10.1109/18.720554; Chang L, 2017, CELL, V169, P1013, DOI 10.1016/j.cell.2017.05.011; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cover TM, 2006, ELEMENTS INFORM THEO; Curby KM., 2010, PERCEPTUAL EXPERTISE, P139; Dotsch R, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-016-0001; Edelman S., 1996, MECH PERCEPTUAL LEAR, P353; Edwards G. J., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P581, DOI 10.1007/BFb0054766; Guan J., 2018, BIORXIV; Halberstadt J, 2000, PSYCHOL SCI, V11, P285, DOI 10.1111/1467-9280.00257; Halberstadt J, 2006, PERS SOC PSYCHOL REV, V10, P166, DOI 10.1207/s15327957pspr1002_5; Halberstadt J, 2003, PSYCHON B REV, V10, P149, DOI 10.3758/BF03196479; Halberstadt J, 2014, J EXP SOC PSYCHOL, V50, P175, DOI 10.1016/j.jesp.2013.08.004; Halberstadt J, 2013, PSYCHOL SCI, V24, P2343, DOI 10.1177/0956797613491969; Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677; Kagian A., 2007, ADV NEURAL INFORM PR, P649; LANGLOIS JH, 1990, DEV PSYCHOL, V26, P153, DOI 10.1037/0012-1649.26.1.153; LANGLOIS JH, 1990, PSYCHOL SCI, V1, P115, DOI 10.1111/j.1467-9280.1990.tb00079.x; Levy WB, 1996, NEURAL COMPUT, V8, P531, DOI 10.1162/neco.1996.8.3.531; Ma DS, 2015, BEHAV RES METHODS, V47, P1122, DOI 10.3758/s13428-014-0532-5; MALT BC, 1995, J EXP PSYCHOL LEARN, V21, P646, DOI 10.1037/0278-7393.21.3.646; Marr D., 1982, VISION COMPUTATIONAL; MAURO R, 1992, MEM COGNITION, V20, P433, DOI 10.3758/BF03210927; Murphy G., 2004, BIG BOOK CONCEPTS; Murphy GL, 2010, J EXP PSYCHOL LEARN, V36, P263, DOI 10.1037/a0018685; Navalpakkam V, 2005, VISION RES, V45, P205, DOI 10.1016/j.visres.2004.07.042; Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105; Owen HE, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0146328; Raichle ME, 2002, P NATL ACAD SCI USA, V99, P10237, DOI 10.1073/pnas.172399499; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Rhodes G, 1996, PSYCHOL SCI, V7, P105, DOI 10.1111/j.1467-9280.1996.tb00338.x; Said CP, 2011, PSYCHOL SCI, V22, P1183, DOI 10.1177/0956797611419169; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Song A., 2017, J VISION, V17, P837, DOI DOI 10.1167/17.10.837; Symons D, 1979, EVOLUTION HUMAN SEXU; THORNHILL R, 1993, HUM NATURE-INT BIOS, V4, P237, DOI 10.1007/BF02692201; Todorov A, 2015, ANNU REV PSYCHOL, V66, P519, DOI 10.1146/annurev-psych-113011-143831; Todorov A, 2011, SOC PERSONAL PSYCHOL, V5, P775, DOI 10.1111/j.1751-9004.2011.00389.x; Todorov A, 2008, TRENDS COGN SCI, V12, P455, DOI 10.1016/j.tics.2008.10.001; Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79; VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966; Vogel T, 2018, J EXP PSYCHOL LEARN, V44, P250, DOI 10.1037/xlm0000446; Winkielman P, 2006, PSYCHOL SCI, V17, P799, DOI 10.1111/j.1467-9280.2006.01785.x	45	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304012
C	Salehi, F; Abbasi, E; Hassibi, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Salehi, Fariborz; Abbasi, Ehsan; Hassibi, Babak			Learning without the Phase: Regularized PhaseMax Achieves Optimal Sample Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SIGNAL RECOVERY; RETRIEVAL; ALGORITHMS	The problem of estimating an unknown signal, x(0) is an element of R-n, from a vector y is an element of R-m consisting of m magnitude-only measurements of the form y(i) = vertical bar a(i)x(0)vertical bar, where a(i)'s are the rows of a known measurement matrix A, is a classical problem known as phase retrieval. This problem arises when measuring the phase is costly or altogether infeasible. In many applications in machine learning, signal processing, statistics, etc., the underlying signal has certain structure (sparse, low-rank, finite alphabet, etc.), opening of up the possibility of recovering x(0) from a number of measurements smaller than the ambient dimension, i.e., m < n. Ideally, one would like to recover the signal from a number of phaseless measurements that is on the order of the "degrees of freedom" of the structured x(0). To this end, inspired by the PhaseMax algorithm, we formulate a convex optimization problem, where the objective function relies on an initial estimate of the true signal and also includes an additive regularization term to encourage structure. The new formulation is referred to as regularized PhaseMax. We analyze the performance of regularized PhaseMax to find the minimum number of phaseless measurements required for perfect signal recovery. The results are asymptotic and are in terms of the geometrical properties (such as the Gaussian width) of certain convex cones. When the measurement matrix has i.i.d. Gaussian entries, we show that our proposed method is indeed order-wise optimal, allowing perfect recovery from a number of phaseless measurements that is only a constant factor away from the optimal number of measurements required when phase information is available. We explicitly compute this constant factor, in terms of the quality of the initial estimate, by deriving the exact phase transition. The theory well matches empirical results from numerical simulations.	[Salehi, Fariborz; Abbasi, Ehsan; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Salehi, F (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.	fsalehi@caltech.edu; eabbasi@caltech.edu; hassibi@caltech.edu			National Science Foundation [CNS-0932428, CCF-1018927, CCF-1423663, CCF-1409204]; Qualcomm Inc.; NASA's Jet Propulsion Laboratory through the President and Director's Fund; King Abdullah University of Science and Technology	National Science Foundation(National Science Foundation (NSF)); Qualcomm Inc.; NASA's Jet Propulsion Laboratory through the President and Director's Fund; King Abdullah University of Science and Technology(King Abdullah University of Science & Technology)	This work was supported in part by the National Science Foundation under grants CNS-0932428, CCF-1018927, CCF-1423663 and CCF-1409204, by a grant from Qualcomm Inc., by NASA's Jet Propulsion Laboratory through the President and Director's Fund, and by King Abdullah University of Science and Technology.	Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005; [Anonymous], 2015, ADV NEURAL INFORM PR; [Anonymous], 2015, MATHEMATICS-BASEL; Bahmani S., 2015, P ADV NEUR INF PROC, V28, P523; Bahmani S, 2017, PR MACH LEARN RES, V54, P252; Bakhshizadeh M, 2018, IEEE INT SYMP INFO, P2291; Bayati M, 2012, IEEE T INFORM THEORY, V58, P1997, DOI 10.1109/TIT.2011.2174612; Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443; Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004; Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Dainty JC, 1987, IMAG RECOVERY THEORY, V13, P231; Dhifallah O., 2018, ARXIV180509555; Dhifallah Oussama, 2017, COMP ADV MULT SENS A, P1; Dierolf M, 2010, NATURE, V467, P436, DOI 10.1038/nature09419; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Goldstein Tom, 2018, IEEE T INFORM THEORY; Hand P., 2016, ARXIV161103935; Jaganathan K, 2017, IEEE T SIGNAL PROCES, V65, P2402, DOI 10.1109/TSP.2017.2656844; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; MA J, 2018, ARXIV180101170; MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394; Mondelli M, 2017, ARXIV170805932; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Oymak S., 2013, ARXIV13110830; Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574; Rockafellar R.T., 2015, CONVEX ANAL; Rudelson M, 2006, 2006 40TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-4, P207, DOI 10.1109/CISS.2006.286463; Rudin W., 1976, PRINCIPLES MATH ANAL, V3; Salehi F, 2018, IEEE INT SYMP INFO, P976; Salehi F, 2017, INT CONF ACOUST SPEE, P3949, DOI 10.1109/ICASSP.2017.7952897; Soltanolkotabi M, 2017, ARXIV170206175; STOJNIC M, 2009, VARIOUS THRESHOLDS 1; Teng Zhang, 2017, ARXIV170608167; Thrampoulidis C., 2015, PROC C LEARN THEORY, P1683; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Tropp JA, 2015, APPL NUMER HARMON AN, P67, DOI 10.1007/978-3-319-19749-4_2; Vershynin Roman, 2016, INTRO APPL; Voroninski V., 2016, COMPRESSED SENSING P; Walther A., 1963, OPT ACTA, V10, P41, DOI DOI 10.1080/713817747; Wang G, 2018, IEEE T INFORM THEORY, V64, P773, DOI 10.1109/TIT.2017.2756858; Wang G, 2018, IEEE T SIGNAL PROCES, V66, P479, DOI 10.1109/TSP.2017.2771733; Wu S., 2018, ARXIV180610175; Yue M L, 2017, ARXIV170206435	49	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003022
C	Salehi, F; Thiran, P; Celis, LE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Salehi, Farnood; Thiran, Patrick; Celis, L. Elisa			Coordinate Descent with Bandit Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Coordinate descent methods usually minimize a cost function by updating a random decision variable (corresponding to one coordinate) at a time. Ideally, we would update the decision variable that yields the largest decrease in the cost function. However, finding this coordinate would require checking all of them, which would effectively negate the improvement in computational tractability that coordinate descent is intended to afford. To address this, we propose a new adaptive method for selecting a coordinate. First, we find a lower bound on the amount the cost function decreases when a coordinate is updated. We then use a multi-armed bandit algorithm to learn which coordinates result in the largest lower bound by interleaving this learning with conventional coordinate descent updates except that the coordinate is selected proportionately to the expected decrease. We show that our approach improves the convergence of coordinate descent methods both theoretically and experimentally.	[Salehi, Farnood; Thiran, Patrick; Celis, L. Elisa] Ecole Polytech Fed Lausanne, Sch Comp & Commun Sci, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Salehi, F (corresponding author), Ecole Polytech Fed Lausanne, Sch Comp & Commun Sci, Lausanne, Switzerland.	farnood.salehi@epfl.ch; patrick.thiran@epfl.ch; l.celis@epfl.ch						Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Arjevani Y., 2016, ADV NEURAL INFORM PR; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Borsos  Z, 2018, INT C LEARN THEOR; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Csiba D, 2015, PR MACH LEARN RES, V37, P674; Dunner C, 2016, PR MACH LEARN RES, V48; Dunner  Celestine, 2017, ADV NEURAL INFORM PR, V30, P4261; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Glasmachers T., 2013, AS C MACH LEARN ACML, P72; Johnson TB, 2017, PR MACH LEARN RES, V70; Namkoong H, 2017, PR MACH LEARN RES, V70; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Osokin A, 2016, PR MACH LEARN RES, V48; Perekrestenko  D, 2017, INT C ART INT STAT; Rakotomamonjy A, 2017, IEEE T NEUR NET LEAR, V28, P2789, DOI 10.1109/TNNLS.2016.2600243; Salehi  F, 2017, ARXIV170802544V2; Shalev-Shwartz S., 2013, ADV NEURAL INFORM PR, P378; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Shi H.-J., 2016, ARXIV161000040; Stich SU, 2017, PR MACH LEARN RES, V70; Zhang A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2035, DOI 10.1145/2939672.2939819; Zhao PL, 2015, PR MACH LEARN RES, V37, P1	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003077
C	Shafieezadeh-Abadeh, S; Nguyen, VA; Kuhn, D; Esfahani, PM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shafieezadeh-Abadeh, Soroosh; Viet Anh Nguyen; Kuhn, Daniel; Esfahani, Peyman Mohajerin			Wasserstein Distributionally Robust Kalman Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study a distributionally robust mean square error estimation problem over a nonconvex Wasserstein ambiguity set containing only normal distributions. We show that the optimal estimator and the least favorable distribution form a Nash equilibrium. Despite the non-convex nature of the ambiguity set, we prove that the estimation problem is equivalent to a tractable convex program. We further devise a Frank-Wolfe algorithm for this convex program whose direction-searching subproblem can be solved in a quasi-closed form. Using these ingredients, we introduce a distributionally robust Kalman filter that hedges against model risk.	[Shafieezadeh-Abadeh, Soroosh; Viet Anh Nguyen; Kuhn, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland; [Esfahani, Peyman Mohajerin] Delft Univ Technol, Delft Ctr Syst & Control, Delft, Netherlands	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Delft University of Technology	Shafieezadeh-Abadeh, S (corresponding author), Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.	soroosh.shafiee@epfl.ch; viet-anh.nguyen@epfl.ch; daniel.kuhn@epfl.ch; P.MohajerinEsfahani@tudelft.nl			Swiss National Science Foundation [BSCGI0_157733]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	We gratefully acknowledge financial support from the Swiss National Science Foundation under grant BSCGI0_157733.	Anderson B. D. O., 1979, OPTIMAL FILTERING; Basar T., 2008, IEEE T AUTOMAT CONTR; BERTSEKAS DP, 1971, IEEE T AUTOMAT CONTR, VAC16, P117, DOI 10.1109/TAC.1971.1099674; Bottou L., 2017, ARXIV170107875STATML; Chen YK, 2016, LECT NOTES COMPUT SC, V9910, P451, DOI 10.1007/978-3-319-46466-4_27; Cuturi M, 2014, J MACH LEARN RES, V15, P533; Eldar YC, 2004, IEEE T SIGNAL PROCES, V52, P1931, DOI 10.1109/TSP.2004.828931; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; GIVENS CR, 1984, MICH MATH J, V31, P231; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Lehmann E.L., 2006, THEORY POINT ESTIMAT; Levy BC, 2013, IEEE T AUTOMAT CONTR, V58, P682, DOI 10.1109/TAC.2012.2219952; Masarotto  V., 2018, ARXIV180101990; Nguyen V. A., 2018, OPTIMIZATION ONLINE; Ning LP, 2015, SIAM REV, V57, P167, DOI 10.1137/130921179; Rao C. R, 1973, LINEAR STAT INFERENC; Sayed AH, 2001, IEEE T AUTOMAT CONTR, V46, P998, DOI 10.1109/9.935054; SHAFIEEZADEHABADEH S, 2017, ARXIV171010016; Shtern S, 2016, MATH PROGRAM, V156, P615, DOI 10.1007/s10107-015-0910-5; Sinha A., 2018, ICLR; SPEYER JL, 1992, PROCEEDINGS OF THE 31ST IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P2293, DOI 10.1109/CDC.1992.371382; Xu H, 2009, IEEE T AUTOMAT CONTR, V54, P1171, DOI 10.1109/TAC.2009.2017816; Zhou K., 1996, ROBUST OPTIMAL CONTR; Zorzi M, 2017, IEEE T AUTOMAT CONTR, V62, P2902, DOI 10.1109/TAC.2016.2601879	27	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003007
C	Shamir, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shamir, Ohad			Are ResNets Provably Better than Linear Predictors?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A residual network (or ResNet) is a standard deep neural net architecture, with stateof-the-art performance across numerous applications. The main premise of ResNets is that they allow the training of each layer to focus on fitting just the residual of the previous layer's output and the target output. Thus, we should expect that the trained network is no worse than what we can obtain if we remove the residual layers and train a shallower network instead. However, due to the non-convexity of the optimization problem, it is not at all clear that ResNets indeed achieve this behavior, rather than getting stuck at some arbitrarily poor local minimum. In this paper, we rigorously prove that arbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor (namely a 1-layer network). Notably, we show this under minimal or no assumptions on the precise network architecture, data distribution, or loss function used. We also provide a quantitative analysis of approximate stationary points for this problem. Finally, we show that with a certain tweak to the architecture, training the network with standard stochastic gradient descent achieves an objective value close or better than any linear predictor.	[Shamir, Ohad] Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel	Weizmann Institute of Science	Shamir, O (corresponding author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel.	ohad.shamir@weizmann.ac.il			European Research Council (ERC) [754705]	European Research Council (ERC)(European Research Council (ERC))	We thank the anonymous NIPS 2018 reviewers for their helpful comments. This research is supported in part by European Research Council (ERC) grant 754705.	[Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Bartlett Peter L, 2018, ARXIV180206093; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; Du Simon S, 2018, ARXIV180301206; Ge R., 2017, ARXIV PREPRINT ARXIV; Hardt M., 2016, ARXIV161104231; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Jin Chi, 2017, ARXIV170300887; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Lee Jason D, 2016, ARXIV171200779; Liang Shiyu, 2018, ARXIV180300909; Ma, 2017, ADV NEURAL INFORM PR, P3656; MCCORMICK GP, 1977, MATH PROGRAM, V13, P111, DOI 10.1007/BF01584328; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; SAFRAN I., 2017, ARXIV171208968; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Soltanolkotabi M., 2017, ARXIV170704926; Soudry Daniel, 2017, ARXIV170205777; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xiong W, 2017, INT CONF ACOUST SPEE, P5255, DOI 10.1109/ICASSP.2017.7953159; Yun C., 2018, ARXIV180203487; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300047
C	Sharma, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sharma, Abhishek			Foreground Clustering for Joint Segmentation and Localization in Videos and Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper presents a novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representations at different levels, exploits the spatial relationship between bounding boxes and superpixels as linear constraints and simultaneously discriminates between foreground and background at bounding box and superpixel level. Different from previous approaches that mainly rely on discriminative clustering, we incorporate a foreground model that minimizes the histogram difference of an object across all image frames. Exploiting the geometric relation between the superpixels and bounding boxes enables the transfer of segmentation cues to improve localization output and vice-versa. Inclusion of the foreground model generalizes our discriminative framework to video data where the background tends to be similar and thus, not discriminative. We demonstrate the effectiveness of our unified framework on the YouTube Object video dataset, Internet Object Discovery dataset and Pascal VOC 2007.	[Sharma, Abhishek] Navinfo Europe Res, Eindhoven, Netherlands		Sharma, A (corresponding author), Navinfo Europe Res, Eindhoven, Netherlands.	kein.iitian@gmail.com			ERC Advanced grant VideoWorld	ERC Advanced grant VideoWorld	Part of this work was partially supported by ERC Advanced grant VideoWorld. Many thanks to Armand Joulin for helpful discussions.	Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; [Anonymous], 2016, ECCV; Bach F., 2007, NIPS; Bojanowski P., 2014, ECCV; Brox T., 2010, ECCV; Caelles S., 2017, CVPR; Caruana R., 1996, ICML; Cho Minsu, 2015, IEEE C COMP VIS PATT; Everingham M., 2007, TECH REPORT; Faktor A., 2013, ICCV; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Hariharan B., 2014, ECCV; He K., 2017, ICCV; Jerripothula Koteswar Rao, 2016, ECCV; Joulin A., 2010, CVPR; Joulin A., 2014, ECCV; Joulin A., 2012, CVPR; Karpathy A., 2014, NIPS; Kim G, 2012, CVPR; Kokkinos I, 2017, CVPR; Kolesnikov A., 2016, P EUR C COMP VIS ECC; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kwak S., 2015, ICCV; Ladicky L., 2010, ECCV; Lapin M., 2014, CVPR; Lee Y. J., 2011, ICCV; Li G., 2015, CVPR; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Maire M., 2011, ICCV; Miech A., 2017, ICCV; Mukherjee L., 2011, CVPR; Papazoglou A., 2013, CVPR; Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209; Pinheiro P.O., 2015, NIPS; Prest A., 2012, CVPR; Quan R., 2016, CVPR; Rother Carsten, 2006, CVPR; Rubinstein M., 2013, CVPR; Rubio J.C., 2012, CVPR; Sharma A., 2016, ECCV; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Tang K., 2014, CVPR; Vedaldi Andrea, 2008, ECCV; Vicente S., 2011, CVPR; Wang F., 2013, ICCV; Wang Z., 2013, ICCV; Xu J., 2015, CVPR; Xu L., 2005, NIPS, p[2, 3]; Yang W., 2010, CVPR; Yu S. X., 2003, NIPS; Zhang D., 2013, CVPR	51	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301066
C	Shin, R; Polosukhin, I; Song, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shin, Richard; Polosukhin, Illia; Song, Dawn			Improving Neural Program Synthesis with Inferred Execution Traces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, compared to other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces. While execution traces can provide highly detailed guidance for a program synthesis method, they are more difficult to obtain than more basic forms of specification such as input/output pairs. Therefore, we use the insight that we can split the process into two parts: infer traces from input/output examples, then infer programs from traces. Our application of this idea leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3% from the 77.12% of prior work.	[Shin, Richard; Song, Dawn] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Polosukhin, Illia] NEAR Protocol, San Francisco, CA USA; [Shin, Richard] NEAR, Oradell, NJ 07649 USA	University of California System; University of California Berkeley	Shin, R (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Shin, R (corresponding author), NEAR, Oradell, NJ 07649 USA.	ricshin@cs.berkeley.edu; illia@nearprotocol.com; dawnsong@cs.berkeley.edu			Berkeley Deep Drive; National Science Foundation [TWC-1409915]; Defence Advanced Research Projects Agency [FA8750-17-2-0091]	Berkeley Deep Drive; National Science Foundation(National Science Foundation (NSF)); Defence Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This material is in part based upon work supported by Berkeley Deep Drive, the National Science Foundation under Grant No. TWC-1409915, and the Defence Advanced Research Projects Agency under Grant No. FA8750-17-2-0091. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the above organizations.	[Anonymous], 2016, ARXIV PREPRINT ARXIV; Balog Matej, 2016, ABS161101989 CORR; Bhupatiraju  Surya, 2017, ABS170404327 CORR; Bunel R., 2018, INT C LEARN REPR; Cai J., 2017, P 5 INT C LEARNING R; Devlin J., 2017, ADV NEURAL INFORM PR, P2077; Devlin J, 2017, PR MACH LEARN RES, V70; Ganin Y., 2018, ABS180401118 CORR; Graves A., 2014, ABS14105401 CORR; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Grefenstette E., 2015, ADV NEURAL INF PROCE, P1828; Gregor K., 2015, ABS150204623 CORR; Gulwani S, 2011, POPL 11: PROCEEDINGS OF THE 38TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P317, DOI 10.1145/1926385.1926423; Joulin Armand, 2015, NIPS; Kaiser L., 2015, COMPUTER SCI; Kaiser  Lukasz, 2016, INT C LEARN REPR; Kevin Ellis, 2017, ABS170709627 CORR; Kurach Karol, 2016, ICLR; Manna  Zohar, 1975, P 4 INT JOINT C ART, V1, P288; Parisotto Emilio, 2017, INT C LEARN REPR; Pattis Richard E, 1981, KAREL ROBOT GENTLE I; Reed Scott, 2016, INT C LEARN REPR; Reed Scott, 2015, ICLR, V1, P5; Waldinger R.J., 1969, P 1 INT JOINT C ART, P241; Wang K., 2018, INT C LEARN REPR	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003047
C	Shivkumar, S; Lange, RD; Chattoraj, A; Haefner, RM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shivkumar, Sabyasachi; Lange, Richard D.; Chattoraj, Ankani; Haefner, Ralf M.			A probabilistic population code based on neural samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				STATISTICS; INFERENCE	Sensory processing is often characterized as implementing probabilistic inference: networks of neurons compute posterior beliefs over unobserved causes given the sensory inputs. How these beliefs are computed and represented by neural responses is much-debated (Fiser et al. 2010, Pouget et al. 2013). A central debate concerns the question of whether neural responses represent samples of latent variables (Hoyer & Hyvarinnen 2003) or parameters of their distributions (Ma et al. 2006) with efforts being made to distinguish between them (Grabska-Barwinska et al. 2013). A separate debate addresses the question of whether neural responses are proportionally related to the encoded probabilities (Barlow 1969), or proportional to the logarithm of those probabilities (Jazayeri & Movshon 2006, Ma et al. 2006, Beck et al. 2012). Here, we show that these alternatives - contrary to common assumptions - are not mutually exclusive and that the very same system can be compatible with all of them. As a central analytical result, we show that modeling neural responses in area V1 as samples from a posterior distribution over latents in a linear Gaussian model of the image implies that those neural responses form a linear Probabilistic Population Code (PPC, Ma et al. 2006). In particular, the posterior distribution over some experimenter-defined variable like "orientation" is part of the exponential family with sufficient statistics that are linear in the neural sampling-based firing rates.	[Shivkumar, Sabyasachi; Lange, Richard D.; Chattoraj, Ankani; Haefner, Ralf M.] Univ Rochester, Brain & Cognit Sci, Rochester, NY 14627 USA	University of Rochester	Shivkumar, S (corresponding author), Univ Rochester, Brain & Cognit Sci, Rochester, NY 14627 USA.	sshivkum@ur.rochester.edu; rlange@ur.rochester.edu; achattor@ur.rochester.edu; rhaefne2@ur.rochester.edu		Haefner, Ralf M/0000-0002-5031-0379	NEI/NIH [R01 EY028811, T32 EY007125]; NSF/NRT [NSF-1449828]	NEI/NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); NSF/NRT(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported by NEI/NIH awards R01 EY028811 (RMH) and T32 EY007125 (RDL), as well as an NSF/NRT graduate training grant NSF-1449828 (RDL, SS).	BARLOW HB, 1969, ANN NY ACAD SCI, V156, P872, DOI 10.1111/j.1749-6632.1969.tb14019.x; Beck J., 2012, ADV NEURAL INFORM PR, V25, P3059; Beck JM, 2011, J NEUROSCI, V31, P15310, DOI 10.1523/JNEUROSCI.1706-11.2011; Beck JM, 2008, NEURON, V60, P1142, DOI 10.1016/j.neuron.2008.09.021; Bill J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134356; Bornschein J, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003062; Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211; Fiser J, 2010, TRENDS COGN SCI, V14, P119, DOI 10.1016/j.tics.2010.01.003; Grabska-Barwinska A., 2013, ADV NEURAL INFORM PR, V26, P1968; Haefner RM, 2016, NEURON, V90, P649, DOI 10.1016/j.neuron.2016.03.020; Henniges M, 2010, LECT NOTES COMPUT SC, V6365, P450, DOI 10.1007/978-3-642-15995-4_56; Hoyer PO, 2003, ADV NEURAL INFORM PR, V15, P293; Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Orban G, 2016, NEURON, V92, P530, DOI 10.1016/j.neuron.2016.09.038; Parker AJ, 1998, ANNU REV NEUROSCI, V21, P227, DOI 10.1146/annurev.neuro.21.1.227; Pecevski D, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002294; Pitkow X, 2017, NEURON, V94, P943, DOI 10.1016/j.neuron.2017.05.028; Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495; Raju R.V., 2016, ADV NEURAL INFORM PR, P2029; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Savin C, 2014, ADV NEURAL INF PROCE, P2024; Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193	25	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001060
C	Shrivastava, H; Bart, E; Price, B; Dai, HJ; Dai, B; Aluru, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shrivastava, Harsh; Bart, Eugene; Price, Bob; Dai, Hanjun; Dai, Bo; Aluru, Srinivas			Cooperative neural networks (CoNN): Exploiting prior independence structure for improved classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a new approach, called cooperative neural networks (CoNN), which uses a set of cooperatively trained neural networks to capture latent representations that exploit prior given independence structure. The model is more flexible than traditional graphical models based on exponential family distributions, but incorporates more domain specific prior structure than traditional deep networks or variational autoencoders. The framework is very general and can be used to exploit the independence structure of any graphical model. We illustrate the technique by showing that we can transfer the independence structure of the popular Latent Dirichlet Allocation (LDA) model to a cooperative neural network, CoNN-sLDA. Empirical evaluation of CoNN-sLDA on supervised text classification tasks demonstrates that the theoretical advantages of prior independence structure can be realized in practice - we demonstrate a 23% reduction in error on the challenging MultiSent data set compared to state-of-the-art.	[Shrivastava, Harsh; Dai, Hanjun; Dai, Bo; Aluru, Srinivas] Georgia Tech, Atlanta, GA 30332 USA; [Bart, Eugene; Price, Bob] PARC, Palo Alto, CA USA	University System of Georgia; Georgia Institute of Technology	Shrivastava, H (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	hshrivastava3@gatech.edu; bart@parc.com; bprice@parc.com; hanjundai@gatech.edu; bodai@gatech.edu; aluru@cc.gatech.edu	Dai, Hanjun/AAQ-8943-2021					Bengio Y, 2014, PR MACH LEARN RES, V32, P226; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blitzer J., P 45 ANN M ASS COMP, P440, DOI DOI 10.1109/IRPS.2011.5784441; Chen JS, 2015, ADV NEUR IN, V28; Chien JT, 2018, IEEE T PATTERN ANAL, V40, P318, DOI 10.1109/TPAMI.2017.2677439; Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPRW.2009.5206800, 10.1109/CVPR.2009.5206800]; Dai HJ, 2016, PR MACH LEARN RES, V48; Dieng A.B., 2016, ICLR; Gan Z, 2015, PR MACH LEARN RES, V37, P1823; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Joachims T., 1998, P EUROPEAN C MACHINE, P137, DOI [10.1007/bfb0026683, 10.1007/BFb0026683]; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Lacoste-Julien S, 2009, ADV NEURAL INFORM PR, P897, DOI DOI 10.1007/S10618-010-0175-9; Larochelle H., 2012, ADV NEURAL INFORM PR, P2708; Li Wei, 2006, PACHINKO ALLOCATION; Mcauliffe Jon D., 2008, P ADV NEURAL INFORM, P121; Miao YS, 2016, PR MACH LEARN RES, V48; Mikolov T., 2013, ARXIV; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mousavi HS, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1483; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sabour Sara, 2017, PROC 31 INT C NEURAL; Sriperumbudur Bharath K, 2008, INJECTIVE HILBERT SP; Srivastava N., 2013, P 29 C UNC ART INT U; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tang Yichuan, 2013, ADV NEURAL INFORM PR, P530; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright Martin J, 2003, AISTATS; Zheng Y, 2016, IEEE T PATTERN ANAL, V38, P1056, DOI 10.1109/TPAMI.2015.2476802; Zhu J, 2009, P 26 ANN INT C MACH, P1257	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304016
C	Smieja, M; Struski, L; Tabor, J; Zielinski, B; Spurek, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Smieja, Marek; Struski, Lukasz; Tabor, Jacek; Zielinski, Bartosz; Spurek, Przemyslaw			Processing of missing data by neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CHAINED EQUATIONS; IMPUTATION; MACHINE; VALUES	We propose a general, theoretically justified mechanism for processing missing data by neural networks. Our idea is to replace typical neuron's response in the first hidden layer by its expected value. This approach can be applied for various types of networks at minimal cost in their modification. Moreover, in contrast to recent approaches, it does not require complete data for training. Experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.	[Smieja, Marek; Struski, Lukasz; Tabor, Jacek; Zielinski, Bartosz; Spurek, Przemyslaw] Jagiellonian Univ, Fac Math & Comp Sci, Lojasiewicza 6, PL-30348 Krakow, Poland	Jagiellonian University	Smieja, M (corresponding author), Jagiellonian Univ, Fac Math & Comp Sci, Lojasiewicza 6, PL-30348 Krakow, Poland.	marek.smieja@uj.edu.pl; lukasz.struski@uj.edu.pl; jacek.tabor@uj.edu.pl; bartosz.zielinski@uj.edu.pl; przemyslaw.spurek@uj.edu.pl			National Science Centre, Poland [2016/21/D/ST6/00980, 2015/19/B/ST6/01819, 2015/19/D/ST6/01215, 2015/19/D/ST6/01472]	National Science Centre, Poland(National Science Centre, Poland)	This work was partially supported by National Science Centre, Poland (grants no. 2016/21/D/ST6/00980, 2015/19/B/ST6/01819, 2015/19/D/ST6/01215, 2015/19/D/ST6/01472). We would like to thank the anonymous reviewers for their valuable comments on our paper.	Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907; Asuncion A, 2007, UCI MACHINE LEARNING; Azur MJ, 2011, INT J METH PSYCH RES, V20, P40, DOI 10.1002/mpr.329; Batista GE., 2002, HIS, V87, P48; Bengio Y, 1996, ADV NEUR IN, V8, P395; Chechik G, 2008, J MACH LEARN RES, V9, P1; Dekel O, 2010, MACH LEARN, V81, P149, DOI 10.1007/s10994-009-5124-8; Dick U., 2008, P INT C MACH LEARN, P232; Ghahramani Z., 1994, P ADV NEUR INF PROC, P120; Globerson Amir, 2006, P 23 INT C MACH LEAR, P353, DOI DOI 10.1145/1143844.1143889; Goldberg A., 2010, P NIPS, V23, P757; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gupta  Maya, 2016, J MACHINE LEARNING R, V17, P3790; Hazan E, 2015, PR MACH LEARN RES, V37, P257; Junyuan X, 2012, ADV NEURAL INF PROCE, P341; Liao  Xuejun, 2007, P INT C MACH LEARN, P553; McKnight P. E., 2007, MISSING DATA GENTLE; Mesquita DPP, 2016, PROCEEDINGS OF 2016 5TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS 2016), P85, DOI [10.1109/BRACIS.2016.026, 10.1109/BRACIS.2016.16]; Nowicki R, 2016, 2016 21ST INTERNATIONAL CONFERENCE ON METHODS AND MODELS IN AUTOMATION AND ROBOTICS (MMAR), P820, DOI 10.1109/MMAR.2016.7575243; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pelckmans K, 2005, NEURAL NETWORKS, V18, P684, DOI 10.1016/j.neunet.2005.06.025; Rao C. R., 1973, LINEAR STAT INFERENC, V2; SHARPE PK, 1995, NEURAL COMPUT APPL, V3, P73, DOI 10.1007/BF01421959; Shivaswamy PK, 2006, J MACH LEARN RES, V7, P1283; Smieja  Marek, 2016, ARXIV161201480; Smola Alexander J, 2005, P INT C ART INT STAT; Sovilj D, 2016, NEUROCOMPUTING, V174, P220, DOI 10.1016/j.neucom.2015.03.108; Tresp V., 1994, ADV NEURAL INF PROCE, V6, P128; van Buuren S, 2011, J STAT SOFTW, V45, P1; Williams D., 2005, P 22 INT C MACH LEAR, P972; Williams David, 2005, P ICML WORKSH LEARN; Xia J, 2017, PATTERN RECOGN, V69, P52, DOI 10.1016/j.patcog.2017.04.005; Yang C., 2017, IEEE C COMP VIS PATT, V1, P3; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Yoon J, 2018, PR MACH LEARN RES, V80	36	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302071
C	Srinivasan, S; Lanctot, M; Zambaldi, V; Perolat, J; Tuyls, K; Munos, R; Bowling, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Srinivasan, Sriram; Lanctot, Marc; Zambaldi, Vinicius; Perolat, Julien; Tuyls, Karl; Munos, Remi; Bowling, Michael			Actor-Critic Policy Optimization in Partially Observable Multiagent Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPREHENSIVE SURVEY; REINFORCEMENT; GAME	Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero-sum games, without any domain-specific state space reductions.	[Srinivasan, Sriram; Lanctot, Marc; Zambaldi, Vinicius; Perolat, Julien; Tuyls, Karl; Munos, Remi; Bowling, Michael] DeepMind, London, England		Srinivasan, S (corresponding author), DeepMind, London, England.	srsrinivasan@google.com; lanctot@google.com; vzambaldi@google.com; perolat@google.com; karltuyls@google.com; munos@google.com; bowlingm@google.com						Abdallah S, 2008, J ARTIF INTELL RES, V33, P521, DOI 10.1613/jair.2628; Abdolmaleki A., 2018, CORR; Albrecht SV, 2018, ARTIF INTELL, V258, P66, DOI 10.1016/j.artint.2018.01.002; Allen Cameron, 2017, CORR; [Anonymous], 2018, P INT C LEARN REPR I; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bansal T, 2018, P 6 INT C LEARN REPR; Bansal T., 2018, INT C LEARN REPR 201; Bloembergen D, 2015, J ARTIF INTELL RES, V53, P659, DOI 10.1613/jair.4818; Blum A, 2007, ALGORITHMIC GAME THEORY, P79; Bosansky B, 2016, ARTIF INTELL, V237, P1, DOI 10.1016/j.artint.2016.03.005; Bowling M, 2002, ARTIF INTELL, V136, P215, DOI 10.1016/S0004-3702(02)00121-2; Bowling M., 2005, ADV NEURAL INFORM PR, P209; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Brown GC, 2017, SCIENCE, V356, P1123, DOI 10.1126/science.aan7893; Brown Noam, 2017, P AAAI C ART INT; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Cao K., 2018, P 6 INT C LEARN REPR; Ciosek Kamil, 2018, P 32 AAAI C ART INT; Daskalakis C., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P71, DOI 10.1145/1132516.1132527; Duan Y., 2016, CORR; Espeholt Lasse, 2018, CORR; Foerster J. N., 2017, P INT C AUT AG MULT; Foerster J. N., 2017, P 32 AAAI C ART INT; Gatti N, 2013, P 27 AAAI C ART INT, P335; Gibson Richard, 2013, CORR; Gu S., 2016, CORR; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hazan E., 2015, FDN TRENDS OPTIM, V2, P157; He He, 2016, P 33 INT C MACH LEAR; Heinrich J., 2015, P 32 INT C MACH LEAR; Heinrich Johannes, 2016, CORR; Hernandez-Leal P., 2017, CORR; Hofbauer J., 1998, EVOLUTIONARY GAMES P; Hofbauer J, 2009, MATH OPER RES, V34, P263, DOI 10.1287/moor.1080.0359; Hong Zhang-Wei, 2017, CORR; Jaderberg Max, 2017, P INT C REPR LEARN; Jin Peter H., 2017, CORR; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kakade S., 2002, P 19 INT C MACH LEAR; Kash Ian A., 2018, EUR WORKSH REINF LEA; Kingma D.P, P 3 INT C LEARNING R; Kovarik Vojtech, 2015, CORR; Kuhn HW., 1953, CONTRIBUTIONS THEORY, P193; Lanctot M., 2013, THESIS; Lanctot M., 2009, ADV NEURAL INFORM PR, P1078; Lanctot M., 2017, ADV NEURAL INFORM PR; Lanctot M, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1257; Lazaridou A., 2017, P INT C LEARN REPR I; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lerer A, 2017, ARXIVABS170701068 CO; Lillicrap T., 2015, CORR; Littman ML, 1994, ICML 1994, P157; Lowe R., 2017, P INT C NEUR INF PRO, P6379; Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moravcik M., 2017, SCIENCE, V358; Neller Todd W., 2013, P MOD AI ASS 4 S ED; Nguyen D. T., 2017, ADV NEURAL INFORM PR, P4319; Nowe A, 2012, ADAPT LEARN OPTIM, V12, P441; Oliehoek F.A., 2016, CONCISE INTRO DECENT; Panozzo F, 2014, AAAI CONF ARTIF INTE, P2034; Peng Xingchao, 2018, CORR; Perolat Julien, 2018, P 21 INT C ART INT S, V84, P919; Perolat Julien, 2015, P INT C MACH LEARN I; Perolat Julien, 2016, 19 INT C ART INT STA; Peters Jan, 2002, TRCLMC20071 U SO CAL; Quillen Deirdre, 2018, CORR; Risk N. A., 2010, P 9 INT C AUT AG MUL, V1, P159; Sandholm WilliamH., 2010, POPULATION GAMES EVO; Schulman J., 2015, CORR; Schulman J., 2017, ABS170706347 CORR; Shalev-Shwartz S., 2016, CORR; Shapley L., 1964, ADV GAME THEORY; Shoham Y., 2009, MULTIAGENT SYSTEMS A; Silver D., 2017, CORR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Silver David, 2017, NATURE, V530, P354; Singh S. P., 2000, P 16 C UNC ART INT S, P541; Srinivasan S, 2018, ADV NEURAL INFORM PR, P3426; Sutton Richard S., 2001, COMP POLICY GR UNPUB; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tammelin O., 2015, P 24 INT JOINT C ART; TAYLOR PD, 1978, MATH BIOSCI, V40, P145, DOI 10.1016/0025-5564(78)90077-9; Tuyls K, 2018, AAMAS; VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165; WALSH WE, 2003, P 5 WORKSH AG MED EL; Walsh William E, 2002, AAAI; Waugh K., 2015, P AAAI C ART INT; Wellman M. P., 2006, AAAI, P1552; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu C., 2018, P INT C LEARN REPR I; Wu Yuxin, 2017, P INT C REPR LEARN; Wunder M., 2010, P 27 INT C MACH LEAR; Yu Q, 2016, SCI REP-UK, V6, DOI 10.1038/srep28585; Zhang CJ, 2010, AAAI CONF ARTIF INTE, P927; ZINKEVICH M, 2003, CMUCS03110; Zinkevich M., 2003, INT C MACH LEARN ICM; Zinkevich M, 2005, P 18 INT C NEUR INF, P1641; Zinkevich M., 2008, ADV NEURAL INFORM PR	102	0	0	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303042
C	Strouse, DJ; Kleiman-Weiner, M; Tenenbaum, J; Botvinick, M; Schwab, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Strouse, D. J.; Kleiman-Weiner, Max; Tenenbaum, Josh; Botvinick, Matt; Schwab, David			Learning to Share and Hide Intentions using Information Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning to cooperate with friends and compete with foes is a key component of multi-agent reinforcement learning. Typically to do so, one requires access to either a model of or interaction with the other agent(s). Here we show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction. Our approach is to encourage an agent to reveal or hide their intentions using an information-theoretic regularizer. We consider both the mutual information between goal and action given state, as well as the mutual information between goal and state. We show how to optimize these regularizers in a way that is easy to integrate with policy gradient reinforcement learning. Finally, we demonstrate that cooperative (competitive) policies learned with our approach lead to more (less) reward for a second agent in two simple asymmetric information games.	[Strouse, D. J.] Princeton Univ, Princeton, NJ 08544 USA; [Kleiman-Weiner, Max; Tenenbaum, Josh] MIT, Cambridge, MA 02139 USA; [Botvinick, Matt] DeepMind, London, England; [Botvinick, Matt] UCL, London, England; [Schwab, David] CUNY, Grad Ctr, New York, NY USA	Princeton University; Massachusetts Institute of Technology (MIT); University of London; University College London; City University of New York (CUNY) System	Strouse, DJ (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.				Hertz Foundation; Center for Brain, Minds and Machines (NSF) [1231216]; NSF Center for the Physics of Biological Function [PHY-1734030]; MMLS	Hertz Foundation; Center for Brain, Minds and Machines (NSF); NSF Center for the Physics of Biological Function; MMLS	The authors would like to acknowledge Dan Roberts and our anonymous reviewers for careful comments on the original draft; Jane Wang, David Pfau, and Neil Rabinowitz for discussions on the original idea; and funding from the Hertz Foundation (DJ and Max), The Center for Brain, Minds and Machines (NSF #1231216) (Max and Josh), the NSF Center for the Physics of Biological Function (PHY-1734030) (David), and as a Simons Investigator in the MMLS (David).	Abadi Martin, 2016, P 12TH USENIX C OPER; Baker CL, 2009, COGNITION, V113, P329, DOI 10.1016/j.cognition.2009.07.005; Bellemare M., 2016, NEURIPS; Cho K., 2014, P 2014 C EMP METH NA, P1724; Dragan AD, 2013, ACMIEEE INT CONF HUM, P301, DOI 10.1109/HRI.2013.6483603; Eysenbach B., 2019, INT C LEARN REPR ICL INT C LEARN REPR ICL; Foerster J, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P122; Hadfield-Menell D, 2016, ADV NEURAL INFORM PR, V29, P3909; Ho M. K., 2016, ADV NEURAL INFORM PR, P3027; Hughes E., 2018, ADV NEURAL INFORM PR, P3330; Jaques Natasha, 2018, CORR; Kavukcuoglu K, 2017, INT C LEARN REPR ICL; Kleiman-Weiner M., 2016, P 38 ANN C COGNITIVE; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Littman ML, 1994, ICML 1994, P157; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Ng A., 2004, P 21 INT C MACH LEAR; Ostrovski G, 2017, PR MACH LEARN RES, V70; Peysakhovich A, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P2043; Rabinowitz N. C., 2018, P MACHINE LEARN RES, V80, P4218; Shafto P, 2014, COGNITIVE PSYCHOL, V71, P55, DOI 10.1016/j.cogpsych.2013.12.004; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Teh Y., 2017, ADV NEURAL INFORM PR, P4496; Tomasello M, 2005, BEHAV BRAIN SCI, V28, P675, DOI 10.1017/S0140525X05000129; Tsitsiklis JN, 2018, OPER RES, V66, P587, DOI 10.1287/opre.2017.1682; Ullman T.D., 2009, 22 INT C NEUR INF PR, P1874	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004077
C	Sugiyama, M; Nakahara, H; Tsuda, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sugiyama, Mahito; Nakahara, Hiroyuki; Tsuda, Koji			Legendre Decomposition for Tensors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods.	[Sugiyama, Mahito] PRESTO, JST, Natl Inst Informat, Kawaguchi, Saitama, Japan; [Nakahara, Hiroyuki] RIKEN Ctr Brain Sci, Wako, Saitama, Japan; [Tsuda, Koji] Univ Tokyo, NIMS, Tokyo, Japan; [Tsuda, Koji] RIKEN AIP, Tokyo, Japan	Japan Science & Technology Agency (JST); Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan; RIKEN; National Institute for Materials Science; University of Tokyo; RIKEN	Sugiyama, M (corresponding author), PRESTO, JST, Natl Inst Informat, Kawaguchi, Saitama, Japan.	mahito@nii.ac.jp; hiro@brain.riken.jp; tsuda@k.u-tokyo.ac.jp	Nakahara, Hiroyuki/N-5411-2015; Sugiyama, Mahito/AAD-9213-2019	Nakahara, Hiroyuki/0000-0001-6891-1175; Sugiyama, Mahito/0000-0001-5907-9831	JSPS KAKENHI [26120732, 16H06570, JP16K16115, JP16H02870]; JST, PRESTO, Japan [JPMJPR1855]; JST CREST [JPMJCR1502]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST, PRESTO, Japan; JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	This work was supported by JSPS KAKENHI Grant Numbers JP16K16115, JP16H02870, and JST, PRESTO Grant Number JPMJPR1855, Japan (M.S.); JSPS KAKENHI Grant Numbers 26120732 and 16H06570 (H.N.); and JST CREST JPMJCR1502 (K.T.).	ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Alex M, 2002, LECT NOTES COMPUT SC, V2350, P447; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S, 2001, IEEE T INFORM THEORY, V47, P1701, DOI 10.1109/18.930911; Amari S.-I., 2016, INFORM GEOMETRY ITS; Amari S, 2009, LECT NOTES COMPUT SC, V5416, P75; Bader B. W., 2017, MATLAB TENSOR TOOLBO; Bader BW, 2007, SIAM J SCI COMPUT, V30, P205, DOI 10.1137/060676489; Beckmann CF, 2005, NEUROIMAGE, V25, P294, DOI 10.1016/j.neuroimage.2004.10.043; CENSOR Y, 1981, J OPTIMIZ THEORY APP, V34, P321, DOI 10.1007/BF00934676; Chen J, 2018, PHYS REV B, V97, DOI 10.1103/PhysRevB.97.085104; Cichocki A, 2015, IEEE SIGNAL PROC MAG, V32, P145, DOI 10.1109/MSP.2013.2297439; Davey B. A., 2002, INTRO LATTICES ORDER, V2nd, DOI DOI 10.1017/CBO9780511809088; Gierz G., 2003, CONTINUOUS LATTICES, V93; Harshman R.A., 1970, WORKING PAPERS PHONE; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Kim Y-D, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/BTAS.2007.4401913; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kossaifi J., 2016, ARXIV161009555; LeCun Y., 2007, PREDICTING STRUCTURE; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee DD, 2001, ADV NEUR IN, V13, P556; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Nakahara H, 2002, NEURAL COMPUT, V14, P2269, DOI 10.1162/08997660260293238; Salakhutdinov R., 2008, 2008002 UTML TR; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311; Sejnowski T. J., 1986, AIP Conference Proceedings, P398, DOI 10.1063/1.36246; Shashua A., 2005, P 22 INT C MACHINE L, P792, DOI [10.1145/1102351.1102451, DOI 10.1145/1102351.1102451]; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Sugiyama M., 2017, P 34 INT C MACH LEAR, P3270; Sugiyama M, 2016, IEEE INT SYMP INFO, P575, DOI 10.1109/ISIT.2016.7541364; Symeonidis P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P429, DOI 10.1145/2959100.2959195; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Vasilescu MAO, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.906024; Yilmaz KY, 2011, ADV NEURAL INFORM PR, P2151; Yilmaz YK, 2012, SIGNAL PROCESS, V92, P1853, DOI 10.1016/j.sigpro.2011.09.033	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003037
C	Sun, W; Gordon, GJ; Boots, B; Bagnell, JA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sun, Wen; Gordon, Geoffrey J.; Boots, Byron; Bagnell, J. Andrew			Dual Policy Iteration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SEARCH	A novel class of Approximate Policy Iteration (API) algorithms have recently demonstrated impressive practical performance (e.g., ExIt [1], AlphaGo-Zero [2]). This new family of algorithms maintains, and alternately optimizes, two policies: a fast, reactive policy (e.g., a deep neural network) deployed at test time, and a slow, non-reactive policy (e.g., Tree Search), that can plan multiple steps ahead. The reactive policy is updated under supervision from the non-reactive policy, while the non-reactive policy is improved via guidance from the reactive policy. In this work we study this class of Dual Policy Iteration (DPI) strategy in an alternating optimization framework and provide a convergence analysis that extends existing API theory. We also develop a special instance of this framework which reduces the update of non-reactive policies to model-based optimal control using learned local models, and provides a theoretically sound way of unifying model-free and model-based RL approaches with unknown dynamics. We demonstrate the efficacy of our approach on various continuous control Markov Decision Processes.	[Sun, Wen; Gordon, Geoffrey J.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Boots, Byron] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Bagnell, J. Andrew] Aurora Innovat, Eugene, OR USA	Carnegie Mellon University; University System of Georgia; Georgia Institute of Technology; Aurora Innovation Inc.	Sun, W (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	wensun@cs.cmu.edu; ggordon@cs.cmu.edu; bboots@cc.gatech.edu; dbagnell@cs.cmu.edu			Office of Naval Research [N000141512365]	Office of Naval Research(Office of Naval Research)	We thank Sergey Levine for valuable discussion. WS is supported in part by Office of Naval Research contract N000141512365	Abbeel P., 2005, ADV NEURAL INFORM PR, P1; [Anonymous], 1972, LINEAR OPTIMAL CONTR; [Anonymous], 1996, ROBUST OPTIMAL CONTR; [Anonymous], 2002, ICML; Anthony Thomas, 2017, ARXIV170508439; ATKESON C., 1994, P C NEURAL INFORM PR, P663; Atkeson C. G., 2003, ADV NEURAL INFORM PR, V15, P1643; Atkeson CG, 2012, P AMER CONTR CONF, P5220; Bagnell J. A., 2003, IJCAI; Bagnell JA, 2004, ADV NEUR IN, V16, P831; Bagnell JA, 2001, IEEE INT CONF ROBOT, P1615, DOI 10.1109/ROBOT.2001.932842; Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953; Finn C., 2016, GUIDED POLICY SEARCH; Gorodetsky A, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Kakade Sham, 2002, NIPS; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Lazaric A., 2010, P 27 INT C MACH LEAR, P607; Levine S, 2016, J MACH LEARN RES, V17; Levine S, 2014, ADV NEUR IN, V27; Mania H., 2018, ARXIV PREPRINT ARXIV; Montgomery William, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3373, DOI 10.1109/ICRA.2017.7989383; Montgomery W, 2016, ADV NEUR IN, V29; Ross S., 2012, ICML; Ross S., 2011, P INT C ARTIFICIAL I, P627; Ross S., 2014, ARXIV14065979; Rummery G.A., 1994, ON LINE Q LEARNING U, V37; Scherrer B, 2014, PR MACH LEARN RES, V32, P1314; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Silver D., 2016, NATURE; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sun W., 2017, ICML; Sun W, 2016, PR MACH LEARN RES, V48; Sutton R. S., 1998, INTRO REINFORCEMENT, V135; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Venkatraman Arun, 2015, AAAI; Williams R. J., 1992, MACHINE LEARNING; Ziebart B.D., 2010, THESIS CARNEGIE MELL; Zinkevich M, 2003, ICML	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001059
C	Sun, WW; Lu, JW; Liu, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sun, Will Wei; Lu, Junwei; Liu, Han			Sketching Method for Large Scale Combinatorial Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FALSE DISCOVERY RATE; SELECTION; CLIQUES; NETWORK	We present computationally efficient algorithms to test various combinatorial structures of large-scale graphical models. In order to test the hypotheses on their topological structures, we propose two adjacency matrix sketching frameworks: neighborhood sketching and subgraph sketching. The neighborhood sketching algorithm is proposed to test the connectivity of graphical models. This algorithm randomly subsamples vertices and conducts neighborhood regression and screening. The global sketching algorithm is proposed to test the topological properties requiring exponential computation complexity, especially testing the chromatic number and the maximum clique. This algorithm infers the corresponding property based on the sampled subgraph. Our algorithms are shown to substantially accelerate the computation of existing methods. We validate our theory and method through both synthetic simulations and a real application in neuroscience.	[Sun, Will Wei] Univ Miami, Dept Management Sci, Coral Gables, FL 33124 USA; [Lu, Junwei] Harvard Univ, Dept Biostat, Cambridge, MA 02138 USA; [Liu, Han] Northwestern Univ, Dept Comp Sci, Evanston, IL 60208 USA	University of Miami; Harvard University; Northwestern University	Sun, WW (corresponding author), Univ Miami, Dept Management Sci, Coral Gables, FL 33124 USA.	wsun@bus.miami.edu; junweilu@hsph.harvard.edu; hanliu@northwestern.edu			NSF BIGDATA [1840866]; NSF [RI 1408910]; NSF CAREER [1841569]; Alfred P Sloan Fellowship; NSF TRIPODS [1740735]	NSF BIGDATA; NSF(National Science Foundation (NSF)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Alfred P Sloan Fellowship(Alfred P. Sloan Foundation); NSF TRIPODS	Han Liu's research is supported by the NSF BIGDATA 1840866, NSF RI 1408910, NSF CAREER 1841569, NSF TRIPODS 1740735, along with an Alfred P Sloan Fellowship.	Ames DL, 2015, J COGNITIVE NEUROSCI, V27, P655, DOI 10.1162/jocn_a_00728; Barber RF, 2015, ANN STAT, V43, P2055, DOI 10.1214/15-AOS1337; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; BRON C, 1973, COMMUN ACM, V16, P575, DOI 10.1145/362342.362367; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Donoho D, 2008, P NATL ACAD SCI USA, V105, P14790, DOI 10.1073/pnas.0807471105; Efron B., 2012, LARGE SCALE INFERENC, V1; Eppstein D, 2009, ACM T ALGORITHMS, V5, DOI 10.1145/1497290.1497291; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Goldreich O, 2002, ALGORITHMICA, V32, P302, DOI 10.1007/s00453-001-0078-7; Goldreich O, 1998, J ACM, V45, P653, DOI 10.1145/285055.285060; Lawler E. L., 1976, Information Processing Letters, V5, P66, DOI 10.1016/0020-0190(76)90065-X; LIU H, 2012, NEURAL INFORM PROCES; Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037; Liu H, 2009, J MACH LEARN RES, V10, P2295; Lu J., 2015, ARXIV151208298; LU J, 2017, ARXIV170709114; Luscombe NM, 2004, NATURE, V431, P308, DOI 10.1038/nature02782; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; NEYKOV M., 2018, PROPERTY TESTING HIG; NEYKOV M, 2016, ARXIV160803045; NEYKOV M, 2015, ARXIV151008986; Ning Y, 2017, ANN STAT, V45, P2299, DOI 10.1214/16-AOS1483; Romano JR, 2005, J AM STAT ASSOC, V100, P94, DOI 10.1198/016214504000000539; Rubinov M, 2010, NEUROIMAGE, V52, P1059, DOI 10.1016/j.neuroimage.2009.10.003; Simony E, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12141; Tomita E, 2006, THEOR COMPUT SCI, V363, P28, DOI 10.1016/j.tcs.2006.06.015; Westfall P. H., 1993, TECHNOMETRICS, V35, P450, DOI [10.2307/1270279, DOI 10.2307/1270279]	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005020
C	Tacchetti, A; Voinea, S; Evangelopoulos, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tacchetti, Andrea; Voinea, Stephen; Evangelopoulos, Georgios			Trading robust representations for sample complexity through self-supervised visual experience	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning in small sample regimes is among the most remarkable features of the human perceptual system. This ability is related to robustness to transformations, which is acquired through visual experience in the form of weak- or self-supervision during development. We explore the idea of allowing artificial systems to learn representations of visual stimuli through weak supervision prior to downstream supervised tasks. We introduce a novel loss function for representation learning using unlabeled image sets and video sequences, and experimentally demonstrate that these representations support one-shot learning and reduce the sample complexity of multiple recognition tasks. We establish the existence of a trade-off between the sizes of weakly supervised, automatically obtained from video sequences, and fully supervised data sets. Our results suggest that equivalence sets other than class labels, which are abundant in unlabeled visual experience, can be used for self-supervised learning of semantically relevant image embeddings.	[Tacchetti, Andrea; Voinea, Stephen; Evangelopoulos, Georgios] MIT, Ctr Brains Minds & Machines, McGovern Inst Brain Res, Cambridge, MA 02139 USA; [Tacchetti, Andrea] DeepMind, London, England; [Evangelopoulos, Georgios] X Alphabet, Mountain View, CA USA	Massachusetts Institute of Technology (MIT)	Tacchetti, A (corresponding author), MIT, Ctr Brains Minds & Machines, McGovern Inst Brain Res, Cambridge, MA 02139 USA.; Tacchetti, A (corresponding author), DeepMind, London, England.	atacchet@mit.edu; voinea@mit.edu; gevang@mit.edu			McGovern Institute for Brain Research at MIT; Center for Brains, Minds and Machines (CBMM) - NSF STC [CCF-1231216]	McGovern Institute for Brain Research at MIT; Center for Brains, Minds and Machines (CBMM) - NSF STC	We would like to thank Tomaso Poggio for his advice and supervision throughout the project and the McGovern Institute for Brain Research at MIT for supporting this research. The DGX-1 used for our experiments was donated by NVIDIA. This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216.	[Anonymous], 2011, ICANN; Anselmi F., 2016, THEORETICAL COMPUTER; Anselmi F., 2015, INFORM INFERENCE; Anselmi F., 2019, PATTERN RECOGNITION; Arcaro M. J., 2017, NATURE NEUROSCIENCE; Bengio Y., 2013, IEEE PAMI; Bruna J., 2013, IEEE PAMI; Chopra Sumit, 2005, IEEE CVPR; Cohen Taco, 2016, ICML; Doersch C., 2015, IEEE ICCV; Dosovitskiy A., 2016, IEEE PAMI; Goroshin R., 2015, IEEE ICCV; Gross R., 2010, IMAGE VISION COMPUTI; Hadsell R., 2006, IEEE CVPR; Jaderberg Max, 2015, NEURIPS; Jayaraman D., 2016, IEEE ICCV; Komodakis Nikos, 2018, INT C LEARN REPR; Larsson G., 2017, IEEE CVPR; LeCun Y., 2004, IEEE CVPR; Mobahi H., 2009, ICML; Niyogi P., 1998, P IEEE; Novotny D., 2018, IEEE CVPR; Perry G., 2010, EXPT BRAIN RES; Perry G., 2006, VISION RES; Schroff F., 2015, IEEE CVPR; Simonyan K., 2014, ICLR; Soatto Stefano, 2016, ICLR; Song H. O., 2016, IEEE CVPR; Tacchetti A., 2017, PLOS COMPUTATIONAL B; Tacchetti Andrea, 2018, ANN REV VISION SCI; van der Maaten Laurens, 2008, J MACHINE LEARNING R; VIOLA P, 2001, IEEE CVPR; Wang X., 2017, IEEE ICCV; Wang X., 2015, IEEE ICCV; Weinberger K. Q., 2009, J MACHINE LEARNING R; Wood J. N., 2018, COGNITIVE SCI; Zeiler M. D. ., 2010, IEEE CVPR; Zhao Junbo, 2016, ICLR; Zhu X., 2012, IEEE CVPR; Zou W., 2012, NEURIPS	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004020
C	Tang, JQ; Golbabaee, M; Bach, F; Davies, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tang, Junqi; Golbabaee, Mohammad; Bach, Francis; Davies, Mike			Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHM; SELECTION	We propose a structure-adaptive variant of a state-of-the-art stochastic variance-reduced gradient algorithm Katyusha for regularized empirical risk minimization The proposed method is able to exploit the intrinsic low-dimensional structure of the solution, such as sparsity or low rank which is enforced by a non-smooth regularization, to achieve even faster convergence rate. This provable algorithmic improvement is done by restarting the Katyusha algorithm according to restricted strong-convexity (RSC) constants. We also propose an adaptive-restart variant which is able to estimate the RSC on the fly and adjust the restart period automatically. We demonstrate the effectiveness of our approach via numerical experiments.	[Tang, Junqi; Davies, Mike] Univ Edinburgh, Sch Engn, Edinburgh, Midlothian, Scotland; [Golbabaee, Mohammad] Univ Bath, Dept Comp Sci, Bath, Avon, England; [Bach, Francis] PSL Res Univ, INRIA ENS, Paris, France	University of Edinburgh; University of Bath; UDICE-French Research Universities; PSL Research University Paris	Tang, JQ (corresponding author), Univ Edinburgh, Sch Engn, Edinburgh, Midlothian, Scotland.	J.Tang@ed.ac.uk; M.Golbabaee@bath.ac.uk; Francis.Bach@inria.fr; Mike.Davies@ed.ac.uk	Golbabaee, Mohammad/AAM-1430-2021	Golbabaee, Mohammad/0000-0001-5822-2990	H2020-MSCA-ITN Machine Sensing Training Network (MacSeNet) [642685]; ERC [724063]; EPSRC Compressed Quantitative MRI grant [EP/M019802/1]; ERC Advanced grant [694888]; C-SENSE; Royal Society Wolfson Research Merit Award	H2020-MSCA-ITN Machine Sensing Training Network (MacSeNet); ERC(European Research Council (ERC)European Commission); EPSRC Compressed Quantitative MRI grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ERC Advanced grant(European Research Council (ERC)); C-SENSE; Royal Society Wolfson Research Merit Award(Royal Society of London)	LJT, FB, MG and MD would like to acknowledge the support from H2020-MSCA-ITN Machine Sensing Training Network (MacSeNet), project 642685; ERC grant SEQUOIA, project 724063; EPSRC Compressed Quantitative MRI grant, number EP/M019802/1; and ERC Advanced grant, project 694888, C-SENSE, respectively. MD is also supported by a Royal Society Wolfson Research Merit Award. JT would like to thank Damien Scieur and Vincent Roulet for helpful discussions during his research visit in SIERRA team.	Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; Allen-Zhu Z., 2016, ARXIV160305953; Allen-Zhu Z., 2014, ARXIV14071537; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; [Anonymous], ARXIV150702000; Arjevani Yossi, 2017, ADV NEURAL INFORM PR, P3543; Bach FR, 2008, J MACH LEARN RES, V9, P1179; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Causality workbench team, 2008, A GEN DAT; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A, 2016, ADV NEUR IN, V29; Fercoq O., 2016, ARXIV160907358; Fercoq O., 2017, ARXIV170902300; Fercoq O., 2018, ARXIV180305771; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Lichman M., 2013, UCI MACHINE LEARNING; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Murata Tomoya, 2017, ADV NEURAL INFORM PR, P608; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nesterov Y., 2007, TECHNICAL REPORT; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Pilanci M, 2016, J MACH LEARN RES, V17; Pilanci M, 2017, SIAM J OPTIMIZ, V27, P205, DOI 10.1137/15M1021106; Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722; Qu Chao, 2016, ARXIV161101957; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Roulet V., 2017, ADV NEURAL INFORM PR, V30, P1119; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Tang JQ, 2017, IEEE GLOB CONF SIG, P1305; Tang Junqi, 2017, ARXIV171203156; Tang Junqi, 2017, P MACHINE LEARNING R, V70, P3377; Tang Junqi, 2018, HAL01889990; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R., 2015, STAT LEARNING SPARSI; Vaiter S, 2015, INF INFERENCE, V4, P230, DOI 10.1093/imaiai/iav005; Vapnik V., 2013, NATURE STAT LEARNING; Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643; Wang Jialei, 2017, P MACHINE LEARNING R, V70, P3694; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332; Zhang YC, 2015, PR MACH LEARN RES, V37, P353	50	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300040
C	Tay, Y; Tuan, LA; Hui, SC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tay, Yi; Luu Anh Tuan; Hui, Siu Cheung			Recurrently Controlled Recurrent Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recurrent neural networks (RNNs) such as long short-term memory and gated recurrent units are pivotal building blocks across a broad spectrum of sequence modeling problems. This paper proposes a recurrently controlled recurrent network (RCRN) for expressive and powerful sequence encoding. More concretely, the key idea behind our approach is to learn the recurrent gating functions using recurrent networks. Our architecture is split into two components - a controller cell and a listener cell whereby the recurrent controller actively influences the compositionality of the listener cell. We conduct extensive experiments on a myriad of tasks in the NLP domain such as sentiment analysis (SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment classification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading comprehension (NarrativeQA). Across all 26 datasets, our results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture.	[Tay, Yi; Hui, Siu Cheung] Nanyang Technol Univ, Singapore, Singapore; [Luu Anh Tuan] Inst Infocomm Res, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)	Tay, Y (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	ytay017@e.ntu.edu.sg; at.luu@i2r.a-star.edu.sg; asschui@ntu.edu.sg		Hui, Siu Cheung/0000-0001-5397-4472				[Anonymous], ARXIV180200889; Bengio, 2016, ARXIV160901704; Bengio Y., 2014, ARXIV14061078; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Bradbury J., 2016, CORR; Chang Shiyu, 2017, P ADV NEUR INF PROC, P76; Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152; Choi Jihun, 2017, ARXIV170702786; Danihelka I, 2016, PR MACH LEARN RES, V48; Dieng A.B., 2016, ICLR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; ElHihi S, 1996, ADV NEUR IN, V8, P493; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Guo Hongyu, 2017, ARXIV170405907; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang ML, 2017, ACM T INFORM SYST, V35, DOI 10.1145/3052770; Johnson R, 2016, PR MACH LEARN RES, V48; Khot T, 2018, AAAI CONF ARTIF INTE, P5189; Kiela Douwe, 2018, P 2018 C EMP METH NA, P1466, DOI DOI 10.18653/V1/D18-1176; Kim Seonhoon, 2018, ARXIV180511360; Kim Y., 2014, P 2014 C EMP METH NA; Kingma D.P, P 3 INT C LEARNING R; Kocisky Tomas, 2017, ARXIV171207040; Koutnik Jan, 2014, ARXIV14023511; Kumar A, 2016, PR MACH LEARN RES, V48; Lei Tao, 2017, ARXIV170902755; Liu PF, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1, DOI 10.18653/v1/P17-1001; Longpre Shayne, 2016, ARXIV161105104; Looks M, 2017, ARXIV170202181; Maas A., 2011, P 49 ANN M ASS COMPU, P142; McCann Bryan, 2017, ARXIV170800107; Miyato Takeru, 2016, ARXIV160507725; Munkhdalai T, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P11; Munkhdalai Tsendsuren, 2016, CORR; Nie Yixin, 2017, P 2 WORKSH EV VECT S, P41; Peng Zhou, 2016, ARXIV161106639; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Radford A., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1704.01444; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Rao JF, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1913, DOI 10.1145/2983323.2983872; Rocktaschel Tim, 2015, ARXIV150906664; Rui Zhang, 2016, ARXIV161102361; Santos Cicero Dos, 2016, ARXIV160203609; Schmidhuber J., 2015, CORR; Seo Minjoon, 2016, ARXIV161101603; Shen Tao, 2018, ARXIV180400857; Shen Tao, 2017, ARXIV170904696; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; T_ackstr_om O, 2016, P 2016 C EMP METH NA; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; Tay Yi, 2017, ARXIV180100102; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Voorhees E.M., 1999, NAT LANG ENG, V99, P77, DOI DOI 10.1017/S1351324901002789; Wang M., 2007, P 2007 JOINT C EMPIR, V7; Wang WH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P189, DOI 10.18653/v1/P17-1018; Wieting John, 2015, ARXIV151108198; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xiong C., 2016, CORR; Yang Yi, 2015, P 2015 C EMP METH NA, P2013, DOI DOI 10.18653/V1/D15-1237; Yi Tay, 2018, ARXIV181002938; Yi Tay, 2018, ARXIV180309074; Yue Zhang, 2018, ARXIV180502474; Zeiler Matthew D, 2012, ARXIV12125701; Zhang Y, 2016, INT CONF ACOUST SPEE, P5755, DOI 10.1109/ICASSP.2016.7472780	65	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304072
C	Terada, Y; Obuchi, T; Isomura, T; Kabashima, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Terada, Yu; Obuchi, Tomoyuki; Isomura, Takuya; Kabashima, Yoshiyuki			Objective and efficient inference for couplings in neuronal networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PATTERNS	Inferring directional couplings from the spike data of networks is desired in various scientific fields such as neuroscience. Here, we apply a recently proposed objective procedure to the spike data obtained from the Hodgkin-Huxley type models and in vitro neuronal networks cultured in a circular structure. As a result, we succeed in reconstructing synaptic connections accurately from the evoked activity as well as the spontaneous one. To obtain the results, we invent an analytic formula approximately implementing a method of screening relevant couplings. This significantly reduces the computational cost of the screening method employed in the proposed objective procedure, making it possible to treat large-size systems as in this study.	[Terada, Yu; Isomura, Takuya] RIKEN Ctr Brain Sci, Lab Neural Computat & Adaptat, 2-1 Hirosawa, Wako, Saitama 3510198, Japan; [Terada, Yu; Obuchi, Tomoyuki; Kabashima, Yoshiyuki] Tokyo Inst Technol, Dept Math & Comp Sci, Tokyo 1528550, Japan	RIKEN; Tokyo Institute of Technology	Terada, Y (corresponding author), RIKEN Ctr Brain Sci, Lab Neural Computat & Adaptat, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.; Terada, Y (corresponding author), Tokyo Inst Technol, Dept Math & Comp Sci, Tokyo 1528550, Japan.	yu.terada@riken.jp; obuchi@c.titech.ac.jp; takuya.isomura@riken.jp; kaba@c.titech.ac.jp	Kabashima, Yoshiyuki/F-4719-2015	Kabashima, Yoshiyuki/0000-0002-2949-7108; Terada, Yu/0000-0002-5752-8258; Obuchi, Tomoyuki/0000-0003-1216-489X	MEXT KAKENHI [17H00764, 18K11463]; RIKEN Center for Brain Science	MEXT KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); RIKEN Center for Brain Science	This work was supported by MEXT KAKENHI Grant Numbers 17H00764 (YT, TO, and YK) and 18K11463 (TO), and RIKEN Center for Brain Science (YT and TI).	Aurell E, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.090201; Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228; Buzsaki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233; Capone C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118412; Churchland MM, 2007, CURR OPIN NEUROBIOL, V17, P609, DOI 10.1016/j.conb.2007.11.001; Cocco S, 2009, P NATL ACAD SCI USA, V106, P14058, DOI 10.1073/pnas.0906705106; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Dunn B, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004052; Ferrari U, 2017, PHYS REV E, V95, DOI 10.1103/PhysRevE.95.042321; Ganmor E, 2011, P NATL ACAD SCI USA, V108, P9679, DOI 10.1073/pnas.1019641108; Gao PR, 2015, CURR OPIN NEUROBIOL, V32, P148, DOI 10.1016/j.conb.2015.04.003; Hastie T., 2016, ELEMENTS STAT LEARNI; Isomura T, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/6/066023; Izhikevich E., 2007, DYNAMICAL SYSTEMS NE; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; Maass W, 2016, CURR OPIN BEHAV SCI, V11, P81, DOI 10.1016/j.cobeha.2016.06.003; Marre O, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.138101; Mezard M, 2011, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2011/07/L07001; Obuchi T, 2015, J STAT PHYS, V161, P598, DOI 10.1007/s10955-015-1341-7; Obuchi T, 2015, J PHYS CONF SER, V638, DOI 10.1088/1742-6596/638/1/012018; Ohiorhenuan IE, 2010, NATURE, V466, P617, DOI 10.1038/nature09178; Posani L., 2018, BIORXIV; Roudi Y, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.048702; Roudi Y, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051915; Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701; Sessak V, 2009, J PHYS A-MATH THEOR, V42, DOI 10.1088/1751-8113/42/5/055001; Shlens J, 2006, J NEUROSCI, V26, P8254, DOI 10.1523/JNEUROSCI.1282-06.2006; Takekawa T, 2010, EUR J NEUROSCI, V31, P263, DOI 10.1111/j.1460-9568.2009.07068.x; Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302; Tang A, 2008, J NEUROSCI, V28, P505, DOI 10.1523/JNEUROSCI.3359-07.2008; Terada Y, 2018, ARXIV180304738; Tyrcha J, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03005; Yuste R, 2015, NAT REV NEUROSCI, V16, P487, DOI 10.1038/nrn3962; Zeng HL, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.210601; Zeng HL, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.041135	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305002
C	Theocharous, G; Wen, Z; Abbasi-Yadkori, Y; Vlassis, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Theocharous, Georgios; Wen, Zheng; Abbasi-Yadkori, Yasin; Vlassis, Nikos			Scalar Posterior Sampling with Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a practical non-episodic PSRL algorithm that unlike recent state-of-the-art PSRL algorithms uses a deterministic, model-independent episode switching schedule. Our algorithm termed deterministic schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space complexity. We prove a Bayesian regret bound under mild assumptions. Our result is more generally applicable to multiple parameters and continuous state action problems. We compare our algorithm with state-of-the-art PSRL algorithms on standard discrete and continuous problems from the literature. Finally, we show how the assumptions of our algorithm satisfy a sensible parametrization for a large class of problems in sequential recommendations.	[Theocharous, Georgios; Wen, Zheng; Abbasi-Yadkori, Yasin] Adobe Res, San Jose, CA USA; [Vlassis, Nikos] Netflix, New York, NY USA	Adobe Systems Inc.; Netflix, Inc.	Theocharous, G (corresponding author), Adobe Res, San Jose, CA USA.	theochar@adobe.com; zwen@adobe.com; abbasiya@adobe.com; nvlassis@netflix.com						Abbasi-Yadkori Y., 2015, UAI, P1; Abbasi-Yadkori Yasin, 2011, COLT; Agrawal Shipra, 2017, NIPS; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V2; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Gopalan A., 2015, P 28 C LEARNING THEO, P861; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Osband I., 2014, ADV NEURAL INFORM PR, V27, P1466; Osband Ian, 2016, ARXIV160802731; Russo D., 2013, ADV NEURAL INFORM PR, P3003; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strens, 2000, P 17 INT C MACH LEAR, P943; Sutton R. S., 1998, INTRO REINFORCEMENT, V135; Theocharous Georgios, 2017, P 22 INT C INT US IN, P49, DOI DOI 10.1145/3030024.3040983; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Yi Ouyang, 2017, NIPS; Yi Ouyang, 2017, ARXIV170904047	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002025
C	Thodoroff, P; Durand, A; Pineau, J; Precup, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Thodoroff, Pierre; Durand, Audrey; Pineau, Joelle; Precup, Doina			Temporal Regularization in Markov Decision Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				STATE	Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization. Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories. This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games.	[Thodoroff, Pierre; Durand, Audrey; Pineau, Joelle; Precup, Doina] McGill Univ, Montreal, PQ, Canada; [Pineau, Joelle] Facebook AI Res, Montreal, PQ, Canada	McGill University; Facebook Inc	Thodoroff, P (corresponding author), McGill Univ, Montreal, PQ, Canada.	pierre.thodoroff@mail.mcgill.ca; audrey.durand@mcgill.ca; jpineau@cs.mcgill.ca; dprecup@cs.mcgill.ca		Salguero Tejada, Carlos/0000-0003-0930-9277	NSERC; Compute Canada; Facebook	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Compute Canada; Facebook(Facebook Inc)	The authors wish to thank Pierre-Luc Bacon, Harsh Satija and Joshua Romoff for helpful discussions. Financial support was provided by NSERC and Facebook. This research was enabled by support provided by Compute Canada. We thank the reviewers for insightful comments and suggestions.	Baird L., 1995, P 12 INT C MACH LEAR, P30, DOI DOI 10.1016/B978-1-55860-377-6.50013-X; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Box G. E. P., 1970, Time series analysis, forecasting and control; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Chen JM, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P3056, DOI 10.1109/ICMLC.2008.4620932; Chung  K.-M., 2012, ARXIV12010559; Dhingra B, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P484, DOI 10.18653/v1/P17-1045; Farahmand A. - m., 2011, THESIS; Farahmand AM, 2009, P AMER CONTR CONF, P725, DOI 10.1109/ACC.2009.5160611; GARDNER ES, 1985, J FORECASTING, V4, P1, DOI 10.1002/for.3980040103; Gardner ES, 2006, INT J FORECASTING, V22, P637, DOI 10.1016/j.ijforecast.2006.03.005; Harrigan  C., 2016, DEEP REINFORCEMENT L; Henderson P, 2018, AAAI CONF ARTIF INTE, P3207; Hesse C., 2017, OPENAI BASELINES; Kemeny J.G., 1976, MARKOV CHAINS, V6; Koedinger  K., 2018, AAAI MAGAZINE; Laroche V. S., 2018, ICLR WORKSH; Levin D. A., 2008, MARKOV CHAINS MIXING, V107; Liu B, 2012, P C NEUR INF PROC SY, P25; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pazis  J., 2011, AAAI; Petrik M., 2010, ARXIV10051860; Prasad  N., 2017, UAI; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Shortreed  S., 2011, MACHINE LEARNING; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Singh SP., 1994, MACH LEARN PROC, V1994, P284, DOI [10.1016/c2009-0-27542-8, DOI 10.1016/C2009-0-27542-8]; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tsitsiklis JN, 2002, MACH LEARN, V49, P179, DOI 10.1023/A:1017980312899; Xu  Z., 2017, ADV NEURAL INFORM PR, P2117	36	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301074
C	Thomas, AT; Gu, A; Dao, T; Rudra, A; Re, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Thomas, Anna T.; Gu, Albert; Dao, Tri; Rudra, Atri; Re, Christopher			Learning Compressed Transforms with Low Displacement Rank	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks also outperform general unstructured layers while using more than 20x fewer parameters.	[Thomas, Anna T.; Gu, Albert; Dao, Tri; Re, Christopher] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Rudra, Atri] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY USA	Stanford University; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Thomas, AT (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	thomasat@stanford.edu; albertgu@stanford.edu; trid@stanford.edu; atri@buffalo.edu; chrismre@cs.stanford.edu							0	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003059
C	Thune, TS; Seldin, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Thune, Tobias Sommer; Seldin, Yevgeny			Adaptation to Easy Data in Prediction with Limited Advice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We derive an online learning algorithm with improved regret guarantees for "easy" loss sequences. We consider two types of "easiness": (a) stochastic loss sequences and (b) adversarial loss sequences with small effective range of the losses. While a number of algorithms have been proposed for exploiting small effective range in the full information setting, Gerchinovitz and Lattimore [2016] have shown the impossibility of regret scaling with the effective range of the losses in the bandit setting. We show that just one additional observation per round is sufficient to circumvent the impossibility result. The proposed Second Order Difference Adjustments (SODA) algorithm requires no prior knowledge of the effective range of the losses, epsilon, and achieves an O (epsilon root KT ln K) + (O) over tilde (epsilon K (4)root T) expected regret guarantee, where T is the time horizon and K is the number of actions. The scaling with the effective loss range is achieved under significantly weaker assumptions than those made by Cesa-Bianchi and Shamir [2018] in an earlier attempt to circumvent the impossibility result. We also provide a regret lower bound of Omega(epsilon root TK), which almost matches the upper bound. In addition, we show that in the stochastic setting SODA achieves an O(Sigma(a:Delta a>0) K epsilon(2)/Delta(a)) pseudo-regret bound that holds simultaneously with the adversarial regret guarantee. In other words, SODA is safe against an unrestricted oblivious adversary and provides improved regret guarantees for at least two different types of "easiness" simultaneously.	[Thune, Tobias Sommer; Seldin, Yevgeny] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark	University of Copenhagen	Thune, TS (corresponding author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	tobias.thune@di.ku.dk; seldin@di.ku.dk	Seldin, Yevgeny/G-8955-2015	Seldin, Yevgeny/0000-0003-3152-4635				Alon N, 2017, SIAM J COMPUT, V46, P1785, DOI 10.1137/140989455; Auer P., 2016, P INT C COMP LEARN T; Auer Peter, 2002, SIAM J COMPUTING, V32; Auer  Peter, 2002, MACH LEARN, V47; Bubeck S., 2012, FDN TRENDS MACHINE L, P5; Bubeck Sebastien, 2012, P INT C COMP LEARN T; Cesa-Bianchi  Nicolo, 2007, MACH LEARN, V66; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cesa-Bianchi  Nicolo, 2018, P INT C ALG LEARN TH; Gaillard  Pierre, 2014, P INT C COMP LEARN T; Gerchinovitz  Sebastien, 2016, ADV NEURAL INFORM PR; Kale Satyen, 2014, P COLT; Koolen Wouter M, 2015, C LEARNING THEORY, P1155; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Littlestone  Nick, 1994, INFORM COMPUTATION, V108; Luo  Haipeng, 2015, P INT C COMP LEARN T; Robbins H., 1952, B AM MATH SOC; Seldin Y., 2014, P INT C MACH LEARN I; Seldin Y., 2017, P INT C COMP LEARN T; Seldin Yevgeny, 2013, P COLT; Vladimir Vovk, 1990, P INT C COMP LEARN T; Wei C., 2018, P INT C COMP LEARN T; Wintenberger  Olivier, 2017, MACH LEARN, V106; Zimmert  Julian, 2018, TECHNICAL REPORT	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302089
C	Nguyen, T; Kpotufe, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tin Nguyen; Kpotufe, Samory			PAC-Bayes Tree: weighted subtrees with guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a weighted-majority classification approach over subtrees of a fixed tree, which provably achieves excess-risk of the same order as the best tree-pruning. Furthermore, the computational efficiency of pruning is maintained at both training and testing time despite having to aggregate over an exponential number of subtrees. We believe this is the first subtree aggregation approach with such guarantees. The guarantees are obtained via a simple combination of insights from PAC-Bayes theory, which we believe should be of independent interest, as it generically implies consistency for weighted-voting classifiers w.r.t. Bayes - while, in contrast, usual PAC-bayes approaches only establish consistency of Gibbs classifiers.	[Tin Nguyen] MIT, EECS, Cambridge, MA 02139 USA; [Kpotufe, Samory] Princeton Univ, ORFE, Princeton, NJ 08544 USA	Massachusetts Institute of Technology (MIT); Princeton University	Nguyen, T (corresponding author), MIT, EECS, Cambridge, MA 02139 USA.	tdn@mit.edu; samory@princeton.edu						Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; BUNTINE W, 1992, MACH LEARN, V8, P75, DOI 10.1023/A:1022686419106; Germain P, 2015, J MACH LEARN RES, V16, P787; Gyorfi L., 2002, DISTRIBUTION FREE TH; Hastie T, 1990, SHRINKING TREES; Helmbold DP, 1997, MACH LEARN, V27, P51, DOI 10.1023/A:1007396710653; Igel C., 2017, MACHINE LEARNING RES, P466; Kpotufe S, 2012, J COMPUT SYST SCI, V78, P1496, DOI 10.1016/j.jcss.2012.01.002; Lacarelle A, 2007, SPRINGER PROC PHYS, V117, P769; Langford J., 2003, ADV NEURAL INFORM PR, P439; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Scott C., 2004, THESIS; Szekely LA, 2005, ADV APPL MATH, V34, P138, DOI 10.1016/j.aam.2004.07.002; Tech G., 2012, FDN SOFTWARE TECHNOL, P48; Verma N., 2009, P MONTR QUE 25 C UNC, P565	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004008
C	Tirinzoni, A; Sanchez, RR; Restelli, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tirinzoni, Andrea; Sanchez, Rafael Rodriguez; Restelli, Marcello			Transfer of Value Functions via Variational Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of transferring value functions in reinforcement learning. We propose an approach that uses the given source tasks to learn a prior distribution over optimal value functions and provide to an efficient variational approximation of the corresponding posterior in a new target task. We show our approach to be general, in the sense that it can be combined with complex parametric function approximators and distribution models, while providing two practical algorithms based on Gaussians and Gaussian mixtures. We theoretically analyze them by deriving a finite-sample analysis and provide a comprehensive empirical evaluation in four different domains.	[Tirinzoni, Andrea; Sanchez, Rafael Rodriguez; Restelli, Marcello] Politecn Milan, Milan, Italy	Polytechnic University of Milan	Tirinzoni, A (corresponding author), Politecn Milan, Milan, Italy.	andrea.tirinzoni@polimi.it; rafaelalberto.rodriguez@polimi.it; marcello.restelli@polimi.it		Restelli, Marcello/0000-0002-6322-1076				Alquier P, 2016, J MACH LEARN RES, V17; Amit Ron, 2018, P 35 INT C MACH LEAR; Asadi K., 2017, ICML, P243; Azizzadenesheli K, 2018, ARXIV180204412; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Catoni Olivier, 2007, ARXIV07120248; Doshi-Velez Finale, 2016, IJCAI (U S), V2016, P1432; Fernandez F., 2006, P 5 INT JOINT C AUT, P720, DOI DOI 10.1145/1160633.1160762; Finn C, 2017, ARXIV170303400; Grant E., 2018, ARXIV180108930; Hershey John R, 2007, AC SPEECH SIGN PROC; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Killian Taylor, 2017, Adv Neural Inf Process Syst, V30, P6250; Kober J., 2009, PROC NEURAL INF PROC, P849; Konidaris G., 2006, P 23 INT C MACH LEAR, P489, DOI DOI 10.1145/1143844.1143906; Konidaris George, 2007, BUILDING PORTABLE OP; Lazaric A., 2010, PROC INT C MACH LEAR, P599; Lazaric Alessandro, 2012, REINFORCEMENT LEARNI; Lazaric Alessandro, 2008, P 25 INT C MACH LEAR; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap TP, 2016, 4 INT C LEARN REPR; Maillard OA, 2010, JMLR WORKSH CONF PRO, V13, P299; Mnih V, 2015, NATURE, V518, P529; Osband I., 2014, GEN EXPLORATION VIA; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rezende D.J., 2014, PROC INT CONFER ENCE; Schaul T, 2016, C TRACK P, P1; Scott DW, 2015, WILEY SER PROBAB ST, P1, DOI 10.1002/9781118575574; Seldin Y., 2012, IEEE T INFORM THEORY; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R. S., 1998, REINFORCEMENT LEARNI, V1; Taylor ME, 2008, LECT NOTES ARTIF INT, V5212, P488, DOI 10.1007/978-3-540-87481-2_32; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Tirinzoni Andrea, 2018, P 35 INT C MACH LEAR, V80, P4936; Van Hasselt H., 2016, DEEP REINFORCEMENT L; Wilson A., 2007, PROC INT C MACH LEAR, P1015, DOI DOI 10.1145/1273496	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000066
C	Tong, G; Wu, WL; Du, DZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tong, Guangmo (Amo); Wu, Weili; Du, Ding-Zhu			On Misinformation Containment in Online Social Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present an analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. This paper makes four contributions. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of Omega (2(log1-epsilon n4 )) in polynomial time unless NP subset of DTIME(n(polylogn) ) . Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.	[Tong, Guangmo (Amo)] Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA; [Wu, Weili; Du, Ding-Zhu] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA	University of Delaware; University of Texas System; University of Texas Dallas	Tong, G (corresponding author), Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA.	amotong@udel.edu; weiliwu@utdallas.edu; dzdu@utdallas.edu		Wu, Weili lily/0000-0001-8747-6340	University of Delaware; NSF [1747818]	University of Delaware; NSF(National Science Foundation (NSF))	This work is supported in part by a start-up grant from the University of Delaware and the NSF under grant #1747818.	Allcott H, 2017, J ECON PERSPECT, V31, P211, DOI 10.1257/jep.31.2.211; [Anonymous], 2017, IEEE T NETWORK SCI E; [Anonymous], 2010, P WWW; Borgs C., 2014, P SODA; Budak Ceren, 2011, P WWW; Chen W., 2010, P SIGKDD; De Domenico M, 2013, SCI REP-UK, V3, DOI 10.1038/srep02980; Du Nan, 2013, P NIPS; Fan Lidan, 2013, P ICDCS; Farajtabar Mehrdad, 2017, P ICML; Goyal Amit, 2010, P WSDM; He X., 2012, P SDM; He ZB, 2017, IEEE T VEH TECHNOL, V66, P2789, DOI 10.1109/TVT.2016.2585591; Kempe David, 2003, P SIGKDD; Kumar S., 2018, FALSE INFORM WEB SOC, V1, P1; Leskovec J., 2014, SNAP DATASETS 1071 S; Li Qiang, 2017, P NIPS; Lu W, 2015, PROC VLDB ENDOW, V9, P60; Lynn Christopher, 2016, P NIPS; Miettinen P, 2008, INFORM PROCESS LETT, V108, P219, DOI 10.1016/j.ipl.2008.05.007; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Nguyen H. T., 2016, P SIGMOD; Nguyen Nam P., 2012, P WEBSCI; Smith K, 2018, MARKETING 115 AMAZIN; Tang Y., 2015, P SIGMOD; Tong Guangmo, 2019, P INFOCOM; Wang B, 2017, IEEE T KNOWL DATA EN, V29, P2168, DOI 10.1109/TKDE.2017.2728064; Wu W., 2018, IEEE T COMPUTATIONAL; Zeng F., 2009, J INTERACT ADVERT, V10, P1, DOI [10.1080/15252019.2009.10722159, DOI 10.1080/15252019.2009.10722159]; Zhang H., 2015, P CSONET	31	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300032
C	Tosh, C; Dasgupta, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tosh, Christopher; Dasgupta, Sanjoy			Interactive Structure Learning with Structural Query-by-Committee	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GEOMETRY	In this work, we introduce interactive structure learning, a framework that unifies many different interactive learning tasks. We present a generalization of the query-by-committee active learning algorithm for this setting, and we study its consistency and rate of convergence, both theoretically and empirically, with and without noise.	[Tosh, Christopher] Columbia Univ, New York, NY 10027 USA; [Dasgupta, Sanjoy] Univ Calif San Diego, San Diego, CA USA	Columbia University; University of California System; University of California San Diego	Tosh, C (corresponding author), Columbia Univ, New York, NY 10027 USA.	c.tosh@columbia.edu; dasgupta@cs.ucsd.edu			NSF [CCF-1813160]	NSF(National Science Foundation (NSF))	The authors are grateful to the reviewers for their feedback and to the NSF for support under grant CCF-1813160. Part of this work was done at the Simons Institute for Theoretical Computer Science, Berkeley, during the "Foundations of Machine Learning" program. CT also thanks Stefanos Poulis and Sharad Vikram for helpful discussions and feedback.	[Anonymous], 2017, NEURIPS; Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216; Awasthi P., 2014, P 31 INT C MACH LEAR; Awasthi P., 2015, PROC 28 ANN C LEARN, P167; Awasthi P., 2010, ADV NEURAL INFORM PR; Azuma K., 1967, THOKU MATH J, V19, P357, DOI DOI 10.2748/TMJ/1178243286; Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27; Beygelzimer A, 2009, P 26 INT C MACH LEAR; Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189; Cesa-Bianchi N, 2009, LECT NOTES ARTIF INT, V5809, P110, DOI 10.1007/978-3-642-04414-4_13; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Dasarathy G., 2015, P C LEARN THEOR, P503; DASGUPTA S, 2007, ADV NEURAL INFORM PR, P49566; Dasgupta S, 2008, P 25 INT C MACH LEAR; Dasgupta S., 2004, ADV NEURAL INFORM PR; Dasgupta S., 2017, ARXIV E PRINTS; Dasgupta S., 2005, ADV NEURAL INFORM PR; Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534; Gilad-Bachrach R., 2005, ADV NEURAL INF PROCE, V18; Gonen A, 2013, J MACH LEARN RES, V14, P2583; Guillory A, 2009, LECT NOTES ARTIF INT, V5809, P141, DOI 10.1007/978-3-642-04414-4_15; Hanneke S., 2007, P 25 INT C MACH LEAR; Higham N. J., 2002, ACCURACY STABILITY N; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Huang T.-K., 2015, ADV NEURAL INFORM PR; Kane DM, 2017, ANN IEEE SYMP FOUND, P355, DOI 10.1109/FOCS.2017.40; Kpotufe S., 2015, P 28 ANN C LEARN THE; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lichman M., 2013, UCI MACHINE LEARNING; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298; Poulis S, 2017, PR MACH LEARN RES, V54, P1104; Settles B., 2012, ACTIVE LEARNING; Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417; Tosh C., 2014, 31 INT C MACH LEARN; Tosh Christopher, 2017, P 34 INT C MACH LEAR, P3444; Vikram S., 2016, P 33 INT C MACH LEAR; Wagstaff K., 2000, P 17 INT C MACH LEAR; Zhu X., 2003, ICML WORKSH CONT LAB	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301014
C	Ullah, E; Mianjy, P; Marinov, TV; Arora, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ullah, Enayat; Mianjy, Poorya; Marinov, Teodor V.; Arora, Raman			Streaming Kernel PCA with (O)over-tilde (root n) Random Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GRAM MATRIX	We study the statistical and computational aspects of kernel principal component analysis using random Fourier features and show that under mild assumptions, O(root n log (n)) features suffice to achieve O(1/epsilon(2)) sample complexity. Furthermore, we give a memory efficient streaming algorithm based on classical Oja's algorithm that achieves this rate.	[Ullah, Enayat; Mianjy, Poorya; Marinov, Teodor V.; Arora, Raman] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA	Johns Hopkins University	Ullah, E (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA.	enayat@jhu.edu; mianjy@jhu.edu; tmarino2@jhu.edu; arora@cs.jhu.edu			NSF BIGDATA [IIS-1546482]	NSF BIGDATA	This research was supported in part by NSF BIGDATA grant IIS-1546482.	ALLEN- ZHU Z., 2016, ARXIV160707837; Allen-Zhu Z., 2016, ADV NEURAL INFORM PR, P974; [Anonymous], ARXIV14020119; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Arora R., 2013, ADV NEURAL INFORM PR; Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; Bartlett P. L., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P44; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Ge R., 2017, ARXIV170400708; Ghashami M, 2016, JMLR WORKSH CONF PRO, V51, P1365; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Massart P., 2000, ANN FAC SCI TOULOUSE, V9, P245, DOI DOI 10.5802/afst.961; Mianjy Poorya, 2018, INT C MACH LEARN, P3528; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Reed M, 1972, GOOGL SCHOL, P151; Rudi Alessandro, 2017, ADV NEURAL INFORM PR, P3218; Rudin W., 2017, FOURIER ANAL GROUPS; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Sejdinovic Dino, 2012, WHAT IS AN RKHS; Shawe-Taylor J, 2005, IEEE T INFORM THEORY, V51, P2510, DOI 10.1109/TIT.2005.850052; Smola A.J., 1998, LEARNING KERNELS, V4; Sriperumbudur Bharath, 2018, ARXIV170606296V2; Sriperumbudur Bharath, 2017, ARXIV170606296V1; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Williams CKI, 2001, ADV NEUR IN, V13, P682; Xie Bo, 2015, ABS150403655 CORR; Yang Yun, 2015, ARXIV150106195; ZWALD L, 2006, ADV NEURAL INFORM PR, V18, P1649	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001083
C	Vazquez-Chanlatte, M; Jha, S; Tiwari, A; Ho, MK; Seshia, SA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Vazquez-Chanlatte, Marcell; Jha, Susmit; Tiwari, Ashish; Ho, Mark K.; Seshia, Sanjit A.			Learning Task Specifications from Demonstrations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODEL	Real-world applications often naturally decompose into several sub-tasks. In many settings (e.g., robotics) demonstrations provide a natural way to specify the sub-tasks. However, most methods for learning from demonstrations either do not provide guarantees that the artifacts learned for the sub-tasks can be safely recombined or limit the types of composition available. Motivated by this deficit, we consider the problem of inferring Boolean non-Markovian rewards (also known as logical trace properties or specifications) from demonstrations provided by an agent operating in an uncertain, stochastic environment. Crucially, specifications admit well-defined composition rules that are typically easy to interpret. In this paper, we formulate the specification inference task as a maximum a posteriori (MAP) probability inference problem, apply the principle of maximum entropy to derive an analytic demonstration likelihood model and give an efficient approach to search for the most likely specification in a large candidate pool of specifications. In our experiments, we demonstrate how learning specifications can help avoid common problems that often arise due to ad-hoc reward composition.	[Vazquez-Chanlatte, Marcell; Ho, Mark K.; Seshia, Sanjit A.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Jha, Susmit; Tiwari, Ashish] SRI Int, 333 Ravenswood Ave, Menlo Pk, CA 94025 USA	University of California System; University of California Berkeley; SRI International	Vazquez-Chanlatte, M (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	marcell.vc@eecs.berkeley.edu; susmit.jha@sri.com; tiwari@sri.com; mark_ho@eecs.berkeley.edu; sseshia@eecs.berkeley.edu	Ho, Mark/AGI-5014-2022; Jha, Susmit/AAG-9841-2019; Ho, Mark/V-8928-2019	Ho, Mark/0000-0002-1454-4768; Jha, Susmit/0000-0001-5983-9095; Ho, Mark/0000-0002-1454-4768	US National Science Foundation (NSF) [CNS-1750009, CNS-1740079, CNS-1545126]; DARPA BRASS program [FA8750-16-C0043]; DARPA Assured Autonomy program; Toyota under the iCyPhy center; US ARL [W911NF-17-2-0196]	US National Science Foundation (NSF)(National Science Foundation (NSF)); DARPA BRASS program; DARPA Assured Autonomy program; Toyota under the iCyPhy center; US ARL	We would like to thank the anonymous referees as well as Daniel Fremont, Markus Rabe, Ben Caulfield, Marissa Ramirez Zweiger, Shromona Ghosh, Gil Lederman, Tommaso Dreossi, Anca Dragan, and Natarajan Shankar for their useful suggestions and feedback. The work of the authors on this paper was funded in part by the US National Science Foundation (NSF) under award numbers CNS-1750009, CNS-1740079, CNS-1545126 (VeHICaL), the DARPA BRASS program under agreement number FA8750-16-C0043, the DARPA Assured Autonomy program, by Toyota under the iCyPhy center and the US ARL Cooperative Agreement W911NF-17-2-0196.	Amodei D., 2016, CONCRETE PROBLEMS AI; Bacchus F, 2003, ANN IEEE SYMP FOUND, P340, DOI 10.1109/SFCS.2003.1238208; BRYANT RE, 1992, COMPUT SURV, V24, P293, DOI 10.1145/136035.136043; Chakraborty S., 2016, P IJCAI; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Christofides N., 1975, COMPUTER SCI APPL MA; Farwer B, 2002, LECT NOTES COMPUT SC, V2500, P3; Finn C, 2016, PR MACH LEARN RES, V48; Ghosh S, 2018, I C DEPENDABLE SYST, P194, DOI 10.1109/DSN-W.2018.00064; Haarnoja T, 2018, IEEE INT CONF ROBOT, P6244; Ho M. K., 2015, P 37 ANN C COGN SCI, P920; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Jha S, 2018, J AUTOM REASONING, V60, P43, DOI 10.1007/s10817-017-9413-9; Jha S, 2017, LECT NOTES COMPUT SC, V10548, P208, DOI 10.1007/978-3-319-67531-2_13; Jha S, 2016, LECT NOTES COMPUT SC, V9690, P117, DOI 10.1007/978-3-319-40648-0_10; Joelle Pineau, 2003, IJCAI, P1025; Kasenberg D., 2017, ARXIV171010532; Kress-Gazit H, 2009, IEEE T ROBOT, V25, P1370, DOI 10.1109/TRO.2009.2030225; Levine S., 2011, C NEURAL INFORM PROC, V24, P19; Levine S., 2018, CORR; Li W., 2014, THESIS; Littman Michael L., 2017, ABS170404341 CORR; METROPOLIS N, 1949, J AM STAT ASSOC, V44, P335, DOI 10.2307/2280232; Pnueli A., 1977, 18th Annual Symposium on Foundations of Computer Science, P46, DOI 10.1109/SFCS.1977.32; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Raman V., 2015, P 18 INT C HYBR SYST, P239, DOI DOI 10.1145/2728606.2728628; Saha I, 2014, IEEE INT C INT ROBOT, P1525, DOI 10.1109/IROS.2014.6942758; Seshia S.A., 2016, ARXIV E PRINTS; Todorov E., 2007, ADV NEURAL INFORM PR, V19, DOI 10.7551/mitpress/7503.003.0176; Todorov E, 2008, IEEE DECIS CONTR P, P4286, DOI 10.1109/CDC.2008.4739438; Vardi, 1996, LECT NOTES COMPUTER, P238, DOI [10.1007/3-540-60915-66, DOI 10.1007/3-540-60915-66, DOI 10.1007/3-540-60915-6_6]; Vazquez-Chanlatte M., 2018, S CYB PHYS HUM SYST; Vazquez-Chanlatte M, 2017, LECT NOTES COMPUT SC, V10426, P305, DOI 10.1007/978-3-319-63387-9_15; Ziebart B. D., 2008, AAAI, V8, P1433	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305039
C	Vitale, F; Parotsidis, N; Gentile, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Vitale, Fabio; Parotsidis, Nikos; Gentile, Claudio			Online Reciprocal Recommendation with Theoretical Performance Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A reciprocal recommendation problem is one where the goal of learning is not just to predict a user's preference towards a passive item (e.g., a book), but to recommend the targeted user on one side another user from the other side such that a mutual interest between the two exists. The problem thus is sharply different from the more traditional items-to-users recommendation, since a good match requires meeting the preferences at both sides. We initiate a rigorous theoretical investigation of the reciprocal recommendation task in a specific framework of sequential learning. We point out general limitations, formulate reasonable assumptions enabling effective learning and, under these assumptions, we design and analyze a computationally efficient algorithm that uncovers mutual likes at a pace comparable to that achieved by a clairvoyant algorithm knowing all user preferences in advance. Finally, we validate our algorithm against synthetic and real-world datasets, showing improved empirical performance over simple baselines.	[Vitale, Fabio] Sapienza Univ Rome Italy, Dept Comp Sci, Rome, Italy; [Vitale, Fabio] Univ Lille France, Lille, France; [Vitale, Fabio] INRIA Lille Nord Europe, Lille, France; [Parotsidis, Nikos] Univ Roma Tor Vergata, Rome, Italy; [Gentile, Claudio] INRIA Lille, Lille, France; [Gentile, Claudio] Google New York, New York, NY USA	Sapienza University Rome; University of Rome Tor Vergata; Google Incorporated	Vitale, F (corresponding author), Sapienza Univ Rome Italy, Dept Comp Sci, Rome, Italy.; Vitale, F (corresponding author), Univ Lille France, Lille, France.; Vitale, F (corresponding author), INRIA Lille Nord Europe, Lille, France.	fabio.vitale@inria.fr; nikos.parotsidis@uniroma2.it; cla.gentile@gmail.com			ERC [DMAP 680153]; Google Focused Award [ALL4AI]; Dipartimenti di Eccellenza	ERC(European Research Council (ERC)European Commission); Google Focused Award(Google Incorporated); Dipartimenti di Eccellenza	We would like to thank the anonymous reviewers for their valuable comments and suggestions that helped improving the presentation of this paper. Special thanks to Flavio Chierichetti and Marc Tommasi for helpful discussions in early stages of our investigation. Fabio Vitale acknowledges support from the ERC Starting Grant "DMAP 680153", the Google Focused Award "ALL4AI", and grant "Dipartimenti di Eccellenza 2018-2022", awarded to the Department of Computer Science of Sapienza University.	Akehurst Joshua, 2012, New Frontiers in Applied Data Mining. PAKDD 2011 International Workshops. Revised Selected Papers, P15, DOI 10.1007/978-3-642-28320-8_2; Alanazi Ammar, 2016, 2 INT WORK MACH LEEA; Brozovsky Lukas, 2007, P ZNAL 2007 C OSTR V; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Christiano P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P468, DOI 10.1145/2591796.2591880; Diaz F, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P66; Gentile C., 2013, P 23 C LEARN THEOR 2; Hazan E., 2012, P 25 ANN C LEARN THE; Herbster M., 2016, P ADV NEURAL INFORM, P3954; Hong WX, 2013, J COMPUT, V8, P1960, DOI 10.4304/jcp.8.8.1960-1967; Kleinerman A., 2018, P 12 ACM C REC SYST; Koltchinskii V., 2016, ARXIV10116256V4; Kunegis J., 2012, 4 ACM RECSYS WORKSH; Li L., 2012, CIKM, P35; Maheshwary S, 2018, COMPANION PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2018 (WWW 2018), P87, DOI 10.1145/3184558.3186942; Pilaszy Istvan, 2009, P 3 ACM C REC SYST R; Pizzato L, 2013, USER MODEL USER-ADAP, V23, P447, DOI 10.1007/s11257-012-9125-0; Rej T., 2011, IJCAI, P2199, DOI [10.5591/978, DOI 10.5591/978-1-57735-516-8/IJCAI11-367]; Shalev-Shwartz S., 2004, ICML; Warmuth M. K., 2007, P 24 INT C MACH LEAR, P999; Xia P, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P234, DOI 10.1145/2808797.2809282	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002077
C	Wang, LW; Hu, LJ; Gu, JY; Wu, Y; Hu, ZQ; He, K; Hopcroft, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Liwei; Hu, Lunjia; Gu, Jiayuan; Wu, Yue; Hu, Zhiqiang; He, Kun; Hopcroft, John			Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.	[Wang, Liwei; Gu, Jiayuan; Wu, Yue; Hu, Zhiqiang] Peking Univ, Sch EECS, Key Lab Machine Percept, Beijing, Peoples R China; [Wang, Liwei] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing, Peoples R China; [Hu, Lunjia] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [He, Kun] Huazhong Univ Sci & Technol, Wuhan, Hubei, Peoples R China; [Hopcroft, John] Cornell Univ, Ithaca, NY 14853 USA	Peking University; Peking University; Stanford University; Huazhong University of Science & Technology; Cornell University	Wang, LW (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept, Beijing, Peoples R China.; Wang, LW (corresponding author), Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing, Peoples R China.	wanglw@cis.pku.edu.cn; lunjia@stanford.edu; gujiayuan@pku.edu.cn; frankwu@pku.edu.cn; huzq@pku.edu.cn; brooklet60@hust.edu.cn; jeh17@cornell.edu	Gu, Jiayuan/AAH-2056-2020; He, Kun/AAQ-1555-2020	Gu, Jiayuan/0000-0002-3207-7921; He, Kun/0000-0001-7627-4604	National Basic Research Program of China (973 Program) [2015CB352502]; NSFC [61573026]; BJNSF [L172037]; Microsoft Research Asia	National Basic Research Program of China (973 Program)(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC)); BJNSF; Microsoft Research Asia(Microsoft)	This work is supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and BJNSF (L172037) and a grant from Microsoft Research Asia.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky A., CIFAR10; Li K., 2009, CVPR09; Li Y., 2016, INT C LEARN REPR ICL; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076	8	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004017
C	Wang, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Tong			Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present the Multi-value Rule Set (MRS) for interpretable classification with feature efficient presentations. Compared to rule sets built from single-value rules, MRS adopts a more generalized form of association rules that allows multiple values in a condition. Rules of this form are more concise than classical single-value rules in capturing and describing patterns in data. Our formulation also pursues a higher efficiency of feature utilization, which reduces possible cost in data collection and storage. We propose a Bayesian framework for formulating an MRS model and develop an efficient inference method for learning a maximum a posteriori, incorporating theoretically grounded bounds to iteratively reduce the search space and improve the search efficiency. Experiments on synthetic and real-world data demonstrate that MRS models have significantly smaller complexity and fewer features than baseline models while being competitive in predictive accuracy. Human evaluations show that MRS is easier to understand and use compared to other rule-based models.	[Wang, Tong] Univ Iowa, Tippie Sch Business, Iowa City, IA 52242 USA	University of Iowa	Wang, T (corresponding author), Univ Iowa, Tippie Sch Business, Iowa City, IA 52242 USA.	tong-wang@uiowa.edu						[Anonymous], [No title captured]; Berthold MR, 2003, INT J APPROX REASON, V32, P67, DOI 10.1016/S0888-613X(02)00077-4; Bombardier V, 2007, COMPUT IND, V58, P355, DOI 10.1016/j.compind.2006.07.006; Chi Z., 1996, FUZZY ALGORITHMS APP, V10, P240; CHISNALL PM, 1993, J MARKET RES SOC, V35, P392; Cohen W. W., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P115; DeJong K. A., 1990, TECHNICAL REPORT; Deng HT, 2011, LECT NOTES COMPUT SC, V6792, P293, DOI 10.1007/978-3-642-21738-8_38; Hamrouni T, 2009, DATA KNOWL ENG, V68, P1091, DOI 10.1016/j.datak.2009.05.001; Hamrouni T, 2010, ANN MATH ARTIF INTEL, V59, P201, DOI 10.1007/s10472-010-9192-z; Hilali I., 2013, INT WORKSH INF SEARC, P84; Hornik K, 2009, COMPUTATION STAT, V24, P225, DOI 10.1007/s00180-008-0119-7; Ishibuchi H, 2001, IEEE T FUZZY SYST, V9, P506, DOI 10.1109/91.940964; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Kohavi R., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P202; Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848; Li WM, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P369, DOI 10.1109/ICDM.2001.989541; Ma B. L. W. H. Y., 1998, KDD; Malioutov D., 2013, P INT C MACH LEARN, P765; Mampaey M, 2015, KNOWL INF SYST, V42, P465, DOI 10.1007/s10115-013-0714-y; MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158; Morris J., 2016, PMLR, V56, P164; Nanavati A. A., 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P482, DOI 10.1145/502585.502666; OSOFSKY JD, 1995, AM PSYCHOL, V50, P782, DOI 10.1037/0003-066X.50.9.782; Rijnbeek P. R., 2010, MACHINE LEARNING, V80; Steinbach M, 2007, KNOWL INF SYST, V12, P279, DOI 10.1007/s10115-006-0041-7; Strack B, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/781670; TongWang Cynthia Rudin, 2016, ICDM; Wang F, 2015, JMLR WORKSH CONF PRO, V38, P1013; Wang T., 2017, J MACHINE LEARNING R; Willis GB, 2004, COGNITIVE INTERVIEWI; Yang H., 2017, ICML; Yeh IC, 2009, EXPERT SYST APPL, V36, P2473, DOI 10.1016/j.eswa.2007.12.020; Yin XX, 2003, SIAM PROC S, P331	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005042
C	Wang, Z; Kim, B; Kaelbling, LP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Zi; Kim, Beomjoon; Kaelbling, Leslie Pack			Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Bayesian optimization usually assumes that a Bayesian prior is given. However, the strong theoretical guarantees in Bayesian optimization are often regrettably compromised in practice because of unknown parameters in the prior. In this paper, we adopt a variant of empirical Bayes and show that, by estimating the Gaussian process prior from offline data sampled from the same prior and constructing unbiased estimators of the posterior, variants of both GP-UCB and probability of improvement achieve a near-zero regret bound, which decreases to a constant proportional to the observational noise as the number of offline data and the number of online evaluations increase. Empirically, we have verified our approach on challenging simulated robotic problems featuring task and motion planning.	[Wang, Zi; Kim, Beomjoon; Kaelbling, Leslie Pack] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Wang, Z (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	ziw@csail.mit.edu; beomjoon@mit.edu; lpk@csail.mit.edu			NSF [1420316, 1523767, 1723381]; AFOSR [FA9550-17-1-0165]; Honda Research; Draper Laboratory	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Honda Research; Draper Laboratory	We would like to thank Stefanie Jegelka, Tamara Broderick, Trevor Campbell, Tomas Lozano-Perez for discussions and comments. We would like to thank Sungkyu Jung and Brian Axelrod for discussions on Wishart distributions. We gratefully acknowledge support from NSF grants 1420316, 1523767 and 1723381, from AFOSR grant FA9550-17-1-0165, from Honda Research and Draper Laboratory. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.	Anderson T. W, 1984, INTRO MULTIVARIATE S; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bakshy E, 2018, ARXIV180202219; Bardenet Remi, 2013, INT C MACHINE LEARNI, P199; Baxter  J, 1996, COLT; Bogunovic  Ilija, 2016, NIPS; Brazdil  Pavel, 1994, ECML; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen YT, 2017, PR MACH LEARN RES, V70; Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422; Diankov R., 2010, THESIS; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; Eaton ML., 2007, MULTIVARIATE STAT VE; Efron  Bradley, 2017, BAYES ORACLE BAYES E; Feurer Matthias, 2015, AAAI; Feurer Matthias, 2015, ADV NEURAL INFORM PR, P2962; Gal Y, 2016, PR MACH LEARN RES, V48; Golovin  Daniel, 2017, KDD; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Igel C., 2005, J MATH MODELLING ALG, V3, P313, DOI DOI 10.1023/B:JMMA.0000049381.24625.F7; Kandasamy K, 2015, PR MACH LEARN RES, V37, P295; Kandasamy Kirthevasan, 2018, ADV NEURAL INFORM PR, P2020, DOI DOI 10.5555/3326943.3327130; Kawaguchi  Kenji, 2017, AAAI; Keener Robert W, 2011, THEORETICAL STAT TOP; Kim  Beomjoon, 2017, ICRA; Kushner HaroldJ., 1964, J BASIC ENG-T ASME, V86, P97, DOI [10.1115/1.3653121, DOI 10.1115/1.3653121]; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Laurent B, 2000, ANN STAT, V28, P1302; LaValle Steven M, 2000, WORKSH ALG FDN ROB W; Li  Lisha, 2016, INT C LEARN REPR ICL; Lounici K, 2014, BERNOULLI, V20, P1029, DOI 10.3150/12-BEJ487; Malkomes  Gustavo, 2017, ICML AUTOML WORKSH; Malkomes  Gustavo, 2016, NIPS; Minka T P, 1997, TECHNICAL REPORT; Mockus  J., 1974, OPT TECHN IFIP TECHN, DOI DOI 10.1007/3-540-07165-2_55; Neal RM, 1996, LECT NOTES STAT, V118; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Petrone S, 2014, BIOMETRIKA, V101, P285, DOI 10.1093/biomet/ast067; Platt John C, 2002, NIPS; Poloczek  Matthias, 2016, WINT SIM C WSC; Poloczek  Matthias, 2017, NIPS; Robbins  Herbert, 1956, 3 BERK S MATH STAT P; Schmidhuber, 1995, LEARNING LEARN LEARN; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Shilton  Alistair, 2017, AISTATS; SIOTANI M, 1964, ANN I STAT MATH, V16, P135; Sniekers S, 2015, ELECTRON J STAT, V9, P2475, DOI 10.1214/15-EJS1078; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Wang  Ziyu, 2014, NIPS WORKSH BAYES OP; Wang Ziyu, 2017, 5 INT C LEARN REPR I; Weisstein Eric W., 1999, SQUARE ROOT INEQUALI; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893; Yogatama  Dani, 2014, AISTATS	57	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005009
C	Wei, XH; Yu, H; Ling, Q; Neely, MJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wei, Xiaohan; Yu, Hao; Ling, Qing; Neely, Michael J.			Solving Non-smooth Constrained Programs with Lower Complexity than O(1/epsilon): A Primal-Dual Homotopy Smoothing Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GRADIENT-METHOD; CONVERGENCE; ALGORITHM; OPTIMIZATION; ADMM	We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex. The best known iteration complexity solving such a non-smooth problem is O(epsilon(-1)). In this paper, we show that by leveraging a local error bound condition on the dual function, the proposed algorithm can achieve a better primal convergence time of O(epsilon(-2/(2 + beta)) log(2)(epsilon(-1))), where beta is an element of (0, 1] is a local error bound parameter. As an example application of the general algorithm, we show that the distributed geometric median problem, which can be formulated as a constrained convex program, has its dual function non-smooth but satisfying the aforementioned local error bound condition with beta = 1/2, therefore enjoying a convergence time of O(epsilon(-4/5) log(2)(epsilon(-1))) . This result improves upon the O(epsilon(-1)) convergence time bound achieved by existing distributed optimization algorithms. Simulation experiments also demonstrate the performance of our proposed algorithm.	[Wei, Xiaohan; Neely, Michael J.] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA; [Yu, Hao] Alibaba Grp US Inc, Bellevue, WA 98004 USA; [Ling, Qing] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China	University of Southern California; Sun Yat Sen University	Wei, XH (corresponding author), Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.	xiaohanw@usc.edu; hao.yu@alibaba-inc.com; lingqing556@mail.sysu.edu.cn; mikejneely@gmail.com			National Science Foundation China [61573331]; Guangdong IIET Grant [2017ZT07X355]; National Science Foundation [CCF-1718477]	National Science Foundation China(National Natural Science Foundation of China (NSFC)); Guangdong IIET Grant; National Science Foundation(National Science Foundation (NSF))	The authors thank Stanislav Minsker and Jason D. Lee for helpful discussions related to the geometric median problem. Qing Ling's research is supported in part by the National Science Foundation China under Grant 61573331 and Guangdong IIET Grant 2017ZT07X355. Qing Ling is also affiliated with Guangdong Province Key Laboratory of Computational Science. Michael J. Neely's research is supported in part by the National Science Foundation under Grant CCF-1718477.	[Anonymous], 2017, ADV NEURAL INFORM PR; Beck A, 2014, IEEE T CONTROL NETW, V1, P64, DOI 10.1109/TCNS.2014.2309751; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bertsekas D. P., 2009, CONVEX OPTIMIZATION; Boyd S, 2004, SIAM REV, V46, P667, DOI 10.1137/s0036144503423264; Burke JV, 1996, SIAM J OPTIMIZ, V6, P265, DOI 10.1137/0806015; Cohen MB, 2016, ACM S THEORY COMPUT, P9, DOI 10.1145/2897518.2897647; Deng W, 2017, J SCI COMPUT, V71, P712, DOI 10.1007/s10915-016-0318-2; Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x; Duchi J. C., 2014, ARXIV14050782; Gidel G., 2018, ARXIV180403176; Grant M., 2013, CVX MATLAB SOFTWARE; Han D., 2015, ARXIV150802134; Lan GH, 2013, MATH PROGRAM, V138, P115, DOI 10.1007/s10107-012-0588-x; Li JY, 2016, COMPUT OPTIM APPL, V64, P671, DOI 10.1007/s10589-016-9826-0; Ling Q, 2010, IEEE T SIGNAL PROCES, V58, P3816, DOI 10.1109/TSP.2010.2047721; LUO XD, 1994, SIAM J OPTIMIZ, V4, P383, DOI 10.1137/0804021; Minsker S., 2014, ARXIV14032660; Minsker S., 2017, ARXIV170402658; Motzkin T., 1952, RAND CORPORATION TRA, V22; Necoara I, 2008, IEEE T AUTOMAT CONTR, V53, P2674, DOI 10.1109/TAC.2008.2007159; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2015, MATH PROGRAM, P1; Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Pang JS, 1997, MATH PROGRAM, V79, P299, DOI 10.1007/BF02614322; Tran-Dinh Q, 2018, SIAM J OPTIMIZ, V28, P96, DOI 10.1137/16M1093094; Shi W, 2015, IEEE T SIGNAL PROCES, V63, P6013, DOI 10.1109/TSP.2015.2461520; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Sturmfels B., 2003, DIMACS SERIES DISCRE, V60, P83; Tao Wang, 1994, Optimization, V31, P1, DOI 10.1080/02331939408844003; Tseng P, 2010, MATH PROGRAM, V125, P263, DOI 10.1007/s10107-010-0394-2; Wei X., 2015, ARXIV151002973; Wei X., 2018, ARXIV180600709; Weiszfeld E, 2009, ANN OPER RES, V167, P7, DOI 10.1007/s10479-008-0352-z; Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997; Xu Yi, 2016, ADV NEURAL INFORM PR, P1208; Xue GL, 1997, SIAM J OPTIMIZ, V7, P1017, DOI 10.1137/S1052623495288362; Yang Tianbao, 2015, ARXIV151203107; Yin Dong, 2018, ARXIV180301498; Yu H., 2018, IEEE T AUTOMATIC CON; Yu H., 2017, P 9 ASIA PACIFIC S I, P1, DOI DOI 10.23919/EETA.2017.7993206; Yu H, 2017, SIAM J OPTIMIZ, V27, P759, DOI 10.1137/16M1059011; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Yurtsever A., 2018, ARXIV180408544; Yurtsever Alp, 2015, ADV NEURAL INFORM PR, P3150	47	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304004
C	Wei, ZJ; Wang, BY; Hoai, M; Zhang, JM; Lin, Z; Shen, XH; Mech, R; Samaras, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wei, Zijun; Wang, Boyu; Hoai, Minh; Zhang, Jianming; Lin, Zhe; Shen, Xiaohui; Mech, Radomir; Samaras, Dimitris			Sequence-to-Segments Networks for Segment Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Detecting segments of interest from an input sequence is a challenging problem which often requires not only good knowledge of individual target segments, but also contextual understanding of the entire input sequence and the relationships between the target segments. To address this problem, we propose the Sequence-to-Segments Network ((SN)-N-2), a novel end-to-end sequential encoder-decoder architecture. (SN)-N-2 first encodes the input into a sequence of hidden states that progressively capture both local and holistic information. It then employs a novel decoding architecture, called Segment Detection Unit (SDU), that integrates the decoder state and encoder hidden states to detect segments sequentially. During training, we formulate the assignment of predicted segments to ground truth as the bipartite matching problem and use the Earth Mover's Distance to calculate the localization errors. Experiments on temporal action proposal and video summarization show that (SN)-N-2 achieves state-of-the-art performance on both tasks.	[Wei, Zijun; Wang, Boyu; Hoai, Minh; Samaras, Dimitris] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Zhang, Jianming; Lin, Zhe; Mech, Radomir] Adobe Res, San Jose, CA USA; [Shen, Xiaohui] ByteDance AI Lab, Beijing, Peoples R China	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Adobe Systems Inc.	Wei, ZJ (corresponding author), SUNY Stony Brook, Stony Brook, NY 11794 USA.				Partner University Fund; SUNY2020 Infrastructure Transportation Security Center;  [NSF-CNS-1718014];  [NSF-IIS-1763981];  [NSF-IIS-1566248]	Partner University Fund; SUNY2020 Infrastructure Transportation Security Center; ; ; 	This project was partially supported by NSF-CNS-1718014, NSF-IIS-1763981, NSF-IIS-1566248, the Partner University Fund, the SUNY2020 Infrastructure Transportation Security Center, and a gift from Adobe.	Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675; Buch Shyamal, 2017, BMVC, P2; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung J., 2014, ARXIV14123555; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Escorcia V., 2016, EUR C COMP VIS; Gao J, 2017, PROCEEDINGS OF THE ASME INTERNAL COMBUSTION ENGINE FALL TECHNICAL CONFERENCE, 2017, VOL 2; Gehring J, 2017, PR MACH LEARN RES, V70; Gong B., 2014, ADV NEURAL INFORM PR, P2069; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2014, ARXIV14105401; Graves A., 2006, P INT C MACH LEARN; Gygli M., 2014, P EUR C COMP VIS; Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928; Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211; Hoai M, 2014, PATTERN RECOGN, V47, P1523, DOI 10.1016/j.patcog.2013.09.028; Hou L., 2016, ARXIV161105916; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; Jozefowicz R., 2015, P INT C MACH LEARN; Karpathy A., 2016, P INT C LEARN REPR; Kelley D. R., 2018, GENOME RES; Kingma D.P, P 3 INT C LEARNING R; Li S., 2018, ARXIV180304831; Luenberger D. G., 1973, INTRO LINEAR NONLINE; Ma S, 2016, PROC INT CONF RECON; Mahasseni B, 2017, PROC CVPR IEEE, P2077, DOI 10.1109/CVPR.2017.224; Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3; Minh Hoai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3265, DOI 10.1109/CVPR.2011.5995470; Nguyen M. H., 2009, P INT C COMP VIS; Peng N., 2015, 2015 C EMPIRICAL MET; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Yu G., 2015, P IEEE C COMP VIS PA; Zhang K., 2016, P EUR C COMP VIS; Zhou K., 2017, P AAAI C ART INT	47	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303050
C	Woodworth, B; Feldman, V; Rosset, S; Srebro, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Woodworth, Blake; Feldman, Vitaly; Rosset, Saharon; Srebro, Nathan			The Everlasting Database: Statistical Validity at a Fair Price	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The problem of handling adaptivity in data analysis, intentional or not, permeates a variety of fields, including test-set overfitting in ML challenges and the accumulation of invalid scientific discoveries. We propose a mechanism for answering an arbitrarily long sequence of potentially adaptive statistical queries, by charging a price for each query and using the proceeds to collect additional samples. Crucially, we guarantee statistical validity without any assumptions on how the queries are generated. We also ensure with high probability that the cost for M non-adaptive queries is O(log M), while the cost to a potentially adaptive user who makes M queries that do not depend on any others is O(root M).	[Woodworth, Blake; Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA; [Feldman, Vitaly] Google, Mountain View, CA USA; [Rosset, Saharon] Tel Aviv Univ, Tel Aviv, Israel	Toyota Technological Institute - Chicago; Google Incorporated; Tel Aviv University	Woodworth, B (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.				NSF Graduate Research Fellowship [1754881]	NSF Graduate Research Fellowship(National Science Foundation (NSF))	BW is supported the NSF Graduate Research Fellowship under award 1754881.	Aad G, 2012, PHYS LETT B, V716, P122, DOI 10.1016/j.physletb.2012.08.009; Aharoni E, 2014, J R STAT SOC B, V76, P771, DOI 10.1111/rssb.12048; Baker M, 2016, NATURE, V533, P452, DOI 10.1038/533452a; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Chatterjee N, 2016, NAT REV GENET, V17, P392, DOI 10.1038/nrg.2016.27; Craddock N, 2010, NATURE, V464, P713, DOI 10.1038/nature08979; Dwork C., 2015, ADV NEURAL INF PROCE, P2350; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, SCIENCE, V349, P636, DOI 10.1126/science.aaa9375; Dwork Cynthia, 2014, CORR; Feldman Vitaly, 2017, C LEARN THEOR COLT; Gelman A, 2014, AM SCI, V102, P460, DOI 10.1511/2014.111.460; Hardt M., 2017, CORR; Hardt M, 2015, PR MACH LEARN RES, V37, P1006; Hardt M, 2014, ANN IEEE SYMP FOUND, P454, DOI 10.1109/FOCS.2014.55; Ioannidis JPA, 2005, PLOS MED, V2, P696, DOI 10.1371/journal.pmed.0020124; Nissim K., 2015, CORR; Ullman, 2015, COLT, P1588	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001010
C	Wu, Y; Wayne, G; Gregor, K; Lillicrap, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Yan; Wayne, Greg; Gregor, Karol; Lillicrap, Timothy			Learning Attractor Dynamics for Generative Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PATH-INTEGRATION; NEURAL-NETWORK; MODEL	A central challenge faced by memory systems is the robust retrieval of a stored pattern in the presence of interference due to other stored patterns and noise. A theoretically well-founded solution to robust retrieval is given by attractor dynamics, which iteratively clean up patterns during recall. However, incorporating attractor dynamics into modern deep learning systems poses difficulties: attractor basins are characterised by vanishing gradients, which are known to make training neural networks difficult. In this work, we avoid the vanishing gradient problem by training a generative distributed memory without simulating the attractor dynamics. Based on the idea of memory writing as inference, as proposed in the Kanerva Machine, we show that a likelihood-based Lyapunov function emerges from maximising the variational lower-bound of a generative memory. Experiments shows it converges to correct patterns upon iterative retrieval and achieves competitive performance as both a memory model and a generative model.	[Wu, Yan; Wayne, Greg; Gregor, Karol; Lillicrap, Timothy] DeepMind, London, England		Wu, Y (corresponding author), DeepMind, London, England.	yanwu@google.com; gregwayne@google.com; karolg@google.com; countzero@google.com						Ackley David H., 1987, READINGS COMPUTER VI, P522, DOI [DOI 10.1016/B978-0-08-051581-6.50053-2, 10.1016/B978-0-08-051581-6.50053-2]; Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; Amit Daniel J., 1992, MODELING BRAIN FUNCT; Anderson J., 2000, LEARNING MEMORY INTE, V2nd; [Anonymous], [No title captured]; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Beattie C., 2016, ARXIV161203801; Cohen JD, 1997, NATURE, V386, P604, DOI 10.1038/386604a0; Conklin J, 2005, J COMPUT NEUROSCI, V18, P183, DOI 10.1007/s10827-005-6558-z; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Dayan P, 2001, ADV NEUR IN, V13, P451; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ganguli S, 2008, P NATL ACAD SCI USA, V105, P18970, DOI 10.1073/pnas.0804451105; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Ijspeert AJ, 2013, NEURAL COMPUT, V25, P328, DOI 10.1162/NECO_a_00393; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kanerva P., 1988, SPARSE DISTRIBUTED M; Kingma DP, 2013, P 2 INT C LEARN REPR; Kording KP, 2007, NAT NEUROSCI, V10, P779, DOI 10.1038/nn1901; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Mohamed Shakir, 2014, 31 INT C MACH LEARN; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pearlmutter BA, 1989, NEURAL COMPUT, V1, P263, DOI 10.1162/neco.1989.1.2.263; Rezende Danilo Jimenez, 2015, ARXIV150505770; Samsonovich A, 1997, J NEUROSCI, V17, P5900; Santoro A., 2016, ARXIV160506065; Saul LK, 2000, NEURAL COMPUT, V12, P1313, DOI 10.1162/089976600300015385; Sontag D., 2015, ARXIV151105121; Weston J., 2014, ARXIV14103916; Wu Y., 2018, INT C LEARN REPR; Zemel RS, 2000, ADV NEUR IN, V12, P80	33	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003089
C	Xu, J; Luo, L; Deng, C; Huang, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Jie; Luo, Lei; Deng, Cheng; Huang, Heng			Bilevel Distance Metric Learning for Robust Image Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Metric learning, aiming to learn a discriminative Mahalanobis distance matrix M that can effectively reflect the similarity between data samples, has been widely studied in various image recognition problems. Most of the existing metric learning methods input the features extracted directly from the original data in the preprocess phase. What's worse, these features usually take no consideration of the local geometrical structure of the data and the noise that exists in the data, thus they may not be optimal for the subsequent metric learning task. In this paper, we integrate both feature extraction and metric learning into one joint optimization framework and propose a new bilevel distance metric learning model. Specifically, the lower level characterizes the intrinsic data structure using graph regularized sparse coefficients, while the upper level forces the data samples from the same class to be close to each other and pushes those from different classes far away. In addition, leveraging the KKT conditions and the alternating direction method (ADM), we derive an efficient algorithm to solve the proposed new model. Extensive experiments on various occluded datasets demonstrate the effectiveness and robustness of our method.	[Xu, Jie; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China; [Xu, Jie; Luo, Lei; Huang, Heng] Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA; [Huang, Heng] JDDGlobal Com, Pittsburgh, PA USA	Xidian University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China.; Huang, H (corresponding author), Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA.; Huang, H (corresponding author), JDDGlobal Com, Pittsburgh, PA USA.	jie.xu@pitt.edu; leiluo2017@pitt.edu; chdeng.xd@gmail.com; heng.huang@pitt.edu			National Natural Science Foundation of China [61572388]; National Key Research and Development Program of China [2017YFE0104100]; Key R&D Program-The Key Industry Innovation Chain of Shaanxi [2017ZDCXL-GY-05-04-02, 2018ZDXM-GY-176];  [U.S. NSF-IIS 1836945];  [NSF-IIS 1836938];  [NSF-DBI 1836866];  [NSF-IIS 1845666];  [NSF-IIS 1852606];  [NSF-IIS 1838627];  [NSF-IIS 1837956]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; Key R&D Program-The Key Industry Innovation Chain of Shaanxi; ; ; ; ; ; ; 	J.X. and C.D. were partially supported by the National Natural Science Foundation of China 61572388, the National Key Research and Development Program of China (2017YFE0104100), and the Key R&D Program-The Key Industry Innovation Chain of Shaanxi under Grants 2017ZDCXL-GY-05-04-02 and 2018ZDXM-GY-176.; L. L. and H. H. were partially supported by U.S. NSF-IIS 1836945, NSF-IIS 1836938, NSF-DBI 1836866, NSF-IIS 1845666, NSF-IIS 1852606, NSF-IIS 1838627, NSF-IIS 1837956.	Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101; Chen S, 2017, IEEE T IMAGE PROCESS, V26, P5519, DOI 10.1109/TIP.2017.2738560; Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764; Huo Z, 2016, P 22 ACM SIGKDD INT, P1605; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Law MT, 2014, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2014.138; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin Zhouchen, 2011, NIPS, P612; Lu JW, 2015, IEEE T INF FOREN SEC, V10, P79, DOI 10.1109/TIFS.2014.2363792; Luo L, 2018, AAAI CONF ARTIF INTE, P3722; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Peterson L., 2009, SCHOLARPEDIA, V4, P1883, DOI [10.4249/scholarpedia.1883, DOI 10.4249/SCHOLARPEDIA.1883]; Wang ZY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3932; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Xu J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2919; Xu Jie, 2018, SIGKDD, P2555; Yang JC, 2012, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2012.6247948; Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286; Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535; Zhou P, 2017, IEEE T IMAGE PROCESS, V26, P1173, DOI 10.1109/TIP.2016.2623487	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304023
C	Xu, P; Chen, JH; Zou, DF; Gu, QQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Pan; Chen, Jinghui; Zou, Difan; Gu, Quanquan			Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DIFFUSION	We present a unified framework to analyze the global convergence of Langevin dynamics based algorithms for nonconvex finite-sum optimization with n component functions. At the core of our analysis is a direct analysis of the ergodicity of the numerical approximations to Langevin dynamics, which leads to faster convergence rates. Specifically, we show that gradient Langevin dynamics (GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the almost minimizer(2) within (O) over tilde (nd/(lambda epsilon)) and (O) over tilde (d(7)/ (lambda(5)epsilon(5))) stochastic gradient evaluations respectively(3), where d is the problem dimension, and lambda is the spectral gap of the Markov chain generated by GLD. Both results improve upon the best known gradient complexity(4) results [45]. Furthermore, for the first time we prove the global convergence guarantee for variance reduced stochastic gradient Langevin dynamics (SVRG-LD) to the almost minimizer within (O) over tilde(root nd(5)/(lambda(4)epsilon(5/2))) stochastic gradient evaluations, which outperforms the gradient complexities of GLD and SGLD in a wide regime. Our theoretical analyses shed some light on using Langevin dynamics based algorithms for nonconvex optimization with provable guarantees.	[Xu, Pan; Zou, Difan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA; [Chen, Jinghui] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA	University of California System; University of California Los Angeles; University of Virginia	Xu, P (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	panxu@cs.ucla.edu; jc4zg@virginia.edu; knowzou@cs.ucla.edu; qgu@cs.ucla.edu	Xu, Pan/AAH-3620-2019; X, Pan/GVS-4402-2022	Xu, Pan/0000-0002-2559-8622; 	National Science Foundation [IIS-1652539]	National Science Foundation(National Science Foundation (NSF))	We would like to thank the anonymous reviewers for their helpful comments. We thank Maxim Raginsky for insightful comments and discussion on the first version of this paper. We also thank Tianhao Wang for discussion on this work. This research was sponsored in part by the National Science Foundation IIS-1652539. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; Ahn S., 2012, P 29 INT COF INT C M, P1771; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Bakry D., 2013, ANAL GEOMETRY MARKOV, V348; Bolley F., 2005, ANN FAC SCI TOULOUSE, V14, P331, DOI 10.5802/afst.1095; Brosse Nicolas, 2017, P MACHINE LEARNING R, V65, P319; Bubeck S, 2018, DISCRETE COMPUT GEOM, V59, P757, DOI 10.1007/s00454-018-9992-1; CARMON Y, 2016, ARXIV161200547; Carmon Y, 2018, SIAM J OPTIMIZ, V28, P1751, DOI 10.1137/17M1114296; Chatterji NS, 2018, PR MACH LEARN RES, V80; Chen C., 2015, NIPS; Cheng X., 2018, C LEARNING THEORY, P300; CHIANG TS, 1987, SIAM J CONTROL OPTIM, V25, P737, DOI 10.1137/0325042; Curtis FE, 2017, MATH PROGRAM, V162, P1, DOI 10.1007/s10107-016-1026-2; Dalalyan A. S., 2017, ARXIV170404752; Dalalyan A. S., 2017, ARXIV171000095; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154; Durmus A., 2015, ARXIV150705021; Durmus A., 2016, ARXIV160501559; Dwivedi R., 2018, C LEARNING THEORY PM, P793; Erdogdu MA., 2018, ADV NEURAL INFORM PR, Vvol 31, P9671; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; GE R., 2017, ARXIV171002736; GELFAND SB, 1991, SIAM J CONTROL OPTIM, V29, P999, DOI 10.1137/0329055; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hairer M, 2008, ANN PROBAB, V36, P2050, DOI 10.1214/08-AOP392; Hwang Chii-Ruey, 1980, ANN PROBAB, P1177; Ikeda N, 2014, STOCHASTIC DIFFERENT, V24; Jin C., 2018, P 31 C LEARNING THEO, P1042; Jin C, 2017, PR MACH LEARN RES, V70; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; KLOEDEN PE, 1992, J STAT PHYS, V66, P283, DOI 10.1007/BF01060070; Levin D. A., 2009, MARKOV CHAINS MIXING; Levy K. Y., 2016, ARXIV161104831; Li Z., 2018, ARXIV180311159; Liptser R.S., 2013, STAT RANDOM PROCESSE, V5; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3; Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Polyanskiy Y, 2016, IEEE T INFORM THEORY, V62, P3992, DOI 10.1109/TIT.2016.2562630; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Reddi SJ, 2016, PR MACH LEARN RES, V48; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Sato I, 2014, PR MACH LEARN RES, V32, P982; Simsekli Umut, 2018, P 35 INT C MACH LEAR, P4674; Tzen B., 2018, P 2018 C LEARN THEOR, P857; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zhang Y., 2017, C LEARN THEOR PMLR, P1980; Zhou DR, 2018, PR MACH LEARN RES, V80; Zou D., 2018, P INT C UNC ART INT P INT C UNC ART INT; Zou DF, 2018, PR MACH LEARN RES, V80	56	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303015
C	Xu, ZQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Zhiqiang			Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION	Shift-and-invert preconditioning, as a classic acceleration technique for the leading eigenvector computation, has received much attention again recently, owing to fast least-squares solvers for efficiently approximating matrix inversions in power iterations. In this work, we adopt an inexact Riemannian gradient descent perspective to investigate this technique on the effect of the step-size scheme. The shift-and-inverted power method is included as a special case with adaptive step-sizes. Particularly, two other step-size settings, i.e., constant step-sizes and Barzilai-Borwein (BB) step-sizes, are examined theoretically and/or empirically. We present a novel convergence analysis for the constant step-size setting that achieves a rate at (O) over tilde (root lambda(1)/lambda(1)-lambda(p+1)), where lambda(i) represents the i-th largest eigenvalue of the given real symmetric matrix and p is the multiplicity of lambda(1). Our experimental studies show that the proposed algorithm can be significantly faster than the shift-and-inverted power method in practice.	[Xu, Zhiqiang] Natl Engn Lab Deep Learning Technol & Applicat, CCL, Baidu Res, Beijing, Peoples R China	Baidu	Xu, ZQ (corresponding author), Natl Engn Lab Deep Learning Technol & Applicat, CCL, Baidu Res, Beijing, Peoples R China.	xuzhiqiang04@baidu.com	Xu, Zhiqiang/AAB-7414-2022					Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; Arora Raman, 2013, ADV NEURAL INFORM PR, P1815; Balcan M-F., 2016, C LEARNING THEORY, P284; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; De Sa  Christopher, 2017, ABS170702670 CORR; Fan J, 2018, ARXIV180101602; Gao  Chao, 2017, ABS170206533 CORR; Garber D., 2015, ARXIV150905647; Garber D, 2016, PR MACH LEARN RES, V48; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hastie T, 2015, J MACH LEARN RES, V16, P3367; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Liu G., 2014, P ADV NEUR INF PROC, P1206; Liu HL, 2016, PROC INT CONF DATA, P1158, DOI 10.1109/ICDE.2016.7498321; Musco C, 2015, ADV NEUR IN, V28; Nesterov Y, 2014, INTRO LECT CONVEX OP; Ng AY, 2002, ADV NEUR IN, V14, P849; Qi Lei, 2016, ADV NEURAL INFORM PR, P2056; Shamir O, 2016, PR MACH LEARN RES, V48; Shamir O, 2016, PR MACH LEARN RES, V48; Shamir O, 2015, PR MACH LEARN RES, V37, P144; Wang Jialei, 2017, ABS170207834 CORR; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; Xu ZQ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2933; Xu Zhiqiang, 2017, UAI; Xu Zhiqiang, 2018, INT C ART INT STAT A, P168; Zhu Z. Allen, 2016, ADV NEURAL INFORM PR, P974	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302081
C	Yang, YX; Dai, B; Kiyavash, N; He, NA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yang, Yingxiang; Dai, Bo; Kiyavash, Negar; He, Niao			Predictive Approximate Bayesian Computation via Saddle Points	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE; EVOLUTION	Approximate Bayesian computation (ABC) is an important methodology for Bayesian inference when the likelihood function is intractable. Sampling-based ABC algorithms such as rejection- and K2-ABC are inefficient when the parameters have high dimensions, while the regression-based algorithms such as K- and DR-ABC are hard to scale. In this paper, we introduce an optimization-based ABC framework that addresses these deficiencies. Leveraging a generative model for posterior and joint distribution matching, we show that ABC can be framed as saddle point problems, whose objectives can be accessed directly with samples. We present the predictive ABC algorithm (P-ABC), and provide a probabilistically approximately correct (PAC) bound for its learning consistency. Numerical experiment shows that P-ABC outperforms both K2- and DR-ABC significantly.	[Yang, Yingxiang; He, Niao] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Kiyavash, Negar; He, Niao] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL USA; [Dai, Bo] Google Brain, Mountain View, CA USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; Google Incorporated	Yang, YX (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.	yyang172@illinois.edu; bohr.dai@gmail.com; kiyavash@illinois.edu; niaoheg@illinois.edu	YANG, Yingxiang/GQI-2269-2022		MURI grant ARMY [W911NF-15-1-0479]; ONR [W911NF15-1-0479]; NSF [CCF-1755829, CMMI-1761699]	MURI grant ARMY(MURI); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This work was supported in part by MURI grant ARMY W911NF-15-1-0479, ONR grant W911NF15-1-0479, NSF CCF-1755829 and NSF CMMI-1761699.	[Anonymous], 2017, ARXIV170104722; Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Barthelme S., 2011, P 28 INT C MACH LEAR, P289; Bartlett P. L., 2017, ARXIV; Bernton Espen, 2017, ARXIV170105146; Blum MGB, 2013, STAT SCI, V28, P189, DOI 10.1214/12-STS406; Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0; Csillery K, 2012, METHODS ECOL EVOL, V3, P475, DOI 10.1111/j.2041-210X.2011.00179.x; Dai B, 2017, PR MACH LEARN RES, V54, P1458; Drovandi CC, 2011, BIOMETRICS, V67, P225, DOI 10.1111/j.1541-0420.2010.01410.x; Drummond AJ, 2007, BMC EVOL BIOL, V7, DOI 10.1186/1471-2148-7-214; Gleim A., APPROXIMATE BAYESIAN; Gutmann MU, 2018, STAT COMPUT, V28, P411, DOI 10.1007/s11222-017-9738-6; Gutmann Michael U, 2014, ARXIV14074981; Gutmann Michael U, 2016, J MACHINE LEARNING R; HAUSSLER D, 1995, J COMB THEORY A, V69, P217, DOI 10.1016/0097-3165(95)90052-7; Huelsenbeck JP, 2001, SCIENCE, V294, P2310, DOI 10.1126/science.1065889; Joyce P, 2008, STAT APPL GENET MOL, V7; Kingma DP, 2 INT C LEARN REPR I, P1; Kodali N., 2018, CONVERGENCE STABILIT; Kulkarni T.D., 2014, NIPS; Leuenberger Christoph, 2009, GENETICS, P18, DOI [10.1534/genetics.109.102509, DOI 10.1534/GENETICS.109.102509]; Li J, 2017, COMPUT STAT DATA AN, V106, P77, DOI 10.1016/j.csda.2016.07.005; Li Y., 2017, ADV NEURAL INFORM PR, P597; Martinez G.A., 2014, AGRO IND WASTE VALOR, P1; Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810; Mirza M., 2014, CONDITIONAL GENERATI; Mitrovic J., 2016, DR ABC APPROXIMATE B; Mohamed Shakir, 2016, ARXIV161003483; Nott DJ, 2014, J COMPUT GRAPH STAT, V23, P65, DOI 10.1080/10618600.2012.751874; Nunes MA, 2010, STAT APPL GENET MOL, V9, DOI 10.2202/1544-6115.1576; Raynal L., 2016, ARXIV160505537; Rodrigues GS, 2018, COMPUT STAT DATA AN, V126, P53, DOI 10.1016/j.csda.2018.04.004; Sejdinovic D., 2016, K2 ABC APPROXIMATE B; Sinha Aman, 2017, ARXIV PREPRINT ARXIV; Wegmann D, 2009, GENETICS, V182, P1207, DOI 10.1534/genetics.109.102509; Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319; ZELLNER A, 1988, AM STAT, V42, P278, DOI 10.2307/2685143	40	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004078
C	Yao, QM; Kwok, JT		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yao, Quanming; Kwok, James T.			Scalable Robust Matrix Factorization with Nonconvex Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				VARIABLE SELECTION; COMPLETION; GRADIENT; ALGORITHM	Matrix factorization (MF), which uses the l(2)-loss, and robust matrix factorization (RMF), which uses the l(1)-loss, are sometimes not robust enough for outliers. Moreover, even the state-of-the-art RMF solver (RMF-MM) is slow and cannot utilize data sparsity. In this paper, we propose to improve robustness by using nonconvex loss functions. The resultant optimization problem is difficult. To improve efficiency and scalability, we use majorization-minimization (MM) and optimize the MM surrogate by using the accelerated proximal gradient algorithm on its dual problem. Data sparsity can also be exploited. The resultant algorithm has low time and space complexities, and is guaranteed to converge to a critical point. Extensive experiments show that it outperforms the state-of-the-art in terms of both accuracy and speed.	[Yao, Quanming] 4Paradigm Inc, Beijing, Peoples R China; [Yao, Quanming; Kwok, James T.] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Yao, QM (corresponding author), 4Paradigm Inc, Beijing, Peoples R China.; Yao, QM (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	yaoquanming@4paradigm.com; jamesk@cse.ust.hk	Yao, Quanming/Y-6095-2019	Yao, Quanming/0000-0001-8944-8618				Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyd S, 2004, CONVEX OPTIMIZATION; Burke R., 2015, RECOMMENDER SYSTEMS; Cabral R, 2013, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2013.309; Cambier L, 2016, SIAM J SCI COMPUT, V38, pS440, DOI 10.1137/15M1025153; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Clarke Frank H., 1990, OPTIMIZATION NONSMOO, V5; De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986; Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; GEMAN D, 1995, IEEE T IMAGE PROCESS, V4, P932, DOI 10.1109/83.392335; Gong P., 2015, ADV NEURAL INFORM PR, P415; Gong Pinghua, 2013, JMLR Workshop Conf Proc, V28, P37; Hastie T, 2015, J MACH LEARN RES, V16, P3367; He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848; Huber P. J., 2011, ROBUST STAT; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Jiang WH, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3590; Kim E, 2015, IEEE T NEUR NET LEAR, V26, P237, DOI 10.1109/TNNLS.2014.2312535; KOENDERINK JJ, 1991, J OPT SOC AM A, V8, P377, DOI 10.1364/JOSAA.8.000377; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Lin  Z., 2017, IEEE T PATTERN ANAL; Lin Z., 2015, MACH LEARN, V99, P287, DOI DOI 10.1007/s10994-014-5469-5; Meng D., 2013, P AAAI C ARTIFICIAL, P704; Mensch A, 2016, PR MACH LEARN RES, V48; Mishra B, 2016, SIAM J OPTIMIZ, V26, P635, DOI 10.1137/140970860; Mobasher B, 2007, ACM T INTERNET TECHN, V7, DOI 10.1145/1278366.1278372; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Tanner J, 2016, APPL COMPUT HARMON A, V40, P417, DOI 10.1016/j.acha.2015.08.003; Trzasko J, 2009, IEEE T MED IMAGING, V28, P106, DOI 10.1109/TMI.2008.927346; Yan M, 2013, SIAM J IMAGING SCI, V6, P1227, DOI 10.1137/12087178X; Yan M, 2013, J SCI COMPUT, V56, P433, DOI 10.1007/s10915-013-9682-3; Yang J., 2013, P 6 ACM INT C WEB SE, P587, DOI [DOI 10.1145/2433396.2433471, 10.1145/2433396.2433471]; Yao Quanming, 2018, IEEE T PATTERN ANAL; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828; Zuo WM, 2013, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2013.34	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305010
C	Yen, IEH; Lee, WC; Chang, SE; Zhong, K; Ravikumar, P; Lin, SD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yen, Ian E. H.; Lee, Wei-Cheng; Chang, Sung-En; Zhong, Kai; Ravikumar, Pradeep; Lin, Shou-De			MixLasso: Generalized Mixed Regression via Convex Atomic-Norm Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider a generalization of mixed regression where the response is an additive combination of several mixture components. Standard mixed regression is a special case where each response is generated from exactly one component. Typical approaches to the mixture regression problem employ local search methods such as Expectation Maximization (EM) that are prone to spurious local optima. On the other hand, a number of recent theoretically-motivated Tensor-based methods either have high sample complexity, or require the knowledge of the input distribution, which is not available in most of practical situations. In this work, we study a novel convex estimator MixLasso for the estimation of generalized mixed regression, based on an atomic norm specifically constructed to regularize the number of mixture components. Our algorithm gives a risk bound that trades off between prediction accuracy and model sparsity without imposing stringent assumptions on the input/output distribution, and can be easily adapted to the case of non-linear functions. In our numerical experiments on mixtures of linear as well as nonlinear regressions, the proposed method yields high-quality solutions in a wider range of settings than existing approaches.	[Yen, Ian E. H.; Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Yen, Ian E. H.] Snap Inc, Santa Monica, CA 90405 USA; [Lee, Wei-Cheng; Chang, Sung-En; Lin, Shou-De] Natl Taiwan Univ, Taipei, Taiwan; [Zhong, Kai] Amazon Inc, Seattle, WA USA	Carnegie Mellon University; National Taiwan University; Amazon.com	Yen, IEH (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.; Yen, IEH (corresponding author), Snap Inc, Santa Monica, CA 90405 USA.				NSF [IIS-1149803, IIS-1664720, DMS-1264033]; ONR [N000141812861]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	P.R. acknowledges the support of NSF via IIS-1149803, IIS-1664720, DMS-1264033, and ONR via N000141812861.	BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Hand  P., 2016, ARXIV161206067; Lei Xu, 1995, Advances in Neural Information Processing Systems 7, P633; Nesterov Y., 1997, QUALITY SEMIDEFINITE; Sedghi H, 2016, JMLR WORKSH CONF PRO, V51, P1223; Svenskn M., 2002, P 19 C UNC ART INT, P57; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang P.-W., 2017, ARXIV170600476; Yen IEH, 2015, PR MACH LEARN RES, V37, P2418; Yen Ian E H, 2016, JMLR Workshop Conf Proc, V48, P2272; Yen Ian En-Hsu, 2017, INT C MACH LEARN; Yi XY, 2014, PR MACH LEARN RES, V32, P613; Yi Xinyang, 2016, ARXIV160805749; Zhong K., 2016, ADV NEURAL INFORM PR, P2190	17	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005045
C	Yeom, S; Datta, A; Fredrikson, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yeom, Samuel; Datta, Anupam; Fredrikson, Matt			Hunting for Discriminatory Proxies in Linear Regression Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models, and characterizes a model's constituent behavior that: 1) correlates closely with a protected random variable, and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a "business necessity". Finally, we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes, demonstrating that proxies shed useful light on the causes of discriminatory behavior in models.	[Yeom, Samuel; Datta, Anupam; Fredrikson, Matt] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Yeom, S (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	syeom@cs.cmu.edu; danupam@cmu.edu; mfredrik@cs.cmu.edu	Fredrikson, Matt/GQR-0633-2022		National Science Foundation [CNS-1704845]	National Science Foundation(National Science Foundation (NSF))	The authors would like to thank the anonymous reviewers at NeurIPS 2018 for their thoughtful feedback. This material is based upon work supported by the National Science Foundation under Grant No. CNS-1704845.	Adler P, 2018, KNOWL INF SYST, V54, P95, DOI 10.1007/s10115-017-1116-3; Andersen M. S., CVXOPT PYTHON SOFTWA; Angwin J., 2016, PROPUBLICA, P254; Asher Jeff, 2017, NY TIMES; Barnes JC, 2010, AGGRESSIVE BEHAV, V36, P437, DOI 10.1002/ab.20359; Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31; Boyd S, 2004, CONVEX OPTIMIZATION; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; City of Chicago, 2017, STRAT SUBJ LIST; Datta Amit, 2015, Proceedings on Privacy Enhancing Technologies, V1, P92, DOI 10.1515/popets-2015-0007; Datta A, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1193, DOI 10.1145/3133956.3134097; Datta A, 2016, P IEEE S SECUR PRIV, P598, DOI 10.1109/SP.2016.42; Datta Anupam, 2017, ARXIV170708120; Dieterich William, 2016, COMPAS RISK SCALES D; Dua D., UCI MACHINE LEARNING; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Equivant, 2017, PRACT GUID COMPAS CO; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Ingold D, 2016, BLOOMBERG; Kilbertus Niki, 2017, ADV NEURAL INFORM PR, P656; Marr B., 2018, FORBES; Redmond M, 2002, EUR J OPER RES, V141, P660, DOI 10.1016/S0377-2217(01)00264-8; Vaithianathan R, 2017, DEV PREDICTIVE MODEL; Yeom Samuel, 2018, ARXIV181007155	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304057
C	Yin, Z; Sachidananda, V; Prabhakar, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yin, Zi; Sachidananda, Vin; Prabhakar, Balaji			The Global Anchor Method for Quantifying Linguistic Shifts and Domain Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EVOLUTION	Language is dynamic, constantly evolving and adapting with respect to time, domain or topic. The adaptability of language is an active research area, where researchers discover social, cultural and domain-specific changes in language using distributional tools such as word embeddings. In this paper, we introduce the global anchor method for detecting corpus-level language shifts. We show both theoretically and empirically that the global anchor method is equivalent to the alignment method, a widely-used method for comparing word embeddings, in terms of detecting corpus-level language shifts. Despite their equivalence in terms of detection abilities, we demonstrate that the global anchor method is superior in terms of applicability as it can compare embeddings of different dimensionalities. Furthermore, the global anchor method has implementation and parallelization advantages. We show that the global anchor method reveals fine structures in the evolution of language and domain adaptation. When combined with the graph Laplacian technique, the global anchor method recovers the evolution trajectory and domain clustering of disparate text corpora.	[Yin, Zi; Sachidananda, Vin; Prabhakar, Balaji] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Prabhakar, Balaji] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University; Stanford University	Yin, Z (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	s09600974@gmail.com; vsachi@stanford.edu; balaji@stanford.edu						[Anonymous], 2015, STUCK MATRIX REDDIT; Arora S., 2016, T ASS COMPUTATIONAL, V4, P385, DOI DOI 10.1162/TACL_A_00106; Bai Z., 2008, ADV STAT, P108; Davies Mark, 2015, CORPUS HIST AM ENGLI, DOI [10.7910/DVN/8SRSYK, DOI 10.7910/DVN/8SRSYK]; Dorow B., 2003, P 10 C EUR CHAPT ASS, V2, P79, DOI DOI 10.3115/1067737.1067753; Firth JohnRupert., 1957, SPECIAL VOLUME PHILO, P1; Galantai A, 2006, NUMER LINEAR ALGEBR, V13, P589, DOI 10.1002/nla.491; Gulordava Kristina, 2011, P GEMS 2011 WORKSH G, P67; Hamilton WL, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1489; Hamilton William L, 2016, Proc Conf Empir Methods Nat Lang Process, V2016, P595, DOI 10.18653/v1/D16-1057; Hamilton WL, 2017, P 31 INT C NEUR INF, P1025; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; Jatowt A, 2014, ACM-IEEE J CONF DIG, P229, DOI 10.1109/JCDL.2014.6970173; Kim Yoon., 2014, P ACL 2014 WORKSHOP, P61; Kutuzov Andrey, 2017, P EV STOR NEWS WORKS, P31, DOI DOI 10.18653/V1/W17-2705; Kutuzov Andrey, 2017, P 2017 C EMPIRICAL M, P1824; Lau J. H., 2012, P 13 C EUR CHAPT ASS, P591; Lin Y., 2012, P ACL 2012 SYSTEM DE, P169, DOI DOI 10.1007/978-1-4614-2311-9_8; Lu J, 2018, PR MACH LEARN RES, V80; MARSAGLIA G, 1972, ANN MATH STAT, V43, P645, DOI 10.1214/aoms/1177692644; Mikolov T., 2013, CORR; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mitra S, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1020; Pagel M, 2007, NATURE, V449, P717, DOI 10.1038/nature06176; Pechenick EA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137041; Reali F, 2010, P ROY SOC B-BIOL SCI, V277, P429, DOI 10.1098/rspb.2009.1513; Rudin W, 1987, REAL COMPLEX ANAL; Rudolph M, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1003, DOI 10.1145/3178876.3185999; Sagi Eyal, CURRENT METHODS HIST, P161; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Smith Samuel L, 2017, INT C LEARN REPR ICL; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Yang Xu, 2015, COGNITIVE SCI SOC CO; Yao ZJ, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P673, DOI 10.1145/3159652.3159703; Yin Z, 2018, P ADV NEURAL INFORM, V31, P895; Zhang X., 2013, ADV NEURAL INF PROCE, V26, P1637; Zhang XQ, 2014, AAAI CONF ARTIF INTE, P1362; Zhou Z., 2018, ARXIV181004778; Zi Yin, 2018, ARXIV180300502	40	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004002
C	Yu, M; Yang, ZR; Zhao, T; Kolar, M; Wang, ZR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yu, Ming; Yang, Zhuoran; Zhao, Tuo; Kolar, Mladen; Wang, Zhaoran			Provable Gaussian Embedding with One Observation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SELECTION	The success of machine learning methods heavily relies on having an appropriate representation for data at hand. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about data. However, recently there has been a surge in approaches that learn how to encode the data automatically in a low dimensional space. Exponential family embedding provides a probabilistic framework for learning low-dimensional representation for various types of high-dimensional data [20]. Though successful in practice, theoretical underpinnings for exponential family embeddings have not been established. In this paper, we study the Gaussian embedding model and develop the first theoretical results for exponential family embedding models. First, we show that, under mild condition, the embedding structure can be learned from one observation by leveraging the parameter sharing between different contexts even though the data are dependent with each other. Second, we study properties of two algorithms used for learning the embedding structure and establish convergence results for each of them. The first algorithm is based on a convex relaxation, while the other solved the non-convex formulation of the problem directly. Experiments demonstrate the effectiveness of our approach.	[Yu, Ming; Kolar, Mladen] Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA; [Yang, Zhuoran] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Zhao, Tuo] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; [Wang, Zhaoran] Northwestern Univ, Dept Ind Engn & Management Sci, Evanston, IL 60208 USA	University of Chicago; Princeton University; University System of Georgia; Georgia Institute of Technology; Northwestern University	Yu, M (corresponding author), Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA.	ming93@uchicago.edu	Wang, Zhaoran/P-7113-2018					Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Bunea F, 2011, ANN STAT, V39, P1282, DOI 10.1214/11-AOS876; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen SZ, 2015, BIOMETRIKA, V102, P47, DOI 10.1093/biomet/asu051; Hu GY, 1996, J PHYS A-MATH GEN, V29, P1511, DOI 10.1088/0305-4470/29/7/020; Lauritzen Steffen L., 1996, OXFORD STAT SCI SERI, V17; Ledoux M., 2013, PROBABILITY BANACH S, P86; Lee JD, 2015, J COMPUT GRAPH STAT, V24, P230, DOI 10.1080/10618600.2014.900500; Levy O., 2015, T ASSOC COMPUT LING, V3, P211, DOI [10.1162/tacl_a_00134, DOI 10.1162/TACL_A_00134]; Levy O, 2014, ADV NEUR IN, V27; Liu, 2015, ADV NEURAL INFORM PR; Mikolov T., 2013, P NAACL 2013, P746; Mikolov T., 2013, EFFICIENT ESTIMATION; Mukherjee T., 2016, P C EMP METH NAT LAN, P912; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rudolph M., 2017, ADV NEURAL INFORM PR, P250; Rudolph M., 2016, P ANN C NEURAL INFOR, P478; Rudolph Maja, 2017, ARXIV170308052; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Wang JL, 2016, JMLR WORKSH CONF PRO, V51, P1042; Wang YJ, 2008, BIOMETRIKA, V95, P735, DOI 10.1093/biomet/asn029; Wang Z., 2014, ARXIV14085352; Yang EH, 2015, J MACH LEARN RES, V16, P3813; Yu M., 2018, ARXIV180206967; Yu M., 2016, ADV NEURAL INFORM PR, V29, P2829; Yu M, 2017, IEEE DATA MINING, P1141, DOI 10.1109/ICDM.2017.152; Yu Ming, 2018, ARXIV180605730; Zou Will Y, 2013, P 2013 C EMP METH NA, P1393	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001032
C	Yu, MC; Lin, ZF; Narra, K; Li, SZ; Li, YJ; Kim, NS; Schwing, A; Annavaram, M; Avestimehr, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yu, Mingchao; Lin, Zhifeng; Narra, Krishna; Li, Songze; Li, Youjie; Kim, Nam Sung; Schwing, Alexander; Annavaram, Murali; Avestimehr, Salman			GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Data parallelism can boost the training speed of convolutional neural networks (CNN), but could suffer from significant communication costs caused by gradient aggregation. To alleviate this problem, several scalar quantization techniques have been developed to compress the gradients. But these techniques could perform poorly when used together with decentralized aggregation protocols like ring all-reduce (RAR), mainly due to their inability to directly aggregate compressed gradients. In this paper, we empirically demonstrate the strong linear correlations between CNN gradients, and propose a gradient vector quantization technique, named GradiVeQ, to exploit these correlations through principal component analysis (PCA) for substantial gradient dimension reduction. GradiVeQ enables direct aggregation of compressed gradients, hence allows us to build a distributed learning system that parallelizes GradiVeQ gradient compression and RAR communications. Extensive experiments on popular CNNs demonstrate that applying GradiVeQ slashes the wall-clock gradient aggregation time of the original RAR by more than 5X without noticeable accuracy loss, and reduces the end-to-end training time by almost 50%. The results also show that GradiVeQ is compatible with scalar quantization techniques such as QSGD (Quantized SGD), and achieves a much higher speed-up gain under the same compression ratio.	[Yu, Mingchao; Lin, Zhifeng; Narra, Krishna; Li, Songze; Annavaram, Murali; Avestimehr, Salman] Univ Southern Calif, Los Angeles, CA 90089 USA; [Li, Youjie; Kim, Nam Sung; Schwing, Alexander] Univ Illinois, Champaign, IL USA	University of Southern California; University of Illinois System; University of Illinois Urbana-Champaign	Yu, MC (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.		Avestimehr, Amir Salman/O-7864-2019; Li, Songze/AAU-6876-2021; narra, hema venkata krishna giri/AAB-9242-2020	Li, Songze/0000-0003-4282-3307; 	Defense Advanced Research Projects Agency (DARPA) [HR001117C0053]; NSF [CCF-1763673, CCF-1703575, CNS-1705047, CNS-1557244, SHF-1719074]	Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF))	This material is based upon work supported by Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0053. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. This work is also supported by NSF Grants CCF-1763673, CCF-1703575, CNS-1705047, CNS-1557244, SHF-1719074.	Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736; Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864; Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Alistarh D, 2017, ADV NEUR IN, V30; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Dean J., 2012, NIPS 12, V1, P1223; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dryden N, 2016, PROCEEDINGS OF 2016 2ND WORKSHOP ON MACHINE LEARNING IN HPC ENVIRONMENTS (MLHPC), P1, DOI [10.1109/MLHPC.2016.4, 10.1109/MLHPC.2016.004]; Gibiansky Andrew, 2017, BRINGING HPC TECHNIQ; Goyal P., 2018, ARXIV170602677; Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223; Iandola F. N., 2016, CVPR; Krizhevsky A, 2009, CIFAR 10; Langford J., 2009, PROC 22 INT C NEURAL, P2331; Li M., 2014, NIPS; Li Mu, 2014, P 11 USENIX C OP SYS, P583; Lian X., 2018, ARXIV171006952; Lin Y., 2018, ICLR; McDonald R., 2010, HUMAN LANGUAGE TECHN, P456; Miao YS, 2016, PR MACH LEARN RES, V48; Patarasuk P, 2009, J PARALLEL DISTR COM, V69, P117, DOI 10.1016/j.jpdc.2008.09.002; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Seide F, 2014, INTERSPEECH, P1058; Simonyan Karen, 2015, INT C LEARN REPR; Thakur R, 2005, INT J HIGH PERFORM C, V19, P49, DOI 10.1177/1094342005051521; Wen W., 2017, NIPS 17, V30, P1509; Yong Zhuang, 2013, P 7 ACM C REC SYST, P249, DOI DOI 10.1145/2507157.2507164; Zhang Tong, 2014, ARXIV PREPRINT ARXIV; Zhou Shuchang, 2016, P IEEE C COMP VIS PA; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	32	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305016
C	Yu, YD; Xu, P; Gu, QQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yu, Yaodong; Xu, Pan; Gu, Quanquan			Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently. More specifically, the proposed algorithm only needs ((O) over tilde(epsilon(-10)(/3)) stochastic gradient evaluations to converge to an approximate local minimum x, which satisfies parallel to del f (x)parallel to(2) <= epsilon and lambda(min) (del(2) f (x)) >= - root epsilon in unconstrained stochastic optimization, where (O) over tilde(.) hides logarithm polynomial terms and constants. This improves upon the (O) over tilde(epsilon(-7/2)) gradient complexity achieved by the state-of-the-art stochastic local minima finding algorithms by a factor of (O) over tilde(epsilon(-1/6)). Experiments on two nonconvex optimization problems demonstrate the effectiveness of our algorithm and corroborate our theory.	[Yu, Yaodong] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA; [Xu, Pan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of Virginia; University of California System; University of California Los Angeles	Yu, YD (corresponding author), Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA.	yy8ms@virginia.edu; panxu@cs.ucla.edu; qgu@cs.ucla.edu	X, Pan/GVS-4402-2022; Xu, Pan/AAH-3620-2019; Yu, Yaodong/AAN-1642-2021	Xu, Pan/0000-0002-2559-8622; Yu, Yaodong/0000-0003-0540-8526	National Science Foundation [IIS-1652539, BIGDATA IIS-1855099]; NSF BIGDATA award	National Science Foundation(National Science Foundation (NSF)); NSF BIGDATA award	We would like to thank the anonymous reviewers for their helpful comments, and Yu Chen, Xuwang Yin for their helpful discussions on the experiments. This research was sponsored in part by the National Science Foundation IIS-1652539 and BIGDATA IIS-1855099. We also thank AWS for providing cloud computing credits associated with the NSF BIGDATA award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	AGARWAL N, 2016, ARXIV161101146; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Anandkumar A., 2016, 29 ANN C LEARNING TH, P81; Carmon Y., 2017, ARXIV170502766; CARMON Y, 2016, ARXIV161200547; Carmon Yair, 2016, ARXIV161100756; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; CURTIS FE, 2017, ARXIV170300412; Curtis FE, 2017, MATH PROGRAM, V162, P1, DOI 10.1007/s10107-016-1026-2; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Garber D, 2016, PR MACH LEARN RES, V48; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Jin C., 2018, C LEARN THEOR COLT 2; Jin Chi, 2017, ARXIV170300887; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kohler Jonas Moritz, 2017, ARXIV170505933; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lei L., 2017, ARXIV170609156; Levy K. Y., 2016, ARXIV161104831; MORE JJ, 1979, MATH PROGRAM, V16, P1, DOI 10.1007/BF01582091; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Reddi S. J., 2017, ARXIV170901434; Reddi SJ, 2016, PR MACH LEARN RES, V48; Tripuraneni N., 2017, ARXIV171102838; Xu Peng, 2017, ARXIV170807164; Xu Yi, 2017, ARXIV171101944; Yu Y., 2017, ARXIV171203950; Zeyuan Allen-Zhu, 2017, ARXIV170808694; Zeyuan Allen-Zhu, 2017, ARXIV171106673; Zeyuan Allen-Zhu, 2017, ARXIV170101722; Zhou DR, 2018, PR MACH LEARN RES, V80	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304053
C	Zhang, C; Matsen, FA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Cheng; Matsen, Frederick A.			Generalizing Tree Probability Estimation via Bayesian Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTIPLE GENE LOCI; PHYLOGENETIC INFERENCE; EM ALGORITHM; LIKELIHOOD; PROPOSALS; SEQUENCES; EVOLUTION; MRBAYES	Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete objects do not handle object structure in a satisfactory manner. In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations. We show that efficient algorithms for learning Bayesian networks can be easily extended to probability estimation on this challenging structured space. Experiments on both synthetic and real data show that our methods greatly outperform the current practice of using the empirical distribution, as well as a previous effort for probability estimation on trees.	[Zhang, Cheng; Matsen, Frederick A.] Fred Hutchinson Canc Res Ctr, Computat Biol Program, Seattle, WA 98109 USA	Fred Hutchinson Cancer Center	Zhang, C (corresponding author), Fred Hutchinson Canc Res Ctr, Computat Biol Program, Seattle, WA 98109 USA.	chengz23@fredhutch.org; matsen@fredhutch.org			National Science Foundation [CISE-1564137]; National Institutes of Health [R01-GM113246, U54-GM111274]; Howard Hughes Medical Institute; Simons Foundation	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Howard Hughes Medical Institute(Howard Hughes Medical Institute); Simons Foundation	This work supported by National Science Foundation grant CISE-1564137, as well as National Institutes of Health grants R01-GM113246 and U54-GM111274. The research of Frederick Matsen was supported in part by a Faculty Scholar grant from the Howard Hughes Medical Institute and the Simons Foundation.	Buntine W., 1991, P 7 C UNC ART INT, P52, DOI DOI 10.1016/B978-1-55860-203-8.50010-3; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; DeSalle R, 2004, NAT REV GENET, V5, P702, DOI 10.1038/nrg1425; Felsenstein J., 2004, INFERRING PHYLOGENIE; Friedman N, 2002, J COMPUT BIOL, V9, P331, DOI 10.1089/10665270252935494; Garey JR, 1996, J MOL EVOL, V43, P287, DOI 10.1007/BF02338837; HEDGES SB, 1990, MOL BIOL EVOL, V7, P607; Henk DA, 2003, MYCOLOGIA, V95, P561, DOI 10.2307/3761931; Hohna S, 2012, SYST BIOL, V61, P1, DOI 10.1093/sysbio/syr074; Hothna S, 2014, SYST BIOL, V63, P753, DOI 10.1093/sysbio/syu039; Huelsenbeck JP, 2001, SCIENCE, V294, P2310, DOI 10.1126/science.1065889; Huelsenbeck JP, 2001, BIOINFORMATICS, V17, P754, DOI 10.1093/bioinformatics/17.8.754; JUKES T H, 1969, P21; Lakner C, 2008, SYST BIOL, V57, P86, DOI 10.1080/10635150801886156; Larget B, 2013, SYST BIOL, V62, P501, DOI 10.1093/sysbio/syt014; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mau B, 1999, BIOMETRICS, V55, P1, DOI 10.1111/j.0006-341X.1999.00001.x; Neher RA, 2015, BIOINFORMATICS, V31, P3546, DOI 10.1093/bioinformatics/btv381; Ronquist F, 2012, SYST BIOL, V61, P539, DOI 10.1093/sysbio/sys029; Rossman AY, 2001, MYCOLOGIA, V93, P100, DOI 10.2307/3761609; Strimmer K, 2000, MOL BIOL EVOL, V17, P875, DOI 10.1093/oxfordjournals.molbev.a026367; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Whidden C, 2015, SYST BIOL, V64, P472, DOI 10.1093/sysbio/syv006; Yang ZH, 1997, MOL BIOL EVOL, V14, P717, DOI 10.1093/oxfordjournals.molbev.a025811; Yang ZH, 2003, SYST BIOL, V52, P705, DOI 10.1080/10635150390235557; Yoder AD, 2004, MOL ECOL, V13, P757, DOI 10.1046/j.1365-294X.2004.02106.x; Zhang N, 2001, MYCOLOGIA, V93, P355, DOI 10.2307/3761657	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301043
C	Zhang, LJ; Zhou, ZH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Lijun; Zhou, Zhi-Hua			l(1) -regression with Heavy-tailed Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BOUNDS	In this paper, we consider the problem of linear regression with heavy-tailed distributions. Different from previous studies that use the squared loss to measure the performance, we choose the absolute loss, which is capable of estimating the conditional median. To address the challenge that both the input and output could be heavy-tailed, we propose a truncated minimization problem, and demonstrate that it enjoys an (O) over tilde(root d/n) excess risk, where d is the dimensionality and n is the number of samples. Compared with traditional work on l(1)-regression, the main advantage of our result is that we achieve a high-probability risk bound without exponential moment conditions on the input and output. Furthermore, if the input is bounded, we show that the classical empirical risk minimization is competent for l(1)-regression even when the output is heavy-tailed.	[Zhang, Lijun; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Zhang, LJ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	zhanglj@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn			NSFC [61751306, 2017QNRC001]; Collaborative Innovation Center of Novel Software Technology and Industrialization	NSFC(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Center of Novel Software Technology and Industrialization	This work was partially supported by the NSFC (61751306), YESS (2017QNRC001), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. We thank an anonymous reviewer of COLT 2018 for helping us simplify the proof of Theorem 1.	Alquier P., 2017, ARXIV170201402; Audibert JY, 2011, ANN STAT, V39, P2766, DOI 10.1214/11-AOS918; Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720; Brownlees C, 2015, ANN STAT, V43, P2507, DOI 10.1214/15-AOS1350; Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454; Cucker F, 2002, B AM MATH SOC, V39, P1; Dinh V.C., 2016, ADV NEURAL INFORM PR, P505; Finkenstadt B., 2003, EXTREME VALUES FINAN; Foss S., 2013, INTRO HEAVY TAILED S, V2nd; Gyorfi L., 2002, DISTRIBUTION FREE TH; Hastie T., 2013, ELEMENTS STAT LEARNI, DOI DOI 10.1007/978-0-387-84858-7_16; Hazan E., 2015, ADV NEURAL INFORM PR, P1594; Hsu D, 2016, J MACH LEARN RES, V17; Hsu D, 2014, PR MACH LEARN RES, V32, P37; Lugosi G., 2016, ARXIV160800757; Lugosi G., 2009, TECHNICAL REPORT; Mendelson S., 2014, C LEARN THEOR, P25; Mendelson S, 2015, J ACM, V62, DOI 10.1145/2699439; Peter A. M. L., 1987, ROBUST REGRESSION OU; Pisier G, 1989, CAMBRIDGE TRACTS MAT, V94; Plan Y, 2013, COMMUN PUR APPL MATH, V66, P1275, DOI 10.1002/cpa.21442; Talagrand M., 2005, SPRINGER MG MATH, P222, DOI 10.1007/3-540-27499-5; Vapnik V., 2000, NATURE STAT LEARNING; Zhang L., 2017, COLT, P1954; Zhang L., 2018, ARXIV180500616	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301010
C	Zhang, LS; Rosenblatt, G; Fetaya, E; Liao, RJ; Byrd, WE; Might, M; Urtasun, R; Zemel, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Lisa; Rosenblatt, Gregory; Fetaya, Ethan; Liao, Renjie; Byrd, William E.; Might, Matthew; Urtasun, Raquel; Zemel, Richard			Neural Guided Constraint Logic Programming for Program Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren's internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren . We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.	[Zhang, Lisa; Fetaya, Ethan; Liao, Renjie; Urtasun, Raquel; Zemel, Richard] Univ Toronto, Toronto, ON, Canada; [Zhang, Lisa; Fetaya, Ethan; Liao, Renjie; Urtasun, Raquel; Zemel, Richard] Vector Inst, Hyderabad, India; [Liao, Renjie; Urtasun, Raquel] UBER ATG, Pittsburgh, PA USA; [Rosenblatt, Gregory; Byrd, William E.; Might, Matthew] Univ Alabama Birmingham, Birmingham, AL USA	University of Toronto; University of Alabama System; University of Alabama Birmingham	Zhang, LS (corresponding author), Univ Toronto, Toronto, ON, Canada.; Zhang, LS (corresponding author), Vector Inst, Hyderabad, India.	lczhang@cs.toronto.edu; gregr@uab.edu; ethanf@cs.toronto.edu; rjliao@cs.toronto.edu; webyrd@uab.edu; might@uab.edu; urtasun@cs.toronto.edu; zemel@cs.toronto.edu		Byrd, William/0000-0003-4730-5293	Natural Sciences and Engineering Research Council of Canada; National Center For Advancing Translational Sciences of the National Institutes of Health [OT2TR002517]; Connaught International Scholarship	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); National Center For Advancing Translational Sciences of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Advancing Translational Sciences (NCATS)); Connaught International Scholarship	Research reported in this publication was supported in part by the Natural Sciences and Engineering Research Council of Canada, and the National Center For Advancing Translational Sciences of the National Institutes of Health under Award Number OT2TR002517. R.L. was supported by Connaught International Scholarship. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.	Albarghouthi A., 2013, LNCS, V8044, P934; Allamanis M., 2018, INT C LEARN REPR; [Anonymous], 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2016, ADV NEURAL INFORM PR; BALOG M, 2017, P 5 INT C LEARN REPR; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bhupatiraju Surya, 2017, ARXIV170404327; BIERMANN AW, 1978, IEEE T SYST MAN CYB, V8, P585, DOI 10.1109/TSMC.1978.4310035; Byrd WE, 2017, P ACM PROGRAM LANG, V1, DOI 10.1145/3110252; Byrd William E., 2012, P 2012 ANN WORKSH SC, P8, DOI DOI 10.1145/2661103.2661105; Byrd William E., 2006, P 2006 SCHEM FUNCT P, P105; Chen X., 2018, P 32 C NEUR INF PROC; Devlin J, 2017, PR MACH LEARN RES, V70; Feser JK, 2015, ACM SIGPLAN NOTICES, V50, P229, DOI [10.1145/2737924.2737977, 10.1145/2813885.2737977]; Graves A., 2014, ARXIV14105401; Gulwani S, 2011, ACM SIGPLAN NOTICES, V46, P317, DOI 10.1145/1925844.1926423; Hemann Jason, 2013, SCHEM FUNCT PROGR WO; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kalyan Ashwin, 2018, INT C LEARN REPR; Li Yujia, 2016, P INT C LEARN REPR I, P2; Neelakantan A., 2016, INT C LEARN REPR; Osera PM, 2015, ACM SIGPLAN NOTICES, V50, P619, DOI [10.1145/2737924.2738007, 10.1145/2813885.2738007]; Parisotto Emilio, 2017, INT C LEARN REPR; Reed S., 2016, INT C LEARN REPR; Rocktaschel T, 2017, ADV NEUR IN, V30; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schaul T., 2016, ABS151105952 CORR; Selsam D., 2018, ARXIV180203685; SUMMERS PD, 1977, J ACM, V24, P161, DOI 10.1145/321992.322002; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301070
C	Zhang, YZ; Galley, M; Gao, JF; Gan, Z; Li, XJ; Brockett, C; Dolan, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Yizhe; Galley, Michel; Gao, Jianfeng; Gan, Zhe; Li, Xiujun; Brockett, Chris; Dolan, Bill			Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Responses generated by neural conversational models tend to lack informativeness and diversity. We present a novel adversarial learning method, called Adversarial Information Maximization (AIM) model, to address these two related but distinct problems. To foster response diversity, we leverage adversarial training that allows distributional matching of synthetic and real responses. To improve informativeness, we explicitly optimize a variational lower bound on pairwise mutual information between query and response. Empirical results from automatic and human evaluations demonstrate that our methods significantly boost informativeness and diversity.	[Zhang, Yizhe; Galley, Michel; Gao, Jianfeng; Gan, Zhe; Li, Xiujun; Brockett, Chris; Dolan, Bill] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Zhang, YZ (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	yizzhang@microsoft.com; mgalley@microsoft.com; jfgao@microsoft.com; zhgan@microsoft.com; xiul@microsoft.com; chrisbkt@microsoft.com; billdol@microsoft.com	li, xiu/GXV-1745-2022					[Anonymous], 2017, ICCV; Ba J., 2017, P 3 INT C LEARN REPR; Bahdanau Dzmitry, 2017, ICLR; Barber D., 2003, NIPS; Bottou L., 2017, ICML; Chen X, 2016, ADV NEUR IN, V29; Dai B., 2017, ICCV; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Forgues G., 2014, NIPS MOD MACH LEARN; Gan Zhe, 2017, NIPS; Gan Zhe, 2017, EMNLP; Gao JF, 2018, ACM/SIGIR PROCEEDINGS 2018, P1371, DOI 10.1145/3209978.3210183; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo J., 2018, AAAI; Huang P.- S, 2013, CIKM; Kim T, 2017, ICML; Lamb A. M., 2016, NIPS; Li C., 2017, NIPS; Li J., 2016, NAACL; Li J., 2017, EMNLP; Li J., 2016, EMNLP; Lin C, 2004, ACL WORKSH; Mitchell J., 2008, ACL; Mnih A., 2014, ICML; Papineni K., 2002, P ANN M ASS COMP LIN; Pu Yunchen, 2018, ICML; Ren Z., 2017, CVPR; Rus Vasile, 2012, P 7 WORKSH BUILD ED; Serban I. V., 2017, AAAI; Serban I. V., 2016, AAAI; Shang L., 2015, ACL; Shen Dinghan, 2018, AAAI; Silver D., 2014, ICML; Sordoni Alessandro, 2016, NAACL; Vinyals O., 2015, ICML DEEP LEARN WORK; Wang G., 2018, ACL; Wang W., 2018, AAAI; Williams R. J., 1992, MACHINE LEARNING; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xia Y., 2017, ICML; Xu J, 2018, EMNLP; Xu Z, 2017, EMNLP; Yang Zhen, 2018, NAACL; Yu L., 2017, AAAI; Zhang Y., 2017, ADV NEURAL INFORM PR; Zhang Y., 2017, ICML	46	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301077
C	Zhang, YQ; Kuo, HW; Wright, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Yuqian; Kuo, Han-Wen; Wright, John			Structured Local Minima in Sparse Blind Deconvolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	Blind deconvolution is a ubiquitous problem of recovering two unknown signals from their convolution. Unfortunately, this is an ill-posed problem in general. This paper focuses on the short and sparse blind deconvolution problem, where the one unknown signal is short and the other one is sparsely and randomly supported. This variant captures the structure of the unknown signals in several important applications. We assume the short signal to have unit l(2) norm and cast the blind deconvolution problem as a nonconvex optimization problem over the sphere. We demonstrate that (i) in a certain region of the sphere, every local optimum is close to some shift truncation of the ground truth, and (ii) for a generic short signal of length k, when the sparsity of activation signal theta less than or similar to k(-2/3) and number of measurements m greater than or similar to poly (k), a simple initialization method together with a descent algorithm which escapes strict saddle points recovers a near shift truncation of the ground truth kernel.	[Zhang, Yuqian] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA; Columbia Univ, Data Sci Inst, New York, NY 10027 USA	Columbia University; Columbia University	Zhang, YQ (corresponding author), Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.	yz2409@columbia.edu; hk2673@columbia.edu; jw2966@columbia.edu			NSF [1343282, CCF 1527809, CCF 1740833, IIS 1546411]	NSF(National Science Foundation (NSF))	The authors gratefully acknowledge support from NSF 1343282, NSF CCF 1527809, NSF CCF 1740833, and NSF IIS 1546411.	Absil PA, 2007, FOUND COMPUT MATH, V7, P303, DOI 10.1007/s10208-005-0179-9; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Ahmed  A., 2012, 12115608 ARXIV; [Anonymous], 2011, P ADV NEURAL PROCESS; Benichoux  A., 2013, 38 INT C AC SPEECH S; Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187; Cheung  Sky, 2017, FOURIER TRANSF UNPUB; Chi YJ, 2016, IEEE J-STSP, V10, P782, DOI 10.1109/JSTSP.2016.2543462; David Wipf, 2013, 13052362 ARXIV; Goldfarb D, 2009, SIAM J IMAGING SCI, V2, P84, DOI 10.1137/080726926; Hopkins SB, 2016, ACM S THEORY COMPUT, P178, DOI 10.1145/2897518.2897529; Jin C, 2017, PR MACH LEARN RES, V70; Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268; Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148; Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001; Li  Xiaodong, 2016, PREPRINT; Ling SY, 2017, IEEE T INFORM THEORY, V63, P4497, DOI 10.1109/TIT.2017.2701342; Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002; Perrone D, 2014, PROC CVPR IEEE, P2909, DOI 10.1109/CVPR.2014.372; Qu  Qing, 2016, IEEE T INFORM THEORY; Spielman  Daniel, 2012, PREPRINT; Sun J, 2015, 2015 INTERNATIONAL CONFERENCE ON SAMPLING THEORY AND APPLICATIONS (SAMPTA), P407, DOI 10.1109/SAMPTA.2015.7148922; Wang LM, 2016, IEEE SIGNAL PROC LET, V23, P1384, DOI 10.1109/LSP.2016.2599104; Xu  Peng, 2017, ARXIV170807827; Zhang HC, 2013, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2013.140; Zhang Yuting, 2017, CVPR	26	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302034
C	Zhao, H; Du, L; Buntine, W; Zhou, MY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhao, He; Du, Lan; Buntine, Wray; Zhou, Mingyuan			Dirichlet belief networks for topic structure learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recently, considerable research effort has been devoted to developing deep architectures for topic models to learn topic structures. Although several deep models have been proposed to learn better topic proportions of documents, how to leverage the benefits of deep structures for learning word distributions of topics has not yet been rigorously studied. Here we propose a new multi-layer generative process on word distributions of topics, where each layer consists of a set of topics and each topic is drawn from a mixture of the topics of the layer above. As the topics in all layers can be directly interpreted by words, the proposed model is able to discover interpretable topic hierarchies. As a self-contained module, our model can be flexibly adapted to different kinds of topic models to improve their modelling accuracy and interpretability. Extensive experiments on text corpora demonstrate the advantages of the proposed model.	[Zhao, He; Du, Lan; Buntine, Wray] Monash Univ, Fac Informat Technol, Clayton, Vic, Australia; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	Monash University; University of Texas System; University of Texas Austin	Du, L (corresponding author), Monash Univ, Fac Informat Technol, Clayton, Vic, Australia.; Zhou, MY (corresponding author), Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.		Du, Lan/AAY-1249-2021; Zhao, He/GON-4192-2022; Zhou, Mingyuan/AAE-8717-2021	Zhao, He/0000-0003-0894-2265; 	U.S. National Science Foundation [IIS-1812699]	U.S. National Science Foundation(National Science Foundation (NSF))	M. Zhou acknowledges the support of Award IIS-1812699 from the U.S. National Science Foundation.	Ahmed A., 2013, P 30 INT C MACH LEAR, P1426; Aletras Nikolaos, 2013, P 10 INT C COMP SEM, P13; Archambeau C, 2015, IEEE T PATTERN ANAL, V37, P321, DOI 10.1109/TPAMI.2014.2313122; Blei DM, 2010, J ACM, V57, DOI 10.1145/1667053.1667056; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blunsom, 2017, P 34 INT C MACH LEAR, V70, P2410; Buntine WL, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P881, DOI 10.1145/2623330.2623691; Chen CY, 2015, IEEE T PATTERN ANAL, V37, P230, DOI 10.1109/TPAMI.2014.2313127; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Cong Y., 2017, P 34 INT C MACH LEAR, P864; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; Downey D., 2015, P C EMP METH NAT LAN, P308, DOI DOI 10.1016/J.JNEUMETH.2010.12.002; Du Lan, 2012, P 2012 JOINT C EMP M, P535; Gan Z, 2015, PR MACH LEARN RES, V37, P1823; Guhaniyogi R., 2018, J COMPUTATIONAL GRAP; Henao R., 2015, NIPS, P2800; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Kim J. H., 2012, P 21 ACM INT C INF K, P783; Lafferty JohnD., 2006, ADV NEURAL INFORM PR, P147; Larochelle H., 2012, ADV NEURAL INFORM PR, P2708; Lau J. H., 2014, P 14 C EUR CHAPT ASS, P530, DOI DOI 10.3115/V1/E14-1056; Li W., 2006, P 23 INT C MACH LEAR, P577, DOI DOI 10.1145/1143844.1143917; Lindsey Robert, 2012, P 2012 JOINT C EMP M, P214; Mcauliffe Jon D., 2008, P ADV NEURAL INFORM, P121; Paisley J, 2015, IEEE T PATTERN ANAL, V37, P256, DOI 10.1109/TPAMI.2014.2318728; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Rosen-Zvi Michal, 2012, ARXIV12074169; Salakhutdinov R, 2009, ADV NEURAL INFORM PR; Sato I., 2010, P 16 ACM SIGKDD INT, DOI DOI 10.1145/1835804.1835890; Srivastava A., 2017, AUTOENCODING VARIATI; Srivastava N., 2013, P 20 9 C UNCERTAINTY, P616; Tang Yichuan, 2013, ADV NEURAL INFORM PR, P530; Teh Y. W., 2012, J AM STAT ASSOC, V101, P1566; Wallach H., 2009, ADV NEURAL INFORM PR, V22, P1973, DOI DOI 10.1007/S10708-008-9161-9; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wood F., 2009, INT C ART INT STAT, V5, P607; Zhang H., 2018, WHAI WEIBULL HYBRID; Zhao H., 2017, ACML, V77, P423; Zhao H., 2018, ICML, P5887; Zhao H., 2018, KAIS, P1; Zhao H, 2017, PR MACH LEARN RES, V70; Zhao H, 2017, IEEE DATA MINING, P635, DOI 10.1109/ICDM.2017.73; Zhou M., 2018, BAYESIAN ANAL; Zhou M, 2016, ACSR ADV COMPUT, V44, P1; Zhou MY, 2015, ADV NEUR IN, V28; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211; Zhou Mingyuan, 2012, AISTATS, P1462	49	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002050
C	Zhao, SY; Zhang, GD; Li, MW; Li, WJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhao, Shen-Yi; Zhang, Gong-Duo; Li, Ming-Wei; Li, Wu-Jun			Proximal SCOPE for Distributed Sparse Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SHRINKAGE; SELECTION; PARALLEL	Distributed sparse learning with a cluster of multiple machines has attracted much attention in machine learning, especially for large-scale applications with high-dimensional data. One popular way to implement sparse learning is to use L-1 regularization. In this paper, we propose a novel method, called proximal SCOPE (pSCOPE), for distributed sparse learning with L(1 )regularization. pSCOPE is based on a cooperative autonomous local learning (CALL) framework. In the CALL framework of pSCOPE, we find that the data partition affects the convergence of the learning procedure, and subsequently we define a metric to measure the goodness of a data partition. Based on the defined metric, we theoretically prove that pSCOPE is convergent with a linear convergence rate if the data partition is good enough. We also prove that better data partition implies faster convergence rate. Furthermore, pSCOPE is also communication efficient. Experimental results on real data sets show that pSCOPE can outperform other state-of-the-art distributed methods for sparse learning.	[Zhao, Shen-Yi; Zhang, Gong-Duo; Li, Ming-Wei; Li, Wu-Jun] Nanjing Univ, Dept Comp Sci & Tech, Natl Key Lab Novel Software Tech, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Zhao, SY (corresponding author), Nanjing Univ, Dept Comp Sci & Tech, Natl Key Lab Novel Software Tech, Nanjing 210023, Jiangsu, Peoples R China.	zhaosy@lamda.nju.edu.cn; zhanggd@lamda.nju.edu.cn; limw@lamda.nju.edu.cn; liwujun@nju.edu.cn			"DengFeng" project of Nanjing University	"DengFeng" project of Nanjing University	This work is partially supported by the "DengFeng" project of Nanjing University.	Ahmed A., 2014, P 11 USENIX C OP SYS, P583; [Anonymous], CORR; Aybat NS, 2015, PR MACH LEARN RES, V37, P2454; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bradley J. K., 2011, P 28 INT C MACH LEAR, P321; Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362; De S, 2016, IEEE DATA MINING, P111, DOI [10.1109/ICDM.2016.0022, 10.1109/ICDM.2016.177]; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Fercoq O, 2016, SIAM REV, V58, P739, DOI 10.1137/16M1085905; Gong PH, 2015, PR MACH LEARN RES, V37, P276; Huo Z., 2016, CORR; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Langford J., 2008, ADV NEURAL INFORM PR, P905; Leblond R, 2017, PR MACH LEARN RES, V54, P46; Lee Jason D., 2017, J MACHINE LEARNING R, V18; Li Y., 2016, ABS161001206 CORR; Mahajan Dhruv, 2017, J MACHINE LEARNING R, V18; Meng Q, 2017, AAAI CONF ARTIF INTE, P2329; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Reddi S. J., 2015, ADV NEURAL INF PROCE, P2647; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; Scherrer C., 2012, P 29 INT C MACH LEAR, P1407; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shi ZQ, 2015, LECT NOTES ARTIF INT, V9284, P691, DOI 10.1007/978-3-319-23528-8_43; Smith Virginia, 2015, CORR; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Wang Jialei, 2017, P MACHINE LEARNING R, V54, P1150; Wu TT, 2008, ANN APPL STAT, V2, P224, DOI 10.1214/07-AOAS147; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xing EP, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1335, DOI 10.1145/2783258.2783323; Zhao SY, 2017, AAAI CONF ARTIF INTE, P2928; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	36	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001012
C	Zheng, ZH; Hong, PY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zheng, Zhihao; Hong, Pengyu			Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GAME; GO	It has been shown that deep neural network (DNN) based classifiers are vulnerable to human-imperceptive adversarial perturbations which can cause DNN classifiers to output wrong predictions with high confidence. We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers. Our approach tries to capture the intrinsic properties of a DNN classifier and uses them to detect adversarial inputs. The intrinsic properties used in this study are the output distributions of the hidden neurons in a DNN classifier presented with natural images. Our approach can be easily applied to any DNN classifiers or combined with other defense strategies to improve robustness. Experimental results show that our approach demonstrates state-of-the-art robustness in defending black-box and gray-box attacks.	[Zheng, Zhihao; Hong, Pengyu] Brandeis Univ, Dept Comp Sci, Waltham, MA 02453 USA	Brandeis University	Zheng, ZH (corresponding author), Brandeis Univ, Dept Comp Sci, Waltham, MA 02453 USA.	zhihaozh@brandeis.edu; hongpeng@brandeis.edu	Hong, Pengyu/AAE-6736-2020					Ackerman E, 2017, IEEE SPECTRUM; [Anonymous], 2016, ARXIV160704311; Bojarski M., 2016, ARXIV160407316; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Goodfellow I. J., 2014, ARXIV14126572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A, 2016, INT C LEARN REPR SAN; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Lu J., 2017, ABS170400103 CORR; McLachlan G., 2007, EM ALGORITHM EXTENSI, V382; Metzen J.H., 2017, ICLR; Miyato T., 2015, ARXIV PREPRINT ARXIV; Mnih V., 2013, PLAYING ATARI DEEP R, P1; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moosavi Dezfooli S. M., 2016, P 2016 IEEE C COMP V; MoosaviDezfooli S. M., 2017, UNIVERSAL ADVERSARIA; Papernot N., 2016, PRACTICAL BLACK BOX; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Samangouei P., 2018, INT C LEARN REPR, V9; Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI 10.1146/annurev-bioeng-071516-044442; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simonyan K., 2015, ICLR; Su J., 2017, ARXIV171008864; Sun Y, 2015, ARXIV150200873; Szegedy C, 2013, 2 INT C LEARNING REP; Xiao H., 2017, ARXIV170807747; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]	36	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002046
C	Zhou, S; Gupta, S; Udell, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Song; Gupta, Swati; Udell, Madeleine			Limited memory Kelley's Method Converges for Composite Convex and Submodular Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SIMPLICIAL DECOMPOSITION; BUNDLE METHODS; OPTIMIZATION	The original simplicial method (OSM), a variant of the classic Kelley's cutting plane method, has been shown to converge to the minimizer of a composite convex and submodular objective, though no rate of convergence for this method was known. Moreover, OSM is required to solve subproblems in each iteration whose size grows linearly in the number of iterations. We propose a limited memory version of Kelley's method (L-KM) and of OSM that requires limited memory (at most n+1 constraints for an n-dimensional problem) independent of the iteration. We prove convergence for L-KM when the convex part of the objective (g) is strongly convex and show it converges linearly when g is also smooth. Our analysis relies on duality between minimization of the composite objective and minimization of a convex function over the corresponding submodular base polytope. We introduce a limited memory version, L-FCFW, of the Fully-Corrective Frank-Wolfe (FCFW) method with approximate correction, to solve the dual problem. We show that L-FCFW and L-KM are dual algorithms that produce the same sequence of iterates; hence both converge linearly (when g is smooth and strongly convex) and with limited memory. We propose L-KM to minimize composite convex and submodular objectives; however, our results on L-FCFW hold for general polytopes and may be of independent interest.	[Zhou, Song; Udell, Madeleine] Cornell Univ, Ithaca, NY 14853 USA; [Gupta, Swati] Georgia Inst Technol, Atlanta, GA 30332 USA	Cornell University; University System of Georgia; Georgia Institute of Technology	Zhou, S (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	sz557@cornell.edu; swatig@gatech.edu; udell@cornell.edu	Gupta, Swati/AAV-6851-2021	Gupta, Swati/0000-0002-9566-3856; Udell, Madeleine/0000-0002-3985-915X	DARPA [FA8750-17-2-0101]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported in part by DARPA Award FA8750-17-2-0101. A part of this work was done while the first author was at the Department of Mathematical Sciences, Tsinghua University and while the second author was visiting the Simons Institute, UC Berkeley. The authors would also like to thank Sebastian Pokutta for invaluable discussions on the Frank-Wolfe algorithm and its variants.	[Anonymous], 1959, NUMER MATH; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Bach F., 2010, ADV NEURAL INFORM PR, P118; Bach F, 2015, SIAM J OPTIMIZ, V25, P115, DOI 10.1137/130941961; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Boyd S., 2009, CONVEX OPTIMIZATION; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; Drori Y, 2016, MATH PROGRAM, V160, P321, DOI 10.1007/s10107-016-0985-7; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; HEARN DW, 1987, MATH PROGRAM STUD, V31, P99; KELLEY JE, 1960, J SOC IND APPL MATH, V8, P703, DOI 10.1137/0108053; Kivinen J., 2010, COLT; KIWIEL KC, 1995, MATH PROGRAM, V69, P89, DOI 10.1007/BF01585554; Krichene W, 2015, 2015 EUROPEAN CONTROL CONFERENCE (ECC), P569, DOI 10.1109/ECC.2015.7330604; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; LEMARECHAL C, 1995, MATH PROGRAM, V69, P111, DOI 10.1007/BF01585555; Lovsz L., 1983, MATH PROGRAMMING STA; Makela MM, 2002, OPTIM METHOD SOFTW, V17, P1, DOI 10.1080/10556780290027828; Nagano K., 2011, P 28 INT C INT C MAC, P977; Ryu EK, 2016, APPL COMPUT MATH-BAK, V15, P3; Udell M, 2016, COMPUT OPTIM APPL, V64, P355, DOI 10.1007/s10589-015-9819-4; Vershynin R., 2018, HIGH DIMENSIONAL PRO, V47; VONHOHENBALKEN B, 1977, MATH PROGRAM, V13, P49, DOI 10.1007/BF01584323; Warmuth Manfred K., 2006, ADV NEURAL INFORM PR, P1481; Yasutake S, 2011, LECT NOTES COMPUT SC, V7074, P534, DOI 10.1007/978-3-642-25591-5_55	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304043
C	Zhu, GX; Huang, ZA; Zhang, CJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhu, Guangxiang; Huang, Zhiao; Zhang, Chongjie			Object-Oriented Dynamics Predictor	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PERMANENCE	Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.	[Zhu, Guangxiang; Huang, Zhiao; Zhang, Chongjie] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China	Tsinghua University	Zhu, GX (corresponding author), Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China.	guangxiangzhu@outlook.com; hza14@mails.tsinghua.edu.cn; chongjie@tsinghua.edu.cn						[Anonymous], 2003, IJCAI; BAILLARGEON R, 1985, COGNITION, V20, P191, DOI 10.1016/0010-0277(85)90008-3; BAILLARGEON R, 1987, DEV PSYCHOL, V23, P655, DOI 10.1037/0012-1649.23.5.655; Battaglia Peter W, 2016, ARXIV161200222; Chang Michael B, 2016, ARXIV161200341; Chiappa S., 2017, RECURRENT ENV SIMULA; Cobo Luis C, 2013, INT C AUT AG MULT SY, P1061; Diuk C., 2008, ICML, DOI [10.1145/1390156.1390187, DOI 10.1145/1390156.1390187]; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kwak S, 2015, IEEE I CONF COMP VIS, P3173, DOI 10.1109/ICCV.2015.363; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Levine Sergey, 2013, ICML; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Lo BPL, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P158, DOI 10.1109/ISIMP.2001.925356; Lotter William, 2015, COMPUTER SCI; Mathieu Michael, 2016, ICLR; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair V, 2010, P 27 INT C MACHINE L, P807; Piaget Jean, 1970, PIAGETS THEORY; Ranzato MarcAurelio, 2014, ARXIV14126604; Santoro A, 2017, ADV NEUR IN, V30; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Spelke Elizabeth S, 2013, PERCEPTUAL DEV INFAN, P209; Springenberg J.T., 2014, ARXIV14126806; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Tasfi N., 2016, PYGAME LEARNING ENV; VanSteenkiste S., 2018, P INT C LEARN REPR P; Vijayanarasimhan Sudheendra, 2017, ARXIV170407804; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Watters Nicholas, 2017, ADV NEURAL INFORM PR, P4540; Wu JJ, 2017, ADV NEUR IN, V30; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957; Zhang Q., 2017, ARXIV171000935, V2, P5; Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808; Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004037
C	Zhu, JQ; Sanborn, AN; Chater, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhu, Jian-Qiao; Sanborn, Adam N.; Chater, Nick			Mental Sampling in Multimodal Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SCALAR EXPECTANCY-THEORY; PROBABILISTIC MODELS; BAYESIAN-INFERENCE; SIMULATION; PATTERNS; NOISE	Both resources in the natural environment and concepts in a semantic space are distributed "patchily", with large gaps in between the patches. To describe people's internal and external foraging behavior, various random walk models have been proposed. In particular, internal foraging has been modeled as sampling: in order to gather relevant information for making a decision, people draw samples from a mental representation using random-walk algorithms such as Markov chain Monte Carlo (MCMC). However, two common empirical observations argue against people using simple sampling algorithms such as MCMC for internal foraging. First, the distance between samples is often best described by a Levy flight distribution: the probability of the distance between two successive locations follows a power-law on the distances. Second, humans and other animals produce long-range, slowly decaying autocorrelations characterized as 1/f-like fluctuations, instead of the 1/f(2) fluctuations produced by random walks. We propose that mental sampling is not done by simple MCMC, but is instead adapted to multimodal representations and is implemented by Metropolis-coupled Markov chain Monte Carlo (MC3), one of the first algorithms developed for sampling from multimodal distributions. MC3 involves running multiple Markov chains in parallel but with target distributions of different temperatures, and it swaps the states of the chains whenever a better location is found. Heated chains more readily traverse valleys in the probability landscape to propose moves to far-away peaks, while the colder chains make the local steps that explore the current peak or patch. We show that MC3 generates distances between successive samples that follow a Levy flight distribution and produce 1/f -like autocorrelations, providing a single mechanistic account of these two puzzling empirical phenomena of internal foraging.	[Zhu, Jian-Qiao; Sanborn, Adam N.] Univ Warwick, Dept Psychol, Coventry, W Midlands, England; [Chater, Nick] Warwick Business Sch, Behav Sci Grp, Coventry, W Midlands, England	University of Warwick; University of Warwick	Zhu, JQ (corresponding author), Univ Warwick, Dept Psychol, Coventry, W Midlands, England.	j.zhu@warwick.ac.uk; a.n.sanborn@warwick.ac.uk; nick.chater@wbs.ac.uk	Chater, Nick/GRX-6742-2022; Sanborn, Adam/G-3715-2010	Sanborn, Adam/0000-0003-0442-4372	China Scholarship Council; Alan Turing Institute under the EPSRC [EP/N510129/1]; ERC [295917-RATIONALITY]; ESRC Network for Integrated Behavioural Science [ES/K002201/1, ES/P008976/1]; Leverhulme Trust [RP2012-V-022]; RCUK Grant [EP/K039830/1]	China Scholarship Council(China Scholarship Council); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ERC(European Research Council (ERC)European Commission); ESRC Network for Integrated Behavioural Science(UK Research & Innovation (UKRI)Economic & Social Research Council (ESRC)); Leverhulme Trust(Leverhulme Trust); RCUK Grant	JQZ was supported by China Scholarship Council. ANS was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. NC was supported by ERC grant 295917-RATIONALITY, the ESRC Network for Integrated Behavioural Science [grant numbers ES/K002201/1 and ES/P008976/1], the Leverhulme Trust [grant number RP2012-V-022], and RCUK Grant EP/K039830/1.	Abbott JT., 2012, P ADV NEURAL INFORM, V25, P3050; Aitchison L, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005186; ANDERSON JR, 1991, PSYCHOL REV, V98, P409, DOI 10.1037/0033-295X.98.3.409; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870; Berkolaiko G, 1996, PHYS REV E, V53, P5774, DOI 10.1103/PhysRevE.53.5774; Bousfield WA, 1944, J GEN PSYCHOL, V30, P149, DOI 10.1080/00221309.1944.10544467; Chater N, 2006, TRENDS COGN SCI, V10, P287, DOI 10.1016/j.tics.2006.05.007; Chater N, 2006, TRENDS COGN SCI, V10, P335, DOI 10.1016/j.tics.2006.05.006; Dasgupta I, 2017, COGNITIVE PSYCHOL, V96, P1, DOI 10.1016/j.cogpsych.2017.05.001; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Gao J B, 2006, Cogn Process, V7, P105, DOI 10.1007/s10339-006-0030-5; Gershman SJ, 2012, NEURAL COMPUT, V24, P1, DOI 10.1162/NECO_a_00226; GEYER CJ, 1991, COMPUTING SCIENCE AND STATISTICS, P156; GIBBON J, 1977, PSYCHOL REV, V84, P279, DOI 10.1037/0033-295X.84.3.279; GILDEN DL, 1995, SCIENCE, V267, P1837, DOI 10.1126/science.7892611; Gilden DL, 1997, PSYCHOL SCI, V8, P296, DOI 10.1111/j.1467-9280.1997.tb00441.x; Gonzalez MC, 2008, NATURE, V453, P779, DOI 10.1038/nature06958; Griffiths TL, 2011, J EXP PSYCHOL GEN, V140, P725, DOI 10.1037/a0024899; Haefner RM, 2016, NEURON, V90, P649, DOI 10.1016/j.neuron.2016.03.020; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Hills TT, 2012, PSYCHOL REV, V119, P431, DOI 10.1037/a0027373; Hoyer PO, 2003, ADV NEURAL INFORM PR, V15, P293; Kello CT, 2010, TRENDS COGN SCI, V14, P223, DOI 10.1016/j.tics.2010.02.005; Kemp C, 2009, PSYCHOL REV, V116, P20, DOI 10.1037/a0014282; Lieder F., 2012, ADV NEURAL INFORM PR, P2690; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Neal RM, 1996, STAT COMPUT, V6, P353, DOI 10.1007/BF00143556; Orban G, 2016, NEURON, V92, P530, DOI 10.1016/j.neuron.2016.09.038; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rakitin BC, 1998, J EXP PSYCHOL-ANIM B, V24, P15, DOI 10.1037/0097-7403.24.1.15; Ramos-Fernandez G, 2004, BEHAV ECOL SOCIOBIOL, V55, P223, DOI 10.1007/s00265-003-0700-6; Rhodes T., 2011, 33 ANN M COGN SCI SO; Rhodes T, 2007, PHYSICA A, V385, P255, DOI 10.1016/j.physa.2007.07.001; Sanborn AN, 2016, TRENDS COGN SCI, V20, P883, DOI 10.1016/j.tics.2016.10.003; Sanborn AN, 2013, PSYCHOL REV, V120, P411, DOI 10.1037/a0031912; Savin C, 2014, ADV NEURAL INF PROCE, P2024; Savin C, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003489; Shlesinger M.F., 1995, LECT NOTE PHYS, V450, P52; Sims DW, 2008, NATURE, V451, P1098, DOI 10.1038/nature06518; SWENDSEN RH, 1986, PHYS REV LETT, V57, P2607, DOI 10.1103/PhysRevLett.57.2607; Troyer AK, 1997, NEUROPSYCHOLOGY, V11, P138, DOI 10.1037/0894-4105.11.1.138; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Orden GC, 2003, J EXP PSYCHOL GEN, V132, P331, DOI 10.1037/0096-3445.132.3.331; Viswanathan GM, 1996, NATURE, V381, P413, DOI 10.1038/381413a0; Viswanathan GM, 1999, NATURE, V401, P911, DOI 10.1038/44831; Vul E, 2014, COGNITIVE SCI, V38, P599, DOI 10.1111/cogs.12101; Wagenmakers EJ, 2004, PSYCHON B REV, V11, P579, DOI 10.3758/BF03196615; Ward L. M., 2002, DYNAMICAL COGNITIVE; Wolpert DM, 2007, HUM MOVEMENT SCI, V26, P511, DOI 10.1016/j.humov.2007.05.005; Xu J., 2009, ADV NEURAL INFORM PR, P1809; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002	54	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000027
C	Ziko, IM; Granger, E; Ben Ayed, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ziko, Imtiaz Masud; Granger, Eric; Ben Ayed, Ismail			Scalable Laplacian K-modes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MEAN-SHIFT; ALGORITHMS	We advocate Laplacian K-modes for joint clustering and density mode finding, and propose a concave-convex relaxation of the problem, which yields a parallel algorithm that scales up to large datasets and high dimensions. We optimize a tight bound (auxiliary function) of our relaxation, which, at each iteration, amounts to computing an independent update for each cluster-assignment variable, with guaranteed convergence. Therefore, our bound optimizer can be trivially distributed for large-scale data sets. Furthermore, we show that the density modes can be obtained as byproducts of the assignment variables via simple maximum-value operations whose additional computational cost is linear in the number of data points. Our formulation does not need storing a full affinity matrix and computing its eigenvalue decomposition, neither does it perform expensive projection steps and Lagrangian-dual inner iterates for the simplex constraints of each point. Furthermore, unlike mean-shift, our density-mode estimation does not require inner-loop gradient-ascent iterates. It has a complexity independent of feature-space dimension, yields modes that are valid data points in the input set and is applicable to discrete domains as well as arbitrary kernels. We report comprehensive experiments over various data sets, which show that our algorithm yields very competitive performances in term of optimization quality (i.e., the value of the discrete-variable objective at convergence) and clustering accuracy.	[Ziko, Imtiaz Masud; Granger, Eric; Ben Ayed, Ismail] ETS Montreal, Montreal, PQ, Canada	University of Quebec; Ecole de Technologie Superieure - Canada	Ziko, IM (corresponding author), ETS Montreal, Montreal, PQ, Canada.	imtiaz-masud.ziko.1@etsmtl.ca						Belkin M, 2006, J MACH LEARN RES, V7, P2399; Ben Salah M, 2014, IEEE T IMAGE PROCESS, V23, P1143, DOI 10.1109/TIP.2013.2297019; Bengio Y, 2004, ADV NEUR IN, V16, P177; Carreira-Perpinan M. A., 2013, ARXIV13046478; Carreira-Perpinan M. A., 2015, ARXIV150300687; Carreira-Perpinan MA, 2007, IEEE T PATTERN ANAL, V29, P767, DOI 10.1109/TPAMI.2007.1057; Chen C., 2014, P 27 C ADV NEUR INF, P1323; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; diaeresis>ahenb <spacing diaeresis>uhl Philipp Kr<spacing, 2013, P 30 INT C MACH LEAR, P513; Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612; Gong YC, 2015, PROC CVPR IEEE, P19, DOI 10.1109/CVPR.2015.7298596; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li J, 2007, J MACH LEARN RES, V8, P1687; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Narasimhan M., 2005, UAI, P404; Newling J., 2016, ADV NEURAL INFORM PR, V29, P1352; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Shaham Uri, 2018, ICLR; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Tang M., 2018, INT J COMPUT VISION, P1; Tian F, 2014, AAAI CONF ARTIF INTE, P1293; Vladymyrov M, 2016, PR MACH LEARN RES, V48; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang W., 2014, ARXIV14063895; Wolf L., 2011, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2011.5995566; Yuan J, 2017, LECT NOTES COMPUT SC, V10302, P524, DOI 10.1007/978-3-319-58771-4_42; Yuille A. L., 2001, P ADV NEUR INF PROC, P1033; Zhang ZH, 2007, MACH LEARN, V69, P1, DOI 10.1007/s10994-007-5022-x	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004058
C	Zimmert, J; Seldin, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zimmert, Julian; Seldin, Yevgeny			Factored Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce the factored bandits model, which is a framework for learning with limited (bandit) feedback, where actions can be decomposed into a Cartesian product of atomic actions. Factored bandits incorporate rank-1 bandits as a special case, but significantly relax the assumptions on the form of the reward function. We provide an anytime algorithm for stochastic factored bandits and up to constants matching upper and lower regret bounds for the problem. Furthermore, we show how a slight modification enables the proposed algorithm to be applied to utility-based dueling bandits. We obtain an improvement in the additive terms of the regret bound compared to state-of-the-art algorithms (the additive terms are dominating up to time horizons that are exponential in the number of arms).	[Zimmert, Julian; Seldin, Yevgeny] Univ Copenhagen, Copenhagen, Denmark	University of Copenhagen	Zimmert, J (corresponding author), Univ Copenhagen, Copenhagen, Denmark.	zimmert@di.ku.dk; seldin@di.ku.dk	Seldin, Yevgeny/G-8955-2015	Seldin, Yevgeny/0000-0003-3152-4635				Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Ailon N, 2014, PR MACH LEARN RES, V32, P856; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen W., 2016, JMLR, V17, P1746; Combes R., 2017, ADV NEURAL INFORM PR, P1761; Filippi S., 2010, NIPS, P586; Katariya S., 2017, INT JOINT C ART INT; Katariya S, 2017, PR MACH LEARN RES, V54, P392; Komiyama Junpei, 2015, C LEARN THEOR, V40, P1141; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore T, 2017, PR MACH LEARN RES, V54, P728; Urvoy T., 2013, INT C MACH LEARN, V28, P91; WU H, 2016, ADV NEURAL INFORM PR, P649; Yue Y., 2009, ICML, P1201; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zoghi M, 2014, PR MACH LEARN RES, V32, P10	17	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302082
C	Abbasi-Yadkori, Y; Bartlett, PL; Gabillon, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Abbasi-Yadkori, Yasin; Bartlett, Peter L.; Gabillon, Victor			Near Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study minimax strategies for the online prediction problem with expert advice. It has been conjectured that a simple adversary strategy, called COMB, is near optimal in this game for any number of experts. Our results and new insights make progress in this direction by showing that, up to a small additive term, COMB is minimax optimal in the finite-time three expert problem. In addition, we provide for this setting a new near minimax optimal COMB-based learner. Prior to this work, in this problem, learners obtaining the optimal multiplicative constant in their regret rate were known only when K = 2 or K -> infinity We characterize, when K = 3, the regret of the game scaling as root 8/(9 pi)T +/- log(T)(2) which gives for the first time the optimal constant in the leading (root T) term of the regret.	[Abbasi-Yadkori, Yasin] Adobe Res, San Jose, CA 95110 USA; [Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA USA; [Gabillon, Victor] Queensland Univ Technol, Brisbane, Qld, Australia	Adobe Systems Inc.; University of California System; University of California Berkeley; Queensland University of Technology (QUT)	Abbasi-Yadkori, Y (corresponding author), Adobe Res, San Jose, CA 95110 USA.		Jeong, Yongwook/N-7413-2016		NSF [IIS-1619362]; Australian Research Council through an Australian Laureate Fellowship [FL110100281]; Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS)	NSF(National Science Foundation (NSF)); Australian Research Council through an Australian Laureate Fellowship(Australian Research Council); Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS)(Australian Research Council)	We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS). We would like to thank Nate Eldredge for pointing us to the results in [18] and Wouter Koolen for pointing us at [19]!	Abernethy J, 2008, PROC 21 ANN C LEARN, P437; Abernethy Jacob D, 2010, ADV NEURAL INFORM PR, V23, P1; [Anonymous], 2008, INTRO PROBABILITY TH; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; Cover T. M, 1965, T PRAG C INF THEOR S, P263; Drenska Nadeja, PDE APPROACH MIXED S; Gravin N, 2016, PROCEED INGS 27 ANN, P528; Gravin Nick, 2014, ARXIV160304981; Gravin Nick, 2017, 44 INT C AUTOMATA LA; Grinstead C.M., 2012, INTRO PROBABILITY; Howard Ronald A., 1960, DYNAMIC PROGRAMMING; Luo Haipeng, 2014, JMLR, P226; Orabona Francesco, 2015, ARXIV151102176; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Stanica P., 2001, J INEQUAL PURE APPL, V2, P30; van der Hofstad R, 2008, AM MATH MON, V115, P753, DOI 10.1080/00029890.2008.11920588; Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556	19	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403010
C	Abbe, E; Kulkarni, S; Lee, EJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Abbe, Emmanuel; Kulkarni, Sanjeev; Lee, Eun Jee			Nonbacktracking Bounds on the Influence in Independent Cascade Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper develops upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit nonbacktracking walks, Fortuin-Kasteleyn-Ginibre type inequalities, and are computed by message passing algorithms. Nonbacktracking walks have recently allowed for headways in community detection, and this paper shows that their use can also impact the influence computation. Further, we provide parameterized versions of the bounds that control the trade-off between the efficiency and the accuracy. Finally, the tightness of the bounds is illustrated with simulations on various network models.	[Abbe, Emmanuel; Lee, Eun Jee] Princeton Univ, Program Appl & Computat Math, Princeton, NJ 08544 USA; [Abbe, Emmanuel; Kulkarni, Sanjeev] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA	Princeton University; Princeton University	Abbe, E (corresponding author), Princeton Univ, Program Appl & Computat Math, Princeton, NJ 08544 USA.; Abbe, E (corresponding author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.	eabbe@princeton.edu; kulkarni@princeton.edu; ejlee@princeton.edu	Jeong, Yongwook/N-7413-2016		NSF CAREER Award [CCF-1552131]; ARO [W911NF-16-1-0051]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); ARO	The authors thank Colin Sandon for helpful discussions. This research was partly supported by the NSF CAREER Award CCF-1552131 and the ARO grant W911NF-16-1-0051	Abbe E., 2015, ARXIV151209080; Bordenave C, 2015, ANN IEEE SYMP FOUND, P1347, DOI 10.1109/FOCS.2015.86; Chen W, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P967, DOI 10.1145/3038912.3052608; Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047; Draief M., 2006, PROC 1 INT C PERFORM, P51; Eun Jee Lee, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P649, DOI 10.1109/CISS.2016.7460579; FORTUIN CM, 1971, COMMUN MATH PHYS, V22, P89, DOI 10.1007/BF01651330; Goyal A., 2011, PROC 20 INT C COMPAN, P47, DOI DOI 10.1145/1963192.1963217; GRANOVETTER M, 1978, AM J SOCIOL, V83, P1420, DOI 10.1086/226707; Jiang CR, 2010, INT ASIA CONF INFORM, P88, DOI 10.1109/CAR.2010.5456772; Karrer B, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.208702; Karrer B, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.016101; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Khelil A, 2002, P 5 ACM INT WORKSH M, P54, DOI DOI 10.1145/570758.570768; Khim J. T., 2016, ADV NEURAL INFORM PR, P4538; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Lemonnier R, 2014, ADV NEUR IN, V27; Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Lopez-Pintado D, 2008, RATION SOC, V20, P399, DOI 10.1177/1043463108096787; Shulgin B, 1998, B MATH BIOL, V60, P1123, DOI 10.1016/S0092-8240(98)90005-2; Tang YZ, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P75, DOI 10.1145/2588555.2593670; Wang C, 2012, DATA MIN KNOWL DISC, V25, P545, DOI 10.1007/s10618-012-0262-1; Watts DJ, 2002, P NATL ACAD SCI USA, V99, P5766, DOI 10.1073/pnas.082090499; Yang J., 2010, PREDICTING SPEED SCA	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401043
C	Alber, M; Kindermans, PJ; Schutt, KT; Muller, KR; Sha, F		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alber, Maximilian; Kindermans, Pieter-Jan; Schuett, Kristof T.; Mueller, Klaus-Robert; Sha, Fei			An Empirical Study on The Properties of Random Bases for Kernel Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NETWORKS	Kernel machines as well as neural networks possess universal function approximation properties. Nevertheless in practice their ways of choosing the appropriate function class differ. Specifically neural networks learn a representation by adapting their basis functions to the data and the task at hand, while kernel methods typically use a basis that is not adapted during training. In this work, we contrast random features of approximated kernel machines with learned features of neural networks. Our analysis reveals how these random and adaptive basis functions affect the quality of learning. Furthermore, we present basis adaptation schemes that allow for a more compact representation, while retaining the generalization properties of kernel machines.	[Alber, Maximilian; Kindermans, Pieter-Jan; Schuett, Kristof T.; Mueller, Klaus-Robert] Tech Univ Berlin, Berlin, Germany; [Mueller, Klaus-Robert] Korea Univ, Seoul, South Korea; [Mueller, Klaus-Robert] Max Planck Inst Informat, Saarbrucken, Germany; [Sha, Fei] Univ Southern Calif, Los Angeles, CA USA	Technical University of Berlin; Korea University; Max Planck Society; University of Southern California	Alber, M (corresponding author), Tech Univ Berlin, Berlin, Germany.	maximilian.alber@tu-berlin.de; feisha@usc.edu	Mueller, Klaus-Robert/Y-3547-2019; Schütt, Kristof T/Q-2604-2017; Jeong, Yongwook/N-7413-2016	Mueller, Klaus-Robert/0000-0002-3861-7685; Schütt, Kristof T/0000-0001-8342-0964; 	Federal Ministry of Education and Research (BMBF) [01IS14013A]; European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant [657679]; Institute for Information & Communications Technology Promotion (IITP) - Korea government [2017-0-00451]; BK21; DFG; NSF [IIS-1065243, 1451412, 1513966/1632803, 1208500, CCF-1139148]; Google Research Award; ARO [W911NF-12-1-0241, W911NF-15-1-0484]; Alfred. P. Sloan Research Fellowship	Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant; Institute for Information & Communications Technology Promotion (IITP) - Korea government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); BK21(Ministry of Education & Human Resources Development (MOEHRD), Republic of Korea); DFG(German Research Foundation (DFG)); NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated); ARO; Alfred. P. Sloan Research Fellowship(Alfred P. Sloan Foundation)	MA, KS, KRM, and FS acknowledge support by the Federal Ministry of Education and Research (BMBF) under 01IS14013A. PJK has received funding from the European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement NO 657679. KRM further acknowledges partial funding by the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (No. 2017-0-00451), BK21 and by DFG. FS is partially supported by NSF IIS-1065243, 1451412, 1513966/1632803, 1208500, CCF-1139148, a Google Research Award, an Alfred. P. Sloan Research Fellowship and ARO#W911NF-12-1-0241 and W911NF-15-1-0484. This work was supported by NVIDIA with a hardware donation.	Ben-Hur A., 2005, ADV NEURAL INFORM PR, V17, P545; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Burges, 1998, MNIST DATABASE HANDW; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Chollet F., 2015, KERAS; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; CRISTIANINI N, 2001, ADV NEURAL INFORM PR; Dai B., 2014, NIPS; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Feng C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3490; FREY PW, 1991, MACH LEARN, V6, P161, DOI 10.1023/A:1022606404104; Glorot X., 2011, P 14 INT C ART INT S, P315; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Jones Eric, 2014, SCPY OPEN SOURCE SCI; Kingma D.P, P 3 INT C LEARNING R; Kohavi R., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P202; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Le Q., 2013, ICML; Lu Z, 2014, ARXIV14114000; Montavon G, 2011, J MACH LEARN RES, V12, P2563; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281; Muller KR, 1999, ADVANCES IN KERNEL METHODS, P243; Nair V, 2010, P 27 INT C MACHINE L, P807; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Sutherland Dougal J, 2015, ERROR RANDOM FOURIER; van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37; Wilson A.G., 2015, ARXIV151102222; Yang T., 2012, ADV NEURAL INFORM PR, P476; Yang ZC, 2015, JMLR WORKSH CONF PRO, V38, P1098; Yang Zichao, 2015, DEEP FRIED CONVNETS; Yu F. X., 2015, ARXIV150303893; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402079
C	Allen-Zhu, Z; Hazan, E; Hu, W; Li, YZ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Allen-Zhu, Zeyuan; Hazan, Elad; Hu, Wei; Li, Yuanzhi			Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a rank-k variant of the classical Frank-Wolfe algorithm to solve convex optimization over a trace-norm ball. Our algorithm replaces the top singular-vector computation (1-SVD) in Frank-Wolfe with a top-k singular-vector computation (k-SVD), which can be done by repeatedly applying 1-SVD k times. Alternatively, our algorithm can be viewed as a rank-k restricted version of projected gradient descent. We show that our algorithm has a linear convergence rate when the objective function is smooth and strongly convex, and the optimal solution has rank at most k. This improves the convergence rate and the total time complexity of the Frank-Wolfe method and its variants.	[Allen-Zhu, Zeyuan] Microsoft Res, Redmond, WA 98052 USA; [Hazan, Elad; Hu, Wei; Li, Yuanzhi] Princeton Univ, Princeton, NJ 08544 USA	Microsoft; Princeton University	Allen-Zhu, Z (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu; ehazan@cs.princeton.edu; huwei@cs.princeton.edu; yuanzhil@cs.princeton.edu	Li, Yuan/GXV-1310-2022; Jeong, Yongwook/N-7413-2016		NSF [1523815]; Google research award	NSF(National Science Foundation (NSF)); Google research award(Google Incorporated)	Elad Hazan was supported by NSF grant 1523815 and a Google research award. The authors would like to thank Dan Garber for sharing his code for [6].	Allen-Zhu Z., 2016, ADV NEURAL INFORM PR, P974; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099; Dudik M., 2012, AISTATS, P327; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Garber D., 2016, ADV NEURAL INFORM PR, P874; Garber D., 2013, ARXIV13014666; Garber D, 2015, PR MACH LEARN RES, V37, P541; Hazan E, 2008, LECT NOTES COMPUT SC, V4957, P306, DOI 10.1007/978-3-540-78773-0_27; Jaggi M., 2010, P 27 INT C MACHINE L; Lacoste-Julien S., 2013, ARXIV13127864; Livni R., 2014, NIPS, V1, P855; Musco C, 2015, ADV NEUR IN, V28; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Shalev-Shwartz S, 2011, ARXIV11061622	16	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406026
C	Alon, N; Babaioff, M; Gonczarowski, YA; Mansour, Y; Moran, S; Yehudayoff, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alon, Noga; Babaioff, Moshe; Gonczarowski, Yannai A.; Mansour, Yishay; Moran, Shay; Yehudayoff, Amir			Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this work we derive a variant of the classic Glivenko-Cantelli Theorem, which asserts uniform convergence of the empirical Cumulative Distribution Function (CDF) to the CDF of the underlying distribution. Our variant allows for tighter convergence bounds for extreme values of the CDF. We apply our bound in the context of revenue learning, which is a well-studied problem in economics and algorithmic game theory. We derive sample-complexity bounds on the uniform convergence rate of the empirical revenues to the true revenues, assuming a bound on the kth moment of the valuations, for any (possibly fractional) k > 1. For uniform convergence in the limit, we give a complete characterization and a zero-one law: if the first moment of the valuations is finite, then uniform convergence almost surely occurs; conversely, if the first moment is infinite, then uniform convergence almost never occurs.	[Alon, Noga; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Alon, Noga; Babaioff, Moshe; Gonczarowski, Yannai A.] Microsoft Res, Redmond, WA 98052 USA; [Gonczarowski, Yannai A.] Hebrew Univ Jerusalem, Jerusalem, Israel; [Moran, Shay] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA; [Mansour, Yishay] Google Res, Haifa, Israel; [Yehudayoff, Amir] Technion IIT, Haifa, Israel	Tel Aviv University; Microsoft; Hebrew University of Jerusalem; Institute for Advanced Study - USA; Google Incorporated; Technion Israel Institute of Technology	Alon, N (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.; Alon, N (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	nogaa@tau.ac.il; moshe@microsoft.com; yannai@gonch.name; mansour@tau.ac.il; shaymoranl@gmail.com; amir.yehudayoff@gmail.com	Jeong, Yongwook/N-7413-2016; Alon, Noga/GOV-5970-2022	Alon, Noga/0000-0003-1332-4883	ISF grant; GIF grant; Adams Fellowship Program of the Israel Academy of Sciences and Humanities; ISF [1162/15]; Israel-USA Bi-national Science Foundation (BSF) [2014389]; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [740282]; Israeli Centers of Research Excellence (I-CORE) program [4/11]; Israel Science Foundation; United States-Israel Binational Science Foundation (BSF); National Science Foundations; Simons Foundations	ISF grant; GIF grant; Adams Fellowship Program of the Israel Academy of Sciences and Humanities; ISF(Israel Science Foundation); Israel-USA Bi-national Science Foundation (BSF)(US-Israel Binational Science Foundation); European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); Israeli Centers of Research Excellence (I-CORE) program; Israel Science Foundation(Israel Science Foundation); United States-Israel Binational Science Foundation (BSF)(US-Israel Binational Science Foundation); National Science Foundations(National Science Foundation (NSF)); Simons Foundations	The research of Noga Alon is supported in part by an ISF grant and by a GIF grant. Yannai Gonczarowski is supported by the Adams Fellowship Program of the Israel Academy of Sciences and Humanities; his work is supported by ISF grant 1435/14 administered by the Israeli Academy of Sciences and by Israel-USA Bi-national Science Foundation (BSF) grant number 2014389; this project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 740282). The research of Yishay Mansour was supported in part by The Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11), by a grant from the Israel Science Foundation, and by a grant from United States-Israel Binational Science Foundation (BSF); the research was done while author was co-affiliated with Microsoft Research. The research of Shay Moran is supported by the National Science Foundations and the Simons Foundations; part of the research was done while author was co-affiliated with Microsoft Research. The research of Amir Yehudayoff is supported by ISF grant 1162/15.	Balcan Maria-Florina F, 2016, P 29 C NEURAL INFORM, P2083; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BIRNBAUM ZW, 1958, ANN MATH STAT, V29, P558, DOI 10.1214/aoms/1177706631; Borel E., 1909, REND CIRC MAT PALERM, V27, P247, DOI [10.1007/BF03019651, DOI 10.1007/BF03019651]; Cantelli F.P., 1917, ATTI ACCAD NAZ LIN, V26, P39; Cantelli FP, 1933, GIORN I ITAL ATTUARI, V4, P421; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Cortes Corinna, 2013, ABS13105796 CORR; Devanur NR, 2016, ACM S THEORY COMPUT, P426, DOI 10.1145/2897518.2897553; Dhangwatnotai P, 2015, GAME ECON BEHAV, V91, P318, DOI 10.1016/j.geb.2014.03.011; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Glivenko V., 1933, GIORNALE DELLISTITUT, V4, P92; Gonczarowski YA, 2017, ACM S THEORY COMPUT, P856, DOI 10.1145/3055399.3055427; Huang Zhiyi, 2015, 16 ACM C EC COMP EC, P45; Koltchinskii Vladimir, 2000, RADEMACHER PROCESSES, P443; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P1, DOI 10.1145/2940716.2940723; Van der Vaart A.W., 1996, WEAK CONVERGENCE EMP; Vapnik V.N, 1998, STAT LEARNING THEORY; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401067
C	Andersen, G; Konidaris, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Andersen, Garrett; Konidaris, George			Active Exploration for Learning Symbolic Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MOTION	We introduce an online active exploration algorithm for data-efficiently learning an abstract symbolic model of an environment. Our algorithm is divided into two parts: the first part quickly generates an intermediate Bayesian symbolic model from the data that the agent has collected so far, which the agent can then use along with the second part to guide its future exploration towards regions of the state space that the model is uncertain about. We show that our algorithm outperforms random and greedy exploration policies on two different computer game domains. The first domain is an Asteroids-inspired game with complex dynamics but basic logical structure. The second is the Treasure Game, with simpler dynamics but more complex logical structure.	[Andersen, Garrett] PROWLER Io, Cambridge, England; [Konidaris, George] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA	Brown University	Andersen, G (corresponding author), PROWLER Io, Cambridge, England.	garrett@prowler.io; gdk@cs.brown.edu	Jeong, Yongwook/N-7413-2016		National Institutes of Health [R01MH109177]	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research was supported in part by the National Institutes of Health under award number R01MH109177. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.	[Anonymous], 2000, THESIS; Barto AG, 2003, DISCRETE EVENT DYN S, V13, P41, DOI 10.1023/A:1025696116075; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Cambon S, 2009, INT J ROBOT RES, V28, P104, DOI 10.1177/0278364908097884; Choi J, 2009, IEEE INT CONF ROBOT, P4374; Dornhege C., 2009, IEEE INT WORKSH SAF; Friedman N, 1999, ADV NEUR IN, V11, P417; Gat E, 1998, ARTIFICIAL INTELLIGENCE AND MOBILE ROBOTS, P195; Ghahramani Z., 2005, P 22 INT C MACH LEAR, P297, DOI DOI 10.1145/1102351.1102389; Kaelbling L., 2011, P IEEE C ROB AUT; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Konidaris G, 2014, AAAI CONF ARTIF INTE, P1932; Konidaris G, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3619; Malcolm C., 1990, Robotics and Autonomous Systems, V6, P123, DOI 10.1016/S0921-8890(05)80032-6; Mobin S. A., 2014, ADV NEURAL INFORM PR, P3023; Nilsson N., 1984, SHAKEY ROBOT; Orseau L, 2013, LECT NOTES ARTIF INT, V8139, P158; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Wolfe Jason, 2010, INT C AUT PLANN SCHE	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405009
C	Andrychowicz, M; Wolski, F; Ray, A; Schneider, J; Fong, R; Welinder, P; McGrew, B; Tobin, J; Abbeel, P; Zaremba, W		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Andrychowicz, Marcin; Wolski, Filip; Ray, Alex; Schneider, Jonas; Fong, Rachel; Welinder, Peter; McGrew, Bob; Tobin, Josh; Abbeel, Pieter; Zaremba, Wojciech			Hindsight Experience Replay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORKS	Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.	[Andrychowicz, Marcin; Wolski, Filip; Ray, Alex; Schneider, Jonas; Fong, Rachel; Welinder, Peter; McGrew, Bob; Tobin, Josh; Abbeel, Pieter; Zaremba, Wojciech] OpenAI, San Francisco, CA 94110 USA		Andrychowicz, M (corresponding author), OpenAI, San Francisco, CA 94110 USA.	marcin@openai.com	Jeong, Yongwook/N-7413-2016					Abadi M., TENSORFLOW LARGE SCA; [Anonymous], ARXIV170506366; Ba J., 2017, P 3 INT C LEARN REPR; Bakker B., 2004, P 8 C INT AUT SYST I, P438; Bellemare M., 2016, NEURIPS; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Chebotar Y., 2016, ARXIV161000529; Da Silva B, 2012, ARXIV12066398; Devin C., 2016, ARXIV160907088; ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4; Foster D, 2002, MACH LEARN, V49, P325, DOI 10.1023/A:1017944732463; Graves A., 2017, ARXIV170403003; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Gu S., 2016, ARXIV160300748; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Kober J, 2012, AUTON ROBOT, V33, P361, DOI 10.1007/s10514-012-9290-3; Kolter J. Z., 2009, P 26 ANN INT C MACHI, P513, DOI DOI 10.1145/1553374.1553441; Levine S., 2015, ARXIV150400702; Lillicrap TP, 2016, 4 INT C LEARN REPR; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Metz L., 2017, ARXIV170505035; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng AY, 2006, SPRINGER TRAC ADV RO, V21, P363; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Ostrovski G., 2017, ARXIV170301310; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Pinto L., 2016, ARXIV160909025; Popov I., 2017, ARXIV170403073; Schaul T, 2016, C TRACK P, P1; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schmidhuber J, 2004, MACH LEARN, V54, P211, DOI 10.1023/B:MACH.0000015880.99707.b2; Schmidhuber J., 1990, LEARNING GENERATE FO; Schmidhuber J, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00313; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srivastava RK, 2013, NEURAL NETWORKS, V41, P130, DOI 10.1016/j.neunet.2013.01.022; Strehl Alexander L., 2005, P 22 INT C MACH LEAR, P856, DOI [DOI 10.1145/1102351.1102459, 10.1145/1102351.1102459]; Sukhbaatar S, 2017, ARXIVABS170305407 CO; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Tang H., 2016, EXPLORATION STUDY CO; Tobin J., 2017, ARXIV170306907; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Vezhnevets A. S., 2017, ARXIV170301161	45	0	0	6	26	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405013
C	Arjevani, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Arjevani, Yossi			Limitations on Variance-Reduction and Acceleration Schemes for Finite Sum Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DESCENT	We study the conditions under which one is able to efficiently apply variance-reduction and acceleration schemes on finite sum optimization problems. First, we show that, perhaps surprisingly, the finite sum structure by itself, is not sufficient for obtaining a complexity bound of (O) over tilde((n + L/mu) ln(1/epsilon)) for L-smooth and mu-strongly convex individual functions - one must also know which individual function is being referred to by the oracle at each iteration. Next, we show that for a broad class of first-order and coordinate-descent finite sum algorithms (including, e.g., SDCA, SVRG, SAG), it is not possible to get an 'accelerated' complexity bound of (O) over tilde((n + root nL/mu) ln(1/epsilon)), unless the strong convexity parameter is given explicitly. Lastly, we show that when this class of algorithms is used for minimizing L-smooth and convex finite sums, the iteration complexity is bounded from below by Omega(n + L/epsilon), assuming that (on average) the same update rule is used in any iteration, and Omega(n + root nL/epsilon) otherwise.	[Arjevani, Yossi] Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-7610001 Rehovot, Israel	Weizmann Institute of Science	Arjevani, Y (corresponding author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-7610001 Rehovot, Israel.	yossi.arjevani@weizmann.ac.il	Jeong, Yongwook/N-7413-2016					[Anonymous], ARXIV150702000; Arjevani Y., 2016, ADV NEURAL INFORM PR; Arjevani Y, 2016, PR MACH LEARN RES, V48; Arjevani Yossi, 2016, ARXIV161104982; Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022; Bietti Alberto, 2016, ARXIV161000970; Blatt D, 2007, SIAM J OPTIMIZ, V18, P29, DOI 10.1137/040615961; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lin Hongzhou, 2015, ADV NEURAL INFORM PR, P3366; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nitanda A, 2016, JMLR WORKSH CONF PRO, V51, P195; Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Shalev-Shwartz Shai, 2015, ARXIV PREPRINT ARXIV	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403059
C	Audiffren, J; Ralaivola, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Audiffren, Julien; Ralaivola, Liva			Bandits Dueling on Partially Ordered Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ASYMMETRIC DOMINANCE; SELECTION	We address the problem of dueling bandits defined on partially ordered sets, or posets. In this setting, arms may not be comparable, and there may be several (incomparable) optimal arms. We propose an algorithm, UnchainedBandits, that efficiently finds the set of optimal arms -the Pareto front- of any poset even when pairs of comparable arms cannot be a priori distinguished from pairs of incomparable arms, with a set of minimal assumptions. This means that unchainedBandits does not require information about comparability and can be used with limited knowledge of the poset. To achieve this, the algorithm relies on the concept of decoys, which stems from social psychology. We also provide theoretical guarantees on both the regret incurred and the number of comparison required by UnchainedBandits, and we report compelling empirical results.	[Audiffren, Julien] Univ Paris Saclay, ENS Paris Saclay, CNRS, CMLA, Cachan, France; [Ralaivola, Liva] Aix Marseille Univ, Lab Informat Fondamentale Marseille, CNRS, Inst Univ France, F-13288 Marseille 9, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); Institut Universitaire de France; UDICE-French Research Universities; Aix-Marseille Universite	Audiffren, J (corresponding author), Univ Paris Saclay, ENS Paris Saclay, CNRS, CMLA, Cachan, France.	julien.audiffren@gmail.com; liva.ralaivola@lif.univ-mrs.fr	Jeong, Yongwook/N-7413-2016	Audiffren, Julien/0000-0003-4321-2575				Ailon N, 2014, PR MACH LEARN RES, V32, P856; ARIELY D, 1995, ORGAN BEHAV HUM DEC, V63, P223, DOI 10.1006/obhd.1995.1075; Balsubramani Akshay, 2016, C LEARN THEOR, P336; Daskalakis C, 2011, SIAM J COMPUT, V40, P597, DOI 10.1137/070697720; Drugan M. M., 2013, 2013 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2013.6707036; Dudik M., 2015, P ANN C LEARN THEOR, P563; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Heath TB, 1995, J CONSUM RES, V22, P268, DOI 10.1086/209449; HUBER J, 1982, J CONSUM RES, V9, P90, DOI 10.1086/208899; Komiyama J., 2016, ARXIV160501677; Komiyama Junpei, 2015, C LEARN THEOR, V40, P1141; Ramamohan S. Y., 2016, ADV NEURAL INFORM PR, P1253; Robert B., 1990, ASH INFORM THEORY; Sedikides C, 1999, SOC COGNITION, V17, P118, DOI 10.1521/soco.1999.17.2.118; TVERSKY A, 1981, SCIENCE, V211, P453, DOI 10.1126/science.7455683; WU H, 2016, ADV NEURAL INFORM PR, P649; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zoghi M, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P17, DOI 10.1145/2684822.2685290; Zoghi M, 2014, PR MACH LEARN RES, V32, P10; Zoghi Masrour, 2015, ADV NEURAL INFORM PR, P307	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402018
C	Backurs, A; Indyk, P; Schmidt, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Backurs, Arturs; Indyk, Piotr; Schmidt, Ludwig			On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Empirical risk minimization (ERM) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various ERM problems, the exact computational complexity of ERM is still not understood. We address this issue for multiple popular ERM problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on complexity-theoretic assumptions such as the Strong Exponential Time Hypothesis. Under these assumptions, we show that there are no algorithms that solve the aforementioned ERM problems to high accuracy in sub-quadratic time. We also give similar hardness results for computing the gradient of the empirical loss, which is the main computational burden in many non-convex learning tasks.	[Backurs, Arturs; Indyk, Piotr; Schmidt, Ludwig] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Backurs, A (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	backurs@mit.edu; indyk@mit.edu; ludwigs@mit.edu	Jeong, Yongwook/N-7413-2016		Google PhD fellowship; IBM Research fellowship; NSF; Simons Foundation	Google PhD fellowship(Google Incorporated); IBM Research fellowship(International Business Machines (IBM)); NSF(National Science Foundation (NSF)); Simons Foundation	Ludwig Schmidt is supported by a Google PhD fellowship. Arturs Backurs is supported by an IBM Research fellowship. This research was supported by grants from NSF and Simons Foundation.	Abboud A., 2015, FDN COMPUTER SCI FOC; Abboud A., 2014, INT C AUT LANG PROGR; Agarwal A., 2015, INT C MACH LEARN ICM; Alman J., 2015, S FDN COMP SCI FOCS; Alman J., 2016, POLYNOMIAL REPRESENT; Andoni A., 2015, ADV NEURAL INFORM PR; Andoni A., 2015, S THEOR COMP STOC; Andoni A., 2006, S FDN COMP SCI FOCS; Arjevani Y., 2016, ADV NEURAL INFORM PR; Arjevani Y., 2016, ABS161104982 CORR; Bach F., 2013, C LEARN THEOR COLT; Bach F., 2016, NIPS TUTORIAL; Backurs A., 2015, S THEOR COMP STOC; Backurs A., 2017, INT C MACH LEARN ICM; Bottou L., 2007, ADV NEURAL INFORM PR; Bringmann K., 2015, FDN COMPUTER SCI FOC; Bringmann Karl, 2014, S FDN COMP SCI FOCS; Carmosino ML, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P261; Cesa-Bianchi N., 2015, C LEARN THEOR COLT; Charikar Moses, 2017, FOCS; Chechik S., 2014, S DISCR ALG SODA; Impagliazzo R, 2001, J COMPUT SYST SCI, V63, P512, DOI 10.1006/jcss.2001.1774; Impagliazzo R, 2001, J COMPUT SYST SCI, V62, P367, DOI 10.1006/jcss.2000.1727; Livni R., 2014, NIPS, V1, P855; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Musco Cameron, 2016, ADV NEURAL INFORM PR; Nesterov Y., 2018, APPL OPTIMIZATION; Rahimi A, 2007, NEURAL INFORM PROCES; Razavian A. S., 2014, C COMP VIS PATT REC; Roditty L., 2013, S THEOR COMP STOC; Scholkopf B., 2001, LEARNING KERNELS SUP; Shalev-Shwartz S., 2007, INT C MACH LEARN ICM; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Valiant G., 2012, S FDN COMP SCI FOCS; Vapnik V.N, 1998, STAT LEARNING THEORY; VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010; Williams C., 2001, ADV NEURAL INFORM PR; Williams Virginia Vassilevska, 2015, LIPICS LEIBNIZ INT P, V43; Woodworth B. E., 2016, ADV NEURAL INFORM PR; Yang Y, 2017, ANN STAT, V45, P991, DOI 10.1214/16-AOS1472	42	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404037
C	Balkanski, E; Immorlica, N; Singer, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Balkanski, Eric; Immorlica, Nicole; Singer, Yaron			The Importance of Communities for Learning to Influence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the canonical problem of influence maximization in social networks. Since the seminal work of Kempe, Kleinberg, and Tardos [KKT03] there have been two, largely disjoint efforts on this problem. The first studies the problem associated with learning the generative model that produces cascades, and the second focuses on the algorithmic challenge of identifying a set of influencers, assuming the generative model is known. Recent results on learning and optimization imply that in general, if the generative model is not known but rather learned from training data, no algorithm for influence maximization can yield a constant factor approximation guarantee using polynomially-many samples, drawn from any distribution. In this paper we describe a simple algorithm for maximizing influence from training data. The main idea behind the algorithm is to leverage the strong community structure of social networks and identify a set of individuals who are influentials but whose communities have little overlap. Although in general, the approximation guarantee of such an algorithm is unbounded, we show that this algorithm performs well experimentally. To analyze its performance, we prove this algorithm obtains a constant factor approximation guarantee on graphs generated through the stochastic block model, traditionally used to model networks with community structure.	[Balkanski, Eric; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA; [Immorlica, Nicole] Microsoft Res, Redmond, WA USA	Harvard University; Microsoft	Balkanski, E (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	ericbalkanski@g.harvard.edu; nicimm@microsoft.com; yaron@seas.harvard.edu	Jeong, Yongwook/N-7413-2016					Abrahao Bruno D., 2013, KDD; Adar Eytan, 2005, WI; Angell Rico, 2016, ARXIV160906520; Badanidiyuru A., 2012, SODA; Balkanski E., 2016, ADV NEURAL INFORM PR, P4017; Balkanski E., 2017, STOC; Balkanski Eric., 2017, P C LEARNING THEORY; Blum Avrim, FDN DATA SCI; Borgs C., 2014, SODA, P946; Cheng J., 2014, WWW; Chierichetti F., 2011, NIPS; Daneshmand Hadi, 2014, ICML; DeAbir, 2014, CIKM; Domingos P., 2001, KDD; Du N., 2012, NIPS; Du Nan, 2013, NIPS; Du Nan, 2014, ICML; ERDOS P, 1960, B INT STATIST INST, V38, P343; Feldman Vitaly, 2014, COLT; Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741; GomezRodriguez M., 2011, ICML; Goyal Amit, 2010, KDD; Hassidim Avinatan, 2017, COLT; He X., 2016, P INT C NEUR INF PRO, P2065; He XR, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P885, DOI 10.1145/2939672.2939760; Honorio J, 2015, J MACH LEARN RES, V16, P1157; Horel T., 2016, P NIPS BARC, P3045; Horel T, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P441, DOI 10.1145/2736277.2741127; Kempe D, 2005, LECT NOTES COMPUT SC, V3580, P1127, DOI 10.1007/11523468_91; Kempe D., 2003, KDD; Leskovec J., 2015, SNAP DATASETSSTANFOR; Leskovec J., 2007, SDM; Liben-Nowell D., 2003, CIKM; MOSSEL E, 2007, STOC; Narasimhan H., 2015, ADV NEURAL INFORM PR, P3186; Netrapalli Praneeth, 2012, SIGMETRICS PERFORMAN; Richardson Matthew, 2002, KDD; Seeman L, 2013, ANN IEEE SYMP FOUND, P459, DOI 10.1109/FOCS.2013.56	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405091
C	Balseiro, S; Lin, M; Mirrokni, V; Leme, RP; Zuo, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Balseiro, Santiago; Lin, Max; Mirrokni, Vahab; Leme, Renato Paes; Zuo, Song			Dynamic Revenue Sharing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Many online platforms act as intermediaries between a seller and a set of buyers. Examples of such settings include online retailers (such as Ebay) selling items on behalf of sellers to buyers, or advertising exchanges (such as AdX) selling pageviews on behalf of publishers to advertisers. In such settings, revenue sharing is a central part of running such a marketplace for the intermediary, and fixed-percentage revenue sharing schemes are often used to split the revenue among the platform and the sellers. In particular, such revenue sharing schemes require the platform to (i) take at most a constant fraction alpha of the revenue from auctions and (ii) pay the seller at least the seller declared opportunity cost c for each item sold. A straightforward way to satisfy the constraints is to set a reserve price at c/(1 - alpha) for each item, but it is not the optimal solution on maximizing the profit of the intermediary. While previous studies (by Mirrokni and Gomes, and by Niazadeh et al) focused on revenue-sharing schemes in static double auctions, in this paper, we take advantage of the repeated nature of the auctions. In particular, we introduce dynamic revenue sharing schemes where we balance the two constraints over different auctions to achieve higher profit and seller revenue. This is directly motivated by the practice of advertising exchanges where the fixed-percentage revenue-share should be met across all auctions and not in each auction. In this paper, we characterize the optimal revenue sharing scheme that satisfies both constraints in expectation. Finally, we empirically evaluate our revenue sharing scheme on real data.	[Balseiro, Santiago] Columbia Univ, New York, NY 10027 USA; [Lin, Max; Mirrokni, Vahab; Leme, Renato Paes] Google, New York, NY USA; [Zuo, Song] Tsinghua Univ, Beijing, Peoples R China	Columbia University; Google Incorporated; Tsinghua University	Balseiro, S (corresponding author), Columbia Univ, New York, NY 10027 USA.	srb2155@columbia.edu; whlin@google.com; mirrokni@google.com; renatoppl@google.com; songzuo.z@gmail.com			National Basic Research Program of China [2011CBA00300, 2011CBA00301]; Natural Science Foundation of China [61033001, 61361136003, 61303077, 61561146398]; Tsinghua Initiative Scientific Research Grant; China Youth 1000-talent program	National Basic Research Program of China(National Basic Research Program of China); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Tsinghua Initiative Scientific Research Grant; China Youth 1000-talent program	The work was done when this author was an intern at Google. This author was supported by the National Basic Research Program of China Grant 2011CBA00300, 2011CBA00301, the Natural Science Foundation of China Grant 61033001, 61361136003, 61303077, 61561146398, a Tsinghua Initiative Scientific Research Grant and a China Youth 1000-talent program.	Balseiro SR, 2014, MANAGE SCI, V60, P2886, DOI 10.1287/mnsc.2014.2017; Gomes R, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P19, DOI 10.1145/2566486.2568029; Leme RP, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1093, DOI 10.1145/2872427.2883071; MCAFEE RP, 1987, J ECON LIT, V25, P699; MYERSON RB, 1983, J ECON THEORY, V29, P265, DOI 10.1016/0022-0531(83)90048-0; Niazadeh R, 2014, LECT NOTES COMPUT SC, V8877, P386, DOI 10.1007/978-3-319-13129-0_31; Talluri K, 1998, MANAGE SCI, V44, P1577, DOI 10.1287/mnsc.44.11.1577	7	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402071
C	Baltaoglu, S; Tong, L; Zhao, Q		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Baltaoglu, Sevi; Tong, Lang; Zhao, Qing			Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				REGRET	We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his T-period payoff, the bidder determines the optimal allocation of his budget among his bids for K goods at each period. As a bidding strategy, we propose a polynomial-time algorithm, inspired by the dynamic programming approach to the knapsack problem. The proposed algorithm, referred to as dynamic programming on discrete set (DPDS), achieves a regret order of O(root T log T). By showing that the regret is lower bounded by Omega(root T) for any strategy, we conclude that DPDS is order optimal up to a root log T term. We evaluate the performance of DPDS empirically in the context of virtual trading in wholesale electricity markets by using historical data from the New York market. Empirical results show that DPDS consistently outperforms benchmark heuristic methods that are derived from machine learning and online learning approaches.	[Baltaoglu, Sevi; Tong, Lang; Zhao, Qing] Cornell Univ, Ithaca, NY 14850 USA	Cornell University	Baltaoglu, S (corresponding author), Cornell Univ, Ithaca, NY 14850 USA.	msb372@cornell.edu; lt35@cornell.edu; qz16@cornell.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [1549989]; Army Research Laboratory Network Science CTA [W911NF-09-2-0053]	National Science Foundation(National Science Foundation (NSF)); Army Research Laboratory Network Science CTA	This work was supported in part by the National Science Foundation under Award 1549989 and by the Army Research Laboratory Network Science CTA under Cooperative Agreement W911NF-09-2-0053.	Amin K, 2012, P 28 C UNC ART INT; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P382, DOI 10.1145/167088.167198; Cope EW, 2009, IEEE T AUTOMAT CONTR, V54, P1243, DOI 10.1109/TAC.2009.2019797; Daskalakis C, 2016, ANN IEEE SYMP FOUND, P219, DOI 10.1109/FOCS.2016.31; Dudik M, 2017, ANN IEEE SYMP FOUND, P528, DOI 10.1109/FOCS.2017.55; DUDZINSKI K, 1987, EUR J OPER RES, V28, P3, DOI 10.1016/0377-2217(87)90165-2; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Kellerer H., 2004, KNAPSACK PROBLEMS, DOI [10.1007/978-3-540-24777-7, DOI 10.1007/978-3-540-24777-7]; Kleinberg R, 2005, P 17 INT C NEUR INF, V17, P697; Kleinberg R, 2010, PROC APPL MATH, V135, P827; Kleinberg Robert, 2008, P 21 ANN C LEARN THE, P425; Kleinberg Robert, 2015, ARXIV13121277V2; Krichene W, 2015, PR MACH LEARN RES, V37, P824; Li RY, 2015, J REGUL ECON, V48, P245, DOI 10.1007/s11149-015-9281-3; Long TT, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P809; Milgrom Paul., 2004, PUTTING AUCTION THEO; Mohri Mehryar, 2014, ADV NEURAL INFORM PR, P1871; Parsons J. E., 2015, 15002 MIT; Patton David B., 2015, TECHNICAL REPORT; PJM, 2015, TECHNICAL REPORT; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Tang WY, 2016, IEEE DECIS CONTR P, P6645, DOI 10.1109/CDC.2016.7799292; Tian F, 2017, ASIA COMMUN PHOTON; VAPNIK V, 1992, ADV NEUR IN, V4, P831; Weed J., 2016, PROC 29 ANN C LEARN, P1562	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404056
C	Barash, D; Gavish, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Barash, Danny; Gavish, Matan			Optimal Shrinkage of Singular Values Under Random Data Contamination	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MATRIX; COMPONENTS; ALGORITHM; NUMBER	A low rank matrix X has been contaminated by uniformly distributed noise, missing values, outliers and corrupt entries. Reconstruction of X from the singular values and singular vectors of the contaminated matrix Y is a key problem in machine learning, computer vision and data science. In this paper, we show that common contamination models (including arbitrary combinations of uniform noise, missing values, outliers and corrupt entries) can be described efficiently using a single framework. We develop an asymptotically optimal algorithm that estimates X by manipulation of the singular values of Y, which applies to any of the contamination models considered. Finally, we find an explicit signal-to-noise cutoff, below which estimation of X from the singular value decomposition of Y must fail, in a well-defined sense.	[Barash, Danny; Gavish, Matan] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel	Hebrew University of Jerusalem	Barash, D (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.	danny.barash@mail.huji.ac.il; gavish@cs.huji.ac.il	Jeong, Yongwook/N-7413-2016		Israeli Science Foundation [1523/16]; German-Israeli Foundation for scientific research and development program [I-1100-407.1-2015]	Israeli Science Foundation(Israel Science Foundation); German-Israeli Foundation for scientific research and development program(German-Israeli Foundation for Scientific Research and Development)	DB was supported by Israeli Science Foundation grant no. 1523/16 and German-Israeli Foundation for scientific research and development program no. I-1100-407.1-2015.	Bloemendal A, 2014, ELECTRON J PROBAB, V19, DOI 10.1214/EJP.v19-3054; Boutsidis C, 2015, IEEE T INFORM THEORY, V61, P1045, DOI 10.1109/TIT.2014.2375327; Bouwmans Thierry, 2016, COMPUTER SCI REV; Buuren S., 2011, J STAT SOFTWARE, V45; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2013, IEEE T SIGNAL PROCES, V61, P4643, DOI 10.1109/TSP.2013.2270464; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; CATTELL RB, 1966, MULTIVAR BEHAV RES, V1, P245, DOI 10.1207/s15327906mbr0102_10; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Das R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P795; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gavish M, 2017, IEEE T INFORM THEORY, V63, P2137, DOI 10.1109/TIT.2017.2653801; Gavish M, 2014, IEEE T INFORM THEORY, V60, P5040, DOI 10.1109/TIT.2014.2323359; GNANADESIKAN R, 1972, BIOMETRICS, V28, P81, DOI 10.2307/2528963; Golub G., 1965, J SOC IND APPL MATH, V2, P205, DOI [10.1137/0702016, DOI 10.1137/0702016]; Hastie T., 1999, IMPUTING MISSING DA, P1; Huber P. J., 2011, ROBUST STAT; Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849; Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544; Lin Z, 2013, AUGMENTED LAGRANGE M; Luo X, 2014, IEEE T IND INFORM, V10, P1273, DOI 10.1109/TII.2014.2308433; Marenko V. A., 1967, MATH USSR SB, V1, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]; Meloun M, 2000, ANAL CHIM ACTA, V423, P51, DOI 10.1016/S0003-2670(00)01100-4; Nadakuditi RR, 2014, IEEE T INFORM THEORY, V60, P3002, DOI 10.1109/TIT.2014.2311661; Rao N., 2015, PROC 28 INT C NEURAL, P2107, DOI DOI 10.5555/2969442.2969475; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Rubin DB, 1996, J AM STAT ASSOC, V91, P473, DOI 10.1080/01621459.1996.10476908; Schafer JL., 1997, ANAL INCOMPLETE MULT, DOI 10.1201/9781439821862; Shabalin AA, 2013, J MULTIVARIATE ANAL, V118, P67, DOI 10.1016/j.jmva.2013.03.005; Stein C., 1986, J SOV MATH, V34, P1373, DOI [10.1007/BF01085007, DOI 10.1007/BF01085007]; Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1002/CPA.20132; Yang JC, 2016, IEEE ACCESS, V4, DOI 10.1109/ACCESS.2016.2601069	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406023
C	Bassily, R; Nissim, K; Stemmer, U; Thakurta, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bassily, Raef; Nissim, Kobbi; Stemmer, Uri; Thakurta, Abhradeep			Practical Locally Private Heavy Hitters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error - TreeHist and Bitstogram. In both algorithms, server running time is (O) over tilde (n) and user running time is (O) over tilde (1), hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring (O) over tilde (n(5/2)) server time and (O) over tilde (n(3/2)) user time. With a typically large number of participants in local algorithms (n in the millions), this reduction in time complexity, in particular at the user side, is crucial for the use of such algorithms in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.	[Bassily, Raef] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA; [Nissim, Kobbi] Georgetown Univ, Dept Comp Sci, Washington, DC 20057 USA; [Stemmer, Uri] Harvard Univ, Ctr Res Computat & Soc, Cambridge, MA 02138 USA; [Thakurta, Abhradeep] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA	University System of Ohio; Ohio State University; Georgetown University; Harvard University; University of California System; University of California Santa Cruz	Bassily, R (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.	bassily.1@osu.edu; kobbi.nissim@georgetown.edu; u@uri.co.il; aguhatha@ucsc.edu						[Anonymous], WALL STREET J; Bassily R, 2015, ACM S THEORY COMPUT, P127, DOI 10.1145/2746539.2746632; Bassily Raef, 2017, CORR; Beimel A, 2016, THEOR COMPUT, V12, DOI 10.4086/toc.2016.v012a001; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Chan THH, 2012, LECT NOTES COMPUT SC, V7501, P277, DOI 10.1007/978-3-642-33090-2_25; Charikar Moses, 2002, ICALP; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Erlingsson Ulfar, 2014, CCS; Evfimievski A., 2003, P 22 ACM SIGACT SIGM, P211, DOI DOI 10.1145/773153.773174; Fanti Giulia, 2015, ARXIV150301214; Hsu J, 2012, LECT NOTES COMPUT SC, V7391, P461, DOI 10.1007/978-3-642-31594-7_39; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Mishra N., 2006, PODS 06, P143; Thakurta Abhradeep Guha, 2017, US Patent, Patent No. [9,594,741, 9594741]	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402033
C	Bauer, S; Gorbach, NS; Miladinovic, D; Buhmann, JM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bauer, Stefan; Gorbach, Nico S.; Miladinovic, Dorde; Buhmann, Joachim M.			Efficient and Flexible Inference for Stochastic Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DIFFERENTIAL-EQUATIONS	Many real world dynamical systems are described by stochastic differential equations. Thus parameter inference is a challenging and important problem in many disciplines. We provide a grid free and flexible algorithm offering parameter and state inference for stochastic systems and compare our approch based on variational approximations to state of the art methods showing significant advantages both in runtime and accuracy.	[Bauer, Stefan; Gorbach, Nico S.; Miladinovic, Dorde; Buhmann, Joachim M.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Bauer, S (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	bauers@inf.ethz.ch; ngorbach@inf.ethz.ch; djordjem@inf.ethz.ch; jbuhmann@inf.ethz.ch	Jeong, Yongwook/N-7413-2016; Buhmann, Joachim/AAU-4760-2020		Max Planck ETH Center for Learning Systems; SystemsX.ch project SignalX	Max Planck ETH Center for Learning Systems; SystemsX.ch project SignalX	This research was partially supported by the Max Planck ETH Center for Learning Systems and the SystemsX.ch project SignalX.	Archambeau C., 2008, ADV NEURAL INFORM PR; Asai Y, 2013, STOCH ANAL APPL, V31, P293, DOI 10.1080/07362994.2013.759738; Asai Y., 2013, COMMUN APPL ANAL, V17, P521; Calderhead Ben, 2008, NEURAL INFORM PROCES; DOSS H, 1977, ANN I H POINCARE B, V13, P99; Evensen G., 2003, OCEAN DYNAM, V53, P343, DOI [10.1007/s10236-003-0036-9, DOI 10.1007/S10236-003-0036-9]; Gorbach Nico S, 2017, ARXIV1895944; Grune L, 2001, BIT, V41, P711, DOI 10.1023/A:1021995918864; Han X., 2017, RANDOM ORDINARY DIFF, V1st; Imkeller P., 2001, Journal of Dynamics and Differential Equations, V13, P215, DOI 10.1023/A:1016673307045; Jentzen A, 2011, TAYLOR APPROXIMATION; Jentzen A, 2009, BIT, V49, P113, DOI 10.1007/s10543-009-0211-6; Kloeden PE, 2007, P ROY SOC A-MATH PHY, V463, P2929, DOI 10.1098/rspa.2007.0055; LEDIMET FX, 1986, TELLUS A, V38, P97, DOI 10.1111/j.1600-0870.1986.tb00459.x; Lorenz EN, 1998, J ATMOS SCI, V55, P399, DOI 10.1175/1520-0469(1998)055<0399:OSFSWO>2.0.CO;2; Lyons Simon, 2012, NEURAL INFORM PROCES; Riesinger C, 2016, SIAM J SCI COMPUT, V38, pC372, DOI 10.1137/15M1036014; Snyder C, 2008, MON WEATHER REV, V136, P4629, DOI 10.1175/2008MWR2529.1; SUSSMANN HJ, 1978, ANN PROBAB, V6, P19, DOI 10.1214/aop/1176995608; Tornoe CW, 2005, PHARM RES-DORDR, V22, P1247, DOI 10.1007/s11095-005-5269-5; Vrettas MD, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.012148; Vrettas MD, 2011, PHYSICA D, V240, P1877, DOI 10.1016/j.physd.2011.08.013	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407008
C	Beirami, A; Razaviyayn, M; Shahrampour, S; Tarokh, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Beirami, Ahmad; Razaviyayn, Meisam; Shahrampour, Shahin; Tarokh, Vahid			On Optimal Generalizability in Parametric Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VARIABLE SELECTION; CROSS-VALIDATION; MODEL SELECTION; REGULARIZATION	We consider the parametric learning problem, where the objective of the learner is determined by a parametric loss function. Employing empirical risk minimization with possibly regularization, the inferred parameter vector will be biased toward the training samples. Such bias is measured by the cross validation procedure in practice where the data set is partitioned into a training set used for training and a validation set, which is not used in training and is left to measure the outof-sample performance. A classical cross validation strategy is the leave-one-out cross validation (LOOCV) where one sample is left out for validation and training is done on the rest of the samples that are presented to the learner, and this process is repeated on all of the samples. LOOCV is rarely used in practice due to the high computational complexity. In this paper, we first develop a computationally efficient approximate LOOCV (ALOOCV) and provide theoretical guarantees for its performance. Then we use ALOOCV to provide an optimization algorithm for finding the regularizer in the empirical risk minimization framework. In our numerical experiments, we illustrate the accuracy and efficiency of ALOOCV as well as our proposed framework for the optimization of the regularizer.	[Beirami, Ahmad; Shahrampour, Shahin; Tarokh, Vahid] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA; [Razaviyayn, Meisam] Univ Southern Calif, Dept Ind & Syst Engn, Los Angeles, CA 90089 USA	Harvard University; University of Southern California	Beirami, A (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	beirami@seas.harvard.edu; razaviya@usc.edu; shahin@seas.harvard.edu; vahid@seas.harvard.edu	Jeong, Yongwook/N-7413-2016; Poor, H. Vincent/S-5027-2016	Poor, H. Vincent/0000-0002-2062-131X	DARPA [W911NF-16-1-0561]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported in part by DARPA under Grant No. W911NF-16-1-0561. The authors are thankful to Jason D. Lee (USC) who brought to their attention the recent work [14] on influence functions for approximating leave-one-out cross validation.	AKAIKE H, 1970, ANN I STAT MATH, V22, P203, DOI 10.1007/BF02506337; ALLEN DM, 1974, TECHNOMETRICS, V16, P125, DOI 10.2307/1267500; Arlot S, 2010, STAT SURV, V4, P40, DOI 10.1214/09-SS054; Barron Andrew R, 1984, PREDICTED SQUARED ER; Bennett KP, 2006, IEEE IJCNN, P1922; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Brochu E, 2010, ARXIV PREPRINT ARXIV; Burnham K.P., 2003, MODEL SELECTION MULT; Christensen R, 2011, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-1-4419-9816-3; COOK RD, 1980, TECHNOMETRICS, V22, P495; Craven P., 1979, Numerische Mathematik, V31, P377, DOI 10.1007/BF01404567; Deledalle CA, 2014, SIAM J IMAGING SCI, V7, P2448, DOI 10.1137/140968045; EFRON B, 1987, J AM STAT ASSOC, V82, P171, DOI 10.2307/2289144; Eggensperger K., 2013, P NIPS WORKSH BAYES, P1; GEISSER S, 1975, J AM STAT ASSOC, V70, P320, DOI 10.2307/2285815; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Hirotugu Akaike, 1973, P 2 INT S INF THEOR, P267, DOI 10.1007/978-1-4612-1694-0; Koh P. W., 2017, INT C MACH LEARN; Kunapuli G, 2008, CRM PROC & LECT NOTE, V45, P129; Li LX, 2017, LECT NOTES COMPUT SC, V10538, P17, DOI 10.1007/978-3-319-68155-9-2; Mokus J., 1975, LECT NOTES COMPUTER, P400, DOI [10.1007/3-540-07165-2_55, DOI 10.1007/3-540-07165-2_55, DOI 10.1007/978-3-662-38527-2_55, 10.1007/3-540-07165-2, DOI 10.1007/3-540-07165-2]; Moody J. E., 1992, ADV NEURAL INFORM PR, P847; Ramani S, 2012, IEEE T IMAGE PROCESS, V21, P3659, DOI 10.1109/TIP.2012.2195015; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; STONE M, 1974, BIOMETRIKA, V61, P509, DOI 10.1093/biomet/61.3.509; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Takeuchi K., 1976, SURI KAGAKU, V153, P12; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	31	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403051
C	Blondel, M; Niculae, V; Otsuka, T; Ueda, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Blondel, Mathieu; Niculae, Vlad; Otsuka, Takuma; Ueda, Naonori			Multi-output Polynomial Networks and Factorization Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONDITIONAL GRADIENT ALGORITHMS	Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.	[Blondel, Mathieu; Niculae, Vlad; Otsuka, Takuma] NTT Commun Sci Labs, Kyoto, Japan; [Niculae, Vlad] Cornell Univ, Ithaca, NY USA; [Ueda, Naonori] RIKEN, NTT Commun Sci Labs, Kyoto, Japan	Nippon Telegraph & Telephone Corporation; Cornell University; Nippon Telegraph & Telephone Corporation; RIKEN	Blondel, M (corresponding author), NTT Commun Sci Labs, Kyoto, Japan.	mathieu@mblondel.org; vlad@cs.cornell.edu; otsuka.takuma@lab.ntt.co.jp; ueda.naonori@lab.ntt.co.jp	Jeong, Yongwook/N-7413-2016					[Anonymous], NIPS; Bach F., 2017, JMLR; Bengio Yoshua, 2005, NIPS; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Blondel M., 2016, ICML; Blondel M, 2013, MACH LEARN, V93, P31, DOI 10.1007/s10994-013-5367-2; Blondel Mathieu, 2015, ECML PKDD; Bordes A, 2005, J MACH LEARN RES, V6, P1579; Chang Y.W., 2010, J MACH LEARN RES, V11, P1471, DOI DOI 10.5555/1756006.1859899; DUNN JC, 1978, J MATH ANAL APPL, V62, P432, DOI 10.1016/0022-247X(78)90137-3; Gautier A., 2016, NIPS; Jaggi Martin., 2013, ICML; Journee M, 2010, J MACH LEARN RES, V11, P517; Juan Y., 2016, ACM RECSYS; Lacoste-Julien S., 2012, ICML, V28, P53; Li P., 2007, NIPS; Luss R, 2013, SIAM REV, V55, P65, DOI 10.1137/110839072; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Manning C, 2014, EMNLP; Novikov A, 2016, ARXIV160503795; Podosinnikova A., 2016, ICML; Rendle S., 2010, ICDM; Shalev-Shwartz Shai, 2011, ICML; Shalev-Shwartz Shai, 2011, NIPS; Sonnenburg Soren, 2010, ICML; Stoudenmire E., 2016, NIPS; Wang Z., 2010, ICML; Yamada M., 2017, KDD; Zhong E., 2016, CIKM	30	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403041
C	Blum, A; Haghtalab, N; Procaccia, AD; Qiao, MD		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Blum, Avrim; Haghtalab, Nika; Procaccia, Ariel D.; Qiao, Mingda			Collaborative PAC Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MODEL	We consider a collaborative PAC learning model, in which k players attempt to learn the same underlying concept. We ask how much more information is required to learn an accurate classifier for all players simultaneously. We refer to the ratio between the sample complexity of collaborative PAC learning and its non-collaborative (single-player) counterpart as the overhead. We design learning algorithms with O(ln(k)) and O(ln(2)(k)) overhead in the personalized and centralized variants our model. This gives an exponential improvement upon the naive algorithm that does not share information among players. We complement our upper bounds with an Omega(ln(k)) overhead lower bound, showing that our results are tight up to a logarithmic factor.	[Blum, Avrim] Toyota Technol Inst Chicago, Chicago, IL 60637 USA; [Haghtalab, Nika; Procaccia, Ariel D.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; [Qiao, Mingda] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing 100084, Peoples R China	Toyota Technological Institute - Chicago; Carnegie Mellon University; Tsinghua University	Blum, A (corresponding author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	avrim@ttic.edu; nhaghtal@cs.cmu.edu; arielpro@cs.cmu.edu; qmd14@mails.tsinghua.edu.cn		Qiao, Mingda/0000-0002-9182-6152	NSF [CCF-1525971, CCF-1536967, CCF-1331175, IIS-1350598, IIS-1714140, CCF-1525932, CCF-1733556]; Office of Naval Research [N00014-16-1-3075, N00014-17-1-2428]; Sloan Research Fellowship; Microsoft Research Ph.D. fellowship	NSF(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); Sloan Research Fellowship(Alfred P. Sloan Foundation); Microsoft Research Ph.D. fellowship	We thank the anonymous reviewers for their helpful remarks and suggesting an alternative boosting-based approach for the centralized setting. This work was partially supported by the NSF grants CCF-1525971, CCF-1536967, CCF-1331175, IIS-1350598, IIS-1714140, CCF-1525932, and CCF-1733556, Office of Naval Research grants N00014-16-1-3075 and N00014-17-1-2428, a Sloan Research Fellowship, and a Microsoft Research Ph.D. fellowship. This work was done while Avrim Blum was working at Carnegie Mellon University.	Anthony M., 1999, NEURAL NETWORK LEARN, V9; Balcan Maria-Florina, 2012, P 25 C COMP LEARN TH; Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Ben-David S, 2003, LECT NOTES ARTIF INT, V2777, P567, DOI 10.1007/978-3-540-45167-9_41; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Dekel O., 2011, P 28 INT C MACH LEAR, P713; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Kumar A, 2012, P 29 INT C MACH LEAR, P1103; Mansour Y., 2009, P COLT, P19; Pontil M., 2013, PROC ANN C LEARN THE, P55; Rostamizadeh A., 2009, ADV NEURAL INFORM PR, P1041; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Wang JW, 2016, AER ADV ENG RES, V79, P751	15	0	0	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402043
C	Bonald, T; Combes, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bonald, Thomas; Combes, Richard			A Minimax Optimal Algorithm for Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ERROR	We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide, which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE), an algorithm for estimating the reliability of workers. TE has low complexity, may be implemented in a streaming setting when labels are provided by workers in real time, and does not rely on an iterative procedure. We prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data.	[Bonald, Thomas] Telecom ParisTech, Paris, France; [Combes, Richard] Cent Supelec, L2S, Chatenay Malabry, France	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Bonald, T (corresponding author), Telecom ParisTech, Paris, France.	thomas.bonald@telecom-paristech.fr; richard.combes@supelec.fr	Jeong, Yongwook/N-7413-2016					Albert PS, 2004, BIOMETRICS, V60, P427, DOI 10.1111/j.0006-341X.2004.00187.x; Chao Gao, 2015, MINIMAX OPTIMAL CONV; Dalvi Nilesh, 2013, P WWW; Dawid A. P., 1979, APPL STAT, V28, P20, DOI DOI 10.2307/2346806; Ghosh A., 2011, P ACM EC; HUI SL, 1980, BIOMETRICS, V36, P167, DOI 10.2307/2530508; Karger David R., 2013, Performance Evaluation Review, V41, P81; Karger D. R., 2011, P ADV NEUR INF PROC, P1; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Liu Q., 2012, P NIPS; NITZAN S, 1982, INT ECON REV, V23, P289, DOI 10.2307/2526438; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; SHAPLEY L, 1984, PUBLIC CHOICE, V43, P329, DOI 10.1007/BF00118940; Smyth Padhraic, 1995, P NIPS; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Wang Dong, 2013, P IEEE ICDCS; Welinder Peter, 2010, P IEEE CVPR WORKSH; Whitehill J., 2009, P NIPS; Zhang Y., 2014, P NIPS; Zhou D., 2015, REGULARIZED MINIMAX	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404041
C	Borgs, C; Chayes, J; Lee, CE; Shah, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Borgs, Christian; Chayes, Jennifer; Lee, Christina E.; Shah, Devavrat			Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COMPLETION	The sparse matrix estimation problem consists of estimating the distribution of an n x n matrix Y, from a sparsely observed single instance of this matrix where the entries of Y are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filtering-style algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to 0 at the rate of O(d(2) (pn)(-2/5)) as long as omega(d(5) n) random entries from a total of n(2) entries of Y are observed (uniformly sampled), E [Y] has rank d, and the entries of Y have bounded support. The maximum squared error across all entries converges to 0 with high probability as long as we observe a little more, Omega (d(5) n ln(5) (n)) entries. Our results are the best known sample complexity results in this generality.	[Borgs, Christian; Chayes, Jennifer; Lee, Christina E.] Microsoft Res New England, One Mem Dr, Cambridge, MA 02142 USA; [Shah, Devavrat] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Microsoft; Massachusetts Institute of Technology (MIT)	Borgs, C (corresponding author), Microsoft Res New England, One Mem Dr, Cambridge, MA 02142 USA.	borgs@microsoft.com; jchayes@microsoft.com; celee@mit.edu; devavrat@mit.edu			NSF [CMMI-1462158, CMMI-1634259]; DARPA [W911NF-16-1-0551]; NSF Graduate Fellowship; Claude E. Shannon Research Assistantship	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF Graduate Fellowship(National Science Foundation (NSF)); Claude E. Shannon Research Assistantship	This work is supported in parts by NSF under grants CMMI-1462158 and CMMI-1634259, by DARPA under grant W911NF-16-1-0551, and additionally by a NSF Graduate Fellowship and Claude E. Shannon Research Assistantship.	Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Abbe Emmanuel, 2015, ADV NEURAL INFORM PR; Airoldi EM., 2013, ADV NEURAL INFORM PR, V26, P692; ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3; Anandkumar Animashree, 2013, C LEARN THEOR, P867; [Anonymous], 2015, ARXIV150903025; Austin Tim, 2012, NOT IAS WORKSH; Bordenave C, 2015, ANN IEEE SYMP FOUND, P1347, DOI 10.1109/FOCS.2015.86; Borgs C., 2016, ARXIV160107134; BORGS C, 2014, ARXIV14080744; Borgs C, 2014, ARXIV14012906; Borgs C., 2015, ARXIV150806675; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Goldberg D., 1992, COMMUN ACM; HOOVER DN, 1981, EXCHANGEABILITY PROB, P281; Karrer B, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.208702; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Klopp Olga, 2015, ANN STAT; Koren Y, 2011, RECOMMENDER SYSTEMS HANDBOOK, P145, DOI 10.1007/978-0-387-85820-3_5; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Lovasz L, 2012, LARGE NETWORKS GRAPH, V60; Massoulie L, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P694, DOI 10.1145/2591796.2591857; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Veitch V., 2015, ARXIV151203099; Wolfe P.J., 2013, ARXIV13095936; Xu J., 2014, C LEARNING THEORY, P903; Zhang Y., 2015, ARXIV150908588	44	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404076
C	Bringmann, K; Kolev, P; Woodruff, DP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bringmann, Karl; Kolev, Pavel; Woodruff, David P.			Approximation Algorithms for l(0)-Low Rank Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MATRIX FACTORIZATION; COMPONENT ANALYSIS; COMPRESSION; FIELDS	We study the l(0)-Low Rank Approximation Problem, where the goal is, given an m x n matrix A, to output a rank-k matrix A' for which parallel to A' - A parallel to(0) is minimized. Here, for a matrix B, parallel to B parallel to(0) denotes the number of its non-zero entries. This NP-hard variant of low rank approximation is natural for problems with no underlying metric, and its goal is to minimize the number of disagreeing data positions. We provide approximation algorithms which significantly improve the running time and approximation factor of previous work. For k > 1, we show how to find, in poly(mn) time for every k, a rank O(k log(n/k)) matrix A' for which parallel to A' - A parallel to(0) <= O(k(2) log(n/k)) OPT. To the best of our knowledge, this is the first algorithm with provable guarantees for the l(0)-Low Rank Approximation Problem for k > 1, even for bicriteria algorithms. For the well-studied case when k = 1, we give a (2+epsilon)-approximation in sublinear time, which is impossible for other variants of low rank approximation such as for the Frobenius norm. We strengthen this for the well-studied case of binary matrices to obtain a (1 + O(psi))-approximation in sublinear time, where psi = OPT /parallel to A parallel to(0). For small psi, our approximation factor is 1 + o(1).	[Bringmann, Karl; Kolev, Pavel] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany; [Woodruff, David P.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Max Planck Society; Carnegie Mellon University	Bringmann, K (corresponding author), Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany.	kbringma@mpi-inf.mpg.de; pkolev@mpi-inf.mpg.de; dwoodruf@cs.cmu.edu			Cluster of Excellence "Multimodal Computing and Interaction" within the Excellence Initiative of the German Federal Government	Cluster of Excellence "Multimodal Computing and Interaction" within the Excellence Initiative of the German Federal Government	This work has been funded by the Cluster of Excellence "Multimodal Computing and Interaction" within the Excellence Initiative of the German Federal Government.	Alekhnovich M, 2011, COMPUT COMPLEX, V20, P755, DOI 10.1007/s00037-011-0029-x; Alon N, 2009, LECT NOTES COMPUT SC, V5687, P339, DOI 10.1007/978-3-642-03685-9_26; [Anonymous], 2005, P 11 ACM SIGKDD INT; Arora S, 2016, SIAM J COMPUT, V45, P1582, DOI 10.1137/130913869; Belohlavek R, 2010, J COMPUT SYST SCI, V76, P3, DOI 10.1016/j.jcss.2009.05.002; Berman P, 2002, SIAM PROC S, P514; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chierichetti F., 2017, P 34 INT C MACHINE L, V70, P806; Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Dagum P, 2000, SIAM J COMPUT, V29, P1484, DOI 10.1137/S0097539797315306; Dan C., 2015, ARXIV E PRINTS; Fomin Fedor V., 2017, STACS; Gillis Nicolas, 2015, ARXIV150909236; Grigoriev D, 1980, J SOVIET MATH, V14, P1450, DOI [10.1007/BF01693976, DOI 10.1007/BF01693976]; Grigoriev D, 1976, NOTES LENINGRAD BRAN; Gutch HW, 2012, SIGNAL PROCESS, V92, P1796, DOI 10.1016/j.sigpro.2011.10.003; Jiang P, 2014, STUD BIG DATA, V1, P281, DOI 10.1007/978-3-642-40837-3_9; Kannan R, 2008, FOUND TRENDS THEOR C, V4, P157, DOI 10.1561/0400000025; Koyuturk M, 2005, IEEE T KNOWL DATA EN, V17, P447, DOI 10.1109/TKDE.2005.55; Koyuturk M, 2003, P 9 ACM SIGKDD INT C, P147, DOI [10.1145/956750.956770, DOI 10.1145/956750.956770]; Koyuturk M, 2006, ACM T MATH SOFTWARE, V32, P33, DOI 10.1145/1132973.1132976; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Meeds Edward, 2006, ADV NEURAL INFORM PR, P977; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; MIETTINEN P, 2014, TKDD, V8; Miettinen P, 2008, IEEE T KNOWL DATA EN, V20, P1348, DOI 10.1109/TKDE.2008.53; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Painsky A., 2015, ARXIV E PRINTS; Ravanbakhsh S., 2015, ARXIV E PRINTS; Razenshteyn I, 2016, ACM S THEORY COMPUT, P250, DOI 10.1145/2897518.2897639; Seppanen JK, 2003, LECT NOTES ARTIF INT, V2838, P423; Shen BH, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P757; Singliar T, 2006, J MACH LEARN RES, V7, P2189; Song Zhao, 2016, CORR; Song Zhao, 2018, ENTRYWISE LOW RANK A; Vaidya J, 2007, SACMAT'07: PROCEEDINGS OF THE 12TH ACM SYMPOSIUM ON ACCESS CONTROL MODELS AND TECHNOLOGIES, P175; Valiant Leslie G., 1977, P 2 INT S MATH FDN C, V53, P162, DOI [10.1007/3-540-08353-7_135, DOI 10.1007/3-540-08353-7_135]; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Yeredor A, 2011, IEEE T INFORM THEORY, V57, P5342, DOI 10.1109/TIT.2011.2145090; Zhang ZY, 2010, DATA MIN KNOWL DISC, V20, P28, DOI 10.1007/s10618-009-0145-2; Zhang ZG, 2007, IEEE DATA MINING, P391, DOI 10.1109/ICDM.2007.99	42	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406069
C	Caiafa, CF; Sporns, O; Saykin, AJ; Pestilli, F		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Caiafa, Cesar F.; Sporns, Olaf; Saykin, Andrew J.; Pestilli, Franco			Unified representation of tractography and diffusion-weighted MRI data using sparse multidimensional arrays	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				TENSOR DECOMPOSITIONS; COMPARTMENT MODELS; WHITE-MATTER; SIGNAL	Recently, linear formulations and convex optimization methods have been proposed to predict diffusion-weighted Magnetic Resonance Imaging (dMRI) data given estimates of brain connections generated using tractography algorithms. The size of the linear models comprising such methods grows with both dMRI data and connectome resolution, and can become very large when applied to modern data. In this paper, we introduce a method to encode dMRI signals and large connectomes, i.e., those that range from hundreds of thousands to millions of fascicles (bundles of neuronal axons), by using a sparse tensor decomposition. We show that this tensor decomposition accurately approximates the Linear Fascicle Evaluation (LiFE) model, one of the recently developed linear models. We provide a theoretical analysis of the accuracy of the sparse decomposed model, LiFESD, and demonstrate that it can reduce the size of the model significantly. Also, we develop algorithms to implement the optimization solver using the tensor representation in an efficient way.	[Caiafa, Cesar F.; Sporns, Olaf; Pestilli, Franco] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA; [Caiafa, Cesar F.] Consejo Nacl Invest Cient & Tecn, CIC PBA, IAR CCT La Plata, RA-1894 V Elisa, Argentina; [Saykin, Andrew J.] Indiana Univ Sch Med, Dept Radiol, Indianapolis, IN 46202 USA	Indiana University System; Indiana University Bloomington; Consejo Nacional de Investigaciones Cientificas y Tecnicas (CONICET); Indiana University System; Indiana University Bloomington	Caiafa, CF (corresponding author), Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA.; Caiafa, CF (corresponding author), Consejo Nacl Invest Cient & Tecn, CIC PBA, IAR CCT La Plata, RA-1894 V Elisa, Argentina.	ccaiafa@gmail.com; osporns@indiana.edu; asaykin@iupui.edu; franpest@indiana.edu	Jeong, Yongwook/N-7413-2016; Sporns, Olaf/A-1667-2010	Sporns, Olaf/0000-0001-7265-4036	NSF [IIS-1636893, BCS-1734853, BCS 1228397]; NIH [ULTTR001108]; Indiana University Areas of Emergent Research initiative Learning: Brains, Machines, Children	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Indiana University Areas of Emergent Research initiative Learning: Brains, Machines, Children	This research was supported by (NSF IIS-1636893; BCS-1734853; NIH ULTTR001108) to F.P. Data provided by Stanford University (NSF BCS 1228397). F.P. were partially supported by the Indiana University Areas of Emergent Research initiative Learning: Brains, Machines, Children.	Alexander D. C., 2017, HUMAN BRAIN MAPPING, P1; Anandkumar A, 2014, J MACH LEARN RES, V15, P2239; Barnathan M, 2011, NEUROIMAGE, V58, P537, DOI 10.1016/j.neuroimage.2011.06.043; Basser PJ, 2000, MAGNET RESON MED, V44, P625, DOI 10.1002/1522-2594(200010)44:4<625::AID-MRM17>3.0.CO;2-O; BASSER PJ, 1994, J MAGN RESON SER B, V103, P247, DOI 10.1006/jmrb.1994.1037; Bassett DS, 2017, NAT NEUROSCI, V20, P353, DOI 10.1038/nn.4502; Bastiani M, 2012, NEUROIMAGE, V62, P1732, DOI 10.1016/j.neuroimage.2012.06.002; Beckmann CF, 2005, NEUROIMAGE, V25, P294, DOI 10.1016/j.neuroimage.2004.10.043; Caiafa CF, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-09250-w; Caiafa CF, 2013, NEURAL COMPUT, V25, P186, DOI 10.1162/NECO_a_00385; Cichocki A, 2015, IEEE SIGNAL PROC MAG, V32, P145, DOI 10.1109/MSP.2013.2297439; Comon P, 2014, IEEE SIGNAL PROC MAG, V31, P44, DOI 10.1109/MSP.2014.2298533; Cong FY, 2015, J NEUROSCI METH, V248, P59, DOI 10.1016/j.jneumeth.2015.03.018; Daducci A., 2016, FRONTIERS NEUROSCIEN, V10, P1374; Daducci A, 2015, IEEE T MED IMAGING, V34, P246, DOI 10.1109/TMI.2014.2352414; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Descoteaux M, 2009, IEEE T MED IMAGING, V28, P269, DOI 10.1109/TMI.2008.2004424; Drysdale Andrew T, 2016, NAT MED, P1; GILBERT JR, 1992, SIAM J MATRIX ANAL A, V13, P333, DOI 10.1137/0613024; Glasser MF, 2016, NATURE, V536, P171, DOI 10.1038/nature18933; Hazlett HC, 2017, NATURE, V542, P348, DOI 10.1038/nature21369; Kim D, 2013, OPTIM METHOD SOFTW, V28, P1012, DOI 10.1080/10556788.2012.656368; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Li JN, 2016, IEEE SIGNAL PROC MAG, V33, P36, DOI 10.1109/MSP.2015.2510024; Miwakeichi F, 2004, NEUROIMAGE, V22, P1035, DOI 10.1016/j.neuroimage.2004.03.039; Morup M, 2006, NEUROIMAGE, V29, P938, DOI 10.1016/j.neuroimage.2005.08.005; Morup M, 2011, WIRES DATA MIN KNOWL, V1, P24, DOI 10.1002/widm.1; Nedjati-Gilani GL, 2017, NEUROIMAGE, V150, P119, DOI 10.1016/j.neuroimage.2017.02.013; Neher Peter Florian, 2017, BIORXIV, P1; Panagiotaki E, 2012, NEUROIMAGE, V59, P2241, DOI 10.1016/j.neuroimage.2011.09.081; Pestilli F, 2014, NAT METHODS, V11, P1058, DOI 10.1038/nmeth.3098; Rokem A, 2017, J VISION, V17, DOI 10.1167/17.2.4; Rokem A, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123272; Shah Parikshit, 2015, NIPS; Smith RE, 2015, NEUROIMAGE, V119, P338, DOI 10.1016/j.neuroimage.2015.06.092; Smith SM, 2015, NAT NEUROSCI, V18, P1565, DOI 10.1038/nn.4125; Sporns O, 2013, NAT METHODS, V10, P491, DOI 10.1038/nmeth.2485; Takemura H, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004692; Tax C. M. W., 2016, HUMAN BRAIN MAPPING, P1; Tournier JD, 2012, INT J IMAG SYST TECH, V22, P53, DOI 10.1002/ima.22005; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; van den Heuvel MP, 2016, TRENDS COGN SCI, V20, P345, DOI 10.1016/j.tics.2016.03.001; van den Heuvel MP, 2011, J NEUROSCI, V31, P15775, DOI 10.1523/JNEUROSCI.3539-11.2011; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Wandell BA, 2016, ANNU REV NEUROSCI, V39, P103, DOI 10.1146/annurev-neuro-070815-013815; Wimalawarne K., 2014, NIPS; Yu Y, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092714; Zhao Q., 2011, ADV NEURAL INFORM PR, V24, P1269; Zhao QB, 2013, IEEE T PATTERN ANAL, V35, P1660, DOI 10.1109/TPAMI.2012.254; Zhu DJ, 2016, I S BIOMED IMAGING, P554, DOI 10.1109/ISBI.2016.7493329	51	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404040
C	Cecchi, F; Hegde, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cecchi, Fabio; Hegde, Nidhi			Adaptive Active Hypothesis Testing under Limited Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SEQUENTIAL DESIGN	We consider the problem of active sequential hypothesis testing where a Bayesian decision maker must infer the true hypothesis from a set of hypotheses. The decision maker may choose for a set of actions, where the outcome of an action is corrupted by independent noise. In this paper we consider a special case where the decision maker has limited knowledge about the distribution of observations for each action, in that only a binary value is observed. Our objective is to infer the true hypothesis with low error, while minimizing the number of action sampled. Our main results include the derivation of a lower bound on sample size for our system under limited knowledge and the design of an active learning policy that matches this lower bound and outperforms similar known algorithms.	[Cecchi, Fabio] Eindhoven Univ Technol, Eindhoven, Netherlands; [Hegde, Nidhi] Nokia Bell Labs, Paris, France	Eindhoven University of Technology; Nokia Corporation	Cecchi, F (corresponding author), Eindhoven Univ Technol, Eindhoven, Netherlands.	f.cecchi@tue.nl; nidhi.hegde@nokia-bell-labs.com	Jeong, Yongwook/N-7413-2016	Cecchi, Fabio/0000-0002-2705-1479				ALBERT AE, 1961, ANN MATH STAT, V32, P774, DOI 10.1214/aoms/1177704973; Berry SM, 2010, CH CRC BIOSTAT SER, P1, DOI 10.1201/EBK1439825488; CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205; Ghosh B., 1991, HDB SEQUENTIAL ANAL, V1; Hui SC, 2000, INFORM MANAGE-AMSTER, V38, P1, DOI 10.1016/S0378-7206(00)00051-3; KIEFER J, 1963, ANN MATH STAT, V34, P705, DOI 10.1214/aoms/1177704000; Lalitha A, 2014, IEEE INT SYMP INFO, P551, DOI 10.1109/ISIT.2014.6874893; Naghshvar M, 2012, IEEE INT SYMP INFO; Naghshvar M, 2013, ANN STAT, V41, P2703, DOI 10.1214/13-AOS1144; Naghshvar M, 2012, ANN ALLERTON CONF, P1626, DOI 10.1109/Allerton.2012.6483415; Nowak R., 2009, ADV NEURAL INFORM PR, P1366; Olfati-Saber R, 2007, P IEEE, V95, P215, DOI 10.1109/JPROC.2006.887293; Vaidhiyan NK, 2012, IEEE INT SYMP INFO	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404011
