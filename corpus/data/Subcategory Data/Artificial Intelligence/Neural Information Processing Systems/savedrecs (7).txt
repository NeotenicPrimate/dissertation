PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Jang, M; Kim, S; Suh, C; Oh, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jang, Minje; Kim, Sunghyun; Suh, Changho; Oh, Sewoong			Optimal Sample Complexity of M-wise Data for Top-K Ranking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CHOICE	We explore the top-K rank aggregation problem in which one aims to recover a consistent ordering that focuses on top-K ranked items based on partially revealed preference information. We examine an M-wise comparison model that builds on the Plackett-Luce (PL) model where for each sample, M items are ranked according to their perceived utilities modeled as noisy observations of their underlying true utilities. As our result, we characterize the minimax optimality on the sample size for top-K ranking. The optimal sample size turns out to be inversely proportional to M. We devise an algorithm that effectively converts M-wise samples into pairwise ones and employs a spectral method using the refined data. In demonstrating its optimality, we develop a novel technique for deriving tight l(infinity) estimation error bounds, which is key to accurately analyzing the performance of top-K ranking algorithms, but has been challenging. Recent work relied on an additional maximum-likelihood estimation (MLE) stage merged with a spectral method to attain good estimates in l(infinity) error to achieve the limit for the pairwise model. In contrast, although it is valid in slightly restricted regimes, our result demonstrates a spectral method alone to be sufficient for the general M-wise model. We run numerical experiments using synthetic data and confirm that the optimal sample size decreases at the rate of 1/M. Moreover, running our algorithm on real-world data, we find that its applicability extends to settings that may not fit the PL model.	[Jang, Minje; Suh, Changho] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Kim, Sunghyun] Elect & Telecommun Res Inst, Daejeon, South Korea; [Oh, Sewoong] UIUC, Ind & Enterprise Syst Engn Dept, Champaign, IL USA	Korea Advanced Institute of Science & Technology (KAIST); Electronics & Telecommunications Research Institute - Korea (ETRI); University of Illinois System; University of Illinois Urbana-Champaign	Jang, M (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	jmj427@kaist.ac.kr; koishkim@etri.re.kr; chsuh@kaist.ac.kr; swoh@illinois.edu			Institute for Information & communications Technology Promotion(IITP) - Korea government(MSIT) [2017-0-00694]	Institute for Information & communications Technology Promotion(IITP) - Korea government(MSIT)	This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (2017-0-00694, Coding for High-Speed Distributed Networks).	Ailon N., 2007, ARXIV07102889; Ailon N, 2012, J MACH LEARN RES, V13, P137; Ammar Ammar, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P776; Baltrunas L., 2010, P 4 ACM C RECOMMENDE, P119, DOI DOI 10.1145/1864708.1864733; BELL DE, 1982, OPER RES, V30, P961, DOI 10.1287/opre.30.5.961; Bergstrom CT, 2008, J NEUROSCI, V28, P11433, DOI 10.1523/JNEUROSCI.0003-08.2008; Bonacich P, 2001, SOC NETWORKS, V23, P191, DOI 10.1016/S0378-8733(01)00038-7; Borda J.C., 1781, MEMOIRE ELECTIONS SC; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Braverman M, 2016, ACM S THEORY COMPUT, P851, DOI 10.1145/2897518.2897642; Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X; CAPLIN A, 1991, ECONOMETRICA, V59, P1, DOI 10.2307/2938238; Chen X., 2013, WSDM, P193; Chen YX, 2015, PR MACH LEARN RES, V37, P371; Cheng W., 2010, ICML, P215; Cooley O., 2016, ELECTRON J COMB, P2; Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]; FISHBURN PC, 1982, J MATH PSYCHOL, V26, P31, DOI 10.1016/0022-2496(82)90034-7; Ford L.R., 1957, AM MATH MON, V64, P28, DOI DOI 10.2307/2308513; Graham L., 1982, ECON J, V92, P805; Guiver J., 2009, P 26 ANN INT C MACHI, P377; Hajek S., 2014, ADV NEURAL INFORM PR, P1475; HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943; Heckel R., 2016, ARXIV160608842; Hunter DR, 2004, ANN STAT, V32, P384; Jamieson Kevin G, 2011, ADV NEURAL INFORM PR, P2240; Janson S, 2004, RANDOM STRUCT ALGOR, V24, P234, DOI 10.1002/rsa.20008; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Khetan A, 2016, J MACH LEARN RES, V17; Luce R, 1959, INDIVIDUAL CHOICE BE; Maystre Lucas, 2015, ADV NEURAL INFORM PR, P172; MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093; Mohajer S, 2017, PR MACH LEARN RES, V70; Negahban S, 2017, OPER RES, V65, P266, DOI 10.1287/opre.2016.1534; OH S, 2015, ADV NEURAL INFORM PR, P1909; Pardo B., 2002, Computer Music Journal, V26, P27, DOI 10.1162/014892602760137167; Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567; Rajkumar A, 2014, PR MACH LEARN RES, V32; Seeley JR, 1949, CAN J PSYCHOLOGY, V3, P234, DOI 10.1037/h0084096; Shah N. B., 2015, ARXIV151208949; Soufiani HA, 2014, ADV NEUR IN, V27; Soufiani Hossein Azari, 2013, P 26 INT C NEUR INF, P2706; Szorenyi B., 2015, ADV NEURAL INFORM PR, V28, P604; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vigna S, 2016, NETW SCI, V4, P433, DOI 10.1017/nws.2016.21; Walker J, 2002, MATH SOC SCI, V43, P303, DOI 10.1016/S0165-4896(02)00023-9; Wei T. -H., 1952, THESIS	47	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401070
C	Karami, M; White, M; Schuurmans, D; Szepesvari, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Karami, Mahdi; White, Martha; Schuurmans, Dale; Szepesvari, Csaba			Multi-view Matrix Factorization for Linear Dynamical System Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider maximum likelihood estimation of linear dynamical systems with generalized-linear observation models. Maximum likelihood is typically considered to be hard in this setting since latent states and transition parameters must be inferred jointly. Given that expectation-maximization does not scale and is prone to local minima, moment-matching approaches from the subspace identification literature have become standard, despite known statistical efficiency issues. In this paper, we instead reconsider likelihood maximization and develop an optimization based strategy for recovering the latent states and transition parameters. Key to the approach is a two-view reformulation of maximum likelihood estimation for linear dynamical systems that enables the use of global optimization algorithms for matrix factorization. We show that the proposed estimation strategy outperforms widely-used identification algorithms such as subspace identification methods, both in terms of accuracy and runtime.	[Karami, Mahdi; White, Martha; Schuurmans, Dale; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada	University of Alberta	Karami, M (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	karami1@ualberta.ca; whitem@ualberta.ca; daes@ualberta.ca; szepesva@ualberta.ca	White, Martha/AAF-7066-2020; Jeong, Yongwook/N-7413-2016	White, Martha/0000-0002-5356-2950; 	Alberta Machine Intelligence Institute; NSERC	Alberta Machine Intelligence Institute; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported in part by the Alberta Machine Intelligence Institute and NSERC. During this work, M. White was with the Department of Computer Science, Indiana University.	Andersson S., 2009, ANN STAT; ASTROM KJ, 1980, AUTOMATICA, V16, P551, DOI 10.1016/0005-1098(80)90078-3; Bach F., 2008, ARXIV08121869V1; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boots B., 2012, INT C MACH LEARN; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Buesing L., 2012, NETWORK COMPUTATION; Buesing L., 2012, ADV NEURAL INFORM PR; CRAMER H., 1946, Mathematical methods of statistics.; Foster D., 2012, ARXIV12036130V1; Gelfand AE, 2010, CH CRC HANDB MOD STA, P517; Ghahramani Zoubin, 1996, CRGTR961 U TOR; Haeffele B., 2014, INT C MACH LEARN; Hsu D., 2012, J COMPUTER SYSTEM SC; Katayama T., 2006, SUBSPACE METHODS SYS; Ljung L, 1999, SYSTEM IDENTIFICATIO, DOI DOI 10.1002/047134608X.W1046.PUB2; Macke J., 2015, ADV STATE SPACE METH; Moonen M., 1993, IEEE T AUTOMATIC CON; Parikh Neal, 2013, FDN TRENDS OPTIMIZAT; Roweis S., 1999, NEURAL COMPUTATION; Siddiqi S., 2007, ADV NEURAL INFORM PR; Song L., 2010, P 27 INT C MACH LEAR, P991; Van Overschee P., 1994, AUTOMATICA; Viberg M., 1995, AUTOMATICA; White M, 2015, AAAI CONF ARTIF INTE, P3080; White Martha, 2012, ADV NEURAL INFORM PR, P1682; Yu H.-F., 2015, J PHYS C SERIES, DOI DOI 10.48550/ARXIV.1509.08333; Yu Y., 2014, ARXIV14104828; Zhang X., 2012, ADV NEURAL INFORM PR; Zhao H., 2014, ARXIV14064631	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407018
C	Koren, T; Livni, R; Mansour, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Koren, Tomer; Livni, Roi; Mansour, Yishay			Multi-Armed Bandits with Metric Movement Costs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MARKOV DECISION-PROCESSES; SPACES; RATES	We consider the non-stochastic Multi-Armed Bandit problem in a setting where there is a fixed and known metric on the action space that determines a cost for switching between any pair of actions. The loss of the online learner has two components: the first is the usual loss of the selected actions, and the second is an additional loss due to switching between actions. Our main contribution gives a tight characterization of the expected minimax regret in this setting, in terms of a complexity measure C of the underlying metric which depends on its covering numbers. In finite metric spaces with k actions, we give an efficient algorithm that achieves regret of the form (O) over tilde (max{(CT2/3)-T-1/3, root kT}), and show that this is the best possible. Our regret bound generalizes previous known regret bounds for some special cases: (i) the unit-switching cost regret (Theta) over tilde (max{k(1/3)T(2/3), root kT}) where C = Theta(k), and (ii) the interval metric with regret (Theta) over tilde (max{T-2/3, root kT})where C = Theta(1). For infinite metrics spaces with Lipschitz loss functions, we derive a tight regret bound of (Theta) over tilde (Td+1/d+2) where d >= 1 is the Minkowski dimension of the space, which is known to be tight even when there are no switching costs.	[Koren, Tomer] Google Brain, Mountain View, CA 94043 USA; [Livni, Roi] Princeton Univ, Princeton, NJ 08544 USA; [Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Mansour, Yishay] Google, Mountain View, CA USA	Google Incorporated; Princeton University; Tel Aviv University; Google Incorporated	Koren, T (corresponding author), Google Brain, Mountain View, CA 94043 USA.	tkoren@google.com; rlivni@cs.princeton.edu; mansour@cs.tau.ac.il	Jeong, Yongwook/N-7413-2016		Eric and Wendy Schmidt Foundation; Israel Science Foundation; United States-Israel Binational Science Foundation (BSF); Israeli Centers of Research Excellence (I-CORE) program [4/11]	Eric and Wendy Schmidt Foundation; Israel Science Foundation(Israel Science Foundation); United States-Israel Binational Science Foundation (BSF)(US-Israel Binational Science Foundation); Israeli Centers of Research Excellence (I-CORE) program	RL is supported in funds by the Eric and Wendy Schmidt Foundation for strategic innovations. YM is supported in part by a grant from the Israel Science Foundation, a grant from the United States-Israel Binational Science Foundation (BSF), and the Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11).	AGRAWAL R, 1988, IEEE T AUTOMAT CONTR, V33, P899, DOI 10.1109/9.7243; Asawa M, 1996, IEEE T AUTOMAT CONTR, V41, P328, DOI 10.1109/9.486316; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2007, LECT NOTES COMPUT SC, V4539, P454, DOI 10.1007/978-3-540-72927-3_33; BANKS JS, 1994, ECONOMETRICA, V62, P687, DOI 10.2307/2951664; Bartal Y, 1996, AN S FDN CO, P184, DOI 10.1109/SFCS.1996.548477; BORODIN A, 1992, J ACM, V39, P745, DOI 10.1145/146585.146588; Borodin A., 1998, ONLINE COMPUTATION C; Bubeck S., 2011, J MACHINE LEARNING R, V12, P1587; Cope EW, 2009, IEEE T AUTOMAT CONTR, V54, P1243, DOI 10.1109/TAC.2009.2019797; Dekel O, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P459, DOI 10.1145/2591796.2591868; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; Fakcharoenphol J, 2004, J COMPUT SYST SCI, V69, P485, DOI 10.1016/j.jcss.2004.04.01; Feldman M., 2016, ANN C NEUR INF PROC; Geulen S., 2010, P 23 ANN C LEARN THE, P132; Gittins J. C., 2011, MULTIARMED BANDIT AL, V2nd; Guha S, 2009, LECT NOTES COMPUT SC, V5556, P496, DOI 10.1007/978-3-642-02930-1_41; Gyorgy A, 2014, IEEE T INFORM THEORY, V60, P2823, DOI 10.1109/TIT.2014.2307062; Jun TS, 2004, ECONOMIST-NETHERLAND, V152, P513, DOI 10.1007/s10645-004-2477-z; Kleinberg R., 2004, ADV NEURAL INFORM PR, V17, P697; Kleinberg R, 2010, PROC APPL MATH, V135, P827; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Koren T., 2017, COLT; Koren T., 2017, ARXIV171008997; Magureanu S., 2014, P C LEARN THEOR COLT, P975; Neu G, 2014, IEEE T AUTOMAT CONTR, V59, P676, DOI 10.1109/TAC.2013.2292137; Ortner R, 2010, THEOR COMPUT SCI, V411, P2684, DOI 10.1016/j.tcs.2010.04.005; SLIVKINS A, 2011, ADV NEURAL INFORM PR, P1602; Slivkins A, 2013, J MACH LEARN RES, V14, P399; Tao T., 2009, 245C NOTES 5 HAUSDOR; Tewari A., 2012, P 29 INT C MACH LEAR; Yu J.Y., 2011, P 28 INT C MACH LEAR	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404019
C	Kumagai, W		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kumagai, Wataru			Regret Analysis for Continuous Dueling Bandit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The dueling bandit is a learning framework wherein the feedback information in the learning process is restricted to a noisy comparison between a pair of actions. In this research, we address a dueling bandit problem based on a cost function over a continuous space. We propose a stochastic mirror descent algorithm and show that the algorithm achieves an O(root T log T)-regret bound under strong convexity and smoothness assumptions for the cost function. Subsequently, we clarify the equivalence between regret minimization in dueling bandit and convex optimization for the cost function. Moreover, when considering a lower bound in convex optimization, our algorithm is shown to achieve the optimal convergence rate in convex optimization and the optimal regret in dueling bandit except for a logarithmic factor.	[Kumagai, Wataru] RIKEN, Ctr Adv Intelligence Project, Chuo Ku, 1-4-1 Nihonbashi, Tokyo 1030027, Japan	RIKEN	Kumagai, W (corresponding author), RIKEN, Ctr Adv Intelligence Project, Chuo Ku, 1-4-1 Nihonbashi, Tokyo 1030027, Japan.	wataru.kumagai@riken.jp	Jeong, Yongwook/N-7413-2016		JSPS KAKENHI [17K12653]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	We would like to thank Professor Takafumi Kanamori for helpful comments. This work was supported by JSPS KAKENHI Grant Number 17K12653.	Agarwal A., 2010, P COLT, P28; Ailon Nir, 2014, ARXIV14053396; Bubeck Sebastien, 2014, ARXIV14121587; Busa-Fekete R., 2013, ICML; Busa-Fekete Robert, 2014, INT C MACH LEARN, P1071; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Griva I, 2009, OTHER TITL APPL MATH, V108, P1, DOI 10.1137/1.9780898717730; Hazan E., 2014, ADV NEURAL INFORM PR, V27, P784; Jamieson K. G., 2015, AISTATS; Matsui K., 2016, J GLOBAL OPTIM, P1; Nesterov Y., 1994, INTERIOR POINT POLYN, V13; Shamir O., 2013, P C LEARN THEOR, P3; Shamir O, 2017, J MACH LEARN RES, V18; Urvoy T., 2013, INT C MACH LEARN, V28, P91; Yue Y., 2009, ICML, P1201; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zhang LJ, 2016, PR MACH LEARN RES, V48; Zoghi M, 2014, PR MACH LEARN RES, V32, P10	21	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401051
C	Ling, H; Fidler, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ling, Huan; Fidler, Sanja			Teaching Machines to Describe Images via Natural Language Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. In particular, we first train a captioning model on a subset of images paired with human written captions. We then let the model describe new images and collect human feedback on the generated descriptions. We propose a hierarchical phrase-based captioning model, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback on new images our model learns to perform better than when given human written captions on these images.	[Ling, Huan; Fidler, Sanja] Univ Toronto, Toronto, ON, Canada; [Fidler, Sanja] Vector Inst, Toronto, ON, Canada	University of Toronto	Ling, H (corresponding author), Univ Toronto, Toronto, ON, Canada.	linghuan@cs.toronto.edu; fidler@cs.toronto.edu	Jeong, Yongwook/N-7413-2016		NSERC	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	We gratefully acknowledge the support from NVIDIA for their donation of the GPUs used for this research. This work was partially supported by NSERC. We also thank Relu Patrascu for infrastructure support.	[Anonymous], 2015, P INT C MACH LEARN; [Anonymous], [No title captured]; Ba J., 2017, P 3 INT C LEARN REPR; Dai B., 2017, ARXIV170306029; Das A., 2016, ARXIV161108669; Griffith S, 2013, NIPS; Judah K., 2010, AAAI; Kaplan R., 2017, ARXIV170405539; Karpathy A., 2015, CVPR; Kiros R., 2014, CORR; Knox W. B., 2012, INT C AUT AG MULT SY; Knox W. B., 2013, INT C SOC ROB; Krause J., 2017, CVPR; Kuhlmann G., 2004, AAAI WORKSH SUP CONT; Levine S, 2016, J MACH LEARN RES, V17; Li J., 2016, ICLR, P1; Li Jiwei, 2016, P 2016 C EMP METH NA; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S., 2016, P IEEE INT C COMP VI; MACLIN R, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P694; Manning Christopher D., 2014, ICLR; Matari Maja J., 2017, SCI ROBOTICS, V2; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Parikh D., 2012, EUR C COMP VIS ECCV; Pascanu Razvan, 2014, ASS COMPUTATIONAL LI, P55; Ranzato MarcAurelio, 2015, ARXIV; Rennie S. J., 2016, CORR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simo-Serra Edgar, 2015, CVPR; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO, P23; Tan Ying Hua, 2016, ACCV; Thomaz A. L., 2006, AAAI; Westlund J. Kory, 2016, INT C HUM ROB INT; Weston Jason, 2016, ARXIV160406045; Williams R. J., 1992, MACHINE LEARNING; Xu K., 2015, ICML; Yu Yanchao, 2016, P SIGDIAL; Yu Yanchao, 2017, WORKSH VIS LANG	39	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405015
C	Mariet, Z; Sra, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mariet, Zelda; Sra, Suvrit			Elementary Symmetric Polynomials for Optimal Experimental Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHM; PERFORMANCE; DERIVATIVES	We revisit the classical problem of optimal experimental design (OED) under a new mathematical model grounded in a geometric motivation. Specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture "partial volumes" and offer a graded interpolation between the widely used A-optimal design and D-optimal design models, obtaining each of them as special cases. We analyze properties of our models, and derive both greedy and convex-relaxation algorithms for computing the associated designs. Our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate a curious phenomenon concerning our greedy method. Finally, as a byproduct, we obtain new results on the theory of elementary symmetric polynomials that may be of independent interest.	[Mariet, Zelda; Sra, Suvrit] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mariet, Z (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	zelda@csail.mit.edu; suvrit@mit.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1409802]; DARPA Fundamental Limits of Learning grant [W911NF-16-1-0551]	NSF(National Science Foundation (NSF)); DARPA Fundamental Limits of Learning grant	Suvrit Sra acknowledges support from NSF grant IIS-1409802 and DARPA Fundamental Limits of Learning grant W911NF-16-1-0551.	Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287; BARNES ER, 1982, IBM J RES DEV, V26, P759, DOI 10.1147/rd.266.0759; Bauschke HH, 2001, CAN J MATH, V53, P470, DOI 10.4153/CJM-2001-020-6; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bhatia Rajendra, 1997, MATRIX ANAL, DOI 10.1007/978-1-4612-0653-8; Boyd S, 2004, CONVEX OPTIMIZATION; Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939; Cohn D.A., 1994, ADV NEURAL INFORM PR, V6, P679, DOI DOI 10.1016/j.jspi.2009.08.006; Cominetti R, 2014, MATH PROGRAM COMPUT, V6, P151, DOI 10.1007/s12532-014-0066-y; Davenport MA, 2016, IEEE T SIGNAL PROCES, V64, P5437, DOI 10.1109/TSP.2016.2597130; Davis TA, 2016, ACM T MATH SOFTWARE, V42, DOI 10.1145/2828635; Dette H, 2006, STAT SINICA, V16, P789; Dolia AN, 2006, LECT NOTES COMPUT SC, V4212, P630; Dolia E. N., 2004, P 7 INT C SIGN IM PR, P73; ELFVING G, 1952, ANN MATH STAT, V23, P255, DOI 10.1214/aoms/1177729442; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; Gu YJ, 2013, NEURAL COMPUT APPL, V23, P2085, DOI 10.1007/s00521-012-1155-3; He XF, 2010, IEEE T IMAGE PROCESS, V19, P254, DOI 10.1109/TIP.2009.2032342; Horel T., 2014, BUDGET FEASIBLE MECH, P719; Horn R. A., 1986, MATRIX ANAL; Jackson DA, 2004, ENVIRONMETRICS, V15, P129, DOI 10.1002/env.628; Jain T, 2011, LINEAR ALGEBRA APPL, V435, P1111, DOI 10.1016/j.laa.2011.02.026; Jozsa R, 2015, J MATH PHYS, V56, DOI 10.1063/1.4922317; Khuri AI, 2006, STAT SCI, V21, P376, DOI 10.1214/088342306000000105; KIEFER J, 1975, BIOMETRIKA, V62, P277, DOI 10.1093/biomet/62.2.277; Krause A, 2008, J MACH LEARN RES, V9, P235; Lewis AS, 1996, MATH OPER RES, V21, P576, DOI 10.1287/moor.21.3.576; Liu ZK, 2016, COMPUTATIONAL THERMODYNAMICS OF MATERIALS, DOI 10.1017/CBO9781139018265; Macdonald I. G., 2015, SYMMETRIC FUNCTIONS, V2nd; MILLER AJ, 1994, APPL STAT-J ROY ST C, V43, P669, DOI 10.2307/2986264; MUIR WW, 1974, P EDINBURGH MATH SOC, V19, P109, DOI 10.1017/S001309150001021X; Pukelsheim F, 2006, CLASS APPL MATH, V50, P1, DOI 10.1137/1.9780898719109; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; Sagnol G., 2010, THESIS; Lasheras FS, 2010, MATH COMPUT MODEL, V52, P1169, DOI 10.1016/j.mcm.2010.03.007; SCHEIN AI, 2004, A OPTIMALITY ACTIVE; Shpilka A, 2001, COMPUT COMPLEX, V10, P1, DOI 10.1007/PL00001609; SILVEY SD, 1978, COMMUN STAT A-THEOR, V7, P1379, DOI 10.1080/03610927808827719; Smith J. D., 2017, CORR; Sra S, 2015, SIAM J OPTIMIZ, V25, P713, DOI 10.1137/140978168; Sun P, 2004, OPER RES, V52, P690, DOI 10.1287/opre.1040.0115; Todd MJ, 2016, MOS-SIAM SER OPTIMIZ, P1, DOI 10.1137/1.9781611974386; Vandenberghe L, 1998, SIAM J MATRIX ANAL A, V19, P499, DOI 10.1137/S0895479896303430; Wang ZF, 2016, J COMB OPTIM, V31, P29, DOI 10.1007/s10878-014-9707-3; Xygkis T. C., 2016, IEEE T SMART GRID; Yeh IC, 1998, CEMENT CONCRETE RES, V28, P1797, DOI 10.1016/S0008-8846(98)00165-3; Yu YM, 2010, ANN STAT, V38, P1593, DOI 10.1214/09-AOS761	48	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402019
C	Mattila, R; Rojas, CR; Krishnamurthy, V; Wahlberg, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mattila, Robert; Rojas, Cristian R.; Krishnamurthy, Vikram; Wahlberg, Bo			Inverse Filtering for Hidden Markov Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper considers a number of related inverse filtering problems for hidden Markov models (HMMs). In particular, given a sequence of state posteriors and the system dynamics; i) estimate the corresponding sequence of observations, ii) estimate the observation likelihoods, and iii) jointly estimate the observation likelihoods and the observation sequence. We show how to avoid a computationally expensive mixed integer linear program (MILP) by exploiting the algebraic structure of the HMM filter using simple linear algebra operations, and provide conditions for when the quantities can be uniquely reconstructed. We also propose a solution to the more general case where the posteriors are noisily observed. Finally, the proposed inverse filtering algorithms are evaluated on real-world polysomnographic data used for automatic sleep segmentation.	[Mattila, Robert; Rojas, Cristian R.; Wahlberg, Bo] KTH Royal Inst Technol, Dept Automat Control, Stockholm, Sweden; [Krishnamurthy, Vikram] Cornell Univ, Cornell Tech, Ithaca, NY 14853 USA	Royal Institute of Technology; Cornell University	Mattila, R (corresponding author), KTH Royal Inst Technol, Dept Automat Control, Stockholm, Sweden.	rmattila@kth.se; crro@kth.se; vikramk@cornell.edu; bo@kth.se	Jeong, Yongwook/N-7413-2016		Swedish Research Council [2016-06079]; U.S. Army Research Office [12346080]; National Science Foundation [1714180]	Swedish Research Council(Swedish Research CouncilEuropean Commission); U.S. Army Research Office; National Science Foundation(National Science Foundation (NSF))	This work was partially supported by the Swedish Research Council under contract 2016-06079, the U.S. Army Research Office under grant 12346080 and the National Science Foundation under grant 1714180. The authors would like to thank Alexandre Proutiere for helpful comments during the preparation of this work.	Anderson B. D. O., 1979, OPTIMAL FILTERING; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Caplin A, 2015, AM ECON REV, V105, P2183, DOI 10.1257/aer.20140117; Cappe O., 2005, SPR S STAT; Chen J., 2012, ROBUST MODEL BASED F, V3; Chen Y, 2015, IEEE ENG MED BIO, P530, DOI 10.1109/EMBC.2015.7318416; Choi J., 2012, ADV NEURAL INFORM PR; Elliott R.J., 1995, HIDDEN MARKOV MODELS; Flexer A, 2002, APPL ARTIF INTELL, V16, P199, DOI 10.1080/088395102753559271; Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215; Gustafsson F., 2000, ADAPTIVE FILTERING C; Hadfield-Menell D., 2016, ADV NEURAL INFORM PR, P3909; Hornik K, 2012, J STAT SOFTW, V50, P1; Kalman R.E., 1964, J FLUID ENG-T ASME, V86, P51, DOI [10.1115/1.3653115, DOI 10.1115/1.3653115]; Klein Edouard, 2012, ADV NEURAL INFORM PR; Koller D., 2009, PROBABILISTIC GRAPHI; Krishnamurthy V, 2016, PARTIALLY OBSERVED M; Levine S., 2011, P ADV NEUR INF PROC, V24; Pan ST, 2012, BIOMED ENG ONLINE, V11, DOI 10.1186/1475-925X-11-52; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Terzano MG, 2001, SLEEP MED, V2, P537, DOI 10.1016/S1389-9457(01)00149-6; Varian HR., 1992, MICROECONOMIC ANAL	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404027
C	Mazumdar, A; Pal, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mazumdar, Arya; Pal, Soumyabrata			Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RECOVERY	Source coding is the canonical problem of data compression in information theory. In a locally encodable source coding, each compressed bit depends on only few bits of the input. In this paper, we show that a recently popular model of semisupervised clustering is equivalent to locally encodable source coding. In this model, the task is to perform multiclass labeling of unlabeled elements. At the beginning, we can ask in parallel a set of simple queries to an oracle who provides (possibly erroneous) binary answers to the queries. The queries cannot involve more than two (or a fixed constant number Delta of) elements. Now the labeling of all the elements (or clustering) must be performed based on the (noisy) query answers. The goal is to recover all the correct labelings while minimizing the number of such queries. The equivalence to locally encodable source codes leads us to find lower bounds on the number of queries required in variety of scenarios. We are also able to show fundamental limitations of pairwise 'same cluster' queries - and propose pairwise AND queries, that provably performs better in many situations.	[Mazumdar, Arya; Pal, Soumyabrata] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Mazumdar, A (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	arya@cs.umass.edu; soumyabratap@umass.edu			NSF [CCF-BSF 1618512, CCF 1642550]; NSF CAREER Award [CCF 1642658]	NSF(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	This research is supported in parts by NSF Awards CCF-BSF 1618512, CCF 1642550 and an NSF CAREER Award CCF 1642658. The authors thank Barna Saha for many discussions on the topics of this paper. The authors also thank the volunteers who participated in the crowdsourcing experiments for this paper.	Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Ahn K, 2016, ANN ALLERTON CONF, P657, DOI 10.1109/ALLERTON.2016.7852294; Alon N., 2004, PROBABILISTIC METHOD; Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216; Buhrman H, 2002, SIAM J COMPUT, V31, P1723, DOI 10.1137/S0097539702405292; Chandar V. B., 2010, THESIS; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Firmani D, 2016, PROC VLDB ENDOW, V9, P384; Gruenheid Anja, 2015, FAULT TOLERANT ENTIT; Hajek B, 2016, IEEE T INFORM THEORY, V62, P5918, DOI 10.1109/TIT.2016.2594812; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Lahouti F., 2016, P ADV NEUR INF PROC, P5059; Liu Q., 2012, NIPS, P692; Makhdoumi A, 2015, IEEE ICC, P4394, DOI 10.1109/ICC.2015.7249014; Massey J. L., 1977, TECHNICAL REPORT; Mazumdar A., 2017, THIRT 1 AAAI C ART I; Mazumdar A., 2017, ADV NEURAL INFORM PR; Mazumdar A, 2015, IEEE INT SYMP INFO, P2984, DOI 10.1109/ISIT.2015.7283004; Mazumdar A, 2014, IEEE J SEL AREA COMM, V32, P976, DOI 10.1109/JSAC.2014.140517; Montanari A, 2008, IEEE INT SYMP INFO, P2474, DOI 10.1109/ISIT.2008.4595436; Ng AY, 2002, ADV NEUR IN, V14, P849; Nushi B., 2015, ARXIV151200537; Pananjady A, 2015, IEEE INT SYMP INFO, P2979, DOI 10.1109/ISIT.2015.7283003; Patrascu M, 2008, ANN IEEE SYMP FOUND, P305, DOI 10.1109/FOCS.2008.83; Prelec D, 2017, NATURE, V541, P532, DOI 10.1038/nature21054; Vempaty A, 2014, IEEE J-STSP, V8, P667, DOI 10.1109/JSTSP.2014.2316116; Verroios V, 2015, PROC INT CONF DATA, P219, DOI 10.1109/ICDE.2015.7113286; Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982; Vinayak R.K., 2016, ADV NEURAL INFORM PR, P1316; Viola E, 2012, SIAM J COMPUT, V41, P1593, DOI 10.1137/090766619; Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263; Zhou D., 2012, ADV NEURAL INFORM PR, P2195	33	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406054
C	Minsker, S; Wei, XH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Minsker, Stanislav; Wei, Xiaohan			Estimation of the covariance structure of heavy-tailed distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				HIGH-DIMENSIONAL COVARIANCE; MATRICES OPTIMAL RATES; ADAPTIVE ESTIMATION	We propose and analyze a new estimator of the covariance matrix that admits strong theoretical guarantees under weak assumptions on the underlying distribution, such as existence of moments of only low order. While estimation of covariance matrices corresponding to sub-Gaussian distributions is well-understood, much less in known in the case of heavy-tailed data. As K. Balasubramanian and M. Yuan write(1), "data from real-world experiments oftentimes tend to be corrupted with outliers and/or exhibit heavy tails. In such cases, it is not clear that those covariance matrix estimators .. remain optimal" and "..what are the other possible strategies to deal with heavy tailed distributions warrant further studies." We make a step towards answering this question and prove tight deviation inequalities for the proposed estimator that depend only on the parameters controlling the "intrinsic dimension" associated to the covariance matrix (as opposed to the dimension of the ambient space); in particular, our results are applicable in the case of high-dimensional observations.	[Minsker, Stanislav] Univ Southern Calif, Dept Math, Los Angeles, CA 90007 USA; [Wei, Xiaohan] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90007 USA	University of Southern California; University of Southern California	Minsker, S (corresponding author), Univ Southern Calif, Dept Math, Los Angeles, CA 90007 USA.	minsker@usc.edu; xiaohanw@usc.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [NSF DMS-1712956]	National Science Foundation(National Science Foundation (NSF))	Research of S. Minsker and X. Wei was partially supported by the National Science Foundation grant NSF DMS-1712956.	Allard WK, 2012, APPL COMPUT HARMON A, V32, P435, DOI 10.1016/j.acha.2011.08.001; Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Balasubramanian K, 2016, ELECTRON J STAT, V10, P71, DOI 10.1214/15-EJS1006; Bhatia R., 2013, MATRIX ANAL, V169; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Cai TT, 2016, ELECTRON J STAT, V10, P1, DOI 10.1214/15-EJS1081; Catoni O, 2016, ARXIV160305229; Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454; Chen M., 2015, ARXIV150600691; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Fan J., 2017, J AM STAT ASS; Fan J., 2016, ARXIV160303516; Fan J., 2016, ARXIV160308315; Fan JQ, 2017, J R STAT SOC B, V79, P247, DOI 10.1111/rssb.12166; Fan JQ, 2016, ECONOMET J, V19, pC1, DOI 10.1111/ectj.12061; Fang K.-T., 1990, SYMMETRIC MULTIVARIA; Giulini I., 2015, ARXIV151106263; Han F., 2017, J AM STAT ASS; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Hubert M, 2008, STAT SCI, V23, P92, DOI 10.1214/088342307000000087; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Ledoit O, 2012, ANN STAT, V40, P1024, DOI 10.1214/12-AOS989; LEPSKII OV, 1991, THEOR PROBAB APPL+, V36, P682, DOI 10.1137/1136085; Lounici K, 2014, BERNOULLI, V20, P1029, DOI 10.3150/12-BEJ487; Minsker S., 2016, ARXIV160507129; Novembre J, 2008, NATURE, V456, P98, DOI 10.1038/nature07331; Saal LH, 2007, P NATL ACAD SCI USA, V104, P7564, DOI 10.1073/pnas.0702507104; Tropp J. A., 2015, ARXIV150101571; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Tukey J., 1975, P INT C MATH, V2; TYLER DE, 1987, ANN STAT, V15, P234, DOI 10.1214/aos/1176350263; Wegkamp M, 2016, BERNOULLI, V22, P1184, DOI 10.3150/14-BEJ690; ZWALD L, 2006, ADV NEURAL INFORM PR, V18, P1649	35	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402088
C	Nan, F; Saligrama, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nan, Feng; Saligrama, Venkatesh			Adaptive Classification for Prediction Under a Budget	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a novel adaptive approximation approach for test-time resource-constrained prediction motivated by Mobile, IoT, health, security and other applications, where constraints in the form of computation, communication, latency and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to hard input instances and gates easy-to-handle input instances to a low-cost model. Our method is based on adaptively approximating the high-cost model in regions where low-cost models suffice for making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.	[Nan, Feng] Boston Univ, Syst Engn, Boston, MA 02215 USA; [Saligrama, Venkatesh] Boston Univ, Elect Engn, Boston, MA 02215 USA	Boston University; Boston University	Nan, F (corresponding author), Boston Univ, Syst Engn, Boston, MA 02215 USA.	fnan@bu.edu; srv@bu.edu	Jeong, Yongwook/N-7413-2016	Saligrama, Venkatesh/0000-0002-0675-2268	NSF [CCF: 1320566, CNS: 1330008, CCF: 1527618, DHS 2013-ST-061-ED0001]; NGA Grant [HM1582-09-1-0037]; ONR [N00014-13-C-0288]	NSF(National Science Foundation (NSF)); NGA Grant; ONR(Office of Naval Research)	Feng Nan would like to thank Dr Ofer Dekel for ideas and discussions on resource constrained machine learning during an internship in Microsoft Research in summer 2016. Familiarity and intuition gained during the internship contributed to the motivation and formulation in this paper. We also thank Dr Joseph Wang and Tolga Bolukbasi for discussions and helps in experiments. This material is based upon work supported in part by NSF Grants CCF: 1320566, CNS: 1330008, CCF: 1527618, DHS 2013-ST-061-ED0001, NGA Grant HM1582-09-1-0037 and ONR Grant N00014-13-C-0288.	Bolukbasi T, 2017, PR MACH LEARN RES, V70; Busa-Fekete Robert, 2012, P 29 INT C MACH LEAR; Chapelle O., 2011, P YAH LEARN RANK CHA; Chen M., 2012, P INT C ART INT STAT, P218; Frank A., 2010, UCI MACHINE LEARNING; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Ganchev J., 2008, P C NEUR INF PROC SY, P569; Gao T, 2011, ADV NEURAL INFORM PR; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Kumar A, 2017, PR MACH LEARN RES, V70; Kusner M, 2014, AAAI C ART INT; Lopez-Paz D., 2016, INT C LEARN REPR; Nan F., 2015, INT C MACH LEARN, P1983; Nan F, 2016, ADV NEUR IN, V29; Nan F, 2014, INT CONF ACOUST SPEE; Olshen R., 1984, CLASSIFICATION REGRE; Robinson Daniel P., 2016, P 25 INT JOINT C ART, P1974; Trapeznikov K, 2013, ARTIF INTELL, P581; Wang Joseph, 2015, ADV NEURAL INFORM PR, V28, P2143; Wang Joseph, 2014, MODEL SELECTION LINE, P647; Weiss D, 2013, IEEE I CONF COMP VIS, P2656, DOI 10.1109/ICCV.2013.330; Xu Z, 2013, INT C MACH LEARN; Xu Z., 2012, P 29 INT C MACH LEAR	24	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404077
C	Pedregosa, F; Leblond, R; Lacoste-Julien, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pedregosa, Fabian; Leblond, Remi; Lacoste-Julien, Simon			Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints. In this work, we propose and analyze PROXASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.	[Pedregosa, Fabian; Leblond, Remi] PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France; [Lacoste-Julien, Simon] Univ Montreal, MILA, Montreal, PQ, Canada; [Lacoste-Julien, Simon] Univ Montreal, DIRO, Montreal, PQ, Canada	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Universite de Montreal; Universite de Montreal	Pedregosa, F (corresponding author), PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France.		Jeong, Yongwook/N-7413-2016; Pedregosa, Fabian/U-3477-2019		Google Research Award; chaire Economie des nouvelles donnees; fonds AXA pour la recherche; data science joint research initiative	Google Research Award(Google Incorporated); chaire Economie des nouvelles donnees; fonds AXA pour la recherche(AXA Research Fund); data science joint research initiative	This work was partially supported by a Google Research Award. FP acknowledges support from the chaire Economie des nouvelles donnees with the data science joint research initiative with the fonds AXA pour la recherche.	[Anonymous], 2012, ARXIV12112717; [Anonymous], 2010, KDD CUP; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Beck A., 2009, CONVEX OPTIMIZATION; Davis D, 2016, ADV NEUR IN, V29; Defazio Aaron, 2014, ADV NEURAL INFORM PR; Gu Bin, 2016, ARXIV161009447V3; Hsieh C. - J., 2015, ICML; Johoson R., 2013, ADV NEURAL INFORM PR; Juan YC, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P43, DOI 10.1145/2959100.2959134; Le Roux Nicolas, 2012, ADV NEURAL INFORM PR; Liu Ji, 2015, SIAM J OPTIMIZATION; Mania Horia, 2017, SIAM J OPTIMIZATION; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Yurii, 2013, MATH PROGRAMMING; Niu F., 2011, P 24 INT C NEUR INF; Peng Zhimin, 2016, SIAM J SCI COMPUTING; Qi Meng, 2017, AAAI; Reddi Sashank J, 2015, ADV NEURAL INFORM PR; Remi Leblond, 2017, P 20 INT C ART INT S; Schmidt M., 2016, MATH PROGRAMMING; Shai Shalev-Shwartz, 2013, J MACHINE LEARNING R; Xiao Lin, 2014, SIAM J OPTIMIZATION; You Yang, 2016, ADV NEURAL INFORM PR; Zhao T., 2014, ADV NEURAL INFORM PR	25	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400006
C	Perolat, J; Leibo, JZ; Zambaldi, V; Beattie, C; Tuyls, K; Graepel, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Perolat, Julien; Leibo, Joel Z.; Zambaldi, Vinicius; Beattie, Charles; Tuyls, Karl; Graepel, Thore			A multi-agent reinforcement learning model of common-pool resource appropriation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				TERRITORIALITY; TRAGEDY	Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria-a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.	[Perolat, Julien; Leibo, Joel Z.; Zambaldi, Vinicius; Beattie, Charles; Graepel, Thore] DeepMind, London, England; [Tuyls, Karl] Univ Liverpool, Liverpool, Merseyside, England	University of Liverpool	Perolat, J (corresponding author), DeepMind, London, England.	perolat@google.com; jzl@google.com; yzambaldi@google.com; cbeattie@google.com; karltuyls@google.com; thore@google.com	Tuyls, Karl P/Q-7328-2018; Jeong, Yongwook/N-7413-2016	Tuyls, Karl P/0000-0001-7929-1944; 				Acheson JM, 2005, RATION SOC, V17, P309, DOI 10.1177/1043463105051634; [Anonymous], 2003, ICML; Axelrod R., 1984, EVOLUTION COOPERATIO, DOI DOI 10.2307/3323905; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Camerer CF, 1997, J ECON PERSPECT, V11, P167, DOI 10.1257/jep.11.4.167; Chalkiadakis Georgios, 2003, P 2 INT JOINT C AUT, P709; Dietz T, 2003, SCIENCE, V302, P1907, DOI 10.1126/science.1091015; Gardner R, 1990, RATION SOC, V2, P335, DOI DOI 10.1177/1043463190002003005; Gini C., 1912, J R STAT SOC, V76, P326; Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579; Gordon HS, 1954, J POLIT ECON, V62, P124, DOI 10.1086/257497; HARDIN G, 1968, SCIENCE, V162, P1243, DOI 10.1126/science.162.3859.1243; Janssen M., 2010, ECOLOGY SOC, V15; Janssen Marco, 2008, INT J COMMONS, V2; Janssen MA, 2008, RATION SOC, V20, P371, DOI 10.1177/1043463108096786; Janssen MA, 2013, ECOL SOC, V18, DOI 10.5751/ES-05664-180404; Janssen MA, 2010, SCIENCE, V328, P613, DOI 10.1126/science.1183532; Junling Hu, 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P242; Kleiman-Weiner M., 2016, P 38 ANN C COGNITIVE; Laurent GJ, 2011, INT J KNOWL-BASED IN, V15, P55, DOI 10.3233/KES-2010-0206; Leibo J.Z., 2017, P 16 INT C AUT AG MU; Littman ML, 1994, ICML 1994, P157; Martin K.O., 1979, N ATLANTIC MARITIME, P277; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; NOWAK M, 1993, NATURE, V364, P56, DOI 10.1038/364056a0; Ostrom E, 1999, SCIENCE, V284, P278, DOI 10.1126/science.284.5412.278; OSTROM E, 1993, J ECON PERSPECT, V7, P93, DOI 10.1257/jep.7.4.93; Ostrom E., 1992, CRAFTING I SELF GOVE; Ostrom E., 1994, RULES GAMES COMMON P; Ostrom V., 1977, ALTERNATIVES DELIVER, P1, DOI 10.4324/9780429047978-2; Rankin DJ, 2007, TRENDS ECOL EVOL, V22, P643, DOI 10.1016/j.tree.2007.07.009; SCHELLING TC, 1973, J CONFLICT RESOLUT, V17, P381, DOI 10.1177/002200277301700302; Shapley L. S., 1953, P NATL ACAD SCI US; Shoham Y, 2007, ARTIF INTELL, V171, P365, DOI 10.1016/j.artint.2006.02.006; SMITH VL, 1968, AM ECON REV, V58, P409; Turner RA, 2013, SOC NATUR RESOUR, V26, P491, DOI 10.1080/08941920.2012.709313; Varakantham P., 2009, P 19 INT C AUT PLANN; Yu C, 2015, IEEE T NEUR NET LEAR, V26, P3083, DOI 10.1109/TNNLS.2015.2403394	38	2	2	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403069
C	Rabusseau, G; Balle, B; Pineau, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rabusseau, Guillaume; Balle, Borja; Pineau, Joelle			Multitask Spectral Learning of Weighted Automata	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the problem of estimating multiple related functions computed by weighted automata (WFA). We first present a natural notion of relatedness between WFAs by considering to which extent several WFAs can share a common underlying representation. We then introduce the novel model of vector-valued WFA which conveniently helps us formalize this notion of relatedness. Finally, we propose a spectral learning algorithm for vector-valued WFAs to tackle the multitask learning problem. By jointly learning multiple tasks in the form of a vector-valued WFA, our algorithm enforces the discovery of a representation space shared between tasks. The benefits of the proposed multitask approach are theoretically motivated and showcased through experiments on both synthetic and real world datasets.	[Rabusseau, Guillaume; Pineau, Joelle] McGill Univ, Montreal, PQ, Canada; [Balle, Borja] Amazon Res Cambridge, Cambridge, England	McGill University	Rabusseau, G (corresponding author), McGill Univ, Montreal, PQ, Canada.	guillaume.rabusseau@mail.mcgill.ca; pigem@amazon.co.uk; jpineau@cs.mcgill.ca	Jeong, Yongwook/N-7413-2016	Salguero Tejada, Carlos/0000-0003-0930-9277	NSERC; CIFAR; IVADO postdoctoral fellowship	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR(Canadian Institute for Advanced Research (CIFAR)); IVADO postdoctoral fellowship	G. Rabusseau acknowledges support of an IVADO postdoctoral fellowship. B. Balle completed this work while at Lancaster University. We thank NSERC and CIFAR for their financial support.	Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Bailly Raphael, 2009, P 26 ANN INT C MACH, P33; Balle B, 2013, THESIS; Balle B, 2014, PR MACH LEARN RES, V32, P1386; Balle B, 2014, MACH LEARN, V96, P33, DOI 10.1007/s10994-013-5416-x; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Ben-David S, 2003, LECT NOTES ARTIF INT, V2777, P567, DOI 10.1007/978-3-540-45167-9_41; Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092; Carlyl J. W., 1971, Journal of Computer and System Sciences, V5, P26, DOI 10.1016/S0022-0000(71)80005-3; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Cohen Shay B., 2013, P NAACL HLT, P148; Denis F, 2008, FUND INFORM, V86, P41; FLIESS M, 1974, J MATH PURE APPL, V53, P197; Girolami Mark A, 2003, P 16 INT C NEUR INF, V16, P9; Hsu D. J., 2009, COLT; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kulesza Alex, 2015, AISTATS; Li RC, 1998, SIAM J MATRIX ANAL A, V20, P471, DOI 10.1137/S0895479896298506; Liu P., 2016, IJCAI, P2873, DOI DOI 10.5555/3060832.3061023; Luong M.T., 2015, P ICLR; Ni K., 2007, PROC 24 INT C MACH L, P689; Nivre Joakim, 2016, UNIVERSAL DEPENDENCI; Petrov Slav, 2011, ARXIV11042086; Thon M, 2015, J MACH LEARN RES, V16, P103; Tony Cai  T, 2016, ARXIV160500353; Verwer Sicco, 2012, ICGI, P243; Wang BY, 2016, AAAI CONF ARTIF INTE, P2115; Wedin P.-A., 1972, BIT (Nordisk Tidskrift for Informationsbehandling), V12, P99, DOI 10.1007/BF01932678; Willsky A. S., 2009, ADV NEURAL INFORM PR, P549; ZWALD L, 2006, ADV NEURAL INFORM PR, V18, P1649	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402062
C	Rajeswaran, A; Lowrey, K; Todorov, E; Kakade, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rajeswaran, Aravind; Lowrey, Kendall; Todorov, Emanuel; Kakade, Sham			Towards Generalization and Simplicity in Continuous Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of widely studied continuous control tasks, including the gym-v1 benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, the standard training and testing scenarios for these tasks are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution induces more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video.	[Rajeswaran, Aravind; Lowrey, Kendall; Todorov, Emanuel; Kakade, Sham] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Rajeswaran, A (corresponding author), Univ Washington, Seattle, WA 98195 USA.	aravraj@cs.washington.edu; klowrey@cs.washington.edu; todorov@cs.washington.edu; sham@cs.washington.edu	Jeong, Yongwook/N-7413-2016		NSF	NSF(National Science Foundation (NSF))	This work was supported in part by the NSF. The authors would like to thank Vikash Kumar, Igor Mordatch, John Schulman, and Sergey Levine for valuable comments.	Al Borno M., 2013, IEEE T VISUALIZATION; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], 1995, ICML; Bertsekas Dimitri P, 2008, APPROXIMATE DYNAMIC; Brockman G., 2016, OPENAI GYM; Duan Y., 2016, ICML; Erez T, 2013, IEEE-RAS INT C HUMAN, P292, DOI 10.1109/HUMANOIDS.2013.7029990; Erez Tom, 2011, RSS; Gu S., 2017, ICLR; Heess Nicolas, 2015, NIPS; Kakade S., 2001, NIPS; Kumar V., 2016, ICRA; Kumar V., 2016, ARXIV E PRINTS; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap T., 2015, ARXIV E PRINTS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mordatch I., 2015, IROS; Mordatch I., 2012, ACM SIGGRAPH; Peters J., 2007, THESIS; Pinto L., 2016, ICRA; Rahimi A., 2007, NIPS; Rajeswaran A., 2017, 5 INT C LEARN REPR; Sadeghi Fereshteh, 2016, ARXIV E PRINTS; Schulman J., 2015, ICML; Schulman J., 2016, 4 INT C LEARN REPR; Si J., 2004, HDB LEARNING APPROXI, V2; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Tassa Y., 2012, INT C INT ROB SYST; TOBIN J., 2017, ARXIV E PRINTS; Todorov E., 2012, INT C INT ROB SYST; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	32	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406060
C	Roughgarden, T; Schrijvers, O		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Roughgarden, Tim; Schrijvers, Okke			Online Prediction with Selfish Experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SCORING RULES	We consider the problem of binary prediction with expert advice in settings where experts have agency and seek to maximize their credibility. This paper makes three main contributions. First, it defines a model to reason formally about settings with selfish experts, and demonstrates that "incentive compatible" (IC) algorithms are closely related to the design of proper scoring rules. Second, we design IC algorithms with good performance guarantees for the absolute loss function. Third, we give a formal separation between the power of online prediction with selfish versus honest experts by proving lower bounds for both IC and non-IC algorithms. In particular, with selfish experts and the absolute loss function, there is no (randomized) algorithm for online prediction-IC or otherwise-with asymptotically vanishing regret.	[Roughgarden, Tim; Schrijvers, Okke] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Roughgarden, T (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	tim@cs.stanford.edu; okkes@cs.stanford.edu	Jeong, Yongwook/N-7413-2016					Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12; Babaioff M., 2010, P 11 ACM C EL COMM, P43, DOI [10.1145/1807342.1807349, DOI 10.1145/1807342.1807349]; BAYARRI MJ, 1989, J AM STAT ASSOC, V84, P214, DOI 10.2307/2289866; Bickel JE, 2008, MON WEATHER REV, V136, P4867, DOI 10.1175/2008MWR2547.1; BONIN JP, 1976, AM ECON REV, V66, P682; Boutilier C., 2012, P 11 INT C AUTONOMOU, V2, P737; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Bruckner M., 2011, P 17 ACM SIGKDD INT, P547, DOI DOI 10.1145/2020408.2020495; Buja A., 2005, LOSS FUNCTIONS BINAR; Cai Yang, 2015, P C LEARN THEOR COLT, P280; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Chen Y, 2016, ADV NEURAL INFORM PR, P1813; Chen YL, 2010, AI MAG, V31, P42, DOI 10.1609/aimag.v31i4.2313; Dekel O, 2010, J COMPUT SYST SCI, V76, P759, DOI 10.1016/j.jcss.2010.03.003; Frazier P, 2014, P 15 ACM C EC COMP, P5, DOI DOI 10.1145/2600057.2602897; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Goldberg AV, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P620; Hardt M, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P111, DOI 10.1145/2840728.2840730; Jain K, 2007, ALGORITHMIC GAME THEORY, P385; Jose VRR, 2008, OPER RES, V56, P1146, DOI 10.1287/opre.1070.0498; Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704; Mansour Y., 2016, ARXIV160207570; MCCARTHY J, 1956, P NATL ACAD SCI USA, V42, P654, DOI 10.1073/pnas.42.9.654; Merkle EC, 2013, DECIS ANAL, V10, P292, DOI 10.1287/deca.2013.0280; Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/mnsc.1050.0379; Moulin H, 1999, SOC CHOICE WELFARE, V16, P279, DOI 10.1007/s003550050145; Ohneberg M., 2014, J PREDICT MARK, V8, P89, DOI [10.5750/jpm.v8i2.889, DOI 10.5750/JPM.V8I2.889]; Roughgarden T, 2007, ALGORITHMIC GAME THEORY, P443; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; SCHERVISH MJ, 1989, ANN STAT, V17, P1856, DOI 10.1214/aos/1176347398; Shah NB, 2015, ADV NEURAL INFORM PR, V28, P1; THOMSON W, 1979, J ECON THEORY, V20, P360, DOI 10.1016/0022-0531(79)90042-5	34	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401033
C	Suggala, AS; Kolar, M; Ravikumar, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Suggala, Arun Sai; Kolar, Mladen; Ravikumar, Pradeep			The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COMPATIBILITY; SELECTION	Non-parametric multivariate density estimation faces strong statistical and computational bottlenecks, and the more practical approaches impose near-parametric assumptions on the form of the density functions. In this paper, we leverage recent developments to propose a class of non-parametric models which have very attractive computational and statistical properties. Our approach relies on the simple function space assumption that the conditional distribution of each variable conditioned on the other variables has a non-parametric exponential family form.	[Suggala, Arun Sai; Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Kolar, Mladen] Univ Chicago, Chicago, IL 60637 USA	Carnegie Mellon University; University of Chicago	Suggala, AS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	asuggala@cs.cmu.edu; mkolar@chicagobooth.edu; pradeepr@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1447574, DMS-1264033]; NIH as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences [R01 GM117594-01]; IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business	ARO; NSF(National Science Foundation (NSF)); NIH as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences; IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business(International Business Machines (IBM))	A.S. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1447574, DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences. M. K. acknowledges support by an IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business.	Arnold BC, 2001, STAT SCI, V16, P249; Berti P, 2014, J MULTIVARIATE ANAL, V125, P190, DOI 10.1016/j.jmva.2013.12.009; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009; Chen HY, 2010, STAT PROBABIL LETT, V80, P670, DOI 10.1016/j.spl.2009.12.025; Dias R, 1998, J STAT COMPUT SIM, V60, P277, DOI 10.1080/00949659808811893; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; GOOD IJ, 1971, BIOMETRIKA, V58, P255, DOI 10.1093/biomet/58.2.255; GU C, 1993, ANN STAT, V21, P217, DOI 10.1214/aos/1176349023; GU C, 1995, STAT SINICA, V5, P709; Gu C, 2003, STAT SINICA, V13, P811; Gu C, 2013, STAT SINICA, V23, P1131, DOI 10.5705/ss.2011.319; Jalali Ali, 2011, P 14 INT C ART INT S, P378; Jeon YH, 2006, STAT SINICA, V16, P353; LEONARD T, 1978, J ROY STAT SOC B MET, V40, P113; Liu, 2015, ADV NEURAL INFORM PR; Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037; Liu H, 2009, J MACH LEARN RES, V10, P2295; Masse BR, 1999, CAN J STAT, V27, P819, DOI 10.2307/3316133; Mei S., 2016, ARXIV160706534; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; SILVERMAN BW, 1982, ANN STAT, V10, P795, DOI 10.1214/aos/1176345872; SPEED TP, 1986, ANN STAT, V14, P138, DOI 10.1214/aos/1176349846; Stone CJ, 1997, ANN STAT, V25, P1371; Sun S., 2015, ADV NEURAL INFORM PR, V28, P2287; Varin C, 2011, STAT SINICA, V21, P5; Voorman A, 2014, BIOMETRIKA, V101, P85, DOI 10.1093/biomet/ast053; Wang YJ, 2008, BIOMETRIKA, V95, P735, DOI 10.1093/biomet/asn029; Yang EH, 2015, J MACH LEARN RES, V16, P3813; Yang Z., 2014, ARXIV14128697; Yuan X., 2016, ADV NEURAL INFORM PR, P4367; Zhang HH, 2006, STAT SINICA, V16, P1021	32	2	2	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404050
C	Wang, Y; Chen, W; Liu, YT; Ma, ZM; Liu, TY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Yue; Chen, Wei; Liu, Yuting; Ma, Zhi-Ming; Liu, Tie-Yan			Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In reinforcement learning (RL), one of the key components is policy evaluation, which aims to estimate the value function (i.e., expected long-term accumulated reward) of a policy. With a good policy evaluation method, the RL algorithms will estimate the value function more accurately and find a better policy. When the state space is large or continuous Gradient-based Temporal Difference(GTD) policy evaluation algorithms with linear function approximation are widely used. Considering that the collection of the evaluation data is both time and reward consuming, a clear understanding of the finite sample performance of the policy evaluation algorithms is very important to reinforcement learning. Under the assumption that data are i.i.d. generated, previous work provided the finite sample analysis of the GTD algorithms with constant step size by converting them into convex-concave saddle point problems. However, it is well-known that, the data are generated from Markov processes rather than i.i.d. in RL problems.. In this paper, in the realistic Markov setting, we derive the finite sample bounds for the general convex-concave saddle point problems, and hence for the GTD algorithms. We have the following discussions based on our bounds. (1) With variants of step size, GTD algorithms converge. (2) The convergence rate is determined by the step size, with the mixing time of the Markov process as the coefficient. The faster the Markov processes mix, the faster the convergence. (3) We explain that the experience replay trick is effective by improving the mixing property of the Markov process. To the best of our knowledge, our analysis is the first to provide finite sample bounds for the GTD algorithms in Markov setting.	[Wang, Yue; Liu, Yuting] Beijing Jiaotong Univ, Sch Sci, Beijing, Peoples R China; [Chen, Wei; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China; [Ma, Zhi-Ming] Chinese Acad Sci, Acad Math & Syst Sci, Beijing, Peoples R China; [Wang, Yue] Microsoft Res Asia, Beijing, Peoples R China	Beijing Jiaotong University; Microsoft; Chinese Academy of Sciences; Academy of Mathematics & System Sciences, CAS; Microsoft; Microsoft Research Asia	Wang, Y (corresponding author), Beijing Jiaotong Univ, Sch Sci, Beijing, Peoples R China.	11271012@bjtu.edu.cn; wche@microsoft.com; ytliu@bjtu.edu.cn; mazm@amt.ac.cn; Tie-Yan.Liu@microsoft.com	Jeong, Yongwook/N-7413-2016		A Foundation for the Author of National Excellent Doctoral Dissertation of RP China [FANEDD 201312]; National Center for Mathematics and Interdisciplinary Sciences of CAS	A Foundation for the Author of National Excellent Doctoral Dissertation of RP China(Foundation for the Author of National Excellent Doctoral Dissertation of China); National Center for Mathematics and Interdisciplinary Sciences of CAS	This work was supported by A Foundation for the Author of National Excellent Doctoral Dissertation of RP China (FANEDD 201312) and National Center for Mathematics and Interdisciplinary Sciences of CAS.	Bahdanau D., 2016, ARXIV160707086; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Durrett R, 2016, SPRINGER TEXTS STAT, P95, DOI 10.1007/978-3-319-45614-0_2; Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721; Lazaric A, 2012, J MACH LEARN RES, V13, P3041; Levin D. A., 2009, MARKOV CHAINS MIXING; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Long-Ji Lin, 1993, THESIS; Maei Hamid Reza, 2011, THESIS; Meyn S. P., 2012, MARKOV CHAINS STOCHA; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Silver D, 2014, ICML ICML 14, P387; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2009, ADV NEURAL INFORM PR, V21, P1609; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tagorti M, 2015, PR MACH LEARN RES, V37, P1521; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Yu Huizhen, 2015, ANN C LEARN THEOR	24	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405057
C	Xu, JS; Chi, EC; Lange, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Jason; Chi, Eric C.; Lange, Kenneth			Generalized Linear Model Regression under Distance-to-set Penalties	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VARIABLE SELECTION; SPLIT FEASIBILITY; OPTIMIZATION; MATRIX; REGULARIZATION; MAJORIZATION; MINIMIZATION; LIKELIHOOD	Estimation in generalized linear models (GLM) is complicated by the presence of constraints. One can handle constraints by maximizing a penalized log-likelihood. Penalties such as the lasso are effective in high dimensions, but often lead to unwanted shrinkage. This paper explores instead penalizing the squared distance to constraint sets. Distance penalties are more flexible than algebraic and regularization penalties, and avoid the drawback of shrinkage. To optimize distance penalized objectives, we make use of the majorization-minimization principle. Resulting algorithms constructed within this framework are amenable to acceleration and come with global convergence guarantees. Applications to shape constraints, sparse regression, and rank-restricted matrix regression on synthetic and real data showcase strong empirical performance, even under non-convex constraints.	[Xu, Jason; Lange, Kenneth] Univ Calif Los Angeles, Los Angeles, CA 90095 USA; [Chi, Eric C.] North Carolina State Univ, Raleigh, NC 27695 USA	University of California System; University of California Los Angeles; University of North Carolina; North Carolina State University	Xu, JS (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.	jqxu@ucla.edu; eric_chi@ncsu.edu; klange@ucla.edu	XU, Jason/GPW-9039-2022; Xu, Jason/GRR-9638-2022; Jeong, Yongwook/N-7413-2016		NSF MSPRF [1606177]	NSF MSPRF(National Science Foundation (NSF)NSF - Directorate for Mathematical & Physical Sciences (MPS))	We would like to thank Hua Zhou for helpful discussions about matrix regression and the EEG data. JX was supported by NSF MSPRF #1606177.	[Anonymous], 2012, P 29 INT C MACH LEAR; Barlow R. E., 1972, STAT INFERENCE ORDER; Becker M P, 1997, Stat Methods Med Res, V6, P38, DOI 10.1191/096228097677258219; Bertsimas D, 2016, ANN STAT, V44, P813, DOI 10.1214/15-AOS1388; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Byrne C, 2001, ANN OPER RES, V105, P77, DOI 10.1023/A:1013349430987; Censor Y, 2005, INVERSE PROBL, V21, P2071, DOI 10.1088/0266-5611/21/6/017; Censor Y., 1994, NUMER ALGORITHMS, V8, P221, DOI DOI 10.1007/BF02142692; Chi EC, 2014, MATH PROGRAM, V146, P409, DOI 10.1007/s10107-013-0697-1; Dedieu, 2017, ARXIV PREPRINT ARXIV; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fan JQ, 2010, STAT SINICA, V20, P101; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Gaines B. R., 2016, ARXIV161101511; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Hastie T., 2017, ARXIV PREPRINT ARXIV; Hung H, 2013, BIOSTATISTICS, V14, P189, DOI 10.1093/biostatistics/kxs023; Jones P. D., 2016, TRENDS COMPENDIUM DA; Lange K, 2016, MM OPTIMIZATION ALGORITHMS, P1, DOI 10.1137/1.9781611974409; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Lange K., 2015, ARXIV150707598; Li B, 2010, ANN STAT, V38, P1094, DOI 10.1214/09-AOS737; MCCuLLAGH P., 1989, GEN LINEAR MODELS, V37; Meinshausen N, 2007, COMPUT STAT DATA AN, V52, P374, DOI 10.1016/j.csda.2006.12.019; More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105; Park MY, 2007, J ROY STAT SOC B, V69, P659, DOI 10.1111/j.1467-9868.2007.00607.x; Polson NG, 2015, STAT SCI, V30, P559, DOI 10.1214/15-STS530; Su W., 2017, ANN STAT, V45; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wu WB, 2001, BIOMETRIKA, V88, P793, DOI 10.1093/biomet/88.3.793; Xu J., 2017, ARXIV161205614; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; ZHANG XL, 1995, BRAIN RES BULL, V38, P531, DOI 10.1016/0361-9230(95)02023-5; Zhou H, 2014, J R STAT SOC B, V76, P463, DOI 10.1111/rssb.12031; Zhou H, 2011, STAT COMPUT, V21, P261, DOI 10.1007/s11222-009-9166-3; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	38	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401041
C	Ye, NY; Zhu, ZX; Mantiuk, RK		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ye, Nanyang; Zhu, Zhanxing; Mantiuk, Rafal K.			Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Minimizing non-convex and high-dimensional objective functions is challenging, especially when training modern deep neural networks. In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the 'fat" modes; and the second one is to fine-tune the parameter learned from the first phase. In the Bayesian learning phase, we apply continuous tempering and stochastic approximation into the Langevin dynamics to create an efficient and effective sampler, in which the temperature is adjusted automatically according to the designed "temperature dynamics". These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments.	[Ye, Nanyang; Mantiuk, Rafal K.] Univ Cambridge, Cambridge, England; [Zhu, Zhanxing] Peking Univ, Ctr Data Sci, BIBDR, Beijing, Peoples R China	University of Cambridge; Peking University	Ye, NY (corresponding author), Univ Cambridge, Cambridge, England.	yn272@cam.ac.uk; zhanxing.zhu@pku.edu.cn; rafal.mantiuk@cl.cam.ac.uk	Mantiuk, Rafał K./AAP-9514-2020; Zhu, Zhanxing/GQA-7335-2022; Jeong, Yongwook/N-7413-2016	Mantiuk, Rafał K./0000-0003-2353-0349; 				Ba J., 2017, P 3 INT C LEARN REPR; Chen C., 2016, AISTATS; Chen C., 2015, NIPS; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Choromanska A., 2015, AISTATS; Dauphin Y. N., 2014, NIPS; GEMAN S, 1986, SIAM J CONTROL OPTIM, V24, P1031, DOI 10.1137/0324060; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Gobbo G, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.061301; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; INGBER L, 1993, MATH COMPUT MODEL, V18, P29, DOI 10.1016/0895-7177(93)90204-C; Karpathy A., 2015, CORR; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Keskar N.S., 2016, ABS160904836; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Laio A, 2002, P NATL ACAD SCI USA, V99, P12562, DOI 10.1073/pnas.202427399; Lenner N, 2016, J CHEM THEORY COMPUT, V12, P486, DOI 10.1021/acs.jctc.5b00751; Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527; Neelakantan A., 2015, P INT C LEARN REPR; Rapaport D.C, 2004, ART MOL DYNAMICS SIM, DOI [DOI 10.1017/CBO9780511816581, 10.1017/CBO9780511816581]; Saxe Andrew M., 2014, ICLR; Sutskever I., 2013, P 30 INT C MACH LEAR; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Welling M., 2011, ICML	25	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400059
C	Apthorpe, NJ; Riordan, AJ; Aguilar, RE; Homann, J; Gu, Y; Tank, DW; Seung, HS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Apthorpe, Noah J.; Riordan, Alexander J.; Aguilar, Rob E.; Homann, Jan; Gu, Yi; Tank, David W.; Seung, H. Sebastian			Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CELLULAR-RESOLUTION; CORTEX; AWAKE	Calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously. As calcium imaging datasets grow in size, automated detection of individual neurons is becoming important. Here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed. Accuracy is superior to the popular PCA/ICA method based on precision and recall relative to ground truth annotation by a human expert. These results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data.	[Apthorpe, Noah J.; Aguilar, Rob E.; Seung, H. Sebastian] Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA; [Riordan, Alexander J.; Homann, Jan; Gu, Yi; Tank, David W.; Seung, H. Sebastian] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA	Princeton University; Princeton University	Apthorpe, NJ (corresponding author), Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA.	apthorpe@princeton.edu; ariordan@princeton.edu; dwtank@princeton.edu; sseung@princeton.edu			IARPA [D16PC00005]; Mathers Foundation; Simons Foundation SCGB; U.S. Army Research Office [W911NF-12-1-0594]; NIH [R01 MH083686, U01 NS090541, U01 NS090562]	IARPA; Mathers Foundation; Simons Foundation SCGB; U.S. Army Research Office; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We thank Kisuk Lee, Jingpeng Wu, Nicholas Turner, and Jeffrey Gauthier for technical assistance. We also thank Sue Ann Koay, Niranjani Prasad, Cyril Zhang, and Hussein Nagree for discussions. This work was supported by IARPA D16PC00005 (HSS), the Mathers Foundation (HSS), NIH R01 MH083686 (DWT), NIH U01 NS090541 (DWT, HSS), NIH U01 NS090562 (HSS), Simons Foundation SCGB (DWT), and U.S. Army Research Office W911NF-12-1-0594 (HSS).	Chen TW, 2013, NATURE, V499, P295, DOI 10.1038/nature12354; DENK W, 1990, SCIENCE, V248, P73, DOI 10.1126/science.2321027; Diego F., 2014, ADV NEURAL INFORM PR, P64; Dombeck DA, 2007, NEURON, V56, P43, DOI 10.1016/j.neuron.2007.08.003; Greenberg DS, 2008, NAT NEUROSCI, V11, P749, DOI 10.1038/nn.2140; Harvey CD, 2012, NATURE, V484, P62, DOI 10.1038/nature10918; Huber D, 2012, NATURE, V484, P473, DOI 10.1038/nature11039; Kaifosh P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00080; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907; Mukamel EA, 2009, NEURON, V63, P747, DOI 10.1016/j.neuron.2009.08.009; Pachitariu M., 2013, ADV NEURAL INFORM PR; Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037; Rickgauer JP, 2014, NAT NEUROSCI, V17, P1816, DOI 10.1038/nn.3866; Smith SL, 2010, NAT NEUROSCI, V13, P1144, DOI 10.1038/nn.2620; Valmianski I, 2010, J NEUROPHYSIOL, V104, P1803, DOI 10.1152/jn.00484.2010; Walker T, 2014, CELL MAGIC WAND TOOL; Zlateski A., 2015, ARXIV151006706	18	2	2	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700102
C	Balamurugan, P; Bach, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Balamurugan, P.; Bach, Francis			Stochastic Variance Reduction Methods for Saddle-Point Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONVERGENCE	We consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems which are common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities, (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be easily accelerated using a simple extension of the "catalyst" framework, leading to an algorithm which is always superior to accelerated batch algorithms.	[Balamurugan, P.; Bach, Francis] INRIA, Ecole Normale Super, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Balamurugan, P (corresponding author), INRIA, Ecole Normale Super, Paris, France.	balamurugan.palaniappan@inria.fr; francis.bach@ens.fr						Bach F., 2008, 08121869 ARXIV; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; BenTal A, 2009, PRINC SER APPL MATH, P1; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658; Davis D., 2016, 160100698 ARXIV; Defazio A., 2014, ADV NIPS; Harikandeh R., 2015, ADV NIPS; HARKER PT, 1990, MATH PROGRAM, V48, P161, DOI 10.1007/BF01582255; Herbrich R., 1999, ADV NIPS; Joachims T., 2005, P ICML; Johnson R., 2013, ADV NIPS; Le Roux N., 2012, ADV NIPS; Lin H., 2015, ADV NIPS; Nesterov Y., 2018, APPL OPTIMIZATION; Raguet H, 2013, SIAM J IMAGING SCI, V6, P1199, DOI 10.1137/120872802; Rockafellar R.T., 1970, NONLINEAR FUNCTIONAL, V18, P397; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Rosasco L., 2014, 14037999 ARXIV; Ryu EK, 2016, APPL COMPUT MATH-BAK, V15, P3; Schmidt M., 2015, P AISTATS; Woodruff D., 2014, 14114357 ARXIV; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xu L., 2004, ADV NIPS; Zeng XR, 2014, DIGIT SIGNAL PROCESS, V31, P124, DOI 10.1016/j.dsp.2014.03.010; Zhang Y., 2015, P ICML; Zhu DL, 1996, SIAM J OPTIMIZ, V6, P714, DOI 10.1137/S1052623494250415; Zhu ZX, 2015, LECT NOTES ARTIF INT, V9284, P645, DOI 10.1007/978-3-319-23528-8_40	31	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703107
C	Belilovsky, E; Varoquaux, G; Blaschko, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Belilovsky, Eugene; Varoquaux, Gael; Blaschko, Matthew			Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INVERSE COVARIANCE ESTIMATION; CONFIDENCE-INTERVALS; SELECTION; STATES	Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two populations calls for comparing these estimated GGMs. Our goal is to identify differences in GGMs known to have similar structure. We characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings. Characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator. Recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso, whose distribution can be characterized in an efficient manner. We then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in GGMs. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.	[Belilovsky, Eugene] Univ Paris Saclay, St Aubin, France; [Belilovsky, Eugene; Varoquaux, Gael] INRIA, Villers Les Nancy, France; [Belilovsky, Eugene; Blaschko, Matthew] Katholieke Univ Leuven, Leuven, Belgium	UDICE-French Research Universities; Universite Paris Saclay; Inria; KU Leuven	Belilovsky, E (corresponding author), Univ Paris Saclay, St Aubin, France.; Belilovsky, E (corresponding author), INRIA, Villers Les Nancy, France.; Belilovsky, E (corresponding author), Katholieke Univ Leuven, Leuven, Belgium.	eugene.belilovsky@inria.fr; gael.varoquaux@inria.fr; matthew.blaschko@esat.kuleuven.be		Blaschko, Matthew/0000-0002-2640-181X	Internal Funds KU Leuven; ERC [259112, FP7-MC-CIG 334380, DIGITEO 2013-0788D - SOPRANO]; NiConnect [ANR-11-BINF-0004]	Internal Funds KU Leuven; ERC(European Research Council (ERC)European Commission); NiConnect	This work is partially funded by Internal Funds KU Leuven, ERC Grant 259112, FP7-MC-CIG 334380, and DIGITEO 2013-0788D - SOPRANO, and ANR-11-BINF-0004 NiConnect.	Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Button KS, 2013, NAT REV NEUROSCI, V14, P365, DOI 10.1038/nrn3475; Castellanos FX, 2013, NEUROIMAGE, V80, P527, DOI 10.1016/j.neuroimage.2013.04.083; Chen  X., 2011, UAI; Da Mota B, 2014, NEUROIMAGE, V89, P203, DOI 10.1016/j.neuroimage.2013.11.012; Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033; Dezeure R, 2015, STAT SCI, V30, P533, DOI 10.1214/15-STS527; Di Martino A, 2014, MOL PSYCHIATR, V19, P659, DOI 10.1038/mp.2013.78; Fazayeli  F., 2016, ICML; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Ganguly  A., 2014, ARXIV14108766; Honey CJ, 2009, P NATL ACAD SCI USA, V106, P2035, DOI 10.1073/pnas.0811168106; Honorio  J., 2010, ICML; Jankova J, 2015, ELECTRON J STAT, V9, P1205, DOI 10.1214/15-EJS1031; Javanmard A, 2014, J MACH LEARN RES, V15, P2869; Kelly C, 2012, TRENDS COGN SCI, V16, P181, DOI 10.1016/j.tics.2012.02.001; Lindquist MA, 2008, STAT SCI, V23, P439, DOI 10.1214/09-STS282; Lockhart R, 2014, ANN STAT, V42, P413, DOI 10.1214/13-AOS1175; Markov NT, 2013, SCIENCE, V342, P578, DOI 10.1126/science.1238406; MARSAGLIA G, 1964, J AM STAT ASSOC, V59, P1203, DOI 10.2307/2282635; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Mohan Karthik, 2012, Adv Neural Inf Process Syst, V2012, P629; Narayan  M., 2015, BIORXIV027516; Nichols TE, 2002, HUM BRAIN MAPP, V15, P1, DOI 10.1002/hbm.1058; Richiardi J, 2011, NEUROIMAGE, V56, P616, DOI 10.1016/j.neuroimage.2010.05.081; Shirer WR, 2012, CEREB CORTEX, V22, P158, DOI 10.1093/cercor/bhr099; Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063; Taylor J., 2013, ARXIV13074765; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221; Varoquaux G, 2013, NEUROIMAGE, V80, P405, DOI 10.1016/j.neuroimage.2013.04.007; Varoquaux Gael, 2011, IPMI; Varoquaux Gael, 2010, NIPS; Waldorp  L., 2014, STAT SURVEY; Zhao SD, 2014, BIOMETRIKA, V101, P253, DOI 10.1093/biomet/asu009	35	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704055
C	Belousov, B; Neumann, G; Rothkopf, CA; Peters, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Belousov, Boris; Neumann, Gerhard; Rothkopf, Constantin A.; Peters, Jan			Catching heuristics are optimal control policies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				OPTIMIZATION; MODEL	Two seemingly contradictory theories attempt to explain how humans move to intercept an airborne ball. One theory posits that humans predict the ball trajectory to optimally plan future actions; the other claims that, instead of performing such complicated computations, humans employ heuristics to reactively choose appropriate actions based on immediate visual feedback. In this paper, we show that interception strategies appearing to be heuristics can be understood as computational solutions to the optimal control problem faced by a ball-catching agent acting under uncertainty. Modeling catching as a continuous partially observable Markov decision process and employing stochastic optimal control theory, we discover that the four main heuristics described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball. Specifically, by varying model parameters such as noise, time to ground contact, and perceptual latency, we show that different strategies arise under different circumstances. The catcher's policy switches between generating reactive and predictive behavior based on the ratio of system to observation noise and the ratio between reaction time and task duration. Thus, we provide a rational account of human ball-catching behavior and a unifying explanation for seemingly contradictory theories of target interception on the basis of stochastic optimal control.	[Belousov, Boris; Neumann, Gerhard; Peters, Jan] Tech Univ Darmstadt, Dept Comp Sci, Darmstadt, Germany; [Rothkopf, Constantin A.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany; [Rothkopf, Constantin A.] Tech Univ Darmstadt, Dept Psychol, Darmstadt, Germany	Technical University of Darmstadt; Technical University of Darmstadt; Technical University of Darmstadt	Belousov, B (corresponding author), Tech Univ Darmstadt, Dept Comp Sci, Darmstadt, Germany.		Peters, Jan R/D-5068-2009; Peters, Jan/P-6027-2019	Peters, Jan R/0000-0002-5266-8091; Peters, Jan/0000-0002-5266-8091	European Union [640554]	European Union(European Commission)	This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 640554.	Anderson FC, 2001, J BIOMECH ENG-T ASME, V123, P381, DOI 10.1115/1.1392310; BRANCAZIO PJ, 1985, AM J PHYS, V53, P849, DOI 10.1119/1.14350; Bry Adam, 2011, IEEE International Conference on Robotics and Automation, P723; CHAPMAN S, 1968, AM J PHYS, V36, P868, DOI 10.1119/1.1974297; Diehl, 2012, RECENT ADV ALGORITHM, P297, DOI DOI 10.1007/978-3-642-30023-3_27; Diehl M, 2006, LECT NOTES CONTR INF, V340, P65; Fink PW, 2009, J VISION, V9, DOI 10.1167/9.13.14; FLASH T, 1985, J NEUROSCI, V5, P1688, DOI 10.1523/jneurosci.05-07-01688.1985; Gigerenzer G., 2007, GUT FEELINGS; Gigerenzer G, 2009, TOP COGN SCI, V1, P107, DOI 10.1111/j.1756-8765.2008.01006.x; Harris CM, 1998, NATURE, V394, P780, DOI 10.1038/29528; Hayhoe M. M., 2004, J VISION CHARLOTTESV, V4, P156, DOI DOI 10.1167/4.8.156; MCBEATH MK, 1995, SCIENCE, V268, P569, DOI 10.1126/science.7725104; McIntyre J, 2001, NAT NEUROSCI, V4, P693, DOI 10.1038/89477; McLeod P, 2006, J EXP PSYCHOL HUMAN, V32, P139, DOI 10.1037/0096-1523.32.1.139; Miall R, 1996, FORWARD MODELS PHYSL; Patil S, 2015, SPRINGER TRAC ADV RO, V107, P515, DOI 10.1007/978-3-319-16595-0_30; Simon HA, 1955, Q J ECON, V69, P99, DOI 10.2307/1884852; Tedrake R., 2010, BELIEF SPACE PLANNIN; Thrun S., 2005, PROBABILISTIC ROBOTI; Todorov E, 2002, NAT NEUROSCI, V5, P1226, DOI 10.1038/nn963; UNO Y, 1989, BIOL CYBERN, V61, P89, DOI 10.1007/BF00204593; van den Berg J, 2012, INT J ROBOT RES, V31, P1263, DOI 10.1177/0278364912456319; Vitus MP, 2011, IEEE INT CONF ROBOT, P2152, DOI 10.1109/ICRA.2011.5980257; Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y; Zago M., 2009, VISUO MOTOR COORDINA	27	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704072
C	Bertinetto, L; Henriques, JF; Valmadre, J; Torr, PHS; Vedaldi, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bertinetto, Luca; Henriques, Joao F.; Valmadre, Jack; Torr, Philip H. S.; Vedaldi, Andrea			Learning feed-forward one-shot learners	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.	[Bertinetto, Luca; Henriques, Joao F.; Valmadre, Jack; Torr, Philip H. S.; Vedaldi, Andrea] Univ Oxford, Oxford, England	University of Oxford	Bertinetto, L (corresponding author), Univ Oxford, Oxford, England.	luca@robots.ox.ac.uk; joao@robots.ox.ac.uk; jvlmdr@robots.ox.ac.uk; philip.torr@eng.ox.ac.uk; vedaldi@robots.ox.ac.uk			Apical Ltd.; ERC [ERC-2012-AdG 321162-HELIOS, HELIOS-DFR00200, EP/L024683/1]	Apical Ltd.; ERC(European Research Council (ERC)European Commission)	This research was supported by Apical Ltd. and ERC grants ERC-2012-AdG 321162-HELIOS, HELIOS-DFR00200 and "Integrated and Detailed Image Understanding" (EP/L024683/1).	[Anonymous], 2015, TRANSFERRING RICH FE; Bertinetto Luca, 2016, FULLY CONVOLUTIONAL; Bromley J., 1993, INT J PATTERN RECOGN; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Denil M., 2013, NIPS; Fan H., 2014, LEARNING DEEP FACE R; Han B., 2016, P CVPR; He K., 2015, ICCV; Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675; Ioffe S., 2015, ARXIV; Kingma D.P., 2013, INT C LEARN REPR; Koch G., 2016, ICML 2015 DEEP LEARN; Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Malisiewicz T., 2011, ICCV; Parkhi O. M., 2015, BMVC; Possegger H., 2015, CVPR; Rezende D. J., 2016, ONE SHOT GEN DEEP GE; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Socher R., 2013, ADV NEURAL INFORM PR, V26; Vedaldi A., 2015, P ACM INT C MULT; Zhang J., 2014, ECCV	24	2	2	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700032
C	Cheng, DH; Peng, R; Perros, I; Liu, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cheng, Dehua; Peng, Richard; Perros, Ioakeim; Liu, Yan			SPALS: Fast Alternating Least Squares via Implicit Leverage Scores Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Tensor CANDECOMP/PARAFAC (CP) decomposition is a powerful but computationally challenging tool in modern data analytics. In this paper, we show ways of sampling intermediate steps of alternating minimization algorithms for computing low rank tensor CP decompositions, leading to the sparse alternating least squares (SPALS) method. Specifically, we sample the Khatri-Rao product, which arises as an intermediate object during the iterations of alternating least squares. This product captures the interactions between different tensor modes, and form the main computational bottleneck for solving many tensor related tasks. By exploiting the spectral structures of the matrix Khatri-Rao product, we provide efficient access to its statistical leverage scores. When applied to the tensor CP decomposition, our method leads to the first algorithm that runs in sublinear time per-iteration and approximates the output of deterministic alternating least squares algorithms. Empirical evaluations of this approach show significant speedups over existing randomized and deterministic routines for performing CP decomposition. On a tensor of the size 2.4m x 6.6m x 92k with over 2 billion nonzeros formed by Amazon product reviews, our routine converges in two minutes to the same error as deterministic ALS.	[Cheng, Dehua; Liu, Yan] Univ Southern Calif, Los Angeles, CA 90089 USA; [Peng, Richard; Perros, Ioakeim] Georgia Inst Technol, Atlanta, GA 30332 USA	University of Southern California; University System of Georgia; Georgia Institute of Technology	Cheng, DH (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	dehua.cheng@usc.edu; rpeng@cc.gatech.edu; perros@gatech.edu; yanliu.cs@usc.edu			U.S. Army Research Office [W911NF-15-1-0491]; NSF [IIS-1254206, IIS-1134990]	U.S. Army Research Office; NSF(National Science Foundation (NSF))	This work is supported in part by the U.S. Army Research Office under grant number W911NF-15-1-0491, NSF Research Grant IIS-1254206 and IIS-1134990. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government.	Barak B., 2015, STOC; Bhojanapalli S., 2015, ARXIV E PRINTS; Carroll J. D., 1970, PSYCHOMETRIKA; Cheng Dehua, 2015, ARXIV150203496; Clarkson K., 2015, SODA; Clarkson Kenneth L, 2013, STOC; Clarkson Kenneth L, 2015, FOCS; Cohen M., 2015, STOC; Cohen M. B., 2015, ITCS; Dasgupta A., 2009, SIAM J COMPUTING; De Lathauwer L., 1998, I MATH ITS APPL C SE; De Lathauwer Lieven, 2000, SIAM J MATRIX ANAL A; De Silva V., 2008, SIAM J MATRIX ANAL A; Drineas P., 2011, NUMERISCHE MATH; Ge R, 2015, COLT; Harshman Richard A., 1970, FDN PARAFAC PROCEDUR; Hillar C. J., 2013, J ACM JACM; Jeon I., 2015, ICDE; Kang U., 2012, KDD; Kolda Tamara G, 2009, SIAM REV; Li M., 2013, FOCS; Mahoney M. W., 2011, FDN TRENDS MACHINE L; McAuley J., 2013, RECSYS; Meng X., 2013, STOC; Needell D., 2014, NIPS; Nelson J., 2013, FOCS; Nguyen N. H., 2010, CORR; Novikov A., 2015, NIPS; Papalexakis E. E., 2012, MACHINE LEARNING KNO; Pham N., 2013, KDD; Phan A.-H., 2013, SIGNAL PROCESSING IE; Smith S., 2015, 29 IEEE INT PAR DIST; Strohmer T., 2009, JFAA; Sun Jimeng, 2009, SDM; Tsourakakis CE, 2010, SDM; Wang  Y., 2015, NIPS; Woodruff David P, 2014, FDN TRENDS THEORETIC; Yang J., 2016, SODA; Yu R, 2015, PR MACH LEARN RES, V37, P238	39	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703072
C	Ciliberto, C; Rudi, A; Rosasco, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ciliberto, Carlo; Rudi, Alessandro; Rosasco, Lorenzo			A Consistent Regularization Approach for Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed method. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.	[Ciliberto, Carlo; Rudi, Alessandro; Rosasco, Lorenzo] Ist Italiano Tecnol, Lab Computat & Stat Learning, Genoa, Italy; [Ciliberto, Carlo; Rudi, Alessandro; Rosasco, Lorenzo] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Rudi, Alessandro; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy	Istituto Italiano di Tecnologia - IIT; Massachusetts Institute of Technology (MIT); University of Genoa	Ciliberto, C (corresponding author), Ist Italiano Tecnol, Lab Computat & Stat Learning, Genoa, Italy.; Ciliberto, C (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	cciliber@mit.edu; ale_rudi@mit.edu; lrosasco@mit.edu						Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Berlinet A., 2011, REPRODUCING KERNEL H; Brouard C, 2016, J MACH LEARN RES, V17; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1; Cortes C., 2005, P 22 INT C MACH LEAR; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dekel Ofer, 2004, ADV NEURAL INFORM PR; Duchi John C., 2010, P 27 INT C MACH LEAR, P327; EADES P, 1993, INFORM PROCESS LETT, V47, P319, DOI 10.1016/0020-0190(93)90079-O; Gao Wei, 2013, ARTIFICIAL INTELLIGE; Geurts P., 2006, ICML; Giguere S., 2013, ICML; HARDLE W, 1984, J MULTIVARIATE ANAL, V14, P169, DOI 10.1016/0047-259X(84)90003-4; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Herbrich R, 2000, ADV NEUR IN, P115; Hofmann Thomas, 2007, PREDICTING STRUCTURE; Huber P. J., 2011, ROBUST STAT; Kadri H., 2013, P INT C MACH LEARN I; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Micchelli Charles A, 2004, ADV NEURAL INFORM PR, P921; Mroueh Youssef, 2012, NIPS, V25, P2798; Schoen R., 1994, LECT DIFFERENTIAL GE, V2; Scholkopf B., 2001, LEARNING KERNELS SUP; Steinwart I., 2008, SUPPORT VECTOR MACHI; Steinwart I, 2011, BERNOULLI, V17, P211, DOI 10.3150/10-BEJ267; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Weston J., 2002, NIPS, P873; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2	32	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700057
C	Feldman, D; Volkov, M; Rus, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Feldman, Dan; Volkov, Mikhail; Rus, Daniela			Dimensionality Reduction of Massive Sparse Datasets Using Coresets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATION; ALGORITHMS	In this paper we present a practical solution with performance guarantees to the problem of dimensionality reduction for very large scale sparse matrices. We show applications of our approach to computing the Principle Component Analysis (PCA) of any n x d matrix, using one pass over the stream of its rows. Our solution uses coresets: a scaled subset of the n rows that approximates their sum of squared distances to every k-dimensional affine subspace. An open theoretical problem has been to compute such a coreset that is independent of both n and d. An open practical problem has been to compute a non-trivial approximation to the PCA of very large but sparse databases such as the Wikipedia document-term matrix in a reasonable time. We answer both of these questions affirmatively. Our main technical result is a new framework for deterministic coreset constructions based on a reduction to the problem of counting items in a stream.	[Feldman, Dan] Univ Haifa, Haifa, Israel; [Volkov, Mikhail; Rus, Daniela] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA	University of Haifa; Massachusetts Institute of Technology (MIT)	Feldman, D (corresponding author), Univ Haifa, Haifa, Israel.	dannyf.post@gmail.com; mikhail@csail.mit.edu; rus@csail.mit.edu			Hon Hai/Foxconn Technology Group; NSFSaTC-BSF [CNC 1526815]; Singapore MIT Alliance on Research and Technology through the Future of Urban Mobility project; Toyota Research Institute (TRI); TRI	Hon Hai/Foxconn Technology Group; NSFSaTC-BSF; Singapore MIT Alliance on Research and Technology through the Future of Urban Mobility project; Toyota Research Institute (TRI); TRI	Support for this research has been provided by Hon Hai/Foxconn Technology Group and NSFSaTC-BSF CNC 1526815, and in part by the Singapore MIT Alliance on Research and Technology through the Future of Urban Mobility project and by Toyota Research Institute (TRI). TRI provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. We are grateful for this support.	Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097; Arora S, 2006, LECT NOTES COMPUT SC, V4110, P272; Batson J, 2012, SIAM J COMPUT, V41, P1704, DOI 10.1137/090772873; Caratheodory C., 1911, REND CIRC MAT PALERM, V32, P193, DOI [DOI 10.1007/BF03014795, 10.1007/BF03014795]; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; COHEN M. B., 2016, CORR; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Drineas P, 2011, INFORM PROCESS LETT, V111, P385, DOI 10.1016/j.ipl.2011.01.010; Feldman D., 2010, P 41 ANN ACM S THE0R; Feldman D., 2013, P ACM SIAM S DISCR A; Halko NP, 2012, THESIS; Inaba M., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P332, DOI 10.1145/177424.178042; Journee M, 2010, J MACH LEARN RES, V11, P517; Lanczos C., 1950, ITERATION METHOD SOL; Langberg M., 2010, P ACM SIAM S DISCR A; Liberty E, 2007, P NATL ACAD SCI USA, V104, P20167, DOI 10.1073/pnas.0709640104; Paige C. C., 1972, Journal of the Institute of Mathematics and Its Applications, V10, P373; Papadimitriou C. H., 1998, Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1998, P159, DOI 10.1145/275487.275505; Ruvrek R., 2011, GENSIMSTATISTICAL SE; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143	21	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705010
C	Germain, P; Bach, F; Lacoste, A; Lacoste-Julien, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Germain, Pascal; Bach, Francis; Lacoste, Alexandre; Lacoste-Julien, Simon			PAC-Bayesian Theory Meets Bayesian Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BOUNDS	We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.	[Germain, Pascal; Bach, Francis; Lacoste-Julien, Simon] INRIA Paris, Ecole Normale Super, Paris, France; [Lacoste, Alexandre] Google, Mountain View, CA USA	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Google Incorporated	Germain, P (corresponding author), INRIA Paris, Ecole Normale Super, Paris, France.	pascal.germain@inria.fr; francis.bach@inria.fr; alexandre.lacoste@inria.fr; simon.lacoste-julien@inria.fr						Alquier P, 2016, J MACH LEARN RES, V17; Ambroladze A., 2006, NIPS; Begin L, 2014, JMLR WORKSH CONF PRO, V33, P105; Bishop C.M., 2014, ANTIMICROB AGENTS CH, V58, P7250; Bissiri Pier Giovanni, 2016, J ROYAL STAT SOC B; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; C. Williamson  Robert, 1997, COLT; Catoni O., 2007, PAC BAYESIAN SUPERVI, V56; Dalalyan A, 2008, MACH LEARN, V72, P39, DOI 10.1007/s10994-008-5051-0; Germain P, 2016, PR MACH LEARN RES, V48; Germain Pascal, 2009, INT C MACH LEARN; Germain  Pascal, 2015, JMLR, V16; Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541; Grunwald P., 2012, ALT; Grunwald Peter D., 2016, ABS160500252 CORR; Guyon I, 2010, J MACH LEARN RES, V11, P61; Hazan T., 2013, ADV NEURAL INFORM PR, P1887; Jeffreys William H., 1992, AM SCI; Keshet J., 2011, P 24 INT C NEUR INF, P2205; Lacoste  Alexandre, 2015, THESIS; Langford J., 2002, P ANN C NEUR INF PRO, P423; Lever G, 2013, THEOR COMPUT SCI, V473, P4, DOI 10.1016/j.tcs.2012.10.013; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Maurer A., 2004, CSLG0411099 CORR; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Noy  Asf, 2014, AISTATS; Pentina, 2014, ICML; Seeger M., 2002, J MACHINE LEARNING R, P233; Seeger Matthias, 2003, THESIS; Seldin Y., 2011, NIPS, V24, P1683; Seldin  Yevgeny, 2010, JMLR, V11; Seldin  Yevgeny, 2012, UAI; Tolstikhin Ilya O., 2013, NIPS; Zhang T, 2006, IEEE T INFORM THEORY, V52, P1307, DOI 10.1109/TIT.2005.864439	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704093
C	Harwath, D; Torralba, A; Glass, JR		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Harwath, David; Torralba, Antonio; Glass, James R.			Unsupervised Learning of Spoken Language with Visual Context	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Humans learn to speak before they can read or write, so why can't computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.	[Harwath, David; Torralba, Antonio; Glass, James R.] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02115 USA	Massachusetts Institute of Technology (MIT)	Harwath, D (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02115 USA.	dharwath@csail.mit.edu; torralba@csail.mit.edu; jrg@csail.mit.edu						Barnard Kobus, 2003, J MACHINE LEARNING R, V2003; Chopra S., 2005, P CVPR; Dredze M., 2010, P EMP METH NAT LANG, P460; Fang H., 2015, P CVPR; Frome Andrea, 2013, P NEUR INF PROC SOC; Garofolo JS, 1993, TIMIT ACOUSTIC PHONE; Girshick R., 2013, P CVPR; Glass James, 2012, ISSPA KEYNOTE; Goldwater S, 2009, COGNITION, V112, P21, DOI 10.1016/j.cognition.2009.03.008; Harwath David, 2012, P ICASSP; Harwath David, 2015, P IEEE WORKSHOP AUTO; Jansen A., 2010, P INTERSPEECH; Jansen A., 2011, P IEEE WORKSH AUT SP; Johnson Mark, 2008, P ACL SIG COMP MORPH; Karpathy A., 2015, P 2015 C COMP VIS PA; Karpathy Andrej, 2014, P NEUR INF PROC SOC; Lee Chia-Ying, 2012, P 2012 M ASS COMP LI; Lee Chia-Ying, 2015, T ASS COMPUTATIONAL; Lewis Paul, 2016, ETHNOLOGUE LANGUAGES, V19th; Lin T.Y., 2014, P EUR C COMP VIS 201; Malioutov I., 2007, P ASS COMPUTATIONAL; Park AS, 2008, IEEE T AUDIO SPEECH, V16, P186, DOI 10.1109/TASL.2007.909282; Povey D., IEEE 2011 WORKSH AUT; Rashtchian Cyrus, 2010, P NAACL HLT 2010 WOR; Saylor P., 2015, THESIS; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Socher R., 2014, T ASSOC COMPUT LING; Socher R., 2010, P CVPR; van der Maaten Laurens, 2008, J MACHINE LEARNING R; Vinyals O., 2015, P 2015 C COMP VIS PA; Young P., 2014, T ACL, V2; Zhang Y., 2009, P ASRU; Zhou Bolei, 2014, P NEUR INF PROC SOC	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701043
C	Horel, T; Singer, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Horel, Thibaut; Singer, Yaron			Maximization of Approximately Submodular Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of maximizing a function that is approximately submodular under a cardinality constraint. Approximate submodularity implicitly appears in a wide range of applications as in many cases errors in evaluation of a submodular function break submodularity. Say that F is epsilon-approximately submodular if there exists a submodular function f such that (1 - epsilon)f (S) <= F(S) <= (1+epsilon) f (S) for all subsets S. We are interested in characterizing the query-complexity of maximizing F subject to a cardinality constraint k as a function of the error level epsilon > 0. We provide both lower and upper bounds: for epsilon > n(-1/2) we show an exponential query-complexity lower bound. In contrast, when epsilon < 1/k or under a stronger bounded curvature assumption, we give constant approximation algorithms.	[Horel, Thibaut; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Horel, T (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	thorel@seas.harvard.edu; yaron@seas.harvard.edu						[Anonymous], 2014, ARXIV PREPRINT ARXIV; Bach F., 2010, NIPS; Badanidiyuru A., 2012, SODA; Balcan MF, 2011, ACM S THEORY COMPUT, P793; Belloni A., 2015, C LEARN THEOR, P240; Chen Y., 2015, C LEARN THEOR, P338; Das A., 2012, NIPS; Das A., 2011, ICML; Defazio  A., 2012, NIPS; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gomes R., 2010, ICML; Gomez Rodriguez  M., 2011, ACM TKDD, V5; GUESTRIN C, 2005, INT C MACH LEARN ICM; Guillory A., 2011, ICML; Hassidim Avinatan, 2016, ABS160103095 CORR; Hoi S. C., 2006, ICML; Iyer Rishabh K., 2013, P 8 C WORKSHOP NEURA, P2436; Iyer Rishabh K., 2013, ADV NEURAL INFORM PR, P2742; Kempe D., 2003, KDD; Krause A., 2007, NAT C ART INT AAAI N; KRAUSE A, 2007, ICML; Krause A, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1989734.1989736; Lin H, 2011, ACL HLT; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Rodriguez Manuel G., 2012, ICML; Singer Yaron, 2015, NIPS, P3186; Singla  A., 2015, ARXIV151107211; Tschiatschek S., 2014, NIPS; Zheng J., 2014, NIPS	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701093
C	Kadmon, J; Sompolinsky, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kadmon, Jonathan; Sompolinsky, Haim			Optimal Architectures in a Solvable Model of Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REPRESENTATIONS	Deep neural networks have received a considerable attention due to the success of their training for real world machine learning applications. They are also of great interest to the understanding of sensory processing in cortical sensory hierarchies. The purpose of this work is to advance our theoretical understanding of the computational benefits of these architectures. Using a simple model of clustered noisy inputs and a simple learning rule, we provide analytically derived recursion relations describing the propagation of the signals along the deep network. By analysis of these equations, and defining performance measures, we show that these model networks have optimal depths. We further explore the dependence of the optimal architecture on the system parameters.	[Kadmon, Jonathan; Sompolinsky, Haim] Hebrew Univ Jerusalem, Racah Inst Phys, Jerusalem, Israel; [Kadmon, Jonathan; Sompolinsky, Haim] Hebrew Univ Jerusalem, ELSC, Jerusalem, Israel; [Sompolinsky, Haim] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA	Hebrew University of Jerusalem; Hebrew University of Jerusalem; Harvard University	Kadmon, J (corresponding author), Hebrew Univ Jerusalem, Racah Inst Phys, Jerusalem, Israel.; Kadmon, J (corresponding author), Hebrew Univ Jerusalem, ELSC, Jerusalem, Israel.	jonathan.kadmon@mail.huji.ac.il	Sompolinsky, Haim/ABB-8398-2021		IARPA [D16PC00002]; Gatsby Charitable Foundation; Simons Foundation SCGB grant	IARPA; Gatsby Charitable Foundation; Simons Foundation SCGB grant	This work was partially supported by IARPA (contract #D16PC00002), Gatsby Charitable Foundation, and Simons Foundation SCGB grant.	[Anonymous], 2013, EXACT SOLUTIONS NONL; Babadi B, 2014, NEURON, V83, P1213, DOI 10.1016/j.neuron.2014.07.035; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Cho YM, 2010, NEURAL COMPUT, V22, P2678, DOI 10.1162/NECO_a_00018; Cohen William W, 2008, EXTRACTING COMPOSING; DOMANY E, 1989, J PHYS A-MATH GEN, V22, P2081, DOI 10.1088/0305-4470/22/12/013; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; KANTER I, 1987, PHYS REV A, V35, P380, DOI 10.1103/PhysRevA.35.380; Lee Honglak, 2008, ADV NEURAL INFORM PR, V20; PERSONNAZ L, 1985, J PHYS LETT-PARIS, V46, pL359, DOI 10.1051/jphyslet:01985004608035900; Rumelhart D., 1986, P 1986 PARALLEL DIST, P194; Saxe A. M., 2011, P ADV NEUR INF PROC, V24, P1971; Turner GC, 2008, J NEUROPHYSIOL, V99, P734, DOI 10.1152/jn.01283.2007; Vincent P, 2010, J MACH LEARN RES, V11, P3371	14	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702077
C	Lee, S; Purushwalkam, S; Cogswell, M; Ranjan, V; Crandall, D; Batra, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lee, Stefan; Purushwalkam, Senthil; Cogswell, Michael; Ranjan, Viresh; Crandall, David; Batra, Dhruv			Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks - introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity.	[Lee, Stefan; Cogswell, Michael; Ranjan, Viresh; Batra, Dhruv] Virginia Tech, Blacksburg, VA 24061 USA; [Purushwalkam, Senthil] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Crandall, David] Indiana Univ, Bloomington, IN 47405 USA	Virginia Polytechnic Institute & State University; Carnegie Mellon University; Indiana University System; Indiana University Bloomington	Lee, S (corresponding author), Virginia Tech, Blacksburg, VA 24061 USA.	steflee@vt.edu; spurushw@andrew.cmu.edu; cogswell@vt.edu; rviresh@vt.edu; djcran@indiana.edu; dbatra@vt.edu			National Science Foundation CAREER award; Army Research Office YIP award; ICTAS Junior Faculty award; Office of Naval Research grant [N00014-14-1-0679]; Google Faculty Research award; AWS in Education Research grant; NSF CAREER award [IIS-1253549]; Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory [FA8650-12-C-7212]; NSF [ACI-0910812, CNS-0521433]; Lily Endowment, Inc.; Indiana METACyt Initiative	National Science Foundation CAREER award(National Science Foundation (NSF)); Army Research Office YIP award; ICTAS Junior Faculty award; Office of Naval Research grant(Office of Naval Research); Google Faculty Research award(Google Incorporated); AWS in Education Research grant; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory; NSF(National Science Foundation (NSF)); Lily Endowment, Inc.; Indiana METACyt Initiative	This work was supported in part by a National Science Foundation CAREER award, an Army Research Office YIP award, ICTAS Junior Faculty award, Office of Naval Research grant N00014-14-1-0679, Google Faculty Research award, AWS in Education Research grant, and NVIDIA GPU donation, all awarded to DB, and by an NSF CAREER award (IIS-1253549), the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory contract FA8650-12-C-7212, a Google Faculty Research award, and an NVIDIA GPU donation, all awarded to DC. Computing resources used by this work are supported in part by NSF (ACI-0910812 and CNS-0521433), the Lily Endowment, Inc., and the Indiana METACyt Initiative. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL, NSF, or the U.S. Government.	Ahmed K., 2016, ARXIV160406119; Batra D., 2012, EUR C COMP VIS ECCV; Chatfield K, 2014, P BRIT MACH VIS C 20, P1; Dean J., 2014, DEEP LEARN REPR LEAR; Dey D., 2015, P IEEE INT C COMP VI; Everingham M., 2011, PASCAL VISUAL OBJECT; Geiger A., 2013, INT J ROBOTICS RES I; Guzman-Rivera A., 2014, P INT C ART INT STAT; Guzman-Rivera A., 2012, ADV NEURAL INFORM PR; Hariharan B., 2011, P IEEE INT C COMP VI; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jia Y., 2013, CAFFE OPEN SOURCE CO; Karpathy Andrey, 2015, P IEEE C COMP VIS PA; Kirillov A., 2015, ADV NEURAL INFORM PR; Kirillov A., 2015, P IEEE INT C COMP VI; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lin T.-Y., 2014, P EUROPEAN C COMPUTE, P740; Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI 10.1109/CVPR.2015.7298965; Melville P., 2005, Information Fusion, V6, P99, DOI 10.1016/j.inffus.2004.04.001; Microsoft, 2016, DEC COMP VIS RES ON; Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552; Prasad A., 2014, ADV NEURAL INFORM PR; Russakovsky  O., 2012, IMAGENET LARGE SCALE; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Tumer K., 1996, Connection Science, V8, P385, DOI 10.1080/095400996116839; Vedantam R., 2015, P IEEE C COMP VIS PA; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; WIRED, 2015, FAC AI CAN CAPT PHOT	29	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702017
C	Li, SCX; Marlin, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Steven Cheng-Xian; Marlin, Benjamin			A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SPECTRAL-ANALYSIS	We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixed-dimensional feature spaces. To address these challenges, we propose an uncertainty-aware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.	[Li, Steven Cheng-Xian; Marlin, Benjamin] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Li, SCX (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	cxl@cs.umass.edu; marlin@cs.umass.edu			National Science Foundation [1350522]	National Science Foundation(National Science Foundation (NSF))	This work was supported by the National Science Foundation under Grant No. 1350522.	BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582; BJORCK A, 1983, LINEAR ALGEBRA APPL, V52-3, P127, DOI 10.1016/0024-3795(83)90010-1; Chow E, 2014, SIAM J SCI COMPUT, V36, pA588, DOI 10.1137/130920587; Clark JS, 2004, ECOLOGY, V85, P3140, DOI 10.1890/03-0520; Dubrulle A. A., 2001, ELECTRON T NUMER ANA, V12, P216, DOI [DOI 12.2001/PP216-233.DIR/PP216-233.PDF, 12.2001/pp216-233.dir/pp216-233.pdf]; Feng YT, 1995, COMPUTER METHODS APP; Golub G, 1977, MATH SOFTWARE, P361, DOI DOI 10.1016/B978-0-12-587260-7.50018-2; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Kingma Diederik P, 2014, P 21 INT C LEARN REP; LeCun Y., 2004, P COMP VIS PATT REC; Li Steven Cheng-Xian, 2015, 31 C UNC ART INT; Liu Jiayang, 2009, PERVASIVE MOBILE COM; Marlin B.M., 2012, P 2 ACM SIGHIT INT H, P389; Parlett B. N., 1980, SYMMETRIC EIGENVALUE, V7; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ruf T, 1999, BIOL RHYTHM RES, V30, P178, DOI 10.1076/brhm.30.2.178.1422; SAAD Y, 1980, SIAM J NUMER ANAL, V17, P687, DOI 10.1137/0717059; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; SCARGLE JD, 1982, ASTROPHYS J, V263, P835, DOI 10.1086/160554; Schulz M, 1997, COMPUT GEOSCI, V23, P929, DOI 10.1016/S0098-3004(97)00087-3; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Turner MIlicIan W, 2009, IMA J NUMER ANAL; Wilson A., 2015, P 32 INT C MACH LEAR; Wilson AG, 2013, P 30 INT C MACH LEAR	25	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703111
C	Li, Z; Gong, BQ; Yang, TB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Zhe; Gong, Boqing; Yang, Tianbao			Improved Dropout for Shallow and Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named evolutional dropout) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10% on the prediction performance and over 50% on the convergence speed compared to the standard dropout.	[Li, Zhe; Yang, Tianbao] Univ Iowa, Iowa City, IA 52245 USA; [Gong, Boqing] Univ Cent Florida, Orlando, FL 32816 USA	University of Iowa; State University System of Florida; University of Central Florida	Li, Z (corresponding author), Univ Iowa, Iowa City, IA 52245 USA.	zhe-li-1@uiowa.edu; bgong@crcv.ucf.edu; tianbao-yang@uiowa.edu	Li, Zhe/AAJ-6070-2020		National Science Foundation [IIS-1463988, IIS-1545995]; NSF [IIS-1566511]	National Science Foundation(National Science Foundation (NSF)); NSF(National Science Foundation (NSF))	We thank anonymous reviewers for their comments. Z. Li and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995). B. Gong is supported in part by NSF (IIS-1566511) and a gift from Adobe.	Ba J., 2013, ADV NEURAL INFORM PR, P3084; Baldi P., 2013, ADV NEURAL INFORM PR, V26, P2814, DOI DOI 10.17744/MEHC.25.2.XHYREGGXDCD0Q4NY; Graham B., 2015, CORR; Hinton G., 2012, ARXIV PREPRINT ARXIV; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kingma D.P, P 3 INT C LEARNING R; KINGMA D. P., 2015, CORR; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Netzer Y., 2011, NIPS DLW; Neyshabur B., 2015, ADV NEURAL INFORM PR, P2413; Ranzato M., 2010, P 13 INT C ART INT S, P621; Shalev-Shwartz S., 2009, 22 C LEARN THEOR COL; Srebro N., 2010, ADV NEURAL INFORM PR, P2199; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Wager Stefan, 2013, ADV NEURAL INFORM PR, P351; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang S., 2013, P 2013 C EMP METH NA, P1170; Zhang S., 2014, ARXIV14126651, V28; Zhuo JW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4126	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704085
C	Lynn, CW; Lee, DD		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lynn, Christopher W.; Lee, Daniel D.			Maximizing Influence in an Ising Network: A Mean-Field Optimal Solution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				STATISTICAL-MECHANICS	Influence maximization in social networks has typically been studied in the context of contagion models and irreversible processes. In this paper, we consider an alternate model that treats individual opinions as spins in an Ising system at dynamic equilibrium. We formalize the Ising influence maximization problem, which has a natural physical interpretation as maximizing the magnetization given a budget of external magnetic field. Under the mean-field (MF) approximation, we present a gradient ascent algorithm that uses the susceptibility to efficiently calculate local maxima of the magnetization, and we develop a number of sufficient conditions for when the MF magnetization is concave and our algorithm converges to a global optimum. We apply our algorithm on random and real-world networks, demonstrating, remarkably, that the MF optimal external fields (i.e., the external fields which maximize the MF magnetization) shift from focusing on high-degree individuals at high temperatures to focusing on low-degree individuals at low temperatures. We also establish a number of novel results about the structure of steady-states in the ferromagnetic MF Ising model on general graph topologies, which are of independent interest.	[Lynn, Christopher W.] Univ Penn, Dept Phys & Astron, Philadelphia, PA 19104 USA; [Lee, Daniel D.] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA	University of Pennsylvania; University of Pennsylvania	Lynn, CW (corresponding author), Univ Penn, Dept Phys & Astron, Philadelphia, PA 19104 USA.	chlynn@sas.upenn.edu; ddlee@seas.upenn.edu			U.S. National Science Foundation; Air Force Office of Scientific Research; Department of Transportation	U.S. National Science Foundation(National Science Foundation (NSF)); Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Department of Transportation	We thank Michael Kearns and Eric Horsley for enlightening discussions, and we acknowledge support from the U.S. National Science Foundation, the Air Force Office of Scientific Research, and the Department of Transportation.	BLUME LE, 1993, GAME ECON BEHAV, V5, P387, DOI 10.1006/game.1993.1023; Castellano C, 2009, REV MOD PHYS, V81, P591, DOI 10.1103/RevModPhys.81.591; De A., 2015, ARXIV150605474; Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57, DOI 10.1145/502512.502525; GALAM S, 1991, EUR J SOC PSYCHOL, V21, P49, DOI 10.1002/ejsp.2420210105; Galam S, 2008, INT J MOD PHYS C, V19, P409, DOI 10.1142/S0129183108012297; Goyal S., 2014, GEB; ISENBERG DJ, 1986, J PERS SOC PSYCHOL, V50, P1141, DOI 10.1037/0022-3514.50.6.1141; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kindermann R., 1980, MARKOV RANDOM FIELDS, DOI [10.1090/conm/001, DOI 10.1090/CONM/001]; Leskovec J, 2014, SNAP DATASETS STANFO; Mas M, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000959; MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023; Montanari A., 2010, PNAS, V107; Mossel E, 2007, ACM S THEORY COMPUT, P128; Moussaid M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0084592; NEWMAN M, 2001, PNAS, V98; Nishimori H, 1999, PHYS REV E, V60, P132, DOI 10.1103/PhysRevE.60.132; Opper M, 2001, ADV MEAN FIELD METHO; Richardson M., 2002, P 9 ACM SIGKDD INT C, P61, DOI [DOI 10.1145/775047.775057, 10.1145/775047.775057]; Rodriguez Manuel G., 2012, ICML; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Sznajd-Weron K., 2000, INT J MODERN PHYS C, V11; Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302; Teboulle M., 2010, 1 ORDER ALGORITHMS C; Yedidia JS, 2001, NEU INF PRO, P21	27	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700083
C	McNamee, D; Wolpert, D; Lengyel, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		McNamee, Daniel; Wolpert, Daniel; Lengyel, Mate			Efficient state-space modularization for planning: theory, behavioral and neural signatures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INFERENCE; SYSTEMS; MDPS	Even in state-spaces of modest size, planning is plagued by the "curse of dimensionality". This problem is particularly acute in human and animal cognition given the limited capacity of working memory, and the time pressures under which planning often occurs in the natural environment. Hierarchically organized modular representations have long been suggested to underlie the capacity of biological systems (1,2) to efficiently and flexibly plan in complex environments. However, the principles underlying efficient modularization remain obscure, making it difficult to identify its behavioral and neural signatures. Here, we develop a normative theory of efficient state-space representations which partitions an environment into distinct modules by minimizing the average (information theoretic) description length of planning within the environment, thereby optimally trading off the complexity of planning across and within modules. We show that such optimal representations provide a unifying account for a diverse range of hitherto unrelated phenomena at multiple levels of behavior and neural representation.	[McNamee, Daniel; Wolpert, Daniel; Lengyel, Mate] Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Cambridge CB2 1PZ, England	University of Cambridge	McNamee, D (corresponding author), Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Cambridge CB2 1PZ, England.	d.mcnamee@eng.cam.ac.uk; wolpert@eng.cam.ac.uk; m.lengyel@eng.cam.ac.uk	McNamee, Daniel/AAK-4700-2020; Lengyel, Mate/A-6665-2013	McNamee, Daniel/0000-0001-9928-4960; Lengyel, Mate/0000-0001-7266-0049				Balasubramanian V, 1997, NEURAL COMPUT, V9, P349, DOI 10.1162/neco.1997.9.2.349; Barnes TD, 2005, NATURE, V437, P1158, DOI 10.1038/nature04053; Bonasia K, 2016, HIPPOCAMPUS, V26, P9, DOI 10.1002/hipo.22539; Boutilier C, 1999, J ARTIF INTELL RES, V11, P1; Daw ND, 2005, NAT NEUROSCI, V8, P1704, DOI 10.1038/nn1560; Dayan P, 1992, ADV NEURAL INFORM PR; Foster D, 2002, MACH LEARN, P325; Fujii N, 2003, SCIENCE, V301, P1246, DOI 10.1126/science.1086872; Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638; Gershman SJ, 2009, J NEUROSCI, V29, P13524, DOI 10.1523/JNEUROSCI.2469-09.2009; Hauskrecht M, 1998, UNCERTAINTY ARTIFICI; Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045; Huys QJM, 2015, P NATL ACAD SCI USA, V112, P3098, DOI 10.1073/pnas.1414219112; Javadi AH, 2016, NATURE COMMUNICATION; Jin X, 2010, NATURE, V466, P457, DOI 10.1038/nature09263; Kafsi M, 2013, IEEE T INFORM THEORY, V59, P5577, DOI 10.1109/TIT.2013.2262497; Kemeny J.G., 1983, FINITE MARKOV CHAINS; Kim KE, 2003, ARTIF INTELL, V147, P225, DOI 10.1016/S0004-3702(02)00377-6; Lashley K.S., 1951, CEREBRAL MECH BEHAV, P112, DOI DOI 10.1093/RFS/HHQ153; Lengyel M, 2007, ADV NEURAL INFORM PR, V19; Littman ML, 1998, J ARTIF INTELL RES, V9, P1; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Moore AW, 1999, IJCAI INT JOINT C AR, V2, P1318; Otto AR, 2013, PSYCHOL SCI, V24, P751, DOI 10.1177/0956797612463080; PARR R, 1997, ADV NEURAL INFORM PR; Rissanen J., 2007, INFORM COMPLEXITY ST; Rosvall M, 2008, P NATL ACAD SCI USA, V105, P1118, DOI 10.1073/pnas.0706851105; Rothkopf CA, 2010, FRONT PSYCHOL, V1, DOI 10.3389/fpsyg.2010.00173; Russo E, 2008, NEW J PHYS, V10, DOI 10.1088/1367-2630/10/1/015008; Schapiro AC, 2013, NAT NEUROSCI, V16, P486, DOI 10.1038/nn.3331; Simon H, 1971, HUMAN PROBLEM SOLVIN; Simsek O, 2008, ADV NEURAL INFORM PR; Singh SP, 1995, ADV NEURAL INFORM PR; Smith KS, 2013, NEURON, V79, P361, DOI 10.1016/j.neuron.2013.05.038; Solway A, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003779; Stachenfeld K. L, 2014, ADV NEURAL INFORM PR; Stalnaker TA, 2010, FRONT INTEGR NEUROSC, V4, DOI 10.3389/fnint.2010.00012; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Todd MT, 2008, ADV NEURAL INFORM PR	40	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702067
C	Mercado, P; Tudisco, F; Hein, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mercado, Pedro; Tudisco, Francesco; Hein, Matthias			Clustering Signed Networks with the Geometric Mean of Laplacians	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				STRUCTURAL BALANCE	Signed networks allow to model positive and negative relationships. We analyze existing extensions of spectral clustering to signed networks. It turns out that existing approaches do not recover the ground truth clustering in several situations where either the positive or the negative network structures contain no noise. Our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of the Laplacians of the positive and negative part. As a solution we propose to use the geometric mean of the Laplacians of positive and negative part and show that it outperforms the existing approaches. While the geometric mean of matrices is computationally expensive, we show that eigenvectors of the geometric mean can be computed efficiently, leading to a numerical scheme for sparse matrices which is of independent interest.	[Mercado, Pedro; Hein, Matthias] Saarland Univ, Saarbrucken, Germany; [Tudisco, Francesco] Univ Padua, Padua, Italy	Saarland University; University of Padua	Mercado, P (corresponding author), Saarland Univ, Saarbrucken, Germany.				ERC starting grant NOLEPRO	ERC starting grant NOLEPRO	The authors acknowledge support by the ERC starting grant NOLEPRO	[Anonymous], 2010, P 2010 SIAM INT C DA, DOI [10.1137/1.9781611972801.49, DOI 10.1137/1.9781611972801.49]; Bhatia R., 2009, POSITIVE DEFINITE MA; Bini D., 2015, MATRIX MEANS TOOLBOX; CARTWRIGHT D, 1956, PSYCHOL REV, V63, P277, DOI 10.1037/h0046049; Chiang Kai-Yang, 2012, P 21 ACM INT C INF K, P615, DOI DOI 10.1145/2396761.2396841; DAVIS JA, 1967, HUM RELAT, V20, P181, DOI 10.1177/001872676702000206; DESAI M, 1994, J GRAPH THEOR, V18, P181, DOI 10.1002/jgt.3190180210; Doreian P, 2009, SOC NETWORKS, V31, P1, DOI 10.1016/j.socnet.2008.08.001; Druskin V, 1998, SIAM J MATRIX ANAL A, V19, P755, DOI 10.1137/S0895479895292400; Fasi M., 201629 MIMS; Harary F, 1953, MICH MATH J, V2, P143; Higham NJ, 2005, SIAM J MATRIX ANAL A, V26, P849, DOI 10.1137/S0895479804442218; Iannazzo B., 2015, NUMER LINEAR ALGEBRA; Iannazzo B., 2015, OPTIMIZATION ONLINE; Knizhnerman L, 2010, NUMER LINEAR ALGEBR, V17, P615, DOI 10.1002/nla.652; Kropivnik S, 1996, DEV STAT METHODOLOGY, P209; Leskovec J, 2014, SNAP DATASETS STANFO; Liu SP, 2015, ADV MATH, V268, P306, DOI 10.1016/j.aim.2014.09.023; Raissouli M, 2003, LINEAR ALGEBRA APPL, V359, P37, DOI 10.1016/S0024-3795(02)00427-5; Read KE, 1954, SOUTHWEST J ANTHROP, V10, P1; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Tang, 2015, ACM COMPUT SURV; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701021
C	Niu, G; du Plessis, MC; Sakai, T; Ma, Y; Sugiyama, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Niu, Gang; du Plessis, Marthinus C.; Sakai, Tomoya; Ma, Yao; Sugiyama, Masashi			Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing, it sometimes outperforms PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper, we theoretically compare PU (and NU) learning against PN learning based on the upper bounds on estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning, and we prove that, in terms of the upper bounds, either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly.	[Niu, Gang; du Plessis, Marthinus C.; Sakai, Tomoya; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan; [Sugiyama, Masashi] RIKEN, Wako, Saitama, Japan; [Ma, Yao] Boston Univ, Boston, MA 02215 USA	University of Tokyo; RIKEN; Boston University	Niu, G (corresponding author), Univ Tokyo, Tokyo, Japan.	gang@ms.k.u-tokyo.ac.jp; christo@ms.k.u-tokyo.ac.jp; sakai@ms.k.u-tokyo.ac.jp; yao@ms.k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	JST CREST program; Microsoft Research Asia; JSPS KAKENHI [15J09111]	JST CREST program(Core Research for Evolutional Science and Technology (CREST)Japan Science & Technology Agency (JST)); Microsoft Research Asia(Microsoft); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	GN was supported by the JST CREST program and Microsoft Research Asia. MCdP, YM, and MS were supported by the JST CREST program. TS was supported by JSPS KAKENHI 15J09111.	[Anonymous], NIPS; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169; Chapelle O., 2006, IEEE T NEURAL NETW, V20, P542; Chapelle O., 2009, WWW; Chung K. L., 1968, COURSE PROBABILITY T; Collobert R., 2006, ICML; Craswell N., 2008, WSDM; Denis F., 1998, ALT; du Plessis M. C., 2012, ICML; du Plessis M. C., 2015, ACML; du Plessis M. C., 2015, ICML; Dupret G., 2008, SIGIR; ELKAN C, 2008, KDD; Iyer A., 2014, ICML; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Letouzey F., 2000, ALT; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Meir R., 2003, J MACHINE LEARNING R, V4, P839; Mohri M., 2018, FDN MACHINE LEARNING; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Ramaswamy H., 2016, ICML; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446; Scholkopf B., 2001, LEARNING KERNELS; Scott  C., 2009, AISTATS; Vapnik V.N, 1998, STAT LEARNING THEORY; Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x; Yuille A. L., 2001, NIPS	32	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702101
C	Pan, XH; Lam, M; Tu, S; Papailiopoulos, D; Zhang, C; Jordan, MI; Ramchandran, K; Re, C; Recht, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Pan, Xinghao; Lam, Maximilian; Tu, Stephen; Papailiopoulos, Dimitris; Zhang, Ce; Jordan, Michael I.; Ramchandran, Kannan; Re, Chris; Recht, Benjamin			CYCLADES: Conflict-free Asynchronous Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present CYCLADES, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. CYCLADES is asynchronous during model updates, and requires no memory locking mechanisms, similar to HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts during parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms. Due to its inherent cache locality and conflict-free nature, our multi-core implementation of CYCLADES consistently outperforms HOGWILD!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to HOGWILD!, and up to 5x gains over asynchronous implementations of variance reduction algorithms.	[Pan, Xinghao; Lam, Maximilian; Tu, Stephen; Papailiopoulos, Dimitris; Jordan, Michael I.; Ramchandran, Kannan; Recht, Benjamin] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Zhang, Ce; Re, Chris] Stanford Univ, Dept Comp Sci, Palo Alto, CA 94304 USA; [Jordan, Michael I.; Recht, Benjamin] Univ Calif Berkeley, Dept Stat, Berkeley, CA USA	University of California System; University of California Berkeley; Stanford University; University of California System; University of California Berkeley	Pan, XH (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Arora Sanjeev, 2015, ARXIV150203520; Bertsekas D.P., 1989, PARALLEL DISTRIBUTED, V23; Chilimbi T., 2014, USENIX OSDI; De Sa C., 2015, ARXIV150606438; Dean J., 2012, ADV NEURAL INFORM PR, V25; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Duchi J., 2013, ADV NEURAL INFORM PR, P2832; Jin Chi, 2015, ARXIV151008896; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Jordan M.I., 2015, ADV NEURAL INFORM PR, P82; Krivelevich M., 2016, ELECTRON J COMB, V23; Langford J., 2009, PROC 22 INT C NEURAL, P2331; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Low Y., 2014, ARXIV PREPRINT ARXIV, V89, P90; Ma J., 2009, PROC 26 ANN INT C MA, V382, P681, DOI [DOI 10.1145/1553374.1553462, 10.1145/1553374.1553462]; Mania H., 2015, ARXIV150706970; Pan X., 2014, NIPS 27; Pan X., 2013, NIPS 26; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Reddi S. J., 2015, ARXIV150606840; Richtarik P., 2012, ARXIV12120873; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Yong Zhuang, 2013, P 7 ACM C REC SYST, P249, DOI DOI 10.1145/2507157.2507164; Zhang C, 2014, PROC VLDB ENDOW, V7, P1283, DOI 10.14778/2732977.2733001	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705018
C	Rabusseau, G; Kadri, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rabusseau, Guillaume; Kadri, Hachem			Low-Rank Regression with Tensor Responses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR computes accurate solutions while being computationally very competitive.	[Rabusseau, Guillaume; Kadri, Hachem] Aix Marseille Univ, CNRS, LIF, Marseille, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Aix-Marseille Universite	Rabusseau, G (corresponding author), Aix Marseille Univ, CNRS, LIF, Marseille, France.	guillaume.rabusseau@lif.univ.mrs.fr; hachem.kadri@lif.univ.mrs.fr			ANR JCJC program MAD [ANR- 14-CE27-0002]	ANR JCJC program MAD(French National Research Agency (ANR))	We thank Francois Denis and the reviewers for their helpful comments and suggestions. This work was partially supported by ANR JCJC program MAD (ANR- 14-CE27-0002).	ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580; Bahadori M. T., 2014, NIPS; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Cichocki Andrzej, 2009, NONNEGATIVE MATRIX T, P2; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Foygel R., 2012, NIPS; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1; Izenman AJ, 2008, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-78189-1_1; Kar P., 2012, AISTATS; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Long X., 2012, UBICOMP; LOZANO AC, 2009, KDD; Lu H., 2013, MULTILINEAR SUBSPACE; Mohri M., 2018, FDN MACHINE LEARNING; Mukherjee Ashin, 2011, Stat Anal Data Min, V4, P612, DOI 10.1002/sam.10138; Nickel Maximilian, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P272, DOI 10.1007/978-3-642-40991-2_18; Rahimi A., 2007, NIPS; Reinsel Gregory C., 1998, MULTIVARIATE REDUCED, DOI DOI 10.1007/978-1-4757-2853-8; Romera-Paredes B., 2013, ICML; Signoretto M., 2013, MACH LEARN, P1; Signoretto Marco, 2013, ARXIV13104977; Srebro N., 2004, NIPS; WARREN HE, 1968, T AM MATH SOC, V133, P167, DOI 10.2307/1994937; Wimalawarne K., 2014, NIPS; Zhao Q., 2013, ICASSP; Zhao QB, 2013, IEEE T PATTERN ANAL, V35, P1660, DOI 10.1109/TPAMI.2012.254; Zhou H, 2013, J AM STAT ASSOC, V108, P540, DOI 10.1080/01621459.2013.776499	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702049
C	Seeger, M; Salinas, D; Flunkert, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Seeger, Matthias; Salinas, David; Flunkert, Valentin			Bayesian Intermittent Demand Forecasting for Large Inventories	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INFERENCE	We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.	[Seeger, Matthias; Salinas, David; Flunkert, Valentin] Amazon Dev Ctr Germany, Krausenstr 38, D-10115 Berlin, Germany	Amazon.com	Seeger, M (corresponding author), Amazon Dev Ctr Germany, Krausenstr 38, D-10115 Berlin, Germany.	matthis@amazon.de; dsalina@amazon.de; flunkert@amazon.de						Barber D., 2011, BAYESIAN TIME SERIES; Barber D, 2006, J MACH LEARN RES, V7, P2515; Beal M.J, 2003, THESIS; Bishop C.M, 2006, PATTERN RECOGN; Box G.E., 2013, TIME SERIES ANAL FOR; Chapados N, 2014, PR MACH LEARN RES, V32, P1395; Durbin J., 2012, OXFORD STATIST SCI S, Vsecond; Heskes Tom, 2002, UNCERTAINTY ARTIFICI, V18; Hyndman RJ, 2008, SPRINGER SER STAT, P1, DOI 10.1007/978-3-540-71918-2; Hyndman RJ, 2008, J STAT SOFTW, V27, P1, DOI 10.18637/jss.v027.i03; McCullach P., 1983, GEN LINEAR MODELS; Minka T. P., 2001, P 17 C UNCER TAINTY, P362; Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x; Snyder LV., 2011, FUNDAMENTALS SUPPLY; Snyder RD, 2012, INT J FORECASTING, V28, P485, DOI 10.1016/j.ijforecast.2011.03.009; Zaharia M., 2012, USENIX NSDI 12, P2, DOI DOI 10.1111/J.1095-8649.2005.00662.X	16	2	2	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702060
C	Teymur, O; Zygalakis, K; Calderhead, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Teymur, Onur; Zygalakis, Konstantinos; Calderhead, Ben			Probabilistic Linear Multistep Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature [1, 2, 3, 4]. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice.	[Teymur, Onur; Calderhead, Ben] Imperial Coll London, Dept Math, London, England; [Zygalakis, Konstantinos] Univ Edinburgh, Sch Math, Edinburgh, Midlothian, Scotland	Imperial College London; University of Edinburgh	Teymur, O (corresponding author), Imperial Coll London, Dept Math, London, England.	o@teymur.uk; k.zygalakis@ed.ac.uk; b.calderhead@imperial.ac.uk	Zygalakis, Konstantinos/AAZ-8636-2020	Zygalakis, Konstantinos/0000-0002-3860-9167	Simons Foundation; Alan Turing Institute under the EPSRC [EP/N510129/1]	Simons Foundation; Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	KZ was partially supported by a grant from the Simons Foundation and by the Alan Turing Institute under the EPSRC grant EP/N510129/1. Part of this work was done during the author's stay at the Newton Institute for the programme Stochastic Dynamical Systems in Biology: Numerical Methods and Applications.	Bilotta E, 2008, WORLD SCI SERIES NON; Buckwar E, 2006, SIAM J NUMER ANAL, V44, P779, DOI 10.1137/040602857; Butcher J C, 2008, NUMERICAL METHODS OR; CHKREBTII O. A., 2016, BAYESIAN ANAL; Chua L.O., 2007, SCHOLARPEDIA, V2, P1488; CHUA LO, 1992, AEU-INT J ELECTRON C, V46, P250; CONRAD P. R., 2016, STAT COMPUTING; Diaconis P., 1988, STAT DECISION THEORY, V1, P163, DOI DOI 10.1007/978-1-4613-8768-8_20; FORNBERG B, 1988, MATH COMPUT, V51, P699, DOI 10.1090/S0025-5718-1988-0935077-0; Girolami M, 2008, THEOR COMPUT SCI, V408, P4, DOI 10.1016/j.tcs.2008.07.005; Hairer E., 2008, SOLVING ORDINARY DIF; Hennig P., 2014, P 17 INT C ART INT S, V33; Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142; Iserles A., 2008, 1 COURSE NUMERICAL A; Kennedy MC, 2001, J R STAT SOC B, V63, P425, DOI 10.1111/1467-9868.00294; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schober M, 2014, ADV NEURAL INFORM PR, V27, P739; SKILLING J, 1993, PHYSICS AND PROBABILITY, P207, DOI 10.1017/CBO9780511524448.020	18	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702103
C	Woodworth, B; Srebro, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Woodworth, Blake; Srebro, Nathan			Tight Complexity Bounds for Optimizing Composite Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.	[Woodworth, Blake; Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Woodworth, B (corresponding author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	blake@ttic.edu; nati@ttic.edu						Agarwal, 2014, ARXIV14100723; Agarwal A, 2009, IMMUNE INFERTILITY, P155, DOI 10.1007/978-3-642-01379-9_3.2; Allen-Zhu Z., 2016, ARXIV160305953; Allen-Zhu Zeyuan, 2016, ARXIV160305642; [Anonymous], ARXIV150702000; Arjevani Y., 2015, NIPS, P1747; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lin Hongzhou, 2015, ADV NEURAL INFORM PR, P3366; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Orabona F., 2012, ARXIV12062372; Schmidt Mark, 2013, ARXIV13092388; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Shalev-Shwartz Shai, 2016, STOCHASTIC OPTIMIZAT; Shamir O., 2013, ARXIV13127853; Yu B., 1997, FESTSCHRIFT L LECAM, P423; ZHANG Y, 2015, ARXIV150100263	20	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700022
C	Xu, LB; Davenport, MA		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xu, Liangbei; Davenport, Mark A.			Dynamic matrix recovery from incomplete observations under an exact low-rank constraint	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				COMPLETION	Low-rank matrix factorizations arise in a wide variety of applications-including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the matrix sensing and matrix completion observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results.	[Xu, Liangbei; Davenport, Mark A.] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30318 USA	University System of Georgia; Georgia Institute of Technology	Xu, LB (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30318 USA.	lxu66@gatech.edu; mdav@gatech.edu			NRL [N00173-14-2-C001]; AFOSR [FA9550-14-1-0342]; NSF [CCF-1409406, CCF-1350616, CMMI-1537261]	NRL; AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); NSF(National Science Foundation (NSF))	This work was supported by grants NRL N00173-14-2-C001, AFOSR FA9550-14-1-0342, NSF CCF-1409406, CCF-1350616, and CMMI-1537261.	Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Dror G., 2011, P ACM SIGKDD INT C K; Hardt M., 2014, P IEEE S FDN COMP SC; Hardt M., 2014, P C LEARN THEOR BARC; Jain P., 2015, P C LEARN THEOR PAR; Jain Prateek, 2013, P ACM S THEOR COMP S; Keshavan R., 2009, P ADV NEUR PROC SYST; Klopp O, 2014, BERNOULLI, V20, P282, DOI 10.3150/12-BEJ486; Koren Y., 2009, THE NETFLIX PRIZE, V81, P1; Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677; Mohammadiha N, 2015, IEEE T SIGNAL PROCES, V63, P949, DOI 10.1109/TSP.2014.2385655; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Recht B., 2008, P IEEE C DEC CONTR C; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Sun R., 2015, P IEEE S FDN COMP SC; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Zhao T., 2015, P ADV NEURAL PROCESS	25	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703096
C	Zhao, H; Poupart, P; Gordon, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhao, Han; Poupart, Pascal; Gordon, Geoff			A Unified Approach for Learning the Parameters of Sum-Product Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.	[Zhao, Han; Gordon, Geoff] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Poupart, Pascal] Univ Waterloo, Sch Comp Sci, Waterloo, ON, Canada	Carnegie Mellon University; University of Waterloo	Zhao, H (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	han.zhao@cs.cmu.edu; ppoupart@uwaterloo.ca; ggordon@cs.cmu.edu			ONR [N000141512365]	ONR(Office of Naval Research)	HZ and GG gratefully acknowledge support from ONR contract N000141512365. HZ also thanks Ryan Tibshirani for the helpful discussion about CCCP.	[Anonymous], 2015, THESIS GRAZ U TECHNO; Boyd S, 2007, OPTIM ENG, V8, P67, DOI 10.1007/s11081-007-9001-7; Chan H., P 22 C UNC ART INT; Chiang M., 2005, GEOMETRIC PROGRAMMIN, DOI DOI 10.1561/9781933019574; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Dennis A., 2015, INT JOINT C ART INT, V24; Gens R., 2012, 26 ADV NEURAL INFORM, P3239; Gens R., 2013, 30 INT C MACHINE LEA, P873; Gunawardana A, 2005, J MACH LEARN RES, V6, P2049; HARTMAN P., 1959, PACIFIC J MATH, V9, P707, DOI [10.2140/pjm.1959.9.707, DOI 10.2140/PJM.1959.9.707]; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Lanckriet G. R., 2009, CONVERGENCE CONCAVE, P1759; Peharz R., 2015, AISTATS; Poon H., 2011, P 12 C UNC ART INT, P2551; Rooshenas A, 2014, ICML; Salakhutdinov R., 2002, UAI; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Yuille AL, 2002, ADV NEUR IN, V14, P1033; Zangwill W. I., 1969, NONLINEAR PROGRAMMIN, V196; Zhao H., 2015, ICML; Zhao Han, 2016, ICML	21	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703059
C	Adeli-Mosabbeb, E; Thung, KH; An, L; Shi, F; Shen, DG		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Adeli-Mosabbeb, Ehsan; Thung, Kim-Han; An, Le; Shi, Feng; Shen, Dinggang		ADNI	Robust Feature-Sample Linear Discriminant Analysis for Brain Disorders Diagnosis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SEGMENTATION; IMAGES	A wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks. However, many existing discriminative methods assume that the input data is nearly noise-free, which limits their applications to solve real-world problems. Particularly for disease diagnosis, the data acquired by the neuroimaging devices are always prone to different sources of noise. Robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers. These methods focus on detecting either the sample-outliers or feature-noises. Moreover, they usually use unsupervised de-noising procedures, or separately de-noise the training and the testing data. All these factors may induce biases in the learning process, and thus limit its performance. In this paper, we propose a classification method based on the least-squares formulation of linear discriminant analysis, which simultaneously detects the sample-outliers and feature-noises. The proposed method operates under a semi-supervised setting, in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space. Therefore, the violating samples or feature values are identified as sample-outliers or feature-noises, respectively. We test our algorithm on one synthetic and two brain neurodegenerative databases (particularly for Parkinson's disease and Alzheimer's disease). The results demonstrate that our method outperforms all baseline and state-of-the-art methods, in terms of both accuracy and the area under the ROC curve.	[Adeli-Mosabbeb, Ehsan] Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA; Univ N Carolina, BRIC, Chapel Hill, NC 27599 USA	University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina; University of North Carolina Chapel Hill	Adeli-Mosabbeb, E (corresponding author), Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA.	eadeli@med.unc.edu; khthung@med.unc.edu; le_an@med.unc.edu; fengshi@med.unc.edu; dgshen@med.unc.edu	Shen, Dinggang/ABF-6812-2020	Shen, Dinggang/0000-0002-7934-5698				Adeli-Mosabbeb E, 2015, IMAGE VISION COMPUT, V39, P38, DOI 10.1016/j.imavis.2015.04.006; Bissantz N, 2009, SIAM J OPTIMIZ, V19, P1828, DOI 10.1137/050639132; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Braak H, 2003, NEUROBIOL AGING, V24, P197, DOI 10.1016/S0197-4580(02)00065-9; Cai D., 2007, CVPR; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chapelle O., 2006, IEEE T NEURAL NETW, V20, P542; Croux C, 2001, CAN J STAT, V29, P473, DOI 10.2307/3316042; De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184; Elhamifar E., 2011, CVPR; Fidler S, 2006, IEEE T PATTERN ANAL, V28, P337, DOI 10.1109/TPAMI.2006.46; Fritsch V, 2012, MED IMAGE ANAL, V16, P1359, DOI 10.1016/j.media.2012.05.002; Goldberg A., 2010, P NIPS, V23, P757; Huang D, 2012, LECT NOTES COMPUT SC, V7575, P616, DOI 10.1007/978-3-642-33765-9_44; Joulin A., 2012, ICML; Kim S. J., 2005, P NEUR INF PROC SYST, P659; Li H., 2003, P ADV NEUR INF PROC, P97; Li H., 2015, IEEE TIP, V24; LIM KO, 1989, J COMPUT ASSIST TOMO, V13, P588, DOI 10.1097/00004728-198907000-00006; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Lu CW, 2013, PROC CVPR IEEE, P415, DOI 10.1109/CVPR.2013.60; Marek K, 2011, PROG NEUROBIOL, V95, P629, DOI 10.1016/j.pneurobio.2011.09.005; PEARCE BR, 1984, NEUROCHEM PATHOL, V2, P221; Shen DG, 2002, IEEE T MED IMAGING, V21, P1421, DOI 10.1109/TMI.2002.803111; Thung KH, 2014, NEUROIMAGE, V91, P386, DOI 10.1016/j.neuroimage.2014.01.033; Tzourio-Mazoyer N, 2002, NEUROIMAGE, V15, P273, DOI 10.1006/nimg.2001.0978; Wagner A, 2009, PROC CVPR IEEE, P597, DOI 10.1109/CVPRW.2009.5206654; Wang YY, 2014, NANOSCALE RES LETT, V9, P1, DOI 10.1186/1556-276X-9-251; Wang YP, 2011, LECT NOTES COMPUT SC, V6893, P635, DOI 10.1007/978-3-642-23626-6_78; WORKER A, 2014, PLOS ONE, V9; Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008; Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424; Zhu X.J, 2005, SEMISUPERVISED LEARN; Ziegler David A, 2013, Imaging Med, V5, P91	35	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100030
C	Asteris, M; Papailiopoulos, D; Kyrillidis, A; Dimakis, AG		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Asteris, Megasthenis; Papailiopoulos, Dimitris; Kyrillidis, Anastasios; Dimakis, Alexandros G.			Sparse PCA via Bipartite Matchings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PRINCIPAL COMPONENT; POWER METHOD; ROTATION	We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. Such components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the input data. We evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches.	[Asteris, Megasthenis; Kyrillidis, Anastasios; Dimakis, Alexandros G.] Univ Texas Austin, Austin, TX 78712 USA; [Papailiopoulos, Dimitris] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of Texas System; University of Texas Austin; University of California System; University of California Berkeley	Asteris, M (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	megas@utexas.edu; dimitrisp@berkeley.edu; anastasios@utexas.edu; dimakis@austin.utexas.edu	Dimakis, Alexandros G/P-6034-2019; Dimakis, Alexandros G/A-5496-2011	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033	NSF [CCF-1217058, CCF-1116404, CCF 1344179, 1344364, 1407278, 1422549]; MURI AFOSR [556016]; ARO [YIP W911NF-14-1-0258]	NSF(National Science Foundation (NSF)); MURI AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI); ARO	DP is generously supported by NSF awards CCF-1217058 and CCF-1116404 and MURI AFOSR grant 556016. This research has been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258.	Asteris M., 2015, ARXIV150800625; Asteris M, 2014, IEEE T INFORM THEORY, V60, P2281, DOI 10.1109/TIT.2014.2303975; Boutsidis C., 2011, ADV NEURAL INFORM PR, P2285; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Jenatton Rodolphe, 2010, P 13 INT C ART INT S, P366; Jiang R., 2011, P 17 ACM SIGKDD INT, P886, DOI DOI 10.1145/2020408.2020557; Johnstone Iain M, 2009, J AM STAT ASS, V104; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Journee M, 2010, J MACH LEARN RES, V11, P517; KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; Lichman M., 2013, UCI MACHINE LEARNING; Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097; Mackey L., 2009, P ADV NEUR INF PROC, V21, P1017; Magdon-Ismail M., 2015, ARXIV150206626; Magdon-Ismail M., 2015, CORR; Majumdar A, 2009, SIGNAL IMAGE VIDEO P, V3, P27, DOI 10.1007/s11760-008-0056-5; Moghaddam B., 2006, ADV NEURAL INFORM PR, P915; Papailiopoulos D. S., 2013, ICML, P747; Ramshaw L., 2012, HPL201240; Richard E., 2014, P ADV NEUR INF PROC, P3284; Sigg C.D., 2008, P 25 INT C MACH LEAR, P960, DOI [DOI 10.1145/1390156.1390277, 10.1145/1390156.1390277]; Vu V., 2012, INT C ARTIFICIAL INT, P1278; Vu VQ, 2013, ADV NEURAL INFORM PR, V26; Wang Z., 2014, ARXIV14085352; Wang Z., 2013, P MACHINE LEARNING R, V31, P48; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zhang YW, 2012, INT SER OPER RES MAN, V166, P915, DOI 10.1007/978-1-4614-0769-0_31; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102050
C	Bachman, P; Precup, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bachman, Philip; Precup, Doina			Data Generation as Sequential Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We connect a broad class of generative models through their shared reliance on sequential decision making. Motivated by this view, we develop extensions to an existing model, and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct the models using neural networks and train them using a form of guided policy search [9]. Our models generate predictions through an iterative process of feedback and refinement. We show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets.	[Bachman, Philip; Precup, Doina] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada	McGill University	Bachman, P (corresponding author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.	phil.bachman@gmail.com; dprecup@cs.mcgill.ca						Denton E. L., 2015, ARXIV150605751CSCV; Graves A., 2013, ARXIV13080850CSNE; Gregor K., 2015, INT C MACH LEARN ICM; Gregor K., 2014, INT C MACH LEARN ICM; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma D. P, 2014, ADV NEURAL INFORM PR; Larochelle Hugo, 2011, INT C MACH LEARN ICM; Levine S., 2014, INT C MACH LEARN ICM; Levine S., 2014, ADV NEURAL INFORM PR; Levine S., 2013, ADV NEURAL INFORM PR; Levine Sergey, 2013, INT C MACH LEARN ICM; Mnih Andriy, 2014, INT C MACH LEARN ICM; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Rezende Danilo, 2015, INT C MACH LEARN ICM; Rezende Danilo Jimenez, 2014, INT C MACH LEARN ICM; Sohl-Dickstein Jascha, 2015, INT C MACH LEARN ICM; Susskind J. M., 2010, TORONTO FACE DATABAS, V3	17	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101019
C	Chwialkowski, K; Ramdas, A; Sejdinovic, D; Gretton, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chwialkowski, Kacper; Ramdas, Aaditya; Sejdinovic, Dino; Gretton, Arthur			Fast Two-Sample Testing with Analytic Representations of Probability Measures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				KERNEL HILBERT-SPACES; STATISTICS	We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distancebased tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.	[Chwialkowski, Kacper; Gretton, Arthur] UCL, Gatsby Computat Neurosci Unit, London, England; [Ramdas, Aaditya] Univ Calif Berkeley, Dept EECS & Stat, Berkeley, CA USA; [Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England	University of London; University College London; University of California System; University of California Berkeley; University of Oxford	Chwialkowski, K (corresponding author), UCL, Gatsby Computat Neurosci Unit, London, England.	kacper.chwialkowski@gmail.com; aramdas@cs.berkeley.edu; dino.sejdinovic@gmail.com; arthur.gretton@gmail.com						Anderson T. W., 2003, INTRO MULTIVARIATE S; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Berlinet A., 2004, REPRODUCING KERNEL H, V3; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; DAVIDSON KR, 1983, AM MATH MON, V90, P391, DOI 10.2307/2975578; Epps TW., 1986, J STAT COMPUT SIM, V26, P177, DOI [10.1080/00949658608810963, DOI 10.1080/00949658608810963]; Gretton A., 2009, NIPS; Gretton A., 2012, ADV NEURAL INFORM PR, P1214; Gretton A, 2012, J MACH LEARN RES, V13, P723; Harchaoui Z., 2008, NIPS; HEATHCOTE CR, 1977, BIOMETRIKA, V64, P255, DOI 10.2307/2335691; HEATHCOTE CR, 1972, AUST J STAT, V14, P172, DOI 10.1111/j.1467-842X.1972.tb00355.x; Ho HC, 2006, SCAND J STAT, V33, P861, DOI 10.1111/j.1467-9469.2006.00516.x; Hotelling H, 1931, ANN MATH STAT, V2, P360, DOI 10.1214/aoms/1177732979; Le Q.V., 2013, JMLR W CP, P244; Lichman M., 2013, UCI MACHINE LEARNING; Lloyd J. R., 2014, TECHNICAL REPORT; Pevny T, 2008, LECT NOTES COMPUT SC, V5284, P251; Rahimi A., 2007, NIPS; Ramdas A., 2015, AAAI; Reddi S., 2015, AISTATS; Rudin W, 1987, REAL COMPLEX ANAL; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Steinwart I, 2006, IEEE T INFORM THEORY, V52, P4635, DOI 10.1109/TIT.2006.881713; Sun HW, 2008, J FOURIER ANAL APPL, V14, P89, DOI 10.1007/s00041-007-9003-z; Szekely GJ, 2003, TECHNICAL REPORT; Zaremba W., 2013, NIPS; Zhao J, 2015, NEURAL COMPUT, V27, P1345, DOI 10.1162/NECO_a_00732; Zinger A.A., 1992, J MATH SCI, V4, P914, DOI DOI 10.1007/BF01099119	34	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100053
C	Clevert, DA; Mayr, A; Unterthiner, T; Hochreiter, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Clevert, Djork-Arne; Mayr, Andreas; Unterthiner, Thomas; Hochreiter, Sepp			Rectified Factor Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NEURAL-NETWORKS; OPTIMIZATION	We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods. RFN package for GPU/CPU is available at http://www.bioinf.jku.at/software/rfn.	[Clevert, Djork-Arne; Mayr, Andreas; Unterthiner, Thomas; Hochreiter, Sepp] Johannes Kepler Univ Linz, Inst Bioinformat, Linz, Austria	Johannes Kepler University Linz	Clevert, DA (corresponding author), Johannes Kepler Univ Linz, Inst Bioinformat, Linz, Austria.	okko@bioinf.jku.at; mayr@bioinf.jku.at; unterthiner@bioinf.jku.at; hochreit@bioinf.jku.at	Clevert, Djork Arne/AAT-1782-2021; Hochreiter, Sepp/AAI-5904-2020; Unterthiner, Thomas/K-7231-2018	Hochreiter, Sepp/0000-0001-7449-2528; Unterthiner, Thomas/0000-0001-5361-3087				Abadie J., 1969, GENERALIZATION WOLFE; Ben-Tal A., 2001, INTERIOR POINT POLYN, P377; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Y., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556; BERTSEKAS DP, 1976, IEEE T AUTOMAT CONTR, V21, P174, DOI 10.1109/TAC.1976.1101194; BERTSEKAS DP, 1982, SIAM J CONTROL OPTIM, V20, P221, DOI 10.1137/0320018; Frey BJ, 1999, NEURAL COMPUT, V11, P193, DOI 10.1162/089976699300016872; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Glorot X, 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1177/1753193410395357; Gunawardana A, 2005, J MACH LEARN RES, V6, P2049; Harva M, 2007, SIGNAL PROCESS, V87, P509, DOI 10.1016/j.sigpro.2006.06.006; Haug E.J., 1979, APPL OPTIMAL DESIGN; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hochreiter S, 2006, BIOINFORMATICS, V22, P943, DOI 10.1093/bioinformatics/btl033; Hochreiter S, 2013, NUCLEIC ACIDS RES, V41, DOI 10.1093/nar/gkt1013; Hochreiter S, 2010, BIOINFORMATICS, V26, P1520, DOI 10.1093/bioinformatics/btq227; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; Kelley C. T., 1999, ITERATIVE METHODS OP; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; LeCun Y., 2004, P IEEE C COMP VIS PA; Nair V., 2010, ICML, P807; Palmer Jason, 2006, ADV NEURAL INFORM PR, P1059; ROSEN JB, 1961, J SOC IND APPL MATH, V9, P514, DOI 10.1137/0109044; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Srebro N., 2004, THESIS; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Verbist B, 2015, DRUG DISCOV TODAY, V20, P505, DOI 10.1016/j.drudis.2014.12.014; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Zangwill W. I., 1969, NONLINEAR PROGRAMMIN	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103001
C	Desjardins, G; Simonyan, K; Pascanu, R; Kavukcuoglu, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Desjardins, Guillaume; Simonyan, Karen; Pascanu, Razvan; Kavukcuoglu, Koray			Natural Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.	[Desjardins, Guillaume; Simonyan, Karen; Pascanu, Razvan; Kavukcuoglu, Koray] Google DeepMind, London, England	Google Incorporated	Desjardins, G (corresponding author), Google DeepMind, London, England.	gdesjardins@google.com; simonyan@google.com; razp@google.com; korayk@google.com						Amari S., 1998, NEURAL COMPUTATION; Ba J., 2014, NIPS; Beck Amir, 2003, OPER RES LETT; Bottou L, 1998, LECT NOTES COMPUTER, V1524; Combettes P. L., 2009, ARXIV E PRINTS; Duchi J., 2011, JMLR; Glorot X., 2010, P 13 INT C ART INT S, VVolume 9, P249; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Martens J., 2010, ICML; Martens Roger Grosse James, 2015, ICML; Miiller K.-R., 2013, NEURAL NETWORKS TRIC; Nicol N, 1998, IDSIA3398; Ollivier Yann, 2013, ABS13030818 ARXIV; Pascanu Razvan, 2014, ICLR; Povey Daniel, 2015, ICLR WORKSH; Raiko T., 2012, AISTATS; Raskutti G., 2013, INFORM GEOMETRY MIRR; Russakovsky O, 2015, IMAGENET LARGE SCALE, V115, P211; Salakhutdinov Ruslan, 2015, ICML; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Sohl-Dickstein Jascha, 2012, NATURAL GRADIENT ANA; Srivastava N., 2014, J MACHINE LEARNING R; Szegedy C., 2014, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7298594; Thomas Philip S, 2013, ADV NEURAL INFORM PR, V26; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Vatanen Tommi, 2013, ICONIP	28	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102101
C	Diakonikolas, I; Hardt, M; Schmidt, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Diakonikolas, Ilias; Hardt, Moritz; Schmidt, Ludwig			Differentially Private Learning of Structured Discrete Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DENSITY; ALGORITHM	We investigate the problem of learning an unknown probability distribution over a discrete population from random samples. Our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing Differential Privacy to the individuals of the population. We describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families. Our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient-both in terms of sample size and running time-as their non-private counterparts. We complement our theoretical guarantees with an experimental evaluation. Our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set.	[Diakonikolas, Ilias] Univ Edinburgh, Edinburgh, Midlothian, Scotland; [Hardt, Moritz] Google Res, Mountain View, CA USA; [Schmidt, Ludwig] MIT, Cambridge, MA 02139 USA	University of Edinburgh; Google Incorporated; Massachusetts Institute of Technology (MIT)	Diakonikolas, I (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.				EPSRC [EP/L021749/1]; Marie Curie Career Integration grant; MADALGO; MIT-Shell Initiative	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Marie Curie Career Integration grant; MADALGO; MIT-Shell Initiative	Ilias Diakonikolas was supported by EPSRC grant EP/L021749/1 and a Marie Curie Career Integration grant. Ludwig Schmidt was supported by MADALGO and a grant from the MIT-Shell Initiative.	Acharya J., 2015, SAMPLE OPTIMAL DENSI; Balabdaoui F, 2007, ANN STAT, V35, P2536, DOI 10.1214/009053607000000262; Baldi P., 2014, NATURE COMMUNICATION; Beimel Amos, 2013, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Algorithms and Techniques. 16th International Workshop, APPROX 2013 and 17th International Workshop, RANDOM 2013. Proceedings: LNCS 8096, P363, DOI 10.1007/978-3-642-40328-6_26; Birge L, 1997, ANN STAT, V25, P970, DOI 10.1214/aos/1069362733; BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488; Bun M., 2015, CORR; Chan S., 2013, SODA; Chan SO, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P604, DOI 10.1145/2591796.2591848; Chan Siu-on, 2014, ADV NEURAL INFORM PR, P1844; Daskalakis C., 2012, SODA, P1371; Devroye L., 2001, SPRINGER SERIES STAT; Duchi J., 2013, ADV NEURAL INFORM PR, V26, P1529; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dumbgen L, 2009, BERNOULLI, V15, P40, DOI 10.3150/08-BEJ141; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Dwork C., 2010, FOCS; Dwork C, 2009, LECT NOTES COMPUT SC, V5444, P496; Feldman J, 2005, ANN IEEE SYMP FOUND, P501; Freund Y., 1999, COLT; Grenander U, 1956, SKAND AKTUARIETIDSK, V1956, P125; Groeneboom P., 1985, P BERK C HON JERZ NE, VII, P539; Hardt M., 2012, NIPS; Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155; Li C, 2014, PROC VLDB ENDOW, V7, P341, DOI 10.14778/2732269.2732271; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; RAO BLSP, 1969, SANKHYA SER A, V31, P23; ROTE G, 1992, COMPUTING, V48, P337, DOI 10.1007/BF02238642; Walther G., 2009, STAT SCI; WARNER S, 1965, J AM STAT ASS, V60	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100081
C	Gabrie, M; Tramel, EW; Krzakala, F		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gabrie, Marylou; Tramel, Eric W.; Krzakala, Florent			Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				LEARNING ALGORITHM	Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence, for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function. We believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training Boltzmann machines with hidden units.	[Gabrie, Marylou] Ecole Normale Super, CNRS, Lab Phys Stat, UMR 8550, F-75005 Paris, France; Univ Paris 06, F-75005 Paris, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute of Physics (INP); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Sorbonne Universite; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite	Gabrie, M (corresponding author), Ecole Normale Super, CNRS, Lab Phys Stat, UMR 8550, F-75005 Paris, France.	marylou.gabrie@lps.ens.fr; eric.tramel@lps.ens.fr; florent.krzakala@ens.fr	Krzakala, Florent/Q-9652-2019	Krzakala, Florent/0000-0003-2313-2578	European Research Council under the European Union's 7th Framework Programme (FP/2007-2013/ERC Grant) [307087-SPARCS]	European Research Council under the European Union's 7th Framework Programme (FP/2007-2013/ERC Grant)	We would like to thank F. Caltagirone and A. Decelle for many insightful discussions. This research was funded by European Research Council under the European Union's 7th Framework Programme (FP/2007-2013/ERC Grant Agreement 307087-SPARCS).	Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Cocco S, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.090601; Cocco S, 2009, P NATL ACAD SCI USA, V106, P14058, DOI 10.1073/pnas.0906705106; GALLAND CC, 1993, NETWORK-COMP NEURAL, V4, P355, DOI 10.1088/0954-898X/4/3/007; GEORGES A, 1991, J PHYS A-MATH GEN, V24, P2173, DOI 10.1088/0305-4470/24/9/024; Goodfellow I. J., 2013, 13013568 ARXIV; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hinton GE, 1989, NEURAL COMPUT, V1, P143, DOI 10.1162/neco.1989.1.1.143; Kappen HJ, 1998, ADV NEUR IN, V10, P280; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Marlin BM., 2010, P 13 INT C ART INT S, V9, P509; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Opper M, 2001, ADV MEAN FIELD METHO; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Peterson C., 1987, Complex Systems, V1, P995; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; Salakhutdinov R., 2007, P 24 INT C MACHINE L, V227, P791, DOI [DOI 10.1145/1273496.1273596, 10.1145/1273496.1273596]; Salakhutdinov R, 2009, ADV NEURAL INFORM PR; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Smolensky P., 1986, PROCESSING PARALLEL, V1; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Welling M, 2002, LECT NOTES COMPUT SC, V2415, P351	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101048
C	Gillenwater, J; Iyer, R; Lusch, B; Kidambi, R; Bilmes, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gillenwater, Jennifer; Iyer, Rishabh; Lusch, Bethany; Kidambi, Rahul; Bilmes, Jeff			Submodular Hamming Metrics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We show that there is a largely unexplored class of functions (positive polymatroids) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over. By exploiting submodularity, we are able to give hardness results and approximation algorithms for optimizing over such metrics. Additionally, we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task (a form of clustering) and also a metric maximization task (generating diverse k-best lists).	[Gillenwater, Jennifer; Iyer, Rishabh; Kidambi, Rahul; Bilmes, Jeff] Univ Washington, Dept EE, Seattle, WA 98195 USA; [Lusch, Bethany] Univ Washington, Dept Appl Math, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Gillenwater, J (corresponding author), Univ Washington, Dept EE, Seattle, WA 98195 USA.	jengi@uw.edu; rkiyer@uw.edu; herwaldt@uw.edu; rkidambi@uw.edu; bilmes@uw.edu	Lusch, Bethany/AAQ-7328-2020	Lusch, Bethany/0000-0002-9521-9990				Arthur D, 2007, SODA; Bateni M., 2010, TECHNICAL REPORT; Batra D., 2012, ECCV; Buchbinder N., 2012, FOCS; Buchbinder N., 2014, SODA; CUNNINGHAM WH, 1985, COMBINATORICA, V5, P185, DOI 10.1007/BF02579361; Goel G., 2009, FOCS; Gusfield D., 1997, ALGORITHMS STRINGS T; Halmos P.R., 1974, MEASURE THEORY; HAZAN T, 2013, NIPS; Iyer R., 2013, ICML; Iyer R., 2012, NIPS; Iyer R. K, 2013, NIPS; Jegelka S., 2011, ICML; Jegelka S., 2011, CVPR; Lin H., ACL; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Mikolov T., 2013, ADV NEURAL INF PROCE, V26; Nemhauser G., 1978, ANAL APPROXIMATIONS, V14; Osokin A., 2014, ECCV; Svitkina Z., 2008, FOCS; Szummer M., 2008, ECCV; Tschiatschek S., 2014, NIPS; Vondrak J., 2010, RIMS KOKYUROKU BESSA, V23; Yu J., 2015, ICML	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101001
C	Hartline, J; Syrgkanis, V; Tardos, E		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hartline, Jason; Syrgkanis, Vasilis; Tardos, Eva			No-Regret Learning in Bayesian Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information.	[Hartline, Jason] Northwestern Univ, Evanston, IL 60208 USA; [Syrgkanis, Vasilis] Microsoft Res, New York, NY USA; [Tardos, Eva] Cornell Univ, Ithaca, NY USA	Northwestern University; Microsoft; Cornell University	Hartline, J (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.	hartline@northwestern.edu; vasy@microsoft.com; eva@cs.cornell.edu	Hartline, Jason/B-7167-2009					Bergemann D., 2011, COWLES FDN DISCUSSIO; Blum A, 2008, ACM S THEORY COMPUT, P373; Cai Y., 2014, ACM C EC COMPUTATION, P895; Caragiannis Ioannis, 2014, J EC THEORY; de Keijzer B, 2013, LECT NOTES COMPUT SC, V8125, P385, DOI 10.1007/978-3-642-40450-4_33; FORGES F, 1993, THEOR DECIS, V35, P277, DOI 10.1007/BF01075202; Foster DP, 1998, BIOMETRIKA, V85, P379, DOI 10.1093/biomet/85.2.379; Kaplan TR, 2012, ECON THEOR, V50, P269, DOI 10.1007/s00199-010-0563-9; Koutsoupias E, 1999, LECT NOTES COMPUT SC, V1563, P404; Lucier B, 2010, PROC APPL MATH, V135, P537; Roughgarden T, 2009, ACM S THEORY COMPUT, P513; Syrgkanis V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P211; Vetta A, 2002, ANN IEEE SYMP FOUND, P416, DOI 10.1109/SFCS.2002.1181966	13	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103054
C	Hofmann, T; Lucchi, A; Lacoste-Julien, S; McWilliams, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hofmann, Thomas; Lucchi, Aurelien; Lacoste-Julien, Simon; McWilliams, Brian			Variance Reduced Stochastic Gradient Descent with Neighbors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its slow convergence can be a computational bottleneck. Variance reduction techniques such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving linear convergence. However, these methods are either based on computations of full gradients at pivot points, or on keeping per data point corrections in memory. Therefore speed-ups relative to SGD may need a minimal number of epochs in order to materialize. This paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points, which offers advantages in the transient optimization phase. As a side-product we provide a unified convergence analysis for a family of variance reduction algorithms, which we call memorization algorithms. We provide experimental results supporting our theory.	[Hofmann, Thomas; Lucchi, Aurelien; McWilliams, Brian] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Lacoste-Julien, Simon] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France	Swiss Federal Institutes of Technology Domain; ETH Zurich; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Hofmann, T (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.							Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Dasgupta S, 2015, ALGORITHMICA, V72, P237, DOI 10.1007/s00453-014-9885-5; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Konecny J., 2013, ARXIV PREPRINT ARXIV; Mark Schmidt, 2014, CONVERGENCE RATE STO; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schmidt M., 2013, ARXIV13092388; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4	11	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102067
C	Jamieson, K; Jain, L; Fernandez, C; Glattard, N; Nowak, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Jamieson, Kevin; Jain, Lalit; Fernandez, Chris; Glattard, Nick; Nowak, Robert			NEXT: A System for Real-World Development, Evaluation, and Application of Active Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for real-world, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.	[Jamieson, Kevin] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Jain, Lalit; Fernandez, Chris; Glattard, Nick; Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA	University of California System; University of California Berkeley; University of Wisconsin System; University of Wisconsin Madison	Jamieson, K (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	kjamieson@berkeley.edu; ljain@wisc.edu; crfernandez@wisc.edu; glattard@wisc.edu; rdnowak@wisc.edu						Agarwal A, 2014, J MACH LEARN RES, V15, P1111; Agarwal Alekh, 2013, ARXIV13108243; Agarwal Deepak, 2009, DAT MIN 2009 ICDM 9; Agarwal Sameer, 2007, J MACHINE LEARNING R, P11; Ambert KH, 2013, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00038; Barowy DW, 2012, ACM SIGPLAN NOTICES, V47, P639, DOI 10.1145/2398857.2384663; Crankshaw D., 2015, CIDR 2015; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Gureckis T. M., PSITURK OPEN SOURCE; Jamieson K., 2011, ALL C COMM CONTR COM; Jamieson K., 2014, P 27 C LEARN THEOR; Jamieson K. G., 2015, AISTATS; Leunis E., 2012, IEEE INT WORKSH MACH, P1, DOI DOI 10.1109/MLSP.2012.6349720; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li Lihong, 2010, INT C WORLD WID WEB; Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354; Meng X., 2015, ARXIV150506807; Moon Seungwhan, 2014, DAT SCI ADV AN DSAA; Recht Benjamin, 2011, ADV NEURAL INFORM PR; Settles B., U WISCONSIN MADISON, V52, P11; Tamuz Omer, 2011, P 28 INT C MACH LEAR; Urvoy T., 2013, P 30 INT C MACH LEAR, P9199; Wallace B.C., 2012, P ACM INT HLTH INF S, P819, DOI DOI 10.1145/2110363.2110464; Yue Y., 2012, J COMPUTER SYSTEM SC, V78; Yue Y., 2011, P 28 INT C MACH LEAR, P241	25	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102017
C	Johnson, R; Zhang, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Johnson, Rie; Zhang, Tong			Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.	[Johnson, Rie] RJ Res Consulting, Tarrytown, NY USA; [Zhang, Tong] Baidu Inc, Beijing, Peoples R China; [Zhang, Tong] Rutgers State Univ, Piscataway, NJ USA	Baidu; Rutgers State University New Brunswick	Johnson, R (corresponding author), RJ Res Consulting, Tarrytown, NY USA.	riejohnson@gmail.com; tzhang@stat.rutgers.edu	Zhang, Tong/HGC-1090-2022		NSF [IIS-1407939, IIS-1250985]; NIH [R01AI116744]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Tong Zhang would like to acknowledge NSF IIS-1250985, NSF IIS-1407939, and NIH R01AI116744 for supporting his research.	Ando RK, 2005, J MACH LEARN RES, V6, P1817; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Collobert R, 2008, P ICML; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Dhillon Paramveer, 2011, P NIPS; Gao J., 2014, P EMNLP; Glorot X, 2011, DEEP LEARNING APPROA; Hinton GE, 2012, IMPROVING NEURAL NET; Joachims Thorsten, 1999, P ICML; Johnson R., 2015, P NAACL HLT; Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062; Kim Y., 2014, P 2014 C EMP METH NA; Lecun Y, P IEEE; Maas Andrew L., 2011, P 49 ANN M ASS COMP; Mesnil Gregoire, 2015, ARXIV14125335V5; Mikolov Tomas, 2013, ADV NEURAL INFORM PR; Mnih A., 2008, NIPS; Quoc Le, 2014, P ICML; Rie K, 2007, P ICML; Shen Yelong, 2014, P CIKM; Tang DY, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1555; Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384; Weston Jason, 2014, EMNLP ASS COMP LING, P1822, DOI [10.3115/v1/d14-1194, DOI 10.3115/V1/D14-1194]; Xu LH, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P336; Xu P., 2013, ASRU	26	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101108
C	Jun, KS; Zhu, XJ; Rogers, T; Yang, ZR; Yuan, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Jun, Kwang-Sung; Zhu, Xiaojin; Rogers, Timothy; Yang, Zhuoran; Yuan, Ming			Human Memory Search as Initial-Visit Emitting Random Walk	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				FLUENCY	Imagine a random walk that outputs a state only when visiting it for the first time. The observed output is therefore a repeat-censored version of the underlying walk, and consists of a permutation of the states or a prefix of it. We call this model initial-visit emitting random walk (INVITE). Prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks, which is of great interest in both the study of human cognition and various clinical applications. However, parameter estimation in INVITE is challenging, because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable. In this paper, we propose the first efficient maximum likelihood estimate (MLE) for INVITE by decomposing the censored output into a series of absorbing random walks. We also prove theoretical properties of the MLE including identifiability and consistency. We show that INVITE outperforms several existing methods on real-world human response data from memory search tasks.	[Jun, Kwang-Sung] Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53706 USA; [Zhu, Xiaojin] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA; [Rogers, Timothy] Univ Wisconsin, Dept Psychol, Madison, WI 53706 USA; [Yang, Zhuoran] Tsinghua Univ, Dept Math Sci, Beijing, Peoples R China; [Yuan, Ming] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison; Tsinghua University; University of Wisconsin System; University of Wisconsin Madison	Jun, KS (corresponding author), Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53706 USA.	kjun@discovery.wisc.edu; jerryzhu@cs.wisc.edu; ttrogers@wisc.edu; yzr11@mails.tsinghua.edu.cn; myuan@stat.wisc.edu		Jun, Kwang-Sung/0000-0001-5483-3161	NSF [IIS-0953219, DGE-1545481, DMS-1265202]; NIH Big Data to Knowledge [1U54AI117924-01]; NIH [1U54AI117924-01]	NSF(National Science Foundation (NSF)); NIH Big Data to Knowledge(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The authors are thankful to the anonymous reviewers for their comments. This work is supported in part by NSF grants IIS-0953219 and DGE-1545481, NIH Big Data to Knowledge 1U54AI117924-01, NSF Grant DMS-1265202, and NIH Grant 1U54AI117924-01.	Abbott JT., 2012, P ADV NEURAL INFORM, V25, P3050; Abrahao B. D., 2013, CORR; [Anonymous], 2010, PROC 16 ACM SIGKDD I, DOI DOI 10.1145/1835804.1835933; Bottou L., 2012, NEURAL NETWORKS TRIC, P430; BRODER A, 1989, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.1989.63516; CHAN AS, 1993, J COGNITIVE NEUROSCI, V5, P254, DOI 10.1162/jocn.1993.5.2.254; Cockrell J.R., 2002, PRINC PRACT GERIATR, P140, DOI [10.1002/0470846410.ch27, DOI 10.1002/0470846410.CH27(II)]; Doyle P. G., 1984, RANDOM WALKS ELECT N; Durrett R., 2012, SPRINGER TEXTS STAT; Flory P.J., 1953, PRINCIPLES POLYM CHE; Glenberg A. M., 2009, ITALIAN J LINGUISTIC; Goni J, 2011, COGN PROCESS, V12, P183, DOI 10.1007/s10339-010-0372-x; Griffiths TL, 2007, PSYCHOL SCI, V18, P1069, DOI 10.1111/j.1467-9280.2007.02027.x; HENLEY NM, 1969, J VERB LEARN VERB BE, V8, P176, DOI 10.1016/S0022-5371(69)80058-7; Hills TT, 2012, PSYCHOL REV, V119, P431, DOI 10.1037/a0027373; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; PASQUIER F, 1995, J NEUROL NEUROSUR PS, V58, P81, DOI 10.1136/jnnp.58.1.81; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Rogers TT, 2006, NEUROPSYCHOLOGY, V20, P319, DOI 10.1037/0894-4105.20.3.319; Ruppert D., 1988, TECH REP; Troyer A., 1998, NEUROPSYCHOLOGIA, V36	21	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102027
C	Kairouz, P; Oh, S; Viswanath, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kairouz, Peter; Oh, Sewoong; Viswanath, Pramod			Secure Multi-party Differential Privacy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of interactive function computation by multiple parties, each possessing a bit, in a differential privacy setting (i.e., there remains an uncertainty in any party's bit even when given the transcript of interactions and all the other parties' bits). Each party wants to compute a function, which could differ from party to party, and there could be a central observer interested in computing a separate function. Performance at each party is measured via the accuracy of the function to be computed. We allow for an arbitrary cost metric to measure the distortion between the true and the computed function values. Our main result is the optimality of a simple non-interactive protocol: each party randomizes its bit (sufficiently) and shares the privatized version with the other parties. This optimality result is very general: it holds for all types of functions, heterogeneous privacy conditions on the parties, all types of cost metrics, and both average and worst-case (over the inputs) measures of accuracy.	[Kairouz, Peter; Viswanath, Pramod] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Kairouz, P (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.	kairouz2@illinois.edu; swoh@illinois.edu; pramodv@illinois.edu			NSF CISE [CCF-1422278]; NSF SaTC award [CNS-1527754]; NSF CMMI award [MES-1450848]; NSF ENG award [ECCS-1232257]	NSF CISE; NSF SaTC award(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CMMI award; NSF ENG award	This research is supported in part by NSF CISE award CCF-1422278, NSF SaTC award CNS-1527754, NSF CMMI award MES-1450848 and NSF ENG award ECCS-1232257.	Abbe Emmanuel, 2011, AM ECON REV, V102, P65; Beimel A, 2008, LECT NOTES COMPUT SC, V5157, P451, DOI 10.1007/978-3-540-85174-5_25; Ben-Or M., 1988, Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, P1, DOI 10.1145/62212.62213; BLACKWELL D, 1953, ANN MATH STAT, V24, P265, DOI 10.1214/aoms/1177729032; Blum A., 2005, P 24 ACM SIGMOD SIGA, P128, DOI DOI 10.1145/1065167.1065184; Brenner H, 2010, ANN IEEE SYMP FOUND, P71, DOI 10.1109/FOCS.2010.13; Calandrino JA, 2011, P IEEE S SECUR PRIV, P231, DOI 10.1109/SP.2011.40; Chaudhuri K., 2012, ADV NEURAL INFORM PR, V4, P989; Chaudhuri K, 2013, J MACH LEARN RES, V14, P2905; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chaum D., 1988, Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, P11, DOI 10.1145/62212.62214; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Geng Q., 2012, ARXIV12121186; Geng Q., 2013, ARXIV13120655; Ghosh A, 2012, SIAM J COMPUT, V41, P1673, DOI 10.1137/09076828X; Goldreich O., 1987, P 19 ANN ACM S THEOR, P218, DOI DOI 10.1145/28395.28420; Goyal V, 2013, LECT NOTES COMPUT SC, V8042, P298, DOI 10.1007/978-3-642-40041-4_17; Gupte M, 2010, PODS 2010: PROCEEDINGS OF THE TWENTY-NINTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P135, DOI 10.1145/1807085.1807105; Hardt M, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1255; Homer N, 2008, PLOS GENET, V4, DOI 10.1371/journal.pgen.1000167; Kairouz P., 2014, ADV NEURAL INFORM PR; Kapralov M, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1395; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Kilian J., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P316, DOI 10.1145/335305.335342; Kunzler R, 2009, LECT NOTES COMPUT SC, V5444, P238; McGregor A, 2010, ANN IEEE SYMP FOUND, P81, DOI 10.1109/FOCS.2010.14; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Narayanan A, 2008, P IEEE S SECUR PRIV, P111, DOI 10.1109/SP.2008.33; Oh S., 2013, ARXIV13110776; Prabhakaran MM, 2012, 2012 IEEE INFORMATION THEORY WORKSHOP (ITW), P99, DOI 10.1109/ITW.2012.6404773; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Sweeney L, 2000, CARN MELL U DAT PRIV, V671, P1; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137; Yao Andrew C, 2013, 2013 IEEE 54 ANN S F, P160	39	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103042
C	Khalvati, K; Rao, RPN		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Khalvati, Koosha; Rao, Rajesh P. N.			A Bayesian Framework for Modeling Confidence in Perceptual Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				OBSERVABLE MARKOV-PROCESSES; PROBABILITY	The degree of confidence in one's choice or decision is a critical aspect of perceptual decision making. Attempts to quantify a decision maker's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal. In this paper, we introduce a Bayesian framework to model confidence in perceptual decision making. We show that this model, based on partially observable Markov decision processes (POMDPs), is able to predict confidence of a decision maker based only on the data available to the experimenter. We test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task. In both experiments, we show that our model's predictions closely match experimental data. Additionally, our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making.	[Khalvati, Koosha; Rao, Rajesh P. N.] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Khalvati, K (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	koosha@cs.washington.edu; rao@cs.washington.edu		Rao, Rajesh P. N./0000-0003-0682-8952	NSF [EEC-1028725, 1318733]; ONR [N000141310817]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This research was supported by NSF grants EEC-1028725 and 1318733, and ONR grant N000141310817.	ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X; Drugowitsch J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096511; Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012; Hanks TD, 2011, J NEUROSCI, V31, P6339, DOI 10.1523/JNEUROSCI.5613-10.2011; Huang Y., 2012, ADV NEURAL INFORM PR, V25, P1277; Huang YP, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068314; Juslin P, 1997, J BEHAV DECIS MAKING, V10, P189, DOI 10.1002/(SICI)1099-0771(199709)10:3<189::AID-BDM258>3.0.CO;2-4; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kepecs A, 2008, NATURE, V455, P227, DOI 10.1038/nature07200; Kepecs A, 2012, PHILOS T R SOC B, V367, P1322, DOI 10.1098/rstb.2012.0037; Khalvati K., 2013, P 27 AAAI C ART INT, P187; Kiani R, 2014, NEURON, V84, P1329, DOI 10.1016/j.neuron.2014.12.015; Kiani R, 2009, SCIENCE, V324, P759, DOI 10.1126/science.1169405; Kurniawati H., 2008, P ROB SCI SYST 4; Persaud N, 2007, NAT NEUROSCI, V10, P257, DOI 10.1038/nn1840; Rao RPN, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00146; Ross Stephane, 2008, J ARTIFICIAL INTELLI, V32; Shadlen MN, 1996, P NATL ACAD SCI USA, V93, P628, DOI 10.1073/pnas.93.2.628; SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071; SONDIK EJ, 1978, OPER RES, V26, P282, DOI 10.1287/opre.26.2.282	20	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100027
C	Korattikara, A; Rathod, V; Murphy, K; Welling, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Korattikara, Anoop; Rathod, Vivek; Murphy, Kevin; Welling, Max			Bayesian Dark Knowledge	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/or where we need accurate posterior predictive densities p(yjx;D), e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.	[Korattikara, Anoop; Rathod, Vivek; Murphy, Kevin] Google Res, Mountain View, CA 94043 USA; [Welling, Max] Univ Amsterdam, Amsterdam, Netherlands	Google Incorporated; University of Amsterdam	Korattikara, A (corresponding author), Google Res, Mountain View, CA 94043 USA.	kbanoop@google.com; rathodv@google.com; kpmurphy@google.com; m.welling@uva.nl						Ahn S., 2012, ICML; Ahn S., 2014, ICML; [Anonymous], 2016, ICML; Ba J., 2017, P 3 INT C LEARN REPR; Bickel J. Eric, 2007, DECIS ANAL, V4, P49, DOI DOI 10.1287/DECA.1070.0089; Blundell C., 2015, ICML; Bucila C., 2006, KDD; Chen T., 2014, ICML; Ding N., 2014, NIPS; Graves Alex, 2011, NIPS; Hernandez-Lobato J. M., 2015, ICML; Hinton G, 2014, ADV NEURAL INFORM PR, P9; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Patterson S., 2013, NIPS; Rezende D.J., 2014, PROC INT CONFER ENCE; Romero Adriana, 2014, FITNETS HINTS THIN D, V19; Snelson Edward, 2005, ICML; Szegedy Christian, 2014, ICLR; Welling M., 2011, ICML	19	2	2	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103003
C	Li, TY; Prasad, A; Ravikumar, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Li, Tianyang; Prasad, Adarsh; Ravikumar, Pradeep			Fast Classification Rates for High-dimensional Gaussian Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				REGULARIZATION; CENTROIDS; FEATURES	We consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate Gaussian distributions. We focus on the setting where the covariance matrices for the two conditional distributions are the same. The corresponding generative model classifier, derived via the Bayes rule, also called Linear Discriminant Analysis, has been shown to behave poorly in high-dimensional settings. We present a novel analysis of the classification error of any linear discriminant approach given conditional Gaussian models. This allows us to compare the generative model classifier, other recently proposed discriminative approaches that directly learn the discriminant function, and then finally logistic regression which is another classical discriminative model classifier. As we show, under a natural sparsity assumption, and letting s denote the sparsity of the Bayes classifier, p the number of covariates, and n the number of samples, the simple (l(1)-regularized) logistic regression classifier achieves the fast misclassification error rates of O(s log p/n), which is much better than the other approaches, which are either inconsistent under high-dimensional settings, or achieve a slower rate of O (root s log p/n).	[Li, Tianyang; Prasad, Adarsh; Ravikumar, Pradeep] UT Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Li, TY (corresponding author), UT Austin, Dept Comp Sci, Austin, TX 78712 USA.	lty@cs.utexas.edu; adarsh@cs.utexas.edu; pradeepr@cs.utexas.edu			ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033]; NIH as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences [R01 GM117594-01]	ARO; NSF(National Science Foundation (NSF)); NIH as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences	We acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences.	[Anonymous], 2014, PROC NEURIPS; [Anonymous], [No title captured]; Banerjee A, 2014, ADV NEURAL INFORM PR, V27, P1556; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600; Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847; Bishop C.M, 2006, PATTERN RECOGN; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Cai T., 2011, J AM STAT ASS, V106; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Devroye Luc P., 1996, PROBABILISTIC THEORY, V31; Donoho D, 2008, P NATL ACAD SCI USA, V105, P14790, DOI 10.1073/pnas.0807471105; Fan JQ, 2012, J R STAT SOC B, V74, P745, DOI 10.1111/j.1467-9868.2012.01029.x; Fan JQ, 2008, ANN STAT, V36, P2605, DOI 10.1214/07-AOS504; Fan YY, 2013, ANN STAT, V41, P2537, DOI 10.1214/13-AOS1163; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Gopal S., 2013, INT C MACHINE LEARNI, P289; Hastie T., 2009, ELEMENTS STAT LEARNI, DOI [10.1007/978-0-387-84858-7, DOI 10.1007/978-0-387-84858-7]; Kolar M., 2013, P 30 INT C MACH LEAR, P329; Mai Q., 2012, BIOMETRIKA; Mammen E, 1999, ANN STAT, V27, P1808; Shao J, 2011, ANN STAT, V39, P1241, DOI 10.1214/10-AOS870; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Wang SJ, 2007, BIOINFORMATICS, V23, P972, DOI 10.1093/bioinformatics/btm046; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Zhang T, 2004, ANN STAT, V32, P56	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103037
C	Lloyd, JR; Ghahramani, Z		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lloyd, James Robert; Ghahramani, Zoubin			Statistical Model Criticism using Kernel Two Sample Tests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose an exploratory approach to statistical model criticism using maximum mean discrepancy (MMD) two sample tests. Typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model. MMD two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy. We demonstrate on synthetic data that the selected statistic, called the witness function, can be used to identify where a statistical model most misrepresents the data it was trained on. We then apply the procedure to real data where the models being assessed are restricted Boltzmann machines, deep belief networks and Gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on.	[Lloyd, James Robert; Ghahramani, Zoubin] Univ Cambridge, Dept Engn, Cambridge, England	University of Cambridge	Lloyd, JR (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.							Bayarri MJ, 2007, STAT SCI, V22, P322, DOI 10.1214/07-STS235; Bayarri M J, 1999, BAYES STAT; Benjamini Y, 1995, J R STAT SOC B; BICKEL PJ, 1969, ANN MATH STAT, V40, P1, DOI 10.1214/aoms/1177697800; BOX GEP, 1980, J ROY STAT SOC A STA, V143, P383, DOI 10.2307/2982063; Cook Dennis, 1982, MON STAT APP PROB; Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683; Gelfand A E, 1992, 462 STANF U CA DEP S; Gelman A, 1996, STAT SINICA, V6, P733; Gelman A., 2013, CHAPMAN HALL CRC TEX, V3, DOI [10.1201/b16018, DOI 10.1201/B16018]; Gelman Andrew, 2003, INT STAT REV; Gelman Andrew, 2013, ELEC J STAT; Geweke J, 2004, J AM STAT ASSOC, V99, P799, DOI 10.1198/016214504000001132; Goodman Noah D, 2008, C UNC ART INT UAI; Gretton A, 2008, KERNEL METHOD 2 SAMP; Grosse Roger B, 2012, C UNC ART INT UAI; GUTTMAN I, 1967, J ROY STAT SOC B, V29, P83; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hinton GE, 2007, PROG BRAIN RES, V165, P535, DOI 10.1016/S0079-6123(06)65034-6; Hotelling Harold, 1951, P 2 BERK S MATH STAT; Iwata Tomoharu, 2013, C UNC ART INT UAI; Koller D, 1997, ASS ADV ARTIFICIAL I; Lloyd J. R., 2014, ASS ADV ARTIFICIAL I; Marshall EC, 2007, BAYESIAN ANAL, V2, P409, DOI 10.1214/07-BA218; Milch B, 2005, P INT JOINT C ART IN; O'HAGAN A., 2003, HIGHLY STRUCTURED ST, V27, P423; Parzen E., 1962, ANN MATH STAT; Peel D, 2000, STAT COMPUT, V10, P339, DOI 10.1023/A:1008981510081; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robins JM, 2000, J AM STAT ASSOC, V95, P1143, DOI 10.2307/2669750; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; RUBIN DB, 1984, ANN STAT, V12, P1151, DOI 10.1214/aos/1176346785; Stan Development Team, 2014, STAN C PLUS PLUS LIB; STIGLER SM, 1977, ANN STAT, V5, P1055, DOI 10.1214/aos/1176343997; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Vehtari A, 2012, STAT SURV, V6, P142, DOI 10.1214/12-SS102; Wilson A. G., 2013, P INT C MACH LEARN	38	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100025
C	Meeds, E; Welling, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Meeds, Edward; Welling, Max			Optimization Monte Carlo: Efficient and Embarrassingly Parallel Likelihood-Free Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models. The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic. For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.	[Meeds, Edward; Welling, Max] Univ Amsterdam, Inst Informat, Amsterdam, Netherlands; [Welling, Max] Univ Calif Irvine, Donald Bren Sch Informat & Comp Sci, Irvine, CA USA; [Welling, Max] Canadian Inst Adv Res, Toronto, ON, Canada	University of Amsterdam; University of California System; University of California Irvine; Canadian Institute for Advanced Research (CIFAR)	Meeds, E (corresponding author), Univ Amsterdam, Inst Informat, Amsterdam, Netherlands.	tmeeds@gmail.com; welling.max@gmail.com						Ahn S, 2015, KDD; Ahn S, 2014, PR MACH LEARN RES, V32, P1044; Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0; Bonassi F. V., 2015, BAYESIAN ANAL, V10; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Drovandi CC, 2011, J R STAT SOC C-APPL, V60, P317, DOI 10.1111/j.1467-9876.2010.00747.x; Fearnhead P, 2012, J R STAT SOC B, V74, P419, DOI 10.1111/j.1467-9868.2011.01010.x; Forneron J.-J., 2015, ARXIV150101265V2; Forneron J.-J., 2015, ARXIV150604017V1; GOURIEROUX C, 1993, J APPL ECONOM, V8, pS85, DOI 10.1002/jae.3950080507; Gutmann M. U., 2015, J MACHINE LEARNING R; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Maclaurin D., 2015, AUTOGRAD; Meeds E, 2015, UNCERTAINTY IN AI, V31; Neal P, 2012, STAT COMPUT, V22, P1239, DOI 10.1007/s11222-010-9216-x; Paige B., 2014, ADV NEURAL INFORM PR, P3410; Shestopaloff A. Y., 2013, TECHNICAL REPORT; Sisson S, 2007, P NATL ACAD SCI, V104; Sisson S, 2009, P NATL ACAD SCI, V106; Snoek J., 2012, P ADV NEUR INF PROC, P1; SPALL JC, 1992, IEEE T AUTOMAT CONTR, V37, P332, DOI 10.1109/9.119632	22	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102030
C	Miller, A; Wu, A; Regier, J; McAuliffe, J; Lang, D; Prabhat; Schlegel, D; Adams, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Miller, Andrew; Wu, Albert; Regier, Jeffrey; McAuliffe, Jon; Lang, Dustin; Prabhat; Schlegel, David; Adams, Ryan			A Gaussian Process Model of Quasar Spectral Energy Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a method for combining two sources of astronomical data, spectroscopy and photometry, that carry information about sources of light (e.g., stars, galaxies, and quasars) at extremely different spectral resolutions. Our model treats the spectral energy distribution (SED) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations. We place a flexible, nonparametric prior over the SED of a light source that admits a physically interpretable decomposition, and allows us to tractably perform inference. We use our model to predict the distribution of the redshift of a quasar from five-band (low spectral resolution) photometric data, the so called "photo-z" problem. Our method shows that tools from machine learning and Bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties.	[Miller, Andrew; Wu, Albert; Adams, Ryan] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA; [Regier, Jeffrey; McAuliffe, Jon] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; [Lang, Dustin] Carnegie Mellon Univ, McWilliams Ctr Cosmol, Pittsburgh, PA 15213 USA; [Prabhat; Schlegel, David] Lawrence Berkeley Natl Lab, Berkeley, CA USA	Harvard University; University of California System; University of California Berkeley; Carnegie Mellon University; United States Department of Energy (DOE); Lawrence Berkeley National Laboratory	Miller, A (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	acm@seas.harvard.edu; awu@college.harvard.edu; jeff@stat.berkeley.edu; jon@stat.berkeley.edu; dstn@cmu.edu; prabhat@lbl.gov; djschlegel@lbl.gov; rpa@seas.harvard.edu		Regier, Jeffrey/0000-0002-1472-5235; Schlegel, David/0000-0002-5042-5088	Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy [DE-AC02-05CH11231]	Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy(United States Department of Energy (DOE))	The authors would like to thank Matthew Hoffman and members of the HIPS lab for helpful discussions. This work is supported by the Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy under contract No. DE-AC02-05CH11231. This work used resources of the National Energy Research Scientific Computing Center (NERSC). We would like to thank Tina Butler, Tina Declerck and Yushu Yao for their assistance.	Alam S., 2015, ARXIV150100963; Bovy J, 2012, ASTROPHYS J, V749, DOI 10.1088/0004-637X/749/1/41; Brescia M, 2013, ASTROPHYS J, V772, DOI 10.1088/0004-637X/772/2/140; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Christopher Martin D, 2005, ASTROPHYSICAL J LETT, V619; Dawson KS, 2013, ASTRON J, V145, DOI 10.1088/0004-6256/145/1/10; Gray RO, 2001, ASTRON J, V121, P2159, DOI 10.1086/319957; HARRISON E, 1993, ASTROPHYS J, V403, P28, DOI 10.1086/172179; Hogg D.W., 1999, ASTROPH9905116 ARXIV; Maclaurin Dougal, 2015, ICML WORKSH AUT MACH; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.1090/S0025-5718-1980-0572855-7; Paris I, 2014, ASTRON ASTROPHYS, V563, DOI 10.1051/0004-6361/201322691; Regier J., 2015, P 32 INT C MACH LEAR; SDSSIII, 2013, MEAS FLUX MAGN; Silk Joseph, 1998, ASTRONOMY ASTROPHYSI; Stoughton C, 2002, ASTRON J, V123, P485, DOI 10.1086/324741; Walcher J, 2011, ASTROPHYS SPACE SCI, V331, P1, DOI 10.1007/s10509-010-0458-z; Weinberg David H, 2003, P 13 ANN ASTR C MAR, V666	19	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102108
C	Mohamed, S; Rezende, DJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mohamed, Shakir; Rezende, Danilo J.			Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				EMPOWERMENT	The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm - an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.	[Mohamed, Shakir; Rezende, Danilo J.] Google DeepMind, London, England	Google Incorporated	Mohamed, S (corresponding author), Google DeepMind, London, England.	shakir@google.com; danilor@google.com						Andrew Y, 1999, ICML; Barber D, 2004, ADV NEUR IN, V16, P201; Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115; Buhmann J. M., 2012, WORKSH UNS TRANSF LE; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gao S., 2014, ARXIV14112003; Gretton A., 2003, ICASP, V4, pIV; ITTI L, 2009, VISION RES, V49, P1295, DOI DOI 10.1016/J.VISRES.2008.09.007; Klyubin AS, 2005, IEEE C EVOL COMPUTAT, P128; Koutnik J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P541, DOI 10.1145/2576768.2598358; LeCun Yann, 1995, HDB BRAIN THEORY NEU, V3361, P310; Little DY, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00037; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nelson JD, 2005, PSYCHOL REV, V112, P979, DOI 10.1037/0033-295X.112.4.979; Oudeyer P., 2008, INT C EP ROB; Rubin J, 2012, INTEL SYST REF LIBR, V28, P57; Salge C, 2014, EMERGENCE COMPLEX CO, V9, P67, DOI 10.1007/978-3-642-53734-9_4; Salge C, 2014, ENTROPY-SWITZ, V16, P2789, DOI 10.3390/e16052789; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Singh S. P., 2005, NIPS; Still S, 2012, THEOR BIOSCI, V131, P139, DOI 10.1007/s12064-011-0142-z; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tishby N., 1999, ALL C COMM CONTR COM; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Yeung RW, 2008, INFORM TECH TRANS PR, P211	29	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100036
C	Ping, W; Liu, Q; Ihler, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ping, Wei; Liu, Qiang; Ihler, Alexander			Decomposition Bounds for Marginal MAP	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods.	[Ping, Wei; Ihler, Alexander] UC Irvine, Comp Sci, Irvine, CA 92697 USA; [Liu, Qiang] Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA	University of California System; University of California Irvine; Dartmouth College	Ping, W (corresponding author), UC Irvine, Comp Sci, Irvine, CA 92697 USA.	wping@ics.uci.edu; qliu@cs.dartmouth.edu; ihler@ics.uci.edu	Ping, Wei/O-4470-2019		NSF [IIS-1065618, IIS-1254071]; United States Air Force under the DARPA PPAML program [FA8750-14-C-0011]	NSF(National Science Foundation (NSF)); United States Air Force under the DARPA PPAML program	This work is sponsored in part by NSF grants IIS-1065618 and IIS-1254071. Alexander Ihler is also funded in part by the United States Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML program.	Dechter R., 2003, JACM; Dechter R, 2013, SYNTHESIS LECT ARTIF; Domke J., 2011, AAAI; Doucet A., 2002, STAT COMPUTING; Globerson A., 2008, NIPS; Globerson A., 2007, AISTATS; Hardy G. H., 1952, INEQUALITIES; Hazan T., 2012, UAI; Hazan T., 2010, IEEE T INFORM THEORY; Ihler Alexander T., 2012, UAI; Jancsary J., 2011, AISTATS; Kiselev I., 2014, AAMAS; Komodakis N., 2011, TPAMI; Liu Q., 2014, THESIS; Liu Q., 2013, JMLR; Liu Q., 2011, ICML; Marinescu R., 2014, UAI; Maua D, 2012, ICML; Meek C., 2011, BAYESIAN STAT; Meltzer T., 2009, UAI; Meshi O., 2011, ECML PKDD; Meshi O., 2010, ICML; Mooij J., 2010, JMLR; Naradowsky J., 2012, EMNLP; Nowozin Sebastian, 2011, FDN TRENDS COMPUTER; Park J., 2003, UAI; Park J., 2004, JAIR; Ping W., 2014, ICML; Ruozzi N., 2013, IEEE T INFORM THEORY; Sontag D., 2009, AISTATS; Sontag D., 2008, UNCERTAINTY ARTIFICI; Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219; Wainwright M. J., 2005, IEEE T INFORM THEORY; Weiss Y., 2007, UAI; Werner T., 2007, TPAMI; Yarkony J., 2010, CVPR; Yuan C., 2009, IJCAI; Yuan C., 2004, UAI; [No title captured]	39	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100068
C	Qian, C; Yu, Y; Zhou, ZH		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Qian, Chao; Yu, Yang; Zhou, Zhi-Hua			Subset Selection by Pareto Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				VARIABLE SELECTION; SPARSE; APPROXIMATION; RECOVERY	Selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection, sparse regression, dictionary learning, etc. In this paper, we propose the POSS approach which employs evolutionary Pareto optimization to find a small-sized subset with good performance. We prove that for sparse regression, POSS is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently. Particularly, for the Exponential Decay subclass, POSS is proven to achieve an optimal solution. Empirical study verifies the theoretical results, and exhibits the superior performance of POSS to greedy and convex relaxation methods.	[Qian, Chao; Yu, Yang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Qian, C (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210023, Jiangsu, Peoples R China.	qianc@lamda.nju.edu.cn; yuy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn			973 Program [2014CB340501]; NSFC [61333014, 61375061]	973 Program(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC))	We want to thank Lijun Zhang and Jianxin Wu for their helpful comments. This research was supported by 973 Program (2014CB340501) and NSFC (61333014, 61375061).	[Anonymous], 2002, SUBSET SELECTION REG, DOI DOI 10.1201/9781420035933; Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968; Das A., 2011, P 28 INT C MACH LEAR, P1057; Das A, 2008, ACM S THEORY COMPUT, P45; Davis G, 1997, CONSTR APPROX, V13, P57, DOI 10.1007/BF02678430; Demsar J, 2006, J MACH LEARN RES, V7, P1; Diekhoff G., 1992, STAT SOCIAL BEHAV SC; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Gilbert AC, 2003, SIAM PROC S, P243; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Johnson RA, 2007, APPL MULTIVARIATE ST; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Qian C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P389; Qian C, 2015, AAAI CONF ARTIF INTE, P2935; Qian C, 2013, ARTIF INTELL, V204, P99, DOI 10.1016/j.artint.2013.09.002; Tan MK, 2015, IEEE T SIGNAL PROCES, V63, P727, DOI 10.1109/TSP.2014.2385036; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Tropp JA, 2003, IEEE IMAGE PROC, P37; Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997; Yu Y, 2012, ARTIF INTELL, V180, P20, DOI 10.1016/j.artint.2012.01.001; Yu Y, 2008, IEEE C EVOL COMPUTAT, P835, DOI 10.1109/CEC.2008.4630893; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690; Zhang T, 2009, J MACH LEARN RES, V10, P555; Zhou H., 2012, ARXIV12013528; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	28	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101081
C	Reddi, SJ; Hefny, A; Sra, S; Poczos, B; Smola, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Reddi, Sashank J.; Hefny, Ahmed; Sra, Suvrit; Poczos, Barnabas; Smola, Alex			On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms-a crucial requirement for modern large-scale applications-have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.	[Reddi, Sashank J.; Hefny, Ahmed; Poczos, Barnabas; Smola, Alex] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Sra, Suvrit] MIT, Cambridge, MA 02139 USA	Carnegie Mellon University; Massachusetts Institute of Technology (MIT)	Reddi, SJ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	sjakkamr@cs.cmu.edu; ahefny@cs.cmu.edu; suvrit@mit.edu; bapoczos@cs.cmu.edu; alex@smola.org			NSF [IIS-1409802]	NSF(National Science Foundation (NSF))	SS was partially supported by NSF IIS-1409802.	Agarwal, 2014, ARXIV14100723; Agarwal A, 2011, ADV NEURAL INFORM PR, P873; [Anonymous], 2001, STUDIES COMPUTATIONA; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; BERTSEKAS D. P., 2011, OPTIMIZATION MACHINE, V2010, P1; Defazio A., 2014, THESIS AUSTR NATL U; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; DEFAZIO AJ, 2014, ARXIV14072710; Dekel O, 2012, J MACH LEARN RES, V13, P165; Gurbuzbalaban M, 2015, MATH PROGRAM, V151, P283, DOI 10.1007/s10107-015-0897-y; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Konecny J., 2013, ARXIV PREPRINT ARXIV; Konecny J., 2015, ARXIV150404407; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; Liu J, 2014, IEEE INT CONF ROBOT, P469, DOI 10.1109/ICRA.2014.6906897; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Mairal J., 2013, ICML; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Reddi S., 2015, UAI 31; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schmidt Mark, 2013, ARXIV13092388; Shalev-Shwartz S., 2013, ADV NEURAL INFORM PR, P378; Shamir O., 2014, P 52 ANN ALL C COMM; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101080
C	Sun, Q; Batra, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sun, Qing; Batra, Dhruv			SubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper formulates the search for a set of bounding boxes (as needed in object proposal generation) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image. Since the number of possible bounding boxes in an image is very large O(#pixels(2)), even a single linear scan to perform the greedy augmentation for submodular maximization is intractable. Thus, we formulate the greedy augmentation step as a Branch-and-Bound scheme. In order to speed up repeated application of B&B, we propose a novel generalization of Minoux's 'lazy greedy' algorithm to the B&B tree. Theoretically, our proposed formulation provides a new understanding to the problem, and contains classic heuristic approaches such as Sliding Window+ Non-Maximal Suppression (NMS) and and Efficient Subwindow Search (ESS) as special cases. Empirically, we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure.	[Sun, Qing; Batra, Dhruv] Virginia Tech, Blacksburg, VA 24061 USA	Virginia Polytechnic Institute & State University	Sun, Q (corresponding author), Virginia Tech, Blacksburg, VA 24061 USA.	sunqing@vt.edu			National Science Foundation CAREER award; Army Research Office YIP award; Office of Naval Research grant; AWS in Education Research Grant; NVIDIA	National Science Foundation CAREER award(National Science Foundation (NSF)); Army Research Office YIP award; Office of Naval Research grant(Office of Naval Research); AWS in Education Research Grant; NVIDIA	This work was partially supported by a National Science Foundation CAREER award, an Army Research Office YIP award, an Office of Naval Research grant, an AWS in Education Research Grant, and GPU support by NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor.	Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Arbelaez Pablo, 2014, CVPR; Blaschko Matthew B., 2011, Energy Minimization Methods in Computer Vision and Pattern Recognition. Proceedings 8th International Conference, EMMCVPR 2011, P385, DOI 10.1007/978-3-642-23094-3_28; Blaschko Matthew B, 2008, ECCV; Buchbinder N., 2012, FOCS; Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025; Carreira J., 2010, CVPR; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deselaers T., 2010, ECCV; Dey D, 2012, ROBOTICS SCI SYSTEMS; Eigen D., 2014, 6 INT C LEARN REPR I; Everingham M., 2012, PASCAL VISUAL OBJECT; Everingham M., 2007, PASCAL VISUAL OBJECT; Feige U., 2007, FOCS; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gonzalez-Garcia A., 2015, CVPR; He K., 2014, ECCV; Hosang J., 2014, BMVC; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kempe D., 2003, ACM SIGKDD C KNOWL D; Krahenbuhl P., 2015, CVPR; Krause A, 2008, J MACH LEARN RES, V9, P235; Krause A, 2014, TRACTABILITY, P71; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; LAWLER EL, 1966, OPER RES, V14, P699, DOI 10.1287/opre.14.4.699; Lin H., 2011, ACL; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Prasad A., 2014, NIPS; Ren S., 2015, CORR ABS150601497; Ross S., 2013, ICML; Streeter M., 2008, NIPS; Szegedy C., 2013, NIPS; Szegedy C., 2014, GOING DEEPER CONVOLU; Taskar Ben, 2003, NIPS; Uijlings J., 2013, IJCV; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Zitnick C. L., 2014, ECCV	41	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101039
C	Sun, SQ; Kolar, M; Xu, JB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sun, Siqi; Kolar, Mladen; Xu, Jinbo			Learning Structured Densities via Infinite Dimensional Exponential Families	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				GRAPHICAL MODELS; SELECTION	Learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications. Current approaches are mainly focused on learning the structure under restrictive parametric assumptions, which limits the applicability of these methods. In this paper, we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model. We consider probabilities that are members of an infinite dimensional exponential family [4], which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel k. One difficulty in learning nonparametric densities is the evaluation of the normalizing constant. In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11]. We show how to efficiently minimize the proposed objective using existing group lasso solvers. Furthermore, we prove that our procedure recovers the graph structure with high-probability under mild conditions. Simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.	[Sun, Siqi; Xu, Jinbo] TTI Chicago, Chicago, IL 60637 USA; [Kolar, Mladen] Univ Chicago, Chicago, IL 60637 USA	University of Chicago	Sun, SQ (corresponding author), TTI Chicago, Chicago, IL 60637 USA.	siqi.sun@ttic.edu; mkolar@chicagobooth.edu; jinbo.xu@gmail.com			National Institutes of Health [R01GM0897532]; National Science Foundation CAREER award [CCF-1149811]; IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation CAREER award(National Science Foundation (NSF)); IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business(International Business Machines (IBM))	The authors are grateful to the financial support from National Institutes of Health R01GM0897532, National Science Foundation CAREER award CCF-1149811 and IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business. This work was completed in part with resources provided by the University of Chicago Research Computing Center.	Albert R, 2005, J CELL SCI, V118, P4947, DOI 10.1242/jcs.02714; Ambroise C, 2009, ELECTRON J STAT, V3, P205, DOI 10.1214/08-EJS314; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009; Defazio A, 2012, ADV NEURAL INFORM PR, P1250; Friedman J., 2010, ARXIV; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Fukumizu K, 2007, J MACH LEARN RES, V8, P361; Geman S., 1986, P INT C MATH INT C M, V1, P2; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003; Jeon YH, 2006, STAT SINICA, V16, P353; Kindermann R., 1980, CONTEMP MATH, V1; Koller D., 2009, PROBABILISTIC GRAPHI; Kourmpetis YAI, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0009293; Lafferty John, 2001, P INT C MACH LEARN J; Li S. Z., 2011, MARKOV RANDOM FIELD; Liu H, 2009, J MACH LEARN RES, V10, P2295; Liu Q., 2011, AISTATS, P40; Manning CD, 1999, FDN STAT NATURAL LAN; Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Ravikumar P., 2008, HIGH DIMENSIONAL GRA; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Rockafellar R. T., 1970, CONVEX ANAL; Sriperumbudur B, 2013, ARXIV13123516; Sun SQ, 2015, JMLR WORKSH CONF PRO, V38, P939; Wei Z, 2007, BIOINFORMATICS, V23, P1537, DOI 10.1093/bioinformatics/btm129; Yang E., 2012, ADV NEURAL INFORM PR, P1358; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Zhao T, 2012, J MACH LEARN RES, V13, P1059	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103044
C	Sun, W; Wang, ZR; Liu, H; Cheng, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sun, Wei; Wang, Zhaoran; Liu, Han; Cheng, Guang			Non-convex Statistical Optimization for Sparse Tensor Graphical Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				COVARIANCE ESTIMATION; SELECTION; DECOMPOSITIONS; CONVERGENCE; LASSO	We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.	[Sun, Wei] Yahoo Labs, Sunnyvale, CA 94089 USA; [Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Cheng, Guang] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA	Princeton University; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Sun, W (corresponding author), Yahoo Labs, Sunnyvale, CA 94089 USA.	sunweisurrey@yahoo-inc.com; zhaoran@princeton.edu; hanliu@princeton.edu; chengg@stat.purdue.edu	Wang, Zhaoran/P-7113-2018		NSF CAREER Award [DMS1454377, DMS1151692]; NSF [IIS1408910, IIS1332109, DMS1418042]; NIH [R01MH102339, R01GM083084, R01HG06841]; Simons Fellowship in Mathematics; Indiana Clinical and Translational Sciences Institute; ONR [N00014-15-1-2331]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Simons Fellowship in Mathematics; Indiana Clinical and Translational Sciences Institute; ONR(Office of Naval Research)	We would like to thank the anonymous reviewers for their helpful comments. Han Liu is grateful for the support of NSF CAREER Award DMS1454377, NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841. Guang Cheng's research is sponsored by NSF CAREER Award DMS1151692, NSF DMS1418042, Simons Fellowship in Mathematics, ONR N00014-15-1-2331 and a grant from Indiana Clinical and Translational Sciences Institute.	Allen G., 2012, INT C ARTIFICIAL INT; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Arora S., 2015, J MACH LEARN RES, V40; Cai T., 2015, ANN STAT; DAWID AP, 1981, BIOMETRIKA, V68, P265, DOI 10.1093/biomet/68.1.265; Fan JQ, 2009, ANN APPL STAT, V3, P521, DOI 10.1214/08-AOAS215; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Gupta A. K., 1999, MATRIX VARIATE DISTR; He SY, 2014, J MULTIVARIATE ANAL, V128, P165, DOI 10.1016/j.jmva.2014.03.007; Hoff PD, 2011, BAYESIAN ANAL, V6, P179, DOI 10.1214/11-BA606; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Ledoux M, 2011, CLASS MATH, P1; Leng CL, 2012, J AM STAT ASSOC, V107, P1187, DOI 10.1080/01621459.2012.706133; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Rendle S., 2010, INT C WEB SEARCH DAT; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Sun J., 2015, ARXIV150406785; Sun W., 2015, ARXIV150201425; Sun W, 2013, J MACH LEARN RES, V14, P3419; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tsiligkaridis T, 2013, IEEE T SIGNAL PROCES, V61, P1743, DOI 10.1109/TSP.2013.2240157; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Yin JX, 2012, J MULTIVARIATE ANAL, V107, P119, DOI 10.1016/j.jmva.2012.01.005; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Zahn JM, 2007, PLOS GENET, V3, P2326, DOI 10.1371/journal.pgen.0030201; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zhao T, 2012, J MACH LEARN RES, V13, P1059; Zhe S., 2015, 29 AAAI C ART INT; Zhe S., 2015, INT C ART INT STAT; Zhou SH, 2014, ANN STAT, V42, P532, DOI 10.1214/13-AOS1187	33	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102068
C	Ulrich, K; Carlson, DE; Dzirasa, K; Carin, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ulrich, Kyle; Carlson, David E.; Dzirasa, Kafui; Carin, Lawrence			GP Kernels for Cross-Spectrum Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Multi-output Gaussian processes provide a convenient framework for multi-task problems. An illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data, where experimentalists are interested in both power and phase coherence between channels. Recently, Wilson and Adams (2013) proposed the spectral mixture (SM) kernel to model the spectral density of a single task in a Gaussian process framework. In this paper, we develop a novel covariance kernel for multiple outputs, called the cross-spectral mixture (CSM) kernel. This new, flexible kernel represents both the power and phase relationship between multiple observation channels. We demonstrate the expressive capabilities of the CSM kernel through implementation of a Bayesian hidden Markov model, where the emission distribution is a multi-output Gaussian process with a CSM covariance kernel. Results are presented for measured multi-region electrophysiological data.	[Ulrich, Kyle; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Dzirasa, Kafui] Duke Univ, Dept Psychiat & Behav Sci, Durham, NC 27706 USA; [Carlson, David E.] Columbia Univ, Dept Stat, New York, NY 10027 USA	Duke University; Duke University; Columbia University	Ulrich, K (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	kyle.ulrich@duke.edu; david.edwin.carlson@gmail.com; kafui.dzirasa@duke.edu; lcarin@duke.edu	Dzirasa, Kafui/GQB-1424-2022		ARO; DARPA; ONR; DOE; NGA	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); DOE(United States Department of Energy (DOE)); NGA	The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR.	AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Beal M, THESIS; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Dietrich CR, 1997, SIAM J SCI COMPUT, V18, P1088, DOI 10.1137/S1064827592240555; Dzirasa K, 2006, J NEUROSCI, V26, P10577, DOI 10.1523/JNEUROSCI.1767-06.2006; Dzirasa K, 2011, J NEUROSCI METH, V195, P36, DOI 10.1016/j.jneumeth.2010.11.014; Gallager R. G., 2008, PRINCIPLES DIGITAL C, P229; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Goovaerts P, 1997, GEOSTATISTICS NATURA, DOI DOI 10.2307/1270969; Gregoriou GG, 2009, SCIENCE, V324, P1207, DOI 10.1126/science.1171402; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Lloyd James Robert, 2014, AAAI; Mackay D, 1997, TECHNICAL REPORT; Pfaff D., 2008, CONCEPTS MECH GEN CE; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sauseng P, 2008, NEUROSCI BIOBEHAV R, V32, P1001, DOI 10.1016/j.neubiorev.2008.03.014; Sweeney-Reed CM, 2014, ELIFE, V3, DOI 10.7554/eLife.05352; Teh Y.W., 2005, WORKSH ART INT STAT, V2005, P333; Tucker MA, 2006, NEUROBIOL LEARN MEM, V86, P241, DOI 10.1016/j.nlm.2006.03.005; Ulrich K., 2014, NIPS; WELCH PD, 1967, IEEE T ACOUST SPEECH, VAU15, P70, DOI 10.1109/TAU.1967.1161901; Wilson A., 2013, ICML; Wilson A. G., 2014, NIPS; Wilson A.G., 2014, COVARIANCE KERNELS F; Wilson A. G., 2012, ICML; Yang Zichao, 2015, AISTATS	27	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103004
C	Waggoner, B; Frongillo, R; Abernethy, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Waggoner, Bo; Frongillo, Rafael; Abernethy, Jacob			A Market Framework for Eliciting Private Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DIFFERENTIAL PRIVACY	We propose a mechanism for purchasing information from a sequence of participants. The participants may simply hold data points they wish to sell, or may have more sophisticated information; either way, they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism's future prediction on a test set. The mechanism, which draws on the principles of prediction markets, has a bounded budget and minimizes generalization error for Bregman divergence loss functions. We then show how to modify this mechanism to preserve the privacy of participants' information: At any given time, the current prices and predictions of the mechanism reveal almost no information about any one participant, yet in total over all participants, information is accurately aggregated.	[Waggoner, Bo] Harvard SEAS, Cambridge, MA 02138 USA; [Frongillo, Rafael] Univ Colorado, Boulder, CO 80309 USA; [Abernethy, Jacob] Univ Michigan, Ann Arbor, MI 48109 USA	Harvard University; University of Colorado System; University of Colorado Boulder; University of Michigan System; University of Michigan	Waggoner, B (corresponding author), Harvard SEAS, Cambridge, MA 02138 USA.	bwaggoner@fas.harvard.edu; raf@colorado.edu; jabernet@umich.edu			US National Science Foundation under CAREER [IIS-1453304, IIS-1421391]	US National Science Foundation under CAREER(National Science Foundation (NSF))	J. Abernethy acknowledges the generous support of the US National Science Foundation under CAREER Grant IIS-1453304 and Grant IIS-1421391.	Abernethy J., 2014, P 15 ACM C EC COMP, P395; Abernethy J.D., 2011, ADV NEURAL INFORM PR, P2600; Abernethy Jacob, 2013, ACM T EC COMPUTATION, V1; Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009; Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626; Chen Y., 2010, P 11 ACM C EL COMM, P189; Chen Yiling, 2007, UAI; Dwork C., 2014, FDN TRENDS THEORETIC; Dwork C, 2010, ACM S THEORY COMPUT, P715; Hall R, 2013, J MACH LEARN RES, V14, P703; Hanson R, 2002, ENTREPRENEURIAL EC B, P79; Hanson R., 2007, J PREDICTION MARKETS, V1, P3, DOI [DOI 10.5750/JPM.V1I1.417, 10.5750/jpm.v1i1.417]; Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481; Othman Abraham, 2011, P 2 C AUCT MARK MECH, P19; Peters J, 2017, ADAPT COMPUT MACH LE; Storkey A. J., 2011, P 14 INT C ART INT S, P716; Wolfers J, 2004, J ECON PERSPECT, V18, P107, DOI 10.1257/0895330041371321; Wolfers J., 2006, TECHNICAL REPORT; Yiling Chen, 2011, Internet and Network Economics. Proceedings 7th International Workshop, WINE 2011, P72, DOI 10.1007/978-3-642-25510-6_7; Zawadzki E, 2015, AAAI CONF ARTIF INTE, P3635; Zhang L, 2012, AAAI; Zhang L J, 2013, P 30 INT C MACHINE L, P621	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103033
C	Bahadori, MT; Yu, Q; Liu, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bahadori, Mohammad Taha; Yu, Qi (Rose); Liu, Yan			Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.	[Bahadori, Mohammad Taha] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA; [Yu, Qi (Rose); Liu, Yan] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA	University of Southern California; University of Southern California	Bahadori, MT (corresponding author), Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.	mohammab@usc.edu; qiyu@usc.edu; yanliu.cs@usc.edu			NSF [IIS-1134990, IIS-1254206]; Okawa Foundation Research Award	NSF(National Science Foundation (NSF)); Okawa Foundation Research Award	We thank the anonymous reviewers for their helpful feedback and comments. The research was sponsored by the NSF research grants IIS-1134990, IIS-1254206 and Okawa Foundation Research Award. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government.	ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580; [Anonymous], 2010, NIPS; Barron A., 2008, ANN STAT; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Bonilla E.V., 2007, ADV NEURAL INF PROCE, V20, P601; Chu W., 2006, NIPS; Cressie N., 1999, JASA; Cressie N, 2011, STAT SPATIO TEMPORAL; Cressie N., 2010, J COMP GRAPH STAT; Cressie N, 2008, J R STAT SOC B, V70, P209, DOI 10.1111/j.1467-9868.2007.00633.x; Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Gu M., 2000, 5 GEN HERMITIAN EIGE; Isaaks E., 2011, APPL GEOSTATISTICS; Kolda Tamara G, 2009, SIAM REV; Li W.-J., 2009, IJCAI; Long X., 2012, UBICOMP; LOZANO AC, 2009, KDD; Nie F., 2010, NIPS; Romera-Paredes B., 2013, ICML; Shalev-Shwartz S., 2010, SIAM J OPTIMIZATION; Shalev-Shwartz Shai, 2011, ICML; Tomioka R., 2013, NIPS; Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690; Zhou D., 2003, NIPS; Zhou H., 2013, JASA; Zhou J., 2011, MALSAR MULTITASK LEA	27	2	2	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101100
C	Bui, T; Turner, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bui, Thang; Turner, Richard			Tree-structured Gaussian Process Approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Gaussian process regression can be accelerated by constructing a small pseudodataset to summarize the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimization. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.	[Bui, Thang; Turner, Richard] Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Trumpington St, Cambridge CB2 1PZ, England	University of Cambridge	Bui, T (corresponding author), Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Trumpington St, Cambridge CB2 1PZ, England.	tdb40@cam.ac.uk; ret26@cam.ac.uk	Bui, Thang/AAZ-5360-2021	Bui, Thang/0000-0002-7878-9748	EPSRC [EP/G050821/1, EP/L000776/1]; Google	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google(Google Incorporated)	We would like to thank the EPSRC (grant numbers EP/G050821/1 and EP/L000776/1) and Google for funding.	Chalupka K, 2013, J MACH LEARN RES, V14, P333; Gilboa E., 2013, PATT AN MACH INT IEE; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Koller D., 2009, PROBABILISTIC GRAPHI; Lazaro-Gredilla M., 2009, ADV NEURAL INFORM PR, V22, P1087; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Qi YA, 2010, C UNC ART INT, P450; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sarkka S, 2013, IEEE SIGNAL PROC MAG, V30, P51, DOI 10.1109/MSP.2013.2246292; Seeger M. W., 2003, INT C ART INT STAT; Seeger Matthias, 2003, THESIS; Snelson E.L., 2007, FLEXIBLE EFFICIENT G; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Snelson Edward, 2007, P 11 INT C ARTIFICIA, P524; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908; Turner R., 2011, ADV NEURAL INF PROCE, P981; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; Turner R. E., 2014, SIGNAL PROCESSING IE; Turner RE, 2010, THESIS; Wilson A., 2013, INT C MACH LEARN, P1067	22	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102020
C	Chen, SY; Lin, T; King, I; Lyu, MR; Chen, W		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chen, Shouyuan; Lin, Tian; King, Irwin; Lyu, Michael R.; Chen, Wei			Combinatorial Pure Exploration of Multi-Armed Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study the combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a decision class, which is a collection of subsets of arms with certain combinatorial structures such as size-K subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-K arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.	[Chen, Shouyuan; King, Irwin; Lyu, Michael R.] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Lin, Tian] Tsinghua Univ, Beijing, Peoples R China; [Chen, Shouyuan; Lin, Tian; Chen, Wei] Microsoft Res Asia, Beijing, Peoples R China	Chinese University of Hong Kong; Tsinghua University; Microsoft; Microsoft Research Asia	Chen, SY (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	sychen@cse.cuhk.edu.hk; lint10@mails.tsinghua.edu.cn; king@cse.cuhk.edu.hk; lyt@cse.cuhk.edu.hk; weic@microsoft.com	King, Irwin/C-9681-2015	King, Irwin/0000-0001-8106-6447	National Grand Fundamental Research 973 Program of China [2014CB340401, 2014CB340405]; Research Grants Council of the Hong Kong Special Administrative Region, China [CUHK 413212, CUHK 415113]; Microsoft Research Asia Regional Seed Fund in Big Data Research [FY13-RES-SPONSOR-036]	National Grand Fundamental Research 973 Program of China(National Basic Research Program of China); Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council); Microsoft Research Asia Regional Seed Fund in Big Data Research(Microsoft)	The work described in this paper was partially supported by the National Grand Fundamental Research 973 Program of China (No. 2014CB340401 and No. 2014CB340405), the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK 413212 and CUHK 415113), and Microsoft Research Asia Regional Seed Fund in Big Data Research (Grant No. FY13-RES-SPONSOR-036).	[Anonymous], 2010, COLT; Audibert J., 2010, P COLT; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Berge C., 1957, PNAS; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Bubeck S., 2012, COLT; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen W., 2013, ICML 2013, P151; Even-Dar E., 2006, JMLR; Gabillon V., 2011, NIPS; Gabillon V., 2012, NIPS; Gopalan A, 2014, PR MACH LEARN RES, V32; Jamieson K., 2014, COLT; Kale S., 2010, NIPS; Kalyanakrishnan S., 2010, P 27 INT C MACH LEAR, P511; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Kaufmann E., 2013, COLT; Kveton B., 2014, UAI; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lin Tsun-Han, 2014, ICML; Mannor S, 2004, J MACH LEARN RES, V5, P623; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; OXLEY J. G., 2006, MATROID THEORY, V3; Pollard D., 2000, ASYMPTOPIA UNPUB; Rivasplata O., 2012, SUBGAUSSIAN RANDOM V; ROSS S. M, 1996, STOCHASTIC PROCESSES, V2; Spring N, 2002, ACM SIGCOMM COMP COM, V32, P133, DOI 10.1145/964725.633039; Zhou Y., 2014, ICML	31	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101104
C	Hoffman, J; Guadarrama, S; Tzeng, E; Hu, RH; Donahue, J; Girshick, R; Darrell, T; Saenko, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hoffman, Judy; Guadarrama, Sergio; Tzeng, Eric; Hu, Ronghang; Donahue, Jeff; Girshick, Ross; Darrell, Trevor; Saenko, Kate			LSDA: Large Scale Detection through Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at lsda.berkeleyvision.org.	[Hoffman, Judy; Guadarrama, Sergio; Tzeng, Eric; Donahue, Jeff; Girshick, Ross; Darrell, Trevor] Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA; [Hu, Ronghang] Tsinghua Univ, EE, Beijing, Peoples R China; [Saenko, Kate] UMass Lowell, CS, Lowell, MA USA	University of California System; University of California Berkeley; Tsinghua University; University of Massachusetts System; University of Massachusetts Lowell	Hoffman, J (corresponding author), Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA.	jhoffman@eecs.berkeley.edu; sguada@eecs.berkeley.edu; tzeng@eecs.berkeley.edu; hrh11@mails.tsinghua.edu.cn; jdonahue@eecs.berkeley.edu; rbg@eecs.berkeley.edu; trevor@eecs.berkeley.edu; saenko@cs.uml.edu	Hu, Ronghang/Q-8559-2019		DARPA; NSF [IIS-1427425, IIS-1212798, IIS-1116411]; Toyota	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Toyota	This work was supported in part by DARPA's MSEE and SMISC programs, by NSF awards IIS-1427425, and IIS-1212798, IIS-1116411, and by support from Toyota.	Ali K., 2014, IEEE C COMP VIS PATT; Aytar Y., 2011, P ICCV; Aytar Y, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.79; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Berg A., 2012, IMAGENET LARGE SCALE; Dalal N., 2005, P CVPR; Donahue J., 2013, COMPUTER VISION PATT; Donahue J., 2014, P ICML; Duan L., 2012, P ICML; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fischer P., 2014, ARXIV E PRINTS; Girshick R., 2014, P IEEE C COMPUTER VI, P7011; Goehring Daniel, 2014, INT C ROB AUT ICRA; He K., 2014, P ECCV; Hoeim D., 2012, P ECCV; Hoffman J., 2013, P ICLR; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krahenbuhl P., 2014, P ECCV; Krizhevsky A., 2012, NIPS, P1106, DOI DOI 10.1145/3065386; Kulis B., 2011, P CVPR; Lowe D. G., 2004, IJCV; Smucker M.D., 2007, C INF KNOWL MAN; Song H. O., 2014, P INT C MACH LEARN I; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Xu J., 2014, IEEE T PATTERN ANAL; YANG J, 2007, ACM MULTIMEDIA; Yang J., 2007, ICDM WORKSH; Zhang X, 2013, ARXIV PREPRINT ARXIV	28	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101089
C	Kar, P; Narasimhan, H; Jain, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kar, Purushottam; Narasimhan, Harikrishna; Jain, Prateek			Online and Stochastic Gradient Methods for Non-decomposable Loss Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, Prec(@k )and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes Prec(@k), pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.	[Kar, Purushottam; Jain, Prateek] Microsoft Res, Bengaluru, India; [Narasimhan, Harikrishna] Indian Inst Sci, Bangalore, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Kar, P (corresponding author), Microsoft Res, Bengaluru, India.	t-purkar@microsoft.com; harikrishna@csa.iisc.ernet.in; prajain@microsoft.com	Kar, Purushottam/W-8113-2019	Kar, Purushottam/0000-0003-2096-5267	Google India PhD Fellowship	Google India PhD Fellowship(Google Incorporated)	The authors thank Shivani Agarwal for helpful comments. They also thank the anonymous reviewers for their suggestions. HN thanks support from a Google India PhD Fellowship.	Bertsekas D. P., 2004, NONLINEAR PROGRAMMIN; Chakrabarti S., 2008, KDD; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Daskalaki S, 2006, APPL ARTIF INTELL, V20, P381, DOI 10.1080/08839510500313653; Dekel O, 2012, J MACH LEARN RES, V13, P165; Dembczynski K. J., 2011, NIPS; Dembczynski Krzysztof, 2013, 30 INT C MACH LEARN; Frank A., 2010, UCI MACHINE LEARNING; Hazan E, 2006, LECT NOTES ARTIF INT, V4005, P499, DOI 10.1007/11776420_37; Joachims T., 2005, ICML; Kar P., 2013, ICML; Kubat Miroslav, 1997, 24 INT C MACH LEARN; Lanckriet G.R.G, 2010, ICML; Narasimhan Harikrishna, 2013, INT C MACH LEARN ICM; Narasimhan Harikrishna, 2013, KDD; Qi YJ, 2006, PROTEINS, V63, P490, DOI 10.1002/prot.20865; Rakhlin A, 2009, LECT NOTES ONLINE LE; Rakhlin Alexander, 2011, 24 ANN C LEARN THEOR 24 ANN C LEARN THEOR; Rao RB, 2008, SIGKDD EXPLOR, V10, P34; Saha Ankan, 2012, CORR; SERFLING RJ, 1974, ANN STAT, V2, P39, DOI 10.1214/aos/1176342611; Ye Nan, 2012, 29 INT C MACH LEARN; Yue Yisong, 2007, SIGIR; Zhang YC, 2013, J MACH LEARN RES, V14, P3321; Zhao Peilin, 2011, ICML; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101038
C	Luo, HP; Schapire, RE		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Luo, Haipeng; Schapire, Robert E.			A Drifting-Games Analysis for Online Learning and Applications to Boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS	We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.	[Luo, Haipeng; Schapire, Robert E.] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA; [Schapire, Robert E.] Microsoft Res, New York, NY USA	Princeton University; Microsoft	Luo, HP (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.	haipengl@cs.princeton.edu; schapire@cs.princeton.edu			NSF [1016029]	NSF(National Science Foundation (NSF))	Support for this research was provided by NSF Grant #1016029. The authors thank Yoav Freund for helpful discussions and the anonymous reviewers for their comments.	Abernethy J., 2009, P 22 ANN C LEARN THE; Abernethy Jacob, 2010, ADV NEURAL INFORM PR, V23; Abernethy Jacob D, 2009, COMPETING DARK EFFIC; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Cesa-Bianchi N, 2003, MACH LEARN, V51, P239, DOI 10.1023/A:1022901500417; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; Chaudhuri Kamalika, 2009, ADV NEURAL INFORM PR, V22; Chernov A., 2010, P UNC ART INT; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kleinberg R, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P928, DOI 10.1145/1109557.1109659; Kleinberg Robert, 2005, THESIS; Luo Haipeng, 2014, P 31 INT C MACH LEAR; McMahan H. B., 2014, C LEARN THEORY, P1020; Mukherjee I, 2010, THEOR COMPUT SCI, V411, P2670, DOI 10.1016/j.tcs.2010.04.004; Narayanan H., 2010, ADV NEURAL INFORM PR, V23; ORABONA F, 2014, ADV NEUR IN, V27, pNI568; Rakhlin Alexander, 2012, ADV NEURAL INFORM PR, V25; Reyzin L, 2006, P 23 INT C MACH LEAR; Schapire RE, 2001, MACH LEARN, V43, P265, DOI 10.1023/A:1010800213066; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Zinkevich M., 2003, INT C MACH LEARN ICM	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102030
C	Oh, SY; Dalal, O; Khare, K; Rajaratnam, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Oh, Sang-Yun; Dalal, Onkar; Khare, Kshitij; Rajaratnam, Bala			Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of l(1)-penalized estimation in the Gaussian framework. Though many of these inverse covariance estimation approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing l(1)-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous payoffs for l(1)-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.	[Oh, Sang-Yun] Lawrence Berkeley Natl Lab, Computat Res Div, Berkeley, CA 94720 USA; [Dalal, Onkar] Stanford Univ, Stanford, CA 94305 USA; [Khare, Kshitij] Univ Florida, Dept Stat, Gainesville, FL 32611 USA; [Rajaratnam, Bala] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; Stanford University; State University System of Florida; University of Florida; Stanford University	Oh, SY (corresponding author), Lawrence Berkeley Natl Lab, Computat Res Div, Berkeley, CA 94720 USA.	syoh@lbl.gov; onkar@alumni.stanford.edu; kdkhare@stat.ufl.edu; brajarat@stanford.edu			National Science Foundation [DMS-0906392, DMS-CMG 1025465, AGS-1003823, DMS-1106642, DMS CAREER-1352656, DARPA-YFAN66001-111-4131, SMC-DBNKY]; NSF [DMS-1106084]; Laboratory Directed Research and Development Program of Lawrence Berkeley National Laboratory under U.S. Department of Energy [DE-AC02-05CH11231]	National Science Foundation(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Laboratory Directed Research and Development Program of Lawrence Berkeley National Laboratory under U.S. Department of Energy(United States Department of Energy (DOE))	S.O., O.D. and B.R. were supported in part by the National Science Foundation under grants DMS-0906392, DMS-CMG 1025465, AGS-1003823, DMS-1106642, DMS CAREER-1352656 and grants DARPA-YFAN66001-111-4131 and SMC-DBNKY. K.K was partially supported by NSF grant DMS-1106084. S.O. was supported also in part by the Laboratory Directed Research and Development Program of Lawrence Berkeley National Laboratory under U.S. Department of Energy Contract No. DE-AC02-05CH11231.	Banerjee O, 2008, J MACH LEARN RES, V9, P485; BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Chang HY, 2005, P NATL ACAD SCI USA, V102, P3738, DOI 10.1073/pnas.0409462102; Dalal Onkar Anant, 2014, ARXIV14053034; Friedman J., 2010, TECHNICAL REPORT; Khare Kshitij, 2014, J ROYAL STAT SOC B; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Peng J, 2009, J AM STAT ASSOC, V104, P735, DOI 10.1198/jasa.2009.0126; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; [No title captured]	12	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103027
C	Pan, Y; Theodorou, EA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Pan, Yunpeng; Theodorou, Evangelos A.			Probabilistic Differential Dynamic Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradient-based policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.	[Pan, Yunpeng; Theodorou, Evangelos A.] Georgia Inst Technol, Inst Robot & Intelligent Machines, Daniel Guggenheim Sch Aerosp Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Pan, Y (corresponding author), Georgia Inst Technol, Inst Robot & Intelligent Machines, Daniel Guggenheim Sch Aerosp Engn, Atlanta, GA 30332 USA.	ypan37@gatech.edu; evangelos.theodorou@ae.gatech.edu			National Science Foundation [NRI-1426945]	National Science Foundation(National Science Foundation (NSF))	This work was partially supported by a National Science Foundation grant NRI-1426945.	Abbeel R., 2006, P NEURAL INFORM PROC, P1; [Anonymous], 2008, ADV NEURAL INFORM PR; Candela J. Quinonero, 2003, IEEE INT C AC SPEECH; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth M. P., 2014, IEEE TRANSSACTIONS P, V27, P75; Deisenroth MP, 2009, NEUROCOMPUTING, V72, P1508, DOI 10.1016/j.neucom.2008.12.019; Hemakumara P, 2013, IEEE T ROBOT, V29, P813, DOI 10.1109/TRO.2013.2258732; Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC; Levine Sergey, 2013, P 26 INT C NEUR INF, P207; Mitrovic D, 2010, STUD COMPUT INTELL, V264, P65; Morimoto J., 2002, ADV NEURAL INFORM PR, P1539; Rasmussen CE, 2004, ADV NEUR IN, V16, P751; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Snelson E., 2005, ADV NEURAL INFORM PR, P1257; Tassa Y., NIPS, P1465; Theodorou E, 2010, P AMER CONTR CONF, P1125; Todorov E, 2005, P AMER CONTR CONF, P300, DOI 10.1109/acc.2005.1469949; van den Berg J, 2012, INT J ROBOT RES, V31, P1263, DOI 10.1177/0278364912456319; van Hasselt H. P., 2011, INSIGHTS REINFORCEME; Zhong W, 2001, PROCEEDINGS OF THE 2001 IEEE INTERNATIONAL CONFERENCE ON CONTROL APPLICATIONS (CCA'01), P896, DOI 10.1109/CCA.2001.973983	22	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100027
C	Paulus, R; Socher, R; Manning, CD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Paulus, Romain; Socher, Richard; Manning, Christopher D.			Global Belief Recursive Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MODELS	Recursive Neural Networks have recently obtained state of the art performance on several natural language processing tasks. However, because of their feedforward architecture they cannot correctly predict phrase or word labels that are determined by context. This is a problem in tasks such as aspect-specific sentiment classification which tries to, for instance, predict that the word Android is positive in the sentence Android beats iOS. We introduce global belief recursive neural networks (GB-RNNs) which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference. This allows phrase level predictions and representations to give feedback to words. We show the effectiveness of this model on the task of contextual sentiment analysis. We also show that dropout can improve RNN training and that a combination of unsupervised and supervised word vector representations performs better than either alone. The feedbackward step improves F1 performance by 3% over the standard RNN on this task, obtains state-of-the-art performance on the SemEval 2013 challenge and can accurately predict the sentiment of specific entities.	[Paulus, Romain; Socher, Richard] MetaMind, Palo Alto, CA 94301 USA; [Socher, Richard; Manning, Christopher D.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Paulus, R (corresponding author), MetaMind, Palo Alto, CA 94301 USA.	romain@metamind.io; richard@metamind.io; manning@stanford.edu	Manning, Christopher/AAM-9535-2020	Manning, Christopher/0000-0001-6155-649X				[Anonymous], 2013, EMNLP; Barbosa L., 2010, COLING 2010 23 INT C, V2, P36; Bifet A., 2010, P 13 INT C DISC SCI; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Duchi J., 2011, JMLR; ELMAN JL, 1991, MACH LEARN, V7, P195, DOI 10.1007/BF00114844; Goller C., 1996, P INT C NEUR NETW; Grefenstette E., 2013, IWCS; Huang E. H., 2012, ACL; Irsoy O., 2013, NIPS DEEP LEARN WORK; Kozareva Z, 2013, P 7 INT WORKSH SEM E; Landauer TK, 1997, PSYCHOL REV, V104, P211, DOI 10.1037/0033-295X.104.2.211; Le P., 2014, EMNLP; Mikolov T, 2013, P 2013 C N AM CHAPTE; Mitchell J, 2010, COGNITIVE SCI, V34, P1388, DOI 10.1111/j.1551-6709.2010.01106.x; Ng A.Y, 2010, P NIPS 2010 DEEP LEA; Pennington Jeffrey, 2014, EMNLP; Pollack J. B., 1990, ARTIFICIAL INTELLIGE, V46; Rolfe J. T., 2013, ARXIV13013775V4; Routledge B.R., 2010, INT AAAI C WEBL SOC; Schuster M., 1997, SIGNAL PROCESSING IE; Sobel K., 2009, J AM SOC INFORM SCI; Socher R., 2013, ACL 2013; Socher Richard, 2011, NIPS; Socher Richard, 2012, EMNLP; Tsur O., 2010, ASS COMPUTATIONAL LI; Turney PD, 2010, J ARTIF INTELL RES, V37, P141, DOI 10.1613/jair.2934; Yessenalina A., 2011, EMNLP; Zanzotto F.M., 2010, COLING	29	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100054
C	Petrik, M; Subramanian, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Petrik, Marek; Subramanian, Dharmashankar			RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We describe how to use robust Markov decision processes for value function approximation with state aggregation. The robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration. This results in reducing the bounds on the gamma-discounted infinite horizon performance loss by a factor of 1/(1 - gamma) while preserving polynomial-time computational complexity. Our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.	[Petrik, Marek; Subramanian, Dharmashankar] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Petrik, M (corresponding author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	mpetrik@us.ibm.com; dharmash@us.ibm.com		Petrik, Marek/0000-0002-4568-7948				BEAN JC, 1987, OPER RES, V35, P215, DOI 10.1287/opre.35.2.215; Bernstein A., 2008, C LEARN THEOR COLT; BERTSEKAS DP, 1989, IEEE T AUTOMAT CONTR, V34, P589, DOI 10.1109/9.24227; De Farias DP, 2003, OPER RES, V51, P850, DOI 10.1287/opre.51.6.850.24925; Desai VV, 2012, OPER RES, V60, P655, DOI 10.1287/opre.1120.1044; FILAR J, 1997, COMPETITIVE MARKOV D; Hansen TD, 2013, J ACM, V60, DOI 10.1145/2432622.2432623; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Kaufman DL, 2013, INFORMS J COMPUT, V25, P396, DOI 10.1287/ijoc.1120.0509; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Le Tallec Y, 2007, THESIS; Mannor S., 2012, INT C MACH LEARN; Marecki J., 2013, UNCERTAINTY ARTIFICI; Munos R., 2005, NAT C ART INT AAAI; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Petrik M., 2012, INT C MACH LEARN; Petrik M., 2009, INT C MACH LEARN; Petrik M., 2010, INT C MACH LEARN; Porteus Evan L, 2002, FDN STOCHASTIC INVEN; Puterman M. L., 1994, MARKOV DECISION PROC; Tsitsiklis J. N., 1996, ANAL TEMPORAL DIFFER; Van Roy B, 2006, MATH OPER RES, V31, P234, DOI 10.1287/moor.1060.0188; Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566	24	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102008
C	Qu, Q; Sun, J; Wright, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Qu, Qing; Sun, Ju; Wright, John			Finding a sparse vector in a subspace: Linear sparsity using alternating directions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				EQUATIONS	We consider the problem of recovering the sparsest vector in a subspace S is an element of R-p with dim (S) = n. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds 1/root n. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is ohm (1). To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.	[Qu, Qing; Sun, Ju; Wright, John] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA	Columbia University	Qu, Q (corresponding author), Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.	fqq2105@columbia.edu; js4038@columbia.edu; jw2966@columbia.edu	Qu, Qing/AAA-8226-2019	Qu, Qing/0000-0001-9136-558X				Agarwal A., 2013, STAT-US, V1050, P839; Agarwal A., 2013, ARXIV13107991; Anandkumar A., 2013, ADV NEURAL INFORM PR, P1986; ARORA S., 2014, ARXIV14010579; Arora S., 2013, ARXIV13086273; Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Barak B, 2013, ARXIV13126652; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Berthet Q., 2013, C LEARN THEOR, P1046; Beylkin G, 2005, APPL COMPUT HARMON A, V19, P17, DOI 10.1016/j.acha.2005.01.003; Candes E., 2014, ARXIV14071065; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; COLEMAN TF, 1986, SIAM J ALGEBRA DISCR, V7, P527, DOI 10.1137/0607059; Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905; de la Pena V., 2012, DECOUPLING DEPENDENC; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Hand P., 2013, ARXIV13101654; Hardt M., 2013, ARXIV13120925; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; SPIELMAN D. A., 2012, P 25 ANN C LEARN THE; Xie Yuchen, 2013, JMLR Workshop Conf Proc, V28, P1480; Yi X., 2013, ARXIV13103745; Zhao YB, 2013, APPL MATH COMPUT, V219, P5569, DOI 10.1016/j.amc.2012.11.022; Zibulevsky M, 2001, NEURAL COMPUT, V13, P863, DOI 10.1162/089976601300014385; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	29	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101073
C	Razaviyayn, M; Hong, MY; Luo, ZQ; Pang, JS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Razaviyayn, Meisam; Hong, Mingyi; Luo, Zhi-Quan; Pang, Jong-Shi			Parallel Successive Convex Approximation for Nonsmooth Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				COORDINATE DESCENT METHODS; ALGORITHM	Consider the problem of minimizing the sum of a smooth (possibly non-convex) and a convex (possibly nonsmooth) function involving a large number of variables. A popular approach to solve this problem is the block coordinate descent (BCD) method whereby at each iteration only one variable block is updated while the remaining variables are held fixed. With the recent advances in the developments of the multi-core parallel processing technology, it is desirable to parallelize the BCD method by allowing multiple blocks to be updated simultaneously at each iteration of the algorithm. In this work, we propose an inexact parallel BCD approach where at each iteration, a subset of the variables is updated in parallel by minimizing convex approximations of the original objective function. We investigate the convergence of this parallel BCD method for both randomized and cyclic variable selection rules. We analyze the asymptotic and non-asymptotic convergence behavior of the algorithm for both convex and non-convex objective functions. The numerical experiments suggest that for a special case of Lasso minimization problem, the cyclic block selection rule can outperform the randomized rule.	[Razaviyayn, Meisam] Stanford Univ, Elect Engn Dept, Stanford, CA 94305 USA; [Hong, Mingyi] Iowa State Univ, Ind & Mfg Syst Engn, Ames, IA USA; [Luo, Zhi-Quan] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA; [Pang, Jong-Shi] Univ Southern Calif, Dept Ind & Syst Engn, Los Angeles, CA USA	Stanford University; Iowa State University; University of Minnesota System; University of Minnesota Twin Cities; University of Southern California	Razaviyayn, M (corresponding author), Stanford Univ, Elect Engn Dept, Stanford, CA 94305 USA.	meisamr@stanford.edu; mingyi@iastate.edu; luozg@umn.edu; jongship@usc.edu	Hong, Mingyi/H-6274-2013		University of Minnesota Graduate School Doctoral Dissertation Fellowship; AFOSR [FA9550-12-1-0340]	University of Minnesota Graduate School Doctoral Dissertation Fellowship(University of Minnesota System); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	The authors are grateful to the University of Minnesota Graduate School Doctoral Dissertation Fellowship and AFOSR, grant number FA9550-12-1-0340 for the support during this research.	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bradley Joseph K, 2011, ARXIV11055379; Facchinei F., 2013, ARXIV13112444; Fercoq O., 2013, ARXIV13095885; Fercoq O., 2014, ARXIV14055300; Fercoq O., 2013, ARXIV13125799; Hong M., 2013, ARXIV13106957; Lee YT, 2013, ANN IEEE SYMP FOUND, P147, DOI 10.1109/FOCS.2013.24; Liu J., 2013, ARXIV13111873; Mairal J., 2013, ICML; Mairal J., 2014, ARXIV14024419; Necoara I., 2013, ARXIV13125302; Necoara I, 2013, J PROCESS CONTR, V23, P243, DOI 10.1016/j.jprocont.2012.12.012; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Patrascu A, 2013, 2013 EUROPEAN CONTROL CONFERENCE (ECC), P2789; Peng Z., 2013, PREPRINT; Razaviyayn M., 2013, ARXIV13074457; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Richtarik P., 2012, ARXIV12120873; Richtarik P, 2013, ARXIV13103438; Richtarik P, 2012, OPERAT RES PROCEED, P27, DOI 10.1007/978-3-642-29210-1_5; Scherrer C, 2012, ADV NEURAL INFORM PR, P28; Scherrer Chad, 2012, ARXIV12066409; Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892	29	2	2	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103060
C	Sabato, S; Munos, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sabato, Sivan; Munos, Remi			Active Regression by Stratification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				WEIGHTS; DESIGNS; MODELS	We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of O (1/is an element of) cannot in general be improved upon. Nonetheless, the so-called 'constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches the optimal risk using piecewise constant approximations.	[Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel; [Munos, Remi] INRIA, Lille, France; [Munos, Remi] Google DeepMind, London, England	Ben Gurion University; Inria; Google Incorporated	Sabato, S (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel.	sabatos@cs.bgu.ac.il; remi.munos@inria.fr	Sabato, Sivan/U-4730-2017	Sabato, Sivan/0000-0002-7975-0044				Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Bujrbidge R, 2007, LECT NOTES COMPUT SC, V4881, P209; Cai WB, 2013, IEEE DATA MINING, P51, DOI 10.1109/ICDM.2013.104; Carpentier Alexandra, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P229, DOI 10.1007/978-3-642-34106-9_20; CASELLA G, 1981, ANN STAT, V9, P870, DOI 10.1214/aos/1176345527; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295; Dasgupta S., 2008, ADV NEURAL INFORM PR, P353; Efromovich S., 2007, SEQUENTIAL ANAL, V26, P3, DOI [10.1080/07474940601109670, DOI 10.1080/07474940601109670]; Ganti R, 2012, INT C ART INT STAT, V22, P422; GLASSERMAN P, 2004, APPL MATH, V53, P1; Gyorfi L., 2002, DISTRIBUTION FREE TH; Hsu D., 2012, 25 C LEARN THEOR; Hsu D, 2014, PR MACH LEARN RES, V32, P37; Kanamori T, 2003, J STAT PLAN INFER, V116, P149, DOI 10.1016/S0378-3758(02)00234-3; Kanamori T, 2002, ANN I STAT MATH, V54, P459, DOI 10.1023/A:1022446624428; Needell D., 2013, STOCHASTIC GRADIENT; Sugiyama M, 2006, J MACH LEARN RES, V7, P141; Sugiyama M, 2009, MACH LEARN, V75, P249, DOI 10.1007/s10994-009-5100-3; Von Neumann J., 1951, APPL MATH SERIES, V12, P1; Wiens DP, 1998, J AM STAT ASSOC, V93, P1440, DOI 10.2307/2670058; Wiens DP, 2000, J STAT PLAN INFER, V83, P395, DOI 10.1016/S0378-3758(99)00102-0	22	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102029
C	Saxena, S; Dahleh, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Saxena, Shreya; Dahleh, Munther			Real-Time Decoding of an Integrate and Fire Encoder	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Neuronal encoding models range from the detailed biophysically-based Hodgkin Huxley model, to the statistical linear time invariant model specifying firing rates in terms of the extrinsic signal. Decoding the former becomes intractable, while the latter does not adequately capture the nonlinearities present in the neuronal encoding system. For use in practical applications, we wish to record the output of neurons, namely spikes, and decode this signal fast in order to act on this signal, for example to drive a prosthetic device. Here, we introduce a causal, real-time decoder of the biophysically-based Integrate and Fire encoding neuron model. We show that the upper bound of the real-time reconstruction error decreases polynomially in time, and that the L-2 norm of the error is bounded by a constant that depends on the density of the spikes, as well as the bandwidth and the decay of the input signal. We numerically validate the effect of these parameters on the reconstruction error.	[Saxena, Shreya; Dahleh, Munther] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Saxena, S (corresponding author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.	ssaxena@mit.edu; dahleh@mit.edu			National Science Foundation's Emerging Frontiers in Research and Innovation Grant [1137237]	National Science Foundation's Emerging Frontiers in Research and Innovation Grant(National Science Foundation (NSF))	Research supported by the National Science Foundation's Emerging Frontiers in Research and Innovation Grant (1137237).	Brown EN, 1998, J NEUROSCI, V18, P7411; Carmena JM, 2003, PLOS BIOL, V1, P193, DOI 10.1371/journal.pbio.0000042; Eden UT, 2004, NEURAL COMPUT, V16, P971, DOI 10.1162/089976604773135069; Feichtinger H. G., 1994, WAVELETS MATH APPL, V1994, P305; Feichtinger HG, 2012, ADV COMPUT MATH, V36, P67, DOI 10.1007/s10444-011-9180-9; Gerstner W., 2002, SPIKING NEURON MODEL; Gontier D, 2014, APPL COMPUT HARMON A, V36, P63, DOI 10.1016/j.acha.2013.02.002; HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764; Lazar AA, 2004, NEUROCOMPUTING, V58, P53, DOI 10.1016/j.neucom.2004.01.022; Lazar AA, 2004, IEEE T CIRCUITS-I, V51, P2060, DOI 10.1109/TCSI.2004.835026; Lazar AA, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL VI, PROCEEDINGS, P709; Lazar AA, 2010, IEEE T INFORM THEORY, V56, P821, DOI 10.1109/TIT.2009.2037040; Sarma SV, 2007, IEEE T AUTOMAT CONTR, V52, P284, DOI 10.1109/TAC.2006.886539; Sarma SV, 2010, INT J ROBUST NONLIN, V20, P41, DOI 10.1002/rnc.1418; Saxena S., 2014, P 53 IEEE C DEC CONT; Serruya MD, 2002, NATURE, V416, P141, DOI 10.1038/416141a; Wu W, 2009, IEEE T NEUR SYS REH, V17, P370, DOI 10.1109/TNSRE.2009.2023307	17	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103073
C	Soudry, D; Hubara, I; Meir, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Soudry, Daniel; Hubara, Itay; Meir, Ron			Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a "mean-field" factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.	[Soudry, Daniel] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Hubara, Itay; Meir, Ron] Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel	Columbia University; Technion Israel Institute of Technology	Soudry, D (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.	daniel.soudry@gmail.com; itayhubara@gmail.com; rmeir@ee.technion.ac.il			Technion V.P.R. fund; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI); Gruss Lipper Charitable Foundation	Technion V.P.R. fund; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI); Gruss Lipper Charitable Foundation	The authors are grateful to C. Baldassi, A. Braunstein and R. Zecchina for helpful discussions and to A. Hallak, T. Knafo and U. Sumbul for reviewing parts of this manuscript. The research was partially funded by the Technion V.P.R. fund, by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), and by the Gruss Lipper Charitable Foundation.	Baldassi C, 2007, P NATL ACAD SCI USA, V104, P11079, DOI 10.1073/pnas.0700324104; Barber D, 1998, ADV NEUR IN, V10, P395; BATTITI R, 1995, IEEE T NEURAL NETWOR, V6, P1185, DOI 10.1109/72.410361; Bishop, 1995, NEURAL NETWORKS PATT; Bishop C.M, 2006, PATTERN RECOGN; Braunstein A, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.030201; Crammer K, 2013, MACH LEARN, V91, P155, DOI 10.1007/s10994-013-5327-x; Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090; Graves A., 2011, ADV NEURAL INF PROCE, V24, P1; Hinton G E, 1993, COLT 93; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Karakiewicz R, 2012, IEEE SENS J, V12, P785, DOI 10.1109/JSEN.2011.2113393; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y. A., 2012, NEURAL NETWORKS TRIC; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Mayoraz E, 1996, INT J NEURAL SYST, V7, P149, DOI 10.1142/S0129065796000129; Minka T.P., 2001, P 17 C UNC ART INT, P362; Moerland P., 1997, HDB NEURAL COMPUTATI; Neal R. M., 2012, BAYESIAN LEARNING NE; Ribeiro F, 2011, NEURAL COMPUT, V23, P1047, DOI 10.1162/NECO_a_00104; Saad D., 1990, Complex Systems, V4, P573; SOLLA S, 1998, ON LINE LEARNING NEU; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Xiong HY, 2011, BIOINFORMATICS, V27, P2554, DOI 10.1093/bioinformatics/btr444	26	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100048
C	Szorenyi, B; Kedenburg, G; Munos, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Szorenyi, Balazs; Kedenburg, Gunnar; Munos, Remi			Optimistic planning in Markov decision processes using a generative model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample complexity problem of computing, with probability 1-delta, an is an element of-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm, called StOP (for Stochastic-Optimistic Planning), based on the "optimism in the face of uncertainty" principle. StOP can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the MDP.	[Szorenyi, Balazs; Kedenburg, Gunnar; Munos, Remi] INRIA Lille Nord Europe, SequeL Project, Lille, France; [Szorenyi, Balazs] MTA SZTE Res Grp Artificial Intelligence, Budapest, Hungary; [Munos, Remi] Google DeepMind, London, England	Google Incorporated	Szorenyi, B (corresponding author), INRIA Lille Nord Europe, SequeL Project, Lille, France.	balazs.szorenyi@inria.fr; gunnar.kedenburg@inria.fr; remi.munos@inria.fr			French Ministry of Higher Education and Research; European Community's Seventh Framework Programme (FP7/2007-2013) [270327]; BMBF project ALICE [01IB10003B]	French Ministry of Higher Education and Research; European Community's Seventh Framework Programme (FP7/2007-2013); BMBF project ALICE(Federal Ministry of Education & Research (BMBF))	This work was supported by the French Ministry of Higher Education and Research, and by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement no 270327 (project CompLACS). Author two would like to acknowledge the support of the BMBF project ALICE (01IB10003B).	[Anonymous], 1980, PRINCIPLES ARTIFICIA; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bertsekas D. P., 2001, DYNAMIC PROGRAMMING; Bubeck S, 2010, C LEARN THEOR; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Busoniu L., 2012, JMLR WORKSHOP C P, P182; Camacho C., 2004, ADV TK CONT SIGN PRO, V18, P1, DOI 10.1007/978-0-85729-398-5; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Coulom Remi, 2006, P COMP GAM 2006; Even-Dar E., 2003, P 20 INT C MACH LEAR, P162; Gelly S., 2006, RR6062 INRIA; Hansen Eric A., 1999, P BAR IL S FDN ART I; Hren JF, 2008, LECT NOTES ARTIF INT, V5323, P151, DOI 10.1007/978-3-540-89722-4_12; Kearns M, 2002, MACH LEARN, V49, P193, DOI 10.1023/A:1017932429737; Kedenburg G., 2013, P INT C ADV NEUR INF, V26, P2382; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Walsh TJ, 2010, AAAI CONF ARTIF INTE, P612	21	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101039
C	Titsias, MK; Yau, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Titsias, Michalis K.; Yau, Christopher			Hamming Ball Auxiliary Sampling for Factorial Hidden Markov Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We introduce a novel sampling algorithm for Markov chain Monte Carlo-based Bayesian inference for factorial hidden Markov models. This algorithm is based on an auxiliary variable construction that restricts the model space allowing iterative exploration in polynomial time. The sampling approach overcomes limitations with common conditional Gibbs samplers that use asymmetric updates and become easily trapped in local modes. Instead, our method uses symmetric moves that allows joint updating of the latent sequences and improves mixing. We illustrate the application of the approach with simulated and a real data example.	[Titsias, Michalis K.] Athens Univ Econ & Business, Dept Informat, Athens, Greece; [Yau, Christopher] Univ Oxford, Wellcome Trust Ctr Human Genet, Oxford, England	Athens University of Economics & Business; University of Oxford; Wellcome Centre for Human Genetics	Titsias, MK (corresponding author), Athens Univ Econ & Business, Dept Informat, Athens, Greece.	mtitsias@aueb.gr; cyau@well.ox.ac.uk			UK Medical Research Council New Investigator Research Grant [MR/L001411/1]; Research Funding at AUEB for Excellence and Extroversion, Action 1	UK Medical Research Council New Investigator Research Grant(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); Research Funding at AUEB for Excellence and Extroversion, Action 1	We thank the reviewers for insightful comments. MKT greatly acknowledges support from "Research Funding at AUEB for Excellence and Extroversion, Action 1: 2012-2014". CY acknowledges the support of a UK Medical Research Council New Investigator Research Grant (Ref No. MR/L001411/1). CY is also affiliated with the Department of Statistics, University of Oxford.	Griffiths T.L., 2005, ADV NEURAL INFORM PR; Kim H., 2011, P SIAM C DAT MIN MES, DOI [10.1137/1.9781611972818.64, 10.1137/1, DOI 10.1137/1]; Kolter J.Z., 2012, P ARTIFICIAL INTELLI, P1472; Kolter Zico, 2011, ARTIF INTELL, V25; Li N, 2003, GENETICS, V165, P2213; Marchini J, 2010, NAT REV GENET, V11, P499, DOI 10.1038/nrg2796; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Scott SL, 2002, J AM STAT ASSOC, V97, P409, DOI 10.1198/016214502760046961; SWENDSEN RH, 1987, PHYS REV LETT, V58, P86, DOI 10.1103/PhysRevLett.58.86; Van Gael J, 2009, ADV NEURAL INFORM PR, V21; Yau C, 2013, BIOINFORMATICS, V29, P2482, DOI 10.1093/bioinformatics/btt416	14	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102017
C	Van den Oord, A; Schrauwen, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Van den Oord, Aaron; Schrauwen, Benjamin			Factoring Variations in Natural Images with Deep Gaussian Mixture Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.	[Van den Oord, Aaron; Schrauwen, Benjamin] Univ Ghent, Elect & Informat Syst Dept ELIS, Ghent, Belgium	Ghent University	Van den Oord, A (corresponding author), Univ Ghent, Elect & Informat Syst Dept ELIS, Ghent, Belgium.	aaron.vandenoord@ugent.be; benjamin.schrauwen@ugent.be						[Anonymous], 2014, INT C MACH LEARN; Bengio Y., 2013, INT C MACH LEARN; Bengio Y., 2006, INNOVATIONS MACHINE; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Byrd R. H., 1995, SIAM J SCI COMPUTING; Ghahramani Z, 1996, SWITCHING STATE SPAC; Gregor Karol, 2013, INT C MACH LEARN; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Krizhevsky Alex, 2014, P INT C LEARN REPR; Lake B. M., 2013, ONE SHOT LEARNING IN; MARTIN D, 2001, P INT C COMP VIS; Mikolov Tomas, 2013, P WORKS ICLR; Pascanu Razvan, 2013, P INT C LEARN REPR; Tang Y., 2012, INT C MACH LEARN; Torralba A., 2008, IEEE T PATTERN ANAL; Uria B., 2013, ADV NEURAL INFORM PR; Uria Benigno, 2013, P INT C MACH LEARN; van den Oord Aaron<spacing diaeresis>, 2014, J MACHINE LEARNING R, p2014b; Zoran D., 2011, INT C COMP VIS	19	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100006
C	Ver Steeg, G; Galstyan, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ver Steeg, Greg; Galstyan, Aram			Discovering Structure in High-Dimensional Data Through Correlation Explanation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.	[Ver Steeg, Greg; Galstyan, Aram] Univ Southern Calif, Informat Sci Inst, Marina Del Rey, CA 90292 USA	University of Southern California	Ver Steeg, G (corresponding author), Univ Southern Calif, Informat Sci Inst, Marina Del Rey, CA 90292 USA.	gregv@isi.edu; galstyan@isi.edu		Ver Steeg, Greg/0000-0002-0793-141X	AFOSR [FA9550-12-1-0417]; DARPA [W911NF-12-1-0034]	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank Virgil Griffith, Shuyang Gao, Hsuan-Yi Chu, Shirley Pepke, Bilal Shaw, Jose-Luis Ambite, and Nathan Hodas for helpful conversations. This research was supported in part by AFOSR grant FA9550-12-1-0417 and DARPA grant W911NF-12-1-0034.	Adams RP, 2009, ARXIV10010160; Anandkumar Animashree, 2011, ADV NEURAL INF PROCE, P2025; Ay Nihat, 2006, P EUROPEAN COMPLEX S; Beer, 2010, ARXIV10042515, DOI DOI 10.HTTPS://ARXIV.0RG/ABS/1004.2515; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Chang Jonathan., 2009, P 22 INT C NEURAL IN, P288, DOI DOI 10.5555/2984093.2984126; Choi MJ, 2011, J MACH LEARN RES, V12, P1771; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Goldberg LR., 1992, PSYCHOL ASSESSMENT, V4, P26, DOI [10.1037/1040-3590.4.1.26, DOI 10.1037/1040-3590.4.1.26]; Griffith V., 2012, ARXIV12054265; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Kraskov A, 2005, EUROPHYS LETT, V70, P278, DOI 10.1209/epl/i2004-10483-y; Kumar Gowtham Ramani, 2014, ARXIV14020062; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Lee H., 2009, P ANN INT C MACH LEA, P609; Lichman M, 2013, UCI MACHINE LEARNING; Mikolov T., 2013, ARXIV; Mourad R, 2013, J ARTIF INTELL RES, V47, P157, DOI 10.1613/jair.3879; Pearl J., 2009, CAUSALITY MODELS REA; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rosenberg NA, 2002, SCIENCE, V298, P2381, DOI 10.1126/science.1078311; Schneidman E, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.238701; Schneidman E, 2003, J NEUROSCI, V23, P11539; Slonim N., 2002, THESIS EBREW U JERU; Slonim N, 2006, NEURAL COMPUT, V18, P1739, DOI 10.1162/neco.2006.18.8.1739; Steeg Greg Ver, 2014, ARXIV14107404; Steudel B., 2010, ARXIV10105720; Studeny M, 1998, NATO ADV SCI I D-BEH, V89, P261; Tishby Naftali, 2000, PHYSICS0004057 ARXIV; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066	34	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103031
C	Vertechi, P; Brendel, W; Machens, CK		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Vertechi, Pietro; Brendel, Wieland; Machens, Christian K.			Unsupervised learning of an efficient short-term memory network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				NEURONS	Learning in recurrent neural networks has been a topic fraught with difficulties and problems. We here report substantial progress in the unsupervised learning of recurrent networks that can keep track of an input signal. Specifically, we show how these networks can learn to efficiently represent their present and past inputs, based on local learning rules only. Our results are based on several key insights. First, we develop a local learning rule for the recurrent weights whose main aim is to drive the network into a regime where, on average, feedforward signal inputs are canceled by recurrent inputs. We show that this learning rule minimizes a cost function. Second, we develop a local learning rule for the feedforward weights that, based on networks in which recurrent inputs already predict feedforward inputs, further minimizes the cost. Third, we show how the learning rules can be modified such that the network can directly encode non-whitened inputs. Fourth, we show that these learning rules can also be applied to a network that feeds a time-delayed version of the network output back into itself. As a consequence, the network starts to efficiently represent both its signal inputs and their history. We develop our main theory for linear networks, but then sketch how the learning rules could be transferred to balanced, spiking networks.	[Vertechi, Pietro; Brendel, Wieland; Machens, Christian K.] Champalimaud Ctr Unknown, Champalimaud Neurosci Programme, Lisbon, Portugal	Fundacao Champalimaud	Vertechi, P (corresponding author), Champalimaud Ctr Unknown, Champalimaud Neurosci Programme, Lisbon, Portugal.	pietro.vertechi@neuro.fchampalimaud.org; wieland.brendel@neuro.fchampalimaud.org; christian.machens@neuro.fchampalimaud.org		Vertechi, Pietro/0000-0003-3751-1181				Barrett DGT, 2013, ADV NEURAL INFORM PR, V26, P1538; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258; Bourdoukan R, 2012, ADV NEURAL INFORM PR, V25; Fusi S, 2005, NEURON, V45, P599, DOI 10.1016/j.neuron.2005.02.001; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Hu T, 2012, NEURAL COMPUT, V24, P2852, DOI 10.1162/NECO_a_00353; Jaeger H., 2001, GERMAN NATL RES CTR, V48; Lazar A, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.023.2009; Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005; Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955; Machens CK, 2005, SCIENCE, V307, P1121, DOI 10.1126/science.1104171; Major G, 2004, CURR OPIN NEUROBIOL, V14, P675, DOI 10.1016/j.conb.2004.10.017; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Renart A, 2004, MATH COMP BIOL SER, P431; Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Wang XJ, 2002, NEURON, V36, P955, DOI 10.1016/S0896-6273(02)01092-9; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102104
C	Wimalawarne, K; Sugiyama, M; Tomioka, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wimalawarne, Kishan; Sugiyama, Masashi; Tomioka, Ryota			Multitask learning meets tensor factorization: task imputation via convex optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.	[Wimalawarne, Kishan] Tokyo Inst Technol, Meguro Ku, Tokyo, Japan; [Sugiyama, Masashi] Univ Tokyo, Bunkyo Ku, Tokyo, Japan; [Tomioka, Ryota] TTI C, Chicago, IL USA	Tokyo Institute of Technology; University of Tokyo	Wimalawarne, K (corresponding author), Tokyo Inst Technol, Meguro Ku, Tokyo, Japan.	kishan@sg.cs.titech.ac.jp; sugi@k.u-tokyo.ac.jp; tomioka@ttic.edu	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	JST CREST program	JST CREST program(Core Research for Evolutional Science and Technology (CREST)Japan Science & Technology Agency (JST))	MS acknowledges support from the JST CREST program.	Ando RK, 2005, J MACH LEARN RES, V6, P1817; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Argyriou Andreas, 2008, ADV NEURAL INFORM PR, P25; Bakker B, 2004, J MACH LEARN RES, V4, P83, DOI 10.1162/153244304322765658; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730; Foygel R., 2011, ARXIV11023923; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Liu Ji, 2009, P ICCV; Maurer A., 2012, ARXIV12121496; Romera-Paredes B., 2013, P 30 INT C MACHINE L, P1444; Signoretto M., 2010, 10186 ESATSISTA; Signoretto Marco, 2013, ARXIV13104977; Srebro N., 2005, P ADV NEURAL INFORM; Tomioka R., 2011, ARXIV10100789; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Vargas-Govea B., 2011, P 3 WORKSH CONT AW R	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103079
C	Yang, JB; Liao, XJ; Chen, MH; Carin, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yang, Jianbo; Liao, Xuejun; Chen, Minhua; Carin, Lawrence			Compressive Sensing of Signals from a GMM with Sparse Precision Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper is concerned with compressive sensing of signals drawn from a Gaussian mixture model (GMM) with sparse precision matrices. Previous work has shown: (i) a signal drawn from a given GMM can be perfectly reconstructed from r noise-free measurements if the (dominant) rank of each covariance matrix is less than r; (ii) a sparse Gaussian graphical model can be efficiently estimated from fully-observed training signals using graphical lasso. This paper addresses a problem more challenging than both (i) and (ii), by assuming that the GMM is unknown and each signal is only observed through incomplete linear measurements. Under these challenging assumptions, we develop a hierarchical Bayesian method to simultaneously estimate the GMM and recover the signals using solely the incomplete measurements and a Bayesian shrinkage prior that promotes sparsity of the Gaussian precision matrices. In addition, we provide theoretical performance bounds to relate the reconstruction error to the number of signals for which measurements are available, the sparsity level of precision matrices, and the "incompleteness" of measurements. The proposed method is demonstrated extensively on compressive sensing of imagery and video, and the results with simulated and hardware-acquired real measurements show significant performance improvement over state-of-the-art methods.	[Yang, Jianbo; Liao, Xuejun; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Chen, Minhua] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Chen, Minhua] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA	Duke University; University of Chicago; University of Chicago	Yang, JB (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	jianbo.yang@duke.edu; xjliao@duke.edu; dukemeeting@gmail.com; lcarin@duke.edu		Carin, Lawrence/0000-0001-6277-7948	ARO; ONR; DARPA; DOE; NGA	ARO; ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA	The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR.	Bioucas-Dias J., 2007, IEEE T IMAGE PROCESS; CANDES E, 2006, IEEE T INFORM THEORY; Cevher V., 2010, IEEE SIGNAL PROCESSI; Chen M., 2010, IEEE T SIGNAL PROCES; Dempster A., 1977, J ROYAL STAT SOC B; DONOHO DL, 2006, IEEE T INFORM THEORY; Duarte M. F., 2008, IEEE SIGNAL PROCESSI; Fan J., 2009, ANN APPL STAT; Friedman Jerome, 2008, BIOSTATISTICS; Heitz F., 1993, IEEE T PATTERN ANAL; Hitomi Y., 2011, ICCV; Jordan M. I., 1999, MACHINE LEARNING; Liao X., 2014, SIAM J IMAGING SCI; Llull P., 2013, OPTICS EXPRESS; Loh P.-L., 2012, ANN STAT; Parisi G., 1998, STAT FIELD THEORY, V1; Park T., 2008, J AM STAT ASS; Polson N. G., 2010, BAYESIAN STAT; Renna F., 2014, IEEE T SIGNAL PROCES; Roth S., 2009, INT J COMPUT VISION; RUE H., 2005, GAUSSIAN MARKOV RAND; Tappen M. F., 2007, CVPR; Tenenbaum J. B., 2000, SCIENCE; Yang JB, 2013, IEEE IMAGE PROC, P19, DOI 10.1109/ICIP.2013.6738005; Yang JB, 2014, IEEE T IMAGE PROCESS, V23, P4863, DOI 10.1109/TIP.2014.2344294; Yu G., 2011, IEEE T SIGNAL PROCES; Yu G., 2012, IEEE T IMAGE PROCESS; Zoran Daniel, 2011, ICCV	28	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101074
C	Yu, A; Grauman, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yu, Aron; Grauman, Kristen			Predicting Useful Neighborhoods for Lazy Local Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Lazy local learning methods train a classifier "on the fly" at test time, using only a subset of the training instances that are most relevant to the novel test example. The goal is to tailor the classifier to the properties of the data surrounding the test example. Existing methods assume that the instances most useful for building the local model are strictly those closest to the test example. However, this fails to account for the fact that the success of the resulting classifier depends on the full distribution of selected training instances. Rather than simply gathering the test example's nearest neighbors, we propose to predict the subset of training data that is jointly relevant to training its local model. We develop an approach to discover patterns between queries and their "good" neighborhoods using large-scale multi-label classification with compressed sensing. Given a novel test point, we estimate both the composition and size of the training subset likely to yield an accurate local model. We demonstrate the approach on image classification tasks on SUN and aPascal and show its advantages over traditional global and local approaches.	[Yu, Aron; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Yu, A (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	aron.yu@utexas.edu; grauman@cs.utexas.edu			NSF [IIS-1065390]	NSF(National Science Foundation (NSF))	We thank Ashish Kapoor for helpful discussions. This research is supported in part by NSF IIS-1065390.	Agrawal R., 2013, WWW; Atkeson C., 1997, AI REV; Banerjee S., 2009, SIGIR WKSHP; Bellet A., 2013, CORR; Bottou L., 1992, NEURAL COMP; Dhillon I. S., 2007, ICML; Domeniconi C., 2001, NIPS; Farhadi A., 2009, CVPR; Frome Andrea, 2006, NIPS; Gao T., 2011, ICCV; Geng X., 2008, SIGIR; Gong B., 2013, P INT C MACH LEARN J, V711, P712; Hastie T., 1996, PAMI; Hofmann T., 2004, CIKM; Hsu D.J., 2009, NIPS, P772; Huang J., 2007, NIPS; Jiang J., 2007, ACL; Kapoor A., 2012, NIPS; Kirchhoff K., 2008, SIGIR; Lapin M, 2014, NEURAL NETWORKS, V53, P95, DOI 10.1016/j.neunet.2014.02.002; Marszalek M., 2008, ECCV; Patterson G., 2012, CVPR; Settles B, 2009, ACTIVE LEARNING LIT; Veropoulos K., 1999, IJCAI; Vincent P., 2001, NIPS; Weinberger K. Q., 2009, JMLR; Wu X., 2004, KDD; Yang L., 2006, AAAI; Zhang H., 2006, 2006 IEEE COMP SOC C	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100067
C	Zoran, D; Krishnan, D; Bento, J; Freeman, WT		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zoran, Daniel; Krishnan, Dilip; Bento, Jose; Freeman, William T.			Shape and Illumination from Shading using the Generic Viewpoint Assumption	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ENFORCING INTEGRABILITY	The Generic Viewpoint Assumption (GVA) states that the position of the viewer or the light in a scene is not special. Thus, any estimated parameters from an observation should be stable under small perturbations such as object, viewpoint or light positions. The GVA has been analyzed and quantified in previous works, but has not been put to practical use in actual vision tasks. In this paper, we show how to utilize the GVA to estimate shape and illumination from a single shading image, without the use of other priors. We propose a novel linearized Spherical Harmonics (SH) shading model which enables us to obtain a computationally efficient form of the GVA term. Together with a data term, we build a model whose unknowns are shape and SH illumination. The model parameters are estimated using the Alternating Direction Method of Multipliers embedded in a multi-scale estimation framework. In this prior-free framework, we obtain competitive shape and illumination estimation results under a variety of models and lighting conditions, requiring fewer assumptions than competing methods.	[Zoran, Daniel; Krishnan, Dilip; Freeman, William T.] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Bento, Jose] Boston Coll, Chestnut Hill, MA 02167 USA	Massachusetts Institute of Technology (MIT); Boston College	Zoran, D (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	danielz@mit.edu; dilipkay@mit.edu; jose.bento@bc.edu; billf@mit.edu			NSF CISE/IIS award [1212928]; Qatar Computing Research Institute	NSF CISE/IIS award; Qatar Computing Research Institute(Qatar Foundation (QF)Qatar National Research Fund (QNRF))	This work was supported by NSF CISE/IIS award 1212928 and by the Qatar Computing Research Institute. We would like to thank Jonathan Yedidia for fruitful discussions.	ALBERT MK, 1995, GEOMETRIC REPRESENTATIONS OF PERCEPTUAL PHENOMENA, P95; Barron J. T., 2013, UCBEECS2013117; Barron JT, 2012, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2012.6247693; Barron JT, 2012, LECT NOTES COMPUT SC, V7575, P57, DOI 10.1007/978-3-642-33765-9_5; Bento J., 2013, ADV NEURAL INFORM PR, P521; BINFORD TO, 1981, ARTIF INTELL, V17, P205, DOI 10.1016/0004-3702(81)90025-4; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Derbinsky N., 2013, ARXIV13051961; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Freeman WT, 1996, INT J COMPUT VISION, V20, P243, DOI 10.1007/BF00208721; Gehler P.V., 2011, NIPS, V2, P4; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Horn B. K., 1989, OBTAINING SHAPE SHAD; HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; Jepson A. D., 1995, PERCEPTION BAYESIAN, P478; Kautz J., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P291; Kovesi P, 2005, IEEE I CONF COMP VIS, P994; Kunsberg B., 2012, IEEE COMP SOC C COMP, P39; Li Y., 2014, CVPR; MALIK J, 1987, INT J COMPUT VISION, V1, P73, DOI 10.1007/BF00128527; NAKAYAMA K, 1992, SCIENCE, V257, P1357, DOI 10.1126/science.1529336; PENTLAND AP, 1990, INT J COMPUT VISION, V4, P153, DOI 10.1007/BF00127815; Petrovic N, 2001, PROC CVPR IEEE, P743; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Schmidt M., 2005, MINFUNC; Szeliski R., 2010, COMPUTER VISION ALGO, DOI DOI 10.1007/978-3-030-34372-9; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; Xiong Y., 2014, SHADING LOCAL SHAPE; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103018
C	Audibert, JY; Bousquet, O		Thrun, S; Saul, K; Scholkopf, B		Audibert, JY; Bousquet, O			PAC-Bayesian generic chaining	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					There exist many different generalization error bounds for classification. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classifiers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classifiers, and such priors also arise in the PAC-bayesian setting.	Univ Paris 06, Lab Probabil & Modeles Aleatoires, F-75013 Paris, France	UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Audibert, JY (corresponding author), Univ Paris 06, Lab Probabil & Modeles Aleatoires, 175 Rue Chevaleret, F-75013 Paris, France.							AUDIBERT JY, 2003, UNPUB  DATA DEPENDEN; BARTLETT P, 2003, LOCAL RADEMACHER COM; Boucheron S, 2000, RANDOM STRUCT ALGOR, V16, P277, DOI 10.1002/(SICI)1098-2418(200005)16:3<277::AID-RSA4>3.0.CO;2-1; CATONI O, 2003, LOCALIZED EMPIRICAL; Devroye L., 2001, SPRINGER SERIES STAT; DUDLEY RM, 1984, LECT NOTES MATH, V1097, P00002; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; MCALLESTER D, 1999, P 12 ANN C COMP LEAR; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; MCALLESTER DA, 2003, P COMP LEARN THEOR; TALAGRAND M, 1987, ANN PROBAB, V15, P837, DOI 10.1214/aop/1176992069; Vapnik V., 1974, THEORY PATTERN RECOG	13	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1125	1132						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500140
C	Beardsley, SA; Vaina, LM		Thrun, S; Saul, K; Scholkopf, B		Beardsley, SA; Vaina, LM			A functional architecture for motion pattern processing in MSTd	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				EXTRASTRIATE AREA MST; OPTIC FLOW STIMULI; DIRECTION DISCRIMINATION; MACAQUE MONKEY; NEURONS; SENSITIVITY; INTEGRATION; MECHANISMS; FIELD; MODEL	Psychophysical studies suggest the existence of specialized detectors for component motion patterns (radial, circular, and spiral), that are consistent with the visual motion properties of cells in the dorsal medial superior temporal area (MSTd) of non-human primates. Here we use a biologically constrained model of visual motion processing in MSTd, in conjunction with psychophysical performance on two motion pattern tasks, to elucidate the computational mechanisms associated with the processing of widefield motion patterns encountered during self-motion. In both tasks discrimination thresholds varied significantly with the type of motion pattern presented, suggesting perceptual correlates to the preferred motion bias reported in MSTd. Through the model we demonstrate that while independently responding motion pattern units are capable of encoding information relevant to the visual motion tasks, equivalent psychophysical performance can only be achieved using interconnected neural populations that systematically inhibit non-responsive units. These results suggest the cyclic trends in psychophysical performance may be mediated, in part, by recurrent connections within motion pattern responsive areas whose structure is a function of the similarity in preferred motion patterns and receptive field locations between units.	Boston Univ, Dept Biomed Engn, Boston, MA 02215 USA	Boston University	Beardsley, SA (corresponding author), Boston Univ, Dept Biomed Engn, Boston, MA 02215 USA.		Beardsley, Scott/S-9537-2019	Beardsley, Scott/0000-0001-9254-7924				Beardsley SA, 2001, J COMPUT NEUROSCI, V10, P255, DOI 10.1023/A:1011264014799; Burr DC, 1998, VISION RES, V38, P1731, DOI 10.1016/S0042-6989(97)00346-5; CELEBRINI S, 1995, J NEUROPHYSIOL, V73, P437, DOI 10.1152/jn.1995.73.2.437; CELEBRINI S, 1994, J NEUROSCI, V14, P4109; DUFFY CJ, 1991, J NEUROPHYSIOL, V65, P1346, DOI 10.1152/jn.1991.65.6.1346; DUFFY CJ, 1995, J NEUROSCI, V15, P5192; GILBERT CD, 1992, NEURON, V9, P1, DOI 10.1016/0896-6273(92)90215-Y; GRAZIANO MSA, 1994, J NEUROSCI, V14, P54; Koechlin E, 1999, BIOL CYBERN, V80, P25, DOI 10.1007/s004220050502; Malach R, 1997, CEREB CORTEX, V7, P386, DOI 10.1093/cercor/7.4.386; Matthews N, 1999, VISION RES, V39, P2205, DOI 10.1016/S0042-6989(98)00300-9; Meese TS, 2002, VISION RES, V42, P1073, DOI 10.1016/S0042-6989(02)00058-5; STEMMLER M, 1995, SCIENCE, V269, P1877, DOI 10.1126/science.7569930; TANAKA K, 1989, J NEUROPHYSIOL, V62, P626, DOI 10.1152/jn.1989.62.3.626	14	2	2	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1451	1458						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500180
C	Beygelzimer, A; Rish, I		Thrun, S; Saul, K; Scholkopf, B		Beygelzimer, A; Rish, I			Approximability of probability distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				GRAPH	We consider the question of how well a given distribution can be approximated with probabilistic graphical models. We introduce a new parameter, effective treewidth, that captures the degree of approximability as a tradeoff between the accuracy and the complexity of approximation. We present a simple approach to analyzing achievable tradeoffs that exploits the threshold behavior of monotone graph properties, and provide experimental results that support the approach.	IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA	International Business Machines (IBM)	Beygelzimer, A (corresponding author), IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.							BARAK AB, 1984, SIAM J ALGEBRA DISCR, V5, P508, DOI 10.1137/0605049; BEYGELZIMER A, 2002, P 8 INT C PRINC KNOW; Bollobas B, 1997, SIAM J DISCRETE MATH, V10, P318, DOI 10.1137/S0895480194281215; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DECHTER R, 1998, LEARNING GRAPHICAL M; Erdos P., 1961, B I INT STATIST TOKY, V38, P343; Friedgut E, 1996, P AM MATH SOC, V124, P2993, DOI 10.1090/S0002-9939-96-03732-X; Hoffgen K.-U., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P77, DOI 10.1145/168304.168314; JENSEN FV, 1994, P 10 C UNC AI UAI; NAOR J, 1990, PROCEEDINGS OF THE TWENTY SECOND ANNUAL ACM SYMPOSIUM ON THEORY OF COMPUTING, P213, DOI 10.1145/100216.100244; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Srebro N., 2001, P 17 C UNC ART INT U, P504	13	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						377	384						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500048
C	Bonin, V; Mante, V; Carandini, M		Thrun, S; Saul, K; Scholkopf, B		Bonin, V; Mante, V; Carandini, M			Nonlinear processing in LGN neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				LATERAL GENICULATE-NUCLEUS; RETINAL GANGLION-CELLS; CAT STRIATE CORTEX; CONTRAST; SELECTIVITY; INHIBITION; LENGTH	According to a widely held view, neurons in lateral geniculate nucleus (LGN) operate on visual stimuli in a linear fashion. There is ample evidence, however, that LGN responses are not entirely linear. To account for nonlinearities we propose a model that synthesizes more than 30 years of research in the field. Model neurons have a linear receptive field, and a nonlinear, divisive suppressive field. The suppressive field computes local root-mean-square contrast. To test this model we recorded responses from LGN of anesthetized paralyzed cats. We estimate model parameters from a basic set of measurements and show that the model can accurately predict responses to novel stimuli. The model might serve as the new standard model of LGN responses. It specifies how visual processing in LGN involves both linear filtering and divisive gain control.	Smith Kettlewell Eye Res Inst, San Francisco, CA 94115 USA	The Smith-Kettlewell Eye Research Institute	Bonin, V (corresponding author), Smith Kettlewell Eye Res Inst, 2318 Fillmore St, San Francisco, CA 94115 USA.		Bonin, Vincent/AAQ-3833-2020; Carandini, Matteo/ABD-8296-2021	Carandini, Matteo/0000-0003-4880-7682				BONDS AB, 1989, VISUAL NEUROSCI, V2, P41, DOI 10.1017/S0952523800004314; BONIN V, 2002, ABSTR VIEW IT PLANN; Cai DQ, 1997, J NEUROPHYSIOL, V78, P1045, DOI 10.1152/jn.1997.78.2.1045; Cavanaugh JR, 2002, J NEUROPHYSIOL, V88, P2547, DOI 10.1152/jn.00693.2001; CLELAND BG, 1983, J NEUROSCI, V3, P108; Dan Y, 1996, J NEUROSCI, V16, P3351; ENROTHCUGELL C, 1966, J PHYSIOL-LONDON, V187, P517, DOI 10.1113/jphysiol.1966.sp008107; Freeman TCB, 2002, NEURON, V35, P759, DOI 10.1016/S0896-6273(02)00819-X; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; HUBEL DH, 1961, J PHYSIOL-LONDON, V155, P385, DOI 10.1113/jphysiol.1961.sp006635; JONES HE, 1991, J PHYSIOL-LONDON, V444, P329, DOI 10.1113/jphysiol.1991.sp018881; LEVICK WR, 1972, INVEST OPHTH VISUAL, V11, P302; MANTE V, 2002, ABSTR VIEW IT PLANN; RODIECK R. W., 1965, VISION RES, V5, P583, DOI 10.1016/0042-6989(65)90033-7; Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526; SCLAR G, 1990, VISION RES, V30, P1, DOI 10.1016/0042-6989(90)90123-3; SHAPLEY RM, 1978, J PHYSIOL-LONDON, V285, P275, DOI 10.1113/jphysiol.1978.sp012571; Solomon SG, 2002, J NEUROSCI, V22, P338, DOI 10.1523/JNEUROSCI.22-01-00338.2002	18	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1443	1450						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500179
C	Chudova, D; Hart, C; Mjolsness, E; Smyth, P		Thrun, S; Saul, K; Scholkopf, B		Chudova, D; Hart, C; Mjolsness, E; Smyth, P			Gene expression clustering with functional mixture models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We propose a functional mixture model for simultaneous clustering and alignment of sets of curves measured on a discrete time grid. The model is specifically tailored to gene expression time course data. Each functional cluster center is a nonlinear combination of solutions of a simple linear differential equation that describes the change of individual mRNA levels when the synthesis and decay rates are constant. The mixture of continuous time parametric functional forms allows one to (a) account for the heterogeneity in the observed profiles, (b) align the profiles in time by estimating real-valued time shifts, (c) capture the synthesis and decay of mRNA in the course of an experiment, and (d) regularize noisy profiles by enforcing smoothness in the mean curves. We derive an EM algorithm for estimating the parameters of the model, and apply the proposed approach to the set of cycling genes in yeast. The experiments show consistent improvement in predictive power and within cluster variance compared to regular Gaussian mixtures.	Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA	University of California System; University of California Irvine	Chudova, D (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.			Smyth, Padhraic/0000-0001-9971-8378				[Anonymous], 2003, P 9 INT WORKSH ART I; BARJOSEPH Z, 2002, 6 ANN INT C RES COMP, P39; Cho RJ, 1998, MOL CELL, V2, P65, DOI 10.1016/S1097-2765(00)80114-8; Chudova D., 2003, P 9 ACM SIGKDD INT C, P79, DOI DOI 10.1145/956755.956763; DESARBO WS, 1988, J CLASSIF, V5, P249, DOI 10.1007/BF01897167; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Gibson M., 2001, COMPUTATIONAL METHOD; James GM, 2003, J AM STAT ASSOC, V98, P397, DOI 10.1198/016214503000189; Mestl T, 1996, PHYSICA D, V98, P33, DOI 10.1016/0167-2789(96)00086-3; Ramsay J.O., 1997, FUNCTIONAL DATA ANAL; Yeung KY, 2001, BIOINFORMATICS, V17, P977, DOI 10.1093/bioinformatics/17.10.977	11	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						683	690						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500086
C	Fern, A; Yoon, S; Givan, R		Thrun, S; Saul, K; Scholkopf, B		Fern, A; Yoon, S; Givan, R			Approximate policy iteration with a policy language bias	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We explore approximate policy iteration, replacing the usual cost-function learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-specific planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs.	Purdue Univ, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Fern, A (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.							Aler R, 2002, ARTIF INTELL, V141, P29, DOI 10.1016/S0004-3702(02)00246-1; BACCHUS F, 2000, AIJ, V16, P123; BACCHUS F, 2001, AI MAG, V22, P57; Bellman RE, 1957, DYNAMIC PROGRAMMING; Bertini I, 1996, J BIOL INORG CHEM, V1, P1; Boutilier C, 2000, ARTIF INTELL, V121, P49, DOI 10.1016/S0004-3702(00)00033-3; BOUTILIER C, 2001, IJCAI; Dzeroski S, 2001, MACH LEARN, V43, P7, DOI 10.1023/A:1007694015589; ESTLIN TA, 1996, AAAI; Givan R, 2003, ARTIF INTELL, V147, P163, DOI 10.1016/S0004-3702(02)00376-4; Guestrin C., 2001, P 17 INT JOINT C ART, V17, P673; HOFFMANN J, 2001, J ARTIFICIAL INTELLI, V14, P263; Howard Ronald A., 1960, DYNAMIC PROGRAMMING; Kearns M, 2002, MACH LEARN, V49, P193, DOI 10.1023/A:1017932429737; Khardon R, 1999, ARTIF INTELL, V113, P125, DOI 10.1016/S0004-3702(99)00060-0; Lagoudakis M.G., 2003, ICML; Martin M., 2000, KRR; MCALLESTER D, 1993, J ACM, V40, P246, DOI 10.1145/151261.151264; MINTON S, 1989, ARTIF INTELL, V40, P63, DOI 10.1016/0004-3702(89)90047-7; Minton S., 1993, MACHINE LEARNING MET; MINTON S, 1988, AAAI; TESAURO G, 1992, MACH LEARN, V8, P257, DOI 10.1007/BF00992697; TESAURO G, 1996, NIPS; Tsitsiklis JN, 1996, MACH LEARN, V22, P59, DOI 10.1007/BF00114724; Yoon S., 2002, UAI; [No title captured]; [No title captured]	27	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						847	854						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500106
C	Graf, ABA; Wichmann, FA		Thrun, S; Saul, K; Scholkopf, B		Graf, ABA; Wichmann, FA			Insights from machine learning applied to human visual classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We attempt to understand visual classification in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classification task. Human subjects classified the faces and their gender judgment, reaction time and confidence rating were recorded. Several hyperplane learning algorithms were used on the same classification task using the Principal Components of the texture and shape representation of the faces. The classification performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classification can be modeled by some hyperplane algorithms in the feature space we used. For classification, the brain needs more processing for stimuli close to that hyperplane than for those further away.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Graf, ABA (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.	arnulf.graf@tuebingen.mpg.de; felix.wichmann@tuebingen.mpg.de		Wichmann, Felix/0000-0002-2592-634X				Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Bromley J., 1991, 1135991081916TM AT T; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Graf ABA, 2003, IEEE T NEURAL NETWOR, V14, P597, DOI 10.1109/TNN.2003.811708; GRAF ABA, 2002, LECT NOTES COMPUT SC, V2525, P491; REED SK, 1972, COGNITIVE PSYCHOL, V3, P382, DOI 10.1016/0010-0285(72)90014-X; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Wickens, 2002, ELEMENTARY SIGNAL DE	12	2	2	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						905	912						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500113
C	Gruber, AJ; Dayan, P; Gutkin, BS; Solla, SA		Thrun, S; Saul, K; Scholkopf, B		Gruber, AJ; Dayan, P; Gutkin, BS; Solla, SA			Dopamine modulation in a basal ganglio-cortical network implements saliency-based gating of working memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				PREFRONTAL CORTEX; MODEL; RECEPTORS; REWARD	Dopamine exerts two classes of effect on the sustained neural activity in prefrontal cortex that underlies working memory. Direct release in the cortex increases the contrast of prefrontal neurons, enhancing the robustness of storage. Release of dopamine in the striatum is associated with salient stimuli and makes medium spiny neurons bistable; this modulation of the output of spiny neurons affects prefrontal cortex so as to indirectly gate access to working memory and additionally damp sensitivity to noise. Existing models have treated dopamine in one or other structure, or have addressed basal ganglia gating of working memory exclusive of dopamine effects. In this paper we combine these mechanisms and explore their joint effect. We model a memory-guided saccade task to illustrate how dopamine's actions lead to working memory that is selective for salient input and has increased robustness to distraction.	Northwestern Univ, Chicago, IL 60611 USA	Northwestern University	Gruber, AJ (corresponding author), Northwestern Univ, Chicago, IL 60611 USA.	a-gruber1@northwestern.edu; dayan@gatsby.ucl.ac.uk; boris@gatsby.ucl.ac.uk; sollal@northwestern.edu	Gutkin, Boris/ABC-5754-2020; Gutkin, Boris/A-7420-2014	Gutkin, Boris/0000-0001-6409-979X; Gruber, Aaron/0000-0003-2700-5429				Braver T S, 1999, Prog Brain Res, V121, P327; Brunel N, 2001, J COMPUT NEUROSCI, V11, P63, DOI 10.1023/A:1011204814320; Camperi M, 1998, J COMPUT NEUROSCI, V5, P383, DOI 10.1023/A:1008837311948; Cohen J, 2002, CURR OPIN NEUROBIOL, V12, P223, DOI 10.1016/S0959-4388(02)00314-8; Compte A, 2000, CEREB CORTEX, V10, P910, DOI 10.1093/cercor/10.9.910; Durstewitz D, 2000, J NEUROPHYSIOL, V83, P1733, DOI 10.1152/jn.2000.83.3.1733; Frank MJ, 2001, COGN AFFECT BEHAV NE, V1, P137, DOI 10.3758/CABN.1.2.137; FUNAHASHI S, 1989, J NEUROPHYSIOL, V255, P556; Fuster JM, 1995, MEMORY CEREBRAL CORT; GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6; GRUBER AJ, 2003, NIPS, V15; Kawagoe R, 1998, NAT NEUROSCI, V1, P411, DOI 10.1038/1625; O'Reilly RC, 2002, CEREB CORTEX, V12, P246, DOI 10.1093/cercor/12.3.246; Reynolds JNJ, 2000, NEUROSCIENCE, V99, P199, DOI 10.1016/S0306-4522(00)00273-6; SAWAGUCHI T, 1991, SCIENCE, V251, P947, DOI 10.1126/science.1825731; SCHULTZ W, 1993, J NEUROSCI, V13, P900; SERVANSCHREIBER D, 1990, SCIENCE, V249, P892, DOI 10.1126/science.2392679; WILLIAMS GV, 1995, NATURE, V376, P572, DOI 10.1038/376572a0	18	2	2	1	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1271	1278						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500158
C	Natschlager, T; Maass, W		Thrun, S; Saul, K; Scholkopf, B		Natschlager, T; Maass, W			Information dynamics and emergent computation in recurrent circuits of spiking neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				MUTUAL INFORMATION; ENTROPY	We employ an efficient method using Bayesian and linear classifiers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information.	Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria	Graz University of Technology	Natschlager, T (corresponding author), Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria.	tnatschl@igi.tugraz.at; maass@igi.tugraz.at						Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273; HERTZ J, READING INFORMATION; Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955; Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Panzeri S, 1996, NETWORK-COMP NEURAL, V7, P87, DOI [10.1088/0954-898X/7/1/006, 10.1080/0954898X.1996.11978656]; Pola G., 2003, NEUROSCIENCE DATABAS, P139; Roulston MS, 1999, PHYSICA D, V125, P285, DOI 10.1016/S0167-2789(98)00269-3; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0	12	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1255	1262						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500156
C	Sahani, M		Thrun, S; Saul, K; Scholkopf, B		Sahani, M			A biologically plausible algorithm for reinforcement-shaped representational learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Significant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a significant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generative-modelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way.	Univ Calif San Francisco, WM Keck Fdn Ctr Integrat Neurosci, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Sahani, M (corresponding author), Univ Calif San Francisco, WM Keck Fdn Ctr Integrat Neurosci, San Francisco, CA 94143 USA.	maneesh@phy.ucsf.edu						Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Everitt B., 1984, INTRO LATENT VARIABL; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Press WH., 1993, NUMERICAL RECIPES C; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	7	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1287	1294						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500160
C	Shental, N; Zomet, A		Thrun, S; Saul, K; Scholkopf, B		Shental, N; Zomet, A			Pairwise clustering and graphical models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				IMAGE SEGMENTATION	Significant progress in clustering has been achieved by algorithms that are based on pairwise affinities between the datapoints. In particular, spectral clustering methods have the advantage of being able to divide arbitrarily shaped clusters and are based on efficient eigenvector calculations. However, spectral methods lack a straightforward probabilistic interpretation which makes it difficult to automatically set parameters using training data. In this paper we use the previously proposed typical cut framework for pairwise clustering. We show an equivalence between calculating the typical cut and inference in an undirected graphical model. We show that for clustering problems with hundreds of datapoints exact inference may still be possible. For more complicated datasets, we show that loopy belief propagation (BP) and generalized belief propagation (GBP) can give excellent results on challenging clustering problems. We also use graphical models to derive a learning algorithm for affinity matrices based on labeled data.	Hebrew Univ Jerusalem, Ctr Neural Computac, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Shental, N (corresponding author), Hebrew Univ Jerusalem, Ctr Neural Computac, IL-91904 Jerusalem, Israel.		Hertz, Tomer/S-5744-2016	Hertz, Tomer/0000-0002-0561-1578				Blatt M, 1997, NEURAL COMPUT, V9, P1805, DOI 10.1162/neco.1997.9.8.1805; Gdalyahu Y, 2001, IEEE T PATTERN ANAL, V23, P1053, DOI 10.1109/34.954598; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; MEILA M, 2001, ADV NEURAL INFORMATI, V14; MINKA T, 2003, ADV NEURAL INFORMATI, V16; NG A, 2001, ADV NEURAL INFORMATI, V14; SHENTAL N, 2003, 9 INT C COMP VIS; Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407; WANG JS, 1990, PHYSICA A, V167, P565, DOI 10.1016/0378-4371(90)90275-W; Yedidia J. S., 2003, EXPLORING ARTIFICIAL	10	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						185	192						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500024
C	Solan, Z; Horn, D; Ruppin, E; Edelman, S		Thrun, S; Saul, K; Scholkopf, B		Solan, Z; Horn, D; Ruppin, E; Edelman, S			Unsupervised context sensitive language acquisition from a large corpus	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Significant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of significant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proficiency. The results are encouraging: the model attains a level of performance considered to be "intermediate" for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children.	Tel Aviv Univ, Sackler Fac Exact Sci, IL-69978 Tel Aviv, Israel	Tel Aviv University	Solan, Z (corresponding author), Tel Aviv Univ, Sackler Fac Exact Sci, IL-69978 Tel Aviv, Israel.	rsolan@post.tau.ac.il; horn@post.tau.ac.il; ruppin@post.tau.ac.il; se37@cornell.edu	Ruppin, Eytan/R-9698-2017	Ruppin, Eytan/0000-0002-7862-3940; Horn, David/0000-0003-2708-186X				Chomsky N., 1986, KNOWLEDGE LANGUAGE I; Clark A.S., 2001, THESIS U SUSSEX; CROFT W., 2001, RADICAL CONSTRUCTION; Edelman H, 2002, TRENDS COGN SCI, V6, P125; Goldberg Adele, 1995, CONSTRUCTION GRAMMAR; Gross M, 1997, LANG SPEECH & COMMUN, P329; Hopper PJ, 1998, NEW PSYCHOLOGY OF LANGUAGE, P155; KLEIN D, 2002, ADV NEURAL INFORMATI, V14; Langacker R. W., 1987, FDN COGNITIVE GRAMMA, V1; Lari K., 1990, Computer Speech and Language, V4, P35, DOI 10.1016/0885-2308(90)90022-X; MacDonald MC, 2002, PSYCHOL REV, V109, P35, DOI 10.1037//0033-295X.109.1.35; MACWHINNEY B, 1985, J CHILD LANG, V12, P271, DOI 10.1017/S0305000900006449; Pereira F.C.N., 1992, ACL 30, P128; Pinker S., 1994, LANGUAGE INSTINCT; SAG IA, 1999, SYNTACTIC THEORY FOR; SOLAN Z, 2003, ADV NEURAL INFORMATI, V15; VANZAANEN M, 2001, 05 LEEDS U SCH COMP; WOLFF JG, 1988, CATEGORIES PROCESSES, P179; Wray Alison, 2002, FORMULAIC LANGUAGE L	19	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						961	968						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500120
C	Wang, S; Kubota, T; Siskind, JM		Thrun, S; Saul, K; Scholkopf, B		Wang, S; Kubota, T; Siskind, JM			Salient boundary detection using ratio contour	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				SEGMENTATION	This paper presents a novel graph-theoretic approach, named ratio contour, to extract perceptually salient boundaries from a set of noisy boundary fragments detected in real images. The boundary saliency is defined using the Gestalt laws of closure, proximity, and continuity. This paper first constructs an undirected graph with two different sets of edges: solid edges and dashed edges. The weights of solid and dashed edges measure the local saliency in and between boundary fragments, respectively. Then the most salient boundary is detected by searching for an optimal cycle in this graph with minimum average weight. The proposed approach guarantees the global optimality without introducing any biases related to region area or boundary length. We collect a variety of images for testing the proposed approach with encouraging results.	Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA	University of South Carolina; University of South Carolina System; University of South Carolina Columbia	Wang, S (corresponding author), Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.	songwang@cse.sc.edu; kubotA@cse.sc.edu; qobi@purdue.edu		Wang, Song/0000-0003-4152-5295				Ahuja R. K., 1993, NETWORK FLOWS THEORY; Alter TD, 1996, PROC CVPR IEEE, P13, DOI 10.1109/CVPR.1996.517047; Amir A, 1998, IEEE T PATTERN ANAL, V20, P168, DOI 10.1109/34.659934; COX L, 1996, INT C PATT REC, P557; Elder J., 1996, EUR C COMP VIS, P399; Guy G, 1996, INT J COMPUT VISION, V20, P113, DOI 10.1007/BF00144119; Jacobs DW, 1996, IEEE T PATTERN ANAL, V18, P23, DOI 10.1109/34.476008; Jermyn IH, 2001, IEEE T PATTERN ANAL, V23, P1075, DOI 10.1109/34.954599; KUBOTA T, 2003, INT WORKSH EMMCVPR, P467; Mahamud S, 2003, IEEE T PATTERN ANAL, V25, P433, DOI 10.1109/TPAMI.2003.1190570; Sarkar S, 1996, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.1996.517115; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Ullman S., 1988, P 2 INT C COMP VIS, P321, DOI DOI 10.1109/CCV.1988.590008; Wang S, 2003, IEEE T PATTERN ANAL, V25, P675, DOI 10.1109/TPAMI.2003.1201819; Williams LR, 1999, INT J COMPUT VISION, V34, P81, DOI 10.1023/A:1008187804026; WU Z, 1993, IEEE T PATTERN ANAL, V15, P1101, DOI 10.1109/34.244673; [No title captured]	17	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1571	1578						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500195
C	Wu, TF; Lin, CJ; Weng, RC		Thrun, S; Saul, K; Scholkopf, B		Wu, TF; Lin, CJ; Weng, RC			Probability estimates for multi-class classification by pairwise coupling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Pairwise coupling is a popular multi-class classification method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3].	Natl Taiwan Univ, Dept Comp Sci, Taipei 106, Taiwan	National Taiwan University	Wu, TF (corresponding author), Natl Taiwan Univ, Dept Comp Sci, Taipei 106, Taiwan.							Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Friedman J.H., 1996, ANOTHER APPROACH POL; Hastie T, 1998, ANN STAT, V26, P451; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; HUNTER DR, 2004, IN PRESS ANN STAT; Knerr S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P41; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin H-T, 2003, NOTE PLATTS PROBABIL; Michie Donald, 1994, MACHINE LEARNING NEU, P2; Platt JC, 2000, ADV NEUR IN, P61; PRICE D, 1994, NEURAL INFORM PROCES, V7, P1109; REFREGIER P, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P1003	12	2	2	1	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						529	536						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500067
C	Yagi, M; Yamasaki, H; Shibata, T		Thrun, S; Saul, K; Scholkopf, B		Yagi, M; Yamasaki, H; Shibata, T			A mixed-signal VLSI for real-time generation of edge-based image vectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					A mixed-signal image filtering VLSI has been developed aiming at real-time generation of edge-based image vectors for robust image recognition. A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced to determine the threshold value of edge detection, the key processing parameter in vector generation. As a result, a fully seamless pipeline processing from threshold detection to edge feature map generation has been established. A prototype chip was designed in a 0.35-mum double-polysilicon three-metal-layer CMOS technology and the concept was verified by the fabricated chip. The chip generates a 64-dimension feature vector from a 64 x 64-pixel gray scale image every 80 musec. This is about 10(4) times faster than the software computation, making a real-time image recognition system feasible.	Univ Tokyo, Dept Elect Engn, Bunkyo Ku, Tokyo 1138656, Japan	University of Tokyo	Yagi, M (corresponding author), Univ Tokyo, Dept Elect Engn, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138656, Japan.	mgoat@dent.osaka-u.ac.jp; hideo@if.t.u-tokyo.ac.jp; shibata@ee.t.u-tokyo.ac.jp						CHAKRABARTTY S, 2003, ICASSP 2003; Chen YT, 1999, IEEE ENG MED BIOL, V18, P25, DOI 10.1109/51.740961; LEE CL, 1992, IEE PROC-G, V139, P63, DOI 10.1049/ip-g-2.1992.0012; Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679; Ogawa M, 2002, 2002 SYMPOSIUM ON VLSI CIRCUITS, DIGEST OF TECHNICAL PAPERS, P244, DOI 10.1109/VLSIC.2002.1015095; POTLAPALLI H, 1998, IEEE T IND ELECT, V45; Yagi M, 2003, IEEE T NEURAL NETWOR, V14, P1144, DOI 10.1109/TNN.2003.819038; YAGI M, 2002, P 11 EUR SIGN PROC C, V1, P103; YAGI M, 2002, P ISCAS 2002; YAGI M, 2003, P 13 SCAND C IM AN S, P534; YAGI M, 2000, P 10 EUR SIGN PROC C, P729; YAMASAKI T, ADV NEURAL INFORMATI, V14, P1131	12	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1035	1042						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500129
C	Zhang, T		Thrun, S; Saul, K; Scholkopf, B		Zhang, T			Learning bounds for a generalized family of Bayesian posterior distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				CONSISTENCY; CONVERGENCE; RATES	In this paper we obtain convergence bounds for the concentration of Bayesian posterior distributions (around the true distribution) using a novel method that simplifies and enhances previous results. Based on the analysis, we also introduce a generalized family of Bayesian posteriors, and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution. This important and surprising robustness property does not hold for the standard Bayesian posterior in that it may not concentrate when there exist "bad" prior structures even at places far away from the true distribution.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.							Barron A, 1999, ANN STAT, V27, P536; DIACONIS P, 1986, ANN STAT, V14, P1, DOI 10.1214/aos/1176349830; Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Meir R., 2003, J MACHINE LEARNING R, V4, P839; Robert C., 2001, BAYESIAN CHOICE DECI; Seeger M., 2002, J MACHINE LEARNING R, P233; Shen XT, 2001, ANN STAT, V29, P687	8	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1149	1156						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500143
C	Backus, BT		Dietterich, TG; Becker, S; Ghahramani, Z		Backus, BT			Perceptual metamers in stereoscopic vision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SLANT PERCEPTION; VERTICAL DISPARITY; STEREOPSIS; PERSPECTIVE	Theories of cue combination suggest the possibility of constructing visual stimuli that evoke different patterns of neural activity in sensory areas of the brain, but that cannot be distinguished by any behavioral measure of perception. Such stimuli, if they exist, would be interesting for two reasons. First, one could know that none of the differences between the stimuli survive past the computations used to build the percepts. Second, it can be difficult to distinguish stimulus-driven components of measured neural activity from top-down components (such as those due to the interestingness of the stimuli). Changing the stimulus without changing the percept could be exploited to measure the stimulus-driven activity. Here we describe stimuli in which vertical and horizontal disparities trade during the construction of percepts of slanted surfaces, yielding stimulus equivalence classes. Equivalence class membership changed after a change of vergence eye posture alone, without changes to the retinal images. A formal correspondence can be drawn between these "perceptual metamers" and more familiar "sensory metamers" such as color metamers.	Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA	University of Pennsylvania	Backus, BT (corresponding author), Univ Penn, Dept Psychol, 3815 Walnut St, Philadelphia, PA 19104 USA.							Backus BT, 1999, PERCEPTION, V28, P217, DOI 10.1068/p2753; Backus BT, 1999, VISION RES, V39, P1143, DOI 10.1016/S0042-6989(98)00139-4; BACKUS BT, 2001, IN PRESS J VISION, V1; Banks MS, 1998, VISION RES, V38, P187, DOI 10.1016/S0042-6989(97)00179-X; BARLOW HB, 1958, J PHYSIOL-LONDON, V141, P337, DOI 10.1113/jphysiol.1958.sp005978; BAYLOR DA, 1987, J PHYSIOL-LONDON, V390, P145, DOI 10.1113/jphysiol.1987.sp016691; CALKINS DJ, 1992, VISION RES, V32, P2349, DOI 10.1016/0042-6989(92)90098-4; Clark J.J., 1990, DATA FUSION SENSORY; Enright JT, 1996, VISION RES, V36, P307, DOI 10.1016/0042-6989(95)00099-L; FOLEY JM, 1980, PSYCHOL REV, V87, P411, DOI 10.1037/0033-295X.87.5.411; GARDING J, 1995, VISION RES, V35, P703, DOI 10.1016/0042-6989(94)00162-F; LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M; LOFTUS GR, 1994, J EXP PSYCHOL HUMAN, V20, P33, DOI 10.1037/0096-1523.20.1.33; MAYHEW JEW, 1982, NATURE, V297, P376, DOI 10.1038/297376a0; Ogle KN, 1938, ARCH OPHTHALMOL-CHIC, V20, P604, DOI 10.1001/archopht.1938.00850220076005; ROGERS BJ, 1993, NATURE, V361, P253, DOI 10.1038/361253a0; SCHOEMAKER KR, 1990, BIOPOLYMERS, V30, P1; STEVENS KA, 1983, BIOL CYBERN, V46, P183, DOI 10.1007/BF00336800; VanEe R, 1996, VISION RES, V36, P43, DOI 10.1016/0042-6989(95)00078-E; WANDELL BA, 1985, FDN VISION	20	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1223	1230						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100152
C	Colonius, H; Diederich, A		Dietterich, TG; Becker, S; Ghahramani, Z		Colonius, H; Diederich, A			A maximum-likelihood approach to modeling multisensory enhancement	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SUPERIOR; NEURONS	Multisensory response enhancement (MRE) is the augmentation of the response of a neuron to sensory input of one modality by simultaneous input from another modality. The maximum likelihood (ML) model presented here modifies the Bayesian model for MRE (Anastasio et al.) by incorporating a decision strategy to maximize the number of correct decisions. Thus the ML model can also deal with the important tasks of stimulus discrimination and identification in the presence of incongruent visual and auditory cues. It accounts for the inverse effectiveness observed in neurophysiological recording data, and it predicts a functional relation between uni- and bimodal levels of discriminability that is testable both in neurophysiological and behavioral experiments.	Carl Ossietzky Univ, Inst Kognitionsforsch, D-26111 Oldenburg, Germany	Carl von Ossietzky Universitat Oldenburg	Colonius, H (corresponding author), Carl Ossietzky Univ, Inst Kognitionsforsch, D-26111 Oldenburg, Germany.	hans.colonius@uni-oldenburg.de; a.diederich@iu-bremen.de	Colonius, Hans/M-3350-2019	Colonius, Hans/0000-0002-9733-6939				Anastasio TJ, 2000, NEURAL COMPUT, V12, P1165, DOI 10.1162/089976600300015547; Colonius H, 2001, PERCEPT PSYCHOPHYS, V63, P126, DOI 10.3758/BF03200508; CRAIG A, 1976, PERCEPT PSYCHOPHYS, V19, P473, DOI 10.3758/BF03211215; Egan J.P., 1975, SIGNAL DETECTION THE; FRENS MA, 1995, PERCEPT PSYCHOPHYS, V57, P802, DOI 10.3758/BF03206796; Green D. M., 1974, SIGNAL DETECTION THE; Kadunce DC, 2001, EXP BRAIN RES, V139, P303, DOI 10.1007/s002210100772; King AJ, 2001, CURR BIOL, V11, pR322, DOI 10.1016/S0960-9822(01)00175-0; MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0; MEREDITH MA, 1986, BRAIN RES, V365, P350; MEREDITH MA, 1986, J NEUROPHYSIOL, V56, P640, DOI 10.1152/jn.1986.56.3.640; Stein B E, 1989, J Cogn Neurosci, V1, P12, DOI 10.1162/jocn.1989.1.1.12; Stein BE, 1996, J COGNITIVE NEUROSCI, V8, P497, DOI 10.1162/jocn.1996.8.6.497; STEIN BE, 1996, MERGING SENSES; WALLACE MT, 1993, J NEUROPHYSIOL, V69, P1797, DOI 10.1152/jn.1993.69.6.1797; Welch R, 1986, HDB PERCEPTION HUMAN, V1	16	2	2	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						181	187						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100023
C	Greensmith, E; Bartlett, PL		Dietterich, TG; Becker, S; Ghahramani, Z		Greensmith, E; Bartlett, PL			Variance reduction techniques for gradient estimates in reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				INFINITE-HORIZON; ALGORITHMS	We consider the use of two additive control variate methods to reduce the variance of performance gradient estimates in reinforcement learning problems. The first approach we consider is the baseline method, in which a function of the current state is added to the discounted value estimate. We relate the performance of these methods, which use sample paths, to the variance of estimates based on iid data. We derive the baseline function that minimizes this variance, and we show that the variance for any baseline is the sum of the optimal variance and a weighted squared distance to the optimal baseline. We show that the widely used average discounted value baseline (where the reward is replaced by the difference between the reward and its expectation) is suboptimal. The second approach we consider is the actor-critic method, which uses an approximate value function. We give bounds on the expected squared error of its estimates. We show that minimizing distance to the true value function is suboptimal in general; we provide an example for which the true value function gives an estimate with positive variance, but the optimal value function gives an unbiased estimate with zero variance. Our bounds suggest algorithms to estimate the gradient of the performance of parameterized baseline or value functions. We present preliminary experiments that illustrate the performance improvements on a simple control problem.	Australian Natl Univ, Canberra, ACT, Australia	Australian National University	Greensmith, E (corresponding author), Australian Natl Univ, GPO Box 4, Canberra, ACT, Australia.			Bartlett, Peter/0000-0002-8760-3140				Baird L, 1999, ADV NEUR IN, V11, P968; BARTLETT PL, 2002, IN PRESS J COMPUTER; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Baxter J, 2001, J ARTIF INTELL RES, V15, P351, DOI 10.1613/jair.807; Evans M., 2000, APPROXIMATING INTEGR; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; GREENSMITH E, 2002, VARIANCE REDUCTION T; Kimura H., 1997, P 14 INT C MACH LEAR, V97, P152; Konda VR, 2000, ADV NEUR IN, V12, P1008; MARBACH P, 1998, SIMULATION BASED OPT; Rubio B, 2001, CIENC MAR, V27, P175; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 2000, ADV NEUR IN, V12, P1057; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	15	2	2	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1507	1514						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100187
C	Itti, L; Braun, J; Koch, C		Dietterich, TG; Becker, S; Ghahramani, Z		Itti, L; Braun, J; Koch, C			Modeling the modulatory effect of attention on human spatial vision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SELECTIVE ATTENTION; NEURAL MECHANISMS; SENSITIVITY; PERFORMANCE; LUMINANCE; COLOR; V4	We present new simulation results, in which a computational model of interacting visual neurons simultaneously predicts the modulation of spatial vision thresholds by focal visual attention, for five dual-task human psychophysics experiments. This new study complements our previous findings that attention activates a winner-take-all competition among early visual neurons within one cortical hypercolumn. This "intensified competition" hypothesis assumed that attention equally affects all neurons, and yielded two single-unit predictions: an increase in gain and a sharpening of tuning with attention. While both effects have been separately observed in electrophysiology, no single-unit study has yet shown them simultaneously. Hence, we here explore whether our model could still predict our data if attention might only modulate neuronal gain, but do so non-uniformly across neurons and tasks. Specifically, we investigate whether modulating the gain of only the neurons that are loudest, best-tuned, or most informative about the stimulus, or of all neurons equally but in a task-dependent manner, may account for the data. We find that none of these hypotheses yields predictions as plausible as the intensified competition hypothesis, hence providing additional support for our original findings.	Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA	University of Southern California	Itti, L (corresponding author), Univ So Calif, Dept Comp Sci, Hedco Neurosci Bldg HNB-30A, Los Angeles, CA 90089 USA.			Koch, Christof/0000-0001-6482-8067				Barcelo F, 2000, NAT NEUROSCI, V3, P399, DOI 10.1038/73975; BONNEL AM, 1992, Q J EXP PSYCHOL-A, V44, P601, DOI 10.1080/14640749208401302; BRAUN J, 1990, PERCEPT PSYCHOPHYS, V48, P45, DOI 10.3758/BF03205010; Brefczynski JA, 1999, NAT NEUROSCI, V2, P370, DOI 10.1038/7280; Carrasco M, 2000, VISION RES, V40, P1203, DOI 10.1016/S0042-6989(00)00024-9; Chawla D, 1999, NAT NEUROSCI, V2, P671, DOI 10.1038/10230; CHELAZZI L, 1993, NATURE, V363, P345, DOI 10.1038/363345a0; CORBETTA M, 1990, SCIENCE, V248, P1556, DOI 10.1126/science.2360050; Corbetta M, 2000, NAT NEUROSCI, V3, P292, DOI 10.1038/73009; DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev.neuro.18.1.193; Dosher BA, 2000, VISION RES, V40, P1269, DOI 10.1016/S0042-6989(00)00019-5; Itti L, 2000, J OPT SOC AM A, V17, P1899, DOI 10.1364/JOSAA.17.001899; Itti L, 1999, ADV NEUR IN, V11, P789; Kanwisher N, 2000, NAT REV NEUROSCI, V1, P91, DOI 10.1038/35039043; Lee DK, 1999, NAT NEUROSCI, V2, P375, DOI 10.1038/7286; Lee DK, 1997, VISION RES, V37, P2409, DOI 10.1016/S0042-6989(97)00055-2; Luck SJ, 1997, J NEUROPHYSIOL, V77, P24, DOI 10.1152/jn.1997.77.1.24; MORAN J, 1985, SCIENCE, V229, P782, DOI 10.1126/science.4023713; MOTTER BC, 1994, J NEUROSCI, V14, P2178; NAKAYAMA K, 1989, VISION RES, V29, P1631, DOI 10.1016/0042-6989(89)90144-2; Rees G, 1997, SCIENCE, V275, P835, DOI 10.1126/science.275.5301.835; Ress D, 2000, NAT NEUROSCI, V3, P940, DOI 10.1038/78856; Reynolds JH, 2000, NEURON, V26, P703, DOI 10.1016/S0896-6273(00)81206-4; SPITZER H, 1988, SCIENCE, V240, P338, DOI 10.1126/science.3353728; Treue S, 1999, NATURE, V399, P575, DOI 10.1038/21176; Treue S, 1996, NATURE, V382, P539, DOI 10.1038/382539a0; Yeshurun Y, 1998, NATURE, V396, P72, DOI 10.1038/23936	27	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1247	1254						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100155
C	Kepecs, A; Raghavachari, S		Dietterich, TG; Becker, S; Ghahramani, Z		Kepecs, A; Raghavachari, S			3 state neurons for contextual processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				PYRAMIDAL NEURONS; WORKING-MEMORY; SPINY NEURONS; HIPPOCAMPAL; FLUCTUATIONS; DEPENDENCE; NETWORK; CORTEX	Neurons receive excitatory inputs via both fast AMPA and slow NMDA type receptors. We find that neurons receiving input via NMDA receptors can have two stable membrane states which are input dependent. Action potentials can only be initiated from the higher voltage state. Similar observations have been made in several brain areas which might be explained by our model. The interactions between the two kinds of inputs lead us to suggest that some neurons may operate in 3 states: disabled, enabled and firing. Such enabled, but non-firing modes can be used to introduce context-dependent processing in neural networks. We provide a simple example and discuss possible implications for neuronal processing and response variability.	Brandeis Univ, Volen Ctr Complex Syst, Waltham, MA 02454 USA	Brandeis University	Kepecs, A (corresponding author), Brandeis Univ, Volen Ctr Complex Syst, Waltham, MA 02454 USA.			Kepecs, Adam/0000-0003-0049-8120				Anderson J, 2000, NAT NEUROSCI, V3, P617, DOI 10.1038/75797; Compte A, 2000, CEREB CORTEX, V10, P910, DOI 10.1093/cercor/10.9.910; Grillner S, 1998, BRAIN RES REV, V26, P184, DOI 10.1016/S0165-0173(98)00002-2; Harsch A, 2000, J NEUROSCI, V20, P6181, DOI 10.1523/JNEUROSCI.20-16-06181.2000; Hoffman DA, 1997, NATURE, V387, P869, DOI 10.1038/43119; JAHR CE, 1990, J NEUROSCI, V10, P3178; Johnston D, 2000, J PHYSIOL-LONDON, V525, P75, DOI 10.1111/j.1469-7793.2000.00075.x; Kiehn O, 1998, CURR OPIN NEUROBIOL, V8, P746, DOI 10.1016/S0959-4388(98)80117-7; Lewis BL, 2000, CEREB CORTEX, V10, P1168, DOI 10.1093/cercor/10.12.1168; Li YX, 1996, NEUROSCIENCE, V71, P397, DOI 10.1016/0306-4522(95)00483-1; Lisman JE, 1998, NAT NEUROSCI, V1, P273, DOI 10.1038/1086; Lisman JE, 1999, NEURON, V22, P233, DOI 10.1016/S0896-6273(00)81085-5; MCNAUGHTON BL, 1989, EXP BRAIN RES, V76, P485, DOI 10.1007/BF00248904; Otmakhova NA, 1999, J NEUROSCI, V19, P1437; Pinsky P F, 1994, J Comput Neurosci, V1, P39, DOI 10.1007/BF00962717; WILSON CJ, 1981, BRAIN RES, V220, P67, DOI 10.1016/0006-8993(81)90211-0; Wilson CJ, 1996, J NEUROSCI, V16, P2397	17	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						229	236						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100029
C	Morie, T; Matsuura, T; Nagata, M; Iwata, A		Dietterich, TG; Becker, S; Ghahramani, Z		Morie, T; Matsuura, T; Nagata, M; Iwata, A			An efficient clustering algorithm using stochastic association model and its implementation using nanostructures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper describes a clustering algorithm for vector quantizers using a "stochastic association model". It offers a new simple and powerful soft-max adaptation rule. The adaptation process is the same as the on-line K-means clustering method except for adding random fluctuation in the distortion error evaluation process. Simulation results demonstrate that the new algorithm can achieve efficient adaptation as high as the "neural gas" algorithm, which is reported as one of the most efficient clustering methods. It is a key to add uncorrelated random fluctuation in the similarity evaluation process for each reference vector. For hardware implementation of this process, we propose a nanostructure, whose operation is described by a single-electron circuit. It positively uses fluctuation in quantum mechanical tunneling processes.	Hiroshima Univ, Grad Sch Adv Sci Matter, Higashihiroshima 7398526, Japan	Hiroshima University	Morie, T (corresponding author), Hiroshima Univ, Grad Sch Adv Sci Matter, Higashihiroshima 7398526, Japan.		Nagata, Makoto/U-2904-2017	Nagata, Makoto/0000-0002-0625-9107				Iwata A, 1996, IEICE T FUND ELECTR, VE79A, P145; KIEHL RA, 2000, 4 INT WORKSH QUANT F, P49; KOHNO A, 2000, INT C SOL STAT DEV M, P124; Kohonen Teuvo, 1984, SELF ORG ASS MEMORY; MARTINETZ TM, 1993, IEEE T NEURAL NETWOR, V4, P558, DOI 10.1109/72.238311; Matsuura T., 2000, INT C SOL STAT DEV M, P306; Morie T, 2000, SUPERLATTICE MICROST, V27, P613, DOI 10.1006/spmi.2000.0874; MORIE T, 2000, 4 INT WORKSH QUANT F, P210; OHBA R, 2000, INT C SOL STAT DEV M, P122; ROSE K, 1990, PHYS REV LETT, V65, P945, DOI 10.1103/PhysRevLett.65.945; Rovetta S, 1999, IEEE T CIRCUITS-II, V46, P688, DOI 10.1109/82.769777; Saen M, 1998, IEICE T ELECTRON, VE81C, P30; Tiwari S, 1996, APPL PHYS LETT, V68, P1377, DOI 10.1063/1.116085; Yamanaka T, 2000, NANOTECHNOLOGY, V11, P154, DOI 10.1088/0957-4484/11/3/303	14	2	2	1	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1115	1122						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100139
C	Sadr, J; Mukherjee, S; Thoresz, K; Sinha, P		Dietterich, TG; Becker, S; Ghahramani, Z		Sadr, J; Mukherjee, S; Thoresz, K; Sinha, P			The fidelity of local ordinal encoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				CATS STRIATE CORTEX	A key question in neuroscience is how to encode sensory stimuli such as images and sounds. Motivated by studies of response properties of neurons in the early cortical areas, we propose an encoding scheme that dispenses with absolute measures of signal intensity or contrast and uses, instead, only local ordinal measures. In this scheme, the structure of a signal is represented by a set of equalities and inequalities across adjacent regions. In this paper, we focus on characterizing the fidelity of this representation strategy. We develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure. We also present a neurally plausible implementation of this computation that uses only local update rules. The results highlight the robustness and generalization ability of local ordinal encodings for the task of pattern classification.	MIT, Dept Brain & Cognit Sci, Ctr Biol & Computat Learning, Cambridge, MA 02142 USA	Massachusetts Institute of Technology (MIT)	Sadr, J (corresponding author), MIT, Dept Brain & Cognit Sci, Ctr Biol & Computat Learning, E25-618, Cambridge, MA 02142 USA.	sadr@ai.mit.edu; sayan@ai.mit.edu; thorek@ai.mit.edu; sinha@ai.mit.edu		Mukherjee, Sayan/0000-0002-6715-3920				ANZAI A, 1995, VISUAL NEUROSCI, V12, P77, DOI 10.1017/S0952523800007331; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Bhat DN, 1996, PROC CVPR IEEE, P351, DOI 10.1109/CVPR.1996.517096; DEANGELIS GC, 1993, J NEUROPHYSIOL, V69, P1091, DOI 10.1152/jn.1993.69.4.1091; Herbrich R, 1999, IEE CONF PUBL, P97, DOI 10.1049/cp:19991091; Huber P., 1981, ROBUST STAT; Jacobs C. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P277, DOI 10.1145/218380.218454; POGGIO T, 1975, BIOL CYBERN, V19, P201, DOI 10.1007/BF02281970; THORESZ K, 2001, VISI SCI SOC ABSTR, V1, P81; Tikhonov A.N., 1977, SOLUTION ILL POSED P; Wahba G., 1990, SERIES APPL MATH, V59; YOUNG FW, 1978, PSYCHOMETRIKA, V43, P367, DOI 10.1007/BF02293646	12	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1279	1286						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100159
C	Shapiro, JL; Wearden, J		Dietterich, TG; Becker, S; Ghahramani, Z		Shapiro, JL; Wearden, J			Reinforcement learning and time perception - a model of animal experiments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial co-variances.	Univ Manchester, Dept Comp Sci, Manchester M13 9PL, Lancs, England	University of Manchester	Shapiro, JL (corresponding author), Univ Manchester, Dept Comp Sci, Oxford Rd, Manchester M13 9PL, Lancs, England.			Shapiro, Jonathan/0000-0002-9572-9544				ABRAMOWITZ M, 1967, HDB MATH FUNCTIONS; CHURCH RM, 1994, J EXP PSYCHOL ANIM B, V20, P135, DOI 10.1037/0097-7403.20.2.135; GIBBON J, 1990, COGNITION, V37, P23, DOI 10.1016/0010-0277(90)90017-E; GROSSBERG S, 1992, COGNITIVE BRAIN RES, V1, P3, DOI 10.1016/0926-6410(92)90003-A; HINTON SC, 1997, TIME BEHAV PSYCHOL N; MOORE JW, 1989, BIOL CYBERN, V62, P17, DOI 10.1007/BF00217657; MOORE JW, 1990, LEARNING COMPUTATION, P359; SHAPIRO JL, 2001, UNPUB MODELLING SCAL; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	9	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						115	122						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100015
C	Tanaka, T; Ikeda, S; Amari, S		Dietterich, TG; Becker, S; Ghahramani, Z		Tanaka, T; Ikeda, S; Amari, S			Information-geometrical significance of sparsity in Gallager codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1.	Tokyo Metropolitan Univ, Dept Elect & Informat Engn, Setagaya Ku, Tokyo 1920397, Japan	Tokyo Metropolitan University	Tanaka, T (corresponding author), Tokyo Metropolitan Univ, Dept Elect & Informat Engn, Setagaya Ku, Tokyo 1920397, Japan.	tanaka@eei.metro-u.ac.jp; shiro@brain.kyutech.ac.jp; amari@brain.riken.go.jp	Tanaka, Toshiyuki/C-2749-2011; Ikeda, Shiro/E-1736-2016	Ikeda, Shiro/0000-0002-2462-1448				Amari S, 2001, NEU INF PRO, P241; Amari S., 2000, METHODS INFORM GEOME; Gallager R. G., 1960, LOW DENSITY PARITY C; IKEDA S, 2001, UNPUB IEEE T INFORM; IKEDA S, 2002, ADV NEURAL INFORMATI, V14; Kappen HJ, 1998, ADV NEUR IN, V10, P280; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; Kappen HJ, 2001, NEU INF PRO, P37; MacKay DJC, 1999, IEEE T INFORM THEORY, V45, P399, DOI 10.1109/18.748992; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Murayama T, 2000, PHYS REV E, V62, P1577, DOI 10.1103/PhysRevE.62.1577; Richardson TJ, 2001, IEEE T INFORM THEORY, V47, P599, DOI 10.1109/18.910577; Saad D, 2001, NEU INF PRO, P67; Tanaka T, 2000, NEURAL COMPUT, V12, P1951, DOI 10.1162/089976600300015213; Tanaka T, 2001, NEU INF PRO, P259; Tanaka T, 1999, ADV NEUR IN, V11, P351; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Yedidia JS, 2001, NEU INF PRO, P21	18	2	2	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						527	534						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100066
C	Yu, SX; Shi, JB		Dietterich, TG; Becker, S; Ghahramani, Z		Yu, SX; Shi, JB			Grouping with bias	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblem. We demonstrate that simple priors can greatly improve image segmentation results.	Carnegie Mellon Univ, Ctr Neural Basis Cognit, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Yu, SX (corresponding author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Inst Robot, Pittsburgh, PA 15213 USA.	stella.yu@cs.cmu.edu; jshi@cs.cmu.edu						AMIR A, 1996, EUR C COMP VIS, P371; Boykov Y., 1999, INT C COMP VIS; Chawla Shuchi, 2001, LEARNING LABELED UNL; GDALYAHU Y, 1998, NEURAL INFORMATION P, P424; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Hofmann T, 1998, IEEE T PATTERN ANAL, V20, P803, DOI 10.1109/34.709593; Ishikawa H., 1998, IEEE C COMP VIS PATT; JERMYN IH, 1999, INT C COMP VIS; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; Malik J., 2001, INT J COMPUTER VISIO; MEILA M, 2001, NEURAL INFORMATION P; Perona P., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P655, DOI 10.1007/BFb0055696; Sharon E, 2000, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2000.855801; Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407; [No title captured]	15	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1327	1334						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100165
C	Zhang, T		Dietterich, TG; Becker, S; Ghahramani, Z		Zhang, T			A general greedy approximation algorithm with applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				REGRESSION	Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.							Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Hastie T.J., 1990, GEN ADDITIVE MODELS, V43; JONES LK, 1992, ANN STAT, V20, P608, DOI 10.1214/aos/1176348546; Lee WS, 1996, IEEE T INFORM THEORY, V42, P2118, DOI 10.1109/18.556601; Li JQ, 2000, ADV NEUR IN, V12, P279; Schapire RE, 1998, ANN STAT, V26, P1651; Smola AJ, 2001, ADV NEUR IN, V13, P619; ZHANG T, 2001, 18 INT C MACH LEARN, P624	10	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1065	1072						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100133
C	Archer, C; Leen, TK		Leen, TK; Dietterich, TG; Tresp, V		Archer, C; Leen, TK			From mixtures of mixtures to adaptive transform coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				ALGORITHM	We establish a principled framework for adaptive transform coding. Transform coders are often constructed by concatenating an ad hoc choice of transform with suboptimal bit allocation and quantizer design. Instead, we start from a probabilistic latent variable model in the form of a mixture of constrained Gaussian mixtures. From this model we derive a transform coding algorithm, which is a constrained version of the generalized Lloyd algorithm for vector quantizer design. A byproduct of our derivation is the introduction of a new transform basis, which unlike other transforms (PCA, DCT, etc.) is explicitly optimized for coding. Image compression experiments show adaptive transform coders designed with our algorithm improve compressed image signal-to-noise ratio up to 3 dB compared to global transform coding and 0.5 to 2 dB compared to other adaptive transform coders.	Oregon Grad Inst Sci & Technol, Dept Comp Sci & Engn, Beaverton, OR 97006 USA		Archer, C (corresponding author), Oregon Grad Inst Sci & Technol, Dept Comp Sci & Engn, 20000 NW Walker Rd, Beaverton, OR 97006 USA.							ARCHER C, 1999, P INT JOINT C NEUR N; DEMERS D, 1993, ADV NEURAL INFORMATI, V5; DONY RD, 1995, IEEE T IMAGE PROCESS, V4, P1358, DOI 10.1109/83.465101; Gersho A., 1992, VECTOR QUANTIZATION; Hinton G. E., 1995, ADV NEURAL INFORMATI, V7, P1015; KAMBHATLA N, 1994, ADV NEURAL INFORMATI, V6, P152; KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209; LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Nowlan S.J., 1991, THESIS CARNEGIE MELL; RISKIN EA, 1991, IEEE T INFORM THEORY, V37, P400, DOI 10.1109/18.75264; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728	12	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						925	931						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800130
C	Archie, KA; Mel, BW		Leen, TK; Dietterich, TG; Tresp, V		Archie, KA; Mel, BW			Dendritic compartmentalization could underlie competition and attentional biasing of simultaneous visual stimuli	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				SELECTIVE ATTENTION; AREAS V1; V4; MACAQUE; NEURONS; CORTEX; V2; MECHANISMS; MODEL	Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously "seen" by these cells. Recordings from single V4 neurons suggest that simultaneously presented stimuli compete to set the output firing rate, and that attention acts to isolate individual features by biasing the competition in favor of the attended object. We propose that both stimulus competition and attentional biasing arise from the spatial segregation of afferent synapses onto different regions of the excitable dendritic tree of V4 neurons. The pattern of feedforward, stimulus-driven inputs follows from a Hebbian rule: excitatory afferents with similar RFs tend to group together on the dendritic tree, avoiding randomly located inhibitory inputs with similar RFs. The same principle guides the formation of inputs that mediate attentional modulation. Using both biophysically detailed compartmental models and simplified models of computation in single neurons, we demonstrate that such an architecture could account for the response properties and attentional modulation of V4 neurons. Our results suggest an important role for nonlinear dendritic conductances in extrastriate cortical processing.	Univ So Calif, Neurosci Program, Los Angeles, CA 90089 USA	University of Southern California	Archie, KA (corresponding author), Univ So Calif, Neurosci Program, Los Angeles, CA 90089 USA.							Archie KA, 2000, NAT NEUROSCI, V3, P54, DOI 10.1038/71125; Conner CE, 1997, J NEUROSCI, V17, P3201; Desimone R, 1998, PHILOS T R SOC B, V353, P1245, DOI 10.1098/rstb.1998.0280; DESIMONE R, 1987, J NEUROPHYSIOL, V57, P835, DOI 10.1152/jn.1987.57.3.835; DESIMONE R, 1992, NEURAL NETWORKS VISI, P343; Destexhe A, 1999, J NEUROPHYSIOL, V81, P1531, DOI 10.1152/jn.1999.81.4.1531; Destexhe A, 1994, J Comput Neurosci, V1, P195, DOI 10.1007/BF00961734; Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179; Lee DK, 1999, NAT NEUROSCI, V2, P375, DOI 10.1038/7286; Luck SJ, 1997, J NEUROPHYSIOL, V77, P24, DOI 10.1152/jn.1997.77.1.24; McAdams CJ, 1999, J NEUROSCI, V19, P431, DOI 10.1523/JNEUROSCI.19-01-00431.1999; Mel B. W., 1999, DENDRITES, P271; Mel BW, 1998, J NEUROSCI, V18, P4325; MORAN J, 1985, SCIENCE, V229, P782, DOI 10.1126/science.4023713; MOTTER BC, 1993, J NEUROPHYSIOL, V70, P909, DOI 10.1152/jn.1993.70.3.909; Niebur E, 1994, J Comput Neurosci, V1, P141, DOI 10.1007/BF00962722; OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700; Reynolds JH, 1999, J NEUROSCI, V19, P1736; Salinas E, 1997, J NEUROPHYSIOL, V77, P3267, DOI 10.1152/jn.1997.77.6.3267	19	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						82	88						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800012
C	Ben-David, S; Simon, HU		Leen, TK; Dietterich, TG; Tresp, V		Ben-David, S; Simon, HU			Efficient learning of linear perceptrons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We consider the existence of efficient algorithms for learning the class of half-spaces in R-n in the agnostic learning model (i.e., making no prior assumptions on the example-generating distribution). The resulting combinatorial problem - finding the best agreement half-space over an input sample - is NP hard to approximate to within some constant factor. We suggest a way to circumvent this theoretical bound by introducing a new measure of success for such algorithms. An algorithm is mu -margin successful if the agreement ratio of the half-space it outputs is as good as that of any half-space once training points that are inside the mu -margins of its separating hyper-plane are disregarded. We prove crisp computational complexity results with respect to this success measure: On one hand, for every positive mu, there exist efficient (poly-time) mu -margin successful learning algorithms. On the other hand, we prove that unless P=NP, there is no algorithm that runs in time polynomial in the sample size and in 1/mu that is mu -margin successful for all mu > 0.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Ben-David, S (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.			Simon, Hans/0000-0002-1587-0944				HASTAD J, 1997, P 29 ANN ACM S THEOR, P1; SHAI BD, 2000, NONEMEDABILITY EUCLI; SHAI BD, 2000, P 13 ANN C COMP LEAR, P266; SHAI BD, 2000, P 13 ANN C COMP LEAR, P255	4	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						189	195						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800027
C	Frey, BJ; Kannan, A		Leen, TK; Dietterich, TG; Tresp, V		Frey, BJ; Kannan, A			Accumulator networks: Suitors of local probability propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probability propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian networks and in graphs for coding, but for many conditional probability functions - including the sigmoid function - direct application of the sum-product algorithm is not possible. We introduce "accumulator networks" that have low local complexity (but exponential global complexity) so the sum-product algorithm can be directly applied. In an accumulator network, the probability of a child given its parents is computed by accumulating the inputs from the parents in a Markov chain or more generally a tree. After giving expressions for inference and learning in accumulator networks, we give results on the "bars problem" and on the problem of extracting translated, overlapping faces from an image.	Univ Waterloo, Waterloo, ON N2L 3G1, Canada	University of Waterloo	Frey, BJ (corresponding author), Univ Waterloo, Waterloo, ON N2L 3G1, Canada.							BARBER D, 2000, LEARN WORKSH SNOWB U; FREY B, 1998, ADV NEURAL INFORMATI, V10; Frey B.J., 1996, P 34 ALL C COMM CONT; Jordan M. I., 1999, LEARNING GRAPHICAL M; MCELIECE R, 1998, IEEE J SELECTED AREA, V16; Murphy K. P., 1999, P 15 C UNC ART INT; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Russell S., 2021, ARTIF INTELL, V19, P23; WEISS Y, 2000, ADV NEURAL INFORMATI, V12	9	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						486	492						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800069
C	Gray, MS; Sejnowski, TJ; Movellan, JR		Leen, TK; Dietterich, TG; Tresp, V		Gray, MS; Sejnowski, TJ; Movellan, JR			A comparison of image processing techniques for visual speech recognition applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance.	Salk Inst, Computat Neurobiol Lab, San Diego, CA 92186 USA	Salk Institute	Movellan, JR (corresponding author), Salk Inst, Computat Neurobiol Lab, POB 85800, San Diego, CA 92186 USA.		Sejnowski, Terrence/AAV-5558-2021					Bartlett M., 1998, THESIS U CALIFORNIA; Bartlett MS, 1996, ADV NEUR IN, V8, P823; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; COTTRELL GW, 1991, ADV NEURAL INFORMATI, V3, P564; LUETTIN J, 1997, THESIS U SHEFFIELD; MCKEOWN MJ, IN PRESS P NAT ACAD; MOVELLAN JR, 1995, ADV NEURAL INFORMATI, V7, P851; PADGETT C, 1997, ADV NEURAL INFORMATI, V9; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71	9	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						939	945						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800132
C	Kowalczyk, A		Leen, TK; Dietterich, TG; Tresp, V		Kowalczyk, A			Sparsity of data representation of optimal kernel machine and leave-one-out estimator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Vapnik's result that the expectation of the generalisation error of the optimal hyperplane is bounded by the expectation of the ratio of the number of support vectors to the number of training examples is extended to a broad class of kernel machines. The class includes Support Vector Machines for soft margin classification and regression, and Regularization Networks with a variety of kernels and cost functions. We show that key inequalities in Vapnik's result become equalities once "the classification error" is replaced by "the margin error", with the latter defined as an instance with positive cost. In particular we show that expectations of the true margin error and the empirical margin error are equal, and that the sparse solutions for kernel machines are possible only if the cost function is "partially" insensitive.	Telstra, Chief Technol Off, Clayton, Vic 3168, Australia	Telstra	Kowalczyk, A (corresponding author), Telstra, Chief Technol Off, 770 Blackburn Rd, Clayton, Vic 3168, Australia.	adam.kowalczyk@team-telstra.com						ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43; BURGES C, 2000, ADV NEURAL INFO P SY, V12, P144; COX DD, 1990, ANN STAT, V18, P1676, DOI 10.1214/aos/1176347872; JAAKKOLA T, 1999, P 7 WORK AI STAT SAN; JOACHIMS T, 2000, P INT C MACH LEARN; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; LUNTS AL, 1967, ENG CYBERN, P98; OPPER M, 2000, ADV LARGE MARGIN CLA, P301; SMOLA A, 1998, IN PRESS STAT COMPUT; Smola Alex J, 2000, SPARSE GREEDY MATRIX; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vapnik V.N, 1998, STAT LEARNING THEORY; WILLIAMS C, 1998, LEARNING INFERENCE G	15	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						252	258						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800036
C	Leisink, MAR; Kappen, HJ		Leen, TK; Dietterich, TG; Tresp, V		Leisink, MAR; Kappen, HJ			A tighter bound for graphical models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present a method to bound the partition function of a Boltzmann machine neural network with any odd order polynomial. This is a direct extension of the mean field bound, which is first order. We show that the third order bound is strictly better than mean field, Additionally we show the rough outline how this bound is applicable to sigmoid belief networks. Numerical experiments indicate that an error reduction of a factor two is easily re ached in the region where expansion based approximations are useful.	Univ Nijmegen, Dept Biophys, NL-6525 EZ Nijmegen, Netherlands	Radboud University Nijmegen	Leisink, MAR (corresponding author), Univ Nijmegen, Dept Biophys, Geert Grooteplein 21, NL-6525 EZ Nijmegen, Netherlands.		Kappen, H.J./L-4425-2015					ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Barber D, 1999, ADV NEUR IN, V11, P183; KAPPEN HJ, 1999, ADV NEURAL INFORMATI, V11, P280; LEISINK M, 1999, INT C ART NEUR NETW, V1, P425; NEIL R, 1992, ARTIF INTELL, V56, P71; Pearlman RS, 1987, CHEM DES AUTOM NEWS, V2, P1; Peterson C., 1987, Complex Systems, V1, P995; SAUL SK, 1995, 1 COMP COGN SCI; SHERRINGTON D, 1975, PHYS REV LETT, V35, P1793; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992	10	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						266	272						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800038
C	Nemenman, I; Bialek, W		Leen, TK; Dietterich, TG; Tresp, V		Nemenman, I; Bialek, W			Learning continuous distributions: Simulations with field theoretic priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				PROBABILITY-DISTRIBUTIONS; STATISTICAL-INFERENCE	Learning of a smooth but nonparametric probability density can be regularized using methods of Quantum Field Theory. We implement a field theoretic prior numerically, test its efficacy, and show that the free parameter of the theory ('smoothness scale') can be determined self consistently by the data; this forms an infinite dimensional generalization of the MDL principle. Finally, we study the implications of one's choice of the prior and the parameterization and conclude that the smoothness scale determination makes density estimation very weakly sensitive to the choice of the prior, and that even wrong choices can be advantageous for small data sets.	Princeton Univ, Dept Phys, Princeton, NJ 08544 USA	Princeton University	Nemenman, I (corresponding author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.							Aida T, 1999, PHYS REV LETT, V83, P3554, DOI 10.1103/PhysRevLett.83.3554; Bialek W, 1996, PHYS REV LETT, V77, P4693, DOI 10.1103/PhysRevLett.77.4693; Holy TE, 1997, PHYS REV LETT, V79, P3545, DOI 10.1103/PhysRevLett.79.3545; MACKAY DJC, 1997, TUTORIAL LECT NOTES; Mackie NM, 1997, CHEM MATER, V9, P349, DOI 10.1021/cm960388q; Periwal V, 1997, PHYS REV LETT, V78, P4671, DOI 10.1103/PhysRevLett.78.4671; Periwal V, 1999, NUCL PHYS B, V554, P719, DOI 10.1016/S0550-3213(99)00278-3; Press WH, 1988, NUMERICAL RECIPES C; Rissanen Jorma, 1989, STOCHASTIC COMPLEXIT; Vapnik V.N, 1998, STAT LEARNING THEORY; Wahba G, 1999, ADVANCES IN KERNEL METHODS, P69	11	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						287	293						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800041
C	Penev, PS		Leen, TK; Dietterich, TG; Tresp, V		Penev, PS			Redundancy and dimensionality reduction in sparse-distributed representations of natural objects in terms of their local features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Low-dimensional representations are key to solving problems in high-level vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.	Rockefeller Univ, Lab Computat Neurosci, New York, NY 10021 USA	Rockefeller University	Penev, PS (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08550 USA.	penev@rockefeller.edu						ATICK JJ, 1992, NEURAL COMPUT, V4, P196, DOI 10.1162/neco.1992.4.2.196; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Barlow HB, 1989, NEURAL COMPUT, V1, P412, DOI 10.1162/neco.1989.1.3.412; JAYNES ET, 1982, P IEEE, V70, P939, DOI 10.1109/PROC.1982.12425; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Mel BW, 1999, NATURE, V401, P759, DOI 10.1038/44507; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Penev P. S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P264, DOI 10.1109/AFGR.2000.840645; Penev PS, 1996, NETWORK-COMP NEURAL, V7, P477, DOI 10.1088/0954-898X/7/3/002; PENEV PS, 1999, LOCAL FEATURE ANAL F; PENEV PS, 1998, THESIS ROCKEFELLER U; Poggio T, 1998, NEURAL COMPUT, V10, P1445, DOI 10.1162/089976698300017250; RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519; [No title captured]	18	2	2	0	6	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						901	907						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800127
C	Vicente, R; Saad, D; Kabashima, Y		Leen, TK; Dietterich, TG; Tresp, V		Vicente, R; Saad, D; Kabashima, Y			Error-correcting codes on a Bethe-like lattice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				BELIEF PROPAGATION; TURBO-CODES; SYSTEMS; MODELS; TAP	We analyze Gallager codes by employing a simple mean-field approximation that distorts the model geometry and preserves important interactions between sites. The method naturally recovers the probability propagation decoding algorithm as an extremization of a proper free-energy. We find a thermodynamic phase transition that coincides with information theoretical upper-bounds and explain the practical code performance in terms of the free-energy landscape.	Aston Univ, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Vicente, R (corresponding author), Aston Univ, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.		Vicente, Renato/B-2347-2008; Vicente, Renato/X-8205-2019	Vicente, Renato/0000-0003-0671-9895; Vicente, Renato/0000-0003-0671-9895; Saad, David/0000-0001-9821-2623				Berrou C, 1996, IEEE T COMMUN, V44, P1261, DOI 10.1109/26.539767; DERRIDA B, 1981, PHYS REV B, V24, P2613, DOI 10.1103/PhysRevB.24.2613; Frey BJ, 1998, IEEE J SEL AREA COMM, V16, P153, DOI 10.1109/49.661104; Frey BJ, 1998, ADV NEUR IN, V10, P479; Gallager RG, 1963, LOW DENSITY PARITY C; GUJRATI PD, 1995, PHYS REV LETT, V74, P809, DOI 10.1103/PhysRevLett.74.809; Kabashima Y, 1998, EUROPHYS LETT, V44, P668, DOI 10.1209/epl/i1998-00524-7; Kabashima Y, 2000, PHYS REV LETT, V84, P1355, DOI 10.1103/PhysRevLett.84.1355; Kanter I, 2000, J PHYS A-MATH GEN, V33, P1675, DOI 10.1088/0305-4470/33/8/311; MacKay DJC, 1999, IEEE T INFORM THEORY, V45, P399, DOI 10.1109/18.748992; Montanari A, 2000, EUR PHYS J B, V18, P107, DOI 10.1007/PL00011086; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; RIEGER H, 1992, PHYS REV B, V45, P9772, DOI 10.1103/PhysRevB.45.9772; Saul LK, 1996, ADV NEUR IN, V8, P486; SHERRINGTON D, 1987, J PHYS A-MATH GEN, V20, pL785, DOI 10.1088/0305-4470/20/12/007; TANAKA T, IN PRESS NEURAL COMP; Vicente R, 1999, PHYS REV E, V60, P5352, DOI 10.1103/PhysRevE.60.5352	18	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						322	328						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800046
C	Winther, O		Leen, TK; Dietterich, TG; Tresp, V		Winther, O			Computing with finite and infinite networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				GAUSSIAN-PROCESSES	Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires O(d(s)) -training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.	Lund Univ, S-22363 Lund, Sweden	Lund University	Winther, O (corresponding author), Lund Univ, Solvegatan 14 A, S-22363 Lund, Sweden.							Ahr M, 1999, EUR PHYS J B, V10, P583, DOI 10.1007/s100510050889; NEAL R, 1990, LECT NOTES STAT; Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881; Opper M, 1996, PHYS REV LETT, V76, P1964, DOI 10.1103/PhysRevLett.76.1964; Williams CKI, 1996, ADV NEUR IN, V8, P514; WILLIAMS CKI, 1997, NEURAL INFORMATION P, V9, P295	8	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						336	342						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800048
C	Zenger, B; Koch, C		Leen, TK; Dietterich, TG; Tresp, V		Zenger, B; Koch, C			Divisive and subtractive mask effects: Linking psychophysics and biophysics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MECHANISMS	We describe an analogy between psychophysically measured effects in contrast masking, and the behavior of a simple integrate-and-fire neuron that receives time-modulated inhibition. In the psychophysical experiments, we tested observers ability to discriminate contrasts of peripheral Gabor patches in the presence of collinear Gabor flankers. The data reveal a complex interaction pattern that we account for by assuming that flankers provide divisive inhibition to the target unit for low target contrasts, but provide subtractive inhibition to the target unit for higher target contrasts. A similar switch from divisive to subtractive inhibition is observed in an integrate-and-fire unit that receives inhibition modulated in time such that the cell spends part of the time in a high-inhibition state and part of the time in a low-inhibition state. The similarity between the effects suggests that one may cause the other. The biophysical model makes testable predictions for physiological single-cell recordings.	CALTECH, Div Biol, Pasadena, CA 91125 USA	California Institute of Technology	Zenger, B (corresponding author), CALTECH, Div Biol, 139-74, Pasadena, CA 91125 USA.			Koch, Christof/0000-0001-6482-8067				FOLEY JM, 1994, J OPT SOC AM A, V11, P1710, DOI 10.1364/JOSAA.11.001710; LEGGE GE, 1981, VISION RES, V21, P457, DOI 10.1016/0042-6989(81)90092-4; LEVITT H, 1971, J ACOUST SOC AM, V49, P467, DOI 10.1121/1.1912375; Levitt JB, 1997, NATURE, V387, P73, DOI 10.1038/387073a0; POLAT U, 1993, VISION RES, V33, P993, DOI 10.1016/0042-6989(93)90081-7; Polat U, 1998, NATURE, V391, P580, DOI 10.1038/35372; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Sengpiel F, 1998, VISION RES, V38, P2067, DOI 10.1016/S0042-6989(97)00413-6; SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011; Tyler CW, 1997, SPATIAL VISION, V10, P369, DOI 10.1163/156856897X00294	10	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						915	921						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800129
C	Flake, GW; Pearlmutter, BA		Solla, SA; Leen, TK; Muller, KR		Flake, GW; Pearlmutter, BA			Differentiating functions of the Jacobian with respect to the weights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					For many problems, the correct behavior of a model depends not only on its input-output mapping but also on properties of its Jacobian matrix, the matrix of partial derivatives of the model's outputs with respect to its inputs. We introduce the J-prop algorithm, an efficient general method for computing the exact partial derivatives of a variety of simple functions of the Jacobian of a model with respect to its free parameters. The algorithm applies to any parametrized feedforward model, including nonlinear regression, multilayer perceptrons, and radial basis function networks.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Flake, GW (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.		Pearlmutter, Barak A/M-8791-2014; Pearlmutter, Barak A./AAL-8999-2020	Pearlmutter, Barak A/0000-0003-0521-4553; 				DECO G, 1997, MAKING LEARNING SYST, V4, P137; DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3; FLAKE GW, 1999, UNPUB NIPS 99; FLAKE GW, 1999, UNPUB OPTIMIZING PRO; Hinton G, 1986, P 8 ANN C COGN SCI S, V1, DOI DOI 10.1016/J.NEUCOM.2013.03.009; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; PRINCIPE J, 1992, BIFURCATIONS CHAOS, V2; SIMARD P, 1992, ADV NEUR IN, V4, P895; WHITE H, 1992, ARTIFICIAL NEURAL NE, P206	9	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						435	441						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700062
C	Laskov, P		Solla, SA; Leen, TK; Muller, KR		Laskov, P			An improved decomposition algorithm for regression support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					A new decomposition algorithm for training regression Support, Vector Machines (SVM) is presented. The algorithm builds on the basic principles of decomposition proposed by Osuna et. al., and addresses the issue of optimal working set selection. The new criteria for testing optimality of a working set are derived. Based on these criteria, the principle of "maximal inconsistency" is proposed to form (approximately) optimal working sets. Experimental results show superior performance of the new algorithm in comparison with traditional training of regression SVM without decomposition. Similar results have been previously reported on decomposition algorithms for pattern recognition SVM. The new algorithm is also applicable to advanced SVM formulations based on regression, such as density estimation and integral equation SVM.	Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19718 USA	University of Delaware	Laskov, P (corresponding author), Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19718 USA.							Boot J.C.G., 1964, QUADRATIC PROGRAMMIN; Joachims T., 1998, MAKING LARGE SCALE S; KAUFMAN L, 1998, ADV KERNEL METHODS S; Osuna E., 1998, THESIS MIT; OSUNA E, 1997, P IEEE NNSP 97 AM IS; Platt J C, 1999, ADV KERNEL METHODS S; SMOLA A, 1998, NC2TR1998030 NEUR; Vapnik V., 1982, ESTIMATION DEPENDENC	8	2	7	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						484	490						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700069
C	Mansour, Y; McAllester, D		Solla, SA; Leen, TK; Muller, KR		Mansour, Y; McAllester, D			Boosting with multi-way branching in decision trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					It is known that decision tree learning can be viewed as a form of boosting. However, existing boosting theorems for decision tree learning allow only binary-branching trees and the generalisation to multi-branching trees is not immediate. Practical decision tree algorithms, such as CART and C4.5, implement a trade-off between the number of branches and the improvement in tree quality as measured by an index function. Here we give a boosting justification for a particular quantitative trade-off curve. Our main theorem states, in essence, that if we require an improvement proportional to the log of the number of branches then top-down greedy construction of decision trees remains an effective boosting algorithm.	AT&T Labs Res, Florham Park, NJ 07932 USA	AT&T	Mansour, Y (corresponding author), AT&T Labs Res, 180 Park Ave, Florham Park, NJ 07932 USA.							DIETTERICH T, 1996, P 13 INT C MACH LEAR, P96; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y., 1995, Computational Learning Theory. Second European Conference, EuroCOLT '95. Proceedings, P23; Freund Y, 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.5555/3091696.3091715; Kearns M., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P459, DOI 10.1145/237814.237994; Olshen R., 1984, CLASSIFICATION REGRE; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309; SCHAEFER P, 1990, EARTH ISL J, V5, P2	8	2	2	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						300	306						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700043
C	Ng, AY; Jordan, MI		Solla, SA; Leen, TK; Muller, KR		Ng, AY; Jordan, MI			Approximate inference algorithms for two-layer Bayesian networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We present a class of approximate inference algorithms for graphical models of the QMR-DT type. We give convergence rates for these algorithms and for the Jaakkola and Jordan (1999) algorithm, and verify these theoretical predictions empirically. We also present empirical results on the difficult QMR-DT network problem, obtaining performance of the new algorithms roughly comparable to the Jaakkola and Jordan algorithm.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ng, AY (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					Barber D, 1999, J ARTIF INTELL RES, V10, P435, DOI 10.1613/jair.567; Heckerman D., 1989, TRACTABLE INFERENCE; Jaakkola TS, 1999, J ARTIF INTELL RES, V10, P291, DOI 10.1613/jair.583; JORDAN M, 1998, LEARNING GRAPHICAL M; KEARNS M, 1998, P 14 C UNC ART INT; Murphy K. P., 1999, P 15 C UNC ART INT; PLEFKA T, 1982, J PHYS A, V15; SHWE MA, 1991, METHOD INFORM MED, V30, P241, DOI 10.1055/s-0038-1634846	8	2	2	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						533	539						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700076
C	Poirazi, P; Mel, BW		Solla, SA; Leen, TK; Muller, KR		Poirazi, P; Mel, BW			Memory capacity of linear vs. nonlinear models of dendritic integration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Previous biophysical modeling work showed that nonlinear interactions among nearby synapses located on active dendritic trees can provide a large boost in the memory capacity of a cell (Mel, 1992a, 1992b). The aim of our present work is to quantify this boost by estimating the capacity of (1) a neuron model with passive dendritic integration where inputs are combined linearly across the entire cell followed by a single global threshold, and (2) an active dendrite model in which a threshold is applied separately to the output of each branch, and the branch subtotals are combined linearly. We focus here on the limiting case of binary-valued synaptic weights, and derive expressions which measure model capacity by estimating the number of distinct input-output functions available to both neuron types. We show that (1) the application of a fixed nonlinearity to each dendritic compartment substantially increases the model's flexibility, (2) for a neuron of realistic size, the capacity of the nonlinear cell can exceed that of the same-sized linear cell by more than an order of magnitude, and (3) the largest capacity boost occurs for cells with a relatively large number of dendritic subunits of relatively small size. We validated the analysis by empirically measuring memory capacity with randomized two-class classification problems, where a stochastic delta rule was used to train both linear and nonlinear models. We found that large capacity boosts predicted for the nonlinear dendritic model were readily achieved in practice.	Univ So Calif, Dept Biomed Engn, Los Angeles, CA 90089 USA	University of Southern California	Poirazi, P (corresponding author), Univ So Calif, Dept Biomed Engn, Los Angeles, CA 90089 USA.			Poirazi, Panayiota/0000-0001-6152-595X				MEL BW, 1992, ADV NEUR IN, V4, P35; MEL BW, 1992, NEURAL COMPUT, V4, P502, DOI 10.1162/neco.1992.4.4.502; Petersen CCH, 1998, P NATL ACAD SCI USA, V95, P4732, DOI 10.1073/pnas.95.8.4732; POIRAZI P, 1999, IN PRESS NEURAL COMP	4	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						157	163						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700023
C	Risau-Gusman, S; Gordon, MB		Solla, SA; Leen, TK; Muller, KR		Risau-Gusman, S; Gordon, MB			Understanding stepwise generalization of Support Vector Machines: a toy model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In this article we study the effects of introducing structure in the input distribution of the data to be learnt by a simple perceptron. We determine the learning curves within the framework of Statistical Mechanics. Stepwise generalization occurs as a function of the number of examples when the distribution of patterns is highly anisotropic. Although extremely simple, the model seems to capture the relevant features of a class of Support Vector Machines which was recently shown to present this behavior.	CEA, SPSMS, DRFMC, F-38054 Grenoble 09, France	CEA; Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA)	Risau-Gusman, S (corresponding author), CEA, SPSMS, DRFMC, 17 Av Martyrs, F-38054 Grenoble 09, France.							BUHOT A, 1998, CONDMAT9802179; BUHOT A, 1999, ESANN 99 EUR S ART N, P201; GYORGYI G, 1990, STATISTICAL THEORY L, P3; MARANGI C, 1995, EUROPHYS LETT, V30, P117, DOI 10.1209/0295-5075/30/2/010; MEIR R, 1995, NEURAL COMPUT, V7, P144, DOI 10.1162/neco.1995.7.1.144; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Yoon H, 1998, J PHYS A-MATH GEN, V31, P7771, DOI 10.1088/0305-4470/31/38/012	8	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						321	327						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700046
C	Sugiyama, M; Ogawa, H		Solla, SA; Leen, TK; Muller, KR		Sugiyama, M; Ogawa, H			Training data selection for optimal generalization in trigonometric polynomial networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In this paper, we consider the problem of active learning in trigonometric polynomial networks and give a necessary and sufficient condition of sample points to provide the optimal generalization capability. By analyzing the condition from the functional analytic point of view, we clarify the mechanism of achieving the optimal generalization capability. We also show that a set of training examples satisfying the condition does not only provide the optimal generalization but also reduces the computational complexity and memory required for the calculation of learning results. Finally examples of sample points satisfying the condition are given and computer simulations are performed to demonstrate the effectiveness of the proposed active learning method.	Tokyo Inst Technol, Dept Comp Sci, Meguro Ku, Tokyo 1528552, Japan	Tokyo Institute of Technology	Sugiyama, M (corresponding author), Tokyo Inst Technol, Dept Comp Sci, Meguro Ku, 2-12-1 O-okayama, Tokyo 1528552, Japan.	sugi@cs.titech.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743				ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Cohn D.A., 1994, ADV NEURAL INFORM PR, V6, P679, DOI DOI 10.1016/j.jspi.2009.08.006; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; Fukumizu K, 1996, ADV NEUR IN, V8, P295; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Ogawa H., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V808, P189; Ogawa H., 1998, RIMS KOKYUROKU, P24; OGAWA H, 1992, P ICIIPS 92 INT C IN, V2, P1; SUGIYAMA M, 1999, NC9956 IEICE, P15; SUGIYAMA M, 1999, P 1999 WORKSH INF BA, P93	10	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						624	630						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700089
C	Tino, P; Dorffner, G		Solla, SA; Leen, TK; Muller, KR		Tino, P; Dorffner, G			Building predictive models from fractal representations of symbolic sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				UNIVERSAL	We propose a novel approach for building finite memory predictive models similar in spirit to variable memory length Markov models (VLMMs). The models are constructed by first transforming the n-block structure of the training sequence into a spatial structure of points in a unit hypercube, such that the longer is the common suffix shared by any two n-blocks, the closer lie their point representations. Such a transformation embodies a Markov assumption - n-blocks with long common suffixes are likely to produce similar continuations. Finding a set of prediction contexts is formulated as a resource allocation problem solved by vector quantizing the spatial n-block representation. We compare our model with both the classical and variable memory length Markov models on three data sets with different memory and stochastic components. Our models have a superior performance, yet, their construction is fully automatic, which is shown to be problematic in the case of VLMMs.	Austrian Res Inst Artificial Intelligence, A-1010 Vienna, Austria		Tino, P (corresponding author), Austrian Res Inst Artificial Intelligence, Schottengasse 3, A-1010 Vienna, Austria.		Dorffner, Georg/AAQ-1455-2020; Tino, Peter/Z-5748-2019	Dorffner, Georg/0000-0002-3181-2576; Tino, Peter/0000-0003-2330-128X				BUHLMANN P, 1999, IN PRESS ANN I STAT; Freund J, 1996, PHYS REV E, V54, P5561, DOI 10.1103/PhysRevE.54.5561; RON D, 1994, ADV NEURAL INFORMATI, V6, P176; RON D, 1996, MACH LEARN, P25; Tino P, 1999, IEEE T SYST MAN CY A, V29, P386, DOI 10.1109/3468.769757; WEINBERGER MJ, 1995, IEEE T INFORM THEORY, V41, P643, DOI 10.1109/18.382011; [No title captured]	7	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						645	651						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700092
C	Tonkes, B; Blair, A; Wiles, J		Solla, SA; Leen, TK; Muller, KR		Tonkes, B; Blair, A; Wiles, J			Evolving learnable languages	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Recent theories suggest that language acquisition is assisted by the evolution of languages towards forms that are easily learnable. In this paper, we evolve combinatorial languages which can be learned by a recurrent neural network quickly and from relatively few examples. Additionally we evolve languages for generalization in different "worlds", and for generalization from specific examples. We find that languages can be evolved to facilitate different forms of impressive generalization for a minimally biased, general purpose learner. The results provide empirical support for the theory that the language itself as well as the language environment of a learner, plays a substantial role in learning: that there is far more to language acquisition than the language acquisition device.	Univ Queensland, Dept Comp Sci & Elec Engn, Brisbane, Qld 4072, Australia	University of Queensland	Tonkes, B (corresponding author), Univ Queensland, Dept Comp Sci & Elec Engn, Brisbane, Qld 4072, Australia.		Wiles, Janet/C-1989-2008	Wiles, Janet/0000-0002-4051-4116				Batali J., 1998, APPROACHES EVOLUTION, P405; BATALI J, 1994, P 4 ART LIF WORKSH, P160; Chomsky N., 1968, LANGUAGE MIND; CHRISTIANSEN M, 1995, UNPUB LANGUAGE ORGAN; Deacon T., 1997, SYMBOLIC SPECIES COE; Elman J., 1996, RETHINKING INNATENES; ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4; HARE M, 1995, COGNITION, V56, P61, DOI 10.1016/0010-0277(94)00655-5; KIRBY S, 1998, APPROACHES EVOLUTION; KIRBY S, 1999, EVOLUTIONARY EMERGEN; TONKES B, 1999, LECT NOTES ARTIFICIA, V1585	11	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						66	72						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700010
C	Attias, H		Kearns, MS; Solla, SA; Cohn, DA		Attias, H			Learning a hierarchical belief network of independent factor analyzers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BLIND SOURCE SEPARATION; MAXIMUM-LIKELIHOOD; DECONVOLUTION	Many belief networks have been proposed that are composed of binary units. However, for tasks such as object and speech recognition which produce real-valued data, binary network models are usually inadequate. Independent component analysis (ICA) learns a model from real data, but the descriptive power of this model is severly limited. We begin by describing the independent factor analysis (IFA) technique, which overcomes some of the limitations of ICA. We then create a multilayer network by cascading singlelayer IFA models. At each level, the IFA network extracts real-valued latent variables that are non-linear functions of the input data with a highly adaptive functional form, resulting in a hierarchical distributed representation of these data. Whereas exact maximum-likelihood learning of the network is intractable, we derive an algorithm that maximizes a lower bound on the likelihood, based on a variational approach.	Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Attias, H (corresponding author), Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, Box 0444, San Francisco, CA 94143 USA.	hagai@gatsby.ucl.ac.uk						Attias H, 1998, NEURAL COMPUT, V10, P1373, DOI 10.1162/neco.1998.10.6.1373; ATTIAS H, 1999, IN PRESS NEURAL COMP; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Cardoso JF, 1997, IEEE SIGNAL PROC LET, V4, P112, DOI 10.1109/97.566704; FREY BJ, 1997, ADV NEURAL INFORMATI, V9; FREY BJ, 1999, IN PRESS NEURAL COMP; GHAHRAMANI Z, 1998, ADV NEURAL INFORMATI, V10; LEWICKI MS, 1998, ADV NEURAL INFORMATI, V10; Neal R. M., 1998, LEARNING GRAPHICAL M; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearlmutter BA, 1997, ADV NEUR IN, V9, P613; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251	12	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						361	367						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700051
C	Cornford, D; Nabney, IT; Williams, CKI		Kearns, MS; Solla, SA; Cohn, DA		Cornford, D; Nabney, IT; Williams, CKI			Adding constrained discontinuities to Gaussian process models of wind fields	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Gaussian Processes provide good prior models for spatial data, but can be too smooth. In many physical situations there are discontinuities along bounding surfaces, for example fronts in near-surface wind fields. We describe a modelling method for such a constrained discontinuity and demonstrate how to infer the model parameters in wind fields with MCMC sampling.	Aston Univ, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Cornford, D (corresponding author), Aston Univ, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.			Cornford, Dan/0000-0001-8787-6758; Nabney, Ian/0000-0001-7382-2855; Nabney, Ian T/0000-0003-1513-993X				CORNFORD D, 1998, NCRG98017 AST U; Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683; Cressie N., 2011, STAT SPATIO TEMPORAL; Daley R., 1991, ATMOSPHERIC DATA ANA, P457; HANDCOCK MS, 1994, J AM STAT ASSOC, V89, P368, DOI 10.2307/2290832; Neal R. M., 1993, PROBABILISTIC INFERE	6	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						861	867						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700121
C	Grzeszczuk, R; Terzopoulos, D; Hinton, G		Kearns, MS; Solla, SA; Cohn, DA		Grzeszczuk, R; Terzopoulos, D; Hinton, G			Fast neural network emulation of dynamical systems for computer animation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism, but it can he computationally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient "NeuroAnimator" that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physics-based models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. We demonstrate NeuroAnimators fur a variety of physics-based models.	Intel Corp, Microcomp Res Lab, Santa Clara, CA 95052 USA	Intel Corporation	Grzeszczuk, R (corresponding author), Intel Corp, Microcomp Res Lab, 2200 Miss Coll Blvd, Santa Clara, CA 95052 USA.							[Anonymous], 1987, ACM SIGGRAPH COMPUTE, DOI [10.1145/37402.37427, DOI 10.1145/37402.37427]; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; GRZESZCZUK R, 1998, NEUROAMIMATOR FAST N; GRZESZCZUK R, 1998, P ACM SIGGRAPH 98 C; Hahn J. K., 1988, Computer Graphics, V22, P299, DOI 10.1145/378456.378530; HODGINS JK, 1995, P SIGGRAPH 95, P71; JORDAN ML, 1988, 8827 U MASS; NARENDRA KS, 1991, IEEE T NEURAL NETWOR, V2, P252, DOI 10.1109/72.80336; NGUYEN D, 1989, P INT JOINT C NEUR N, P357, DOI DOI 10.1109/IJCNN.1989.118723; Tu X., 1994, P SIGGRAPH 94, P43, DOI DOI 10.1145/192161.192170	10	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						882	888						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700124
C	Harris, JG; Pu, CJ; Principe, JC		Kearns, MS; Solla, SA; Cohn, DA		Harris, JG; Pu, CJ; Principe, JC			A neuromorphic monaural sound localizer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We describe the first single microphone sound localization system and its inspiration from theories of human monaural sound localization. Reflections and diffractions caused by the external ear (pinna) allow humans to estimate sound source elevations using only one ear. Our single microphone localization model relies on a specially shaped reflecting structure that serves the role of the pinna. Specially designed analog VLSI circuitry uses echo-time processing to localize the sound. A CMOS integrated circuit has been designed, fabricated, and successfully demonstrated on actual sounds.	Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA	State University System of Florida; University of Florida	Harris, JG (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.		principe, jose/N-8099-2014					BATTEAU DW, 1967, PROC R SOC SER B-BIO, V168, P158, DOI 10.1098/rspb.1967.0058; CHIANGJUNG P, 1998, THESIS U FLORIDA GAI; *EARTHW INC, M30 MICR; HIRANAKA Y, 1983, J ACOUST SOC AM, V73, P29; KNUDSEN EI, 1979, J COMP PHYSIOL, V133, P13, DOI 10.1007/BF00663106; Lazzaro J, 1989, NEURAL COMPUT, V1, P47, DOI 10.1162/neco.1989.1.1.47; LICKLIDER JCR, 1951, EXPERIENTIA, V7, P128, DOI 10.1007/BF02156143; Mead, 1989, ANALOG VLSI NEURAL S; NEAL A, 1994, P ICNN, P1866; SMITH LS, 1994, J NEW MUSIC RES, P23; WATKINS AJ, 1978, J ACOUST SOC AM, V63, P1152, DOI 10.1121/1.381823	11	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						692	698						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700098
C	Kabashima, Y; Saad, D		Kearns, MS; Solla, SA; Cohn, DA		Kabashima, Y; Saad, D			The belief in TAP	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				ERROR-CORRECTING CODES; SOLVABLE MODEL; SPIN-GLASS	We show the similarity between belief propagation and TAP, for decoding corrupted messages encoded by Sourlas's method. The latter is a special case of the Gallager error-correcting code, where the code word comprises products of Ii bits selected randomly from the original message. We examine the efficacy of solutions obtained by the two methods for Various values of K and show that solutions for K greater than or equal to 3 may be sensitive to the choice of initial conditions in the case of unbiased patterns. Good approximations are obtained generally for K = 2 and for biased patterns in the case of K greater than or equal to 3, especially when Nishimori's temperature is being used.	Tokyo Inst Technol, Dept Compt Int & Syst Sci, Yokohama, Kanagawa 226, Japan	Tokyo Institute of Technology	Kabashima, Y (corresponding author), Tokyo Inst Technol, Dept Compt Int & Syst Sci, Yokohama, Kanagawa 226, Japan.			Saad, David/0000-0001-9821-2623				BETHE H, 1935, P ROY SOC LOND A MAT, V151, P540; DERRIDA B, 1981, PHYS REV B, V24, P2613, DOI 10.1103/PhysRevB.24.2613; GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683; KABASHIMA Y, 1999, IN PRESS EUROPHYS LE, V45; MacKay DJC, 1997, ELECTRON LETT, V33, P457, DOI 10.1049/el:19970362; NISHIMORI H, 1980, J PHYS C SOLID STATE, V13, P4071, DOI 10.1088/0022-3719/13/21/012; NISHIMORI H, 1993, J PHYS SOC JPN, V62, P1169; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; RUJAN P, 1993, PHYS REV LETT, V70, P2968, DOI 10.1103/PhysRevLett.70.2968; SHERRINGTON D, 1975, PHYS REV LETT, V35, P1792, DOI 10.1103/PhysRevLett.35.1792; SHERRINGTON D, 1987, J PHYS A-MATH GEN, V20, pL785, DOI 10.1088/0305-4470/20/12/007; SOURLAS N, 1994, EUROPHYS LETT, V25, P159, DOI 10.1209/0295-5075/25/3/001; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; WEISS Y, 1997, CBCL155; [No title captured]	15	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						246	252						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700035
C	Manwani, A; Koch, C		Kearns, MS; Solla, SA; Cohn, DA		Manwani, A; Koch, C			Signal detection in noisy weakly-active dendrites	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Here we derive measures quantifying the information loss of a synaptic signal due to the presence of neuronal noise sources, as it electrotonically propagates along a weakly-active dendrite. We model the dendrite as an infinite linear cable, with noise sources distributed along its length. The noise sources we consider are thermal noise, channel noise arising from the stochastic nature of voltage-dependent ionic channels (K+ and Na+) and synaptic noise due to spontaneous background activity. We assess the efficacy of information transfer using a signal detection paradigm where the objective is to detect the presence/absence of a presynaptic spike from the post-synaptic membrane voltage. This allows us to analytically assess the role of each of these noise sources in information transfer. For our choice of parameters, we find that the synaptic noise is the dominant noise source which limits the maximum length over which information be reliably transmitted.	CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA	California Institute of Technology	Manwani, A (corresponding author), CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA.			Koch, Christof/0000-0001-6482-8067				Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DEFELICE LJ, 1981, MEMBRANE NOISE; Koch C., 1998, BIOPHYSICS COMPUTATI; MAINEN ZF, 1998, METHODS NEURONAL MOD; MANWANI A, 1998, SOC NEUR ABSTR; MANWANI A, 1998, ADV NEURAL INFORMATI; Papoulis A., 1991, COMMUNICATIONS SIGNA, V3; TUCKWELL HC, 1983, BIOL CYBERN, V49, P99, DOI 10.1007/BF00320390; TUCKWELL HC, 1988, INTRO THEORETICAL NE, P1; YUSTE R, 1996, DENDRITIC INTEGRATIO	10	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						132	138						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700019
C	Nix, DA; Hogden, JE		Kearns, MS; Solla, SA; Cohn, DA		Nix, DA; Hogden, JE			Maximum-Likelihood Continuity Mapping (MALCOM): An alternative to HMMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				SPEECH	We describe Maximum-Likelihood Continuity Mapping (MALCOM), an alternative to hidden Markov models (HMMs) for processing sequence data such as speech. While HMMs have a discrete "hidden" space constrained by a fixed finite-automaton architecture, MALCOM has a continuous hidden space-a continuity map-that is constrained only by a smoothness requirement on paths through the space. MALCOM fits into the same probabilistic framework for speech recognition as HMMs, but it represents a more realistic model of the speech production process. To evaluate the extent to which MALCOM captures speech production information, we generated continuous speech continuity maps for three speakers and used the paths through them to predict measured speech articulator data. The median correlation between the MALCOM paths obtained from only the speech acoustics and articulator measurements was 0.77 on an independent test set not used to train MALCOM or the predictor. This unsupervised model achieved correlations over speakers and articulators only 0.02 to 0.15 lower than those obtained using an analogous supervised method which used articulatory measurements as well as acoustics.	Univ Calif Los Alamos Natl Lab, Los Alamos, NM 87545 USA	United States Department of Energy (DOE); Los Alamos National Laboratory	Nix, DA (corresponding author), Univ Calif Los Alamos Natl Lab, CIC-3,MS B265, Los Alamos, NM 87545 USA.							Bishop, 1995, NEURAL NETWORKS PATT; BOURLARD H, 1995, TR94064 INT COMP SCI; HOGDEN J, 1998, UNPUB AC SOC AM C; HOGDEN J, 1995, LAUR963945 LOS AL NA; HOGDEN J, 1997, LAUR97992 LOS AL NAT; MORGAN DP, 1992, NEURAL NETWORKS SPEE; MORGAN N, 1995, P IEEE, V83, P742, DOI 10.1109/5.381844; NIX DA, 1998, THESIS U CO BOULDER; PERKELL JS, 1992, J ACOUST SOC AM, V92, P3078, DOI 10.1121/1.404204; Press WH, 1988, NUMERICAL RECIPES C; YOUNG S, 1996, IEEE SIGNAL PROC SEP, P45	11	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						744	750						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700105
C	Oliver, NM; Rosario, B; Pentland, A		Kearns, MS; Solla, SA; Cohn, DA		Oliver, NM; Rosario, B; Pentland, A			Graphical models for recognizing human interactions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We describe a real-time computer vision and machine learning system for modeling and recognizing human actions and interactions. Two different domains are explored: recognition of two-handed motions in the martial art 'Tai Chi', and multiple-person interactions in a visual surveillance task. Our system combines top-down with bottom-up information using a feedback loop, and is formulated with a Bayesian framework. Two different graphical models (HMMs and Coupled HMMs) are used for modeling both individual actions and multiple-agent interactions, and CHMMs are shown to work more efficiently and accurately for a given amount of training. Finally, to overcome the limited amounts of training data, we demonstrate that 'synthetic agents' (Alife-style agents) can be used to develop flexible prior models of the person-to-person interactions.	MIT, Media Arts & Sci Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Oliver, NM (corresponding author), MIT, Media Arts & Sci Lab, 20 Ames St,E15-384C, Cambridge, MA 02139 USA.		Oliver, Nuria/AAK-6995-2021	Oliver, Nuria/0000-0001-5985-691X				AZARBAYEJANI A, 1996, P INT C PATT REC VIE; BRAND M, 1996, UNPUB NEURAL COM NOV; BRAND M, 1996, P IEEE CVPR 97; GHAHRAMANI Z, 1996, NIPS, V8; OLIVER N, 1998, IN PRESS P CVPR 98 P; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; SAUL LK, 1995, NIPS, V7; SMYTH P, 1996, 1565 MIT; Williams C., 1991, P CONN MOD, P18	9	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						924	930						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700130
C	Prank, K; Borger, J; von zur Muhlen, A; Brabant, G; Schofl, C		Kearns, MS; Solla, SA; Cohn, DA		Prank, K; Borger, J; von zur Muhlen, A; Brabant, G; Schofl, C			Independent Component Analysis of intracellular calcium spike data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BLIND SEPARATION; OSCILLATIONS; CHANNELS	Calcium (Ca2+)is an ubiquitous intracellular messenger which regulates cellular processes, such as secretion, contraction, and cell proliferation. A number of different cell types respond to hormonal stimuli with periodic oscillations of the intracellular free calcium concentration ([Ca2+](i)). These Ca2+ signals are often organized in complex temporal and spatial patterns even under conditions of sustained stimulation. Here we study the spatio-temporal aspects of intracellular calcium ([Ca2+](i)) oscillations in clonal beta-cells (hamster insulin secreting cells, HIT) under pharmacological stimulation (Schofl et al., 1996). We use a novel fast fixed-point algorithm (Hyvarinen and Oja, 1997) for Independent Component Analysis (ICA) to blind source separation of the spatio-temporal dynamics of [Ca2+](i) in a HIT-cell. Using this approach we find two significant independent components out of five differently mixed input signals: one [Ca2+](i) signal with a mean oscillatory period of 68s and a high frequency signal with a broadband power spectrum with considerable spectral density. This results is in good agreement with a study on high-frequency [Ca2+](i) oscillations (Palus et al., 1998) Further theoretical and experimental studies have to be performed to resolve the question on the functional impact of intracellular signaling of these independent [Ca2+](i) signals.	Hannover Med Sch, Dept Clin Endocrinol, D-30625 Hannover, Germany	Hannover Medical School	Prank, K (corresponding author), Hannover Med Sch, Dept Clin Endocrinol, D-30625 Hannover, Germany.							ARMARI S, 1996, ADV NEURAL INFORMATI, V8, P757; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; BERRIDGE MJ, 1988, FASEB J, V2, P3074, DOI 10.1096/fasebj.2.15.2847949; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; DELFOSSE N, 1995, SIGNAL PROCESS, V45, P59, DOI 10.1016/0165-1684(95)00042-C; Dolmetsch RE, 1998, NATURE, V392, P933, DOI 10.1038/31960; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; Hyvarinen A., 1996, P IEEE INT C NEUR NE, P62; JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X; MOUREAU E, 1993, P IEEE SIGN PROC WOR, P215; Oja E., 1996, COMPUTATIONAL INTELL, P83; PALUS M, 1998, PAC S BIOC, V1998, P645; Prank K, 1998, EUROPHYS LETT, V42, P143, DOI 10.1209/epl/i1998-00220-2; Schofl C, 1996, CELL CALCIUM, V19, P485, DOI 10.1016/S0143-4160(96)90057-3; TSIEN RW, 1990, ANNU REV CELL BIOL, V6, P715, DOI 10.1146/annurev.cb.06.110190.003435; [No title captured]	16	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						931	937						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700131
C	Randlov, J		Kearns, MS; Solla, SA; Cohn, DA		Randlov, J			Learning macro-actions in reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We present a method for automatically constructing macro-actions from scratch from primitive actions during the reinforcement learning process. The overall idea is to reinforce the tendency to perform action b after action a if such a pattern of actions has been rewarded. We test the method on a bicycle task, the car-on-the-hill task, the race-track task and some grid-world tasks. For the bicycle and race-track tasks the use of macro-actions approximately halves the learning time, while for one of the grid-world tasks the learning time is reduced by a factor of 5. The method did not work for the car-on-the-hill task for reasons we discuss in the conclusion.	Univ Copenhagen, Niels Bohr Inst, DK-2100 Copenhagen O, Denmark	University of Copenhagen; Niels Bohr Institute	Randlov, J (corresponding author), Univ Copenhagen, Niels Bohr Inst, Blegdamsvej 17, DK-2100 Copenhagen O, Denmark.							ANDREAE JH, 1969, INT J MAN MACH STUD, V1, P1, DOI 10.1016/S0020-7373(69)80008-8; [Anonymous], 2020, REINFORCEMENT LEARNI; BARTO AG, 1995, ARTIF INTELL, V72, P81, DOI 10.1016/0004-3702(94)00011-O; BOYAN JA, 1995, NIPS, V7, P369; Burgard W., 1998, 15 NAT C ART INT; GULLAPALLI V, 1992, 9210 COINS; HANSEN E, 1997, NIPS, V9; HAUSKRECHT M, 1998, P 14 INT C UNC ART I; IBA GA, 1989, MACHINE LEARNING, V3; KORF RE, 1985, ARTIF INTELL, V26, P35, DOI 10.1016/0004-3702(85)90012-8; KORF RE, 1985, RES NOTES ARTIFICIAL, V5; MCCALLUM RA, 1995, THESIS U ROCHESTER; MCGOVERN A, 1997, 1997 G HOPPER CELEBR; MCGOVERN A, 1998, 9870 U MASS; PRECUP D, 1998, NIPS, V10; Randlov J., 1998, INT C MACH LEARN ICM; Rummery G., 1994, ON LINE Q LEARNING U; RUMMERY GA, 1995, THESIS CAMBRIDGE U E; Singh SP, 1996, MACH LEARN, V22, P123, DOI 10.1007/BF00114726; Sutton RS, 1996, ADV NEUR IN, V8, P1038; SUTTON RS, 1998, UMCS1998074; SUTTON RS, 1999, NIPS, V11	22	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1045	1051						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700147
C	Spence, CD; Sajda, P		Kearns, MS; Solla, SA; Cohn, DA		Spence, CD; Sajda, P			Applications of multi-resolution neural networks to mammography	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We have previously presented a coarse-to-fine hierarchical pyramid/neural network (HPNN) architecture which combines multiscale image processing techniques with neural networks, ill this paper we present applications of this general architecture to two problems in mammographic Computer-Aided Diagnosis (CAD). The first application is the detection of microcalcifications. The coarse-to-fine HPNN was designed to learn large-scale context information for detecting small objects like microcalcifications. Receiver operating characteristic (ROC) analysis suggests that the hierarchical architecture imploves detection performance of a well established CAD system by roughly 50 %. The second application is to detect mammographic masses directly. Since masses are large, extended objects, the coarse-to-fine HPNN architecture is not suitable for this problem, Instead we construct a fine-to-coarse HPNN architecture which is designed to learn small-scale detail structure associated with the extended objects, Our initial results applying the fine-to-coarse HPNN to mass detection are encouraging, with detection performance improvements of about 36 %, We conclude that the ability of the HPNN architecture tli integrate: information across scales, both coarse-to-fine and fine-to-coarse, makes it well suited for detecting objects which may have contextual clues or detail structure occurring at; scales other than the natural scale of the object.	Sarnoff Corp, Princeton, NJ 08543 USA	Sarnoff Corporation	Spence, CD (corresponding author), Sarnoff Corp, CN5300, Princeton, NJ 08543 USA.							BURT P, 1996, NEURO VISION SYSTEMS; BURT PJ, 1988, P IEEE, V76, P1006, DOI 10.1109/5.5971; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; KEELER J, 1991, ADV NEURAL INFORMATI, V3, P557; LECUN Y, 1991, ADV NEURAL INFORMATI, V2, P396; METZ C, 1988, P CHEST IM C MAD WI, P315; NISHIKAWA RM, 1995, P SOC PHOTO-OPT INS, V2434, P65, DOI 10.1117/12.208732; SPENCE CD, 1994, ADV NEURAL INFORMATI, V7, P981; ZHANG W, 1994, MED PHYS, V21, P517, DOI 10.1118/1.597177	9	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						938	944						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700132
C	Yuille, AL; Coughlan, JM		Kearns, MS; Solla, SA; Cohn, DA		Yuille, AL; Coughlan, JM			Convergence rates of algorithms for visual search: Detecting visual contours	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					This paper formulates the problem of visual search as Bayesian inference and defines a Bayesian ensemble of problem instances. In particular, we address the problem of the detection of visual contours in noise/clutter by optimizing a global criterion which combines local intensity and geometry information. We analyze the convergence rates of A* search algorithms using results from information theory to bound the probability of rare events within the Bayesian ensemble. This analysis determines characteristics of the domain, which we call order parameters, that determine the convergence rates. In particular, we present a specific admissible A* algorithm with pruning which converges, with high probability, with expected time O(N) in the size of the problem. In addition, we briefly summarize extensions of this work which address fundamental limits of target contour detectability (i.e. algorithm independent results) and the use of non-admissible heuristics.	Smith Kettlewell Inst, San Francisco, CA 94115 USA	The Smith-Kettlewell Eye Research Institute	Yuille, AL (corresponding author), Smith Kettlewell Inst, San Francisco, CA 94115 USA.			Yuille, Alan L./0000-0001-5207-9249				COUGHLAN JM, 1998, UNPUB ARTIFICIAL INT; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Geman D, 1996, IEEE T PATTERN ANAL, V18, P1, DOI 10.1109/34.476006; Pearl Judea, 1984, HEURISTICS; YUILE AL, 1998, UNPUB PATTERN ANAL M; YUILLE AL, 1997, LECT NOTES COMPUTER, V1223; ZHU SC, UNPUB IEEE COMP SOC	7	2	2	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						641	647						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700091
C	Cross, ADJ; Hancock, ER		Jordan, MI; Kearns, MJ; Solla, SA		Cross, ADJ; Hancock, ER			Recovering perspective pose with a dual step EM algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper describes a new approach to extracting 3D perspective structure from 2D point-sets. The novel feature is to unify the tasks of estimating transformation geometry and identifying point-correspondence matches. Unification is realised by constructing a mixture model over the bi-partite graph representing the correspondence match and by effecting optimisation using the EM algorithm. According to our EM framework the probabilities of structural correspondence gate contributions to the expected likelihood function used to estimate maximum likelihood perspective pose parameters. This provides a means of rejecting structural outliers.	Univ York, Dept Comp Sci, York YO1 5DD, N Yorkshire, England	University of York - UK	Cross, ADJ (corresponding author), Univ York, Dept Comp Sci, York YO1 5DD, N Yorkshire, England.		Hancock, Edwin/N-7548-2019; Hancock, Edwin R/C-6071-2008	Hancock, Edwin/0000-0003-4496-2028; Hancock, Edwin R/0000-0003-4496-2028					0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						780	786						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700110
C	de Sa, VR; deCharms, RC; Merzenich, MM		Jordan, MI; Kearns, MJ; Solla, SA		de Sa, VR; deCharms, RC; Merzenich, MM			Using Helmholtz Machines to analyze multi-channel neuronal recordings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					One of the current challenges to understanding neural information processing in biological systems is to decipher the "code" carried by large populations of neurons acting in parallel. We present an algorithm for automated discovery of stochastic firing patterns in large ensembles of neurons. The algorithm, from the "Helmholtz Machine" family, attempts to predict the observed spike patterns in the data. The model consists of an observable layer which is directly activated by the input spike patterns, and hidden units that are activated through ascending connections from the input layer. The hidden unit activity can be propagated down to the observable layer to create a prediction of the data pattern that produced it. Hidden units are added incrementally and their weights are adjusted to improve the fit between the predictions and data, that is, to increase a bound on the probability of the data given the model. This greedy strategy is not globally optimal but is computationally tractable for large populations of neurons. We show benchmark data on artificially constructed spike trains and promising early results on neurophysiological data collected from our chronic multi-electrode cortical implant.	Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	de Sa, VR (corresponding author), Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, San Francisco, CA 94143 USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						131	137						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700019
C	Dow, ER; Anastasio, TJ		Jordan, MI; Kearns, MJ; Solla, SA		Dow, ER; Anastasio, TJ			Instabilities in eye movement control: A model of periodic alternating nystagmus	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Nystagmus is a pattern of eye movement characterized by smooth rotations of the eye in one direction and rapid rotations in the opposite direction that reset eye position. Periodic alternating nystagmus (PAN) is a form of uncontrollable nystagmus that has been described as an unstable but amplitude-limited oscillation. PAN has been observed previously only in subjects with vestibulo-cerebellar damage. We describe results in which PAN can be produced in normal subjects by prolonged rotation in darkness. We propose a new model in which the neural circuits that control eye movement are inherently unstable, but this instability is kept in check under normal circumstances by the cerebellum. Circumstances which alter this cerebellar restraint, such as vestibulocerebellar damage or plasticity due to rotation in darkness, can lead to PAN.	Univ Illinois, Beckman Inst, Ctr Biophys & Computat Biol, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Dow, ER (corresponding author), Univ Illinois, Beckman Inst, Ctr Biophys & Computat Biol, Urbana, IL 61801 USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						138	144						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700020
C	Goodhill, GJ		Jordan, MI; Kearns, MJ; Solla, SA		Goodhill, GJ			A mathematical model of axon guidance by diffusible factors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In the developing nervous system, gradients of target-derived diffusible factors play an important role in guiding axons to appropriate targets. In this paper, the shape that such a gradient might have is calculated as a function of distance from the target and the time since the start of factor production. Using estimates of the relevant parameter values from the experimental literature, the spatiotemporal domain in which a growth cone could detect such a gradient is derived. For large times, a value for the maximum guidance range of about 1 mm is obtained. This value fits well with experimental data. For smaller times, the analysis predicts that guidance over longer ranges may be possible. This prediction remains to be tested.	Georgetown Univ, Med Ctr, Georgetown Inst Cognit & Computat Sci, Washington, DC 20007 USA	Georgetown University	Goodhill, GJ (corresponding author), Georgetown Univ, Med Ctr, Georgetown Inst Cognit & Computat Sci, 3970 Reservoir Rd, Washington, DC 20007 USA.		Goodhill, Geoffrey J/D-9984-2013	Goodhill, Geoffrey/0000-0001-9789-9355					0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						159	165						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700023
C	Henkel, RD		Jordan, MI; Kearns, MJ; Solla, SA		Henkel, RD			A simple and fast neural network approach to stereovision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A neural network approach to stereovision is presented based on aliasing effects of simple disparity estimators and a fast coherence-detection scheme. Within a single network structure, a dense disparity map with an associated validation map and, additionally, the fused cyclopean view of the scene are available. The network operations are based on simple, biological plausible circuitry; the algorithm is fully parallel and non-iterative.	Univ Bremen, Inst Theoret Phys, D-28334 Bremen, Germany	University of Bremen	Henkel, RD (corresponding author), Univ Bremen, Inst Theoret Phys, POB 330440, D-28334 Bremen, Germany.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						808	814						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700114
C	Hirai, Y		Jordan, MI; Kearns, MJ; Solla, SA		Hirai, Y			A 1,000-neuron system with one million 7-bit physical interconnections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					An asynchronous PDM (Pulse-Density-Modulating) digital neural network system has been developed in our laboratory. It consists of one thousand neurons that are physically interconnected via one million 7-bit synapses. It can solve one thousand simultaneous nonlinear first-order differential equations in a fully parallel and continuous fashion. The performance of this system was measured by a winner-take-all network with one thousand neurons. Although the magnitude of the input and network parameters were identical for each competing neuron, one of them won in 6 milliseconds. This processing speed amounts to 360 billion connections per second. A broad range of neural networks including spatiotemporal filtering, feedforward, and feedback networks can be run by loading appropriate network parameters from a host system.	Univ Tsukuba, Inst Informat Sci & Elect, Tsukuba, Ibaraki 305, Japan	University of Tsukuba	Hirai, Y (corresponding author), Univ Tsukuba, Inst Informat Sci & Elect, 1-1-1 Ten Nodai, Tsukuba, Ibaraki 305, Japan.								0	2	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						705	711						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700100
C	Koistinen, P		Jordan, MI; Kearns, MJ; Solla, SA		Koistinen, P			Asymptotic theory for regularization: One-dimensional linear case	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The generalization ability of a neural network can sometimes be improved dramatically by regularization. To analyze the improvement one needs more refined results than the asymptotic distribution of the weight vector. Here we study the simple case of one-dimensional linear regression under quadratic regularization, i.e., ridge regression. We study the random design, misspecified case, where we derive expansions for the optimal regularization parameter and the ensuing improvement. It is possible to construct examples where it is best to use no regularization.	Univ Helsinki, Rolf Nevanlinna Inst, FIN-00014 Helsinki, Finland	University of Helsinki	Koistinen, P (corresponding author), Univ Helsinki, Rolf Nevanlinna Inst, POB 4, FIN-00014 Helsinki, Finland.		Koistinen, Petri/G-2334-2012						0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						294	300						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700042
C	Lee, DD; Seung, HS		Jordan, MI; Kearns, MJ; Solla, SA		Lee, DD; Seung, HS			A neural network based head tracking system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We have constructed an inexpensive, video-based, motorized tracking system that learns to track a head. It uses real time graphical user inputs or an auxiliary infrared detector as supervisory signals to train a convolutional neural network. The inputs to the neural network consist of normalized luminance and chrominance images and motion information from frame differences. Subsampled images are also used to provide scale invariance. During the online training phase, the neural network rapidly adjusts the input weights depending upon the reliability of the different channels in the surrounding environment. This quick adaptation allows the system to robustly track a head even when other objects are moving within a cluttered background.	AT&T Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T; Nokia Corporation; Nokia Bell Labs	Lee, DD (corresponding author), AT&T Bell Labs, Lucent Technol, 700 Mt Ave, Murray Hill, NJ 07974 USA.	ddlee@bell-labs.com; seung@bell-labs.com	Lee, Daniel D./B-5753-2013	Lee, Daniel/0000-0003-4239-8777					0	2	2	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						908	914						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700128
C	Milostan, JC; Cottrell, GW		Jordan, MI; Kearns, MJ; Solla, SA		Milostan, JC; Cottrell, GW			Serial order in reading aloud: Connectionist models and neighborhood structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Dual-Route and Connectionist Single-Route models of reading have been at odds over claims as to the correct explanation of the reading process. Recent Dual-Route models predict that subjects should show an increased naming latency for irregular words when the irregularity is earlier in the word (e.g. chef is slower than glow) - a prediction that has been confirmed in human experiments. Since this would appear to be an effect of the left-to-right reading process, Coltheart & Rastle (1994) claim that Single-Route parallel connectionist models cannot account for it. A refutation of this claim is presented here, consisting of network models which do show the interaction, along with orthographic neighborhood statistics that explain the effect.	Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Milostan, JC (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.			Cottrell, Garrison/0000-0001-7538-1715					0	2	2	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						59	65						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700009
C	Mineiro, P; Movellan, J; Williams, RJ		Jordan, MI; Kearns, MJ; Solla, SA		Mineiro, P; Movellan, J; Williams, RJ			Learning path distributions using nonequilibrium diffusion networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We propose diffusion networks, a type of recurrent neural network with probabilistic dynamics, as models for learning natural signals that are continuous in time and space. We give a formula for the gradient of the log-likelihood of a path with respect to the drift parameters for a diffusion network. This gradient can be used to optimize diffusion networks in the nonequilibrium regime for a wide variety of problems paralleling techniques which have succeeded in engineering fields such as system identification, state estimation and signal filtering. An aspect of this work which is of particular interest to computational neuroscience and hardware design is that with a suitable choice of activation function, e.g., quasi-linear sigmoidal, the gradient formula is local in space and time.	Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Mineiro, P (corresponding author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						598	604						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700085
C	Shi, BE; Hui, KF		Jordan, MI; Kearns, MJ; Solla, SA		Shi, BE; Hui, KF			An analog VLSI neural network for phase-based machine vision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We describe the design, fabrication and test results of an analog CMOS VLSI neural network prototype chip intended for phase-based machine vision algorithms. The chip implements an image filtering operation similar to Gabor-filtering. Because a Gabor filter's output is complex valued, it can be used to define a phase at every pixel in an image. This phase can be used in robust algorithms for disparity estimation and binocular stereo vergence control in stereo vision and for image motion analysis. The chip reported here takes an input image and generates two outputs at every pixel corresponding to the real and imaginary parts of the output.	Hong Kong Univ Sci & Technol, Dept Elect & Elect Engn, Kowloon, Hong Kong	Hong Kong University of Science & Technology	Shi, BE (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Elect Engn, Kowloon, Hong Kong.								0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						726	732						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700103
C	Weiss, Y		Jordan, MI; Kearns, MJ; Solla, SA		Weiss, Y			Phase transitions and the perceptual organization of video sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Estimating motion in scenes containing multiple moving objects remains a difficult problem in computer vision. A promising approach to this problem involves using mixture models, where the motion of each object is a component in the mixture. However, existing methods typically require specifying in advance the number of components in the mixture, i.e. the number of objects in the scene. Here we show that the number of objects can be estimated automatically in a maximum likelihood framework, given an assumption about the level of noise in the video sequence. We derive analytical results showing the number of models which maximize the likelihood for a given noise level in a given sequence. We illustrate these results on a real video sequence, showing how the phase transitions correspond to different perceptual organizations of the scene.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Weiss, Y (corresponding author), MIT, Dept Brain & Cognit Sci, E10-120, Cambridge, MA 02139 USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						850	856						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700120
C	Yamada, S; Watanabe, A; Nakashima, M		Jordan, MI; Kearns, MJ; Solla, SA		Yamada, S; Watanabe, A; Nakashima, M			Hybrid reinforcement learning and its application to biped robot control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A learning system composed of linear control modules, reinforcement learning modules and selection modules (a hybrid reinforcement learning system) is proposed for the fast learning of real-world control problems. The selection modules choose one appropriate control module dependent on the state. This hybrid learning system was applied to the control of a stilt-type biped robot. It learned the control on a sloped floor more quickly than the usual reinforcement learning because it did not need to learn the control on a fiat floor, where the linear control module can control the robot. When it was trained by a 2-step learning (during the first learning step, the selection module was trained by a training procedure controlled only by the linear controller), it learned the control more quickly. The average number of trials (about 50) is so small that the learning system is applicable to real robot control.	Mitsubishi Elect Corp, Adv Technol R&D Ctr, Amagasaki, Hyogo 6610001, Japan	Mitsubishi Electric Corporation	Yamada, S (corresponding author), Mitsubishi Elect Corp, Adv Technol R&D Ctr, Amagasaki, Hyogo 6610001, Japan.								0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1071	1077						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700151
C	Littlestone, N; Mesterharm, C		Mozer, MC; Jordan, MI; Petsche, T		Littlestone, N; Mesterharm, C			An ApoBayesian relative of Winnow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We study a mistake-driven variant of an on-line Bayesian learning algorithm (similar to one studied by Cesa-Bianchi, Helmbold, and Panizza [CHP96]). This variant only updates its state (learns) on trials in which it makes a mistake. The algorithm makes binary classifications using a linear-threshold classifier and runs in time linear in the number of attributes seen by the learner. We have been able to show, theoretically and in simulations, that this algorithm performs well under assumptions quite different from those embodied in the prior of the original Bayesian algorithm. It can handle situations that we do not know how to handle in linear time with Bayesian algorithms. We expect our techniques to be useful in deriving and analyzing other apobayesian algorithms.			Littlestone, N (corresponding author), NEC RES INST,4 INDEPENDENCE WAY,PRINCETON,NJ 08540, USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						204	210						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00029
C	Maass, W; Orponen, P		Mozer, MC; Jordan, MI; Petsche, T		Maass, W; Orponen, P			On the effect of analog noise in discrete-time analog computations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We introduce a model for noise-robust analog computations with discrete time that is flexible enough to cover the most important concrete cases, such as computations in noisy analog neural nets and networks of noisy spiking neurons. We show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and we also prove a new type of upper bound for the VC-dimension of computational models with analog noise.			Maass, W (corresponding author), GRAZ TECH UNIV,INST THEORET COMP SCI,KLOSTERWIESGASSE 32-2,A-8010 GRAZ,AUSTRIA.		Orponen, Pekka/E-7332-2012	Orponen, Pekka/0000-0002-0417-2104					0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						218	224						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00031
C	Mel, BW; Ruderman, DL; Archie, KA		Mozer, MC; Jordan, MI; Petsche, T		Mel, BW; Ruderman, DL; Archie, KA			Complex-cell responses derived from center-surround inputs: The surprising power of intradendritic computation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Biophysical modeling studies have previously shown that cortical pyramidal cells driven by strong NMDA-type synaptic currents and/or containing dendritic voltage-dependent Ca++ or Na+ channels, respond more strongly when synapses are activated in several spatially clustered groups of optimal size-in comparison to the same number of synapses activated diffusely about the dendritic arbor [8]. The nonlinear intradendritic interactions giving rise to this ''cluster sensitivity'' property are akin to a layer of virtual nonlinear ''hidden units'' in the dendrites, with implications for the cellular basis of learning and memory [7, 6], and for certain classes of nonlinear sensory processing [8]. In the present study, we show that a single neuron, with access only to excitatory inputs from unoriented ON- and OFF-center cells in the LGN, exhibits the principal nonlinear response properties of a ''complex'' cell in primary visual cortex, namely orientation tuning coupled with translation invariance and contrast insensitivity. We conjecture that this type of intradendritic processing could explain how complex cell responses can persist in the absence of oriented simple cell input [13].			Mel, BW (corresponding author), UNIV SO CALIF,DEPT BIOMED ENGN,LOS ANGELES,CA 90089, USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						83	89						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00012
C	Monti, S; Cooper, GF		Mozer, MC; Jordan, MI; Petsche, T		Monti, S; Cooper, GF			Learning Bayesian belief networks with neural network estimators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In this paper we propose a method for learning Bayesian belief networks from data. The method uses artificial neural networks as probability estimators, thus avoiding the need for making prior assumptions on the nature of the probability distributions governing the relationships among the participating variables. This new method has the potential for being applied to domains containing both discrete and continuous variables arbitrarily distributed. We compare the learning performance of this new method with the performance of the method proposed by Cooper and Herskovits in [7]. The experimental results show that, although the learning scheme based on the use of ANN estimators is slower, the learning accuracy of the two methods is comparable. Category: Algorithms and Architectures.			Monti, S (corresponding author), UNIV PITTSBURGH,INTELLIGENT SYST PROGRAM,901M CL,PITTSBURGH,PA 15260, USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						578	584						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00082
C	Papka, R; Callan, JP; Barto, AG		Mozer, MC; Jordan, MI; Petsche, T		Papka, R; Callan, JP; Barto, AG			Text-based information retrieval using exponentiated gradient descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	Advances in Neural Information Processing Systems		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The following investigates the use of single-neuron learning algorithms to improve the performance of text-retrieval systems that accept natural-language queries. A retrieval process is explained that transforms the natural-language query into the query syntax of a real retrieval system: the initial query is expanded using statistical and learning techniques and is then used for document ranking and binary classification. The results of experiments suggest that Kivinen and Warmuth's Exponentiated Gradient Descent learning algorithm works significantly better than previous approaches.			Papka, R (corresponding author), UNIV MASSACHUSETTS, DEPT COMP SCI, AMHERST, MA 01003 USA.								0	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						3	9						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00001
C	Saito, K; Nakano, R		Mozer, MC; Jordan, MI; Petsche, T		Saito, K; Nakano, R			Second-order learning algorithm with squared penalty term	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper compares three penalty terms with respect to the efficiency of supervised learning, by using first- and second-order learning algorithms. Our experiments showed that for a reasonably adequate penalty factor, the combination of the squared penalty term and the second-order learning algorithm drastically improves the convergence performance more than 20 times over the other combinations, at the same time bringing about a better generalization performance.			Saito, K (corresponding author), NTT COMMUN SCI LABS,2 HIKARIDAI,SEIKA,KYOTO 61902,JAPAN.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						627	633						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00089
C	Wan, E; Bone, D		Mozer, MC; Jordan, MI; Petsche, T		Wan, E; Bone, D			Interpolating earth-science data using RBF networks and mixtures of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present a mixture of experts (ME) approach to interpolate sparse, spatially correlated earth-science data. Kriging is an interpolation method which uses a global covariation model estimated from the data to take account of the spatial dependence in the data. Based on the close relationship between kriging and the radial basis function (RBF) network (Wan & Bone, 1996), we use a mixture of generalized RBF networks to partition the input space into statistically correlated regions and learn the local covariation model of the data in each region. Applying the ME approach to simulated and real-world data, we show that it is able to achieve good partitioning of the input space, learn the local covariation models and improve generalization.			Wan, E (corresponding author), CSIRO,DIV INFORMAT TECHNOL,CANBERRA LAB,GPO BOX 664,CANBERRA,ACT 2601,AUSTRALIA.			Bone, Donald/0000-0001-6655-9032					0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						988	994						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00139
C	West, AHL; Saad, D; Nabney, IT		Mozer, MC; Jordan, MI; Petsche, T		West, AHL; Saad, D; Nabney, IT			The learning dynamics of a universal approximator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The learning properties of a universal approximator, a normalized committee machine with adjustable biases, are studied for on-line back-propagation learning. Within a statistical mechanics framework, numerical studies show that this model has features which do not exist in previously studied two-layer network models without adjustable biases, e.g., attractive suboptimal symmetric phases even for realizable cases and noiseless data.			West, AHL (corresponding author), ASTON UNIV,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.			Nabney, Ian T/0000-0003-1513-993X; Saad, David/0000-0001-9821-2623; Nabney, Ian/0000-0001-7382-2855					0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						288	294						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00041
C	Wu, LZ; Moody, J		Mozer, MC; Jordan, MI; Petsche, T		Wu, LZ; Moody, J			Multi-effect decompositions for financial data modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				High frequency foreign exchange data can be decomposed into three components: the inventory effect component, the surprise information (news) component and the regular information component. The presence of the inventory effect and news can make analysis of trends due to the diffusion of information (regular information component) difficult. We propose a neural-net-based, independent component analysis to separate high frequency foreign exchange data into these three components. Our empirical results show that our proposed multi-effect decomposition can reveal the intrinsic price behavior.			Wu, LZ (corresponding author), OREGON GRAD INST,DEPT COMP SCI,POB 91000,PORTLAND,OR 97291, USA.								0	2	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						995	1001						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00140
C	Yen, SC; Finkel, LH		Mozer, MC; Jordan, MI; Petsche, T		Yen, SC; Finkel, LH			Salient contour extraction by temporal binding in a cortically-based network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				It has been suggested that long-range intrinsic connections in striate cortex may play a role in contour extraction (Gilbert et. al., 1996). A number of recent physiological and psychophysical studies have examined the possible role of long range connections in the modulation of contrast detection thresholds (Polat and Sagi, 1993, 1994; Kapadia et al., 1995; Kovacs and Julesz, 1994) and various pre-attentive detection tasks (Kovacs and Julesz, 1993; Field et al., 1993). We have developed a network architecture based on the anatomical connectivity of striate cortex, as well as the temporal dynamics of neuronal processing, that is able to reproduce the observed experimental results. The network has been tested on real images and has applications in terms of identifying salient contours in automatic image processing systems.			Yen, SC (corresponding author), UNIV PENN,DEPT BIOENGN,PHILADELPHIA,PA 19104, USA.		Yen, Shih-Cheng/AAU-6487-2021	Yen, Shih-Cheng/0000-0001-7723-0072					0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						915	921						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00129
C	Zemel, RS; Dayan, P; Pouget, A		Mozer, MC; Jordan, MI; Petsche, T		Zemel, RS; Dayan, P; Pouget, A			Probabilistic interpretation of population codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present a theoretical framework for population codes which generalizes naturally to the important case where the population provides information about a whole probability distribution over an underlying quantity rather than just a single value. We use the framework to analyze two existing models, and to suggest and evaluate a third model for encoding such probability distributions.			Zemel, RS (corresponding author), UNIV ARIZONA,TUCSON,AZ 85721, USA.								0	2	2	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						676	682						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00096
C	Alpaydin, E		Touretzky, DS; Mozer, MC; Hasselmo, ME		Alpaydin, E			Selective attention for handwritten digit recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						BOGAZICI UNIV,DEPT COMP ENGN,TR-80815 ISTANBUL,TURKEY	Bogazici University			ALPAYDIN, ETHEM/E-6127-2013	ALPAYDIN, ETHEM/0000-0001-7506-0321					0	2	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						771	777						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00109
C	Bair, W; Zohary, E; Koch, C		Touretzky, DS; Mozer, MC; Hasselmo, ME		Bair, W; Zohary, E; Koch, C			Correlated neuronal response: Time scales and mechanisms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						NYU,CTR NEURAL SCI,HOWARD HUGHES MED INST,NEW YORK,NY 10003	Howard Hughes Medical Institute; New York University									0	2	2	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						68	74						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00010
C	Choe, Y; Sirosh, J; Miikkulainen, R		Touretzky, DS; Mozer, MC; Hasselmo, ME		Choe, Y; Sirosh, J; Miikkulainen, R			Laterally interconnected self-organizing maps in hand-written digit recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	Advances in Neural Information Processing Systems		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TEXAS, DEPT COMP SCI, AUSTIN, TX 78712 USA	University of Texas System; University of Texas Austin									0	2	2	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						736	742						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00104
C	Dayan, P; Singh, SP		Touretzky, DS; Mozer, MC; Hasselmo, ME		Dayan, P; Singh, SP			Improving policies without measuring merits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,CBCL,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1059	1065						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00149
C	Dodier, R		Touretzky, DS; Mozer, MC; Hasselmo, ME		Dodier, R			Geometry of early stopping in linear networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV COLORADO,DEPT COMP SCI,BOULDER,CO 80309	University of Colorado System; University of Colorado Boulder									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						365	371						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00052
C	Ferguson, A; Sabisch, T; Kaye, P; Dixon, LC; Bolouri, H		Touretzky, DS; Mozer, MC; Hasselmo, ME		Ferguson, A; Sabisch, T; Kaye, P; Dixon, LC; Bolouri, H			High-speed airborne particle monitoring using artificial neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV HERTFORDSHIRE,ERDC,HATFIELD AL10 9AB,HERTS,ENGLAND	United States Department of Defense; United States Army; U.S. Army Corps of Engineers; U.S. Army Engineer Research & Development Center (ERDC); University of Hertfordshire									0	2	2	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						980	986						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00138
C	Helmbold, DP; Kivinen, J; Warmuth, MK		Touretzky, DS; Mozer, MC; Hasselmo, ME		Helmbold, DP; Kivinen, J; Warmuth, MK			Worst-case loss bounds for single neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF SANTA CRUZ,DEPT COMP SCI,SANTA CRUZ,CA 95064	University of California System; University of California Santa Cruz									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						309	315						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00044
C	Hinton, GE; Revow, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Hinton, GE; Revow, M			Using pairs of data-points to define splits for decision trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TORONTO,DEPT COMP SCI,TORONTO,ON M5S 1A4,CANADA	University of Toronto									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						507	513						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00072
C	Howse, JW; Abdallah, CT; Heileman, GL		Touretzky, DS; Mozer, MC; Hasselmo, ME		Howse, JW; Abdallah, CT; Heileman, GL			Gradient and Hamiltonian dynamics applied to learning in neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV NEW MEXICO,DEPT ELECT & COMP ENGN,ALBUQUERQUE,NM 87131	University of New Mexico			Abdallah, Chaouki/ABF-6678-2020	Abdallah, Chaouki/0000-0001-8259-097X					0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						274	280						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00039
C	Konig, Y; Bourlard, H; Morgan, N		Touretzky, DS; Mozer, MC; Hasselmo, ME		Konig, Y; Bourlard, H; Morgan, N			REMAP: Recursive estimation and maximization of a posteriori probabilities - Application to transition-based connectionist speech recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						INT COMP SCI INST,BERKELEY,CA 94704										0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						388	394						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00055
C	Martin, GL		Touretzky, DS; Mozer, MC; Hasselmo, ME		Martin, GL			Human reading and the curse of dimensionality	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MCC,AUSTIN,TX 78613										0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						17	23						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00003
C	MeyerBase, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		MeyerBase, A			Quadratic-type Lyapunov functions for competitive neural networks with different time-scales	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TECH UNIV DARMSTADT,INST TECH INFORMAT,D-64283 DARMSTADT,GERMANY	Technical University of Darmstadt			Meyer-Baese, Anke/E-9745-2010	Meyer-Baese, Anke/0000-0001-6363-2687					0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						337	343						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00048
C	Nakahara, H; Doya, K		Touretzky, DS; Mozer, MC; Hasselmo, ME		Nakahara, H; Doya, K			Dynamics of attention as near saddle-node bifurcation behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TOKYO,MEGURO KU,TOKYO 153,JAPAN	University of Tokyo			Nakahara, Hiroyuki/N-5411-2015	Nakahara, Hiroyuki/0000-0001-6891-1175					0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						38	44						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00006
C	Pessoa, L; Ross, WD		Touretzky, DS; Mozer, MC; Hasselmo, ME		Pessoa, L; Ross, WD			A neural network model of 3-D lightness perception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV FED RIO DE JANEIRO,RIO JANEIRO,RJ,BRAZIL	Universidade Federal do Rio de Janeiro									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						844	850						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00119
C	Pouget, A; Sejnowski, TJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Pouget, A; Sejnowski, TJ			A model of spatial representations in parietal cortex explains hemineglect	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF LOS ANGELES,DEPT NEUROBIOL,LOS ANGELES,CA 90095	University of California System; University of California Los Angeles			Sejnowski, Terrence/AAV-5558-2021						0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						10	16						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00002
C	Rebotier, TP; Elman, JL		Touretzky, DS; Mozer, MC; Hasselmo, ME		Rebotier, TP; Elman, JL			Explorations with the dynamic wave model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF SAN DIEGO,DEPT COGNIT SCI,LA JOLLA,CA 92093	University of California System; University of California San Diego									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						549	555						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00078
C	Ruger, SM		Touretzky, DS; Mozer, MC; Hasselmo, ME		Ruger, SM			Stable dynamic parameter adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TECH UNIV BERLIN,FACHBEREICH INFORMAT,D-10587 BERLIN,GERMANY	Technical University of Berlin									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						225	231						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00032
C	Shibata, T; Nakai, T; Morimoto, T; Kaihara, R; Yamashita, T; Ohmi, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Shibata, T; Nakai, T; Morimoto, T; Kaihara, R; Yamashita, T; Ohmi, T			Neuron-MOS temporal winner search hardware for fully-parallel data processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TOHOKU UNIV,DEPT ELECT ENGN,AOBA KU,SENDAI,MIYAGI 98077,JAPAN	Tohoku University									0	2	3	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						685	691						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00097
C	Walter, J; Ritter, H		Touretzky, DS; Mozer, MC; Hasselmo, ME		Walter, J; Ritter, H			Investment learning with hierarchical PSOMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV BIELEFELD,DEPT INFORMAT SCI,D-33615 BIELEFELD,GERMANY	University of Bielefeld									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						570	576						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00081
C	Yu, SH; Annaswamy, AM		Touretzky, DS; Mozer, MC; Hasselmo, ME		Yu, SH; Annaswamy, AM			Neural control for nonlinear dynamic systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,DEPT MECH ENGN,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)									0	2	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1010	1016						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00142
C	BERTHIER, NE; SINGH, SP; BARTO, AG; HOUK, JC		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BERTHIER, NE; SINGH, SP; BARTO, AG; HOUK, JC			A CORTICO-CEREBELLAR MODEL THAT LEARNS TO GENERATE DISTRIBUTED MOTOR COMMANDS TO CONTROL A KINEMATIC ARM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						611	618						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00075
C	COHEN, MH; POULIQUEN, PO; ANDREOU, AG		MOODY, JE; HANSON, SJ; LIPPMANN, RP		COHEN, MH; POULIQUEN, PO; ANDREOU, AG			ANALOG LSI IMPLEMENTATION OF AN AUTO-ADAPTIVE NETWORK FOR REAL-TIME SEPARATION OF INDEPENDENT SIGNALS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Andreou, Andreas G. G/A-3271-2010; Pouliquen, Philippe O/A-3735-2010	Andreou, Andreas G. G/0000-0003-3826-600X; 					0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						805	812						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00099
C	DEAN, P; MAYHEW, JEW; LANGDON, P		MOODY, JE; HANSON, SJ; LIPPMANN, RP		DEAN, P; MAYHEW, JEW; LANGDON, P			A NEURAL NET MODEL FOR ADAPTIVE-CONTROL OF SACCADIC ACCURACY BY PRIMATE CEREBELLUM AND BRAIN-STEM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						595	602						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00073
C	HENIS, EA; FLASH, T		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HENIS, EA; FLASH, T			A COMPUTATIONAL MECHANISM TO ACCOUNT FOR AVERAGED MODIFIED HAND TRAJECTORIES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						619	626						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00076
C	LEVIN, E; PIERACCINI, R; BOCCHIERI, E		MOODY, JE; HANSON, SJ; LIPPMANN, RP		LEVIN, E; PIERACCINI, R; BOCCHIERI, E			TIME-WARPING NETWORK - A HYBRID FRAMEWORK FOR SPEECH RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	3	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						151	158						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00019
C	MOZER, MC; ZEMEL, RS; BEHRMANN, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MOZER, MC; ZEMEL, RS; BEHRMANN, M			LEARNING TO SEGMENT IMAGES USING DYNAMIC FEATURE BINDING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						436	443						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00054
C	PRINCIPE, JC; DEVRIES, B; KUO, JM; DEOLIVEIRA, PG		MOODY, JE; HANSON, SJ; LIPPMANN, RP		PRINCIPE, JC; DEVRIES, B; KUO, JM; DEOLIVEIRA, PG			MODELING APPLICATIONS WITH THE FOCUSED GAMMA NET	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										principe, jose/N-8099-2014						0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						143	150						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00018
C	RENALS, S; BOURLARD, H; FRANCO, H; MORGAN, N; COHEN, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		RENALS, S; BOURLARD, H; FRANCO, H; MORGAN, N; COHEN, M			CONNECTIONIST OPTIMIZATION OF TIED MIXTURE HIDDEN MARKOV-MODELS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Renals, Steve/L-7175-2014	Renals, Steve/0000-0002-8790-3389					0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						167	174						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00021
C	SACKINGER, E; BOSER, BE; JACKEL, LD		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SACKINGER, E; BOSER, BE; JACKEL, LD			A NEUROCOMPUTER BOARD BASED ON THE ANNA NEURAL NETWORK CHIP	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						773	780						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00095
C	SIU, KY; BRUCK, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SIU, KY; BRUCK, J			NEURAL COMPUTING WITH SMALL WEIGHTS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						944	949						6	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00116
C	SMYTH, P; MELLSTROM, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SMYTH, P; MELLSTROM, J			FAULT-DIAGNOSIS OF ANTENNA POINTING SYSTEMS USING HYBRID NEURAL NETWORK AND SIGNAL-PROCESSING MODELS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						667	674						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00082
C	TSOI, AC		MOODY, JE; HANSON, SJ; LIPPMANN, RP		TSOI, AC			APPLICATION OF NEURAL NETWORK METHODOLOGY TO THE MODELING OF THE YIELD STRENGTH IN A STEEL ROLLING PLATE MILL	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO											Tsoi, Ah Chung/0000-0003-2904-7008					0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						698	705						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00086
C	VANSTEVENINCK, RD; BIALEK, W		MOODY, JE; HANSON, SJ; LIPPMANN, RP		VANSTEVENINCK, RD; BIALEK, W			STATISTICAL RELIABILITY OF A BLOWFLY MOVEMENT-SENSITIVE NEURON	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						27	34						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00004
C	WILES, J; BLOESCH, A		MOODY, JE; HANSON, SJ; LIPPMANN, RP		WILES, J; BLOESCH, A			OPERATORS AND CURRIED FUNCTIONS - TRAINING AND ANALYSIS OF SIMPLE RECURRENT NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	2	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						325	332						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00040
C	Agrawal, A; Sheldon, D; Domke, J		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Agrawal, Abhinav; Sheldon, Daniel; Domke, Justin			Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Recent research has seen several advances relevant to black-box variational inference (VI), but the current state of automatic posterior inference is unclear. One such advance is the use of normalizing flows to define flexible posterior densities for deep latent variable models. Another direction is the integration of Monte-Carlo methods to serve two purposes; first, to obtain tighter variational objectives for optimization, and second, to define enriched variational families through sampling. However, both flows and variational Monte-Carlo methods remain relatively unexplored for black-box VI. Moreover, on a pragmatic front, there are several optimization considerations like step-size scheme, parameter initialization, and choice of gradient estimators, for which there is no clear guidance in the literature. In this paper, we postulate that black-box VI is best addressed through a careful combination of numerous algorithmic components. We evaluate components relating to optimization, flows, and Monte-Carlo methods on a benchmark of 30 models from the Stan model library. The combination of these algorithmic components significantly advances the state-of-the-art "out of the box" variational inference.	[Agrawal, Abhinav; Sheldon, Daniel; Domke, Justin] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Sheldon, Daniel] Mt Holyoke Coll, Dept Comp Sci, S Hadley, MA 01075 USA	University of Massachusetts System; University of Massachusetts Amherst; Mount Holyoke College	Agrawal, A (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	aagrawal@cs.umass.edu; sheldon@cs.umass.edu; domke@cs.umass.edu			National Science Foundation [1908577]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1908577.	[Anonymous], 2017, ICLR; Ba J., 2017, P 3 INT C LEARN REPR; Bachman P., 2015, NIPS APPR INF WORKSH; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Burda Y., 2016, ICLR; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Chen Liqun, 2018, INT C MACH LEARN; Chen Tian Qi, 2019, NEURIPS; Cremer Chris, 2017, ICLR WORKSH; D Maclaurin, 2015, ICML AUTOML WORKSH; Dinh L., 2015, ARXIV14108516; DOMKE J, 2019, NEURIPS; Domke Justin, 2018, NEURIPS; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Durkan Conor, 2019, NEURIPS; Fjelde Tor Erlend, 2020, ADV APPROXIMATE BAYE; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Huang Chin-Wei, 2018, ICML; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2018, NEURIPS; Kingma Diederik P., 2016, NEURIPS; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Maddison Chris J, 2017, NEURIPS; Mishkin Aaron, 2018, SLANG FAST STRUCTURE; Naesseth C. A., 2018, AISTATS; Papamakarios G., 2017, NEURIPS; Papamakarios George, NORMALIZING FLOWS PR; Rainforth Tom, 2018, ICML; Ranganath R., 2014, ARTIFICIAL INTELLIGE; Ranganath R., 2013, ICML; Rezende D.J., 2014, PROC INT CONFER ENCE; REZENDE DJ, 2015, ICML; Roeder Geoffrey, 2017, NEURIPS; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Stan Developers, 2018, EX MOD; Stan Development Team, 2018, STAN COR LIB; Tucker George, 2019, ICLR; Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2; Webb Stefan, 2019, ICML WORKSH AUT MACH	39	1	1	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000051
C	Ahmed, NK; Duffield, N		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Ahmed, Nesreen K.; Duffield, Nick			Adaptive Shrinkage Estimation for Streaming Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				ALGORITHMS	Networks are a natural representation of complex systems across the sciences, and higher-order dependencies are central to the understanding and modeling of these systems. However, in many practical applications such as online social networks, networks are massive, dynamic, and naturally streaming, where pairwise interactions among vertices become available one at a time in some arbitrary order. The massive size and streaming nature of these networks allow only partial observation, since it is infeasible to analyze the entire network. Under such scenarios, it is challenging to study the higher-order structural and connectivity patterns of streaming networks. In this work, we consider the fundamental problem of estimating the higher-order dependencies using adaptive sampling. We propose a novel adaptive, single-pass sampling framework and unbiased estimators for higher-order network analysis of large streaming networks. Our algorithms exploit adaptive techniques to identify edges that are highly informative for efficiently estimating the higher-order structure of streaming networks from small sample data. We also introduce a novel James-Stein shrinkage estimator to reduce the estimation error. Our approach is fully analytic, computationally efficient, and can be incrementally updated in a streaming setting. Numerical experiments on large networks show that our approach is superior to baseline methods.	[Ahmed, Nesreen K.] Intel Labs, Santa Clara, CA 95054 USA; [Duffield, Nick] Texas A&M Univ, College Stn, TX 77843 USA	Intel Corporation; Texas A&M University System; Texas A&M University College Station	Ahmed, NK (corresponding author), Intel Labs, Santa Clara, CA 95054 USA.	nesreen.k.ahmed@intel.com; duffieldng@tamu.edu			National Science Foundation [ENG-1839816, IIS1848596, CCF-1934904]	National Science Foundation(National Science Foundation (NSF))	Nick Duffield is supported by the National Science Foundation under awards ENG-1839816, IIS1848596 and CCF-1934904.	Achlioptas Dimitris, 2013, ADV NEURAL INFORM PR, P1565; Ahmed NK, 2017, PROC VLDB ENDOW, V10, P1430, DOI 10.14778/3137628.3137651; Ahmed NK, 2017, KNOWL INF SYST, V50, P689, DOI 10.1007/s10115-016-0965-5; Ahmed NK, 2015, IEEE DATA MINING, P1, DOI 10.1109/ICDM.2015.141; Ahmed NK, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1446, DOI 10.1145/2623330.2623757; Ahmed NK, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2601438; Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47; Becchetti Luca, 2008, P 14 ACM SIGKDD INT, P16, DOI [10.1145/1401890.1401898, DOI 10.1145/1401890.1401898]; Benson AR, 2016, SCIENCE, V353, P163, DOI 10.1126/science.aad9029; Buriol Luciana S., 2006, P TWENTYFIFTH ACM SI, P253, DOI [DOI 10.1145/1142351.1142388, 10.1145/1142351, DOI 10.1145/1142351]; Chen YL, 2011, IEEE T SIGNAL PROCES, V59, P4097, DOI 10.1109/TSP.2011.2138698; Chen YL, 2010, IEEE T SIGNAL PROCES, V58, P5016, DOI 10.1109/TSP.2010.2053029; Cohen-Steiner D, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1263, DOI 10.1145/3219819.3220119; Das Sarma A, 2011, J ACM, V58, DOI 10.1145/1970392.1970397; De Stefani L, 2017, ACM T KNOWL DISCOV D, V11, DOI 10.1145/3059194; Duffield N, 2007, J ACM, V54, DOI 10.1145/1314690.1314696; Efraimidis PS, 2006, INFORM PROCESS LETT, V97, P181, DOI 10.1016/j.ipl.2005.11.003; Eikmeier N, 2018, IEEE DATA MINING, P941, DOI 10.1109/ICDM.2018.00115; Elenberg ER, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P229, DOI 10.1145/2783258.2783413; Gemulla R, 2008, VLDB J, V17, P173, DOI 10.1007/s00778-007-0065-y; Gleich D. F., 2012, GRAPH FLICKR PHOTO S; Grilli J, 2017, NATURE, V548, P210, DOI 10.1038/nature23273; Gruber M.H.J., 2017, IMPROVING EFFICIENCY; Guha S, 2015, PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P241, DOI 10.1145/2745754.2745763; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; James W., 1992, BREAKTHROUGHS STAT, P443, DOI [10.1007/978- 1-4612-0919-5_30, 10.1007/978-1-4612-0919-5_30, DOI 10.1007/978-1-4612-0919-5_30]; Jha M, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P589, DOI 10.1145/2487575.2487678; KHETAN A., 2017, ADV NEURAL INFORM PR, V30, P6424; Knuth DE, 2014, ART COMPUTER PROGRAM, V2; Ledoit O., 2003, J EMPIR FINANC, V10, P603, DOI [10.1016/S0927-5398(03)00007-0, DOI 10.1016/S0927-5398(03)00007-0]; Leskovec J., 2006, P 12 ACM SIGKDD INT, P631; Leskovec J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1361; Leskovec J, 2009, INTERNET MATH, V6, P29, DOI 10.1080/15427951.2009.10129177; Lim Y, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P685, DOI 10.1145/2783258.2783285; McGregor A, 2014, SIGMOD REC, V43, P9, DOI 10.1145/2627692.2627694; Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824; Mislove A, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29; Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002; Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480; Pavan A, 2013, PROC VLDB ENDOW, V6, P1870, DOI 10.14778/2556549.2556569; Rosen B, 1997, J STAT PLAN INFER, V62, P135, DOI 10.1016/S0378-3758(96)00185-1; Rossi RA, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P483, DOI 10.1145/3336191.3371843; Rossi RA, 2018, COMPANION PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2018 (WWW 2018), P3, DOI 10.1145/3184558.3186900; Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292; Rosvall M, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5630; Schafer J, 2005, STAT APPL GENET MO B, V4, DOI 10.2202/1544-6115.1175; Scholtes I, 2016, EUR PHYS J B, V89, DOI 10.1140/epjb/e2016-60663-0; Seshadhri C., 2013, SIAM, P10, DOI [10.1137/1.9781611972832.2, DOI 10.1137/1.9781611972832.2]; Tsourakakis C. E., 2011, TRIANGLE SPARSIFIERS; Tsourakakis CE, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P333, DOI 10.1145/2556195.2556213; Tsourakakis CE, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P837; Tsourkakis CE, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1451, DOI 10.1145/3038912.3052653; VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165; Xu J, 2016, SCI ADV, V2, DOI 10.1126/sciadv.1600028; Xu KS, 2014, DATA MIN KNOWL DISC, V28, P304, DOI 10.1007/s10618-012-0302-x; Yin H, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P555, DOI 10.1145/3097983.3098069; Zhao H., 2018, AAAI	57	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000045
C	Baby, D; Wang, YX		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Baby, Dheeraj; Wang, Yu-Xiang			Adaptive Online Estimation of Piecewise Polynomial Trends	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We consider the framework of non-stationary stochastic optimization [Besbes et al., 2015] with squared error losses and noisy gradient feedback where the dynamic regret of an online learner against a time varying comparator sequence is studied. Motivated from the theory of non-parametric regression, we introduce a new variational constraint that enforces the comparator sequence to belong to a discrete k(th) order Total Variation ball of radius C-n. This variational constraint models comparators that have piecewise polynomial structure which has many relevant practical applications [Tibshirani, 2014]. By establishing connections to the theory of wavelet based non-parametric regression, we design a polynomial time algorithm that achieves the nearly optimal dynamic regret of (O) over tilde (n(1/2k+3)C(n)(2/k+3)). The proposed policy is adaptive to the unknown radius C-n. Further, we show that the same policy is minimax optimal for several other non-parametric families of interest.	[Baby, Dheeraj; Wang, Yu-Xiang] UC Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA	University of California System; University of California Santa Barbara	Baby, D (corresponding author), UC Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.	dheeraj@ucsb.edu; yuxiangw@cs.ucsb.edu		Wang, Yu-Xiang/0000-0002-6403-212X	UCSB CS department; NSF [2029626]	UCSB CS department; NSF(National Science Foundation (NSF))	The research is partially supported by a start-up grant from UCSB CS department, NSF Award #2029626 and generous gifts from Adobe and Amazon Web Services.	Baby Dheeraj, 2019, NEURAL INFORM PROCES; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408; Box G., 1970, TIME SERIES ANAL; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chen Niangjun, 2018, P C LEARNING THEORY, P1574; Chen Xi, 2018, NONSTATIONARY STOCHA; Cohen A., 1993, Applied and Computational Harmonic Analysis, V1, P54, DOI 10.1006/acha.1993.1005; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; De Boor C., 1978, PRACTICAL GUIDE SPLI, V27; Dekel Ofer, 2005, J MACH LEARN RES; Dingle Brent M., 2005, SYMBOLIC DETERMINANT; DONOHO DL, 1990, ANN STAT, V18, P1416, DOI 10.1214/aos/1176347758; Donoho DL, 1998, ANN STAT, V26, P879; Gaillard P., 2015, C LEARN THEOR, P764; Guntuboyina Adityanand, 2017, ADAPTIVE RISK BOUNDS; Gyorfi L., 2002, DISTRIBUTION FREE TH; Hall, 2013, P 30 INT C MACH LEAR, P579; Hazan E., 2007, ELECT C COMPUTATIONA, V14; Hodrick RJ, 1997, J MONEY CREDIT BANK, V29, P1, DOI 10.2307/2953682; Jadbabaie A, 2015, JMLR WORKSH CONF PRO, V38, P398; Johnstone I.M., 2017, GAUSSIAN ESTIMATION; Kim SJ, 2009, SIAM REV, V51, P339, DOI 10.1137/070690274; Koolen Wouter M, 2015, ADV NEURAL INFORM PR, P2557; Kotlowski Wojciech, 2016, P 29 C LEARN THEOR C, V49, P1165; Mammen E, 1997, ANN STAT, V25, P387; Rakhlin Alexander, 2015, ABS150106598 CORR; Rakhlin Alexander, 2014, C LEARN THEOR, P1232; Sadhanala Veeranjaneyulu, 2016, ADV NEURAL INFORM PR; Schumaker L., 2007, SPLINE FUNCTIONS BAS, DOI DOI 10.1017/CBO9780511618994; Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189; Vovk V, 2001, INT STAT REV, V69, P213; Wang YJ, 2016, J MACH LEARN RES, V17, P1; Yang TB, 2016, PR MACH LEARN RES, V48; Zhang J, 2001, PROCEEDINGS OF THE FIFTEENTH INTERNATIONAL CONFERENCE ON SOIL MECHANICS AND GEOTECHNICAL ENGINEERING VOLS 1-3, P1323; Zhang LJ, 2018, PR MACH LEARN RES, V80; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	37	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000041
C	Bishop, N; Chan, H; Mandal, D; Tran-Thanh, L		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Bishop, Nicholas; Chan, Hau; Mandal, Debmalya; Tran-Thanh, Long			Adversarial Blocking Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We consider a general adversarial multi-armed blocking bandit setting where each played arm can be blocked (unavailable) for some time periods and the reward per arm is given at each time period adversarially without obeying any distribution. The setting models scenarios of allocating scarce limited supplies (e.g., arms) where the supplies replenish and can be reused only after certain time periods. We first show that, in the optimization setting, when the blocking durations and rewards are known in advance, finding an optimal policy (e.g., determining which arm per round) that maximises the cumulative reward is strongly NP-hard, eliminating the possibility of a fully polynomial-time approximation scheme (FPTAS) for the problem unless P = NP. To complement our result, we show that a greedy algorithm that plays the best available arm at each round provides an approximation guarantee that depends on the blocking durations and the path variance of the rewards. In the bandit setting, when the blocking durations and rewards are not known, we design two algorithms, RGA and RGA-META, for the case of bounded duration an path variation. In particular, when the variation budget B-T is known in advance, RGA can achieve O(root T(2 (D) over tilde + K)B-T) dynamic approximate regret. On the other hand, when BT is not known, we show that the dynamic approximate regret of RGA-META is at most O((K (D) over tilde)(1/4)(B) over tilde T-1/2(3/4)) where (B) over tilde is the maximal path variation budget within each batch of RGA-META (which is provably in order of o(root T). We also prove that if either the variation budget or the maximal blocking duration is unbounded, the approximate regret will be at least circle minus(T). We also show that the regret upper bound of RGA is tight if the blocking durations are bounded above by an order of O(1).	[Bishop, Nicholas] Univ Southampton, Southampton, Hants, England; [Chan, Hau] Univ Nebraska Lincoln, Lincoln, NE USA; [Mandal, Debmalya] Columbia Univ, New York, NY 10027 USA; [Tran-Thanh, Long] Univ Warwick, Coventry, W Midlands, England	University of Southampton; University of Nebraska System; University of Nebraska Lincoln; Columbia University; University of Warwick	Bishop, N (corresponding author), Univ Southampton, Southampton, Hants, England.	nb8g13@soton.ac.uk; hchan3@unl.edu; dm3557@columbia.edu; long.tran-thanh@warwick.ac.uk			UK Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Partnership grant; Columbia Data Science Institute Post-Doctoral Fellowship	UK Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Partnership grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Columbia Data Science Institute Post-Doctoral Fellowship	Nicholas Bishop was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Partnership grant. Debmalya Mandal was supported by a Columbia Data Science Institute Post-Doctoral Fellowship.	Agrawal S, 2014, P 15 ACM C EC COMPUT, P989, DOI [10.1145/2600057.2602844, DOI 10.1145/2600057.2602844.URL]; [Anonymous], 2012, 26 AAAI C ART INT; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Badanidiyuru A, 2014, P C LEARN THEOR, V35, P1109; Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30; Basu S., 2019, ADV NEURAL INFORM PR, P4784; Basu Soumya, 2020, ABS200303426 ARXIV; Bender M, 2017, J SCHEDULING, V20, P443, DOI 10.1007/s10951-016-0506-9; Besbes O., 2014, ADV NEURAL INFORM PR, P199; Chakrabarti D., 2009, ADV NEURAL INFORM PR, V21; Ding W., 2013, 27 AAAI C ART INT; Erlebach T., 2000, Algorithms and Computation. 11th International Conference, ISAAC 2000. Proceedings (Lecture Notes in Computer Science Vol.1969), P228; Garey M.R., 1979, COMPUTERS INTRACTABI; Immorlica N, 2019, ANN IEEE SYMP FOUND, P202, DOI 10.1109/FOCS.2019.00022; Jacobs Tobias, 2014, ABS14107237 ARXIV; Kale S., 2016, ADV NEURAL INFORM PR, V29, P2181; Karthik A, 2017, QUEUEING SYST, V85, P1, DOI 10.1007/s11134-016-9488-8; Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7; Kolen AWJ, 2007, NAV RES LOG, V54, P530, DOI 10.1002/nav.20231; Kovalyov MY, 2007, EUR J OPER RES, V178, P331, DOI 10.1016/j.ejor.2006.01.049; Long TT, 2014, ARTIF INTELL, V214, P89, DOI 10.1016/j.artint.2014.04.005; Miyazawa H, 2004, J SCHEDULING, V7, P293, DOI 10.1023/B:JOSH.0000031423.39762.d3; Neu G., 2014, ADV NEURAL INFORM PR, P2780; Neu Gergely, 2016, J MACHINE LEARNING R, V17, P5355; Ortner R., 2019, P 32 INT C LEARNING, P138; Rangi A, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3311; Tran-Thanh L., 2012, 26 AAAI C ART INT; Tran-Thanh L., 2010, 24 AAAI C ART INT; Wei C-Y., 2018, C LEARN THEOR, P1263; Yu G, 2020, IISE TRANS, V52, P432, DOI 10.1080/24725854.2019.1657606	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000055
C	Carmon, Y; Jambulapati, A; Jiang, QJ; Jin, YJ; Lee, YT; Sidford, A; Tian, K		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Carmon, Yair; Jambulapati, Arun; Jiang, Qijia; Jin, Yujia; Lee, Yin Tat; Sidford, Aaron; Tian, Kevin			Acceleration with a Ball Optimization Oracle	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Consider an oracle which takes a point x and returns the minimizer of a convex function f in an l(2) ball of radius r around x. It is straightforward to show that roughly r(-1) log 1/epsilon calls to the oracle suffice to find an epsilon-approximate minimizer of f in an l(2) unit ball. Perhaps surprisingly, this is not optimal: we design an accelerated algorithm which attains an epsilon-approximate minimizer with roughly r(-2/3) log 1/epsilon oracle queries, and give a matching lower bound. Further, we implement ball optimization oracles for functions with locally stable Hessians using a variant of Newton's method and, in certain cases, stochastic first-order methods. The resulting algorithm applies to a number of problems of practical and theoretical import, improving upon previous results for logistic and l(infinity) regression and achieving guarantees comparable to the state-of-the-art for l(p) regression.	[Carmon, Yair; Jambulapati, Arun; Jiang, Qijia; Jin, Yujia; Sidford, Aaron; Tian, Kevin] Stanford Univ, Stanford, CA 94305 USA; [Carmon, Yair] Tel Aviv Univ, Tel Aviv, Israel; [Lee, Yin Tat] Univ Washington, Seattle, WA 98195 USA	Stanford University; Tel Aviv University; University of Washington; University of Washington Seattle	Carmon, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yairc@stanford.edu; jmblpati@stanford.edu; qjiang2@stanford.edu; yujiajin@stanford.edu; yintat@uw.edu; sidford@stanford.edu; kjtian@stanford.edu			NSF CAREER Award [CCF-1844855, CCF-1749609]; NSF [CCF-1955039, CCF-1740551, DMS-1839116, DMS-2023166]; Sloan Research Fellowships; Packard Fellowships; Stanford Graduate Fellowships; PayPal; Microsoft; Microsoft Research Faculty Fellowships	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); Sloan Research Fellowships; Packard Fellowships; Stanford Graduate Fellowships; PayPal; Microsoft(Microsoft); Microsoft Research Faculty Fellowships(Microsoft)	Researchers on this project were supported by NSF CAREER Award CCF-1844855 and CCF-1749609, NSF Grant CCF-1955039, CCF-1740551, DMS-1839116 and DMS-2023166, two Sloan Research Fellowships and Packard Fellowships, and two Stanford Graduate Fellowships. Additional support was provided by PayPal and Microsoft, including two Microsoft Research Faculty Fellowships and a PayPal research gift.	Adil D., 2019, P ADV NEUR INF PROC, V32, P14166, DOI DOI 10.1137/1.9781611975482.86; Adil D, 2020, PROCEEDINGS OF THE THIRTY-FIRST ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA'20), P892; Adil Deeksha, 2019, S DISCR ALG SODA, P1405; Agarwal N, 2017, J MACH LEARN RES, V18; Agarwal Naman, 2017, ARXIV171108426; Bach F, 2010, ELECTRON J STAT, V4, P384, DOI 10.1214/09-EJS521; Bubeck S, 2018, ACM S THEORY COMPUT, P1130, DOI 10.1145/3188745.3188776; Bubeck Sebastien, 2019, ADV NEURAL INFORM PR, P13900; Bullins Brian, 2019, ARXIV190601621; Bullins Brian, 2018, ARXIV181210349; Carmon Y., 2019, ADV NEURAL INFORM PR, P11377; Carmon Yair, 2019, MATH PROGRAMMING; Chi-Chih Yao A., 1977, 18th Annual Symposium on Foundations of Computer Science, P222; Cohen MB, 2018, PR MACH LEARN RES, V80; Conn A., 2000, MPS SIAM SERIES OPTI; Diakonikolas Jelena, 2019, C LEARN THEOR COLT, P1132; Ene Alina, 2019, INT C MACH LEARN, P1794; Gasnikov A., 2019, P 32 C LEARN THEOR, V99, P1392; Guzman C, 2015, J COMPLEXITY, V31, P1, DOI 10.1016/j.jco.2014.08.003; Hager WW, 2001, SIAM J OPTIMIZ, V12, P188, DOI 10.1137/S1052623499356071; Karimireddy S.P., 2018, GLOBAL LINEAR CONVER; Lin CJ, 2008, J MACH LEARN RES, V9, P627; Marteau-Ferey U., 2019, ADV NEURAL INFORM PR, P7636; Monteiro RDC, 2013, SIAM J OPTIMIZ, V23, P1092, DOI 10.1137/110833786; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Schmidt Mark, 2011, OPTIMIZATION MACHINE; Woodworth Blake, 2017, ARXIV170903594	31	1	1	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000016
C	Cho, J; Hwang, G; Suh, C		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Cho, Jaewoong; Hwang, Gyeongjo; Suh, Changho			A Fair Classifier Using Kernel Density Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					As machine learning becomes prevalent in a widening array of sensitive applications such as job hiring and criminal justice, one critical aspect in the design of machine learning classifiers is to ensure fairness: Guaranteeing the irrelevancy of a prediction to sensitive attributes such as gender and race. This work develops a kernel density estimation (KDE) methodology to faithfully respect the fairness constraint while yielding a tractable optimization problem that comes with high accuracy-fairness tradeoff. One key feature of this approach is that the fairness measure quantified based on KDE can be expressed as a differentiable function w.r.t. model parameters, thereby enabling the use of prominent gradient descent to readily solve an interested optimization problem. This work focuses on classification tasks and two well-known measures of group fairness: demographic parity and equalized odds. We empirically show that our algorithm achieves greater or comparable performances against prior fair classifers in accuracy-fairness tradeoff as well as in training stability on both synthetic and benchmark real datasets.	[Cho, Jaewoong; Hwang, Gyeongjo; Suh, Changho] Korea Adv Inst Sci & Technol, EE, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Cho, J (corresponding author), Korea Adv Inst Sci & Technol, EE, Daejeon, South Korea.	cjw2525@kaist.ac.kr; hkj4276@kaist.ac.kr; chsuh@kaist.ac.kr			Institute for Information & communications Technology Planning & Evaluation(IITP) - Korea government(MSIT) [2019-0-01396]	Institute for Information & communications Technology Planning & Evaluation(IITP) - Korea government(MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported by Institute for Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No. 2019-0-01396, Development of framework for analyzing, detecting, mitigating of bias in AI model and training data)	Agarwal A., 2018, P 35 INT C MACH LEAR; Angwin J., 2015, MACHINE BIAS THERES; Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31; Davis R. A., 2011, REMARKS SOME NONPARA; Donini M, 2018, ADV NEUR IN, V31; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Garg S., 2018, AAAI ACM C ART INT E; Geron Aurelien, 2017, HANDS MACHINE LEARNI; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Hardt M, 2016, ADV NEUR IN, V29; Huber Peter J, 1964, ANN MATH STAT; Jiang H., 2017, P 34 INT C MACH LEAR; Jiang R., 2019, P 35 C UNC ART INT U; Kamiran F., 2012, KNOWLEDGE INFORM SYS; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Kingma D.P, P 3 INT C LEARNING R; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Lopez-Benitez M., 2011, IEEE T COMMUNICATION; Lundberg SM, 2017, ADV NEUR IN, V30; Menon A. K., 2018, P 1 C FAIRN ACC TRAN; Nabi R, 2018, AAAI CONF ARTIF INTE, P1931; Narasimhan H., 2018, P 21 INT C ART INT S; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pleiss G, 2017, ADV NEURAL INFORM PR, V30, P5680; Roh Y., 2020, INT C MACH LEARN ICM; Russell C., 2017, ADV NEURAL INFORM PR, V30; Salimans T, 2016, ADV NEUR IN, V29; Scott D., 2005, DATA MINING DATA VIS; Sharifi-Malvajerdi S., 2019, ADV NEURAL INFORM PR; Silverman B. W, 1986, DENSITY ESTIMATION S, DOI 10.1201/9781315140919; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Wightman Linda F, 1998, LSAC NATL LONGITUDIN; Wu YJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3898; Xu D., 2018, IEEE INT C BIG DAT; Yeh I., 2009, EXPERT SYSTEMS APPL; Yurochkin M., 2020, P INT C LEARN REPR I; Zafar M. B., 2017, ART INT STAT C AISTA; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang BH, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P335, DOI 10.1145/3278721.3278779; Zhang J., 2018, P 32 AAAI C ART INT; Zhang J., 2018, ADV NEURAL INFORM PR, V31	46	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000074
C	Curi, S; Levy, KY; Jegelka, S; Krause, A		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Curi, Sebastian; Levy, Kfir Y.; Jegelka, Stefanie; Krause, Andreas			Adaptive Sampling for Stochastic Risk-Averse Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					In high-stakes machine learning applications, it is crucial to not only perform well on average, but also when restricted to difficult examples. To address this, we consider the problem of training models in a risk-averse manner. We propose an adaptive sampling algorithm for stochastically optimizing the Conditional Value-at-Risk (CVaR) of a loss distribution, which measures its performance on the ff fraction of most difficult examples. We use a distributionally robust formulation of the CVaR to phrase the problem as a zero-sum game between two players, and solve it efficiently using regret minimization. Our approach relies on sampling from structured Determinantal Point Processes (DPPs), which enables scaling it to large data sets. Finally, we empirically demonstrate its effectiveness on large-scale convex and non-convex learning tasks.	[Curi, Sebastian; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Levy, Kfir Y.] Technion, Fac Elect Engn, Haifa, Israel; [Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; Technion Israel Institute of Technology; Massachusetts Institute of Technology (MIT)	Curi, S (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	scuri@inf.ethz.ch; kfirylevy@technion.ac.il; stefje@mit.edu; krausea@inf.ethz.ch		Levy, Kfir Yehuda/0000-0003-1236-2626	European Research Council (ERC) under the European Unions Horizon 2020 research, innovation programme grant [815943]; DARPA YFA award; NSF CAREER award [1553284]	European Research Council (ERC) under the European Unions Horizon 2020 research, innovation programme grant(European Research Council (ERC)); DARPA YFA award; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research, innovation programme grant agreement No 815943, a DARPA YFA award, and NSF CAREER award 1553284.	Agarwal N., 2018, ARXIV181007362; Ahmadi-Javid A, 2012, J OPTIMIZ THEORY APP, V155, P1105, DOI 10.1007/s10957-011-9968-2; Alatur P, 2020, J MACH LEARN RES, V21; Anari N, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1015; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Barthelme S., 2019, ASYMPTOTIC EQUIVALEN, P3555; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Borsos Z, 2019, PR MACH LEARN RES, V97; Brown DB, 2007, OPER RES LETT, V35, P722, DOI 10.1016/j.orl.2007.01.001; Brownlees C, 2015, ANN STAT, V43, P2507, DOI 10.1214/15-AOS1350; Bubeck S, 2012, PROC C LEARN THEORY, P41; Carneiro MC, 2010, IND ENG CHEM RES, V49, P3286, DOI 10.1021/ie901265n; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chow Y., 2017, J MACHINE LEARNING R, V18, P6070; Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111; Derezinski M., 2019, ARXIV190513476; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Duchi J., 2016, STAT ROBUST OPTIMIZA; Eaton J. P., 1995, TITANIC TRIUMPH TRAG; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Gotoh J.-y., 2016, FINANCIAL SIGNAL PRO, P233; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185; Kingma D.P, P 3 INT C LEARNING R; Kirschner J., 2020, 23 INT C ART INT STA; Koolen W. M., 2010, P 23 ANN C LEARN THE, P93; Krizhevsky A., 2010, THE CIFAR 10 DATASET; Krokhmal P., 2002, J RISK, V4, P43, DOI DOI 10.21314/JOR.2002.057; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Lee J., 2020, ARXIV200608138; Mehrabi N, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3457607; Mhammedi Z., 2020, ARXIV200614763; Murty K.G., 1983, LINEAR PROGRAMMING; MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948; Namkoong H, 2017, ADV NEURAL INFORM PR, V30, P2971; Namkoong H, 2016, ADV NEURAL INFORM PR, P2208; Nemirovski A, 2006, SIAM J OPTIMIZ, V17, P969, DOI 10.1137/050622328; Ogryczak W, 2003, INFORM PROCESS LETT, V85, P117, DOI 10.1016/S0020-0190(02)00370-8; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pratt J.W, 1978, UNCERTAINTY EC, P59; Rabin M., 2013, HDB FUNDAMENTALS FIN, P241; Rockafellar R., 2000, J RISK, V2, P21, DOI 10.21314/JOR.2000.038; Rubino G, 2009, RARE EVENT SIMULATIO; Sani A., 2012, ADV NEURAL INFORM PR, V25, P3275; Shalev-Shwartz S, 2016, PR MACH LEARN RES, V48; Soma T., 2020, STAT LEARNING CONDIT; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szepesvdri C., 2018, BANDIT ALGORITHMS; Tarnopolskaya T., 2010, ANZIAM, V52, P237; Uchiya T, 2010, LECT NOTES ARTIF INT, V6331, P375, DOI 10.1007/978-3-642-16108-7_30; VAPNIK V, 1992, ADV NEUR IN, V4, P831; Williamson R.C., 2019, ARXIV190108665; Xiao H., 2017, FASHION MNIST NOVEL; Zimmerman DW, 1997, J EDUC BEHAV STAT, V22, P349, DOI 10.3102/10769986022003349; Zinkevich, 2003, P 20 INT C MACH LEAR, P928	67	1	1	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000044
C	Deng, W; Lin, G; Liang, FM		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Deng, Wei; Lin, Guang; Liang, Faming			A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				MONTE-CARLO; APPROXIMATION; CONVERGENCE; ERGODICITY	We propose an adaptively weighted stochastic gradient Langevin dynamics algorithm (SGLD), so-called contour stochastic gradient Langevin dynamics (CSGLD), for Bayesian learning in big data statistics. The proposed algorithm is essentially a scalable dynamic importance sampler, which automatically flattens the target distribution such that the simulation for a multi-modal distribution can be greatly facilitated. Theoretically, we prove a stability condition and establish the asymptotic convergence of the self-adapting parameter to a unique fixed-point, regardless of the non-convexity of the original energy function; we also present an error analysis for the weighted averaging estimators. Empirically, the CSGLD algorithm is tested on multiple benchmark datasets including CIFAR10 and CIFAR100. The numerical results indicate its superiority over the existing state-of-the-art algorithms in training deep neural networks.	[Deng, Wei; Lin, Guang] Purdue Univ, Dept Math, W Lafayette, IN 47907 USA; [Lin, Guang] Purdue Univ, Sch Mech Engn, W Lafayette, IN 47907 USA; [Liang, Faming] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Liang, FM (corresponding author), Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA.	weideng056@gmail.com; guanglin@purdue.edu; fmliang@purdue.edu	Lin, Guang/ABB-2145-2021	Lin, Guang/0000-0002-0976-1987	NSF [DMS-1555072, DMS-1736364]; BNL [382247, W911NF-15-1-0562, DE-SC0021142];  [DMS-2015498];  [R01-GM117597];  [R01-GM126089]	NSF(National Science Foundation (NSF)); BNL; ; ; 	Liang's research was supported in part by the grants DMS-2015498, R01-GM117597 and R01-GM126089. Lin acknowledges the support from NSF (DMS-1555072, DMS-1736364), BNL Subcontract 382247, W911NF-15-1-0562, and DE-SC0021142.	Ahn Sungjin, 2012, P INT C MACH LEARN I; Andrieu C, 2005, SIAM J CONTROL OPTIM, V44, P283, DOI 10.1137/S0363012902417267; Andrieu C, 2006, ANN APPL PROBAB, V16, P1462, DOI 10.1214/105051606000000286; BERG BA, 1991, PHYS LETT B, V267, P249, DOI 10.1016/0370-2693(91)91256-U; Chen C., 2015, NIPS; Chen Tianqi, 2014, P INT C MACH LEARN I; Deng Wei, 2020, P INT C MACH LEARN I; Deng Wei, 2020, ARXIV201001084; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; Fort G, 2011, ANN STAT, V39, P3262, DOI 10.1214/11-AOS938; Fort G, 2015, MATH COMPUT, V84, P2297, DOI 10.1090/S0025-5718-2015-02952-4; GEYER CJ, 1991, COMPUTING SCIENCE AND STATISTICS, P156; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Lang FM, 2010, ANN STAT, V38, P2823, DOI 10.1214/10-AOS807; Lelievre T, 2008, NONLINEARITY, V21, P1155, DOI 10.1088/0951-7715/21/6/001; Li K, 2020, HOLOCENE, V30, P1296, DOI 10.1177/0959683620919980; Li Xuechen, 2019, ADV NEURAL INFORM PR, P7746; Liang FM, 2007, J AM STAT ASSOC, V102, P305, DOI 10.1198/016214506000001202; Liang FM, 2009, STAT PROBABIL LETT, V79, P581, DOI 10.1016/j.spl.2008.10.007; Liang FM, 2005, J AM STAT ASSOC, V100, P1311, DOI 10.1198/016214505000000259; Ma Y.-A., 2015, ADV NEURAL INFORM PR; Mangoubi Oren, 2018, P C LEARN THEORY COL; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Paszke A., 2017, NEURIPS AUT WORKSH; Raginsky M., 2017, P C LEARN THEOR COLT; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roberts GO, 2007, J APPL PROBAB, V44, P458, DOI 10.1239/jap/1183667414; Saatci Y., 2017, ADV NEURAL INFORM PR, P3622; Simsekli U, 2016, PR MACH LEARN RES, V48; SWENDSEN RH, 1986, PHYS REV LETT, V57, P2607, DOI 10.1103/PhysRevLett.57.2607; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xu P., 2018, ADV NEURAL INFORM PR; Ye Mao, 2020, ARXIV200209070V1; Zhang Ruqi, 2020, P INT C LEARN REPR I	41	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE	34556969				2022-12-19	WOS:000627697000024
C	Kamath, GM; Baharav, TZ; Shomorony, I		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Kamath, Govinda M.; Baharav, Tavor Z.; Shomorony, Ilan			Adaptive Learning of Rank-One Models for Efficient Pairwise Sequence Alignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Pairwise alignment of DNA sequencing data is a ubiquitous task in bioinformatics and typically represents a heavy computational burden. State-of-the-art approaches to speed up this task use hashing to identify short segments (k-mers) that are shared by pairs of reads, which can then be used to estimate alignment scores. However, when the number of reads is large, accurately estimating alignment scores for all pairs is still very costly. Moreover, in practice, one is only interested in identifying pairs of reads with large alignment scores. In this work, we propose a new approach to pairwise alignment estimation based on two key new ingredients. The first ingredient is to cast the problem of pairwise alignment estimation under a general framework of rank-one crowdsourcing models, where the workers' responses correspond to k-mer hash collisions. These models can be accurately solved via a spectral decomposition of the response matrix. The second ingredient is to utilise a multi-armed bandit algorithm to adaptively refine this spectral estimator only for read pairs that are likely to have large alignments. The resulting algorithm iteratively performs a spectral decomposition of the response matrix for adaptively chosen subsets of the read pairs.	[Kamath, Govinda M.] Microsoft Res New England, Cambridge, MA 02142 USA; [Baharav, Tavor Z.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Shomorony, Ilan] Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA	Microsoft; Stanford University; University of Illinois System; University of Illinois Urbana-Champaign	Kamath, GM (corresponding author), Microsoft Res New England, Cambridge, MA 02142 USA.	gokamath@microsoft.com; tavorb@stanford.edu; ilans@illinois.edu			Alcatel Lucent Stanford Graduate Fellowship; NSF GRFP; NSF [CCF-2007597]	Alcatel Lucent Stanford Graduate Fellowship; NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF))	G.M. Kamath would like to thank Lester Mackey of Microsoft Research, New England Lab for useful discussions on Rank-one models and their connection to crowd-sourcing. The research of T. Baharav was supported by the Alcatel Lucent Stanford Graduate Fellowship and NSF GRFP. The research of I. Shomorony was supported in part by NSF grant CCF-2007597.	[Anonymous], 2017, ARXIV PREPRINT ARXIV; BAGARIA V, 2018, PR MACH LEARN RES, V84, pN2312; Bagaria V., 2020, ARXIV180508321; Baharav T., 2019, ADV NEURAL INFORM PR, P3650; Baharav Tavor Z., 2020, Research in Computational Molecular Biology. 24th Annual International Conference, RECOMB 2020. Proceedings. Lecture Notes in Bioinformatics. Subseries of Lecture Notes in Computer Science (LNBI 12074), P223, DOI 10.1007/978-3-030-45257-5_15; Berlin K, 2015, NAT BIOTECHNOL, V33, P623, DOI 10.1038/nbt.3238; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Busa-Fekete R., 2013, ICML; CHEN Y., 2016, INT C MACH LEARN, P689; Chin P., 2015, C LEARN THEOR, P391; Chum Ondrej, 2008, BMVC, V810, P812, DOI DOI 10.5244/C.22.50; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; Fan J., 2016, ARXIV160303516; Ghosh A, 2011, EURODYN, P1671; Heckel R, 2019, ANN STAT, V47, P3099, DOI 10.1214/18-AOS1772; Heckel R, 2018, PR MACH LEARN RES, V84; Heidrich-Meisner V., 2009, P 26 ANN INT C MACH, P401, DOI [10.1145/1553374.1553426, DOI 10.1145/1553374.1553426]; Jain C, 2017, LECT N BIOINFORMAT, V10229, P66, DOI 10.1007/978-3-319-56970-3_5; Jun KS, 2016, JMLR WORKSH CONF PRO, V51, P139; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Karger David R., 2013, Performance Evaluation Review, V41, P81; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Karnin Z., 2013, P 30 ICML, P1238; Li H, 2018, BIOINFORMATICS, V34, P3094, DOI 10.1093/bioinformatics/bty191; Li H, 2016, BIOINFORMATICS, V32, P2103, DOI 10.1093/bioinformatics/btw152; Liu Q., 2012, NIPS, P692; Liu Q, 2017, EUR J INORG CHEM, P2195, DOI 10.1002/ejic.201700044; Locatelli A., 2016, ARXIV160508671; Mahoney M. W., 2016, ARXIV160804845; Marcais G, 2019, BIOINFORMATICS, V35, pI127, DOI 10.1093/bioinformatics/btz354; Maron O., 1994, ADV NEURAL INFORM PR, V6, P59; Myers G., 2014, INT WORKSH ALG BIOIN, P52; Ondov BD, 2016, GENOME BIOL, V17, DOI 10.1186/s13059-016-0997-x; Pacific Biosciences Inc, 2013, PACB COL DAT; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Shah Nihar B, 2016, ARXIV160609632; Szorenyi B., 2015, ADV NEURAL INFORM PR, V28, P604; Tropp J. A., 2015, ARXIV150101571; Verdu S., 1998, MULTIUSER DETECTION; Weirather J. L, 2017, F1000RESEARCH, V6; Whitehill J., 2009, ADV NEURAL INFORM PR, P2035; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260; Zhou DY, 2014, PR MACH LEARN RES, V32, P262	49	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000040
C	Ma, QQ; Olshevsky, A		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Ma, Qianqian; Olshevsky, Alex			Adversarial Crowdsourcing Through Robust Rank-One Matrix Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				BOUNDS	We consider the problem of reconstructing a rank-one matrix from a revealed subset of its entries when some of the revealed entries are corrupted with perturbations that are unknown and can be arbitrarily large. It is not known which revealed entries are corrupted. We propose a new algorithm combining alternating minimization with extreme-value filtering and provide sufficient and necessary conditions to recover the original rank-one matrix. In particular, we show that our proposed algorithm is optimal when the set of revealed entries is given by an Erdos-Renyi random graph. These results are then applied to the problem of classification from crowdsourced data under the assumption that while the majority of the workers are governed by the standard single-coin David-Skene model (i.e., they output the correct answer with a certain probability), some of the workers can deviate arbitrarily from this model. In particular, the "adversarial" workers could even make decisions designed to make the algorithm output an incorrect answer. Extensive experimental results show our algorithm for this problem, based on rank-one matrix completion with perturbations, outperforms all other state-of-the-art methods in such an adversarial scenario.(1)	[Ma, Qianqian; Olshevsky, Alex] Boston Univ, Boston, MA 02215 USA	Boston University	Ma, QQ (corresponding author), Boston Univ, Boston, MA 02215 USA.	maqq@bu.edu; alexols@bu.edu		Olshevsky, Alex/0000-0002-5852-9789	NSF [1914792, 1933027]	NSF(National Science Foundation (NSF))	This work is supported by NSF awards 1914792 and 1933027.	Berend D., 2014, P 27 INT C NEUR INF, P3446; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Dagan I, 2006, LECT NOTES ARTIF INT, V3944, P177; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; DAWID AP, 1979, J ROY STAT SOC B MET, V41, P1; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DOLEV D, 1993, J ACM, V40, P17, DOI 10.1145/138027.138036; Fattahi S, 2020, J MACH LEARN RES, V21; Ghosh A, 2011, EURODYN, P1671; Hartsfield N., 2013, PEARLS GRAPH THEORY; Hendrickx J. M., 2020, P 23 INT C ART INT S; Hromkovi J., 2005, TEXT THEORET COMP S; Ibrahim S., 2019, ADV NEURAL INFORM PR, V32, P7845; Ipeirotis Panagiotis G., 2010, P ACM SIGKDD WORKSH, DOI [10.1145/1837885.1837906, DOI 10.1145/1837885.1837906]; Jagabathula S, 2017, J MACH LEARN RES, V18; Karger David R., 2013, Performance Evaluation Review, V41, P81; Khetan A., 2016, ADV NEURAL INFORM PR, P4844; Kiraly FJ, 2015, J MACH LEARN RES, V16, P1391; Kleindessner M., 2018, INT C MACH LEARN, P2708; Kordecki W., 1996, DISCUSS MATH GRAPH T, V16, P157; LANDAU HJ, 1981, LINEAR ALGEBRA APPL, V38, P5, DOI 10.1016/0024-3795(81)90003-3; Lease M., 2011, P TEXT RETR C; LeBlanc HJ, 2013, IEEE J SEL AREA COMM, V31, P766, DOI 10.1109/JSAC.2013.130413; Li H., 2014, ARXIV14114086; Liu G, 2014, INT CONF INSTR MEAS, P788, DOI 10.1109/IMCCC.2014.167; Liu Q., 2012, NIPS, P692; Loni Babak, 2013, P 4 ACM MULT SYST C, P72; Ma Y., 2018, P INT C MACH LEARN, P3335; Negahban S, 2012, J MACH LEARN RES, V13, P1665; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Pradhan Sameer, 2007, P 4 INT WORKSH SEM E, P87; Pustejovsky James, 2003, CORPUS LINGUISTICS, P647; Raykar VC, 2012, J MACH LEARN RES, V13, P491; Shah Nihar B, 2016, ARXIV160609632; Snow Rion, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751; Strapparava Carlo, 2007, P 4 INT WORKSH SEM E, P70, DOI DOI 10.3115/1621474.1621487; Xiao HP, 2019, IEEE T KNOWL DATA EN, V31, P575, DOI 10.1109/TKDE.2018.2837026; Zhang HT, 2015, IEEE T CONTROL NETW, V2, P310, DOI 10.1109/TCNS.2015.2413551; Zhang Y, 2016, 2016 INTERNATIONAL CONGRESS ON COMPUTATION ALGORITHMS IN ENGINEERING (ICCAE 2016), P126; Zhou D., 2012, P ADV NEUR INF PROC, P2195; Zhou D., 2012, ADV NEURAL INFORM PR, P2195; Zhou DY, 2014, PR MACH LEARN RES, V32, P262; Zhou Y., 2017, P 2017 SIAM INT C DA, P579; Zhou Y., 2016, IJCAI, P2435	46	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000057
C	Yang, JC; Zhang, SQ; Kiyavash, N; He, N		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Yang, Junchi; Zhang, Siqi; Kiyavash, Negar; He, Niao			A Catalyst Framework for Minimax Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				VARIATIONAL-INEQUALITIES; MONOTONE-OPERATORS; CONVERGENCE	We introduce a generic two-loop scheme for smooth minimax optimization with strongly-convex-concave objectives. Our approach applies the accelerated proximal point framework (or Catalyst) to the associated dual problem and takes full advantage of existing gradient-based algorithms to solve a sequence of well-balanced strongly-convex-strongly-concave minimax problems. Despite its simplicity, this leads to a family of near-optimal algorithms with improved complexity over all existing methods designed for strongly-convex-concave minimax problems. Additionally, we obtain the first variance-reduced algorithms for this class of minimax problems with finite-sum structure and establish faster convergence rate than batch algorithms. Furthermore, when extended to the nonconvex-concave minimax optimization, our algorithm again achieves the state-of-the-art complexity for finding a stationary point. We carry out several numerical experiments showcasing the superiority of the Catalyst framework in practice.	[Yang, Junchi; Zhang, Siqi; He, Niao] UIUC, Champaign, IL 61801 USA; [Kiyavash, Negar] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [He, Niao] Swiss Fed Inst Technol, Zurich, Switzerland	University of Illinois System; University of Illinois Urbana-Champaign; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Swiss Federal Institutes of Technology Domain; ETH Zurich	Yang, JC (corresponding author), UIUC, Champaign, IL 61801 USA.	junchiy2@illinois.edu; siqiz4@illinois.edu; negar.kiyavash@epfl.ch; niao.he@inf.ethz.ch			ONR [W911NF-15-1-0479]; NSF [CCF-1704970, CMMI-1761699]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This work was supported in part by ONR grant W911NF-15-1-0479, NSF CCF-1704970, and NSF CMMI-1761699.	[Anonymous], 2018, PROC INT C MACH LEAR; Azizian Waiss, 2019, ARXIV190605945; Boyd S, 2004, CONVEX OPTIMIZATION; Carmon Yair, 2019, ARXIV190702056; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Chavdarova T., 2019, ADV NEURAL INFORM PR, V32, P393; Chen YM, 2014, SIAM J OPTIMIZ, V24, P1779, DOI 10.1137/130919362; Dai B, 2017, PR MACH LEARN RES, V54, P1458; Davis D., 2018, ARXIV180202988; Du S. S., 2018, ARXIV180201504; Facchinei F., 2007, FINITE DIMENSIONAL V; Frostig R, 2015, PR MACH LEARN RES, V37, P2540; Garnaev A, 2009, L N INST COMP SCI SO, V19, P142; Gidel G., 2018, ARXIV180210551; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He N, 2015, COMPUT OPTIM APPL, V61, P275, DOI 10.1007/s10589-014-9723-3; Kang M, 2015, COMPUT OPTIM APPL, V62, P373, DOI 10.1007/s10589-015-9742-8; Koepelevich, 1976, EKONOMIKA MATEMATICH, V12, P747; Kong W., 2019, ARXIV190513433; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Lin H., 2017, THE J, V18, P7854; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lin Tianyi, 2020, ARXIV200202417; Liu T, 2018, ADV NEURAL INFORM PR, P8366; Lu S., 2020, IEEE T SIGNAL PROCES; Luo L., 2020, ARXIV200103724; Luo L., 2019, ARXIV190906946; Luo ZQ, 2007, SIAM J OPTIMIZ, V18, P1, DOI 10.1137/050642691; Madry A., 2018, ARXIV PREPRINT ARXIV; Mokhtari A., 2019, ARXIV190108511; Monteiro RDC, 2010, SIAM J OPTIMIZ, V20, P2755, DOI 10.1137/090753127; Namkoong Hongseok, 2017, NEURIPS, P2971; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; Ostrovskii Dmitrii M, 2020, ARXIV200207919; Palaniappan B., 2016, NEURIPS, P1416; PAQUETTE C, 2017, ARXIV170310993; REN ZX, 2019, MATH PROBL ENG, P1; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Thekumparampil K.K., 2019, ADV NEURAL PROCESSIN, P12659; TSENG P, 1995, J COMPUT APPL MATH, V60, P237, DOI 10.1016/0377-0427(94)00094-H; Wang J., UNIFIED MIN MAX FRAM; Xie Z., 2019, ARXIV190607691; Yang J., 2020, ARXIV200209621; Zhang Junyu, 2019, ARXIV191207481; Zhao R., 2020, ARXIV200304375	51	1	1	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000011
C	Abboud, A; Cohen-Addad, V; Houdrouge, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Abboud, Amir; Cohen-Addad, Vincent; Houdrouge, Hussein			Subquadratic High-Dimensional Hierarchical Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the widely-used average-linkage, single-linkage, and Ward's methods for computing hierarchical clusterings of high-dimensional Euclidean inputs. It is easy to show that there is no efficient implementation of these algorithms in high dimensional Euclidean space since it implicitly requires to solve the closest pair problem, a notoriously difficult problem. However, how fast can these algorithms be implemented if we allow approximation? More precisely: these algorithms successively merge the clusters that are at closest average (for average-linkage), minimum distance (for single-linkage), or inducing the least sum-of-square error (for Ward's). We ask whether one could obtain a significant running-time improvement if the algorithm can merge a-approximate closest clusters (namely, clusters that are at distance (average, minimum, or sum-of-square error) at most gamma times the distance of the closest clusters). We show that one can indeed take advantage of the relaxation and compute the approximate hierarchical clustering tree using (O) over tilde (n) gamma-approximate nearest neighbor queries. This leads to an algorithm running in time (O) over tilde (nd) + n(1+O(1/gamma)) for d-dimensional Euclidean space. We then provide experiments showing that these algorithms perform as well as the non-approximate version for classic classification tasks while achieving a significant speed-up.	[Abboud, Amir] IBM Res, Yorktown Hts, NY 10598 USA; [Cohen-Addad, Vincent] CNRS, Paris, France; [Cohen-Addad, Vincent] Sorbonne Univ, Paris, France; [Houdrouge, Hussein] Ecole Polytech, Palaiseau, France	International Business Machines (IBM); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite; Institut Polytechnique de Paris	Abboud, A (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	amir.abboud@gmail.com; vcohenad@gmail.com; hussein.houdrouge@polytechnique.edu						Alman J, 2015, ANN IEEE SYMP FOUND, P136, DOI 10.1109/FOCS.2015.18; Andoni A., 2014, P 25 ANN ACM SIAM S, P1018, DOI DOI 10.1137/1.9781611973402.76; Andoni A, 2015, ACM S THEORY COMPUT, P793, DOI 10.1145/2746539.2746553; Backurs Arturs, 2017, ADV NEURAL INFORM PR, P4311; Balcan MF, 2008, ACM S THEORY COMPUT, P671; Borodin A., 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P435, DOI 10.1145/301250.301367; Breyne P, 2001, CURR OPIN PLANT BIOL, V4, P136, DOI 10.1016/S1369-5266(00)00149-7; Carlsson G, 2010, J MACH LEARN RES, V11, P1425; Charikar M, 2019, P 30 ANN ACM SIAM S, P2291, DOI DOI 10.1137/1.9781611975482.139; Charikar M., 2018, ARXIV181210582; Charikar M, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P841; Chen K, 2009, SIAM J COMPUT, V39, P923, DOI 10.1137/070699007; Cochez M, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P505, DOI 10.1145/2723372.2751521; Cohen-Addad V, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P378; Cohen-Addad V, 2017, ADV NEUR IN, V30; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dasgupta S, 2015, ARXIV151005043; Diez I, 2015, SCI REP-UK, V5, DOI 10.1038/srep10532; Franti P, 2006, IEEE T PATTERN ANAL, V28, P1875, DOI 10.1109/TPAMI.2006.227; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Impagliazzo R, 2001, J COMPUT SYST SCI, V63, P512, DOI 10.1006/jcss.2001.1774; Impagliazzo R, 2001, J COMPUT SYST SCI, V62, P367, DOI 10.1006/jcss.2000.1727; Jeon Y, 2017, IEEE ACCESS, V5, P5594, DOI 10.1109/ACCESS.2017.2690987; Karthik C. S., 2019, 10 INN THEOR COMP SC; Kull M, 2008, BIODATA MIN, V1, DOI 10.1186/1756-0381-1-9; Leskovec J, 2014, MINING OF MASSIVE DATASETS, 2ND EDITION, P1; Moseley B, 2017, ADV NEUR IN, V30; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; MURTAGH F, 1983, COMPUT J, V26, P354, DOI 10.1093/comjnl/26.4.354; MURTAGH F, 1992, IEEE T PATTERN ANAL, V14, P1056, DOI 10.1109/34.159908; Otair Dr, 2013, ARXIV13031951; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Resende Mauricio GC, 2013, HDB MASSIVE DATA SET, V4; Roy A., 2016, ADV NEURAL INFORM PR, P2316; Rubinstein A, 2018, ACM S THEORY COMPUT, P1260, DOI 10.1145/3188745.3188916; Schutze H, 1997, PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P74, DOI 10.1145/278459.258539; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Steinbach M, 2000, KDD WORKSH TEXT MIN, V400, P525, DOI DOI 10.1109/CICN.2014.123; Tumminello M, 2010, J ECON BEHAV ORGAN, V75, P40, DOI 10.1016/j.jebo.2010.01.004; Yildirim P, 2017, ADV ELECTR COMPUT EN, V17, P77, DOI 10.4316/AECE.2017.04010	41	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903024
C	Abdolshah, M; Shilton, A; Rana, S; Gupta, S; Venkatesh, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Abdolshah, Majid; Shilton, Alistair; Rana, Santu; Gupta, Sunil; Venkatesh, Svetha			Multi-objective Bayesian optimisation with preferences over objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM	We present a multi-objective Bayesian optimisation algorithm that allows the user to express preference-order constraints on the objectives of the type "objective A is more important than objective B". These preferences are defined based on the stability of the obtained solutions with respect to preferred objective functions. Rather than attempting to find a representative subset of the complete Pareto front, our algorithm selects those Pareto-optimal points that satisfy these constraints. We formulate a new acquisition function based on expected improvement in dominated hypervolume (EHI) to ensure that the subset of Pareto front satisfying the constraints is thoroughly explored. The hypervolume calculation is weighted by the probability of a point satisfying the constraints from a gradient Gaussian Process model. We demonstrate our algorithm on both synthetic and real-world problems.	[Abdolshah, Majid; Shilton, Alistair; Rana, Santu; Gupta, Sunil; Venkatesh, Svetha] Deakin Univ, Appl Artificial Intelligence Inst A2I2, Geelong, Vic, Australia	Deakin University	Abdolshah, M (corresponding author), Deakin Univ, Appl Artificial Intelligence Inst A2I2, Geelong, Vic, Australia.	majid@deakin.edu.au; alistair.shilton@deakin.edu.au; santu.rana@deakin.edu.au; sunil.gupta@deakin.edu.au; svetha.venkatesh@deakin.edu.au		Shilton, Alistair/0000-0002-0849-3271; gupta, sunil/0000-0002-4669-9940; Abdolshah, Majid/0000-0001-5226-580X; Rana, Santu/0000-0003-2247-850X; venkatesh, svetha/0000-0001-8675-6631	Australian Government through the Australian Research Council (ARC); ARC Australian Laureate Fellowship [FL 170100006]	Australian Government through the Australian Research Council (ARC)(Australian Research Council); ARC Australian Laureate Fellowship(Australian Research Council)	This research was partially funded by Australian Government through the Australian Research Council (ARC). Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL 170100006).	Bernardo J., 2008, BAYESIAN STAT, V7, P651; Calandra R., 2014, P INT C LEARN INT OP, P274; Calandra R, 2016, ANN MATH ARTIF INTEL, V76, P5, DOI 10.1007/s10472-015-9463-9; Deb K., 2000, Parallel Problem Solving from Nature PPSN VI. 6th International Conference. Proceedings (Lecture Notes in Computer Science Vol.1917), P849; Deb K., 2005, SEARCH METHODOLOGIES, P273; Deb Kalyanmoy, 2014, SEARCH METHODOLOGIES, V4, P5, DOI DOI 10.1007/978-1-4614-6940-7_15; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Emmerich M., 2008, COMPUTATION EXPECTED, V34, P7; Feliot P, 2017, J GLOBAL OPTIM, V67, P97, DOI 10.1007/s10898-016-0427-3; Hernandez-Lobato D, 2016, PR MACH LEARN RES, V48; Huband S, 2003, IEEE C EVOL COMPUTAT, P2284; Hupkens I, 2015, LECT NOTES COMPUT SC, V9019, P65, DOI 10.1007/978-3-319-15892-1_5; Ilievski I, 2017, AAAI CONF ARTIF INTE, P822; Klein A., 2016, FAST BAYESIAN OPTIMI; Knowles J, 2006, IEEE T EVOLUT COMPUT, V10, P50, DOI 10.1109/TEVC.2005.851274; Laumanns M., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P298; Li C, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05723-0; O'Hagan A., 1992, BAYESIAN STAT, P345; Paria Biswajit, 2018, ABS180512168 CORR; Picheny V, 2015, STAT COMPUT, V25, P1265, DOI 10.1007/s11222-014-9477-x; Ponweiser W, 2008, LECT NOTES COMPUT SC, V5199, P784, DOI 10.1007/978-3-540-87700-4_78; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schaffer J.D., 1984, SOME EXPT MACHINE LE; Shilton Alistair, 2017, BAYESOPT2017 NIPS WO; Shir Ofer M., 2007, P 2007 IEEE C EV COM; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Van Moffaert K, 2013, IEEE SYMP ADAPT DYNA, P191, DOI 10.1109/ADPRL.2013.6615007; While L, 2006, IEEE T EVOLUT COMPUT, V10, P29, DOI 10.1109/TEVC.2005.851275; Zaefferer M, 2013, LECT NOTES COMPUT SC, V7811, P756, DOI 10.1007/978-3-642-37140-0_56; Zitzler E., 1999, EVOLUTIONARY ALGORIT, V63	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903081
C	Abernethy, J; Jung, YH; Lee, C; McMillan, A; Tewari, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Abernethy, Jacob; Jung, Young Hun; Lee, Chansoo; McMillan, Audra; Tewari, Ambuj			Online Learning via the Differential Privacy Lens	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we use differential privacy as a lens to examine online learning in both full and partial information settings. The differential privacy framework is, at heart, less about privacy and more about algorithmic stability, and thus has found application in domains well beyond those where information security is central. Here we develop an algorithmic property called one-step differential stability which facilitates a more refined regret analysis for online learning methods. We show that tools from the differential privacy literature can yield regret bounds for many interesting online learning problems including online convex optimization and online linear optimization. Our stability notion is particularly well-suited for deriving first-order regret bounds for follow-the-perturbed-leader algorithms, something that all previous analyses have struggled to achieve. We also generalize the standard max-divergence to obtain a broader class called Tsallis max-divergences. These define stronger notions of stability that are useful in deriving bounds in partial information settings such as multi-armed bandits and bandits with experts.	[Abernethy, Jacob] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Jung, Young Hun] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA; [Lee, Chansoo] Google Brain, Mountain View, CA USA; [McMillan, Audra] Boston Univ, Dept Comp Sci, Simons Inst Theory Comp, Boston, MA 02215 USA; [Tewari, Ambuj] Univ Michigan, Dept EECS, Dept Stat, Ann Arbor, MI 48109 USA	University System of Georgia; Georgia Institute of Technology; University of Michigan System; University of Michigan; Google Incorporated; Boston University; University of Michigan System; University of Michigan	Abernethy, J (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	prof@gatech.edu; yhjung@umich.edu; chansoo@google.com; audramarymcmillan@gmail.com; tewaria@umich.edu			NSF [CCF-1763786]; Sloan Foundation Research Award; NSF CAREER grant [IIS-1453304, IIS-1452099]; Sloan Research Fellowship; BU's Hariri Institute for Computing	NSF(National Science Foundation (NSF)); Sloan Foundation Research Award; NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Research Fellowship(Alfred P. Sloan Foundation); BU's Hariri Institute for Computing	Part of this work was done while AM was visiting the Simons Institute for the Theory of Computing. AM was supported by NSF grant CCF-1763786, a Sloan Foundation Research Award, and a postdoctoral fellowship from BU's Hariri Institute for Computing. AT and YJ were supported by NSF CAREER grant IIS-1452099. AT was also supported by a Sloan Research Fellowship. JA was supported by NSF CAREER grant IIS-1453304.	Abernethy J. D., 2015, ADV NEURAL INFORM PR, P2197; Abernethy J. D., 2014, COLT, P807; Agarwal Alekh, 2017, C LEARN THEOR, P4; Allen-Zhu Zeyuan, 2018, INT C MACH LEARN, P186; Allenberg C, 2006, LECT NOTES ARTIF INT, V4264, P229; Alon N, 2019, ACM S THEORY COMPUT, P852, DOI 10.1145/3313276.3316312; Amari S.-I., 2016, INFORM GEOMETRY ITS; [Anonymous], 2016, FDN TRENDS IN OPTIMI; [Anonymous], [No title captured]; [Anonymous], 2005, THESIS; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bassily R., 2016, STOC; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cummings Rachel, 2016, C LEARN THEOR, P23; Devroye L., 2013, P 25 ANN C LEARN THE, P460; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Dwork Cynthia, 2015, STOC; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hardt Moritz, 2016, INT C MACH LEARN; Jain P., 2012, P 25 ANN C LEARNING, V23; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kifer D., 2012, C LEARN THEOR, P25; McSherry Frank, 2007, FOCS; Neu G., 2015, C LEARN THEOR, V40, P1360; Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234; Nissim K., 2015, ARXIV150405800; ORABONA F, 2012, AISTATS, P823; PENOT JP, 1994, NONLINEAR ANAL-THEOR, V23, P689, DOI 10.1016/0362-546X(94)90212-7; Poggio T., 2011, ARXIV11054701; Rakhlin A., 2013, C LEARN THEOR; RAKHLIN A., 2012, ADV NEURAL INFORM PR, V25, P2141; Ross S., 2011, ARXIV11083154; Saha Ankan, 2012, ARXIV12116158; Srebro N., 2010, ADV NEURAL INFORM PR, P2199; Thakurta A., 2013, ADV NEURAL INFORM PR, P2733; Tossou Aristide Charles Yedia, 2017, AAAI; van Erven T., 2014, COLT, P949; Yu B, 2013, BERNOULLI, V19, P1484, DOI 10.3150/13-BEJSP14	42	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900048
C	Adil, D; Peng, R; Sachdeva, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Adil, Deeksha; Peng, Richard; Sachdeva, Sushant			Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REWEIGHTED LEAST-SQUARES; LAPLACIAN; RECOVERY; DESIGN; GRAPHS; IMAGE	Linear regression in l(p)-norm is a canonical optimization problem that arises in several applications, including sparse recovery, semi-supervised learning, and signal processing. Generic convex optimization algorithms for solving l(p)-regression are slow in practice. Iteratively Reweighted Least Squares (IRLS) is an easy to implement family of algorithms for solving these problems that has been studied for over 50 years. However, these algorithms often diverge for p > 3, and since the work of Osborne (1985), it has been an open problem whether there is an IRLS algorithm that is guaranteed to converge rapidly for p > 3. We propose p-IRLS, the first IRLS algorithm that provably converges geometrically for any p is an element of [2, infinity). Our algorithm is simple to implement and is guaranteed to find a high accuracy solution in a sub-linear number of iterations. Our experiments demonstrate that it performs even better than our theoretical bounds, beats the standard Matlab/CVX implementation for solving these problems by 10-50x, and is the fastest among available implementations in the high-accuracy regime.	[Adil, Deeksha; Sachdeva, Sushant] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada; [Peng, Richard] Georgia Inst Technol, Sch Comp Sci, Atlanta, GA 30332 USA	University of Toronto; University System of Georgia; Georgia Institute of Technology	Adil, D (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.	deeksha@cs.toronto.edu; rpeng@cc.gatech.edu; sachdeva@cs.toronto.edu			NSERC Discovery grant; Ontario Graduate Scholarship; Natural Sciences and Engineering Research Council of Canada (NSERC); Connaught New Researcher award; Google Faculty Research award; NSF [1637566, 1718533]	NSERC Discovery grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); Ontario Graduate Scholarship(Ontario Graduate Scholarship); Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); Connaught New Researcher award; Google Faculty Research award(Google Incorporated); NSF(National Science Foundation (NSF))	DA is supported by SS's NSERC Discovery grant and an Ontario Graduate Scholarship. SS is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), a Connaught New Researcher award, and a Google Faculty Research award. RP is partially supported by the NSF under Grants No. 1637566 and No. 1718533.	Adil D., 2019, ARXIV E PRINTS; Adil Deeksha, 2019, P 30 ANN ACM SIAM S, P1405, DOI DOI 10.1137/1.9781611975482.86; Alamgir Morteza, 2011, ADV NEURAL INFORM PR, P379; Alaoui A. E., 2016, P MACHINE LEARNING R, V49, P879; [Anonymous], 2016, PROC 27 ACM SIAM S D; BARRETO JA, 1994, INT CONF ACOUST SPEE, P545; Belkin M, 2004, LECT NOTES COMPUT SC, V3120, P624, DOI 10.1007/978-3-540-27819-1_43; Bi N, 2018, MATH METHOD APPL SCI, V41, P5481, DOI 10.1002/mma.5091; Boaz N., 2009, STAT ANAL SEMISUPERV; Bridle Nick, 2013, 11 WORKSH MIN LEARN; Bubeck S, 2018, ACM S THEORY COMPUT, P1130, DOI 10.1145/3188745.3188776; Bullins Brian, 2018, ARXIV181210349; Burrus C.S., ITERATIVE REWEIGHTED; BURRUS CS, 1994, IEEE T SIGNAL PROCES, V42, P2926, DOI 10.1109/78.330353; Calder J., 2017, ABS171010364 CORR; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Chartrand R, 2008, INT CONF ACOUST SPEE, P3869, DOI 10.1109/ICASSP.2008.4518498; Chierichetti F., 2017, P 34 INT C MACHINE L, V70, P806; Daubechies I, 2008, 2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3, P26, DOI 10.1109/CISS.2008.4558489; Daubechies I, 2010, COMMUN PUR APPL MATH, V63, P1, DOI 10.1002/cpa.20303; Elmoataz A, 2017, EUR J APPL MATH, V28, P922, DOI 10.1017/S0956792517000122; Elmoataz A, 2015, SIAM J IMAGING SCI, V8, P2412, DOI 10.1137/15M1022793; Ene A., 2019, P MACHINE LEARNING R, V97, P1794; Fornasier M, 2011, SIAM J OPTIMIZ, V21, P1614, DOI 10.1137/100811404; Gorodnitsky IF, 1997, IEEE T SIGNAL PROCES, V45, P600, DOI 10.1109/78.558475; Grant M., 2014, CVX MATLAB SOFTWARE; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Hafiene Y., 2018, ARXIV181012817; KAHNG SW, 1972, MATH COMPUT, V26, P505, DOI 10.2307/2005177; Karlovitz L., 1970, J APPROXIMATION THEO, V3, P123; Kyng R., 2015, COLT, P1190; Kyng R, 2019, ACM S THEORY COMPUT, P902, DOI 10.1145/3313276.3316410; Lawson Charles Lawrence, 1961, THESIS, P5; Maddison Chris J, 2018, ARXIV180905042; Nesterov Y., 1994, STUD APPL MATH, V13; Osborne M, 1985, FINITE ALGORITHMS OP; Rice J., 1964, ADDISON WESLEY SERIE; Rios M. F., 2019, LAPLACIAN LP GRAPH S; Rios M. F., 2019, ABS190105031 CORR; Slepcev D., 2017, ABS170706213 CORR; Straszak D., 2016, ABS160102712 CORR; Straszak D, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P291, DOI 10.1145/2840728.2840762; Vargas R. A., 2012, ABS12074526 CORR; Vargas RA, 1999, INT CONF ACOUST SPEE, P1129, DOI 10.1109/ICASSP.1999.756175; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhou X., 2011, P 14 INT C ART INT S, P892; Zhu X., 2005, TECHNICAL REPORT; Zhu Xiaojin., 2003, P ICLR, P912	49	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905080
C	Alafate, J; Freund, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alafate, Julaiti; Freund, Yoav			Faster Boosting with Smaller Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					State-of-the-art implementations of boosting, such as XGBoost and LightGBM, can process large training sets extremely fast. However, this performance requires that the memory size is sufficient to hold a 2-3 multiple of the training set size. This paper presents an alternative approach to implementing the boosted trees, which achieves a significant speedup over XGBoost and LightGBM, especially when the memory size is small. This is achieved using a combination of three techniques: early stopping, effective sample size, and stratified sampling. Our experiments demonstrate a 10-100 speedup over XGBoost when the training data is too large to fit in memory.	[Alafate, Julaiti; Freund, Yoav] Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Alafate, J (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.	jalafate@eng.ucsd.edu; yfreund@ucsd.edu		freund, yoav/0000-0002-3850-6184	NIH [U19 NS107466]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported by the NIH (grant U19 NS107466).	Agarwal A, 2014, J MACH LEARN RES, V15, P1111; Appel R., 2013, INT C MACH LEARN, P594, DOI DOI 10.5555/3042817.3043003; Balsubramani Akshay, 2014, ARXIV14052639CMATHST; Bradley J. K., 2007, P ADV NEUR INF PROC, V20, P185; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Domingo C, 2000, LECT NOTES ARTIF INT, V1805, P317; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gama J., 2003, P 9 ACM SIGKDD INT C, P523, DOI DOI 10.1145/956750.956813; Japan Agency for Marine-Earth Science and Technology JAMSTEC, 2016, DAT SAMPL RES SYST W; Ke G., 2017, P ADV NEURAL INFORM, V30, P3146; Kitagawa G., 1996, J OF COMPUTATIONAL A, V5, P1, DOI DOI 10.2307/1390750; Klabnik S., 2018, RUST PROGRAMMING LAN; Mason L, 2000, ADV NEUR IN, V12, P512; Schapire RE, 2012, ADAPT COMPUT MACH LE, P1; Shi H., 2007, THESIS; Sonnenburg S, 2010, P 27 INT C MACH LEAR, P999; Wald A., 1973, SEQUENTIAL ANAL	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903005
C	Alam, M; Gottschlich, J; Tatbul, N; Turek, J; Mattson, T; Muzahid, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alam, Mejbah; Gottschlich, Justin; Tatbul, Nesime; Turek, Javier; Mattson, Timothy; Muzahid, Abdullah			A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The field of machine programming (MP), the automation of the development of software, is making notable research advances. This is, in part, due to the emergence of a wide range of novel techniques in machine learning. In this paper, we apply MP to the automation of software performance regression testing. A performance regression is a software performance degradation caused by a code change. We present AutoPerf - a novel approach to automate regression testing that utilizes three core techniques: (i) zero-positive learning, (ii) autoencoders, and (iii) hardware telemetry. We demonstrate AutoPerf's generality and efficacy against 3 types of performance regressions across 10 real performance bugs in 7 benchmark and open-source programs. On average, AutoPerf exhibits 4% profiling overhead and accurately diagnoses more performance bugs than prior state-of-the-art approaches. Thus far, AutoPerf has produced no false negatives.	[Alam, Mejbah; Gottschlich, Justin; Tatbul, Nesime; Turek, Javier; Mattson, Timothy] Intel Labs, Santa Clara, CA 95054 USA; [Tatbul, Nesime] MIT, Cambridge, MA 02139 USA; [Muzahid, Abdullah] Texas A&M Univ, College Stn, TX 77843 USA	Intel Corporation; Massachusetts Institute of Technology (MIT); Texas A&M University System; Texas A&M University College Station	Alam, M (corresponding author), Intel Labs, Santa Clara, CA 95054 USA.	mejbah.alam@intel.com; justin.gottschlich@intel.com; tatbul@csail.mit.edu; javier.turek@intel.com; timothy.g.mattson@intel.com; abdullah.muzahid@tamu.edu						Adams A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322967; Ahmad MB, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356549; Alain G, 2014, J MACH LEARN RES, V15, P3563; [Anonymous], 1989, ACM COMPUT SURV, V21, P593, DOI DOI 10.1145/76894.76897; [Anonymous], 2016, IA ARCHITECTURES S B; Arulraj J, 2013, ACM SIGPLAN NOTICES, V48, P101, DOI 10.1145/2499368.2451128; Attariyan M., 2012, 10 USENIX S OPERATIN, P307; Bienia C, 2008, PACT'08: PROCEEDINGS OF THE SEVENTEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P72, DOI 10.1145/1454115.1454128; Brocanelli M, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190525; Cohen Ira, 2004, OSDI, V4, P16; Dean Daniel Joseph, 2012, P 9 INT C AUT COMP, P191; Eizenberg A, 2016, ACM SIGPLAN NOTICES, V51, P251, DOI [10.1145/2980983.2908090, 10.1145/2908080.2908090]; GAIT J, 1986, SOFTWARE PRACT EXPER, V16, P225, DOI 10.1002/spe.4380160304; Glek T., MASSIVE PERFORMANCE; Gottschlich J, 2018, P 2 ACM SIGPLAN INT, P69; Gottschlich J, 2013, INT CONFER PARA, P331, DOI 10.1109/PACT.2013.6618829; Gottschlich JE, 2012, INT CONFER PARA, P159; Greathouse JL, 2011, ISCA 2011: PROCEEDINGS OF THE 38TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P165, DOI 10.1145/2024723.2000084; Gu XH, 2009, PROC INT CONF DATA, P1000, DOI 10.1109/ICDE.2009.128; Han S, 2012, PROC INT CONF SOFTW, P145, DOI 10.1109/ICSE.2012.6227198; Huang L., 2010, SER NIPS 10, V23, P883; Huang P, 2014, 36TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2014), P60, DOI 10.1145/2568225.2568232; Jayasena S., 2013, P INT C HIGH PERF CO, P1; Jin GL, 2012, ACM SIGPLAN NOTICES, V47, P77, DOI 10.1145/2345156.2254075; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kory Becker, 2017, ABS170905703 CORR; Lee T. J., 2018, C SYST MACH LEARN SY; Lemieux C, 2018, ISSTA'18: PROCEEDINGS OF THE 27TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON SOFTWARE TESTING AND ANALYSIS, P254, DOI 10.1145/3213846.3213874; Li J., 2018, J EC SURV, V1, P14; Liu TP, 2016, INT SYM CODE GENER, P1, DOI 10.1145/2854038.2854039; Liu TP, 2014, ACM SIGPLAN NOTICES, V49, P3, DOI [10.1145/2555243.2555244, 10.1145/2692916.2555244]; Liu TP, 2011, OOPSLA 11: PROCEEDINGS OF THE 2011 ACM INTERNATIONAL CONFERENCE ON OBJECT ORIENTED PROGRAMMING SYSTEMS LANGUAGES AND APPLICATIONS, P3; Liu X, 2014, ACM SIGPLAN NOTICES, V49, P259, DOI [10.1145/2692916.2555271, 10.1145/2555243.2555271]; Loncaric C, 2018, PROCEEDINGS 2018 IEEE/ACM 40TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE), P958, DOI 10.1145/3180155.3180211; Luan S., 2019, P ACM PROGRAM LANG, V3; Luo L, 2016, INT S HIGH PERF COMP, P261, DOI 10.1109/HPCA.2016.7446070; Mandal S., 2019, LEARNING FITNESS FUN; Marcus R, 2019, PROC VLDB ENDOW, V12, P1705, DOI 10.14778/3342263.3342644; Moore S. V., 2002, INT C COMP SCI ICCS; Moya MM, 1996, NEURAL NETWORKS, V9, P463, DOI 10.1016/0893-6080(95)00120-4; Nanavati Mihir, 2013, P 8 ACM EUR C COMP S, P141, DOI DOI 10.1145/2465351.2465366; Nelson L., 2019, 27 ACM S OP SYST PRI; Nguyen LTH, 2012, BIOMATERIALS AND STEM CELLS IN REGENERATIVE MEDICINE, P299; Nistor A, 2013, IEEE WORK CONF MIN S, P237, DOI 10.1109/MSR.2013.6624035; Phothilimthana PM, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P65, DOI 10.1145/3297858.3304059; Rane A, 2012, INT CONFER PARA, P147; Ranger C, 2007, INT S HIGH PERF COMP, P13; Rui Yang, 2011, Proceedings of the 25th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2011), P1046, DOI 10.1109/IPDPS.2011.100; Song LH, 2014, ACM SIGPLAN NOTICES, V49, P561, DOI [10.1145/2714064.2660234, 10.1145/2660193.2660234]; Tan YM, 2012, INT CON DISTR COMP S, P285, DOI 10.1109/ICDCS.2012.65; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Zhang X., 2018, ADV NEURAL INFORM PR, P4879; Zhao Q F, 2011, MARINE GEOLOGY FRONT, V27, P11	53	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903028
C	Alet, F; Weng, E; Perez, TL; Kaelbling, LP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alet, Ferran; Weng, Erica; Perez, Tomas Lozano; Kaelbling, Leslie Pack			Neural Relational Inference with Fast Modular Meta-learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph neural networks (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. Relational inference is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a modular meta-learning problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on observed entities. To address the large search space of graph neural network compositions, we meta-learn a proposal function that speeds up the inner-loop simulated annealing search within the modular meta-learning algorithm, providing two orders of magnitude increase in the size of problems that can be addressed.	[Alet, Ferran; Weng, Erica; Perez, Tomas Lozano; Kaelbling, Leslie Pack] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Alet, F (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	alet@mit.edu; ericaw@mit.edu; tlp@mit.edu; lpk@mit.edu			NSF [1523767, 1723381]; AFOSR [FA9550-17-1-0165]; ONR [N00014-18-1-2847]; Honda Research Institute; SUTD Temasek Laboratories	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR(Office of Naval Research); Honda Research Institute; SUTD Temasek Laboratories	We gratefully acknowledge support from NSF grants 1523767 and 1723381; from AFOSR grant FA9550-17-1-0165; from ONR grant N00014-18-1-2847; from the Honda Research Institute; and from SUTD Temasek Laboratories. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.	Alet F., 2018, CORL, V87, P856; Alet Ferran, 2019, P 36 INT C MACH LEAR, V97; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrieu C, 2008, STAT COMPUT, V18, P343, DOI 10.1007/s11222-008-9110-y; [Anonymous], 2012, ARTIF INTELL; Battaglia Peter W, 2018, ARXIV 180601261; Battaglia Peter W, 2016, ARXIV161200222; Bechtle Sarah, 2019, ARXIV90605374; Bengio Yoshua, 2019, ARXIV190110912; Bengio Yoshua, 2019, CORR; Chang Michael B, 2016, ARXIV161200341; Chang Michael B, 2018, ARXIV180704640; Chitnis Rohan, 2018, ARXIV180907878; Clavera Ignasi, 2019, INT C LEARN REPR; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; Ellis Kevin, 2018, ADV NEURAL INFORM PR; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Finn C, 2017, ARXIV170303400; Franceschi L., 2019, ARXIV190311960; Garcia Victor, 2017, ARXIV171104043; Gilmer Justin, 2017, ARXIV170401212; Hamrick J.B., 2018, ARXIV180601203; Jin W., 2018, P 35 INT C MACHINE L; Johnson D.D., 2017, INT C LEARN REPR ICL; Kingma D.P, P 3 INT C LEARNING R; Kipf T, 2018, INT C MACH LEARN PML, P2688; Kipf TN, 2016, P INT C LEARN REPR; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li Ke, 2016, ARXIV160601885; Li Y., 2018, ARXIV; Li Y., 2019, INT C LEARN REPR; Liu Qi, 2018, NEURIPS; Meyerson Elliot, 2017, ARXIV171100108; Mishra N., 2018, INT C LEARN REPR ICL; Mrowca Damian, 2018, ARXIV180608047; Nichol Alex, 2018, CORR; Paszke Adam, 2017, INT C LEARN REPR; Pierrot T., 2019, ARXIV190512941; Rabinowitz N. C., 2018, ARXIV180207740; Ravi S., 2017, P INT C LEARN REPR, P1; Reed Scott, 2015, ICLR, V1, P5; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Schulman J., 2017, ABS170706347 CORR; Selvan R., 2018, ARXIV181108674; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41; Sun C., 2019, ARXIV190209641; Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788; Thrun S., 2012, LEARNING LEARN; Torrey L., 2010, HDB RES MACHINE LEAR, P242; VanSteenkiste S., 2018, P INT C LEARN REPR P; Vinyals Oriol, 2016, ARXIV160604080, P3630; von Humboldt Wilhelm, 1836, VERSCHIEDENHEIT MENS; Wang T., 2018, P INT C LEARN REPR V; Wang Tongzhou, 2018, ADV NEURAL INFORM PR, P4146; Wu Jianchao, 2019, ARXIV190410117; Wu Z., 2019, 190100596 ARXIV; Yu T., 2018, ARXIV180201557; Zhou J., 2018, ARXIV181208434; Zintgraf Luisa M, 2018, ARXIV181003642	61	1	1	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903044
C	Amin, K; Dick, T; Kulesza, A; Medina, AM; Vassilvitskii, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Amin, Kareem; Dick, Travis; Kulesza, Alex; Medina, Andres Mufioz; Vassilvitskii, Sergei			Differentially Private Covariance Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The task of privately estimating a covariance matrix is a popular one due to its applications to regression and PCA. While there are known methods for releasing private covariance matrices, these algorithms either achive only (epsilon,delta)-differential privacy or require very complicated sampling schemes, ultimately performing poorly in real data. In this work we propose a new E-differentially private algorithm for computing the covariance matrix of a dataset that addresses both of these limitations. We show that it has lower error than existing state-of-the-art approaches, both analytically and empirically. In addition, the algorithm is significantly less complicated than other methods and can be efficiently implemented with rejection sampling.	[Amin, Kareem; Kulesza, Alex; Medina, Andres Mufioz; Vassilvitskii, Sergei] Google Res NY, New York, NY 10011 USA; [Dick, Travis] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Google Incorporated; Carnegie Mellon University	Amin, K (corresponding author), Google Res NY, New York, NY 10011 USA.	kamin@google.com; tdick@cs.cmu.edu; kulesza@google.com; ammedina@google.com; sergeiv@google.com						Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chaudhuri Kamalika, 2012, NIPS; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P11, DOI 10.1145/2591796.2591883; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Imtiaz H, 2016, INT CONF ACOUST SPEE, P2339, DOI 10.1109/ICASSP.2016.7472095; Jain P., 2012, P 25 ANN C LEARNING, V23; Jiang WX, 2016, AAAI CONF ARTIF INTE, P1730; Kapralov M, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1395; Kent JT, 2018, J COMPUT GRAPH STAT, V27, P291, DOI 10.1080/10618600.2017.1390468; KIFER D, 2012, J MACH LEARN RES, V23, P1; McSherry F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P627; McSherry Frank, 2007, FOCS; Upadhyay J, 2018, ADV NEUR IN, V31; Wang Di, 2019, ABS190106413 CORR; Wang Yu-Xiang, 2018, P UAI	18	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905082
C	Aminmansour, F; Patterson, A; Le, L; Peng, YS; Mitchell, D; Pestilli, F; Caiafa, C; Greiner, R; White, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aminmansour, Farzane; Patterson, Andrew; Le, Lei; Peng, Yisu; Mitchell, Daniel; Pestilli, Franco; Caiafa, Cesar; Greiner, Russell; White, Martha			Learning Macroscopic Brain Connectomes via Group-Sparse Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TRACTOGRAPHY	Mapping structural brain connectomes for living human brains typically requires expert analysis and rule-based models on diffusion-weighted magnetic resonance imaging. A data-driven approach, however, could overcome limitations in such rule-based approaches and improve precision mappings for individuals. In this work, we explore a framework that facilitates applying learning algorithms to automatically extract brain connectomes. Using a tensor encoding, we design an objective with a group-regularizer that prefers biologically plausible fascicle structure. We show that the objective is convex and has a unique solution, ensuring identifiable connectomes for an individual. We develop an efficient optimization strategy for this extremely high-dimensional sparse problem, by reducing the number of parameters using a greedy algorithm designed specifically for the problem. We show that this greedy algorithm significantly improves on a standard greedy algorithm, called Orthogonal Matching Pursuit. We conclude with an analysis of the solutions found by our method, showing we can accurately reconstruct the diffusion information while maintaining contiguous fascicles with smooth direction changes.	[Aminmansour, Farzane; Patterson, Andrew; Mitchell, Daniel; Greiner, Russell; White, Martha] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [Le, Lei] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA; [Peng, Yisu] Northeastern Univ, Dept Comp Sci, Boston, MA 02115 USA; [Pestilli, Franco] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN USA; [Caiafa, Cesar] Consejo Nacl Invest Cient & Tecn, CIC PBA, CCT La Plata, Inst Argentino Radioastron, V Elisa, Argentina; [Caiafa, Cesar] RIKEN, RIKEN, Tensor Learning Unit, Tokyo, Japan	University of Alberta; Indiana University System; Indiana University Bloomington; Northeastern University; Indiana University System; Indiana University Bloomington; Consejo Nacional de Investigaciones Cientificas y Tecnicas (CONICET); Instituto Argentino de Radioastronomia; National University of La Plata; RIKEN	Aminmansour, F (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	aminmans@ualberta.ca; ap3@ualberta.ca; leile@iu.edu; peng.yis@husky.neu.edu; daniel7@ualberta.ca; franpest@indiana.edu; ccaiafa@gmail.com; rgreiner@ualberta.ca; whitem@ualberta.ca	Patterson, Andrew/AAG-7631-2021; White, Martha/AAF-7066-2020	White, Martha/0000-0002-5356-2950; Pestilli, Franco/0000-0002-2469-0494	NSERC; Amii; CIFAR; NSF [IIS-1636893, BCS-1734853, AOC 1916518]; NIH NCATS [UL1TR002529]; Microsoft Research Award; Google Cloud Platform; Indiana University Areas of Emergent Research initiative "Learning: Brains, Machines, Children	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Amii; CIFAR(Canadian Institute for Advanced Research (CIFAR)); NSF(National Science Foundation (NSF)); NIH NCATS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Advancing Translational Sciences (NCATS)); Microsoft Research Award(Microsoft); Google Cloud Platform(Google Incorporated); Indiana University Areas of Emergent Research initiative "Learning: Brains, Machines, Children	This research was funded by NSERC, Amii and CIFAR. Computing was generously provided by Compute Canada and Cybera.; F.P. was supported by NSF IIS-1636893, NSF BCS-1734853, NSF AOC 1916518, NIH NCATS UL1TR002529, a Microsoft Research Award, Google Cloud Platform, and the Indiana University Areas of Emergent Research initiative "Learning: Brains, Machines, Children.	Allen G., 2012, INT C ARTIFICIAL INT; Basser PJ, 2000, MAGNET RESON MED, V44, P625, DOI 10.1002/1522-2594(200010)44:4<625::AID-MRM17>3.0.CO;2-O; Bassett DS, 2017, NAT NEUROSCI, V20, P353, DOI 10.1038/nn.4502; Behrens T E J, 2003, MAGNETIC RESONANCE M; Benou I, 2019, LECT NOTES COMPUT SC, V11766, P626, DOI 10.1007/978-3-030-32248-9_70; Bonnefoy Antoine, 2015, IEEE T SIGNAL PROCES; Caiafa C. F., 2017, ADV NEURAL INFORM PR, P4340; Caiafa Cesar F, 2013, NEURAL COMPUTATION; Caruyer Emmanuel, 2012, MED IMAGE ANAL; Cichocki Andrzej, 2009, NONNEGATIVE MATRIX T, P2; Cichocki Andrzej, 2015, IEEE SIGNAL PROCESS; Daducci Alessandro, 2018, TECHNICAL REPORT; Das Abhimanyu, 2011, COMPUTING RES REPOSI; Descoteaux M., 2017, MED IMAGE COMPUTING, P540, DOI [10.1007/978-3-319-66182-7_62, DOI 10.1007/978-3-319-66182-7_62, DOI 10.1007/978-3-319-66182-762]; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Morup Morten, 2008, NEURAL COMPUTATION; Neher PF, 2015, LECT NOTES COMPUT SC, V9349, P45, DOI 10.1007/978-3-319-24553-9_6; Ohlson Martin, 2013, J MULTIVARIATE ANAL; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Pestilli F, 2014, NAT METHODS, V11, P1058, DOI 10.1038/nmeth.3098; Rokem A, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123272; Rubinstein R., 2008, CS TECH; Soltani Sara, 2015, ABS150604954 CORR; Swirszcz Grzegorz, 2009, ADV NEURAL INFORM PR; Tournier JD, 2012, INT J IMAG SYST TECH, V22, P53, DOI 10.1002/ima.22005; Tournier JD, 2004, NEUROIMAGE, V23, P1176, DOI 10.1016/j.neuroimage.2004.07.037; van den Heuvel MP, 2016, TRENDS COGN SCI, V20, P345, DOI 10.1016/j.tics.2016.03.001; van den Heuvel MP, 2011, J NEUROSCI, V31, P15775, DOI 10.1523/JNEUROSCI.3539-11.2011; Wandell BA, 2016, ANNU REV NEUROSCI, V39, P103, DOI 10.1146/annurev-neuro-070815-013815; Xiang Z. J., 2011, NIPS; Xu Yangyang, 2013, SIAM J IMAGING SCI; Zhang Zemin, 2015, ABS12026504 CORR; Zubair Syed, 2013, DSP	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900044
C	Anand, A; Racah, E; Ozair, S; Bengio, Y; Cote, MA; Hjelm, RD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Anand, Ankesh; Racah, Evan; Ozair, Sherjil; Bengio, Yoshua; Cote, Marc-Alexandre; Hjelm, R. Devon			Unsupervised State Representation Learning in Atari	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods.	[Anand, Ankesh; Racah, Evan; Ozair, Sherjil; Bengio, Yoshua; Hjelm, R. Devon] Univ Montreal, Mila, Montreal, PQ, Canada; [Anand, Ankesh; Cote, Marc-Alexandre; Hjelm, R. Devon] Microsoft Res, Redmond, WA 98052 USA	Universite de Montreal; Microsoft	Anand, A (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.; Anand, A (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	anandank@mila.quebec; racaheva@mila.quebec; ozairs@mila.quebec						Alain Guillaume, 2017, INT C LEARN REPR WOR; Amodei D, 2016, PR MACH LEARN RES, V48; Aytar Y., 2018, P ADV NEUR INF PROC, P2930; Bachman P., 2019, ARXIV190600910; Belghazi MI, 2018, PR MACH LEARN RES, V80; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bialek W., 1999, CONDMAT9902341 ARXIV; Burda Y., 2019, INT C LEARN REPR; Burgess Christopher P, 2019, ARXIV190111390; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Chen X, 2016, ADV NEUR IN, V29; Choi J., 2018, INT C LEARN REPR; ckovic P., 2018, INT C LEARN REPR; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Conneau Alexis, 2018, P 11 INT C LANG RES; Cuccu Giuseppe, 2019, INT C AUT AG MULT SY; des Combes RT., 2018, ARXIV180906848; Dinh L., 2017, INT C LEARN REPR ICL; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; DONSKER MD, 1983, COMMUN PUR APPL MATH, V36, P183, DOI 10.1002/cpa.3160360204; Duan W., 2017, THESIS; Dumoulin V., 2017, INT C LEARN REPR ICL; Eastwood C, 2018, INT C LEARN REPR ICL; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Friston KJ, 2005, PHILOS T R SOC B, V360, P815, DOI 10.1098/rstb.2005.1622; Gershman S.J., 2017, BEHAV BRAIN SCI, V40; Gordon RD, 1996, PERCEPT PSYCHOPHYS, V58, P1260, DOI 10.3758/BF03207558; Greff Klaus, 2019, ICML; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Ha D., 2018, ADV NEURAL INFORM PR, P2450; Higgins I., 2018, ARXIV PREPRINT ARXIV; Higgins I, 2017, PR MACH LEARN RES, V70; Hjelm R. Devon, 2019, ICLR; Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3; Hyvarinen A, 2004, INDEPENDENT COMPONEN, V46; Hyvarinen A.J., 2017, P MACH LEARN RES; Jentzsch Thomas, 2019, ATARIAGE ATARI 2600; Jonschkowski R, 2017, ARXIV170509805; Jonschkowski R, 2015, AUTON ROBOT, V39, P407, DOI 10.1007/s10514-015-9459-7; Kansky K, 2017, PR MACH LEARN RES, V70; Kim H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3606; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kolesnikov Alexander, 2019, P IEEE C COMP VIS PA; Konorski J., 1967, INTEGRATIVE ACTIVITY; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lesort T., 2018, NEURAL NETWORKS; Locatello Francesco, 2019, INT C MACH LEARN; Ma Zhuang, 2018, ARXIV180901812; Marr D., 1982, VISION COMPUTATIONAL; McAllester D., 2018, ARXIV181104251; Mnih V, 2013, PLAYING ATARI DEEP R; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Oh J., 2015, P ADV NEUR INF PROC, P2863; Oord A. v. d., 2018, ARXIV180703748; Ozair S, 2019, ADV NEUR IN, V32; Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pathak D., 2016, P IEEE C COMP VIS PA; Poole Ben, 2019, INT C MACH LEARN; Pschadka G, 2019, J GYNECOL SURG, V35, P253, DOI 10.1089/gyn.2019.0001; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Schulman J., 2017, ABS170706347 CORR; Sermanet P, 2018, IEEE INT CONF ROBOT, P1134; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sohn K., 2016, P NIPS, P1857, DOI DOI 10.5555/3157096.3157304; Such F.P., 2018, ARXIV PREPRINT ARXIV; Triantafillou Eleni, 2017, FEW SHOT LEARNING IN; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; van Hoof H, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P3928, DOI 10.1109/IROS.2016.7759578; Wang Alex, 2019, GLUE MULTITASK BENCH; Warde-Farley D., 2018, ARXIV181111359; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746; Whalen Z., 2008, HIST NOSTALGIA VIDEO; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Zhang A., 2018, ARXIV181106032; Zhu G., 2018, ADV NEURAL INFORM PR, P9804	81	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900037
C	Arora, R; Marinov, TV		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arora, Raman; Marinov, Teodor V.			Efficient Convex Relaxations for Streaming PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We revisit two algorithms, matrix stochastic gradient (MSG) and l(2)-regularized MSG (RMSG), that are instances of stochastic gradient descent (SGD) on a convex relaxation to principal component analysis (PCA). These algorithms have been shown to outperform Oja's algorithm, empirically, in terms of the iteration complexity, and to have runtime comparable with Oja's. However, these findings are not supported by existing theoretical results. While the iteration complexity bound for l(2)-RMSG was recently shown to match that of Oja's algorithm, its theoretical efficiency was left as an open problem. In this work, we give improved bounds on per iteration cost of mini-batched variants of both MSG and l(2)-RMSG and arrive at an algorithm with total computational complexity matching that of Oj a's algorithm.	[Arora, Raman; Marinov, Teodor V.] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA	Johns Hopkins University	Arora, R (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA.	arora@cs.jhu.edu; tmarino2@jhu.edu			NSF BIGDATA [IIS-1546482, IIS-1838139]	NSF BIGDATA	This research was supported, in part, by NSF BIGDATA grants IIS-1546482 and IIS-1838139.	Allen-Zhu Z., 2017, J MACH LEARN RES, V18, P8194; Allen-Zhu Z, 2017, ANN IEEE SYMP FOUND, P487, DOI 10.1109/FOCS.2017.51; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; [Anonymous], 2012, PROC INT C MACH LEAR; Arora R., INT C MACH LEARN, P1786; Arora R., 2017, P ANN C NEUR INF PRO, P4775; Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; Arora Raman, 2013, ADV NEURAL INFORM PR, P1815; Balcan M-F., 2016, C LEARNING THEORY, P284; Brand M, 2006, LINEAR ALGEBRA APPL, V415, P20, DOI 10.1016/j.laa.2005.07.021; De Sa Christopher, 2014, ARXIV14111134; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Garber D., 2018, ARXIV180910491; Grabowska M., 2018, COMPUTER INFORM SCI, P130, DOI [10.1007/978-3-030-00840-6_15, DOI 10.1007/978-3-030-00840-6_15]; Harvey N. J., 2018, ARXIV181205217; Jain Prateek, 2016, C LEARN THEOR, P1147; LeCun Y., 1996, MNIST DATABASE HANDW; Li CJ, 2018, MATH PROGRAM, V167, P75, DOI 10.1007/s10107-017-1182-z; Liu X, 2013, SIAM J SCI COMPUT, V35, pA1641, DOI 10.1137/120871328; Mianjy P., 2018, STOCHASTIC PCA, P3531; Shamir O., 2013, P INT C MACH LEARN A, P71; Shamir O, 2016, PR MACH LEARN RES, V48; Shamir O, 2016, PR MACH LEARN RES, V48; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Warmuth MK, 2008, J MACH LEARN RES, V9, P2287; Yu Y, 2015, BIOMETRIKA, V102, P315, DOI 10.1093/biomet/asv008	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902016
C	Aubin, B; Loureiro, B; Maillard, A; Krzakala, F; Zdeborova, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aubin, Benjamin; Loureiro, Bruno; Maillard, Antoine; Krzakala, Florent; Zdeborova, Lenka			The spiked matrix model with generative priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MUTUAL INFORMATION	Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks, such as GANs or variational auto-encoders, are particularly performant and are gaining on applicability. In this paper we study spiked matrix models, where a low-rank matrix is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here, we replace the sparsity assumption by generative modelling, and investigate the consequences on statistical and algorithmic properties. We analyze the Bayesoptimal performance under specific generative models for the spike. In contrast with the sparsity assumption, we do not observe regions of parameters where statistical performance is superior to the best known algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theory, showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets.	[Aubin, Benjamin; Loureiro, Bruno; Zdeborova, Lenka] Univ Paris Saclay, CNRS, CEA, Inst Phys Theor, F-91191 Gif Sur Yvette, France; [Maillard, Antoine; Krzakala, Florent] PSL Univ, Ecole Normale Super, Lab Phys, Paris, France; [Maillard, Antoine; Krzakala, Florent] CNRS, Paris, France; [Maillard, Antoine; Krzakala, Florent] Sorbonne Univ, Paris, France	CEA; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite	Aubin, B (corresponding author), Univ Paris Saclay, CNRS, CEA, Inst Phys Theor, F-91191 Gif Sur Yvette, France.				ERC under the European Union's Horizon 2020 Research and Innovation Program [714608-SMiLe]; French Agence Nationale de la Recherche [ANR-17-CE23-0023-01 PAIL]; NVIDIA Corporation; National Science Foundation [NSF PHY-1748958]; 'Chaire de recherche sur les modeles et sciences des donnees', Fondation CFM pour la Recherche-ENS	ERC under the European Union's Horizon 2020 Research and Innovation Program; French Agence Nationale de la Recherche(French National Research Agency (ANR)); NVIDIA Corporation; National Science Foundation(National Science Foundation (NSF)); 'Chaire de recherche sur les modeles et sciences des donnees', Fondation CFM pour la Recherche-ENS	This work is supported by the ERC under the European Union's Horizon 2020 Research and Innovation Program 714608-SMiLe, as well as by the French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. We thank Google Cloud for providing us access to their platform through the Research Credits Application program. We would also like to thank the Kavli Institute for Theoretical Physics (KITP) for welcoming us during part of this research, with the support of the National Science Foundation under Grant No. NSF PHY-1748958. We thank Ahmed El Alaoui for insightful discussions about the proof of the Bayes optimal performance, and Remi Monasson for his insightful lecture series that inspired partly this work. Additional funding is acknowledged by AM from 'Chaire de recherche sur les modeles et sciences des donnees', Fondation CFM pour la Recherche-ENS.	ALAOUI A. E., 2017, ARXIV171002903; Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Barbier J., 2016, ADV NEURAL INFORM PR, V29, P424; Barbier J, 2019, P NATL ACAD SCI USA, V116, P5451, DOI 10.1073/pnas.1802705116; Berthier Raphael, 2019, INFORM INFERENCE J I; Bora A, 2017, PR MACH LEARN RES, V70; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Deshpande Y, 2014, IEEE INT SYMP INFO, P2197, DOI 10.1109/ISIT.2014.6875223; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; El Alaoui A, 2018, IEEE INT SYMP INFO, P1874; Fletcher AK, 2018, IEEE INT SYMP INFO, P1884; Gabrie M, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo DN, 2005, IEEE T INFORM THEORY, V51, P1261, DOI 10.1109/TIT.2005.844072; Hand P, 2018, ADV NEUR IN, V31; Hand Paul, 2018, P 31 C LEARN THEOR, P970; Jenatton R, 2010, P INT C ART INT STAT, V9, P366; Krzakala F, 2016, 2016 IEEE INFORMATION THEORY WORKSHOP (ITW); Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Krzakala L., DEMONSTRATION CODES; Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733; Manoel A, 2017, IEEE INT SYMP INFO, P2098, DOI 10.1109/ISIT.2017.8006899; Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683; Miolane L., 2017, ARXIV170200473; Mixon Dustin G, 2018, ARXIV180309319; Mourrat J.-C., 2019, ARXIV190405294; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Perry A, 2018, ANN STAT, V46, P2416, DOI 10.1214/17-AOS1625; Rangan S., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1246, DOI 10.1109/ISIT.2012.6283056; Rigollet, 2013, C COMP LEARN THEOR; Tramel EW, 2016, 2016 IEEE INFORMATION THEORY WORKSHOP (ITW); Villar S., 2018, GENERATIVE MODELS AR; Xiao H., 2017, FASHION MNIST NOVEL; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	40	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900001
C	Bar-On, Y; Mansour, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bar-On, Yogev; Mansour, Yishay			Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PARALLEL ALGORITHM	We study agents communicating over an underlying network by exchanging messages, in order to optimize their individual regret in a common nonstochastic multi-armed bandit problem. We derive regret minimization algorithms that guarantee for each agent v an individual expected regret of (O) over tilde (root 1 + K/N(c)T), where T is the number of time steps, K is the number of actions and Ai (v) is the set of neighbors of agent v in the communication graph. We present algorithms both for the case that the communication graph is known to all the agents, and for the case that the graph is unknown. When the graph is unknown, each agent knows only the set of its neighbors and an upper bound on the total number of agents. The individual regret between the models differs only by a logarithmic factor. Our work resolves an open problem from [Cesa-Bianchi et al., 2019b].	[Bar-On, Yogev; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Mansour, Yishay] Google Res, Tel Aviv, Israel	Tel Aviv University; Google Incorporated	Bar-On, Y (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.	baronyogev@gmail.com; mansour.yishay@gmail.com			Yandex Initiative in Machine Learning; Israel Science Foundation (ISF)	Yandex Initiative in Machine Learning; Israel Science Foundation (ISF)(Israel Science Foundation)	This work was supported in part by the Yandex Initiative in Machine Learning and by a grant from the Israel Science Foundation (ISF).	ALON N, 1986, J ALGORITHM, V7, P567, DOI 10.1016/0196-6774(86)90019-2; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Avner Orly, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P66, DOI 10.1007/978-3-662-44848-9_5; Awerbuch B, 2008, J COMPUT SYST SCI, V74, P1271, DOI 10.1016/j.jcss.2007.08.004; Bistritz I., 2018, ADV NEURAL INFORM PR, V31, P7222; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2019, J MACH LEARN RES, V20; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cesa-Bianchi Nicolo, 2019, ARXIV190108082; Kar S, 2011, IEEE DECIS CONTR P, P1771; Kolla RK, 2018, IEEE ACM T NETWORK, V26, P1782, DOI 10.1109/TNET.2018.2852361; Landgren P, 2016, 2016 EUROPEAN CONTROL CONFERENCE (ECC), P243, DOI 10.1109/ECC.2016.7810293; Landgren P, 2016, IEEE DECIS CONTR P, P167, DOI 10.1109/CDC.2016.7798264; LUBY M, 1986, SIAM J COMPUT, V15, P1036, DOI 10.1137/0215074; Robert Busa-Fekete, 2013, J MACHINE LEARNING R, V2, P1056; Rosenski J, 2016, PR MACH LEARN RES, V48; Sahu AK, 2017, IEEE GLOB CONF SIG, P528; Seldin Y, 2014, PR MACH LEARN RES, V32; Wei V. K., 1981, TECHNICAL REPORT	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303014
C	Barik, A; Honorio, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Barik, Adarsh; Honorio, Jean			Learning Bayesian Networks with Low Rank Conditional Probability Tables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we provide a method to learn the directed structure of a Bayesian network using data. The data is accessed by making conditional probability queries to a black-box model. We introduce a notion of simplicity of representation of conditional probability tables for the nodes in the Bayesian network, that we call "low rankness". We connect this notion to the Fourier transformation of real valued set functions and propose a method which learns the exact directed structure of a 'low rank' Bayesian network using very few queries. We formally prove that our method correctly recovers the true directed structure, runs in polynomial time and only needs polynomial samples with respect to the number of nodes. We also provide further improvements in efficiency if we have access to some observational data.	[Barik, Adarsh; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Barik, A (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	abarik@purdue.edu; jhonorio@purdue.edu						Anderson T. W., 1962, TECHNICAL REPORT; [Anonymous], 2003, ICML; Bello K., 2018, ADV NEURAL INFORM PR, P10931; Cheng J, 2002, ARTIF INTELL, V137, P43, DOI 10.1016/S0004-3702(02)00191-1; Cussens J., 2008, UNCERTAINTY ARTIFICI; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Eaton D, 2007, P MACHINE LEARNING R, V2, P107; Friedman N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P206; Hausar A., 2012, P 6 EUR WORKSH PROB; He Y., 2008, J MACHINE LEARNING R; Higham N. J., 1994, SURVEY COMPONENTWISE, V48; Jaakkola T, 2010, P 13 INT C ART INT S, P358; Kocaoglu M., 2017, ADV NEURAL INFORM PR, V30, P7018; Koivisto M, 2004, J MACH LEARN RES, V5, P549; Margaritis D, 2000, ADV NEUR IN, V12, P505; Murphy K. P., 2001, TECHNICAL REPORT; Rauhut H, 2010, THEORETICAL FDN NUME, V9, P1, DOI DOI 10.1515/9783110226157.1; Silander T., 2006, P 22 C UNC ART INT, P445; Spirtes P., 2000, CAUSATION PREDICTION; Stobbe Peter, 2012, P 15 INT C ART INT S, P1125; Tong S., 2001, INT JOINT C ARTIFICI, P863; Triantafillou S., 2015, J MACHINE LEARNING R; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Xie XC, 2008, J MACH LEARN RES, V9, P459; Yehezkel R., 2005, ARTIFICIAL INTELLIGE, P429	26	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900054
C	Belakaria, S; Deshwal, A; Doppa, JR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Belakaria, Syrine; Deshwal, Aryan; Doppa, Janardhan Rao			Max-value Entropy Search for Multi-Objective Bayesian Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION; ALGORITHM	We consider the problem of multi-objective (MO) blackbox optimization using expensive function evaluations, where the goal is to approximate the true pareto-set of solutions by minimizing the number of function evaluations. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive computational simulations. In this paper, we propose a novel approach referred as Max-value Entropy Search for Multi-objective Optimization (MESMO) to solve this problem. MESMO employs an output-space entropy based acquisition function to efficiently select the sequence of inputs for evaluation to quickly uncover high-quality pareto-set solutions. We also provide theoretical analysis to characterize the efficacy of MESMO. Our experiments on several synthetic and real-world benchmark problems show that MESMO consistently outperforms the state-of-the-art algorithms.	[Belakaria, Syrine; Deshwal, Aryan; Doppa, Janardhan Rao] Washington State Univ, Sch EECS, Pullman, WA 99164 USA	Washington State University	Belakaria, S (corresponding author), Washington State Univ, Sch EECS, Pullman, WA 99164 USA.	syrine.belakaria@wsu.edu; aryan.deshwal@wsu.edu; jana.doppa@wsu.edu			National Science Foundation (NSF) [IIS-1845922, OAC-1910213]	National Science Foundation (NSF)(National Science Foundation (NSF))	The authors gratefully acknowledge the support from National Science Foundation (NSF) grants IIS-1845922 and OAC-1910213. The views expressed are those of the authors and do not reflect the official policy or position of the NSF.	Almer O, 2011, LECT NOTES COMPUT SC, V6566, P243, DOI 10.1007/978-3-642-19137-4_21; [Anonymous], 2009, ARXIV PREPRINT ARXIV; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Deb K, 2004, ADV INFO KNOW PROC, P105; Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017; Emmerich M., 2008, COMPUTATION EXPECTED, V34, P7; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato D, 2016, PR MACH LEARN RES, V48; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hoffman Matthew W, 2015, NIPS WORKSH BAYES OP; JONES DR, 1993, J OPTIMIZ THEORY APP, V79, P157, DOI 10.1007/BF00941892; Knowles J, 2006, IEEE T EVOLUT COMPUT, V10, P50, DOI 10.1109/TEVC.2005.851274; Kotthoff L, 2017, J MACH LEARN RES, V18; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Michalowicz J.V., 2013, HDB DIFFERENTIAL ENT, DOI DOI 10.1201/B15991; Nowak K, 2014, LECT NOTES COMPUT SC, V8672, P662; Okabe T, 2004, LECT NOTES COMPUT SC, V3242, P792; Picheny V, 2015, STAT COMPUT, V25, P1265, DOI 10.1007/s11222-014-9477-x; Ponweiser W, 2008, LECT NOTES COMPUT SC, V5199, P784, DOI 10.1007/978-3-540-87700-4_78; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Shah A., 2016, INT C MACH LEARN ICM, P1919; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Siegmund N, 2012, PROC INT CONF SOFTW, P167, DOI 10.1109/ICSE.2012.6227196; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Wang Z, 2016, JMLR WORKSH CONF PRO, V51, P1022; Wang Zi, 2017, P INT C MACH LEARN I; Williams Christopher KI, 2006, GAUSSIAN PROCESSES M, V2; Zitzler E., 1999, EVOLUTIONARY ALGORIT, V63; Zuluaga M, 2012, DES AUT CON, P1241; Zuluaga Marcela, 2013, P INT C MACH LEARN A, V28, P462	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307080
C	Belle, V; Juba, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Belle, Vaishak; Juba, Brendan			Implicitly Learning to Reason in First-Order Logic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LEARNABILITY	We consider the problem of answering queries about formulas of first-order logic based on background knowledge partially represented explicitly as other formulas, and partially represented as examples independently drawn from a fixed probability distribution. PAC semantics, introduced by Valiant, is one rigorous, general proposal for learning to reason in formal languages: although weaker than classical entailment, it allows for a powerful model theoretic framework for answering queries while requiring minimal assumptions about the form of the distribution in question. To date, however, the most significant limitation of that approach, and more generally most machine learning approaches with robustness guarantees, is that the logical language is ultimately essentially propositional, with finitely many atoms. Indeed, the theoretical findings on the learning of relational theories in such generality have been resoundingly negative. This is despite the fact that first-order logic is widely argued to be most appropriate for representing human knowledge. In this work, we present a new theoretical approach to robustly learning to reason in first-order logic, and consider universally quantified clauses over a countably infinite domain. Our results exploit symmetries exhibited by constants in the language, and generalize the notion of implicit learnability to show how queries can be computed against (implicitly) learned first-order background knowledge.	[Belle, Vaishak] Univ Edinburgh, Edinburgh, Midlothian, Scotland; [Belle, Vaishak] Alan Turing Inst, London, England; [Juba, Brendan] Washington Univ, St Louis, MO 63110 USA	University of Edinburgh; Washington University (WUSTL)	Belle, V (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.; Belle, V (corresponding author), Alan Turing Inst, London, England.	vaishak@ed.ac.uk; bjuba@wustl.edu			Royal Society University Research Fellowship; NSF [CCF-1718380]	Royal Society University Research Fellowship(Royal Society of London); NSF(National Science Foundation (NSF))	V. Belle was supported by a Royal Society University Research Fellowship. B. Juba was supported by NSF Award CCF-1718380. This work was partially performed while B. Juba was visiting the Simons Institute for the Theory of Computing. We thank our reviewers for their helpful suggestions.	Belle V, 2017, AAAI CONF ARTIF INTE, P3701; Billingsley P., 1995, WILEY SERIES PROBABI, V3rd; Cohen William W., 2016, ARXIV160506523; COHEN WW, 1995, NEW GENERAT COMPUT, V13, P369, DOI 10.1007/BF03037231; COHEN WW, 1994, MACH LEARN, V17, P169, DOI 10.1007/BF00993470; DERAEDT L, 1994, ARTIF INTELL, V70, P375, DOI 10.1016/0004-3702(94)90112-0; Evans R, 2018, J ARTIF INTELL RES, V61, P1; Giacomo G. D., 2011, P IJCAI 2011, P827; Juba B., 2013, P 23 INT JOINT C ART, P939; Juba B., 2012, ARXIV12090056; KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468; Kersting K., 2011, P 11 INT C LOGIC PRO, P1; Khardon R, 1999, MACH LEARN, V35, P95, DOI 10.1023/A:1007581123604; Lakemeyer G., 2002, KR, P73; Levesque H. J., 1998, KR, P14; Levesque HJ., 2001, LOGIC KNOWLEDGE BASE; LEVESQUE HJ, 1984, P AAAI AUST TX, V84, P198; Liu Y., 2005, AAAI, P639; Liu YM, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P860; Liu Yongmei, 2004, INT C PRINC KNOWL RE, P587; Manhaeve R., 2018, ADV NEURAL INFORM PR, P3749; Mccarthy J., 1969, MACHINE INTELLIGENCE, V4, DOI DOI 10.1016/B978-0-934613-03-3.50033-7; Michael L, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1525; Michael L, 2010, ARTIF INTELL, V174, P639, DOI 10.1016/j.artint.2010.03.004; Moore R.C., 1982, AAAI, P428; MUGGLETON S, 1994, J LOGIC PROGRAM, V20, P629, DOI 10.1016/0743-1066(94)90035-3; Rocktaschel T, 2017, ADV NEUR IN, V30; Shalev-Shwartz S., 2016, C LEARNING THEORY, P815; Srivastava S, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P742; Valiant LG, 2000, ARTIF INTELL, V117, P231, DOI 10.1016/S0004-3702(00)00002-3	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303038
C	Bhaskara, A; Wijewardena, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bhaskara, Aditya; Wijewardena, Maheshakya			On Distributed Averaging for Stochastic k-PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In the stochastic k-PCA problem, we are given i.i.d. samples from an unknown distribution over vectors, and the goal is to compute the top k eigenvalues and eigenvectors of the moment matrix. In the simplest distributed variant, we have m machines each of which receives n samples. Each machine performs some computation and sends an O(k)-size summary of the local dataset to a central server. The server performs an aggregation and computes the desired eigenvalues and vectors. The goal is to achieve the same effect as the server computing using mn samples by itself. The main choices in this framework are the choice of the summary, and the method of aggregation. We consider a slight variant of the well-studied distributedaveraging approach, and prove that this leads to significantly better bounds on the dependence between n and the eigenvalue gaps. Our method can also be applied directly to a setting where the 'right' value of the parameter k (i.e., one for which there is a non-trivial eigenvalue gap) is not known exactly. This is a common issue in practice which prior methods were unable to address.	[Bhaskara, Aditya; Wijewardena, Maheshakya] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA	Utah System of Higher Education; University of Utah	Bhaskara, A (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.	bhaskara@cs.utah.edu; pmaheshakya4@gmail.com						[Anonymous], 2011, PRINCIPAL COMPONENT; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; Bosq Denis, 2000, LINEAR PROCESSES FUN, P15; Boutsidis C, 2016, ACM S THEORY COMPUT, P236, DOI 10.1145/2897518.2897646; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Defferrard M., 2017, FMA DATASET MUSIC AN; Fan J., 2017, ARXIV170206488; Garber D, 2017, PR MACH LEARN RES, V70; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Kannan R., 2014, P 27 C LEARN THEOR C, P1040; Koltchinskii V., 2014, ARXIV14052468; MCDONALD R, 2010, HUMAN LANGUAGE TECHN, V2010, P456; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Perrone V, 2016, ARXIV161107460; Reiss M., 2016, ARXIV160903779; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Shamir O, 2015, PR MACH LEARN RES, V37, P144; Stewart G., 1990, MATRIX PERTURBATION; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Yann L., MNIST DATABASE HANDW; Zhang Y., 2012, ADV NEURAL INFORM PR, P1502; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902063
C	Bonnier, P; Kidger, P; Arribas, IP; Salvi, C; Lyons, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bonnier, Patric; Kidger, Patrick; Arribas, Imanol Perez; Salvi, Cristopher; Lyons, Terry			Deep Signature Transforms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The signature is an infinite graded sequence of statistics known to characterise a stream of data up to a negligible equivalence class. It is a transform which has previously been treated as a fixed feature transformation, on top of which a model may be built. We propose a novel approach which combines the advantages of the signature transform with modern deep learning frameworks. By learning an augmentation of the stream prior to the signature transform, the terms of the signature may be selected in a data-dependent way. More generally, we describe how the signature transform may be used as a layer anywhere within a neural network. In this context it may be interpreted as a pooling operation. We present the results of empirical experiments to back up the theoretical justification. Code available at github.com/patrick-kidger/Deep-Signature-Transforms.	[Bonnier, Patric; Kidger, Patrick; Arribas, Imanol Perez; Salvi, Cristopher; Lyons, Terry] Univ Oxford, Math Inst, Oxford, England; [Kidger, Patrick; Arribas, Imanol Perez; Salvi, Cristopher; Lyons, Terry] British Lib, Alan Turing Inst, London, England	University of Oxford	Bonnier, P (corresponding author), Univ Oxford, Math Inst, Oxford, England.	bonnier@maths.ox.ac.uk; kidger@maths.ox.ac.uk; perez@maths.ox.ac.uk; salvi@maths.ox.ac.uk; tlyons@maths.ox.ac.uk			EPSRC [EP/R513295/1, EP/L015811/1]; Alan Turing Institute under the EPSRC [EP/N510129/1]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	PB was supported by the EPSRC grant EP/R513295/1. PK was supported by the EPSRC grant EP/L015811/1. PK, IPA, CS, TL were supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1.	[Anonymous], LEARNING DELAYED REW; Arribas, 2019, ARXIV190500711; Arribas I. P., 2018, ARXIV180909466; Brockman G., 2016, OPENAI GYM; Chang J., 2018, THESIS; Chang JW, 2017, ELECTRON COMMUN PROB, V22, DOI 10.1214/17-ECP70; Chen K.-T., 1954, P LOND MATH SOC, V4, P502; Chen K.-T., 1958, T AM MATH SOC, P395, DOI [10.2307/1993193, DOI 10.1090/S0002-9947-1958-0106258-0]; CHEN KT, 1957, ANN MATH, V65, P163, DOI 10.2307/1969671; Chevyrev I, 2018, ARXIV PREPRINT ARXIV; Chevyrev I., 2016, PRIMER SIGNATURE MET; Dziugaite G. K., 2015, UAI; Embrechts P, 2009, SELFSIMILAR PROCESSE, V21; FRIZ P. K., 2010, CAMBRIDGE STUD ADV M, V120, DOI 10.1017/CBO9780511845079; Gashler M., 2016, NEUROCOMPUTING; Gatheral J, 2018, QUANT FINANC, V18, P933, DOI 10.1080/14697688.2017.1393551; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A., 2007, ADV NEURAL INFORM PR; Gyurko L.G., 2014, ARXIV13077244; Hambly B, 2010, ANN MATH, V171, P109, DOI 10.4007/annals.2010.171.109; Hurst H., 1951, T AM SOC CIVIL ENG; Kalsi J., 2019, ARXIV190500728; Kidger P., 2019, SIGNATORY DIFFERENTI; Kingma D.P., 2015, INT C LEARN REPR, P1; Kiraly F. J., 2019, J MACHINE LEARNING R; Lacasa L, 2009, EPL-EUROPHYS LETT, V86, DOI 10.1209/0295-5075/86/30001; Li CY, 2017, IEEE INT CONF COMP V, P631, DOI 10.1109/ICCVW.2017.80; Li Y., 2015, ICML; Lyons T., 2014, ICBDC; Lyons T., 2014, ARXIV14054537; Lyons T., 2019, ARXIV190501720; Lyons TJ, 2018, J EUR MATH SOC, V20, P1655, DOI 10.4171/JEMS/796; Lyons TJ, 1998, REV MAT IBEROAM, V14, P215, DOI 10.4171/RMI/240; Mingo L., 2004, INT J INF THEORY APP, V11; Mishura YS, 2008, LECT NOTES MATH, V1929, P1; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Reizenstein J., 2018, USIGNATURE LIB EFFIC; Silvescu A., 1999, P INT JOINT C NEUR N; Uhlenbeck GE, 1930, PHYS REV, V36, P0823, DOI 10.1103/PhysRev.36.823; Xie ZC, 2018, IEEE T PATTERN ANAL, V40, P1903, DOI 10.1109/TPAMI.2017.2732978; Yang W., 2017, ARXIV170703993; Yang WX, 2016, INT C PATT RECOG, P4083, DOI 10.1109/ICPR.2016.7900273; Yang WX, 2016, PATTERN RECOGN, V58, P190, DOI 10.1016/j.patcog.2016.04.007; Yang WX, 2016, IEEE INTELL SYST, V31, P45, DOI 10.1109/MIS.2016.22; Yang WX, 2015, PROC INT CONF DOC, P546, DOI 10.1109/ICDAR.2015.7333821; Zhang Q., 1992, IEEE T NEURAL NETW	48	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303013
C	Boob, D; Sawlani, S; Wang, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Boob, Digvijay; Sawlani, Saurabh; Wang, Di			Faster Width-dependent Algorithm for Mixed Packing and Covering LPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION ALGORITHMS; MINIMIZATION	In this paper, we give a faster width-dependent algorithm for mixed packing covering LPs. Mixed packing-covering LPs are fundamental to combinatorial optimization in computer science and operations research. Our algorithm finds a 1 + epsilon approximate solution in time O(Nw/epsilon) where N is number of nonzero entries in the constraint matrix, and tv is the maximum number of nonzeros in any constraint. This algorithm is faster than Nesterov's smoothing algorithm which requires O(N root nw/epsilon) time, where n is the dimension of the problem. Our work utilizes the framework of area convexity introduced in [Sherman-FOCS'17] to obtain the best dependence on epsilon while breaking the infamous l(infinity) barrier to eliminate the factor of root n. The current best width-independent algorithm for this problem runs in time O(N/epsilon(2)) [Young-arXiv-14] and hence has worse running time dependence on epsilon. Many real life instances of mixed packing -covering problems exhibit small width and for such cases, our algorithm can report higher precision results when compared to width-independent algorithms. As a special case of our result, we report a 1 + epsilon approximation algorithm for the densest subgraph problem which runs in time O(md/epsilon), where ni is the number of edges in the graph and d is the maximum graph degree.	[Boob, Digvijay; Sawlani, Saurabh] Georgia Tech, Atlanta, GA 30332 USA; [Wang, Di] Google AI, Atlanta, GA USA	University System of Georgia; Georgia Institute of Technology	Boob, D (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	digvijaybb40@gatech.edu; sawlani@gatech.edu; wadi@google.com						Allen-Zhu Z, 2019, MATH PROGRAM, V175, P307, DOI 10.1007/s10107-018-1244-x; Bahmani B, 2014, LECT NOTES COMPUT SC, V8882, P59, DOI [10.1007/978-3-319-13123-8_6, 10.1007/978-3-319-13123-8]; Bartal Y, 2004, SIAM J COMPUT, V33, P1261, DOI 10.1137/S0097539700379383; Bienstock D, 2006, SIAM J COMPUT, V35, P825, DOI 10.1137/S0097539705447293; Bubeck S., 2014, ARXIV PREPRINT ARXIV; Charikar M., 2000, Approximation Algorithms for Combinatorial Optimization. Third International Workshop, APPROX 2000. Proceedings (Lecture Notes in Computer Science Vol.1913), P84; Grigoriadis MD, 1996, MATH PROGRAM, V75, P477, DOI 10.1007/BF02592195; Guzman C, 2015, J COMPLEXITY, V31, P1, DOI 10.1016/j.jco.2014.08.003; Luby M., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P448, DOI 10.1145/167088.167211; Mahoney M.W., 2016, 43 INT C AUT LANG PR, P52; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; PLOTKIN SA, 1995, MATH OPER RES, V20, P257, DOI 10.1287/moor.20.2.257; Sherman J, 2017, ACM S THEORY COMPUT, P452, DOI 10.1145/3055399.3055501; Wang Di, 2016, 43 INT C AUT LANG PR, P50; Young NE, 2001, ANN IEEE SYMP FOUND, P538, DOI 10.1109/SFCS.2001.959930; Young Neal E., 2014, CORR; Zurel E., 2001, EC'01. Proceedings of the 3rd ACM Conference on Electronic Commerce, P125, DOI 10.1145/501158.501172	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906089
C	Cao, Y; Gu, QQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cao, Yuan; Gu, Quanquan			Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the sample complexity of learning one-hidden-layer convolutional neural networks (CNNs) with non-overlapping filters. We propose a novel algorithm called approximate gradient descent for training CNNs, and show that, with high probability, the proposed algorithm with random initialization grants a linear convergence to the ground-truth parameters up to statistical precision. Compared with existing work, our result applies to general non-trivial, monotonic and Lipschitz continuous activation functions including ReLU, Leaky ReLU, Sigmod and Soft-plus etc. Moreover, our sample complexity beats existing results in the dependency of the number of hidden nodes and filter size. In fact, our result matches the information-theoretic lower bound for learning one-hidden-layer CNNs with linear activation functions, suggesting that our sample complexity is tight. Our theoretical analysis is backed up by numerical experiments.	[Cao, Yuan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Cao, Y (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	yuancao@cs.ucla.edu; qgu@cs.ucla.edu			National Science Foundation CAREER Award [IIS-1906169, IIS-1903202]; Salesforce Deep Learning Research Award	National Science Foundation CAREER Award(National Science Foundation (NSF)); Salesforce Deep Learning Research Award	We thank the anonymous reviewers and area chair for their helpful comments. This research was sponsored in part by the National Science Foundation CAREER Award IIS-1906169, IIS-1903202, and Salesforce Deep Learning Research Award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Allen-Zhu Zeyuan, 2018, ARXIV181104918; [Anonymous], ARXIV181103962; Arora S, 2019, PR MACH LEARN RES, V97; Baum EB, 1990, NEURAL COMPUT, V2, P510, DOI 10.1162/neco.1990.2.4.510; Brutzkus A, 2017, PR MACH LEARN RES, V70; CAO Y., 2019, ARXIV190201384; Cohen N, 2016, PR MACH LEARN RES, V48; Cuadras CM, 2002, J MULTIVARIATE ANAL, V81, P19, DOI 10.1006/jmva.2001.2000; Du S. S., 2018, ARXIV180507798; Du S. S., 2018, ARXIV180507883; Du Simon S, 2018, GRADIENT DESCENT FIN; Du Simon S., 2018, INT C LEARN REPR; Du SS., 2019, P 7 INT C LEARN REPR; FU H., 2018, ARXIV180206463; Ge R., 2017, ARXIV PREPRINT ARXIV; Goel S, 2018, PR MACH LEARN RES, V80; Gunasekar S, 2018, ADV NEUR IN, V31; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hoeffding W., 1940, SCHR MATH I U BERLIN, V5, P181; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Janzamin M., 2015, ARXIV150608473; Klivans AR, 2009, LECT NOTES COMPUT SC, V5687, P588, DOI 10.1007/978-3-642-03685-9_44; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li YZ, 2018, ADV NEUR IN, V31; Mei S., 2016, ARXIV160706534; Mei S., 2018, ARXIV180406561; Nguyen Quynh, 2017, ARXIV171010928; SEN PK, 1994, COLLECTED WORKS W HO, P00029; SHAMIR O., 2016, ARXIV160901037; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; Tian Y., 2016, SYMMETRY BREAKING CO; Vershynin R., 2010, ARXIV10113027; Yi XY, 2015, ADV NEUR IN, V28; Zhang Xiao, 2018, ARXIV180607808; Zhang YT, 2016, PR MACH LEARN RES, V48; Zhong K., 2017, ARXIV171103440; Zhong K, 2017, PR MACH LEARN RES, V70; Zou D, 2018, ARXIV181108888	42	1	1	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902026
C	Cao, Y; Chen, TL; Wang, ZY; Shen, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cao, Yue; Chen, Tianlong; Wang, Zhangyang; Shen, Yang			Learning to Optimize in Swarms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PROTEIN; PREDICTION	Learning to optimize has emerged as a powerful framework for various optimization and machine learning tasks. Current such "meta-optimizers" often learn in the space of continuous optimization algorithms that are point-based and uncertainty-unaware. To overcome the limitations, we propose a meta-optimizer that learns in the algorithmic space of both point-based and population-based optimization algorithms. The meta-optimizer targets at a meta-loss function consisting of both cumulative regret and entropy. Specifically, we learn and interpret the update formula through a population of LSTMs embedded with sample- and feature-level attentions. Meanwhile, we estimate the posterior directly over the global optimum and use an uncertainty measure to help guide the learning process. Empirical results over non-convex test functions and the protein-docking application demonstrate that this new meta-optimizer outperforms existing competitors.	[Cao, Yue] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77840 USA; Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77840 USA	Texas A&M University System; Texas A&M University College Station; Texas A&M University System; Texas A&M University College Station	Cao, Y (corresponding author), Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77840 USA.	cyppsp@tamu.edu; wiwjp619@tamu.edu; atlaswang@tamu.edu; yshen@tamu.edu	Shen, Yang/A-7939-2012	Shen, Yang/0000-0002-1703-7796; Wang, Zhangyang/0000-0002-2050-5693	National Institutes of Health [R35GM124952]	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work is in part supported by the National Institutes of Health (R35GM124952 to YS). Part of the computing time is provided by the Texas A&M High Performance Research.	Andrychowicz Marcin, 2016, ADV NEURAL INFORM PR; [Anonymous], 2010, P INT C MACH LEARN; Bahdanau D., 2015, P 3 INT C LEARNING R; Bansal N., 2018, ADV NEURAL INFORM PR, P4261; BENGIO S, 1995, NEURAL PROCESS LETT, V2, P26, DOI 10.1007/BF02279935; Bengio Samy, 1992, C OPT ART BIOL NEUR, P6; Bengio Y., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), DOI 10.1109/IJCNN.1991.155621; Cao Y, 2019, ARXIV190200067; Chen Tianlong, 2019, ICCV; Chen X, 2018, INT CONF COMPUT NETW, P615; Chen YT, 2017, PR MACH LEARN RES, V70; Chiles J.P., 2012, WILEY SERIES PROBABI, V2nd; Gray Jeffrey J., 2003, J MOL BIOL; HARLOW HF, 1949, PSYCHOL REV, V56, P51, DOI 10.1037/h0062474; Hwang H, 2010, PROTEINS, V78, P3111, DOI 10.1002/prot.22830; Kingma D.P, P 3 INT C LEARNING R; Li Ke, 2016, ARXIV160601885; Liu J., 2019, ICLR; Moal IH, 2010, INT J MOL SCI, V11, P3623, DOI 10.3390/ijms11103623; Mosca R, 2013, NAT METHODS, V10, P47, DOI [10.1038/NMETH.2289, 10.1038/nmeth.2289]; Pierce BG, 2014, BIOINFORMATICS, V30, P1771, DOI 10.1093/bioinformatics/btu097; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Ryu E.K., 2019, ICML; Smith GR, 2002, CURR OPIN STRUC BIOL, V12, P28, DOI 10.1016/S0959-440X(02)00285-3; Tieleman T., 2012, COURSERA NEURAL NETW; Wang Zhangyang, 2016, 13 AAAI C ART INT; Ward Lewis B, 1937, PSYCHOL MONOGR, V49, pi; Wichrowska O, 2017, PR MACH LEARN RES, V70; Yang X.H., 2009, CHINESE BUSINESS, V9, P5, DOI DOI 10.1016/J.PAIN.2009.04.005; Zoph B., 2016, ICLR	30	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906067
C	Celli, A; Marchesi, A; Bianchi, T; Gatti, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Celli, Andrea; Marchesi, Alberto; Bianchi, Tommaso; Gatti, Nicola			Learning to Correlate in Multi-Player General-Sum Sequential Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EQUILIBRIUM; POKER	In the context of multi-player, general-sum games, there is a growing interest in solution concepts involving some form of communication among players, since they can lead to socially better outcomes with respect to Nash equilibria and may be reached through learning dynamics in a decentralized fashion. In this paper, we focus on coarse correlatedequilibria(CCEs) in sequential games. First, we complete the picture on the complexity of finding social-welfare-maximizing CCEs by proving that the problem is not in Poly-APX, unless P = NP, in games with three or more players (including chance). Then, we provide simple arguments showing that CFR-working with behavioral strategies-may not converge to a CCE in multi-player, general-sum sequential games. In order to amend this issue, we devise two variants of CFR that provably converge to a CCE. The first one (CFR-S) is a simple stochastic adaptation of CFR which employs sampling to build a correlated strategy, whereas the second variant (called CFR-Jr) enhances CFR with a more involved reconstruction procedure to recover correlated strategies from behavioral ones. Experiments on a rich testbed of multi-player, general-sum sequential games show that both CFR-S and CFR-Jr are dramatically faster than the state-of-the-art algorithms to compute CCEs, with CFR-Jr being also a good heuristic to find socially-optimal CCEs.	[Celli, Andrea; Marchesi, Alberto; Bianchi, Tommaso; Gatti, Nicola] Politecn Milan, Milan, Italy	Polytechnic University of Milan	Celli, A (corresponding author), Politecn Milan, Milan, Italy.	andrea.celli@polimi.it; alberto.marchesi@polimi.it; tommaso4.bianchi@mail.polimi.it; nicola.gatti@polimi.it	Gatti, Nicola/AFS-7062-2022	Gatti, Nicola/0000-0001-7349-3932; CELLI, ANDREA/0000-0002-2046-4019	Italian MIUR PRIN 2017 Project ALGADIMAR "Algorithms, Games, and Digital Markets"	Italian MIUR PRIN 2017 Project ALGADIMAR "Algorithms, Games, and Digital Markets"	We would like to thank Gabriele Farina for his helpful feedback. This work has been partially supported by the Italian MIUR PRIN 2017 Project ALGADIMAR "Algorithms, Games, and Digital Markets".	[Anonymous], 2017, P ADV NEUR INF PROC; Aumann Robert, 1974, J MATH ECON, V1, P67, DOI 10.1016/0304-4068(74)90037-8; Ausiello Giorgio, 2012, COMPLEXITY APPROXIMA; Barman Siddharth, 2015, ACM C EC COMP EC, P815; Blackwell David, 1956, PAC J MATH, V6, P1, DOI [DOI 10.2140/PJM.1956.6.1, 10.2140/pjm.1956.6.1]; Blum Avrim, 2007, LEARNING REGRET MINI; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2018, ARXIV181100164; Brown N, 2019, AAAI CONF ARTIF INTE, P1829; Brown N, 2018, SCIENCE, V359, P418, DOI 10.1126/science.aao1733; Burch Neil, 2018, CORR; Celli A, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P909; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Farina G, 2019, AAAI CONF ARTIF INTE, P1917; Farina Gabriele, 2018, ADV NEURAL INFORM PR; FORGES F, 1993, THEOR DECIS, V35, P277, DOI 10.1007/BF01075202; Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595; Garey M.R., 1979, COMPUTERS INTRACTABI; Gibson Richard, 2013, CORR; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hartline Jason, 2015, ADV NEURAL INFORM PR, V28, P3061; Huang W, 2008, PROCEEDINGS OF THE TENTH INTERNATIONAL SYMPOSIUM ON STRUCTURAL ENGINEERING FOR YOUNG EXPERTS, VOLS I AND II, P506; Jafari A., 2001, ICML, V1, P226; Jiang AX, 2015, GAME ECON BEHAV, V91, P347, DOI 10.1016/j.geb.2013.02.002; Kuhn H. W., 1950, CONTRIBUTIONS THEORY, V1, P97; Moulin H., 1978, International Journal of Game Theory, V7, P201, DOI 10.1007/BF01769190; MYERSON RB, 1986, ECONOMETRICA, V54, P323, DOI 10.2307/1913154; NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529; Papadimitriou CH, 2008, J ACM, V55, DOI 10.1145/1379759.1379762; Risk N. A., 2010, P 9 INT C AUT AG MUL, V1, P159; ROSS SM, 1971, J APPL PROBAB, V8, P621, DOI 10.2307/3212187; Roughgarden T, 2009, ACM S THEORY COMPUT, P513; Shapley L., 1964, ADV GAME THEORY, V52, P1; Shoham Y., 2008, MULTIAGENT SYSTEMS A, DOI DOI 10.1017/CBO9780511811654; Southey F., 2005, P 21 ANN C UNC ART I; Tammelin O., 2014, ARXIV14075042; Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645; von Stengel B, 2008, MATH OPER RES, V33, P1002, DOI 10.1287/moor.1080.0340; Waugh Kevin, 2009, ABSTRACTION LARGE EX; Zinkevich M., 2008, ADV NEURAL INFORM PR, P1729; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	43	1	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904068
C	Chavdarova, T; Gidel, G; Fleuret, F; Lacoste-Julien, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chavdarova, Tatjana; Gidel, Gauthier; Fleuret, Francois; Lacoste-Julien, Simon			Reducing Noise in GAN Training with Variance Reduced Extragradient	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the effect of the stochastic gradient noise on the training of generative adversarial networks (GANs) and show that it can prevent the convergence of standard game optimization methods, while the batch version converges. We address this issue with a novel stochastic variance-reduced extragradient (SVRE) optimization algorithm, which for a large class of games improves upon the previous convergence rates proposed in the literature. We observe empirically that SVRE performs similarly to a batch method on MNIST while being computationally cheaper, and that SVRE yields more stable GAN training on standard datasets.	[Chavdarova, Tatjana; Gidel, Gauthier; Lacoste-Julien, Simon] Mila, Montreal, PQ, Canada; [Chavdarova, Tatjana; Gidel, Gauthier; Lacoste-Julien, Simon] Univ Montreal, Montreal, PQ, Canada; [Chavdarova, Tatjana; Fleuret, Francois] Idiap, Martigny, Switzerland; [Chavdarova, Tatjana; Fleuret, Francois] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Gidel, Gauthier] Element AI, Montreal, PQ, Canada	Universite de Montreal; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Chavdarova, T (corresponding author), Mila, Montreal, PQ, Canada.; Chavdarova, T (corresponding author), Univ Montreal, Montreal, PQ, Canada.; Chavdarova, T (corresponding author), Idiap, Martigny, Switzerland.; Chavdarova, T (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.				Canada CIFAR AI Chair Program; Canada Excellence Research Chair in "Data Science for Realtime Decision-making"; NSERC [RGPIN-2017-06936]; Hasler Foundation through the MEMUDE project; Google Focused Research Award	Canada CIFAR AI Chair Program; Canada Excellence Research Chair in "Data Science for Realtime Decision-making"; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Hasler Foundation through the MEMUDE project; Google Focused Research Award(Google Incorporated)	This research was partially supported by the Canada CIFAR AI Chair Program, the Canada Excellence Research Chair in "Data Science for Realtime Decision-making", by the NSERC Discovery Grant RGPIN-2017-06936, by the Hasler Foundation through the MEMUDE project, and by a Google Focused Research Award. Authors would like to thank Compute Canada for providing the GPUs used for this research. TC would like to thank Sebastian Stich and Martin Jaggi, and GG and TC would like to thank Hugo Berard for helpful discussions.	Allen-Zhu Zeyuan, 2016, ICML; [Anonymous], ICML; Azizian Waiss, 2019, ARXIV190605945; Bottou L., 2010, COMPSTAT; Boyd S, 2004, CONVEX OPTIMIZATION; Brock Andrew, 2019, ICLR; Daskalakis C., 2018, ICLR; Davis D, 2016, ARXIV160100698; DEFAZIO A, 2014, NIPS; Defazio Aaron, 2018, ARXIV181204529; Facchinei F., 2003, SPRING S OPERAT RES, VI; Gidel G., 2019, INT C LEARNING REPRE; Gidel G., 2019, AISTATS; Glorot X., 2010, P 13 INT C ART INT S, VVolume 9, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Harker Patrick T, 1990, MATH PROGRAMMING; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heusel M., 2017, NIPS; Hofmann T., 2015, NIPS; Ioffe S., 2015, P 32 INT C INT C MAC, V37, P448; Iusem AN, 2017, SIAM J OPTIMIZATION; JOHNSON R., 2013, NIPS; Juditsky Anatoli, 2011, STOCHASTIC SYSTEMS; Kingma D.P., 2015, INT C LEARN REPR, P1; KORPELEVICH GM, 1976, MATECON; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Leblond R., 2018, JMLR, V19, P1; LeCun Yann, MNIST DATABASE HANDW; Lim Jae Hyun, 2017, ABS170502894 ARXIV; Mescheder L., 2017, NIPS; Miyato Takeru, 2018, ICLR; Netzer Y., 2011, P NIPS WORKSH DEEP L; Palaniappan B., 2016, NIPS; Radford A., 2016, ICLR; Reddi S. J., 2016, ICML; Robbins H., 1951, ANN MATH STAT; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans Tim, 2016, ADV NEURAL INFORM PR; Schmidt M., 2017, MATH PROGRAMMING; Shallue Christopher J, 2018, ARXIV181103600; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tseng Paul, 1995, J COMPUTATIONAL APPL; Wilson A. C., 2017, NIPS; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang H., 2018, ARXIV180508318	45	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300036
C	Chen, MM; Gummadi, R; Harris, C; Schuurmans, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Minmin; Gummadi, Ramki; Harris, Chris; Schuurmans, Dale			Surrogate Objectives for Batch Policy Optimization in One-step Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RISK	We investigate batch policy optimization for cost-sensitive classification and contextual bandits-two related tasks that obviate exploration but require generalizing from observed rewards to action selections in unseen contexts. When rewards are fully observed, we show that the expected reward objective exhibits suboptimal plateaus and exponentially many local optima in the worst case. To overcome the poor landscape, we develop a convex surrogate that is calibrated with respect to entropy regularized expected reward. We then consider the partially observed case, where rewards are recorded for only a subset of actions. Here we generalize the surrogate to partially observed data, and uncover novel objectives for batch contextual bandit training. We find that surrogate objectives remain provably sound in this setting and empirically demonstrate state-of-the-art performance.	[Chen, Minmin; Gummadi, Ramki; Harris, Chris; Schuurmans, Dale] Google, Mountain View, CA 94043 USA; [Schuurmans, Dale] Univ Alberta, Edmonton, AB, Canada	Google Incorporated; University of Alberta	Chen, MM (corresponding author), Google, Mountain View, CA 94043 USA.							[Anonymous], 2013, ICML 3; Bartlett Peter L., 2006, J AM STAT ASS, V101; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Chen M, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P456, DOI 10.1145/3289600.3290999; Dmochowski JP, 2010, J MACH LEARN RES, V11, P3313; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Elkan C., 2001, INT JOINT C ART INT, P973; Farajtabar M., 2018, ICML 18, P1446; Haarnoja T., 2018, INT C MACH LEARN, P1856; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011; Joachims Thorsten, 2018, P INT C LEARN REPR I; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lawrence C, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1820; Lawrence Carolin, 2017, P 2017 C EMP METH NA, P2566, DOI DOI 10.18653/V1/D17-1272; Lefortier Damien, 2016, CORR; Lin Hsuan-Tien, 2014, P AS C MACH LEARN AC; Ma Yifei, 2019, P INT C ART INT STAT; Mohan K, 2013, ADV NEUTRAL INFORM P, V26, P1277; Nachum O., 2018, INT C LEARN REPR; O'Brien D., 2008, P 25 INT C MACHINE L, P712, DOI [10.1145/1390156.1390246, DOI 10.1145/1390156.1390246]; Pereyra Gabriel, 2017, CORR; Pires B.A., 2013, P 30 INT C MACH LEAR, V28, P1391; Reid MD, 2011, J MACH LEARN RES, V12, P731; Rosasco L, 2004, NEURAL COMPUT, V16, P1063, DOI 10.1162/089976604773135104; Schnabel T, 2016, PR MACH LEARN RES, V48; Schulman J., 2017, CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Swaminathan A, 2015, PR MACH LEARN RES, V37, P814; Swaminathan A, 2015, ADV NEUR IN, V28; Swaminathan A, 2015, J MACH LEARN RES, V16, P1731; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Thomas P., 2016, INT C MACH LEARN, P2139; Vaswani Ashish, 2018, CORR; Wang Y. -X., 2017, P 34 INT C MACH LEAR, P3589; Xie Yuan, 2019, P INT C LEARN REPR I; Zhang Tong, 2004, J MACHINE LEARNING R, V5; Zhou Z., 2018, CORR	43	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900042
C	Cheung, B; Terekhov, A; Chen, YB; Agrawal, P; Olshausen, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cheung, Brian; Terekhov, Alex; Chen, Yubei; Agrawal, Pulkit; Olshausen, Bruno			Superposition of many models into one	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.	[Cheung, Brian; Chen, Yubei; Olshausen, Bruno] Univ Calif Berkeley, Redwood Ctr, BAIR, Berkeley, CA 94701 USA; [Terekhov, Alex; Agrawal, Pulkit] Univ Calif Berkeley, BAIR, Berkeley, CA 94701 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley	Cheung, B (corresponding author), Univ Calif Berkeley, Redwood Ctr, BAIR, Berkeley, CA 94701 USA.	bcheung@berkeley.edu; aterekhov@berkeley.edu; yubeic@berkeley.edu; pulkitag@berkeley.edu; baolshausen@berkeley.edu						Frankle Jonathan, 2018, ARXIV180303635; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Han S., 2015, 4 INT C LEARN REPR; Han X., 2017, ARXIVCSLGCSLG1708077; Kanerva P, 2009, COGN COMPUT, V1, P139, DOI 10.1007/s12559-009-9009-8; Kirkpatrick J., 2017, P NATL ACAD SCI USA; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Li C., 2018, P 6 INT C LEARN REPR; Liu Zhuang, 2018, ARXIV181005270; Mallya Arun, 2018, P EUR C COMP VIS ECC, P67; Masse NY, 2018, P NATL ACAD SCI USA, V115, pE10467, DOI 10.1073/pnas.1803839115; Mezzadri F., 2006, NOT AM MATH SOC, V54; Mnih V., 2013, ARXIV PREPRINT ARXIV; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Rusu A. A., 2016, PROGR NEURAL NETWORK; Serr Joan, 2018, ARXIV180101423; Terekhov AV, 2015, LECT NOTES ARTIF INT, V9222, P268, DOI 10.1007/978-3-319-22979-9_27; Wan L., 2013, P INT C MACHINE LEAR, P1058; Zenke F., 2017, ARXIV170304200; Zhang Chiyuan, 2016, ARXIV161103530	23	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902049
C	Cosentino, J; Zhu, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cosentino, Justin; Zhu, Jun			Generative Well-intentioned Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose Generative Well-intentioned Networks (GWINs), a novel framework for increasing the accuracy of certainty-based, closed-world classifiers. A conditional generative network recovers the distribution of observations that the classifier labels correctly with high certainty. We introduce a reject option to the classifier during inference, allowing the classifier to reject an observation instance rather than predict an uncertain label. These rejected observations are translated by the generative network to high-certainty representations, which are then relabeled by the classifier. This architecture allows for any certainty-based classifier or rejection function and is not limited to multilayer perceptrons. The capability of this framework is assessed using benchmark classification datasets and shows that GWINs significantly improve the accuracy of uncertain observations.	[Cosentino, Justin; Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, Inst AI,BNRist Ctr, THBI Lab,State Key Lab Intell Tech & Sys, Beijing, Peoples R China	Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, Dept Comp Sci & Tech, Inst AI,BNRist Ctr, THBI Lab,State Key Lab Intell Tech & Sys, Beijing, Peoples R China.	justin@cosentino.io; dcszj@mail.tsinghua.edu.cn		Cosentino, Justin/0000-0003-1012-9556	National Key Research and Development Program of China [2017YFA0700904]; NSFC [61620106010, 61621136008, 61571261]; Beijing NSF Project [L172037]; Beijing Academy of Artificial Intelligence (BAAI); Tiangong Institute for Intelligent Computing; JP Morgan Faculty Research Program; NVIDIA NVAIL Program; GPU/DGX Acceleration	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Beijing NSF Project; Beijing Academy of Artificial Intelligence (BAAI); Tiangong Institute for Intelligent Computing; JP Morgan Faculty Research Program; NVIDIA NVAIL Program; GPU/DGX Acceleration	This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008, 61571261), Beijing NSF Project (No. L172037), Beijing Academy of Artificial Intelligence (BAAI), Tiangong Institute for Intelligent Computing, the JP Morgan Faculty Research Program, and the NVIDIA NVAIL Program with GPU/DGX Acceleration.	[Anonymous], 2018, INT C LEARN REPR; [Anonymous], 2014, EXPLAINING HARNESSIN; Antoniou A, 2017, ARXIV; Arjovsky M., 2017, P 34 INT C MACH LEAR, P214; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5; DILLON J. V., 2017, TENSORFLOW DISTRIBUT; Donahue J., 2017, INT C LEARN REPR; Dumoulin V, 2017, ICLR; Gal Y., 2015, BAYESIAN CONVOLUTION; Gal Y., 2016, U CAMBRIDGE; Gal Y., 2016, ADV NEURAL INFORM PR, P1019; Gal Y, 2016, PR MACH LEARN RES, V48; Geifman Y., 2019, P INT C MACH LEARN L, P2151; Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Hariharan B., 2017, P IEEE INT C COMP VI, P3018; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li C., 2017, ADV NEURAL INFORM PR, V30, P4088; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Mirza M., 2014, COMPUT SCI, P2672; Miyato Takeru, 2018, 6 INT C LEARNING REP, P8; Neal R. M., 2012, BAYESIAN LEARNING NE; Odena A, 2017, PR MACH LEARN RES, V70; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Reed S, 2016, PR MACH LEARN RES, V48; Salimans T., 2016, ADV NEUR IN, P2234; Shi Jiaxin, 2018, INT C MACH LEARN ICM; Shi Jiaxin, 2017, ARXIV170905870; Tortorella F, 2000, LECT NOTES COMPUT SC, V1876, P611; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Wang Zichao, 2019, INT C LEARN REPR ICL; Wen Y, 2018, INT C LEARN REPR; Xiao Han, 2017, FASHION MNIST NOVEL; Zhang H., 2017, ICCV	42	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904070
C	Criscitiello, C; Boumal, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Criscitiello, Chris; Boumal, Nicolas			Efficiently escaping saddle points on manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Smooth, non-convex optimization problems on Riemannian manifolds occur in machine learning as a result of orthonormality, rank or positivity constraints. First- and second-order necessary optimality conditions state that the Riemannian gradient must be zero, and the Riemannian Hessian must be positive semidefinite. Generalizing Jin et al.'s recent work on perturbed gradient descent (PGD) for optimization on linear spaces [How to Escape Saddle Points Efficiently (2017) [17], Stochastic Gradient Descent Escapes Saddle Points Efficiently (2019) [18]], we propose a version of perturbed Riemannian gradient descent (PRGD) to show that necessary optimality conditions can be met approximately with high probability, without evaluating the Hessian. Specifically, for an arbitrary Riemannian manifold Mo f dimension d, a sufficiently smooth (possibly non-convex) objective function f, and under weak conditions on the retraction chosen to move on the manifold, with high probability, our version of PRGD produces a point with gradient smaller than. and Hessian within root epsilon of being positive semidefinite in O((log d)(4)/epsilon(2)) gradient queries. This matches the complexity of PGD in the Euclidean case. Crucially, the dependence on dimension is low. This matters for large-scale applications including PCA and low-rank matrix completion, which both admit natural formulations on manifolds. The key technical idea is to generalize PRGD with a distinction between two types of gradient steps: "steps on the manifold" and "perturbed steps in a tangent space of the manifold." Ultimately, this distinction makes it possible to extend Jin et al.'s analysis seamlessly.	[Criscitiello, Chris; Boumal, Nicolas] Princeton Univ, Dept Math, Princeton, NJ 08544 USA	Princeton University	Criscitiello, C (corresponding author), Princeton Univ, Dept Math, Princeton, NJ 08544 USA.	ccriscitiello6@gmail.com; nboumal@math.princeton.edu			NSF [DMS-1719558]	NSF(National Science Foundation (NSF))	We thank Yue Sun, Nicolas Flammarion and Maryam Fazel, authors of [31], for numerous relevant discussions. NB is partially supported by NSF grant DMS-1719558.	Absil PA, 2012, SIAM J OPTIMIZ, V22, P135, DOI 10.1137/100802529; Absil P.-A., 2010, RECENT ADV OPTIMIZAT, P125; Absil PA, 2007, FOUND COMPUT MATH, V7, P303, DOI 10.1007/s10208-005-0179-9; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; ABSIL PA, 2006, INT CONF ACOUST SPEE, P945; Adler RL, 2002, IMA J NUMER ANAL, V22, P359, DOI 10.1093/imanum/22.3.359; Agarwal N., 2019, ARXIV180600065; [Anonymous], 2017, ARXIV170205594; Bento GC, 2017, J OPTIMIZ THEORY APP, V173, P548, DOI 10.1007/s10957-017-1093-4; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Boumal N, 2019, IMA J NUMER ANAL, V39, P1, DOI 10.1093/imanum/drx080; Cambier L, 2016, SIAM J SCI COMPUT, V38, pS440, DOI 10.1137/15M1025153; Daneshmand H, 2018, PR MACH LEARN RES, V80; Du SS, 2017, ADV NEUR IN, V30; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Jin C., 2019, ARXIV PREPRINT ARXIV; Jin C., 2018, P 31 C LEARNING THEO, P1042; Jin C, 2017, PR MACH LEARN RES, V70; Levy K. Y., 2016, ARXIV161104831; Lezcano Casado M, 2019, ADV NEURAL INFORM PR, V32, P9157; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Mokhtari A, 2018, ADV NEUR IN, V31; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nocedal J., 2006, NUMERICAL OPTIMIZATI; Pascanu R., 2014, ICLR; Pumir T., 2018, ADV NEURAL INFORM PR, V31, P2283; Sang Hejian, 2018, ARXIV180509416; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Sun Yue, 2018, ICML; Turaga P., 2008, PROC IEEE C COMPUT V, P1; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592; Zhang J., 2018, ARXIV180505565; Zhang T, 2018, J MACH LEARN RES, V19	35	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306004
C	Daley, B; Amato, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Daley, Brett; Amato, Christopher			Reconciling lambda-Returns with Experience Replay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modern deep reinforcement learning methods have departed from the incremental learning required for eligibility traces, rendering the implementation of the lambda-return difficult in this context. In particular, off-policy methods that utilize experience replay remain problematic because their random sampling of minibatches is not conducive to the efficient calculation of lambda-returns. Yet replay-based methods are often the most sample efficient, and incorporating lambda-returns into them is a viable way to achieve new state-of-the-art performance. Towards this, we propose the first method to enable practical use of lambda-returns in arbitrary replay-based methods without relying on other forms of decorrelation such as asynchronous gradient updates. By promoting short sequences of past transitions into a small cache within the replay memory, adjacent lambda-returns can be efficiently precomputed by sharing Q-values. Computation is not wasted on experiences that are never sampled, and stored lambda-returns behave as stable temporal-difference (TD) targets that replace the target network. Additionally, our method grants the unique ability to observe TD errors prior to sampling; for the first time, transitions can be prioritized by their true significance rather than by a proxy to it. Furthermore, we propose the novel use of the TD error to dynamically select lambda-values that facilitate faster learning. We show that these innovations can enhance the performance of DQN when playing Atari 2600 games, even under partial observability. While our work specifically focuses on lambda-returns, these ideas are applicable to any multi-step return estimator.	[Daley, Brett; Amato, Christopher] Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA	Northeastern University	Daley, B (corresponding author), Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA.	b.daley@northeastern.edu; c.amato@northeastern.edu			NSF [1734497]; Amazon Research Award (ARA)	NSF(National Science Foundation (NSF)); Amazon Research Award (ARA)	We would like to thank the anonymous reviewers for their valuable feedback. We also gratefully acknowledge NVIDIA Corporation for its GPU donation. This research was funded by NSF award 1734497 and an Amazon Research Award (ARA).	[Anonymous], 2016, ARXIV161101224; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Brockman G., 2016, OPENAI GYM; Degris T., 2012, P 29 INT C MACH LEAR, P457; Duan Y, 2016, INT C MACH LEARN, P1329; Gu SX, 2016, PR MACH LEARN RES, V48; Harb J., 2017, ARXIV170405495; Harutyunyan A, 2016, LECT NOTES ARTIF INT, V9925, P305, DOI 10.1007/978-3-319-46379-7_21; Hausknecht M., 2015, P 2015 AAAI FALL S S, V9, P29; Heess N., 2017, ABS170702286 CORR; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kingma D.P, P 3 INT C LEARNING R; Klopf A. H., 1972, TECHNICAL REPORT; Konidaris G., 2011, ADV NEURAL INFORM PR, P2402; Lample G, 2017, AAAI CONF ARTIF INTE, P2140; Levine S, 2018, INT J ROBOT RES, V37, P421, DOI 10.1177/0278364917710318; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap TP, 2016, 4 INT C LEARN REPR; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Metz L., 2017, ARXIV170505035; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos R, 2016, P 30 INT C NEUR INF; Peng J., 1994, MACHINE LEARNING P, P226, DOI [10.1016/B978-1-55860-335-6.50035-0, DOI 10.1016/B978-1-55860-335-6.50035-0]; Precup D., 2000, P 17 INT C MACH LEAR; Rajeswaran A., 2017, ARXIV PREPRINT ARXIV; Schaul T, 2016, C TRACK P, P1; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R., 1994, CTR SYSTEMS SCI, P91; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton Richard Stuart, 1984, THESIS, P4; Tsitsiklis JN, 1997, ADV NEUR IN, V9, P1075; Watkins CJCH., 1989, THESIS	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301016
C	Demirer, M; Syrgkanis, V; Lewis, G; Chernozhukov, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Demirer, Mert; Syrgkanis, Vasilis; Lewis, Greg; Chernozhukov, Victor			Semi-Parametric Efficient Policy Learning with Continuous Actions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BOUNDS	We consider off-policy evaluation and optimization with continuous action spaces. We focus on observational data where the data collection policy is unknown and needs to be estimated. We take a semi-parametric approach where the value function takes a known parametric form in the treatment, but we are agnostic on how it depends on the observed contexts. We propose a doubly robust off-policy estimate for this setting and show that off-policy optimization based on this estimate is robust to estimation errors of the policy function or the regression model. Our results also apply if the model does not satisfy our semi-parametric form, but rather we measure regret in terms of the best projection of the true value function to this functional space. Our work extends prior approaches of policy optimization from observational data that only considered discrete actions. We provide an experimental evaluation of our method in a synthetic data example motivated by optimal personalized pricing and costly resource allocation.	[Demirer, Mert; Chernozhukov, Victor] MIT, Cambridge, MA 02139 USA; [Syrgkanis, Vasilis; Lewis, Greg] Microsoft Res, Redmond, WA USA	Massachusetts Institute of Technology (MIT); Microsoft	Demirer, M (corresponding author), MIT, Cambridge, MA 02139 USA.	mdemirer@mit.edu; vasy@microsoft.com; glewis@microsoft.com; vchern@mit.edu						Athey Susan, 2018, ARXIV PREPRINT ARXIV; Beygelzimer A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129; Bickel P.J., 1993, EFFICIENT ADAPTIVE E, V4; CHAMBERLAIN G, 1992, ECONOMETRICA, V60, P567, DOI 10.2307/2951584; Chernozhukov V, 2018, ECONOMET J, V21, pC1, DOI 10.1111/ectj.12097; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; ENGLE RF, 1986, J AM STAT ASSOC, V81, P310, DOI 10.2307/2289218; Foster D.J., 2019, ARXIV PREPRINT ARXIV; Graham Bryan S, 2018, SEMIPARAMETRICALLY E; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Kitagawa T, 2018, ECONOMETRICA, V86, P591, DOI 10.3982/ECTA13288; Krishnamurthy Akshay, 2019, ARXIV190201520, P2025; Maurer Andreas, 2009, ARXIV09073740; Qian M, 2011, ANN STAT, V39, P1180, DOI 10.1214/10-AOS864; Rakhlin A, 2017, BERNOULLI, V23, P789, DOI 10.3150/14-BEJ679; ROBINSON PM, 1988, ECONOMETRICA, V56, P931, DOI 10.2307/1912705; Swaminathan A, 2015, PR MACH LEARN RES, V37, P814; Van der Laan Mark J, 2011, TARGETED LEARNING CA; Van Der Vaart A. W., 1996, SPRINGER SERIES; Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771; Wooldridge Jeffrey M, 2004, CWP0304 CEMMAP; Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713; Zhao YQ, 2012, J AM STAT ASSOC, V107, P1106, DOI 10.1080/01621459.2012.695674; Zhou A., 2018, ARXIV PREPRINT ARXIV; Zhou X, 2017, J AM STAT ASSOC, V112, P169, DOI 10.1080/01621459.2015.1093947; Zhou Z., 2018, ARXIV181004778	28	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906069
C	Deng, Y; Schneider, J; Sivan, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Deng, Yuan; Schneider, Jon; Sivan, Balasubramanian			Prior-Free Dynamic Auctions with Low Regret Buyers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the problem of how to repeatedly sell to a buyer running a no-regret, mean-based algorithm. Previous work [Braverman et al., 2018] shows that it is possible to design effective mechanisms in such a setting that extract almost all of the economic surplus, but these mechanisms require the buyer's values each round to be drawn independently and identically from a fixed distribution. In this work, we do away with this assumption and consider the prior-free setting where the buyer's value each round is chosen adversarially (possibly adaptively). We show that even in this prior-free setting, it is possible to extract a (1 - epsilon)-approximation of the full economic surplus for any epsilon > 0. The number of options offered to a buyer in any round scales independently of the number of rounds T and polynomially in epsilon. We show that this is optimal up to a polynomial factor; any mechanism achieving this approximation factor, even when values are drawn stochastically, requires at least Omega(1/epsilon) options. Finally, we examine what is possible when we constrain our mechanism to a natural auction format where overbidding is dominated. Braverman et al. [2018] show that even when values are drawn from a known stochastic distribution supported on [1/H, 1], it is impossible in general to extract more than O(log log H log H) of the economic surplus. We show how to achieve the same approximation factor in the prior-independent setting (where the distribution is unknown to the seller), and an approximation factor of O(1/ log H) in the prior-free setting (where the values are chosen adversarially).	[Deng, Yuan] Duke Univ, Durham, NC 27706 USA; [Schneider, Jon; Sivan, Balasubramanian] Google Res, New York, NY USA	Duke University; Google Incorporated	Deng, Y (corresponding author), Duke Univ, Durham, NC 27706 USA.	ericdy@cs.duke.edu; jschnei@google.com; balusivan@google.com						Agrawal S, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P171, DOI 10.1145/3219166.3219234; Amin K., 2014, ADV NEURAL INFORM PR, P622; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Ashlagi I, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P213, DOI 10.1145/2940716.2940775; Balseiro Santiago, 2018, ARXIV180909582; Balseiro SR, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P165, DOI 10.1145/3033274.3084089; Balseiro Santiago R., 2019, SODA, P157, DOI [10.1137/1.9781611975482.11, DOI 10.1137/1.9781611975482.11]; Braverman M, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P523, DOI 10.1145/3219166.3219233; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cai Yang, 2017, FOCS; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Daskalakis C, 2016, ANN IEEE SYMP FOUND, P219, DOI 10.1109/FOCS.2016.31; Devanur NR, 2016, ACM S THEORY COMPUT, P426, DOI 10.1145/2897518.2897553; Drutsa A, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P33, DOI 10.1145/3038912.3052700; Drutsa Alexey, 2018, INT C MACH LEARN, P1318; Dudik Miroslav, 2017, FOCS; Feng Z, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P505, DOI 10.1145/3219166.3219208; Gonczarowski YA, 2017, ACM S THEORY COMPUT, P856, DOI 10.1145/3055399.3055427; Hartline Jason D, 2013, BOOK DRAFT; Liu Siqi, 2017, ABS170907955 CORR; Mirrokni V, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P169, DOI 10.1145/3219166.3219224; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; Morgenstern J, 2015, P 28 INT C NEUR INF, V1, P136; Nekipelov Denis, 2015, P 16 ACM C EC COMPUT, P1, DOI [10.1145/2764468.2764522, DOI 10.1145/2764468.2764522]; Papadimitriou C., 2016, P 27 ANN ACM SIAM S, P1458; Roughgarden T, 2012, ACM SIGECOM EXCH, V11, P18; Syrgkanis V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P211; ZHENG CF, 2018, ADV NEURAL INFORM PR, P338	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304077
C	Derezinski, M; Mahoney, MW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Derezinski, Michal; Mahoney, Michael W.			Distributed estimation of the inverse Hessian by determinantal averaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In distributed optimization and distributed numerical linear algebra, we often encounter an inversion bias: if we want to compute a quantity that depends on the inverse of a sum of distributed matrices, then the sum of the inverses does not equal the inverse of the sum. An example of this occurs in distributed Newton's method, where we wish to compute (or implicitly work with) the inverse Hessian multiplied by the gradient. In this case, locally computed estimates are biased, and so taking a uniform average will not recover the correct solution. To address this, we propose determinantal averaging,a new approach for correcting the inversion bias. This approach involves reweighting the local estimates of the Newton's step proportionally to the determinant of the local Hessian estimate, and then averaging them together to obtain an improved global estimate. This method provides the first known distributed Newton step that is asymptotically consistent, i.e., it recovers the exact step in the limit as the number of distributed partitions grows to infinity. To show this, we develop new expectation identities and moment bounds for the determinant and adjugate of a random matrix. Determinantal averaging can be applied not only to Newton's method, but to computing any quantity that is a linear transformation of a matrix inverse, e.g., taking a trace of the inverse covariance matrix, which is used in data uncertainty quantification.	[Derezinski, Michal; Mahoney, Michael W.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; [Mahoney, Michael W.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley	Derezinski, M (corresponding author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.	mderezin@berkeley.edu; mmahoney@stat.berkeley.edu			ARO; DARPA; NSF; ONR	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	MWM would like to acknowledge ARO, DARPA, NSF and ONR for providing partial support of this work. Also, MWM and MD thank the NSF for funding via the NSF TRIPODS program. Part of this work was done while MD and MWM were visiting the Simons Institute for the Theory of Computing.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Agarwal N, 2017, J MACH LEARN RES, V18; Bajovic D, 2017, SIAM J OPTIMIZ, V27, P1171, DOI 10.1137/15M1038049; Bekas C., 2009, P 2 WORKSH HIGH PERF; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen RY, 2012, INF INFERENCE, V1, P2, DOI 10.1093/imaiai/ias001; Derezifiski Michal, 2019, P 22 INT C ART INT S; Derezifiski Michal, 2017, ADV NEURAL INFORM PR, V30, P3087; Derezigski Michal, 2019, P 32 C LEARN THEOR; Dereziliski Michal, 2018, ADV NEURAL INFORM PR, V31, P2510; Derezinski M, 2018, J MACH LEARN RES, V19, P1; Kalantzis V, 2013, NUMER ALGORITHMS, V62, P637, DOI 10.1007/s11075-012-9687-2; Konecn Jakub, 2016, ARXIV E PRINTS; Konecns Jakub, 2016, ARXIV E PRINTS; MCDONALD R, 2010, HUMAN LANGUAGE TECHN, V2010, P456; Mokhtari A, 2017, IEEE T SIGNAL PROCES, V65, P146, DOI 10.1109/TSP.2016.2617829; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Reddi Sashank J., 2016, ARXIV E PRINTS; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Stevens GVG, 1998, J FINANC, V53, P1821, DOI 10.1111/0022-1082.00074; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; VANDERVAART HR, 1965, ANN MATH STAT, V36, P1308, DOI 10.1214/aoms/1177700006; WANG S, 2017, P 34 INT C MACH LEAR, V70, P3608; Wang SS, 2018, ADV NEUR IN, V31; Wu LF, 2016, J COMPUT PHYS, V326, P828, DOI 10.1016/j.jcp.2016.09.001; Xu Peng, 2017, MATH PROGRAMMING; Yao Z., 2018, TECHNICAL REPORT; Zhang YC, 2013, J MACH LEARN RES, V14, P3321; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903008
C	Deverett, B; Faulkner, R; Fortunato, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Deverett, Ben; Faulkner, Ryan; Fortunato, Meire			Interval timing in deep reinforcement learning agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEURAL MECHANISMS; TIME	The measurement of time is central to intelligent behavior. We know that both animals and artificial agents can successfully use temporal dependencies to select actions. In artificial agents, little work has directly addressed (1) which architectural components are necessary for successful development of this ability, (2) how this timing ability comes to be represented in the units and actions of the agent, and (3) whether the resulting behavior of the system converges on solutions similar to those of biology. Here we studied interval timing abilities in deep reinforcement learning agents trained end-to-end on an interval reproduction paradigm inspired by experimental literature on mechanisms of timing. We characterize the strategies developed by recurrent and feedforward agents, which both succeed at temporal reproduction using distinct mechanisms, some of which bear specific and intriguing similarities to biological systems. These findings advance our understanding of how agents come to represent time, and they highlight the value of experimentally inspired approaches to characterizing agent abilities.	[Deverett, Ben; Faulkner, Ryan; Fortunato, Meire] DeepMind, London, England		Deverett, B (corresponding author), DeepMind, London, England.	bendeverett@google.com; rfaulk@google.com; meirefortunato@google.com		Leibo, Joel/0000-0002-3153-916X				Beattie C., 2016, CORR; Buhusi CV, 2005, NAT REV NEUROSCI, V6, P755, DOI 10.1038/nrn1764; Cho Kyunghyun, 2014, LEARNING PHRASE REPR, P1; CHURCH RM, 1977, J EXP PSYCHOL ANIM B, V3, P216, DOI 10.1037/0097-7403.3.3.216; Espeholt L., 2018, ABS180201561 CORR; GIBBON J, 1977, PSYCHOL REV, V84, P279, DOI 10.1037/0033-295X.84.3.279; Hazeltine E, 1997, TRENDS COGN SCI, V1, P163, DOI 10.1016/S1364-6613(97)01058-9; He K., 2015, ABS151203385 CORR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hung C-C, 2018, ARXIV181006721; Jaderberg, 2018, ARXIV180701281; Jazayeri M, 2015, CURR BIOL, V25, P2599, DOI 10.1016/j.cub.2015.08.038; Jazayeri M, 2010, NAT NEUROSCI, V13, P1020, DOI 10.1038/nn.2590; Jones E., 2001, SCIPY OPEN SOURCE SC; Karmarkar UR, 2007, NEURON, V53, P427, DOI 10.1016/j.neuron.2007.01.006; Kawai R, 2015, NEURON, V86, P800, DOI 10.1016/j.neuron.2015.03.024; KILLEEN PR, 1988, PSYCHOL REV, V95, P274, DOI 10.1037/0033-295X.95.2.274; Leibo J. Z., 2018, ARXIV180108116; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Mnih V., 2016, ABS160201783 CORR; Orhan AE, 2019, NAT NEUROSCI, V22, P275, DOI 10.1038/s41593-018-0314-y; Pai S, 2011, FRONT SYST NEUROSCI, V5, DOI 10.3389/fnsys.2011.00074; Paton JJ, 2018, NEURON, V98, P687, DOI 10.1016/j.neuron.2018.03.045; Petter EA, 2018, TRENDS COGN SCI, V22, P911, DOI 10.1016/j.tics.2018.08.004; Santoro A, 2018, ADV NEUR IN, V31; Suarez-Pinilla Marta, 2018, PERCEPTUAL CONTENT N; Theraulaz G, 1999, ARTIF LIFE, V5, P97, DOI 10.1162/106454699568700; Vinyals O., 2019, ALPHASTAR MASTERING; Wayne G., 2018, UNSUPERVISED PREDICT; Weinberger K.Q., 2014, ADV NEURAL INFORM PR, V27, P1026; Yang GR, 2019, NAT NEUROSCI, V22, P297, DOI 10.1038/s41593-018-0310-2	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306066
C	Devraj, AM; Chen, JS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Devraj, Adithya M.; Chen, Jianshu			Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider a generic empirical composition optimization problem, where there are empirical averages present both outside and inside nonlinear loss functions. Such a problem is of interest in various machine learning applications, and cannot be directly solved by standard methods such as stochastic gradient descent. We take a novel approach to solving this problem by reformulating the original minimization objective into an equivalent min-max objective, which brings out all the empirical averages that are originally inside the nonlinear loss functions. We exploit the rich structures of the reformulated problem and develop a stochastic primal-dual algorithm, SVRPDA-I, to solve the problem efficiently. We carry out extensive theoretical analysis of the proposed algorithm, obtaining the convergence rate, the computation complexity and the storage complexity. In particular, the algorithm is shown to converge at a linear rate when the problem is strongly convex. Moreover, we also develop an approximate version of the algorithm, named SVRPDA-II, which further reduces the memory requirement. Finally, we evaluate our proposed algorithms on several real-world benchmarks, and experimental results show that the proposed algorithms significantly outperform existing techniques.	[Devraj, Adithya M.] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32603 USA; [Devraj, Adithya M.; Chen, Jianshu] Tencent AI Lab, Bellevue, WA USA	State University System of Florida; University of Florida	Devraj, AM (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32603 USA.	adithyamdevraj@ufl.edu; jianshuchen@tencent.com						Dai B, 2017, PR MACH LEARN RES, V54, P1458; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Du SS, 2019, PR MACH LEARN RES, V89, P196; Du SS, 2017, PR MACH LEARN RES, V70; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Lian XR, 2017, PR MACH LEARN RES, V54, P1159; Lin Tianyi, 2018, ARXIV180600458; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Palaniappan B., 2016, NEURIPS, P1416; Rockafellar R.T., 2015, CONVEX ANAL; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Ruszczynski A, 2006, PROBABILISTIC AND RANDOMIZED METHODS FOR DESIGN UNDER UNCERTAINTY, P119, DOI 10.1007/1-84628-095-8_4; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Macua SV, 2015, IEEE T AUTOMAT CONTR, V60, P1260, DOI 10.1109/TAC.2014.2368731; Vapnik V. N., 1998, STAT LEARNING THEORY, V3; Wang JJ, 2016, IEEE IMAGE PROC, P1714, DOI 10.1109/ICIP.2016.7532651; Wang MD, 2017, MATH PROGRAM, V161, P419, DOI 10.1007/s10107-016-1017-3; Wirawan C, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY AND INTERNET-BASED SYSTEMS (SITIS), P350, DOI 10.1109/SITIS.2017.64; Xie T., 2018, ADV NEURAL INFORM PR, P1065; Yeh C.-K., 2019, P INT C LEARN REPR; Zhang JY, 2019, PR MACH LEARN RES, V97; ZHANG YC, 2017, J MACHINE LEARNING R, V0018, P02939	23	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901050
C	Diakonikolas, I; Kane, DM; Manurangsi, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Diakonikolas, Ilias; Kane, Daniel M.; Manurangsi, Pasin			Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				HARDNESS; MODEL	We study the problem of properly learning large margin halfspaces in the agnostic PAC model. In more detail, we study the complexity of properly learning d-dimensional halfspaces on the unit ball within misclassification error alpha. OPT gamma+epsilon where OPT gamma is the optimal gamma-margin error rate and alpha >= 1 is the approximation ratio. We give learning algorithms and computational hardness results for this problem, for all values of the approximation ratio alpha >= 1, that are nearly-matching for a range of parameters. Specifically, for the natural setting that alpha is any constant bigger than one, we provide an essentially tight complexity characterization. On the positive side, we give an alpha = 1.01-approximate proper learner that uses O(1/(epsilon(2)gamma(2))) samples (which is optimal) and runs in time poly(d/epsilon) . 2((O) over tilde (1/gamma 2)). On the negative side, we show that any constant factor approximate proper learner has runtime poly(d/epsilon) . 2((1/gamma)2-o(1))(,) assuming the Exponential Time Hypothesis.	[Diakonikolas, Ilias] Univ Wisconsin, Madison, WI 53706 USA; [Kane, Daniel M.] Univ Calif San Diego, La Jolla, CA 92093 USA; [Manurangsi, Pasin] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Manurangsi, Pasin] Google Res, Mountain View, CA USA	University of Wisconsin System; University of Wisconsin Madison; University of California System; University of California San Diego; University of California System; University of California Berkeley; Google Incorporated	Diakonikolas, I (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	ilias@cs.wisc.edu; dakane@cs.ucsd.edu; pasin@berkeley.edu			NSF [CCF-1553288, CCF-1652862]; Sloan Research Fellowship	NSF(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	Part of this work was performed while Ilias Diakonikolas was at the Simons Institute for the Theory of Computing during the program on Foundations of Data Science. Ilias Diakonikolas is supported by Supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. Daniel M. Kane is supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research Fellowship.	Arora S, 1998, J ACM, V45, P70, DOI 10.1145/273865.273901; Arvind V, 2011, BULL EUR ASSOC THEOR, P41; Awasthi P, 2017, J ACM, V63, DOI 10.1145/3006384; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bellare M., 1994, P 26 ANN ACM S THEOR, P820; Ben-David Shai, 2000, NIPS, P189; Bhattacharyya Arnab, 2018, C LEARN THEOR COLT 2, P876; Birnbaum Aharon, 2012, NIPS, P935; Bubeck S, 2019, PR MACH LEARN RES, V97; Chow C-K, 1961, P S SWITCH CIRC THEO, P34; Daniely A, 2016, ACM S THEORY COMPUT, P105, DOI 10.1145/2897518.2897520; Daniely Amit, 2014, C LEARN THEOR, P244; De A, 2014, J ACM, V61, DOI 10.1145/2590772; Degwekar A., 2019, ARXIV190201086; Diakonikolas I., 2019, ABS190811335 CORR; DIAKONIKOLAS I, 2011, SODA, P1590; Diakonikolas I, 2019, PR MACH LEARN RES, V97; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1061, DOI 10.1145/3188745.3188754; Diakonikolas I, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2683; Diakonikolas I, 2017, PR MACH LEARN RES, V70; Dinur I, 2007, J ACM, V54, DOI 10.1145/1236457.1236459; Feldman V, 2006, ANN IEEE SYMP FOUND, P563; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Guruswami V, 2006, ANN IEEE SYMP FOUND, P543; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; Impagliazzo R, 2001, J COMPUT SYST SCI, V63, P512, DOI 10.1006/jcss.2001.1774; Impagliazzo R, 2001, J COMPUT SYST SCI, V62, P367, DOI 10.1006/jcss.2000.1727; KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052; KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468; Klivans A., 2018, C LEARNING THEORY, P1420; Klivans A., 2009, P 17 INT C ALG LANG; Lai K. A., 2016, P FOCS 16; Long P. M., 2011, NIPS; McAllester D, 2003, LECT NOTES ARTIF INT, V2777, P203, DOI 10.1007/978-3-540-45167-9_16; Montasser Omar, 2019, ARXIV190204217, V99, P2512; Moshkovitz D, 2010, J ACM, V57, DOI 10.1145/1754399.1754402; Nakkiran Preetum, 2019, ABS190100532 CORR; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Servedio RA, 2001, LECT NOTES ARTIF INT, V2111, P473; Shalev-Shwartz S., 2010, P COLT 10 23 ANN C L, P441; Shalev-Shwartz S., 2009, TECHNICAL REPORT; Valiant L. G., 1985, IJCAI, P283; Vapnik V.N, 1998, STAT LEARNING THEORY	46	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902014
C	Ding, CW; Gong, MM; Zhang, K; Tao, DC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ding, Chenwei; Gong, Mingming; Zhang, Kun; Tao, Dacheng			Likelihood-Free Overcomplete ICA and Applications in Causal Discovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IDENTIFIABILITY; ALGORITHMS; MODELS	Causal discovery witnessed significant progress over the past decades. In particular, many recent causal discovery methods make use of independent, non-Gaussian noise to achieve identifiability of the causal models. Existence of hidden direct common causes, or confounders, generally makes causal discovery more difficult; whenever they are present, the corresponding causal discovery algorithms can be seen as extensions of overcomplete independent component analysis (OICA). However, existing OICA algorithms usually make strong parametric assumptions on the distribution of independent components, which may be violated on real data, leading to sub-optimal or even wrong solutions. In addition, existing OICA algorithms rely on the Expectation Maximization (EM) procedure that requires computationally expensive inference of the posterior distribution of independent components. To tackle these problems, we present a Likelihood-Free Overcomplete ICA algorithm (LFOICA(1)) that estimates the mixing matrix directly by back-propagation without any explicit assumptions on the density function of independent components. Thanks to its computational efficiency, the proposed method makes a number of causal discovery procedures much more practically feasible. For illustrative purposes, we demonstrate the computational efficiency and efficacy of our method in two causal discovery tasks on both synthetic and real data.	[Ding, Chenwei; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, UBTECH Sydney AI Ctr, Sydney, NSW, Australia; [Gong, Mingming] Univ Melbourne, Sch Math & Stat, Melbourne, Vic, Australia; [Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA	University of Sydney; University of Melbourne; Carnegie Mellon University	Ding, CW (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, UBTECH Sydney AI Ctr, Sydney, NSW, Australia.	cdin2224@uni.sydney.edu.au; mingming.gong@unimelb.edu.au; kunz1@cmu.edu; dacheng.tao@uni.sydney.edu.au		Gong, Mingming/0000-0001-7147-5589	Australian Research Council [FL-170100117, DP-180103424]; National Institutes of Health [NIH-1R01EB022858-01, FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, FAIN-U54HG008540]; United States Air Force [FA8650-17-C-7715]; National Science Foundation EAGER [IIS-1829681]	Australian Research Council(Australian Research Council); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); United States Air Force(United States Department of Defense); National Science Foundation EAGER	Chenwei Ding and Dacheng Tao would like to acknowledge the support by Australian Research Council Projects FL-170100117 and DP-180103424. Kun Zhang would like to acknowledge the support by National Institutes of Health under Contract No. NIH-1R01EB022858-01, FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, and FAIN-U54HG008540, by the United States Air Force under Contract No. FA8650-17-C-7715, and by National Science Foundation EAGER Grant No. IIS-1829681. The National Institutes of Health, the U.S. Air Force, and the National Science Foundation are not responsible for the views reported in this article.	Amari S, 1996, ADV NEUR IN, V8, P757; Arjovsky M., 2017, ARXIV170107875; Brakel P., 2017, ARXIV171005050; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Eriksson J, 2004, IEEE SIGNAL PROC LET, V11, P601, DOI 10.1109/LSP.2004.830118; Geiger P, 2015, PR MACH LEARN RES, V37, P1917; Ghassami A, 2018, ADV NEUR IN, V31; Gong MM, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Gong MM, 2015, PR MACH LEARN RES, V37, P1898; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hojen-Sorensen PADFR, 2002, NEURAL COMPUT, V14, P889, DOI 10.1162/089976602317319009; Hoyer P.O., 2009, ADV NEURAL INFORM PR, P689; Hoyer PO, 2008, INT J APPROX REASON, V49, P362, DOI 10.1016/j.ijar.2008.02.006; Huang B, 2017, IEEE DATA MINING, P913, DOI 10.1109/ICDM.2017.114; Huang BW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3561; Huang Biwei, 2019, ARXIV190510857; Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Jang E., 2016, ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lacerda Gustavo, 2012, ARXIV12063273; Le Q.V., 2011, NEURIPS, P1017; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Maddison Chris J, 2017, ICLR; Mirza M., 2014, ARXIV; Mooij JM, 2016, J MACH LEARN RES, V17; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; Olshausen BA, 2000, ADV NEUR IN, V12, P841; Petersen KB, 2005, NEURAL COMPUT, V17, P1921, DOI 10.1162/0899766054322991; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Spirtes P., 2000, CAUSATION PREDICTION; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Tank A, 2019, BIOMETRIKA, V106, P433, DOI 10.1093/biomet/asz007; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647; Zhang K, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1347, DOI 10.24963/ijcai.2017/187; Zhang Kun, 2018, UAI	43	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306084
C	Domke, J; Sheldon, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Domke, Justin; Sheldon, Daniel			Divide and Couple: Using Monte Carlo Variational Objectives for Posterior Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent work in variational inference (VI) uses ideas from Monte Carlo estimation to tighten the lower bounds on the log-likelihood that are used as objectives. However, there is no systematic understanding of how optimizing different objectives relates to approximating the posterior distribution. Developing such a connection is important if the ideas are to be applied to inference-i.e., applications that require an approximate posterior and not just an approximation of the log-likelihood. Given a VI objective defined by a Monte Carlo estimator of the likelihood, we use a "divide and couple" procedure to identify augmented proposal and target distributions. The divergence between these is equal to the gap between the VI objective and the log-likelihood. Thus, after maximizing the VI objective, the augmented variational distribution may be used to approximate the posterior distribution.	[Domke, Justin; Sheldon, Daniel] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Sheldon, Daniel] Mt Holyoke Coll, Dept Comp Sci, South Hadley, MA USA	University of Massachusetts System; University of Massachusetts Amherst; Mount Holyoke College	Domke, J (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.							Agakov FV, 2004, LECT NOTES COMPUT SC, V3316, P561; Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; [Anonymous], 2018, STAN DEV EXAMPLE MOD; Bachman P., 2015, NIPS APPR INF WORKSH; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Buchholz A, 2018, PR MACH LEARN RES, V80; Burda Yuri, 2015, ICLR; Carpenter B., 2017, J STAT SOFTW, DOI [10.18637/jss.v076.i01, DOI 10.18637/JSS.V076.I01]; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Cremer Chris, 2017, ARXIV170402916; Domke J, 2018, ADV NEUR IN, V31; Finke A., 2015, THESIS; Geffner Tomas, 2018, ADV NEURAL INFORM PR; Gray RM, 2011, ENTROPY AND INFORMATION THEORY , SECOND EDITION, P395, DOI 10.1007/978-1-4419-7970-4; Lawson John, 2019, NEURIPS, P13; Le Tuan Anh, 2018, ICLR; Miller AC, 2017, ADV NEUR IN, V30; Naesseth CA, 2018, PR MACH LEARN RES, V84; Naesseth Christian Andersson, 2018, THESIS; Neal R. M., 1998, ARXIVPHYSICS9803008; Nowozin Sebastian, 2018, INT C LEARN REPR ICL; Rainforth Tom, 2018, ARXIV180204537; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; REGIER J, 2017, NEURIPS, P10; Ren HY, 2019, PR MACH LEARN RES, V97; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Titsias MK, 2015, ADV NEUR IN, V28	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300031
C	Evans, T; Burgess, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Evans, Talfan; Burgess, Neil			Coordinated hippocampal-entorhinal replay as structural inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIMULTANEOUS LOCALIZATION; POPULATION CODES; PLACE FIELDS; SPATIAL MAP; GRID CELLS; SEQUENCES; MODEL; REPRESENTATIONS; PLASTICITY; MECHANISM	Constructing and maintaining useful representations of sensory experience is essential for reasoning about ones environment. High-level associative (topological) maps can be useful for efficient planning and are easily constructed from experience. Conversely, embedding new experiences within a metric structure allows them to be integrated with existing ones and novel associations to be implicitly inferred. Neurobiologically, the synaptic associations between hippocampal place cells and entorhinal grid cells are thought to represent associative and metric structures, respectively. Learning the place-grid cell associations can therefore be interpreted as learning a mapping between these two spaces. Here, we show how this map could be constructed by probabilistic message-passing through the hippocampal-entorhinal system, where messages are scheduled to reduce the propagation of redundant information. We propose that this offline inference corresponds to coordinated hippocampal-entorhinal replay during sharp wave ripples. Our results also suggest that the metric map will contain local distortions that reflect the inferred structure of the environment according to associative experience, explaining observed grid deformations.	[Evans, Talfan; Burgess, Neil] UCL, Inst Cognit Neurosci, London, England	University of London; University College London	Evans, T (corresponding author), UCL, Inst Cognit Neurosci, London, England.	talfan.evans.13@ucl.ac.uk; n.burgess@ucl.ac.uk	Burgess, Neil/B-2420-2009	Burgess, Neil/0000-0003-0646-6584	European Union's Horizon 2020 research and innovation programme Human Brain Project SGA2 [785907]; Wellcome; ERC Advanced grant NEUROMEM	European Union's Horizon 2020 research and innovation programme Human Brain Project SGA2; Wellcome; ERC Advanced grant NEUROMEM	We acknowledge funding from European Union's Horizon 2020 research and innovation programme Human Brain Project SGA2 (grant agreement no. 785907), Wellcome and ERC Advanced grant NEUROMEM.	[Anonymous], 2014, ADV NEURAL INFORM PR; Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144; Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6; Barry C, 2007, NAT NEUROSCI, V10, P682, DOI 10.1038/nn1905; Bhattacharyya A., 1943, BULL CALCUTTA MATH S, V35, P99; Burak Y, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000291; Burgess N, 1996, HIPPOCAMPUS, V6, P749; BUZSAKI G, 1983, BRAIN RES REV, V6, P139, DOI 10.1016/0165-0173(83)90037-1; Davidson TJ, 2009, NEURON, V63, P497, DOI 10.1016/j.neuron.2009.07.027; Diba K, 2007, NAT NEUROSCI, V10, P1241, DOI 10.1038/nn1961; Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022; Elidan G., 2012, ARXIV12066837; Evans T, 2016, J PHYSIOL-LONDON, V594, P6535, DOI 10.1113/JP270666; Fiete IR, 2008, J NEUROSCI, V28, P6858, DOI 10.1523/JNEUROSCI.5684-07.2008; Foster DJ, 2006, NATURE, V440, P680, DOI 10.1038/nature04587; Fuhs MC, 2006, J NEUROSCI, V26, P4266, DOI 10.1523/JNEUROSCI.4353-05.2006; Gambino F, 2014, NATURE, V515, P116, DOI 10.1038/nature13664; Garvert MM, 2017, ELIFE, V6, DOI 10.7554/eLife.17086; Golding NL, 2002, NATURE, V418, P326, DOI 10.1038/nature00854; Gorchetchnikov A, 2007, NEURAL NETWORKS, V20, P182, DOI 10.1016/j.neunet.2006.11.007; Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721; Hagglund Martin, 2019, CURRENT BIOL; Hardcastle K, 2015, NEURON, V86, P827, DOI 10.1016/j.neuron.2015.03.039; Hartley T, 2000, HIPPOCAMPUS, V10, P369, DOI 10.1002/1098-1063(2000)10:4<369::AID-HIPO3>3.0.CO;2-0; Hollup SA, 2001, J NEUROSCI, V21, P1635, DOI 10.1523/JNEUROSCI.21-05-01635.2001; Kay Kenneth, 2019, CONSTANT SUBSECOND C, DOI [10.1101/528976, DOI 10.1101/528976]; Kurth-Nelson Z, 2016, NEURON, V91, P194, DOI 10.1016/j.neuron.2016.05.028; Lever C, 2002, NATURE, V416, P90, DOI 10.1038/416090a; Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733; MARR D, 1982, PROC R SOC SER B-BIO, V214, P501, DOI 10.1098/rspb.1982.0024; MARR D, 1971, PHILOS T ROY SOC B, V262, P23, DOI 10.1098/rstb.1971.0078; Mathis A, 2015, ELIFE, V4, DOI 10.7554/eLife.05979; Mathis A, 2012, NEURAL COMPUT, V24, P2280, DOI 10.1162/NECO_a_00319; Mattar MG, 2018, NAT NEUROSCI, V21, P1609, DOI 10.1038/s41593-018-0232-z; Mattar Marcelo Gomes, 2017, BIORXIV, DOI [10.1101/225664, DOI 10.1101/225664]; Milford MJ, 2008, IEEE T ROBOT, V24, P1038, DOI 10.1109/TRO.2008.2004520; Milford MJ, 2004, IEEE INT CONF ROBOT, P403, DOI 10.1109/ROBOT.2004.1307183; Muller RU, 1996, J GEN PHYSIOL, V107, P663, DOI 10.1085/jgp.107.6.663; O'Keefe John, 1996, NATURE; O'Neill J, 2017, SCIENCE, V355, P184, DOI 10.1126/science.aag2787; Ocko SA, 2018, P NATL ACAD SCI USA, V115, pE11798, DOI 10.1073/pnas.1805959115; OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1; OKEEFE J, 1976, EXP NEUROL, V51, P78, DOI 10.1016/0014-4886(76)90055-8; Olafsdottir HF, 2016, NAT NEUROSCI, V19, P792, DOI 10.1038/nn.4291; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Penny WD, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003383; Pfeiffer BE, 2015, SCIENCE, V349, P180, DOI 10.1126/science.aaa9633; Skaggs WE, 1996, SCIENCE, V271, P1870, DOI 10.1126/science.271.5257.1870; Stachenfeld KL, 2017, NAT NEUROSCI, V20, P1643, DOI 10.1038/nn.4650; Stella Federico, 2019, NEURON; Stensola T, 2015, NATURE, V518, P207, DOI 10.1038/nature14151; Thrun S, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P944; Thrun S., 1995, HDB BRAIN SCI NEURAL, P381; Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387; TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626; Weber JP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11380; Wei X.X., 2013, ARXIV13040031; Whittington J., 2018, ADV NEURAL INFORM PR, P8484; Widloski J, 2014, NEURON, V83, P481, DOI 10.1016/j.neuron.2014.06.018; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Yamamoto J, 2017, NEURON, V96, P217, DOI 10.1016/j.neuron.2017.09.017	61	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301069
C	Fang, TT; Schwing, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fang, Tiantian; Schwing, Alexander G.			Co-Generation with GANs using AIS based HMC	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Inferring the most likely configuration for a subset of variables of a joint distribution given the remaining ones - which we refer to as co-generation - is an important challenge that is computationally demanding for all but the simplest settings. This task has received a considerable amount of attention, particularly for classical ways of modeling distributions like structured prediction. In contrast, almost nothing is known about this task when considering recently proposed techniques for modeling high-dimensional distributions, particularly generative adversarial nets (GANs). Therefore, in this paper, we study the occurring challenges for co-generation with GANs. To address those challenges we develop an annealed importance sampling based Hamiltonian Monte Carlo co-generation algorithm. The presented approach significantly outperforms classical gradient based methods on a synthetic and on the CelebA and LSUN datasets.	[Fang, Tiantian; Schwing, Alexander G.] Univ Illinois, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Fang, TT (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	tf6@illinois.edu; aschwing@illinois.edu			NSF [1718221]; MRI [1725729]; UIUC; Samsung; 3M; Cisco Systems Inc. [CG 1377144]; Adobe	NSF(National Science Foundation (NSF)); MRI; UIUC; Samsung(Samsung); 3M(3M); Cisco Systems Inc.; Adobe	This work is supported in part by NSF under Grant No. 1718221 and MRI #1725729, UIUC, Samsung, 3M, Cisco Systems Inc. (Gift Award CG 1377144) and Adobe. We thank NVIDIA for providing GPUs used for this work and Cisco for access to the Arcetri cluster.	Alder B. J., 1959, J CHEM PHYS; Almahairi A, 2018, PR MACH LEARN RES, V80; Anoosheh A., 2017, ARXIV171206909; Arjovsky M, 2017, PR MACH LEARN RES, V70; Benaim S., 2017, P NIPS; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bousmalis K., 2016, NEURIPS; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chen Q., 2017, P ICCV; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Conneau Alexis, 2018, INT C LEARN REPR; Cully R. W. A., 2017, ARXIV170403817, P1; Denton E., 2017, P NIPS; Deshpande I., 2019, P CVPR; Deshpande I., 2018, P CVPR; Dinh Laurent, 2015, ICLR WORKSH; Donahue Chris, 2018, P ICLR; Dosovitskiy A., 2015, P CVPR; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Dumoulin Vincent, 2017, ICLR; Gan Z., 2017, P NIPS; Ghiasi G., 2017, P BMVC; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hoshen Y., 2018, P ICLR; Huang X., 2018, P ECCV; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras T., 2017, PROGR GROWING GANS I; Kim T., 2017, P ICML; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kiros R, 2014, PR MACH LEARN RES, V32, P595; Kolouri S, 2018, PROC CVPR IEEE, P3427, DOI 10.1109/CVPR.2018.00361; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Laffont Pierre- Yves, 2014, TOG; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li C.L, 2017, P NIPS; Li Y., 2017, P NEURIPS; Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI 10.1007/978-3-030-01231-1_42; Li Yijun, 2017, ARXIV170508086; Liang X., 2017, ARXIV170800315; Lin Z., 2018, NEURIPS; Liu M. -Y., 2016, ADV NEURAL INFORM PR, P469; Liu Ming-Yu, 2017, NIPS; Mathieu M, 2016, ADV NEUR IN, V29; Metropolis N., 1953, J CHEM PHYS; Mirza M., 2014, ARXIV; Mroueh Y., 2017, NEURIPS; Mroueh Y, 2017, PR MACH LEARN RES, V70; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Neal RM, 1996, LECT NOTES STAT, V118; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Reed Scott E, 2015, P NIPS; Royer A., 2017, ARXIV PREPRINT ARXIV; Salimans Tim, 2018, ICLR; Shen Tianxiao, 2017, P NIPS, P6830; Shimony S. E., 1994, ARTIF INTELL; Shin HC, 2018, MED IMAGE SYNTHESIS; Shrivastava Ashish, 2017, P CVPR; Sohl-Dickstein J., 2012, HAMILTONIAN ANNEALED; Srivastava N., 2012, P NEURIPS; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Taskar B., 2003, P NEURIPS; Tau T. G., 2018, P ICLR; Tenenbaum J. B., 1997, P NIPS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tulyakov S., 2018, P CVPR; Ulyanov D., 2016, P ICML; Valliant L. G., 1979, THEORETICAL COMPUTER; Villegas Ruben, 2017, ICLR 2017; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wolf L., 2017, P ICCV; Wu Y., 2017, ICLR; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Yim J., 2015, P CVPR; Zhu J. Y., 2017, P NEURIPS; Zhu Jun-Yan, 2016, P ECCV; Zhu Jun-Yan, 2017, ICCV	87	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305076
C	Farina, G; Ling, CK; Fang, F; Sandholm, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Farina, Gabriele; Ling, Chun Kai; Fang, Fei; Sandholm, Tuomas			Efficient Regret Minimization Algorithm for Extensive-Form Correlated Equilibrium	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Self-play methods based on regret minimization have become the state of the art for computing Nash equilibria in large two-players zero-sum extensive-form games. These methods fundamentally rely on the hierarchical structure of the players' sequential strategy spaces to construct a regret minimizer that recursively minimizes regret at each decision point in the game tree. In this paper, we introduce the first efficient regret minimization algorithm for computing extensive-form correlated equilibria in large two-player general-sum games with no chance moves. Designing such an algorithm is significantly more challenging than designing one for the Nash equilibrium counterpart, as the constraints that define the space of correlation plans lack the hierarchical structure and might even form cycles. We show that some of the constraints are redundant and can be excluded from consideration, and present an efficient algorithm that generates the space of extensive-form correlation plans incrementally from the remaining constraints. This structural decomposition is achieved via a special convexity-preserving operation that we coin scaled extension. We show that a regret minimizer can be designed for a scaled extension of any two convex sets, and that from the decomposition we then obtain a global regret minimizer. Our algorithm produces feasible iterates. Experiments show that it significantly outperforms prior approaches and for larger problems it is the only viable option.	[Farina, Gabriele; Ling, Chun Kai; Sandholm, Tuomas] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; [Fang, Fei] Carnegie Mellon Univ, Inst Software Res, Pittsburgh, PA 15213 USA; [Sandholm, Tuomas] Strateg Machine Inc, Morristown, NJ USA; [Sandholm, Tuomas] Strategy Robot Inc, Pittsburgh, PA USA; [Sandholm, Tuomas] Optimized Markets Inc, Pittsburgh, PA USA	Carnegie Mellon University; Carnegie Mellon University	Farina, G (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	gfarina@cs.cmu.edu; chunkail@cs.cmu.edu; feif@cs.cmu.edu; sandholm@cs.cmu.edu			National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]; Facebook fellowship; Lockheed Martin	National Science Foundation(National Science Foundation (NSF)); ARO; Facebook fellowship(Facebook Inc); Lockheed Martin	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Gabriele Farina is supported by a Facebook fellowship. Co-authors Ling and Fang are supported in part by a research grant from Lockheed Martin.	Ashlagi I, 2008, J ARTIF INTELL RES, V33, P575, DOI 10.1613/jair.2588; Aumann Robert, 1974, J MATH ECON, V1, P67, DOI 10.1016/0304-4068(74)90037-8; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2017, SCIENCE; Brown N., 2017, ADV NEURAL INFORM PR, P689; Brown N, 2019, AAAI CONF ARTIF INTE, P1829; Brown N, 2019, SCIENCE, V365, P885, DOI 10.1126/science.aay2400; Brown N, 2017, AAAI CONF ARTIF INTE, P421; Burch N, 2019, J ARTIF INTELL RES, V64, P429, DOI 10.1613/jair.1.11370; Davis T., 2019, AAAI C ART INT AAAI; Farina G., 2019, AAAI C ART INT; Farina G, 2019, PR MACH LEARN RES, V97; Farina G, 2017, PR MACH LEARN RES, V70; Farina Gabriele, 2019, P ANN C NEUR INF PRO; Gordon Geoffrey J, 2014, P 25 INT C MACH LEAR, P360; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Huang W., 2011, THESIS IIIEE; Huang W, 2008, LECT NOTES COMPUT SC, V5385, P506, DOI 10.1007/978-3-540-92185-1_56; Jiang AX, 2015, GAME ECON BEHAV, V91, P347, DOI 10.1016/j.geb.2013.02.002; Koller D, 1996, GAME ECON BEHAV, V14, P247, DOI 10.1006/game.1996.0051; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; Papadimitriou CH, 2008, J ACM, V55, DOI 10.1145/1379759.1379762; Romanovskii I., 1962, SOVIET MATH, V3; Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645; von Stengel B, 2008, MATH OPER RES, V33, P1002, DOI 10.1287/moor.1080.0340; vonStengel B, 1996, GAME ECON BEHAV, V14, P220, DOI 10.1006/game.1996.0050; Wang Y, 2013, J OPTIM, V2013, DOI 10.1155/2013/356420; Zinkevich, 2003, P 20 INT C MACH LEAR, P928; Zinkevich M., 2007, ADV NEURAL INFORM PR, V7, P1729	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305021
C	Flodin, L; Gandikota, V; Mazumdar, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Flodin, Larkin; Gandikota, Venkata; Mazumdar, Arya			Superset Technique for Approximate Recovery in One-Bit Compressed Sensing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS; REGRESSION	One-bit compressed sensing (1bCS) is a method of signal acquisition under extreme measurement quantization that gives important insights on the limits of signal compression and analog-to-digital conversion. The setting is also equivalent to the problem of learning a sparse hyperplane-classifier. In this paper, we propose a generic approach for signal recovery in nonadaptive 1bCS that leads to improved sample complexity for approximate recovery for a variety of signal models, including nonnegative signals and binary signals. We construct 1bCS matrices that are universal - i.e. work for all signals under a model - and at the same time recover very general random sparse signals with high probability. In our approach, we divide the set of samples (measurements) into two parts, and use the first part to recover the superset of the support of a sparse vector. The second set of measurements is then used to approximate the signal within the superset. While support recovery in 1bCS is well-studied, recovery of superset of the support requires fewer samples, which then leads to an overall reduction in sample complexity for approximate recovery.	[Flodin, Larkin; Gandikota, Venkata; Mazumdar, Arya] Univ Massachusetts Amherst, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Flodin, L (corresponding author), Univ Massachusetts Amherst, Amherst, MA 01003 USA.	lflodin@cs.umass.edu; gandikota.venkata@gmail.com; arya@cs.umass.edu			NSF CCF [1618512, 1642658, 1642550]; UMass Center for Data Science	NSF CCF; UMass Center for Data Science	This research is supported in part by NSF CCF awards 1618512, 1642658, and 1642550 and the UMass Center for Data Science.	Acharya J, 2017, IEEE INT SYMP INFO, P2353, DOI 10.1109/ISIT.2017.8006950; Aldridge M, 2014, IEEE T INFORM THEORY, V60, P3671, DOI 10.1109/TIT.2014.2314472; Atia GK, 2012, IEEE T INFORM THEORY, V58, P1880, DOI 10.1109/TIT.2011.2178156; Boufounos PT, 2008, 2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3, P16, DOI 10.1109/CISS.2008.4558487; Cheraghchi M, 2013, DISCRETE APPL MATH, V161, P81, DOI 10.1016/j.dam.2012.07.022; De Bonis A, 2005, SIAM J COMPUT, V34, P1253, DOI 10.1137/S0097539703428002; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Du D., 2000, APPL MATH; Gopi S., 2013, PROC 30 INT C MACH L, P154; Haupt J., 2011, P 45 ANN C INF SCI S; Jacques L, 2013, IEEE T INFORM THEORY, V59, P2082, DOI 10.1109/TIT.2012.2234823; LANDAU HJ, 1967, PR INST ELECTR ELECT, V55, P1701, DOI 10.1109/PROC.1967.5962; Li P, 2016, JMLR WORKSH CONF PRO, V51, P1515; Mazumdar A, 2016, IEEE T INFORM THEORY, V62, P7522, DOI 10.1109/TIT.2016.2613870; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Porat E, 2011, IEEE T INFORM THEORY, V57, P7982, DOI 10.1109/TIT.2011.2163296; Shi HJM, 2016, 2016 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA); Slawski M., 2015, ADV NEURAL INFORM PR, P2062; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902006
C	Fukumizu, K; Yamaguchi, S; Mototake, YI; Tanaka, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fukumizu, Kenji; Yamaguchi, Shoichiro; Mototake, Yoh-ichi; Tanaka, Mirai			Semi-flat minima and saddle points by embedding neural networks to overparameterization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DEEP	We theoretically study the landscape of the training error for neural networks in overparameterized cases. We consider three basic methods for embedding a network into a wider one with more hidden units, and discuss whether a minimum point of the narrower network gives a minimum or saddle point of the wider one. Our results show that the networks with smooth and ReLU activation have different partially flat landscapes around the embedded point. We also relate these results to a difference of their generalization abilities in overparameterized realization.	[Fukumizu, Kenji; Mototake, Yoh-ichi; Tanaka, Mirai] Inst Stat Math, Tachikawa, Tokyo 1908562, Japan; [Fukumizu, Kenji; Yamaguchi, Shoichiro] Preferred Networks Inc, Chiyoda Ku, Tokyo 1000004, Japan	Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan	Fukumizu, K (corresponding author), Inst Stat Math, Tachikawa, Tokyo 1908562, Japan.; Fukumizu, K (corresponding author), Preferred Networks Inc, Chiyoda Ku, Tokyo 1000004, Japan.	fukumizu@ism.ac.jp; guguchi@preferred.jp; mototake@ism.ac.jp; mirai@ism.ac.jp		Fukumizu, Kenji/0000-0002-3488-2625				Allen-Zhu Z., 2018, ABS181104918 CORR; Arora S, 2018, PR MACH LEARN RES, V80; Chaudhari Pratik, 2017, ICLR POSTER; Glorot X., 2011, P 14 INT C ART INT S, P315; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S., 1995, Advances in Neural Information Processing Systems 7, P529; Keskar N. S., 2017, ABS160904836 CORR; Kleinberg R., 2018, P 35 INT C MACH LEAR, V80, P2698; KURKOVA V, 1994, NEURAL COMPUT, V6, P543, DOI 10.1162/neco.1994.6.3.543; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Nair V, 2010, P 27 INT C MACHINE L, P807; Nguyen Q, 2017, PR MACH LEARN RES, V70; Rangamani A., 2019, ARXIV190202434STATML; SUSSMANN HJ, 1992, NEURAL NETWORKS, V5, P589, DOI 10.1016/S0893-6080(05)80037-1; Tsuzuku Yusuke, 2019, ARXIV190104653	17	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905051
C	Gan, LR; Yang, XM; Nariestty, NN; Liang, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gan, Lingrui; Yang, Xinming; Nariestty, Naveen N.; Liang, Feng			Bayesian Joint Estimation of Multiple Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INVERSE COVARIANCE ESTIMATION; VARIABLE SELECTION; INFERENCE; LASSO	In this paper, we propose a novel Bayesian group regularization method based on the spike and slab Lasso priors for jointly estimating multiple graphical models. The proposed method can be used to estimate common sparsity structure underlying the graphical models while capturing potential heterogeneity of the precision matrices corresponding to those models. Our theoretical results show that the proposed method enjoys the optimal rate of convergence in l(infinity) norm for estimation consistency and has a strong structure recovery guarantee even when the signal strengths over different graphs are heterogeneous. Through simulation studies and an application to the capital bike-sharing network data, we demonstrate the competitive performance of our method compared to existing alternatives.	[Gan, Lingrui; Yang, Xinming; Nariestty, Naveen N.; Liang, Feng] Univ Illinois, Dept Stat, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Gan, LR (corresponding author), Univ Illinois, Dept Stat, Champaign, IL 61820 USA.	lgan6@illinois.edu; xyang104@illinois.edu; naveen@illinois.edu; liangf@illinois.edu	Liang, Fenghua/HHM-3798-2022; Liang, Feng/GZK-4305-2022		NSF [DMS-1916472, DMS-1811768]	NSF(National Science Foundation (NSF))	This work is supported in part by grants NSF DMS-1916472 and NSF DMS-1811768.	Banerjee S, 2015, J MULTIVARIATE ANAL, V136, P147, DOI 10.1016/j.jmva.2015.01.015; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Carvalho CM, 2009, BIOMETRIKA, V96, P497, DOI 10.1093/biomet/asp017; Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Dobra A, 2011, J AM STAT ASSOC, V106, P1418, DOI 10.1198/jasa.2011.tm10465; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Gan LR, 2019, J AM STAT ASSOC, V114, P1218, DOI 10.1080/01621459.2018.1482755; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Guo JA, 2011, BIOMETRIKA, V98, P1, DOI 10.1093/biomet/asq060; Lee W, 2015, J MACH LEARN RES, V16, P1035; Li WD, 2019, INT CONF MACH LEARN, P380; Loh PL, 2017, ANN STAT, V45, P2455, DOI 10.1214/16-AOS1530; Loh PL, 2015, J MACH LEARN RES, V16, P559; Ma J, 2016, J MACH LEARN RES, V17; Mazumder R, 2012, ELECTRON J STAT, V6, P2125, DOI 10.1214/12-EJS740; Mohammadi A, 2015, BAYESIAN ANAL, V10, P109, DOI 10.1214/14-BA889; Narisetty NN, 2014, ANN STAT, V42, P789, DOI 10.1214/14-AOS1207; Peterson C, 2015, J AM STAT ASSOC, V110, P159, DOI 10.1080/01621459.2014.896806; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Rockova V, 2018, J AM STAT ASSOC, V113, P431, DOI 10.1080/01621459.2016.1260469; Rockova V, 2014, J AM STAT ASSOC, V109, P828, DOI 10.1080/01621459.2013.869223; Tan LSL, 2017, ANN APPL STAT, V11, P2222, DOI 10.1214/17-AOAS1076; Wang H, 2012, ELECTRON J STAT, V6, P168, DOI 10.1214/12-EJS669; Xu XF, 2015, BAYESIAN ANAL, V10, P909, DOI 10.1214/14-BA929; Yang C., 2019, 28 ACM INT C INF KNO; Yang X., BAYESIAN ANAL; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhou JF, 2015, IEEE SYS MAN CYBERN, P2153, DOI 10.1109/SMC.2015.376	32	1	1	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901043
C	Garg, VK; Jaakkola, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Garg, Vikas K.; Jaakkola, Tommi			Solving graph compression via optimal transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REDUCTION; SELECTION	We propose a new approach to graph compression by appeal to optimal transport. The transport problem is seeded with prior information about node importance, attributes, and edges in the graph. The transport formulation can be setup for either directed or undirected graphs, and its dual characterization is cast in terms of distributions over the nodes. The compression pertains to the support of node distributions and makes the problem challenging to solve directly. To this end, we introduce Boolean relaxations and specify conditions under which these relaxations are exact. The relaxations admit algorithms with provably fast convergence. Moreover, we provide an exact O(d log d) algorithm for the subproblem of projecting a d-dimensional vector to transformed simplex constraints. Our method outperforms state-of-the-art compression methods on graph classification.	[Garg, Vikas K.; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Garg, VK (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	vgarg@csail.mit.edu; tommi@csail.mit.edu			MIT-IBM collaboration	MIT-IBM collaboration(International Business Machines (IBM))	We thank the anonymous reviewers for their thoughtful questions that led to sections 3.3 and 3.4, and experiments on the Tox21 data. We are grateful to Andreas Loukas for the code of their algorithm [1]. VG and TJ were partially supported by a grant from the MIT-IBM collaboration.	Alvarez-Melis D, 2018, PR MACH LEARN RES, V84; Arjovsky M, 2017, PR MACH LEARN RES, V70; Beck A, 2014, OPER RES LETT, V42, P1, DOI 10.1016/j.orl.2013.10.007; Beck M, 2015, GRAPH COMBINATOR, V31, P91, DOI 10.1007/s00373-013-1381-1; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Bruna J., 2014, C TRACK P; Buhlmann P, 2013, BERNOULLI, V19, P1212, DOI 10.3150/12-BEJSP11; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chen J, 2011, SIAM J SCI COMPUT, V33, P3468, DOI 10.1137/090775087; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P752; Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6; Courty N., 2018, ARXIV180509114; Courty N., 2017, NEURAL INFORM PROCES; COWELL RG, 2007, PROBABILISTIC NETWOR; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dhillon BS, 2007, SPRINGER SER RELIAB, P29; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dorfler F, 2013, IEEE T CIRCUITS-I, V60, P150, DOI 10.1109/TCSI.2012.2215780; Duchi J., 2008, PROC 25 INT C MACH L, P272; Essid M, 2018, SIAM J SCI COMPUT, V40, pA1961, DOI 10.1137/17M1132665; Frogner Charlie, 2015, ADV NEURAL INF PROCE, V2, P2053; Garg V. K., 2018, NEURAL INFORM PROCES; Hansen P, 1997, MATH METHOD OPER RES, V45, P145, DOI 10.1007/BF01194253; Hermsdorff G. B., 2019, ARXIV190209702; Karakostas G, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1328911.1328924; Kazius J, 2005, J MED CHEM, V48, P312, DOI 10.1021/jm040835a; Kondor R, 2016, ADV NEUR IN, V29; Kondor R, 2014, PR MACH LEARN RES, V32, P1620; Kriege Nils, 2012, INT C MACH LEARN ICM; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184; Livne OE, 2012, SIAM J SCI COMPUT, V34, pB499, DOI 10.1137/110843563; Loukas Andreas, 2018, P MACHINE LEARNING R; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Pilanci M, 2015, MATH PROGRAM, V151, P63, DOI 10.1007/s10107-015-0894-1; Ravasz E, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.026112; Redko Ievgen, 2019, AISTATS, P849; Ries B, 2007, DISCRETE APPL MATH, V155, P1, DOI 10.1016/j.dam.2006.05.004; Ron D, 2011, MULTISCALE MODEL SIM, V9, P407, DOI 10.1137/100791142; Savas B., 2011, SDM, P164, DOI DOI 10.1137/1.9781611972818.15; Seguy V., 2018, ICLR; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Shuman DI, 2016, IEEE T SIGNAL PROCES, V64, P2119, DOI 10.1109/TSP.2015.2512529; Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Solomon J, 2016, ARXIV160306927; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Sutherland JJ, 2003, J CHEM INF COMP SCI, V43, P1906, DOI 10.1021/ci034143r; Tan MK, 2014, J MACH LEARN RES, V15, P1371; Teneva N, 2016, JMLR WORKSH CONF PRO, V51, P1441; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Wang L, 2014, PROC INT CONF DATA, P568, DOI 10.1109/ICDE.2014.6816682; Wang Weiran, 2013, ARXIV13091541; Weisfeiler B. Y., 1968, NAUCHNOTECHNICHESKAY, V2, P2; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597	60	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308008
C	Gentzel, A; Garant, D; Jensen, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gentzel, Amanda; Garant, Dan; Jensen, David			The Case for Evaluating Causal Models Using Interventional Measures and Empirical Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFERENCE	Causal modeling is central to many areas of artificial intelligence, including complex reasoning, planning, knowledge-base construction, robotics, explanation, and fairness. An active community of researchers develops and enhances algorithms that learn causal models from data, and this work has produced a series of impressive technical advances. However, evaluation techniques for causal modeling algorithms have remained somewhat primitive, limiting what we can learn from experimental studies of algorithm performance, constraining the types of algorithms and model representations that researchers consider, and creating a gap between theory and practice. We argue for more frequent use of evaluation techniques that examine interventional measures rather than structural or observational measures, and that evaluate using empirical data rather than synthetic data. We survey the current practice in evaluation and show that the techniques we recommend are rarely used in practice. We show that such techniques are feasible and that data sets are available to conduct such evaluations. We also show that these techniques produce substantially different results than using structural measures and synthetic data.	[Gentzel, Amanda; Garant, Dan; Jensen, David] Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Gentzel, A (corresponding author), Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01003 USA.			Jensen, David/0000-0001-5653-3349	United States Air Force [FA8750-17-C-0120]	United States Air Force(United States Department of Defense)	This material is based upon work supported by the United States Air Force under Contract No, FA8750-17-C-0120. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force.	Chickering David Maxwell, 2003, J MACHINE LEARNING R, V3; Cohen P., 1995, EMPIRICAL METHODS AR; COHEN PR, 1989, IEEE T SYST MAN CYB, V19, P634, DOI 10.1109/21.31069; Cook TD, 2008, J POLICY ANAL MANAG, V27, P724, DOI 10.1002/pam.20375; Dixit A, 2016, CELL, V167, P1853, DOI 10.1016/j.cell.2016.11.038; Dorie V, 2019, STAT SCI, V34, P43, DOI 10.1214/18-STS667; Eckles Dean, P NATL ACAD SCI, V113, P7316; Eckles Dean, 2017, ARXIV170604692; Galles D., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P185; Gordon BR, 2019, MARKET SCI, V38, P193, DOI 10.1287/mksc.2018.1135; GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791; Guyon Isabelle, 2008, J MACH LEARN RES P T, P1; Hahn P. Richard, 2019, ATL CAS INF C ACIC D; Janzing D., 2010, JMLR WORKSHOP C P, V6, P1; Keane MP, 2007, INT ECON REV, V48, P1351, DOI 10.1111/j.1468-2354.2007.00465.x; Langley P, 2011, MACH LEARN, V82, P275, DOI 10.1007/s10994-011-5242-y; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; MCDONALD CJ, 1992, M D COMPUT, V9, P304; Mooij JM, 2016, J MACH LEARN RES, V17; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Peters J, 2015, NEURAL COMPUT, V27, P771, DOI 10.1162/NECO_a_00708; Peters J, 2014, J MACH LEARN RES, V15, P2009; Rubin DB, 2005, J AM STAT ASSOC, V100, P322, DOI 10.1198/016214504000001880; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Schaffter T, 2011, BIOINFORMATICS, V27, P2263, DOI 10.1093/bioinformatics/btr373; Shadish WR, 2008, J AM STAT ASSOC, V103, P1334, DOI 10.1198/016214508000000733; SHIMONI Y., 2018, ARXIV180205046; Simmons JP, 2011, PSYCHOL SCI, V22, P1359, DOI 10.1177/0956797611417632; Spirtes P., 2000, CAUSATION PREDICTION; Sun W, 2015, AAAI CONF ARTIF INTE, P297; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Zhao QY, 2019, STAT SCI, V34, P72, DOI 10.1214/18-STS680	32	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903036
C	Gidel, G; Bach, F; Lacoste-Julien, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gidel, Gauthier; Bach, Francis; Lacoste-Julien, Simon			Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					When optimizing over-parameterized models, such as deep neural networks, a large set of parameters can achieve zero training error. In such cases, the choice of the optimization algorithm and its respective hyper-parameters introduces biases that will lead to convergence to specific minimizers of the objective. Consequently, this choice can be considered as an implicit regularization for the training of over-parametrized models. In this work, we push this idea further by studying the discrete gradient dynamics of the training of a two-layer linear network with the least-squares loss. Using a time rescaling, we show that, with a vanishing initialization and a small enough step size, this dynamics sequentially learns the solutions of a reduced-rank regression with a gradually increasing rank.	[Gidel, Gauthier; Lacoste-Julien, Simon] Univ Montreal, Mila, Montreal, PQ, Canada; [Gidel, Gauthier; Lacoste-Julien, Simon] Univ Montreal, DIRO, Montreal, PQ, Canada; [Bach, Francis] PSL Res Univ, INRIA, Paris, France; [Bach, Francis] PSL Res Univ, Ecole Normale Super, Paris, France	Universite de Montreal; Universite de Montreal; Inria; UDICE-French Research Universities; PSL Research University Paris; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Gidel, G (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.; Gidel, G (corresponding author), Univ Montreal, DIRO, Montreal, PQ, Canada.	gauthier.gidel@umontreal.ca; francis.bach@umontreal.ca; simon.lacoste-julien@umontreal.ca			Canada CIFAR AI Chair Program; Canada Excellence Research Chair in "Data Science for Realtime Decision-making"; NSERC [RGPIN-2017-06936]; graduate Borealis AI fellowship; Google Focused Research award	Canada CIFAR AI Chair Program; Canada Excellence Research Chair in "Data Science for Realtime Decision-making"; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); graduate Borealis AI fellowship; Google Focused Research award(Google Incorporated)	This research was partially supported by the Canada CIFAR AI Chair Program, the Canada Excellence Research Chair in "Data Science for Realtime Decision-making", by the NSERC Discovery Grant RGPIN-2017-06936, by a graduate Borealis AI fellowship and by a Google Focused Research award.	Advani M. S., 2017, ARXIV171003667; Berglund N., 2001, MATH0111178 ARXIV; Coddington EA, 1955, THEORY ORDINARY DIFF; Combes R.T., 2018, ARXIV180906848; Deng J., 2009, CVPR; Gronwall T. H., 1919, ANN MATH; Gunasekar S., 2018, ARXIV180600468; Gunasekar Suriya, 2017, NIPS; Horn R. A., 1986, MATRIX ANAL; Izenman A. J., 1975, J MULTIVARIATE ANAL; Lampinen A. K., 2019, ICLR; LeCun Y, 2010, ATT LAB; Li Y., 2018, C LEARN THEOR, V75, P2; Nair Vinod, 2014, THE CIFAR 10 DATASET; Nar K., 2018, NEURIPS; Neyshabur B., 2015, NIPS; Neyshabur B., 2017, THESIS; Neyshabur B., 2017, ARXIV170503071; Neyshabur B, 2015, ICLR; Reinsel G. C., 1998, MULTIVARIATE REDUCED; Saxe A M, 2018, ARXIV181010531; Saxe A.M., 2013, P ANN M COGNITIVE SC, P1271; Saxe Andrew M., 2014, ICLR; Soudry D., 2018, ICLR; Uschmajew A., 2018, CRITICAL POINT UNPUB; Zhang Chiyuan, 2017, UNDERSTANDING DEEP L	26	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303022
C	Gopalan, P; Sharan, V; Wieder, U		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gopalan, Parikshit; Sharan, Vatsal; Wieder, Udi			PIDForest: Anomaly Detection via Partial Identification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	We consider the problem of detecting anomalies in a large dataset. We propose a framework called Partial Identification which captures the intuition that anomalies are easy to distinguish from the overwhelming majority of points by relatively few attribute values. Formalizing this intuition, we propose a geometric anomaly measure for a point that we call PIDScore, which measures the minimum density of data points over all subcubes containing the point. We present PIDForest: a random forest based algorithm that finds anomalies based on this definition. We show that it performs favorably in comparison to several popular anomaly detection methods, across a broad range of benchmarks. PIDForest also provides a succinct explanation for why a point is labelled anomalous, by providing a set of features and ranges for them which are relatively uncommon in the dataset.(1)	[Gopalan, Parikshit; Wieder, Udi] VMware Res, Palo Alto, CA 94304 USA; [Sharan, Vatsal] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Gopalan, P (corresponding author), VMware Res, Palo Alto, CA 94304 USA.	pgopalan@vmware.com; vsharan@stanford.edu; uwieder@vmware.com			NSF [1813049]	NSF(National Science Foundation (NSF))	VS's contribution were supported by NSF award 1813049.	Aggarwal, 2015, OUTLIER ANAL, DOI [DOI 10.1007/978-1-4614-6396-2, 10.1007/978-1-4614-6396-2]; Aggarwal C. C., 2015, ACM SIGKDD EXPLORATI, V17, P24, DOI [10.1145/2830544.2830549, DOI 10.1145/2830544.2830549]; Agrawal R., 1998, SIGMOD Record, V27, P94, DOI 10.1145/276305.276314; Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070; Angiulli Fabrizio, 2002, P 6 EUR C PRINC DAT, P15; [Anonymous], 1991, MACHINE LEARNING THE; Anthony M., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P311, DOI 10.1145/130385.130420; Asuncion A, 2007, UCI MACHINE LEARNING; Bandaragoda TR, 2018, COMPUT INTELL-US, V34, P968, DOI 10.1111/coin.12156; Bartos M.D., 2019, J OPEN SOUR SOFTW, V4, P1336, DOI [10.21105/JOSS.01336, DOI 10.21105/JOSS.04200]; Breunig M. M., 2000, SIGMOD Record, V29, P93, DOI 10.1145/335191.335388; Emmott AF, 2013, P ACM SIGKDD WORKSH, P16, DOI [10.1145/2500853.2500858, DOI 10.1145/2500853.2500858]; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Gilbert A. C., 2002, P 34 ANN ACM S THEOR, P389; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Guha S, 2002, LECT NOTES COMPUT SC, V2380, P681; Guha S, 2006, ACM T DATABASE SYST, V31, P396, DOI 10.1145/1132863.1132873; Guha S, 2016, PR MACH LEARN RES, V48; Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1023/B:AIRE.0000045502.10941.a9; Koudas Nick, 2000, P 19 ACM SIGMOD SIGA, P196; Kushilevitz E, 1996, J COMB THEORY A, V73, P376, DOI 10.1006/jcta.1996.0031; Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17; Patcha A, 2007, COMPUT NETW, V51, P3448, DOI 10.1016/j.comnet.2007.02.001; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Ramaswamy S, 2000, SIGMOD REC, V29, P427, DOI 10.1145/335191.335437; Rayana S., 2016, ODDS LIB; Shi T, 2006, J COMPUT GRAPH STAT, V15, P118, DOI 10.1198/106186006X94072; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Wigderson A, 2016, MACH LEARN, V102, P29, DOI 10.1007/s10994-015-5489-9; Yamanishi K, 2004, DATA MIN KNOWL DISC, V8, P275, DOI 10.1023/B:DAMI.0000023676.72185.7c; Zhao Y, 2019, J MACH LEARN RES, V20	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907046
C	Greenewald, K; Katz, D; Shanmugam, K; Magliacane, S; Kocaoglu, M; Boix-Adsera, E; Bresler, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Greenewald, Kristjan; Katz, Dmitriy; Shanmugam, Karthikeyan; Magliacane, Sara; Kocaoglu, Murat; Boix-Adsera, Enric; Bresler, Guy			Sample Efficient Active Learning of Causal Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BINARY SEARCH	We consider the problem of experimental design for learning causal graphs that have a tree structure. We propose an adaptive framework that determines the next intervention based on a Bayesian prior updated with the outcomes of previous experiments, focusing on the setting where observational data is cheap (assumed infinite) and interventional data is expensive. While information greedy approaches are popular in active learning, we show that in this setting they can be exponentially suboptimal (in the number of interventions required), and instead propose an algorithm that exploits graph structure in the form of a centrality measure. If each intervention yields a very large data sample, we show that the algorithm requires a number of interventions less than or equal to a factor of 2 times the minimum achievable number. We show that the algorithm and the associated theory can be adapted to the setting where each performed intervention yields finitely many samples. Several extensions are also presented, to the case where a specified set of nodes cannot be intervened on, to the case where K interventions are scheduled at once, and to the fully adaptive case where each experiment yields only one sample. In the case of finite interventional data, through simulated experiments we show that our algorithms outperform different adaptive baseline algorithms.	[Greenewald, Kristjan; Katz, Dmitriy; Shanmugam, Karthikeyan; Magliacane, Sara; Kocaoglu, Murat] IBM Res, San Jose, CA 95120 USA; [Greenewald, Kristjan; Katz, Dmitriy; Shanmugam, Karthikeyan; Magliacane, Sara; Kocaoglu, Murat; Boix-Adsera, Enric; Bresler, Guy] MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA	International Business Machines (IBM); Massachusetts Institute of Technology (MIT)	Greenewald, K (corresponding author), IBM Res, San Jose, CA 95120 USA.; Greenewald, K (corresponding author), MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA.	kristjan.h.greenewald@ibm.com; dkatzrog@us.ibm.com; karthikeyan.shanmugam2@ibm.com; sara.magliacane@ibm.com; murat@ibm.com; eboix@mit.edu; guy@mit.edu	Magliacane, Sara/ABD-8241-2020					Ben Or M, 2008, ANN IEEE SYMP FOUND, P221, DOI 10.1109/FOCS.2008.58; Cicalese F, 2014, ALGORITHMICA, V68, P1045, DOI 10.1007/s00453-012-9715-6; Cicalese F, 2010, LECT NOTES COMPUT SC, V6507, P206, DOI 10.1007/978-3-642-17514-5_18; Dereniowski D., 2017, ARXIV170208207; Dereniowski D, 2018, ARXIV180402075; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Eberhardt F., 2005, P 21 C UNC ART INT, P178; Emamjomeh-Zadeh E, 2016, ACM S THEORY COMPUT, P519, DOI 10.1145/2897518.2897656; Ghassami A., 2017, ARXIV170208567; Ghassami A., 2017, ARXIV170903625; Hu H., 2014, P NIPS 2014 MONTR CA; Hyttinen A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P340; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Jacobs T, 2010, LECT NOTES COMPUT SC, V6198, P527, DOI 10.1007/978-3-642-14165-2_45; Jamieson K., 2014, C LEARN THEOR, P423; Jordan C., 1869, J REINE ANGEW MATH, V70, P81; Kocaoglu M, 2017, PR MACH LEARN RES, V70; Kontou PI, 2016, GENE, V590, P68, DOI 10.1016/j.gene.2016.05.044; Lindgren E., 2018, ADV NEURAL INFORM PR, P5279; Magliacane S., 2016, CORR; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P403; Ness RO, 2017, LECT N BIOINFORMAT, V10229, P134, DOI 10.1007/978-3-319-56970-3_9; Onak K, 2006, ANN IEEE SYMP FOUND, P379; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Peters J, 2017, ADAPT COMPUT MACH LE; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; R Agrawal, 2019, AISTATS; Shanmugam K., 2015, ADV NEURAL INFORM PR, P3195; Spirtes P., 2000, CAUSATION PREDICTION; Triantafillou S, 2015, J MACH LEARN RES, V16, P2147; Zhang K., 2017, IJCAI P C NIH PUBL A, V2017, P1347	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906001
C	Guibas, L; Huang, QX; Liang, ZX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Guibas, Leonidas; Huang, Qixing; Liang, Zhenxiao			A Condition Number for Joint Optimization of Cycle-Consistent Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A recent trend in optimizing maps such as dense correspondences between objects or neural networks between pairs of domains is to optimize them jointly. In this context, there is a natural cycle-consistency constraint, which regularizes composite maps associated with cycles, i.e., they are forced to be identity maps. However, as there is an exponential number of cycles in a graph, how to sample a subset of cycles becomes critical for efficient and effective enforcement of the cycle-consistency constraint. This paper presents an algorithm that select a subset of weighted cycles to minimize a condition number of the induced joint optimization problem. Experimental results on benchmark datasets justify the effectiveness of our approach for optimizing dense correspondences between 3D shapes and neural networks for predicting dense image flows.	[Guibas, Leonidas] Stanford Univ, Stanford, CA 94305 USA; [Huang, Qixing; Liang, Zhenxiao] Univ Texas Austin, Austin, TX 78712 USA	Stanford University; University of Texas System; University of Texas Austin	Guibas, L (corresponding author), Stanford Univ, Stanford, CA 94305 USA.				NSF [DMS-1546206, DMS-1700234]; Vannevar Bush Faculty Fellowship; Stanford-Toyota AI center	NSF(National Science Foundation (NSF)); Vannevar Bush Faculty Fellowship; Stanford-Toyota AI center	Qixing Huang would like to acknowledge support from NSF DMS-1700234, a Gift from Snap Research, and a hardware Donation from NVIDIA. Leonidas Guibas would like to acknowledge NSF grant DMS-1546206, a grant from the Stanford-Toyota AI center, and a Vannevar Bush Faculty Fellowship.	Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x; Arrigoni F., 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P491, DOI 10.1109/3DV.2014.48; Arrigoni F, 2016, INT CONF 3D VISION, P546, DOI 10.1109/3DV.2016.64; Arrigoni F, 2016, SIAM J IMAGING SCI, V9, P1963, DOI 10.1137/16M1060248; Bajaj C., 2018, INT C MACH LEARN, P324; Bandeira AS, 2017, MATH PROGRAM, V163, P145, DOI 10.1007/s10107-016-1059-6; Bernard F, 2015, PROC CVPR IEEE, P2161, DOI 10.1109/CVPR.2015.7298828; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Chang A. X., 2015, ABS151203012 CORR; Chen YX, 2014, PR MACH LEARN RES, V32, P100; Cosmo L, 2017, COMPUT GRAPH FORUM, V36, P209, DOI 10.1111/cgf.12796; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Hashemifar S, 2016, J COMPUT BIOL, V23, P903, DOI 10.1089/cmb.2016.0025; Huang Q., 2014, ACM T GRAPHIC, V33; Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925; Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184; Huang QX, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366186; Huang Xiangru, 2017, NIPS; Huang Xiangru, 2019, ABS190109458 CORR; HUBER D, 2002, THESIS; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Kavitha T, 2009, COMPUT SCI REV, V3, P199, DOI 10.1016/j.cosrey.2009.08.001; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kim J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966403; Kim VG, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185550; Leonardos Spyridon, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2645, DOI 10.1109/ICRA.2017.7989308; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Rosen David M., 2019, I J ROBOTICS RES, V38; Rustamov Raif M, 2007, P 5 EUR S GEOM PROC, P225, DOI DOI 10.2312/SGP/SGP07/225-233; Shen Y., 2016, ADV NEURAL INFORM PR, V29, P4925; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Wang F, 2014, PROC CVPR IEEE, P3142, DOI 10.1109/CVPR.2014.402; Wang F, 2013, IEEE I CONF COMP VIS, P849, DOI 10.1109/ICCV.2013.110; Wang Yarong, 2012, ADV ENERGY MATER, V10, P1, DOI DOI 10.2514/1.46769; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801; Zhang Zaiwei, 2018, ABS181211647 CORR; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459	47	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301005
C	Gupta, S; Gribonval, R; Daudet, L; Dokmanic, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gupta, Sidharth; Gribonval, Remi; Daudet, Laurent; Dokmanic, Ivan			Don't take it lightly: Phasing optical random projections with unknown operators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SOURCE LOCALIZATION; RETRIEVAL; ALGORITHMS; MATRIX	In this paper we tackle the problem of recovering the phase of complex linear measurements when only magnitude information is available and we control the input. We are motivated by the recent development of dedicated optics-based hardware for rapid random projections which leverages the propagation of light in random media. A signal of interest xi is an element of R-N is mixed by a random scattering medium to compute the projection y = A xi, with A xi C-MxN being a realization of a standard complex Gaussian iid random matrix. Such optics-based matrix multiplications can be much faster and energy-efficient than their CPU or GPU counterparts, yet two difficulties must be resolved: only the intensity vertical bar y vertical bar(2) can be recorded by the camera, and the transmission matrix A is unknown. We show that even without knowing A, we can recover the unknown phase of y for some equivalent transmission matrix with the same distribution as A. Our method is based on two observations: first, conjugating or changing the phase of any row of A does not change its distribution; and second, since we control the input we can interfere xi with arbitrary reference signals. We show how to leverage these observations to cast the measurement phase retrieval problem as a Euclidean distance geometry problem. We demonstrate appealing properties of the proposed algorithm in both numerical simulations and real hardware experiments. Not only does our algorithm accurately recover the missing phase, but it mitigates the effects of quantization and the sensitivity threshold, thus improving the measured magnitudes.	[Gupta, Sidharth; Dokmanic, Ivan] Univ Illinois, Champaign, IL 61820 USA; [Gribonval, Remi] Univ Rennes, INRIA, CNRS, IRISA, Rennes, France; [Daudet, Laurent] LightOn, Paris, France	University of Illinois System; University of Illinois Urbana-Champaign; Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Rennes	Gupta, S (corresponding author), Univ Illinois, Champaign, IL 61820 USA.	gupta67@illinois.edu; remi.gribonval@inria.fr; laurent@lighton.ai; dokmanic@illinois.edu			National Science Foundation [CIF-1817577]	National Science Foundation(National Science Foundation (NSF))	Sidharth Gupta and Ivan Dokmanic would like to acknowledge support from the National Science Foundation under Grant CIF-1817577.	[Anonymous], 2015, MATHEMATICS-BASEL; Barmherzig D. A., 2019, ARXIV190106453; Beck A, 2008, IEEE T SIGNAL PROCES, V56, P1770, DOI 10.1109/TSP.2007.909342; Beinert R, 2017, RESULTS MATH, V72, P1, DOI 10.1007/s00025-016-0633-9; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Dokmanic I, 2015, IEEE SIGNAL PROC MAG, V32, P12, DOI 10.1109/MSP.2015.2398954; Dremeau A, 2015, OPT EXPRESS, V23, P11898, DOI 10.1364/OE.23.011898; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Horisaki R, 2016, OPT EXPRESS, V24, P13738, DOI 10.1364/OE.24.013738; KIM W, 1990, J OPT SOC AM A, V7, P441, DOI 10.1364/JOSAA.7.000441; Le Q., 2013, ICML; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liutkus A, 2014, SCI REP-UK, V4, DOI 10.1038/srep05552; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Saade A, 2016, INT CONF ACOUST SPEE, P6215, DOI 10.1109/ICASSP.2016.7472872; Satat G, 2017, OPT EXPRESS, V25, P17466, DOI 10.1364/OE.25.017466; Schoenemann Peter Hans, 1964, THESIS; Sharma M., 2019, IEEE T COMPUTATIONAL; Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673; Stoica P, 2006, IEEE SIGNAL PROC MAG, V23, P63, DOI 10.1109/SP-M.2006.248717; Yang Y, 2017, ANN STAT, V45, P991, DOI 10.1214/16-AOS1472; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975; Yurtsever Alp, 2017, ARXIV170206838; Zhang Huan, 2019, IEEE T SIGNAL PROCES	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906050
C	Haghi, B; Kellis, S; Shah, S; Ashok, M; Bashford, L; Kramer, D; Lee, B; Liu, C; Andersen, RA; Emami, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Haghi, Benyamin; Kellis, Spencer; Shah, Sahil; Ashok, Maitreyi; Bashford, Luke; Kramer, Daniel; Lee, Brian; Liu, Charles; Andersen, Richard A.; Emami, Azita			Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PREDICTION; SEARCH	We present a new deep multi-state Dynamic Recurrent Neural Network (DRNN) architecture for Brain Machine Interface (BMI) applications. Our DRNN is used to predict Cartesian representation of a computer cursor movement kinematics from open-loop neural data recorded from the posterior parietal cortex (PPC) of a human subject in a BMI system. We design the algorithm to achieve a reasonable trade-off between performance and robustness, and we constrain memory usage in favor of future hardware implementation. We feed the predictions of the network back to the input to improve prediction performance and robustness. We apply a scheduled sampling approach to the model in order to solve a statistical distribution mismatch between the ground truth and predictions. Additionally, we configure a small DRNN to operate with a short history of input, reducing the required buffering of input data and number of memory accesses. This configuration lowers the expected power consumption in a neural network accelerator. Operating on wavelet-based neural features, we show that the average performance of DRNN surpasses other state-of-the-art methods in the literature on both single- and multi-day data recorded over 43 days. Results show that multi-state DRNN has the potential to model the nonlinear relationships between the neural data and kinematics for robust BMIs.	[Haghi, Benyamin; Shah, Sahil; Ashok, Maitreyi; Emami, Azita] CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA; [Kellis, Spencer; Bashford, Luke; Andersen, Richard A.] CALTECH, Biol & Biol Engn Dept, Pasadena, CA 91125 USA; [Kramer, Daniel; Lee, Brian; Liu, Charles] USC Keck Sch Med, Neurorestorat Ctr, Los Angeles, CA USA; [Kramer, Daniel; Lee, Brian; Liu, Charles] USC Keck Sch Med, Neurosurg, Los Angeles, CA USA	California Institute of Technology; California Institute of Technology; University of Southern California; University of Southern California	Haghi, B (corresponding author), CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA.	benyamin.a.haghi@caltech.edu		Bashford, Luke/0000-0003-4391-2491; Emami, Azita/0000-0002-6945-9958; Kramer, Daniel/0000-0003-4551-2977				Aflalo T, 2015, SCIENCE, V348, P906, DOI 10.1126/science.aaa5417; Andersen RA, 2014, CURR BIOL, V24, pR885, DOI 10.1016/j.cub.2014.07.068; Basak D, 2007, NEURAL INFORM PROCES, V11, P203, DOI DOI 10.1007/978-1-4302-5990-9_4; Bengio S., 2015, SCHEDULED SAMPLING S; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Chen T, 2016, P 22 ACM SIGKDD INT, DOI DOI 10.1145/2939672.2939785; Cho Kyunghyun, 2014, LEARNING PHRASE REPR, P1; Christie BP, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016009; Dewhirst MW, 2003, INT J HYPERTHER, V19, P267, DOI 10.1080/0265673031000119006; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Gilja V, 2012, NAT NEUROSCI, V15, P1752, DOI 10.1038/nn.3265; Gowda S, 2014, IEEE T NEUR SYS REH, V22, P911, DOI 10.1109/TNSRE.2014.2309673; Haghi Benyamin, 2018, IEEE BRAIN IN WORKSH; Haghi Benyamin, 2019, SOC NEUR ANN M, V49; Hal Daume III, 2009, MACHINE LEARNING J; Hosman T, 2019, I IEEE EMBS C NEUR E, P1066, DOI 10.1109/NER.2019.8717140; JIN L, 1995, IEEE T AUTOMAT CONTR, V40, P1266, DOI 10.1109/9.400480; Klaes C, 2015, J NEUROSCI, V35, P15466, DOI 10.1523/JNEUROSCI.2747-15.2015; Mandic D., 2001, ADAPT LEARN SYST SIG, DOI 10.1002/047084535X; Musallam S, 2004, SCIENCE, V305, P258, DOI 10.1126/science.1097938; Nan H, 2015, IEEE ENG MED BIO, P2717, DOI 10.1109/EMBC.2015.7318953; Orsborn AL, 2012, IEEE T NEUR SYS REH, V20, P468, DOI 10.1109/TNSRE.2012.2185066; OW PS, 1988, INT J PROD RES, V26, P35, DOI 10.1080/00207548808947840; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Schwemmer MA, 2018, NAT MED, V24, P1669, DOI 10.1038/s41591-018-0171-y; Shah J, 2019, PROCEEDINGS OF 2019 8TH INTERNATIONAL CONFERENCE ON SOFTWARE AND INFORMATION ENGINEERING (ICSIE 2019), P9, DOI 10.1145/3328833.3328836; Shah M, 2018, J SIGNAL PROCESS SYS, V90, P727, DOI 10.1007/s11265-016-1202-x; Shoaran M., 2017, NAT ACAD ENG, V47, P31; Shoaran M, 2018, IEEE J EM SEL TOP C, V8, P693, DOI 10.1109/JETCAS.2018.2844733; Shpigelman Lavi, 2009, ADV NEURAL INFORM PR, V21; Sussillo D, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13749; Sussillo D, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026027; Taghavi Milad, 2018, ANN INT C IEEE ENG M, V40, P3693; Wessberg J, 2000, NATURE, V408, P361, DOI 10.1038/35042582; Whatmough PN, 2018, IEEE J SOLID-ST CIRC, V53, P2722, DOI 10.1109/JSSC.2018.2841824; Wu W, 2008, IEEE T NEUR SYS REH, V16, P213, DOI 10.1109/TNSRE.2008.922679; Zhang CY, 2017, NEURON, V95, P697, DOI 10.1016/j.neuron.2017.06.040; Zhang Mingming, 2018, Bioelectron Med, V4, P11, DOI 10.1186/s42234-018-0011-x	40	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906020
C	Haghtalab, N; Musco, C; Waggoner, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Haghtalab, Nika; Musco, Cameron; Waggoner, Bo			Toward a Characterization of Loss Functions for Distribution Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this work we study loss functions for learning and evaluating probability distributions over large discrete domains. Unlike classification or regression where a wide variety of loss functions are used, in the distribution learning and density estimation literature, very few losses outside the dominant log loss are applied. We aim to understand this fact, taking an axiomatic approach to the design of loss functions for distributions. We start by proposing a set of desirable criteria that any good loss function should satisfy. Intuitively, these criteria require that the loss function faithfully evaluates a candidate distribution, both in expectation and when estimated on a few samples. Interestingly, we observe that no loss function possesses all of these criteria. However, one can circumvent this issue by introducing a natural restriction on the set of candidate distributions. Specifically, we require that candidates are calibrated with respect to the target distribution, i.e., they may contain less information than the target but otherwise do not significantly distort the truth. We show that, after restricting to this set of distributions, the log loss and a large variety of other losses satisfy the desired criteria. These results pave the way for future investigations of distribution learning that look beyond the log loss, choosing a loss function based on application or domain need.	[Haghtalab, Nika] Cornell Univ, Ithaca, NY 14853 USA; [Musco, Cameron] UMass Amherst, Amherst, MA USA; [Waggoner, Bo] Univ Colorado, Boulder, CO 80309 USA	Cornell University; University of Massachusetts System; University of Massachusetts Amherst; University of Colorado System; University of Colorado Boulder	Haghtalab, N (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	nika@cs.cornell.edu; cmusco@cs.umass.edu; bwag@colorado.edu		Waggoner, Bo/0000-0002-1366-1065				Agarwal S, 2015, MAST CORN SURG REC, V40, P4; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; CANONNE C. L., 2015, ELECT C COMPUTATIONA, V22, P1; Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380; Daskalakis Constantinos, 2014, P 27 C LEARN THEOR C, P1183; DAWID AP, 1982, J AM STAT ASSOC, V77, P605, DOI 10.2307/2287720; Ehm W, 2016, J R STAT SOC B, V78, P505, DOI 10.1111/rssb.12154; Fiat A., 2013, P 4 C INN THEOR COMP, P221; Foster DP, 1998, BIOMETRIKA, V85, P379, DOI 10.1093/biomet/85.2.379; Frongillo Rafael, 2015, C LEARN THEOR, P710; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; GOOD IJ, 1952, J ROY STAT SOC B, V14, P107; IZENMAN AJ, 1991, J AM STAT ASSOC, V86, P205, DOI 10.2307/2289732; Kalai AT, 2012, COMMUN ACM, V55, P113, DOI [10.1145/2076450.2076474, 10.1145/2078450.2076474]; Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155; Manning CD, 1999, FDN STAT NATURAL LAN; MCCARTHY J, 1956, P NATL ACAD SCI USA, V42, P654, DOI 10.1073/pnas.42.9.654; Narasimhan H, 2015, PR MACH LEARN RES, V37, P2398; Ramaswamy HG, 2016, J MACH LEARN RES, V17; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; Silverman BW., 1986, DENSITY ESTIMATION S, V26, DOI 10.1201/9781315140919; Valiant G, 2016, ACM S THEORY COMPUT, P142, DOI 10.1145/2897518.2897641	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307027
C	Hanson, J; Raginsky, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hanson, Joshua; Raginsky, Maxim			Universal Approximation of Input-Output Maps by Temporal Convolutional Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STABILITY	There has been a recent shift in sequence-to-sequence modeling from recurrent network architectures to convolutional network architectures due to computational advantages in training and operation while still achieving competitive performance. For systems having limited long-term temporal dependencies, the approximation capability of recurrent networks is essentially equivalent to that of temporal convolutional nets (TCNs). We prove that TCNs can approximate a large class of input-output maps having approximately finite memory to arbitrary error tolerance. Furthermore, we derive quantitative approximation rates for deep ReLU TCNs in terms of the width and depth of the network and modulus of continuity of the original input-output map, and apply these results to input-output maps of systems that admit finite-dimensional state-space realizations (i.e., recurrent models).	[Hanson, Joshua; Raginsky, Maxim] Univ Illinois, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Hanson, J (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	jmh4@illinois.edu; maxim@illinois.edu		Hanson, Joshua/0000-0001-6163-7572	the National Science Foundation under the Center for Advanced Electronics through Machine Learning (CAEML) I/UCRC award [CNS-16-24811]	the National Science Foundation under the Center for Advanced Electronics through Machine Learning (CAEML) I/UCRC award	This work was supported in part by the National Science Foundation under the Center for Advanced Electronics through Machine Learning (CAEML) I/UCRC award no. CNS-16-24811.	[Anonymous], 2016, NEURAL MACHINE TRANS; Bai S., 2018, ARVIX180301271; BOYD S, 1985, IEEE T CIRCUITS SYST, V32, P1150, DOI 10.1109/TCS.1985.1085649; Chelba C., 2017, N GRAM LANGUAGE MODE; Dauphin Y. N., 2017, INT C MACH LEARN; DeVore R. A., 1993, CONSTRUCTIVE APPROXI; Gehring Jonas, 2017, INT C MACH LEARN; Hanin B., 2018, APPROXIMATING CONTIN; Johnson R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P562, DOI 10.18653/v1/P17-1052; Kim KKK, 2014, INT J ROBUST NONLIN, V24, P2458, DOI 10.1002/rnc.3003; LIANG S, 2017, INT C LEARN REPR; Miller John, 2019, INT C LEARN REPR, P2; Nijmeijer H., 2006, SYS CON FDN; Oord A. v. d., 2016, ARXIV160903499; Park Jooyoung, IEEE T CIRCUITS SY 1, V39, P673; SANDBERG IW, 1991, MULTIDIM SYST SIGN P, V2, P267, DOI 10.1007/BF01952236; SANDBERG IW, 1993, AUTOMATICA, V29, P523, DOI 10.1016/0005-1098(93)90150-R; Sarkans E, 2016, SIAM J CONTROL OPTIM, V54, P1739, DOI 10.1137/130939067; Schutze H., 2017, CORR; Sharan Vatsal, 2018, S THEOR COMP; Sontag ED., 1998, MATH CONTROL THEORY, DOI [10.1007/978-1-4612-0577-7, DOI 10.1007/978-1-4612-0577-7]; Tran Duc N., 2017, IEEE T AUTOMATIC CON; TSYPKIN YZ, 1964, DOKL AKAD NAUK SSSR+, V155, P1029; VAIDYANATHAN PP, 1985, IEEE T CIRCUITS SYST, V32, P918, DOI 10.1109/TCS.1985.1085815; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905069
C	Hartmann, M; Kementchedjhieva, Y; Sogaard, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hartmann, Mareike; Kementchedjhieva, Yova; Sogaard, Anders			Comparing Unsupervised Word Translation Methods Step by Step	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Cross-lingual word vector space alignment is the task of mapping the vocabularies of two languages into a shared semantic space, which can be used for dictionary induction, unsupervised machine translation, and transfer learning. In the unsupervised regime, an initial seed dictionary is learned in the absence of any known correspondences between words, through distribution matching, and the seed dictionary is then used to supervise the induction of the final alignment in what is typically referred to as a (possibly iterative) refinement step. We focus on the first step and compare distribution matching techniques in the context of language pairs for which mixed training stability and evaluation scores have been reported. We show that, surprisingly, when looking at this initial step in isolation, vanilla GANs are superior to more recent methods, both in terms of precision and robustness. The improvements reported by more recent methods thus stem from the refinement techniques, and we show that we can obtain state-of-the-art performance combining vanilla GANs with such refinement techniques.	[Hartmann, Mareike; Kementchedjhieva, Yova; Sogaard, Anders] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark	University of Copenhagen	Hartmann, M (corresponding author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	hartmann@di.ku.dk; yova@di.ku.dk; soegaard@di.ku.dk			Carlsberg Foundation; Google Focused Research Award	Carlsberg Foundation(Carlsberg Foundation); Google Focused Research Award(Google Incorporated)	We thank the anonymous reviewers for their comments and suggestions. Mareike Hartmann was supported by the Carlsberg Foundation. Anders Sogaard was supported by a Google Focused Research Award.	Alvarez-Melis David, 2018, EMNLP; [Anonymous], 2015, P ICLR; Arjovsky M., 2017, ARXIV170107875; Artetxe M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P451, DOI 10.18653/v1/P17-1042; Artetxe Mikel, 2018, ACL; Balles Lukas, 2017, P UAI; Barone M., 2016, REP4NLP ACL, P121, DOI [10.18653/v1/W16-1614, DOI 10.18653/V1/W16-1614]; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Brutzkus Alon, 2018, P ICLR; Conneau A., 2018, ICLR; Daskalakis C., 2018, ICLR; Dinu Georgiana, 2015, P ICLR WORKSH PAP; Ezra Ester, 2006, SGC; Faruqui Manaal, 2014, P EACL, DOI [10.3115/v1/E14-1049, DOI 10.3115/V1/E14-1049]; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gouws S., 2015, P NAACL HLT, P1302; Grave Edouard, 2018, CORR; Gulrajani I., 2017, P NIPS; Hauer Bradley., 2017, P 15 C EUR ASS COMP, P619; Hoshen Yedid, 2018, CORR; Jastrzebski Stanislaw, 2018, P ICLR; Lample Guillaume, 2018, P ICLR C PAPERS; Li Hao, 2018, ICLR; Liu J, 2016, PROC SPIE, V10033, DOI 10.1117/12.2248362; Mikolov T., 2013, CORR; Radovanovic M, 2010, J MACH LEARN RES, V11, P2487; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Smith Samuel L, 2017, ARXIV171006451; Smith Samuel L., 2017, P ICLR C TRACK; Sogaard Ander, 2018, P ACL; Vulic I, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P247; Wei Xiang, 2018, P ICLR; Xing Chao, 2015, P NAACL HLT; Xu Ruochen, 2018, P 2018 C EMP METH NA, P2465, DOI DOI 10.18653/V1/D18-1268; Yang Puyudi, 2018, CORR; Zhang Meng, 2017, P ACL	38	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306008
C	Hiraoka, T; Imagawa, T; Mori, T; Onishi, T; Tsuruoka, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hiraoka, Takuya; Imagawa, Takahisa; Mori, Tatsuya; Onishi, Takashi; Tsuruoka, Yoshimasa			Learning Robust Options by Conditional Value at Risk Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MINIMAX; TIME	Options are generally learned by using an inaccurate environment model (or simulator), which contains uncertain model parameters. While there are several methods to learn options that are robust against the uncertainty of model parameters, these methods only consider either the worst case or the average (ordinary) case for learning options. This limited consideration of the cases often produces options that do not work well in the unconsidered case. In this paper, we propose a conditional value at risk (CVaR)-based method to learn options that work well in both the average and worst cases. We extend the CVaR-based policy gradient method proposed by Chow and Ghavamzadeh (2014) to deal with robust Markov decision processes and then apply the extended method to learning robust options. We conduct experiments to evaluate our method in multi-joint robot control tasks (HopperIceBlock, Half-Cheetah, and Walker2D). Experimental results show that our method produces options that 1) give better worst-case performance than the options learned only to minimize the average-case loss, and 2) give better average-case performance than the options learned only to minimize the worst-case loss.	[Hiraoka, Takuya; Mori, Tatsuya; Onishi, Takashi] NEC Corp Ltd, Tokyo, Japan; [Hiraoka, Takuya; Imagawa, Takahisa; Mori, Tatsuya; Onishi, Takashi; Tsuruoka, Yoshimasa] Natl Inst Adv Ind Sci & Technol, Tokyo, Japan; [Hiraoka, Takuya; Mori, Tatsuya] RIKEN, Ctr Adv Intelligence Project, Wako, Saitama, Japan; [Tsuruoka, Yoshimasa] Univ Tokyo, Tokyo, Japan	NEC Corporation; National Institute of Advanced Industrial Science & Technology (AIST); RIKEN; University of Tokyo	Hiraoka, T (corresponding author), NEC Corp Ltd, Tokyo, Japan.; Hiraoka, T (corresponding author), Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.; Hiraoka, T (corresponding author), RIKEN, Ctr Adv Intelligence Project, Wako, Saitama, Japan.	takuya-h1@nec.com; imagawa.t@aist.go.jp; tmori@nec.com; takashi.onishi@nec.com; tsuruoka@logos.t.u-tokyo.ac.jp						[Anonymous], 2009, ADV NEURAL INFORM PR; Bacon P.-L., 2017, P AAAI; Bagnell J. D., 2001, CMURITR0125; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Boda K, 2006, MATH METHOD OPER RES, V63, P169, DOI 10.1007/s00186-005-0045-1; Brockman G., 2016, ARXIV160601540CSLG; Chow Y., 2014, P ADV NEURAL INFORM, P3509; Derman E., 2018, P UAI; Frans K., 2018, P ICLR; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Heger M., 1994, P INT MACH LEARN C, P105; Henderson P., 2017, ARXIV170804352CSAI; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; James S., 2018, ARXIV181207252CSRO ARXIV181207252CSRO; Klissarov M., 2017, ARXIV171200004CSLG; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Mankowitz D. J., 2018, P AAAI; McGovern A., 2001, COMPUTER SCI DEP FAC, V8; Morimura T., 2010, PROC 27 INT C MACH L, P799; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Pinto L, 2017, PR MACH LEARN RES, V70; Rajeswaran A., 2017, P ICLR; Rockafellar RT, 2002, J BANK FINANC, V26, P1443, DOI 10.1016/S0378-4266(02)00271-6; Schulman J., 2017, ARXIV170706347CSLG; Schulman John, 2016, P ICLR; Silver D., 2012, P ICML; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Wawrzynski P, 2009, NEURAL NETWORKS, V22, P1484, DOI 10.1016/j.neunet.2009.05.011; Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566; Xu Huan, 2010, ADV NEURAL INFORM PR, P2505	35	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302060
C	Hirt, M; Dellaportas, P; Durmus, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hirt, Marcel; Dellaportas, Petros; Durmus, Alain			Copula-like Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION; HORSESHOE	This paper considers a new family of variational distributions motivated by Sklar's theorem. This family is based on new copula-like densities on the hypercube with non-uniform marginals which can be sampled efficiently, i.e. with a complexity linear in the dimension d of the state space. Then, the proposed variational densities that we suggest can be seen as arising from these copula-like densities used as base distributions on the hypercube with Gaussian quantile functions and sparse rotation matrices as normalizing flows. The latter correspond to a rotation of the marginals with complexity O(d log d). We provide some empirical evidence that such a variational family can also approximate non-Gaussian posteriors and can be beneficial compared to Gaussian approximations. Our method performs largely comparably to state-of-the-art variational approximations on standard regression and classification benchmarks for Bayesian Neural Networks.	[Hirt, Marcel; Dellaportas, Petros] UCL, Dept Stat Sci, London, England; [Dellaportas, Petros] Athens Univ Econ & Business, Dept Stat, Athens, Greece; [Dellaportas, Petros] Alan Turing Inst, London, England; [Durmus, Alain] Univ Paris Saclay, Ecole Normale Super Paris Saclay, CNRS, CMLA, F-94235 Cachan, France	University of London; University College London; Athens University of Economics & Business; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay	Hirt, M (corresponding author), UCL, Dept Stat Sci, London, England.	marcel.hirt.16@ucl.ac.uk; alain.durmus@cmla.ens-cachan.fr	Dellaportas, Petros/AAI-7042-2021		Chaire BayeScale "P. Laffitte"; Polish National Science Center [NCN UMO-2018/31/B/ST1/0025]; Alan Turing Institute under the EPSRC [EP/N510129/1]	Chaire BayeScale "P. Laffitte"; Polish National Science Center; Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Alain Durmus acknowledges support from Chaire BayeScale "P. Laffitte" and from Polish National Science Center grant: NCN UMO-2018/31/B/ST1/0025. This research has been partly financed by the Alan Turing Institute under the EPSRC grant EP/N510129/1. The authors acknowledge the use of the UCL Myriad High Throughput Computing Facility (Myriad@UCL), and associated support services, in the completion of this work.	Alan Gen z., 1998, MONTE CARLO QUASIMON, P199; Barber D, 1998, ADV NEUR IN, V10, P395; Bedford T, 2001, ANN MATH ARTIF INTEL, V32, P245, DOI 10.1023/A:1016725902970; Berg R. v. d., 2018, P UNC ART INT, P1; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; Carvalho CM, 2010, BIOMETRIKA, V97, P465, DOI 10.1093/biomet/asq017; Davidson TR, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P856; Dellaportas Petros, 2018, J ECONOMETRICS; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dissmann J, 2013, COMPUT STAT DATA AN, V59, P52, DOI 10.1016/j.csda.2012.08.010; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Fang K.T., 2017, SYMMETRIC MULTIVARIA; Figurnov Mikhail, 2018, ADV NEURAL INFORM PR, P441; Gal Y, 2016, PR MACH LEARN RES, V48; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gershman S.J., 2012, P 29 INT C MACH LEAR, P235; Ghosh S., 2017, ARXIV170510388; Ghosh S, 2018, PR MACH LEARN RES, V80; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Guo F., 2016, ADV NEURAL INFORM PR; Han SB, 2016, JMLR WORKSH CONF PRO, V51, P829; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hron J, 2018, PR MACH LEARN RES, V80; Huszar F., 2017, ARXIV PREPRINT ARXIV; Ingraham J, 2017, PR MACH LEARN RES, V70; Jaakkola TS., 1997, P MACHINE LEARNING R, VR1, P283; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Khaled Mohamad A, 2017, ARXIV170510440; Khan Emtiyaz, 2013, P 30 INT C MACH LEAR, P951; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kosmidis I, 2016, STAT COMPUT, V26, P1079, DOI 10.1007/s11222-015-9590-5; Krueger D., 2017, ARXIV PREPRINT ARXIV; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Locatello Francesco, 2018, P 21 INT C ART INT S, V84, P464; Locatello Francesco, 2018, ADV NEURAL INFORM PR; Louizos C, 2017, ADV NEUR IN, V30; Louizos C, 2017, PR MACH LEARN RES, V70; Louizos C, 2016, PR MACH LEARN RES, V48; Mathieu M., 2014, FAST APPROXIMATION R; Mescheder L, 2017, PR MACH LEARN RES, V70; Miller AC, 2017, PR MACH LEARN RES, V70; Mishkin A, 2018, ADV NEUR IN, V31; MOORE DS, 1975, ANN STAT, V3, P599, DOI 10.1214/aos/1176343125; Munkhoeva M, 2018, ADV NEUR IN, V31; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neville SE, 2014, ELECTRON J STAT, V8, P1113, DOI 10.1214/14-EJS910; Ong VMH, 2018, J COMPUT GRAPH STAT, V27, P465, DOI 10.1080/10618600.2017.1390472; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Papaspiliopoulos Omiros, 2003, BAYESIAN STAT, V7; Piironen J, 2017, ELECTRON J STAT, V11, P5018, DOI 10.1214/17-EJS1337SI; Ranganath R, 2016, PR MACH LEARN RES, V48; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sklar A., 1959, PUBLICATIONS I STAT, V8, P229, DOI DOI 10.1007/BF01544178; Titsias MK, 2019, PR MACH LEARN RES, V89, P167; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Tomczak Jakub M, 2016, ARXIV161109630; Tran D., 2017, ARXIV170208896, V7, P3; Tran Dustin, 2015, P NIPS 15, P3564; Vuppalapati C, 2018, INT CONF MACH LEARN, P161; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yin MZ, 2018, PR MACH LEARN RES, V80	70	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302090
C	Hoyer, L; Munoz, M; Katiyar, P; Khoreva, A; Fischer, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hoyer, Lukas; Munoz, Mauricio; Katiyar, Prateek; Khoreva, Anna; Fischer, Volker			Grid Saliency for Context Explanations of Semantic Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently, there has been a growing interest in developing saliency methods that provide visual explanations of network predictions. Still, the usability of existing methods is limited to image classification models. To overcome this limitation, we extend the existing approaches to generate grid saliencies, which provide spatially coherent visual explanations for (pixel-level) dense prediction networks. As the proposed grid saliency allows to spatially disentangle the object and its context, we specifically explore its potential to produce context explanations for semantic segmentation networks, discovering which context most influences the class predictions inside a target object area. We investigate the effectiveness of grid saliency on a synthetic dataset with an artificially induced bias between objects and their context as well as on the real-world Cityscapes dataset using state-of-the-art segmentation networks. Our results show that grid saliency can be successfully used to provide easily interpretable context explanations and, moreover, can be employed for detecting and localizing contextual biases present in the data.	[Hoyer, Lukas; Munoz, Mauricio; Katiyar, Prateek; Khoreva, Anna; Fischer, Volker] Bosch Ctr Artificial Intelligence, Baden, Switzerland		Hoyer, L (corresponding author), Bosch Ctr Artificial Intelligence, Baden, Switzerland.	lukas.hoyer@outlook.com; mauricio.munoz@bosch.com; prateek.katiyar@bosch.com; anna.khoreva@bosch.com; volker.fischer@bosch.com						Al-Shedivat Maruan, 2018, ARXIV170510301; [Anonymous], 2016, P IEEE C COMP VIS PA; [Anonymous], 2018, ADV NEURAL INFORM PR; Azaza A, 2018, COMPUT VIS IMAGE UND, V174, P1, DOI 10.1016/j.cviu.2018.06.002; Bach Sebastian, 2015, PLOS ONE; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Chandra Siddhartha, 2016, P EUR C COMP VIS ECC; Chang C.-H., 2019, INT C LEARN REPR ICL; Chen L.-C., 2018, P EUR C COMP VIS ECC; Chen L.-C., 2015, ICLR; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen ZQ, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511284; Cordts M., 2016, P IEEE C COMP VIS PA; Dabkowski Piotr, 2017, ADV NEURAL INFORM PR; Dong Yinpeng, 2017, P IEEE C COMP VIS PA; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Ghiasi Golnaz, 2016, P EUR C COMP VIS ECC; Gulshad S., 2019, ARXIV190408279; Hendricks Lisa Anne, 2016, P EUR C COMP VIS ECC; Hong S., 2015, ADV NEURAL INFORM PR, P1495; Jampani Varun, 2016, P IEEE C COMP VIS PA; JoseOramas M. Oramas, 2016, ARXIV160400036; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karpathy Andrey, 2015, P IEEE C COMP VIS PA; LeCun Yann, 1998, TECHNICAL REPORT; Lee Mark, 2019, ARXIV190611897; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin T.-Y., 2014, ECCV; Mazzini D, 2018, GUIDED UPSAMPLING NE; Mottaghi R., 2014, ROLE CONTEXT OBJECT; Noh H., 2015, P IEEE INT C COMP VI; Park Dong Huk, 2018, P IEEE C COMP VIS PA; Paszke Adam, 2017, ARXIV160601247; Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353; Poudel R.P., 2019, BMVC; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Selvaraju Ramprasaath R, 2016, ARXIV161107450; Shelhamer Evan, 2015, P IEEE C COMP VIS PA; Simonyan Karen, 2013, INT C LEARN REPR ICL; Simonyan Karen, 2015, INT C LEARN REPR; Smilkov D., 2017, SMOOTHGRAD REMOVING; SPRINGENBERG JT, 2014, INT C LEARN REPR ICL; Sundararajan Mukund, 2017, INT C MACH LEARN ICM; Uijlings JRR, 2012, INT J COMPUT VISION, V96, P46, DOI 10.1007/s11263-011-0443-1; Yu Fisher, 2016, P INT C LEARN REPR; Zeiler M.D., 2014, P EUR C COMP VIS ECC; Zhang Jianming, 2017, INT J COMPUTER VISIO; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhao Hengshuang, 2018, P EUR C COMP VIS ECC; Zheng S., 2015, P IEEE INT C COMP VI; Zintgraf Luisa M., 2017, INT C LEARN REPR ICL	52	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306046
C	Huleihel, W; Mazumdar, A; Medard, M; Pal, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huleihel, Wasim; Mazumdar, Arya; Medard, Muriel; Pal, Soumyabrata			Same-Cluster Querying for Overlapping Clusters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Overlapping clusters are common in models of many practical data-segmentation applications. Suppose we are given n elements to be clustered into k possibly overlapping clusters, and an oracle that can interactively answer queries of the form "do elements ui and v belong to the same cluster?" The goal is to recover the clusters with minimum number of such queries. This problem has been of recent interest for the case of disjoint clusters. In this paper, we look at the more practical scenario of overlapping clusters, and provide upper bounds (with algorithms) on the sufficient number of queries. We provide algorithmic results under both arbitrary (worst-case) and statistical modeling assumptions. Our algorithms are parameter free, efficient, and work in the presence of random noise. We also derive information-theoretic lower bounds on the number of queries needed, proving that our algorithms are order optimal. Finally, we test our algorithms over both synthetic and real-world data, showing their practicality and effectiveness.	[Huleihel, Wasim] Tel Aviv Univ, Dept Elect Engn, IL-6997801 Tel Aviv, Israel; [Mazumdar, Arya; Pal, Soumyabrata] Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Medard, Muriel] MIT, Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Tel Aviv University; University of Massachusetts System; University of Massachusetts Amherst; Massachusetts Institute of Technology (MIT)	Huleihel, W (corresponding author), Tel Aviv Univ, Dept Elect Engn, IL-6997801 Tel Aviv, Israel.	wasimh@mail.tau.ac.il; arya@cs.umass.edu; medard@mit.edu; soumyabratap@umass.edu			NSF [CCF 1642658, 1642550, 1618512, 1909046]	NSF(National Science Foundation (NSF))	This research is supported in part by NSF Grants CCF 1642658, 1642550, 1618512, and 1909046.	Ahn K, 2016, ANN ALLERTON CONF, P657, DOI 10.1109/ALLERTON.2016.7852294; Ailon Nir, 2017, ARXIV170401862; ARABIE P, 1981, J MARKETING RES, V18, P310, DOI 10.2307/3150972; Ashtiani H., 2016, NIPS; Banerjee A, 2005, P 11 ACM SIGKDD INT, P532, DOI DOI 10.1145/1081870.1081932; Bonchi F, 2013, KNOWL INF SYST, V35, P1, DOI 10.1007/s10115-012-0522-9; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chien C., 2018, TARXIV180605938; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Coxeter H.S.M., 2003, PROJECTIVE GEOMETRY; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Firmani D, 2016, PROC VLDB ENDOW, V9, P384; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kim Taewan, 2017, ARXIV170903202; Mao X., 2018, ADV NEURAL INFORM PR, P2126; Mazumdar A., 2017, ADV NEURAL INFORM PR, P31; Mazumdar A., 2017, 31 AAAI C ART INT AA; Mazumdar A., 2017, ADV NEURAL INFORM PR, P4682; Nugent R, 2010, METHODS MOL BIOL, V620, P369, DOI 10.1007/978-1-60761-580-4_12; Tsourakakis C. E., 2017, ARXIV170907308; Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982; Vinayak R.K., 2016, ADV NEURAL INFORM PR, P1316; Wang J., 2013, LEVERAGING TRANSITIV, P229; Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263; Zamir O., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P46, DOI 10.1145/290941.290956; Zou J., 2015, P 3 AAAI C HUM COMP	27	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902015
C	Hurwitz, CL; Xu, K; Srivastava, A; Buccino, AP; Hennig, MH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hurwitz, Cole L.; Xu, Kai; Srivastava, Akash; Buccino, Alessio P.; Hennig, Matthias H.			Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Determining the positions of neurons in an extracellular recording is useful for investigating functional properties of the underlying neural circuitry. In this work, we present a Bayesian modelling approach for localizing the source of individual spikes on high-density, microelectrode arrays. To allow for scalable inference, we implement our model as a variational autoencoder and perform amortized variational inference. We evaluate our method on both biophysically realistic simulated and real extracellular datasets, demonstrating that it is more accurate than and can improve spike sorting performance over heuristic localization methods such as center of mass.	[Hurwitz, Cole L.; Xu, Kai; Hennig, Matthias H.] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland; [Srivastava, Akash] MIT, IBM Watson AI Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Buccino, Alessio P.] Univ Oslo, Dept Informat, Oslo, Norway	University of Edinburgh; Massachusetts Institute of Technology (MIT); University of Oslo	Hurwitz, CL (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	cole.hurwitz@ed.ac.uk; kai.xu@ed.ac.uk; Akash.Srivastava@ibm.com; alessiob@ifi.uio.no; m.hennig@ed.ac.uk		Srivastava, Akash/0000-0002-6218-1770				Ballini M, 2014, IEEE J SOLID-ST CIRC, V49, P2705, DOI 10.1109/JSSC.2014.2359219; Berdondini L, 2005, BIOSENS BIOELECTRON, V21, P167, DOI 10.1016/j.bios.2004.08.011; Blanche TJ, 2005, J NEUROPHYSIOL, V93, P2987, DOI 10.1152/jn.01023.2004; Buccino Alessio P, SPIKEINTERFACE UNIFI; Buccino Alessio P, MEAREC FAST CUSTOMIZ; Buzsaki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233; Carlson D, 2019, CURR OPIN NEUROBIOL, V55, P90, DOI 10.1016/j.conb.2019.02.007; Chelaru MI, 2005, J NEUROSCI METH, V142, P305, DOI 10.1016/j.jneumeth.2004.09.004; Chung JE, 2017, NEURON, V95, P1381, DOI 10.1016/j.neuron.2017.08.030; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Diederik P, 2013, P 2 INT C LEARNING R; Espen Hagen, 2018, FRONTIERS NEUROINFOR; Eversmann B., 2003, IEEE Journal of Solid-State Circuits, V38, P2306, DOI 10.1109/JSSC.2003.819174; Ferenc Mechler, 2011, AM J PHYSL HEART CIR; Frey U, 2010, IEEE J SOLID-ST CIRC, V45, P467, DOI 10.1109/JSSC.2009.2035196; GE H, 2018, INT C ART INT STAT A, P1682; George Dimitriadis, WHY NOT RECORD EVERY; Gray CM, 1995, J NEUROSCI METH, V63, P43, DOI 10.1016/0165-0270(95)00085-2; Hagen E, 2015, J NEUROSCI METH, V245, P182, DOI 10.1016/j.jneumeth.2015.01.029; Harris KD, 2000, J NEUROPHYSIOL, V84, P401, DOI 10.1152/jn.2000.84.1.401; Hennig Matthias H, 2018, ARXIV180901051; Henze DA, 2000, J NEUROPHYSIOL, V84, P390, DOI 10.1152/jn.2000.84.1.390; Hilgen G, 2017, CELL REP, V18, P2521, DOI 10.1016/j.celrep.2017.02.038; Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179; Jaeyoon Jun James, REAL TIME SPIKE SORT; Jimenez Rezende Danilo, 2014, ARXIV14014082; Jouty J, 2018, FRONT CELL NEUROSCI, V12, DOI 10.3389/fncel.2018.00481; Jun James J, 2017, NATURE, V551, P7679; Kelly RC, 2007, J NEUROSCI, V27, P261, DOI 10.1523/JNEUROSCI.4906-06.2007; Kubo T, 2008, IEEE ENG MED BIO, P5021, DOI 10.1109/IEMBS.2008.4650341; Lee CW, 2007, P ANN INT IEEE EMBS, P1282, DOI 10.1109/IEMBS.2007.4352531; Lopez CM, 2016, ISSCC DIG TECH PAP I, V59, P392, DOI 10.1109/ISSCC.2016.7418072; McConnell SC, 2018, ELIFE, V7, DOI 10.7554/eLife.40189; Mechler F, 2012, J COMPUT NEUROSCI, V32, P73, DOI 10.1007/s10827-011-0341-0; Miller EK, 2008, NEURON, V60, P483, DOI 10.1016/j.neuron.2008.10.033; MOON CM, 2017, ADV NEURAL INFORM PR, P540; Muller J, 2015, LAB CHIP, V15, P2767, DOI 10.1039/c5lc00133a; Muthmann JO, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00028; Nadasdy Z, 1998, NEURONAL ENSEMBLES: STRATEGIES FOR RECORDING AND DECODING, P17; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2, P2, DOI DOI 10.1201/B10905-6; Neto JP, 2016, J NEUROPHYSIOL, V116, P892, DOI 10.1152/jn.00103.2016; Obien MEJ, 2015, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00423; Pachitariu M., 2016, NIPS P; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Paolo Buccino Alessio, 2018, J NEUROPHYSIOLOGY; Prentice JS, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0019884; Ramaswamy S, 2015, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00044; Rey HG, 2015, BRAIN RES BULL, V119, P106, DOI 10.1016/j.brainresbull.2015.04.007; Ruz ID, 2014, J NEUROSCI METH, V233, P115, DOI 10.1016/j.jneumeth.2014.05.037; Segev R, 2004, NAT NEUROSCI, V7, P1155, DOI 10.1038/nn1323; Somogyvari Z, 2005, J NEUROSCI METH, V147, P126, DOI 10.1016/j.jneumeth.2005.04.002; Somogyvari Z, 2012, EUR J NEUROSCI, V36, P3299, DOI 10.1111/j.1460-9568.2012.08249.x; Speiser A, 2017, ADV NEUR IN, V30; Yuan J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P1; Zanoci Cristian, 2019, PHYS REV E, V99; ZHANG L, 2009, ADV NEURAL INFORM PR, P2163	56	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304070
C	Ignatiadis, N; Wager, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ignatiadis, Nikolaos; Wager, Stefan			Covariate-Powered Empirical Bayes Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGRESSION	We study methods for simultaneous analysis of many noisy experiments in the presence of rich covariate information. The goal of the analyst is to optimally estimate the true effect underlying each experiment. Both the noisy experimental results and the auxiliary covariates are useful for this purpose, but neither data source on its own captures all the information available to the analyst. In this paper, we propose a flexible plug-in empirical Bayes estimator that synthesizes both sources of information and may leverage any black-box predictive model. We show that our approach is within a constant factor of minimax for a simple data-generating model. Furthermore, we establish robust convergence guarantees for our method that hold under considerable generality, and exhibit promising empirical performance on both real and simulated data.	[Ignatiadis, Nikolaos] Stanford Univ, Stat Dept, Stanford, CA 94305 USA; [Wager, Stefan] Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA	Stanford University; Stanford University	Ignatiadis, N (corresponding author), Stanford Univ, Stat Dept, Stanford, CA 94305 USA.	ignat@stanford.edu; swager@stanford.edu		Ignatiadis, Nikolaos/0000-0001-7767-2583	Google	Google(Google Incorporated)	The authors are grateful for enlightening conversations with Brad Efron, Guido Imbens, Panagiotis Lolas and Paris Syminelakis. This research was funded by a gift from Google.	Abadie, 2018, REV EC STAT; Agarwal A, 2009, IMMUNE INFERTILITY, P155, DOI 10.1007/978-3-642-01379-9_3.2; [Anonymous], [No title captured]; Banerjee Trambak, 2018, ARXIV181111930; Baranchik Alvin J, 1964, TECHNICAL REPORT; Benhaddou R, 2013, J STAT PLAN INFER, V143, P1672, DOI 10.1016/j.jspi.2013.06.005; Besancon M., 2019, ARXIV190708611; Bezanson J, 2017, SIAM REV, V59, P65, DOI 10.1137/141000671; Brown LD, 2007, ANN STAT, V35, P2219, DOI 10.1214/009053607000000145; Brown LD, 2009, ANN STAT, V37, P1685, DOI 10.1214/08-AOS630; BROWN LD, 1971, ANN MATH STAT, V42, P855, DOI 10.1214/aoms/1177693318; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Chernozhukov Victor, 2017, ECONOMETRICS J; CLEVELAND WS, 1988, J AM STAT ASSOC, V83, P596, DOI 10.2307/2289282; Coey D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P285, DOI 10.1145/3308558.3313452; Cohen N, 2013, STAT SINICA, V23, P333, DOI 10.5705/ss.2011.071; Dua D., 2017, UCI MACHINE LEARNING; Duchi John, LECT NOTES STAT; Efron B, 2001, J AM STAT ASSOC, V96, P1151, DOI 10.1198/016214501753382129; EFRON B, 1973, J AM STAT ASSOC, V68, P117, DOI 10.2307/2284155; Efron B., 2010, LARGE SCALE INFERENC; Efron B, 2011, J AM STAT ASSOC, V106, P1602, DOI 10.1198/jasa.2011.tm11181; FAY RE, 1979, J AM STAT ASSOC, V74, P269, DOI 10.1080/01621459.1979.10482505; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; GREEN EJ, 1991, J AM STAT ASSOC, V86, P1001, DOI 10.2307/2290517; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Ibragimov I.A., 1981, STAT ESTIMATION ASYM, V16; Ignatiadis N., 2019, ARXIV190202774; IGNATIADIS N., 2018, ARXIV170105179; Ignatiadis N, 2016, NAT METHODS, V13, P577, DOI [10.1038/NMETH.3885, 10.1038/nmeth.3885]; Ignatiadis Nikolaos, 2019, ARXIV191105970; James W., 1961, P 4 BERKELEY S MATH, V1, P361, DOI DOI 10.1007/978-1-4612-0919-5; Janson L, 2017, J R STAT SOC B, V79, P1037, DOI 10.1111/rssb.12203; Jiang JM, 2011, J AM STAT ASSOC, V106, P732, DOI 10.1198/jasa.2011.tm10221; Jiang WH, 2009, ANN STAT, V37, P1647, DOI 10.1214/08-AOS638; Johnstone IM, 2004, ANN STAT, V32, P1594, DOI 10.1214/009053604000000030; Kou SC, 2017, CONTRIB STAT, P249, DOI 10.1007/978-3-319-41573-4_13; Li JJ, 2005, J STAT PLAN INFER, V131, P101, DOI 10.1016/j.jspi.2003.12.017; LI KC, 1984, ANN STAT, V12, P887, DOI 10.1214/aos/1176346709; LI KC, 1986, ANN STAT, V14, P1101, DOI 10.1214/aos/1176350052; Lonnstedt I, 2002, STAT SINICA, V12, P31; Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8; McMahan HB, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1222; Mogensen P. K., 2018, J OPEN SOURCE SOFTWA, V3; Mukhopadhyay Pushpal, 2004, P ASA SECT SURV RES, V4058, P4065; MUKHOPADHYAY S, 1995, J ROY STAT SOC D-STA, V44, P389; Muralidharan O, 2010, ANN APPL STAT, V4, P422, DOI 10.1214/09-AOAS276; Nie Xinkun, 2018, ARXIV171204912; Opsomer JD, 2008, J R STAT SOC B, V70, P265, DOI 10.1111/j.1467-9868.2007.00635.x; Penskaya M Ya, 1995, J MATH SCI, V75, P1524; Redmond M, 2002, EUR J OPER RES, V141, P660, DOI 10.1016/S0377-2217(01)00264-8; Reid S, 2016, STAT SINICA, V26, P35, DOI 10.5705/ss.2014.042; ROBBINS H, 1964, ANN MATH STAT, V35, P1, DOI 10.1214/aoms/1177703729; Ross SA, 2019, J IND ECOL, V23, P335, DOI 10.1111/jiec.12742; SCHICK A, 1986, ANN STAT, V14, P1139, DOI 10.1214/aos/1176350055; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Stephan J, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms8432; Stephens M, 2017, BIOSTATISTICS, V18, P275, DOI 10.1093/biostatistics/kxw041; Tan ZQ, 2016, STAT SINICA, V26, P1219, DOI 10.5705/ss.202014.0069; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Wager Stefan, 2015, ARXIV150700832; Weinstein A, 2018, J AM STAT ASSOC, V113, P698, DOI 10.1080/01621459.2017.1280406; Xie XC, 2012, J AM STAT ASSOC, V107, P1465, DOI 10.1080/01621459.2012.728154; Zhu A., 2018, BIOINFORMATICS	65	1	1	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901027
C	Ioannou, N; Mendler-Dunner, C; Parnell, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ioannou, Nikolas; Mendler-Dunner, Celestine; Parnell, Thomas			SySCD: A System-Aware Parallel Coordinate Descent Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper we propose a novel parallel stochastic coordinate descent (SCD) algorithm with convergence guarantees that exhibits strong scalability. We start by studying a state-of-the-art parallel implementation of SCD and identify scalability as well as system-level performance bottlenecks of the respective implementation. We then take a principled approach to develop a new SCD variant which is designed to avoid the identified system bottlenecks, such as limited scaling due to coherence traffic of model sharing across threads, and inefficient CPU cache accesses. Our proposed system-aware parallel coordinate descent algorithm (SySCD) scales to many cores and across numa nodes, and offers a consistent bottom line speedup in training time of up to x12 compared to an optimized asynchronous parallel SCD algorithm and up to x42, compared to state-of-the-art GLM solvers (scikit-learn, Vowpal Wabbit, and H2O) on a range of datasets and multi-core CPU architectures.	[Ioannou, Nikolas; Mendler-Dunner, Celestine; Parnell, Thomas] IBM Res, Zurich, Switzerland; [Mendler-Dunner, Celestine] Univ Calif Berkeley, Berkeley, CA USA	International Business Machines (IBM); University of California System; University of California Berkeley	Ioannou, N (corresponding author), IBM Res, Zurich, Switzerland.	nio@zurich.ibm.com; mendler@berkeley.edu; tpa@zurich.ibm.com						[Anonymous], 2008, PASC LARG SCAL LEARN; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Bradley J., 2011, ICML; Criteo-Labs, 2013, TER CLICK LOGS DAT; Dunner C., 2017, ADV NEURAL INFORM PR, P4258; Dunner C., 2016, ICML, P783; Dunner C., 2018, P INT C MACH LEARN I, V80, P1358; Dunner Celestine, 2018, ADV NEURAL INFORM PR, P250; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2370; Kaggle, 2017, KAGGL MACH LEARN DAT; Kaggle, 2019, LIBSVM DATA CLASSIFI; Kaufmann M., 2018, ARXIV181102322; Langford J., 2007, VOWPAL WABBIT; Lee C.-p., 2018, ARXIV170903043; Liu J, 2015, J MACH LEARN RES, V16, P285; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Ma CX, 2015, PR MACH LEARN RES, V37, P1973; Parnell T, 2017, IEEE SYM PARA DISTR, P419, DOI 10.1109/IPDPSW.2017.140; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Richtarik P, 2016, J MACH LEARN RES, V17; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; Smith V., 2018, COCOA GEN FRAMEWORK, P18; The H2O.ai team, 2015, H2O PYTH INT H2O; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Yang T., 2013, ADV NEURAL INFORM PR, P629; Zhang C, 2014, PROC VLDB ENDOW, V7, P1283, DOI 10.14778/2732977.2733001	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300054
C	Irpan, A; Rao, K; Bousmalis, K; Harris, C; Ibarz, J; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Irpan, Alex; Rao, Kanishka; Bousmalis, Konstantinos; Harris, Chris; Ibarz, Julian; Levine, Sergey			Off-Policy Evaluation via Off-Policy Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ARCADE LEARNING-ENVIRONMENT	In this work, we consider the problem of model selection for deep reinforcement learning (RL) in real-world environments. Typically, the performance of deep RL algorithms is evaluated via on-policy interactions with the target environment. However, comparing models in a real-world environment for the purposes of early stopping or hyperparameter tuning is costly and often practically infeasible. This leads us to examine off-policy policy evaluation (OPE) in such settings. We focus on OPE for value-based methods, which are of particular interest in deep RL, with applications like robotics, where off-policy algorithms based on Q-function estimation can often attain better sample complexity than direct policy optimization. Existing OPE metrics either rely on a model of the environment, or the use of importance sampling (IS) to correct for the data being off-policy. However, for high-dimensional observations, such as images, models of the environment can be difficult to fit and value-based methods can make IS hard to use or even ill-conditioned, especially when dealing with continuous action spaces. In this paper, we focus on the specific case of MDPs with continuous action spaces and sparse binary rewards, which is representative of many important real-world applications. We propose an alternative metric that relies on neither models nor IS, by framing OPE as a positive-unlabeled (PU) classification problem with the Q-function as the decision function. We experimentally show that this metric outperforms baselines on a number of tasks. Most importantly, it can reliably predict the relative performance of different policies in a number of generalization scenarios, including the transfer to the real-world of policies trained in simulation for an image-based robotic manipulation task.	[Irpan, Alex; Rao, Kanishka; Harris, Chris; Ibarz, Julian; Levine, Sergey] Google Brain, Mountain View, CA 94043 USA; [Bousmalis, Konstantinos] DeepMind, London, England; [Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	Google Incorporated; University of California System; University of California Berkeley	Irpan, A (corresponding author), Google Brain, Mountain View, CA 94043 USA.	alexirpan@google.com; kanishkarao@google.com; konstantinos@google.com; ckharris@google.com; julianibarz@google.com; slevine@google.com						[Anonymous], 2002, ICML; Arjona-Medina J. A., 2018, ARXIV180607857; Babaeizadeh M., 2018, INT C REPR LEARN; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Brockman G., 2016, OPENAI GYM; Cobbe K., 2018, ARXIV181202341, P1; Deng J., 2009, CVPR 2009; Dudik M., 2011, ICML; Farahmand AM, 2011, MACH LEARN, V85, P299, DOI 10.1007/s10994-011-5254-7; Hanna JP, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P538; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; James S., 2019, IEEE C COMP VIS PATT; Jiang N., 2015, DOUBLY ROBUST POLICY; Kalashnikov D, 2018, P 2 C ROB LEARN, V87, P651; Kiryo R., 2017, NEURIPS, P1675; Koos S., 2010, PROC 2019 GENET EVOL, P119, DOI [10.1145/1830483.1830505, DOI 10.1145/1830483.1830505]; Koos S., 2012, IEEE T EVOLUTIONARY, V17, P122; Lee Alex X, 2018, ARXIV180401523; Lillicrap TP, 2016, 4 INT C LEARN REPR; Liu Y, 2018, NEURIPS; Machado MC, 2018, J ARTIF INTELL RES, V61, P523, DOI 10.1613/jair.5699; Mannor S, 2007, MANAGE SCI, V53, P308, DOI 10.1287/mnsc.1060.0614; Mnih V., 2018, INT C MACH LEARN; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Murphy SA, 2005, J MACH LEARN RES, V6, P1073; Nichol A., 2018, ARXIV180403720; Precup D., 2000, INT C MACH LEARN, P759; Quillen D., 2018, DEEP REINFORCEMENT L; Raghu M., 2018, INT C MACH LEARN, P4235; Ross S., 2010, PROC 13 INT C ARTIF, V9, P661; Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159; THEOCHAROUS G, 2015, IJCAI, P1806; Thomas P., 2016, INT C MACH LEARN, P2139; Thomas P. S., 2015, AAAI; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Zhang A., 2018, DISSECTION OVERFITTI; Zhang C., 2018, STUDY OVERFITTING DE	40	1	1	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305043
C	Jahja, M; Farrow, D; Rosenfeld, R; Tibshirani, RJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jahja, Maria; Farrow, David; Rosenfeld, Roni; Tibshirani, Ryan J.			Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The Kalman filter (KF) is one of the most widely used tools for data assimilation and sequential estimation. In this work, we show that the state estimates from the KF in a standard linear dynamical system setting are equivalent to those given by the KF in a transformed system, with infinite process noise (i.e., a "flat prior") and an augmented measurement space. This reformulation-which we refer to as augmented measurement sensor fusion (SF)-is conceptually interesting, because the transformed system here is seemingly static (as there is effectively no process model), but we can still capture the state dynamics inherent to the KF by folding the process model into the measurement space. Further, this reformulation of the KF turns out to be useful in settings in which past states are observed eventually (at some lag). Here, when the measurement noise covariance is estimated by the empirical covariance, we show that the state predictions from SF are equivalent to those from a regression of past states on past measurements, subject to particular linear constraints (reflecting the relationships encoded in the measurement map). This allows us to port standard ideas (say, regularization methods) in regression over to dynamical systems. For example, we can posit multiple candidate process models, fold all of them into the measurement model, transform to the regression perspective, and apply l(1) penalization to perform process model selection. We give various empirical demonstrations, and focus on an application to nowcasting the weekly incidence of influenza in the US.	[Jahja, Maria] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA; [Farrow, David] Carnegie Mellon Univ, Computat Biol Dept, Pittsburgh, PA 15213 USA; [Rosenfeld, Roni] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Tibshirani, Ryan J.] Carnegie Mellon Univ, Machine Learning Dept, Dept Stat, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University	Jahja, M (corresponding author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.	maria@stat.cmu.edu; dfarrow0@gmail.com; roni@cs.cmu.edu; ryantibs@stat.cmu.edu		Rosenfeld, Ronald/0000-0002-3274-5862	NSF Graduate Research Fellowship [DGE-1745016]; DTRA [HDTRA 1-18-C-0008]	NSF Graduate Research Fellowship(National Science Foundation (NSF)); DTRA(United States Department of DefenseDefense Threat Reduction Agency)	We thank Logan Brooks for several helpful conversations and brainstorming sessions. MJ was supported by NSF Graduate Research Fellowship No. DGE-1745016. RR and RJT were supported by DTRA Contract No. HDTRA 1-18-C-0008.	Anderson Brian D., 1979, OPTIMAL FILTERING; [Anonymous], 2019, STAT LEARNING SPARSI; Breiman L, 1996, MACH LEARN, V24, P49; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; BROWN PJ, 1982, J R STAT SOC B, V44, P287; Brown R.G., 2012, INTRO RANDOM SIGNALS, P324; Evensen Geir, J GEOPHYS RES, V99, P143; Farrow D, 2016, THESIS; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Hastie Trevor, 2009, ELEMENTS STAT CALLER; Houtekamer PL, 1998, MON WEATHER REV, V126, P796, DOI 10.1175/1520-0493(1998)126<0796:DAUAEK>2.0.CO;2; Julier S., 1997, SIGNAL PROCESSING SE; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Overshee P, 1996, SUBSPACE IDENTIFICAT; Smith Gerald L., 1962, NATL AERONAUTICS SPA; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1	18	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904078
C	Jedor, M; Loudec, J; Perchet, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jedor, Matthieu; Loudec, Jonathan; Perchet, Vianney			Categorized Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a new stochastic multi-armed bandit setting where arms are grouped inside "ordered" categories. The motivating example comes from e-commerce, where a customer typically has a greater appetence for items of a specific well-identified but unknown category than any other one. We introduce three concepts of ordering between categories, inspired by stochastic dominance between random variables, which are gradually weaker so that more and more bandit scenarios satisfy at least one of them. We first prove instance-dependent lower bounds on the cumulative regret for each of these models, indicating how the complexity of the bandit problems increases with the generality of the ordering concept considered. We also provide algorithms that fully leverage the structure of the model with their associated theoretical guarantees. Finally, we have conducted an analysis on real data to highlight that those ordered categories actually exist in practice.	[Jedor, Matthieu; Perchet, Vianney] ENS Paris Saclay, CMLA, Cachan, France; [Jedor, Matthieu; Loudec, Jonathan] Cdiscount, Bordeaux, France; [Perchet, Vianney] Criteo AI Lab, Paris, France	UDICE-French Research Universities; Universite Paris Saclay	Jedor, M (corresponding author), ENS Paris Saclay, CMLA, Cachan, France.; Jedor, M (corresponding author), Cdiscount, Bordeaux, France.	jedor@cmla.ens-cachan.fr; jonathan.louedec@cdiscount.com; perchet@cmla.ens-cachan.fr			Investissement d'avenir project, LabEx LMH [ANR-11-LABX-0056-LMH]	Investissement d'avenir project, LabEx LMH(French National Research Agency (ANR))	This work was supported in part by a public grant as part of the Investissement d'avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH, in a joint call with Gaspard Monge Program for optimization, operations research and their interactions with data sciences.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P3; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bawa V.S., 1975, J FINANC ECON, V2, P95, DOI [DOI 10.1016/0304-405X(75)90025-2, 10.1016/0304-405X(75)90025-2]; Bresler G., 2014, ADV NEURAL INFORM PR, P3347; Bubeck S., 2013, P 26 ANN C LEARN THE, P122; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Combes R., 2017, ADV NEURAL INFORM PR, P1761; Coquelin Pierre-Arnaud, 2007, CS0703062 ARXIV; Dani V, 2008, P C LEARN THEOR COLT, P355; David H. A., 2003, ORDER STAT, V3rd; Degenne R., 2016, ICML JMLR WORKSH C P, P1587; Degenne R., 2016, PROC 30 INT C NEURAL, P2972; Deng YJ, 2013, 2013 THIRD INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEM DESIGN AND ENGINEERING APPLICATIONS (ISDEA), P1141, DOI 10.1109/ISDEA.2012.269; Garivier A., 2016, ADV NEURAL INFORM PR, P784; Gentile C, 2014, PR MACH LEARN RES, V32, P757; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; HADAR J, 1969, AM ECON REV, V59, P25; He J, 2010, NINTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, VOLS I-III, P1061; Katariya S., 2018, ARXIV180207176; Katariya S., 2016, ARXIV160803023; Kaufmann Emilie, 2018, ARXIV180600973; Kawale J., 2015, ADV NEURAL INFORM PR, V28, P1297; Kocdik Tomfg, 2014, 28 AAAI C ART INT; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Korda N, 2016, PR MACH LEARN RES, V48; Kwon Joon, 2016, J MACHINE LEARNING R, V17, P8106; Kwon Joon, 2017, 30 ANN C LEARN THEOR, P355; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore T., 2014, ADV NEURAL INFORM PR, V27, P550; Lattimore T, 2017, PR MACH LEARN RES, V54, P728; Li S, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P539, DOI 10.1145/2911451.2911548; Li Shuai, 2016, ARXIV160500596; Maillard OA, 2014, PR MACH LEARN RES, V32; NGUYEN TT, 2014, P 23 ACM INT C C INF, P1959, DOI DOI 10.1145/2661829.2662063; Perrault Pierre, 2019, 22 INT C ART INT STA, P1668; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Valko M, 2014, PR MACH LEARN RES, V32, P46	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906012
C	Jiang, JC; Lu, ZQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jiang, Jiechuan; Lu, Zongqing			Learning Fairness in Multi-Agent Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Fairness is essential for human society, contributing to stability and productivity. Similarly, fairness is also the key for many multi-agent systems. Taking fairness into multi-agent learning could help multi-agent systems become both efficient and stable. However, learning efficiency and fairness simultaneously is a complex, multi-objective, joint-policy optimization. To tackle these difficulties, we propose FEN, a novel hierarchical reinforcement learning model. We first decompose fairness for each agent and proposefair-efficient reward that each agent learns its own policy to optimize. To avoid multi-objective conflict, we design a hierarchy consisting of a controller and several sub-policies, where the controller maximizes the fair-efficient reward by switching among the sub-policies that provides diverse behaviors to interact with the environment. FEN can be trained in a fully decentralized way, making it easy to be deployed in real-world applications. Empirically, we show that FEN easily learns both fairness and efficiency and significantly outperforms baselines in a variety of multi-agent scenarios.	[Jiang, Jiechuan; Lu, Zongqing] Peking Univ, Beijing, Peoples R China	Peking University	Lu, ZQ (corresponding author), Peking Univ, Beijing, Peoples R China.	jiechuan.jiang@pku.edu.cn; zongqing.lu@pku.edu.cn			NSF China [61872009]; Huawei Noah's Ark Lab; Peng Cheng Lab	NSF China(National Natural Science Foundation of China (NSFC)); Huawei Noah's Ark Lab(Huawei Technologies); Peng Cheng Lab	This work was supported in part by NSF China under grant 61872009, Huawei Noah's Ark Lab, and Peng Cheng Lab.	[Anonymous], 1984, TECHNICAL REPORT; [Anonymous], 2019, ARXIV190300714; Bacon Pierre-Luc, 2017, AAAI C ART INT AAAI; Bakker B, 2010, STUD COMPUT INTELL, V281, P475; Beynier Aur~lie, 2018, INT JOINT C AUT AG M; Bol T, 2018, P NATL ACAD SCI USA, V115, P4887, DOI 10.1073/pnas.1719557115; Chen YL, 2013, GAME ECON BEHAV, V77, P284, DOI 10.1016/j.geb.2012.10.009; Cui Jingjing, 2018, ARXIV181010408; Dayan P., 1993, ADV NEURAL INFORM PR; de Jong Steven, 2007, ALAMAS, p104 110; De Witt Christian Schroeder, 2018, ARXIV180311485; Foerster Jakob, 2018, AAAI C ART INT AAAI; Frans Kevin, 2017, P INT C LEARN REPR; Hong Zhang-Wei, 2018, INT C AUT AG MULTIAG; Hughes Edward, 2018, ADV NEURAL INFORM PR; Jain R, 2017, ACM T ARCHIT CODE OP, V14, DOI 10.1145/3132170; Jiang J., 2018, P 32 INT C NEUR INF, VVolume 18, P7265; Jiang J, 2018, ARXIV181009202, V2; Jiang YC, 2012, IEEE T SYST MAN CY A, V42, P1040, DOI 10.1109/TSMCA.2012.2186439; Kash I, 2014, J ARTIF INTELL RES, V51, P579, DOI 10.1613/jair.4405; Lowe Ryan, 2017, ADV NEURAL INFORM PR; Minarolli Dorian, 2013, INT C CLOUD GREEN CO; Nachum O., 2018, ADV NEURAL INFORM PR; Perc M, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2014.0378; Peysakhovich Alexander, 2018, INT C AUT AG MULTIAG; Procaccia Ariel D, 2009, INT JOINT C ART INT; Rabinowitz N. C., 2018, ARXIV180207740; Schulman J., 2017, ABS170706347 CORR; Sukhbaatar Sainbayar, 2016, ADV NEURAL INFORM PR; Sunehag Peter, 2017, ARXIV170605296; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Vezhnevets Alexander Sasha, 2017, INT C MACH LEARN ICM; Wang Jane X, 2019, INT C AUT AG MULTIAG; Xiao L, 2007, J PARALLEL DISTR COM, V67, P33, DOI 10.1016/j.jpdc.2006.08.010; Yang Yaodong, 2018, INT C MACH LEARN ICM; Zeng Yulong, 2019, ARXIV190209089; Zhang Chongjie, 2014, ADV NEURAL INFORM PR	37	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905052
C	Kearns, M; Roth, A; Sharifi-Malvajerdi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kearns, Michael; Roth, Aaron; Sharifi-Malvajerdi, Saeed			Average Individual Fairness: Algorithms, Generalization and Experiments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new family of fairness definitions for classification problems that combine some of the best properties of both statistical and individual notions of fairness. We posit not only a distribution over individuals, but also a distribution over (or collection of) classification tasks. We then ask that standard statistics (such as error or false positive/negative rates) be (approximately) equalized across individuals, where the rate is defined as an expectation over the classification tasks. Because we are no longer averaging over coarse groups (such as race or gender), this is a semantically meaningful individual-level constraint. Given a sample of individuals and problems, we design an oracle-efficient algorithm (i.e. one that is given access to any standard, fairness-free learning heuristic) for the fair empirical risk minimization task. We also show that given sufficiently many samples, the ERM solution generalizes in two directions: both to new individuals, and to new classification tasks, drawn from their corresponding distributions. Finally we implement our algorithm and empirically verify its effectiveness.	[Kearns, Michael; Roth, Aaron; Sharifi-Malvajerdi, Saeed] Univ Penn, Philadelphia, PA 19104 USA	University of Pennsylvania	Kearns, M (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	mkearns@cis.upenn.edu; aaroth@cis.upenn.edu; saeedsh@wharton.upenn.edu						Abdulkadiroglu A, 2005, AM ECON REV, V95, P364, DOI 10.1257/000282805774670167; Agarwal A, 2018, PR MACH LEARN RES, V80; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Gillen S., 2018, ADV NEURAL INFORM PR, P2600; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hebert-Johnson Ursula, 2018, INT C MACH LEARN, P1944; Joseph M, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P158, DOI 10.1145/3278721.3278764; Joseph Matthew, 2016, NIPS, P325; Kearns M, 2018, PR MACH LEARN RES, V80; Kearns M, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P100, DOI 10.1145/3287560.3287592; Kim M., 2018, NEURAL INF PROCESS S, P4842; Kim M. P., 2018, ABS180512317 CORR; Kleinberg J. M., 2017, 8 INN THEOR COMP SCI, V67; Mitchell Shira, 2018, ARXIV181107867; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Yona G., 2018, P MACHINE LEARNING R, V80, P5666	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308028
C	Kirschner, J; Krause, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kirschner, Johannes; Krause, Andreas			Stochastic Bandits with Context Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a stochastic contextual bandit model where at each time step the environment chooses a distribution over a context set and samples the context from this distribution. The learner observes only the context distribution while the exact context realization remains hidden. This allows for a broad range of applications where the context is stochastic or when the learner needs to predict the context. We leverage the UCB algorithm to this setting and show that it achieves an order-optimal high-probability bound on the cumulative regret for linear and kernelized reward functions. Our results strictly generalize previous work in the sense that both our model and the algorithm reduce to the standard setting when the environment chooses only Dirac delta distributions and therefore provides the exact context to the learner. We further analyze a variant where the learner observes the realized context after choosing the action. Finally, we demonstrate the proposed method on synthetic and real-world datasets.	[Kirschner, Johannes; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	ETH Zurich	Kirschner, J (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	jkirschner@inf.ethz.ch; krausea@ethz.ch		Kirschner, Johannes/0000-0002-7228-8280; Krause, Andreas/0000-0001-7260-9673	SNSF [200020 159557]; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [815943]	SNSF(Swiss National Science Foundation (SNSF)); European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	This research was supported by SNSF grant 200020 159557 and has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme grant agreement No 815943.	Abbasi-Yadkori Y., 2012, THESIS U ALBERTA; Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P3; Abeille M, 2017, PR MACH LEARN RES, V54, P176; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bastani H., 2017, ARXIV170409011; Bubeck S., 2012, MACHINE LEARNING, V5; Chowdhury SR, 2017, PR MACH LEARN RES, V70; Dani V., 2008, 21 ANN C LEARN THEOR; Durand A, 2018, J MACH LEARN RES, V19; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; He J, 2010, NINTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, VOLS I-III, P1061; Holzkamper A, 2013, AGR FOREST METEOROL, V168, P149, DOI 10.1016/j.agrformet.2012.09.004; Hsieh P-C., 2019, INT C MACH LEARN, P2800; Hug N., 2017, SURPRISE PYTHON LIB; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Krause A., 2018, P MACHINE LEARNING R, P358; Kveton B., 2019, ARXIV PREPRINT ARXIV; Lamprier S., 2018, J MACHINE LEARNING R, V19, P2060; Langford John, 2007, ADV NEURAL INFORM PR, V20, P817; Li Y., 2019, TIGHT REGRET BOUNDS; Mockus J., 1982, System Modeling and Optimization. Proceedings of the 10th IFIP Conference, P473, DOI 10.1007/BFb0006170; Muandet K., 2012, PROC 25 INT C NEURAL, P10; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Mutny M, 2018, ADV NEUR IN, V31; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; PACHECO J, 2019, ARTIF INTELL, P1172; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szepesvdri C., 2018, BANDIT ALGORITHMS; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Valko M., 2013, ARXIV13096869CSSTAT; Yun S.-Y., 2017, ARXIV170301347	37	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905073
C	Klushyn, A; Chen, N; Kurle, R; Cseke, B; van der Smagt, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Klushyn, Alexej; Chen, Nutan; Kurle, Richard; Cseke, Botond; van der Smagt, Patrick			Learning Hierarchical Priors in VAEs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose to learn a hierarchical prior in the context of variational autoencoders to avoid the over-regularisation resulting from a standard normal prior distribution. To incentivise an informative latent representation of the data, we formulate the learning problem as a constrained optimisation problem by extending the Taming VAEs framework to two-level hierarchical models. We introduce a graph-based interpolation method, which shows that the topology of the learned latent representation corresponds to the topology of the data manifold-and present several examples, where desired properties of latent representation such as smoothness and simple explanatory factors are learned by the prior.	[Klushyn, Alexej; Chen, Nutan; Kurle, Richard; Cseke, Botond; van der Smagt, Patrick] Volkswagen Grp, Machine Learning Res Lab, Wolfsburg, Germany; [Klushyn, Alexej; Kurle, Richard] Tech Univ Munich, Dept Informat, Munich, Germany	Volkswagen; Volkswagen Germany; Technical University of Munich	Klushyn, A (corresponding author), Volkswagen Grp, Machine Learning Res Lab, Wolfsburg, Germany.; Klushyn, A (corresponding author), Tech Univ Munich, Dept Informat, Munich, Germany.	alexej.klushyn@argmax.ai; nutan.chen@argmax.ai; richardk@argmax.ai; botond.cseke@argmax.ai; smagt@argmax.ai						Alemi A., 2018, ICML; Aubry M., 2014, CVPR; Bengio Y., 2007, NEURIPS; Bowman Samuel R, 2016, CONLL; Burda Y., 2016, ICLR; Cayton L., 2005, 12 U CAL SAN DIEG, V12; Chen N., 2015, HUMANOIDS; Chen N., 2019, ICANN; Chen X., 2017, ICLR; CHEN X, 2016, NEURIPS; Cremer Chris, 2017, ARXIV170402916; Dilokthanakul N., 2016, CORR; Goyal P., 2017, ICCV; Higgins I., 2017, ICLR; Kingma D. P., 2014, ICML; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Larochelle H., 2011, AISTATS; LeCun Y., 1998, P IEEE; Molchanov D., 2019, AISTATS; Nalisnick E. T., 2017, ICLR; Neal R. M., 1998, LEARNING GRAPHICAL M; Paysan Pascal, 2009, AVSS; Rezende D. J., 2018, CORR; Rezende D.J., 2014, ICML; REZENDE DJ, 2015, ICML; Rifai S., 2011, NEURIPS; Sonderby Casper Kaae, 2016, NEURIPS; Tomczak J., 2018, AISTATS; Xiao H., 2017, CORR; Yeung S., 2017, CORR; Zhao S., 2017, CORR	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302082
C	Kocaoglu, M; Jaber, A; Shanmugam, K; Bareinboim, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kocaoglu, Murat; Jaber, Amin; Shanmugam, Karthikeyan; Bareinboim, Elias			Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DISCOVERY; SELECTION; DIAGRAMS	The challenge of learning the causal structure underlying a certain phenomenon is undertaken by connecting the set of conditional independences (CIs) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation (Pearl, 1988). In this paper, we investigate the more general setting where multiple observational and experimental distributions are available. We start with the simple observation that the invariances given by CIs/dseparation are just one special type of a broader set of constraints, which follow from the careful comparison of the different distributions available. Remarkably, these new constraints are intrinsically connected with do-calculus (Pearl, 1995) in the context of soft-interventions. We then introduce a novel notion of interventional equivalence class of causal graphs with latent variables based on these invariances, which associates each graphical structure with a set of interventional distributions that respect the do-calculus rules. Given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions, where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules. We introduce a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. We provide a formal graphical characterization of this equivalence. Finally, we extend the FCI algorithm, which was originally designed to operate based on CIs, to combine observational and interventional datasets, including new orientation rules particular to this setting.	[Kocaoglu, Murat] IBM Res MA, MIT, IBM Watson AI Lab, Cambridge, MA 02142 USA; [Jaber, Amin] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA; [Shanmugam, Karthikeyan] IBM Res NY, MIT, IBM Watson AI Lab, New York, NY USA; [Bareinboim, Elias] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Massachusetts Institute of Technology (MIT); Purdue University System; Purdue University; Purdue University West Lafayette Campus; Massachusetts Institute of Technology (MIT); Columbia University	Kocaoglu, M (corresponding author), IBM Res MA, MIT, IBM Watson AI Lab, Cambridge, MA 02142 USA.	murat@ibm.com; jaber0@purdue.edu; karthikeyan.shanmugam2@ibm.com; eb@cs.columbia.edu			NSF [IIS-1704352, IIS-1750807]; IBM Research; MIT-IBM Watson Al Lab; Adobe Research	NSF(National Science Foundation (NSF)); IBM Research(International Business Machines (IBM)); MIT-IBM Watson Al Lab(International Business Machines (IBM)); Adobe Research	Bareinboim and Jaber are supported in parts by grants from NSF IIS-1704352, IIS-1750807 (CAREER), IBM Research, and Adobe Research. Kocaoglu and Shanmugam are supported by the MIT-IBM Watson Al Lab.	Bareinboim Elias, 2012, Graph Structures for Knowledge Representation and Reasoning. Second International Workshop, GKR 2011. Revised Selected Papers, P1, DOI 10.1007/978-3-642-29449-5_1; Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Buhlmann P, 2012, P 6 EUR WORKSH PROB; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Correa Juan, 2019, R51 COL U CAUS ART I; Dawid AP, 2002, INT STAT REV, V70, P161; DAWID AP, 1979, J ROY STAT SOC B MET, V41, P1; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Ghassami A, 2018, PR MACH LEARN RES, V80; Hauser A, 2012, J MACH LEARN RES, V13, P2409; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Kocaoglu M., 2017, ADV NEURAL INFORM PR, V30, P7018; Magliacane S., 2016, ARXIV161110351; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P411; Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.1093/biomet/82.4.669; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Pearl J., 1993, P 49 SESS INT STAT I, P399; Richardson T, 2002, ANN STAT, V30, P962; Shanmugam K., 2015, ADV NEURAL INFORM PR, P3195; Triantafillou S, 2015, J MACH LEARN RES, V16, P2147; Wang Yuhao, 2017, ADV NEURAL INFORM PR, P5822; Yang Karren, 2018, ICML; Zhang J, 2006, THESIS; Zhang J., 2007, P 23 C UNC ART INT U, P450; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001; Zhang JJ, 2008, J MACH LEARN RES, V9, P1437	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906007
C	Koehler, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Koehler, Frederic			Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GRAPHICAL MODELS; CORRECTNESS	Belief propagation is a fundamental message-passing algorithm for probabilistic reasoning and inference in graphical models. While it is known to be exact on trees, in most applications belief propagation is run on graphs with cycles. Understanding the behavior of "loopy" belief propagation has been a major challenge for researchers in machine learning and other fields, and positive convergence results for BP are known under strong assumptions which imply the underlying graphical model exhibits decay of correlations. We show, building on previous work of Dembo and Montanari, that under a natural initialization BP converges quickly to the global optimum of the Bethe free energy for Ising models on arbitrary graphs, as long as the Ising model is ferromagnetic (i.e. neighbors prefer to be aligned). This holds even though such models can exhibit long range correlations and may have multiple suboptimal BP fixed points. We also show an analogous result for iterating the (naive) mean-field equations; perhaps surprisingly, both results are dimension-free in the sense that a constant number of iterations already provides a good estimate to the Bethe/mean-field free energy.	[Koehler, Frederic] MIT, Dept Math, Cambridge, MA 02141 USA	Massachusetts Institute of Technology (MIT)	Koehler, F (corresponding author), MIT, Dept Math, Cambridge, MA 02141 USA.	fkoehler@mit.edu						Augeri Fanny, 2019, ARXIV190308021; Basak A, 2017, PROBAB THEORY REL, V168, P557, DOI 10.1007/s00440-016-0718-0; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Bubeck S., 2015, FDN TRENDS MACHINE L; Dembo A, 2013, ANN PROBAB, V41, P4162, DOI 10.1214/12-AOP828; Dembo A, 2010, ANN APPL PROBAB, V20, P565, DOI 10.1214/09-AAP627; DOBRUSCH.PL, 1968, THEOR PROBAB APPL+, V13, P197, DOI 10.1137/1113026; Eldan Ronen, 2018, ARXIV181111530; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Jain Vishesh, 2018, C LEARN THEOR COLT; Jain Vishesh, 2019, P S THEOR COMP STOC; JERRUM M, 1990, LECT NOTES COMPUT SC, V443, P462; Korc F., 2012, ICML WORKSH INF INT; LEE TD, 1952, PHYS REV, V87, P410, DOI 10.1103/PhysRev.87.410; Li S, 2009, MARKOV RANDOM FIELD; Malioutov DM, 2006, J MACH LEARN RES, V7, P2031; Martinelli F., 1997, LECT NOTES MATH, V1717, P93; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Mooij JM, 2007, IEEE T INFORM THEORY, V53, P4422, DOI 10.1109/TIT.2007.909166; Mossel E., 2004, DIMACS Workshop: Graphs, Morphisms and Statistical Physics (DIMACS: Series in Discrete Mathematics and Theoretical Computer Science Vol.63), P155; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Ruozzi Nicholas, 2012, P 25 INT C NEUR INF, V1, P117; Schlesinger D., 2006, TRANSFORMING ARBITRA; Tarski A., 1955, PAC J MATH, V5, P285, DOI DOI 10.2140/PJM.1955.5.285; TATIKONDA S, 2002, UNC ART INT UAI P 18; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; Weiss Y, 2000, ADV NEUR IN, V12, P673; Weller Adrian, 2013, ARTIF INTELL, P618; Welling M., 2001, P 17 C UNC ART INT, P554; Werner Tomas, 2010, UAI 2010, P651; Yedidia J. S., 2003, ANN MATH STUD, P239	36	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308036
C	Kondo, R; Kawano, K; Koide, S; Kutsuna, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kondo, Ruho; Kawano, Keisuke; Koide, Satoshi; Kutsuna, Takuro			Flow-based Image-to-Image Translation with Feature Disentanglement	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DESIGN	Learning non-deterministic dynamics and intrinsic factors from images obtained through physical experiments is at the intersection of machine learning and material science. Disentangling the origins of uncertainties involved in microstructure growth, for example, is of great interest because future states vary due to thermal fluctuation and other environmental factors. To this end we propose a flow-based image-to-image model, called Flow U-Net with Squeeze modules (FUNS), that allows us to disentangle the features while retaining the ability to generate high-quality diverse images from condition images. Our model successfully captures probabilistic phenomena by incorporating a U-Net-like architecture into the flow-based model. In addition, our model automatically separates the diversity of target images into condition-dependent/independent parts. We demonstrate that the quality and diversity of the images generated for microstructure growth and CelebA datasets outperform existing variational generative models.	[Kondo, Ruho; Kawano, Keisuke; Koide, Satoshi; Kutsuna, Takuro] Toyota Cent R&D Labs, Nagakute, Aichi, Japan	Toyota Central R&D Labs Inc	Kondo, R (corresponding author), Toyota Cent R&D Labs, Nagakute, Aichi, Japan.	r-kondo@mosk.tytlabs.co.jp; kawano@mosk.tytlabs.co.jp; koide@mosk.tytlabs.co.jp; kutsuna@mosk.tytlabs.co.jp		Kutsuna, Takuro/0000-0001-6965-1512				Abadi M, 2015, P 12 USENIX S OPERAT; Ardizzone L., 2019, INT C LEARN REPR; COOK HE, 1970, ACTA METALL MATER, V18, P297, DOI 10.1016/0001-6160(70)90144-6; Dinh L, 2017, 5 INT C LEARN REPR I; Dinh Laurent, 2015, ICLR WORKSH; ELDER KR, 1994, PHYS REV LETT, V72, P677, DOI 10.1103/PhysRevLett.72.677; Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923; Glorot X., 2011, P 14 INT C ART INT S, P315; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Gonzalez-Garcia A, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hensel M, 2017, ADV NEUR IN, V30; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Karma A, 1999, PHYS REV E, V60, P3614, DOI 10.1103/PhysRevE.60.3614; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kohl SAA, 2018, ADV NEUR IN, V31; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Lin JX, 2018, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2018.00579; Liu Runtao, 2019, P IEEE C COMP VIS PA; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; McDowell DL, 2008, SCI MODEL SIMUL, V15, P207, DOI 10.1007/s10820-008-9100-6; Parmar Niki, 2018, P INT C LEARN REPR I; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Uria Benigno, 2013, P 26 INT C NEURAL IN; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068	33	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304020
C	Kuck, J; Dao, T; Rezatofighi, H; Sabharwal, A; Ermon, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kuck, Jonathan; Dao, Tri; Rezatofighi, Hamid; Sabharwal, Ashish; Ermon, Stefano			Approximating the Permanent by Sampling from Adaptive Partitions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM; TRACKING; MATRIX; BOUNDS	Computing the permanent of a non-negative matrix is a core problem with practical applications ranging from target tracking to statistical thermodynamics. However, this problem is also #P-complete, which leaves little hope for finding an exact solution that can be computed efficiently. While the problem admits a fully polynomial randomized approximation scheme, this method has seen little use because it is both inefficient in practice and difficult to implement. We present ADAPART, a simple and efficient method for drawing exact samples from an unnormalized distribution. Using ADAPART, we show how to construct tight bounds on the permanent which hold with high probability, with guaranteed polynomial runtime for dense matrices. We find that ADAPART can provide empirical speedups exceeding 30x over prior sampling methods on matrices that are challenging for variational based approaches. Finally, in the context of multi-target tracking, exact sampling from the distribution defined by the matrix permanent allows us to use the optimal proposal distribution during particle filtering. Using ADAPART, we show that this leads to improved tracking performance using an order of magnitude fewer samples.	[Kuck, Jonathan; Dao, Tri; Rezatofighi, Hamid; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA; [Sabharwal, Ashish] Allen Inst Artificial Intelligence, Seattle, WA USA	Stanford University	Kuck, J (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	kuck@stanford.edu; trid@stanford.edu; hamidrt@stanford.edu; ashishs@allenai.org; ermon@stanford.edu			NSF [1651565, 1522054, 1733686]; ONR [N00014-19-1-2145]; AFOSR [FA9550- 19-1-0024]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	Research supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550- 19-1-0024), and FLI.	Aaronzon S, 2014, QUANTUM INF COMPUT, V14, P541; Anari Nima, 2018, ARXIV181102933; Atanasov N., 2014, ROBOTICS SCI SYSTEMS; Balasubramanian K., 1980, THESIS; Barvinok A, 2016, FOUND COMPUT MATH, V16, P329, DOI 10.1007/s10208-014-9243-7; Bax E, 1996, CALTECHCSTR9604; Beichl I, 1999, J COMPUT PHYS, V149, P128, DOI 10.1006/jcph.1998.6149; Bezakova I, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P900, DOI 10.1145/1109557.1109656; Blom HAP, 2000, IEEE T AUTOMAT CONTR, V45, P247, DOI 10.1109/9.839947; Bregman LM, 1973, SOV MATH DOKL, V14, P945; Broder A, 1986, P 18 ANN ACM S THEOR, P50, DOI DOI 10.1145/12130.12136; Chernick Michael R, 2008, GUIDE PRACTITIONERS; Chertkov M, 2010, P NATL ACAD SCI USA, V107, P7663, DOI 10.1073/pnas.0910994107; Chertkov Michael, 2008, ARXIV08061199; Chong CY, 2018, 2018 21ST INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P452; Clopper CJ, 1934, BIOMETRIKA, V26, P404, DOI 10.2307/2331986; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; FORTMANN TE, 1983, IEEE J OCEANIC ENG, V8, P173, DOI 10.1109/JOE.1983.1145560; GILKS WR, 1992, J R STAT SOC C-APPL, V41, P337; Glynn DG, 2013, DESIGN CODE CRYPTOGR, V68, P39, DOI 10.1007/s10623-012-9618-1; Gurvits L., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P417, DOI 10.1145/1132516.1132578; Gurvits L, 2002, DISCRETE COMPUT GEOM, V27, P531, DOI 10.1007/s00454-001-0083-2; Gurvits L, 2014, ANN IEEE SYMP FOUND, P90, DOI 10.1109/FOCS.2014.18; Huang Bert, 2009, ARXIV09081769; Huber M, 2006, ALGORITHMICA, V44, P183, DOI 10.1007/s00453-005-1175-9; Hwang SG, 1998, LINEAR ALGEBRA APPL, V281, P259, DOI 10.1016/S0024-3795(98)10040-X; Jerrum M, 2004, J ACM, V51, P671, DOI 10.1145/1008731.1008738; JERRUM M, 1989, SIAM J COMPUT, V18, P1149, DOI 10.1137/0218077; Jerrum M., 1996, APPROXIMATION ALGORI, P482; Law Wai Jing, 2009, THESIS; Linial N, 2000, COMBINATORICA, V20, P545, DOI 10.1007/s004930070007; Liu Q., 2011, P 28 INT C MACH LEAR, P849; Lou Q, 2017, AAAI CONF ARTIF INTE, P860; Minc H., 1963, B AM MATH SOC, V69, P789, DOI 155843; Morelande MR, 2009, FUSION: 2009 12TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, VOLS 1-4, P292; REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177; Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349; Ryser H.J., 1963, COMBINATORIAL MATH; Samorodnitsky A, 2008, J COMB THEORY A, V115, P279, DOI 10.1016/j.jcta.2007.05.010; Sarkka S., 2004, Seventh International Conference on Information Fusion, P583; Soules GW, 2000, LINEAR MULTILINEAR A, V47, P77; Soules GW, 2005, LINEAR ALGEBRA APPL, V394, P73, DOI 10.1016/j.laa.2004.06.022; Soules GW, 2003, LINEAR MULTILINEAR A, V51, P319, DOI 10.1080/0308108031000098450; Uhlmann JK, 2004, J FRANKLIN I, V341, P569, DOI 10.1016/j.jfranklin.2004.07.003; Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6; Vontobel PO, 2014, 2014 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA), P489; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright Martin J, 2003, AISTATS; Zhang FZ, 2016, SPEC MATRICES, V4, P305, DOI 10.1515/spma-2016-0030	52	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900045
C	Kumar, KSS; Bach, F; Pock, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumar, K. S. Sesh; Bach, Francis; Pock, Thomas			Fast Decomposable Submodular Function Minimization using Constrained Total Variation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of minimizing the sum of submodular set functions assuming minimization oracles of each summand function. Most existing approaches reformulate the problem as the convex minimization of the sum of the corresponding Lovasz extensions and the squared Euclidean norm, leading to algorithms requiring total variation oracles of the summand functions; without further assumptions, these more complex oracles require many calls to the simpler minimization oracles often available in practice. In this paper, we consider a modified convex problem requiring a constrained version of the total variation oracles that can be solved with significantly fewer calls to the simple minimization oracles. We support our claims by showing results on graph cuts for 2D and 3D graphs.	[Kumar, K. S. Sesh] Imperial Coll London, Data Sci Inst, London, England; [Bach, Francis] PSL Res Univ, INRIA, Paris, France; [Bach, Francis] PSL Res Univ, Ecole Normale Super, Paris, France; [Pock, Thomas] Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria	Imperial College London; Inria; UDICE-French Research Universities; PSL Research University Paris; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Graz University of Technology	Kumar, KSS (corresponding author), Imperial Coll London, Data Sci Inst, London, England.	s.karri@imperial.ac.uk; francis.bach@inria.fr; pock@icg.tugraz.at			Leverhulme Centre for the Future of Intelligence, Cambridge; European Research Council [724063, 640156]; Data Science Institute, Imperial College London	Leverhulme Centre for the Future of Intelligence, Cambridge; European Research Council(European Research Council (ERC)European Commission); Data Science Institute, Imperial College London	This research was funded by the Leverhulme Centre for the Future of Intelligence, Cambridge and the Data Science Institute, Imperial College London. We acknowledge support from the European Research Council (SEQUOIA project 724063) and (HOMOVIS project 640156).	Arora Chetan, 2012, ECCV; Bach F., 2011, NEURAL INFORM PROCES, P10; Bach F., 2013, FDN TRENDS MACHINE L, V6; Bach Francis, 2016, MATH PROGRAMMING, V175, P1; Barbero Alvaro, 2014, 14110589 ARXIV; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Bubeck S., 2015, FDN TRENDS MACHINE L; Chakrabarty D., 2014, ADV NEURAL INFORM PR; CHAMBOLLE A, 2015, SIAM J COMPUT MATH, V1, P29, DOI DOI 10.5802/smai-jcm.3; Chambolle A., 2005, ENERGY MINIMIZATION; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10; Condat Laurent, 2012, TECHNICAL REPORT; Dadush Daniel, 2018, P S DISCR ALG SODA; Edmonds J., 1970, COMBINATORIAL STRUCT, P11; Ene A, 2015, PR MACH LEARN RES, V37, P787; Ene Alina, 2017, ADV NEURAL INFORM PR; GROENEVELT H, 1991, EUR J OPER RES, V54, P227, DOI 10.1016/0377-2217(91)90300-K; Jegelka S., 2013, ADV NEURAL INFO PROC, P1313; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V., 2012, DISCRETE APPL MATH; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Krause A., 2011, ACM T INTELLIGENT SY; KUMAR KS, 2017, J MACH LEARN RES, V18, P4809; Lee Y. T., 2015, ANN S FDN COMP SCI; Li P., 2018, ADV NEURAL INFORM PR; Lin H., 2011, P NAACL HLT; Lovasz L., 1983, MATH PROGRAMMING STA, P235; Nishihara R., 2014, ADV NEURAL INFO PROC, P640; Paul Tseng, 2008, SIAM J OPTIMIZ UNPUB; Rockafellar R. T., 1997, CONVEX ANAL; Sesh Kumar K. S., 2015, 01123492 HAL; Shekhovtsov A., 2011, ENERGY MINIMIZATION; Stobbe P, 2013, THESIS; Stobbe P., 2010, ADV NEURAL INFORM PR; Zhang WZ, 2018, PR MACH LEARN RES, V80	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308023
C	Lahn, N; Mulchandani, D; Raghvendra, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lahn, Nathaniel; Mulchandani, Deepika; Raghvendra, Sharath			A Graph Theoretic Additive Approximation of Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	Transportation cost is an attractive similarity measure between probability distributions due to its many useful theoretical properties. However, solving optimal transport exactly can be prohibitively expensive. Therefore, there has been significant effort towards the design of scalable approximation algorithms. Previous combinatorial results [Sharathkumar, Agarwal STOC '12, Agarwal, Sharathkumar STOC '14] have focused primarily on the design of near-linear time multiplicative approximation algorithms. There has also been an effort to design approximate solutions with additive errors [Cuturi NIPS '13, Altschuler et al. NIPS '17, Dvurechensky et al. ICML '18, Quanrud, SOSA '19] within a time bound that is linear in the size of the cost matrix and polynomial in C/delta; here C is the largest value in the cost matrix and 6 is the additive error. We present an adaptation of the classical graph algorithm of Gabow and Tarjan and provide a novel analysis of this algorithm that bounds its execution time by O(n2C/delta + NC2/delta(2)). Our algorithm is extremely simple and executes, for an arbitrarily small constant E epsilon, only [2C/(1-epsilon)delta] + 1 iterations, where each iteration consists only of a Dijkstra-type search followed by a depth-first search. We also provide empirical results that suggest our algorithm is competitive with respect to a sequential implementation of the Sinkhorn algorithm in execution time. Moreover, our algorithm quickly computes a solution for very small values of delta whereas Sinkhorn algorithm slows down due to numerical instability.	[Lahn, Nathaniel] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA; [Mulchandani, Deepika; Raghvendra, Sharath] Virginia Tech, Blacksburg, VA 24061 USA	Virginia Polytechnic Institute & State University; Virginia Polytechnic Institute & State University	Lahn, N (corresponding author), Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.	lahnn@vt.edu; deepikak@vt.edu; sharathr@vt.edu			NSF [CCF-1909171]	NSF(National Science Foundation (NSF))	Research presented in this paper was funded by NSF CCF-1909171. We would like to thank the anonymous reviewers for their useful feedback. All authors contributed equally to this research.	Agarwal PK, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P555, DOI 10.1145/2591796.2591844; Agarwal PK, 2000, SIAM J COMPUT, V29, P912, DOI 10.1137/S0097539795295936; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Altschuler J., 2018, ARXIV181010046CSDS; Arjovsky M., 2017, ARXIV170107875; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Bigot J, 2017, ANN I H POINCARE-PR, V53, P1, DOI 10.1214/15-AIHP706; Blanchet J., 2018, ARXIV181007717CSDS; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699; Flamary R, 2018, MACH LEARN, V107, P1923, DOI 10.1007/s10994-018-5717-1; GABOW HN, 1989, SIAM J COMPUT, V18, P1013, DOI 10.1137/0218069; Jambulapati A., 2019, 190600618CSDS ARXIV; Kaplan H, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2495; Khesin A. B., 2019, S COMP GEOM; Kuhn HW, 1956, NAV RES LOGISTICS Q, V3, P253, DOI [DOI 10.1002/NAV.3800030404, 10.1002/nav.3800030404]; Lahn N., 2019, S COMP GEOM; Lahn Nathaniel, 2019, ACM SIAM S DISCR ALG, P569, DOI 10.1137/1.9781611975482.36; Lee YT, 2014, ANN IEEE SYMP FOUND, P424, DOI 10.1109/FOCS.2014.52; Lin T., 2019, ARXIV190106482CSDS; Mucha M, 2004, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2004.40; Pavel Dvurechensky, 2018, P 35 INT C MACH LEAR, P1366; Phillips J. M., 2006, CAN C COMP GEOM; Quanmud K., 2019, S SIMPL ALG, V69; Ramshaw L, 2012, ANN IEEE SYMP FOUND, P581, DOI 10.1109/FOCS.2012.9; Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18; Sharathkumar R, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P385; Sharathkumar R., 2012, PROC 23 ANN ACM SIAM, P306; Sherman J, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P772	30	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905048
C	Lamy, A; Zhong, ZY; Menon, AK; Verma, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lamy, Alexandre; Zhong, Ziyuan; Menon, Aditya Krishna; Verma, Nakul			Noise-tolerant fair classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Fairness-aware learning involves designing algorithms that do not discriminate with respect to some sensitive feature (e.g., race or gender). Existing work on the problem operates under the assumption that the sensitive feature available in one's training sample is perfectly reliable. This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features. In this paper, we answer the question in the affirmative: we show that if one measures fairness using the mean-difference score, and sensitive features are subject to noise from the mutually contaminated learning model, then owing to a simple identity we only need to change the desired fairness-tolerance. The requisite tolerance can be estimated by leveraging existing noise-rate estimators from the label noise literature. We finally show that our procedure is empirically effective on two case-studies involving sensitive feature censoring.	[Lamy, Alexandre; Zhong, Ziyuan; Verma, Nakul] Columbia Univ, New York, NY 10027 USA; [Menon, Aditya Krishna] Google, Mountain View, CA 94043 USA	Columbia University; Google Incorporated	Lamy, A (corresponding author), Columbia Univ, New York, NY 10027 USA.	a.lamy@columbia.edu; ziyuan.zhong@columbia.edu; adityakmenon@google.com; verma@cs.columbia.edu						Adler P, 2018, KNOWL INF SYST, V54, P95, DOI 10.1007/s10115-017-1116-3; Agarwal A, 2018, PR MACH LEARN RES, V80; Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Angwin J., 2016, MACHINE BIAS; [Anonymous], 2018, P C FAIRNESS ACCOUNT, DOI DOI 10.1145/3357384.3357857; Awasthi P., 2015, PROC 28 ANN C LEARN, P167; Awasthi P., 2019, EQUALIZED ODDS POSTP; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Brodersen Kay H., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3121, DOI 10.1109/ICPR.2010.764; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83; Calmon Flavio, 2017, NEURAL INFORM PROCES, P1, DOI DOI 10.5555/3294996.3295155; Chan P. K., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P164; Charoenphakdee N, 2019, PR MACH LEARN RES, V97; Cotter Andrew, 2018, ARXIV180904198; del Barrio E., 2018, ARXIV180603195; Denis F., 1998, PAC LEARNING POSITIV; Dheeru D., 2019, UCI MACHINE LEARNING; Donini M, 2018, ADV NEUR IN, V31; Dwork C, 2011, ARXIV11043913 CORR; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; Elkan Charles, 2008, P 14 ACM SIGKDD INT, P213, DOI DOI 10.1145/1401890.1401920; Feldman M., 2015, COMPUTATIONAL FAIRNE; Gupta M. R., 2018, CORR; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hashimoto Tatsunori, 2018, P 35 INT C MACH LEAR, P2; Heidari H., 2019, ACM C FAIRN ACC TRAN; Jagielski M., 2018, DIFFERENTIALLY PRIVA; Jain S., 2017, AAAI; Katz-Samuels J, 2019, J MACH LEARN RES, V20; Kilbertus Niki, 2018, ICML, P2630; Kim M. P., 2018, CORR; Kiryo R, 2017, ADV NEUR IN, V30; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Lahoti P., 2018, IFAIR LEARNING INDIV; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Louizos C., 2015, VARIATIONAL FAIR AUT, P11; Lum K, 2017, ALGORITHM REMOVING S; Lum K., 2016, CORR; Menon A., 2015, P 32 INT C MACH LEAR; Menon A. K., 2013, P 30 INT C MACH LEAR; Mohri M, 2019, PR MACH LEARN RES, V97; Mozannar H., 2019, NEURIPS 2019 WORKSH; Natarajan N., 2013, NEURAL INFORM PROCES; Northcutt CG, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; Ramaswamy HG, 2016, PR MACH LEARN RES, V48; Scott C., 2013, P 26 ANN C LEARN THE, P489; Speicher T, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2239, DOI 10.1145/3219819.3220046; van Rooyen B, 2015, THESIS; van Rooyen B, 2018, J MACH LEARN RES, V18; Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x; Wightman L., 1998, NATL LONGITUDINAL BA; Williamson R.C., 2019, ARXIV190108665; Woodworth B., 2017, ARXIV PREPRINT ARXIV; Zafar MB, 2017, ADV NEURAL INFORM PR, V30, P229; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang D, 2008, 2008 THIRD INTERNATIONAL CONFERENCE ON DIGITAL INFORMATION MANAGEMENT, VOLS 1 AND 2, P651	61	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300027
C	Lei, Y; Yang, P; Tang, K; Zhou, DX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lei, Yunwen; Yang, Peng; Tang, Ke; Zhou, Ding-Xuan			Optimal Stochastic and Online Learning with Individual Iterates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUBGRADIENT METHODS; ALGORITHMS	Stochastic composite mirror descent (SCMD) is a simple and efficient method able to capture both geometric and composite structures of optimization problems in machine learning. Existing strategies require to take either an average or a random selection of iterates to achieve optimal convergence rates, which, however, can either destroy the sparsity of solutions or slow down the practical training speed. In this paper, we propose a theoretically sound strategy to select an individual iterate of the vanilla SCMD, which is able to achieve optimal rates for both convex and strongly convex problems in a non-smooth learning setting. This strategy of outputting an individual iterate can preserve the sparsity of solutions which is crucial for a proper interpretation in sparse learning problems. We report experimental comparisons with several baseline methods to show the effectiveness of our method in achieving a fast training speed as well as in outputting sparse solutions.	[Lei, Yunwen; Yang, Peng; Tang, Ke] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Univ Key Lab Evolving Intelligent Syst Guangdong, Shenzhen 518055, Peoples R China; [Lei, Yunwen] Tech Univ Kaiserslautern, Dept Comp Sci, D-67653 Kaiserslautern, Germany; [Zhou, Ding-Xuan] City Univ Hong Kong, Sch Data Sci, Kowloon, Hong Kong, Peoples R China; [Zhou, Ding-Xuan] City Univ Hong Kong, Dept Math, Kowloon, Hong Kong, Peoples R China	Southern University of Science & Technology; University of Kaiserslautern; City University of Hong Kong; City University of Hong Kong	Tang, K (corresponding author), Southern Univ Sci & Technol, Dept Comp Sci & Engn, Univ Key Lab Evolving Intelligent Syst Guangdong, Shenzhen 518055, Peoples R China.	leiyw@sustech.edu.cn; yangp@sustech.edu.cn; tangk3@sustech.edu.cn; mazhou@cityu.edu.hk	Zhou, Ding-Xuan/B-3160-2013; Lei, Yunwen/V-2782-2018	Zhou, Ding-Xuan/0000-0003-0224-9216; Lei, Yunwen/0000-0002-5383-467X	National Key Research and Development Program of China [2017YFB1003102]; National Natural Science Foundation of China [61806091, 61806090, 61672478]; Program for University Key Laboratory of Guangdong Province [2017KSYS008]; Shenzhen Peacock Plan [KQTD2016112514355531]; Research Grants Council of Hong Kong [CityU 11338616]; National Nature Science Foundation of China [11671307]; Alexander von Humboldt Foundation	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for University Key Laboratory of Guangdong Province; Shenzhen Peacock Plan; Research Grants Council of Hong Kong(Hong Kong Research Grants Council); National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); Alexander von Humboldt Foundation(Alexander von Humboldt Foundation)	The work of Y. Lei, P. Yang and K. Tang is supported partially by the National Key Research and Development Program of China (Grant No. 2017YFB1003102), the National Natural Science Foundation of China (Grant Nos. 61806091, 61806090 and 61672478), the Program for University Key Laboratory of Guangdong Province (Grant No. 2017KSYS008) and Shenzhen Peacock Plan (Grant No. KQTD2016112514355531). The work of D.-X. Zhou is supported partially by the Research Grants Council of Hong Kong [Project No. CityU 11338616] and by National Nature Science Foundation of China [Grant No. 11671307]. Y. Lei also acknowledges a Humboldt Research Fellowship from the Alexander von Humboldt Foundation.	[Anonymous], 1964, COMP MATH MATH PHYS+; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Cai JF, 2009, MATH COMPUT, V78, P1515, DOI 10.1090/S0025-5718-08-02189-3; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Duchi J. C., 2010, COLT, P14; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Duchi Y. J., 2009, ADV NEURAL INFORM PR, P495; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Guo ZC, 2017, ANAL APPL, V15, P433, DOI 10.1142/S0219530517500026; Hansen PC, 2012, J COMPUT APPL MATH, V236, P2167, DOI 10.1016/j.cam.2011.09.039; HARVEY N. J., 2019, PROC MACH LEARN RES, V99, P1579; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Jain P., 2019, C LEARN THEOR, P1752; Kakade Sham M, 2009, ADV NEURAL INFORM PR, P801; Kingma D.P., 2015, INT C LEARN REPR ICL; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lacoste-Julien Simon, 2012, ARXIV12122002; Lei YW, 2020, APPL COMPUT HARMON A, V48, P343, DOI 10.1016/j.acha.2018.05.005; Lei YW, 2018, SIAM J IMAGING SCI, V11, P547, DOI 10.1137/17M1136225; LIN JH, 2016, J MACH LEARN RES, V17; LIONS PL, 1979, SIAM J NUMER ANAL, V16, P964, DOI 10.1137/0716071; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nguyen LM, 2018, PR MACH LEARN RES, V80; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Rakhlin A., 2012, P 29 INT C MACH LEAR, P449; Reddi SJ, 2016, PR MACH LEARN RES, V48; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Schopfer F, 2019, MATH PROGRAM, V173, P509, DOI 10.1007/s10107-017-1229-1; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shamir O, 2012, OPEN PROBLEM IS AVER; Shamir O., 2013, P INT C MACH LEARN A, P71; Steinhardt J., 2014, ARXIV14124182; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Xu Y, 2017, BIOMED RES INT, V2017, DOI 10.1155/2017/4316821; Ying YM, 2017, APPL COMPUT HARMON A, V42, P224, DOI 10.1016/j.acha.2015.08.007; Zhang CH, 2004, I C CONT AUTOMAT ROB, P919; Zhang L., 2013, P 26 INT C NEUR INF, P980; Zhou Zhengyuan, 2017, ADV NEURAL INFORM PR, P7043; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	46	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305041
C	Leibfried, F; Pascual-Diaz, S; Grau-Moya, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Leibfried, Felix; Pascual-Diaz, Sergio; Grau-Moya, Jordi			A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CAPACITY	Empowerment is an information-theoretic method that can be used to intrinsically motivate learning agents. It attempts to maximize an agent's control over the environment by encouraging visiting states with a large number of reachable next states. Empowered learning has been shown to lead to complex behaviors, without requiring an explicit reward signal. In this paper, we investigate the use of empowerment in the presence of an extrinsic reward signal. We hypothesize that empowerment can guide reinforcement learning (RL) agents to find good early behavioral solutions by encouraging highly empowered states. We propose a unified Bellman optimality principle for empowered reward maximization. Our empowered reward maximization approach generalizes both Bellman's optimality principle as well as recent information-theoretical extensions to it. We prove uniqueness of the empowered values and show convergence to the optimal solution. We then apply this idea to develop off-policy actor-critic RL algorithms which we validate in high-dimensional continuous robotics domains (MuJoCo). Our methods demonstrate improved initial and competitive final performance compared to model-free state-of-the-art techniques.	[Leibfried, Felix; Pascual-Diaz, Sergio; Grau-Moya, Jordi] PROWLER Io, Cambridge, England		Leibfried, F (corresponding author), PROWLER Io, Cambridge, England.	felix@prowler.io; sergio.diaz@prowler.io; jordi@prowler.io						Abdolmaleki A., 2018, P INT C LEARN REPR; [Anonymous], 2016, P INT C MACH LEARN; [Anonymous], ADV NEURAL INFORM PR; Azar M. G., 2011, P INT C ART INT STAT; Bellman RE, 1957, DYNAMIC PROGRAMMING; Bialek W, 2001, NEURAL COMPUT, V13, P2409, DOI 10.1162/089976601753195969; BLAHUT RE, 1972, IEEE T INFORM THEORY, V18, P460, DOI 10.1109/TIT.1972.1054855; Brockman G., 2016, OPENAI GYM; Chua K., 2018, ADV NEURAL INFORM PR; Cover TM, 2006, ELEMENTS INFORM THEO; Csiszar I., 1984, STAT DECISIONS, V1, P205; de Abril I. M., 2018, UNIFIED STRATEGY IMP; Degris T., 2012, P INT C MACH LEARN; Fox R., 2016, P C UNC ART INT; Fujimoto S., 2018, P INT C MACH LEARN; Gallager R. G., 1994, TECHNICAL REPORT; Genewein T, 2015, FRONT ROBOT AI, DOI 10.3389/frobt.2015.00027; Grau-Moya J, 2019, P INT C LEARN REPR; Haarnoja T., 2018, P INT C MACH LEARN; Haarnoja T., 2019, SOFT ACTOR CRITIC AL; Haarnoja T., 2017, P INT C MACH LEARN; J. Grau-Moya, 2016, P EUR C MACH LEARN P; Kingma D.P., 2015, ICLR, P1; Klyubin AS, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0004018; Klyubin AS, 2005, IEEE C EVOL COMPUTAT, P128; Kramer G., 1998, THESIS SWISS FEDERAL; Kumar N. M., 2018, NIPS WORKSH; Leibfried F., 2019, P C ROB LEARN; Leibfried F., 2018, NIPS WORKSH; Leibfried F., 2016, P C UNC ART INT; Leibfried F, 2015, NEURAL COMPUT, V27, P1686, DOI 10.1162/NECO_a_00758; Levine S, 2018, REINFORCEMENT LEARNI; Liang E., 2018, P INT C MACH LEARN; Lillicrap T. P., 2016, P 33 INT C MACH LEAR; Long-Ji Lin, 1993, THESIS; MARKO H, 1973, IEEE T COMMUN, VCO21, P1345, DOI 10.1109/TCOM.1973.1091610; Massey J. L., 2005, P INT S INF THEOR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mohamed Shakir, 2015, ADV NEURAL INFORM PR; Montufar G., 2016, INFORM THEORETICALLY; Neu G., 2017, UNIFIED VIEW ENTROPY; Ortega P. A., 2013, P ROYAL SOC A, V469, P2153, DOI 1098/rspa.2012.0683; Pineau J., 2018, NIPS INVITED TALK; Prokopenko M., 2006, P INT C SIM AD BEH; Rubin J., 2012, DECISION MAKING IMPE; Russell S., 2002, ARTIF INTELL; Salge C., 2014, GUIDED SELF ORG INCE; Schossau J, 2016, ENTROPY-SWITZ, V18, DOI 10.3390/e18010006; Schulman J., 2015, P INT C MACH LEARN; Schulman J., 2017, EQUIVALENCE POLICY G; Schulman J., 2017, PROXIMAL POLICY OPTI; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Shannon Claude E, 1959, IRE NAT CONV REC, V4, P1, DOI DOI 10.1234/12345678; Sims CA, 2003, J MONETARY ECON, V50, P665, DOI 10.1016/S0304-3932(03)00029-1; Still S, 2012, THEOR BIOSCI, V131, P139, DOI 10.1007/s12064-011-0142-z; Sutton R. S., 2000, ADV NEURAL INFORM PR; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tiomkin S., 2018, UNIFIED BELLMAN EQUA; Todorov E., 2012, P IEEE RSJ INT C INT; van Hasselt H., 2010, ADV NEURAL INFORM PR; vanHasselt H., 2016, P AAAI C ART INT; Welling M, 2014, AUTOENCODING VARIATI; Yip M. C, 2019, INT C LEARN REPR ICL; Zahedi K, 2010, ADAPT BEHAV, V18, P338, DOI 10.1177/1059712310375314; Ziebart B. D., 2010, THESIS	70	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307084
C	Levy, D; Duchi, JC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Levy, Daniel; Duchi, John C.			Necessary and Sufficient Geometries for Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUBGRADIENT METHODS	We study the impact of the constraint set and gradient geometry on the convergence of online and stochastic methods for convex optimization, providing a characterization of the geometries for which stochastic gradient and adaptive gradient methods are (minimax) optimal. In particular, we show that when the constraint set is quadratically convex, diagonally pre-conditioned stochastic gradient methods are minimax optimal. We further provide a converse that shows that when the constraints are not quadratically convex-for example, any l(p)-ball for p < 2-the methods are far from optimal. Based on this, we can provide concrete recommendations for when one should use adaptive, mirror or stochastic gradient methods.	[Levy, Daniel; Duchi, John C.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Levy, D (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	danilevy@stanford.edu; jduchi@stanford.edu	Levy, Daniel/ABB-2752-2021		NSF-CAREER Award [1553086]; ONR-YIP [N00014-19-1-2288]; Stanford DAWN Project	NSF-CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR-YIP(Office of Naval Research); Stanford DAWN Project	This work was supported by NSF-CAREER Award 1553086, ONR-YIP N00014-19-1-2288, and the Stanford DAWN Project. We thank Aditya Grover, Annie Marsden and Hongseok Namkoong for valuable comments on the draft as well as Quentin Guignard for pointing us to the Banach-Mazur distance for Theorem 4.	[Anonymous], THESIS; ASSOUAD P, 1983, CR ACAD SCI I-MATH, V296, P1021; Bartlett PL, 2007, P 21 ANN C ADV NEUR, P65; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Cutcosky A., 2019, P 36 INT C MACH LEAR; DONOHO DL, 1990, ANN STAT, V18, P1416, DOI 10.1214/aos/1176347758; Duchi J., 2013, ADV NEURAL INFORM PR, P2832; Duchi J. C., 2018, IAS PARK CITY MATH S; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Duchi John C., 2019, LECT NOTES STAT, P377; Gentile C, 2003, MACH LEARN, V53, P265, DOI 10.1023/A:1026319107706; Hiriart-Urruty Jean-Baptiste, 1993, CONVEX ANAL MINIMIZA; Mallat S., 2008, WAVELET TOUR SIGNAL; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Orabona F., 2010, ADV NEURAL INFORM PR, P1840; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Vershynin R., 2009, LECT GEOMETRIC FUNCT; Wainwright M. J., 2019, HIGH DIMENSIONALSTAT; Wilson AC, 2017, ADV NEUR IN, V30; Yu B., 1997, FESTSCHRIFT L LECAM, P423	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903016
C	Li, JJ; Huang, S; So, AMC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Jiajin; Huang, Sen; So, Anthony Man-Cho			A First-Order Algorithmic Framework for Wasserstein Distributionally Robust Logistic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVEX-OPTIMIZATION	Wasserstein distance-based distributionally robust optimization (DRO) has received much attention lately due to its ability to provide a robustness interpretation of various learning models. Moreover, many of the DRO problems that arise in the learning context admits exact convex reformulations and hence can be tackled by off-the-shelf solvers. Nevertheless, the use of such solvers severely limits the applicability of DRO in large-scale learning problems, as they often rely on general purpose interior-point algorithms. On the other hand, there are very few works that attempt to develop fast iterative methods to solve these DRO problems, which typically possess complicated structures. In this paper, we take a first step towards resolving the above difficulty by developing a first-order algorithmic framework for tackling a class of Wasserstein distance-based distributionally robust logistic regression (DRLR) problem. Specifically, we propose a novel linearized proximal ADMM to solve the DRLR problem, whose objective is convex but consists of a smooth term plus two non-separable non-smooth terms. We prove that our method enjoys a sublinear convergence rate. Furthermore, we conduct three different experiments to show its superb performance on both synthetic and real-world datasets. In particular, our method can achieve the same accuracy up to 800+ times faster than the standard off-the-shelf solver.	[Li, Jiajin; Huang, Sen; So, Anthony Man-Cho] Chinese Univ Hong Kong, Dept Syst Engn & Engn Management, Shatin, Hong Kong, Peoples R China	Chinese University of Hong Kong	Li, JJ (corresponding author), Chinese Univ Hong Kong, Dept Syst Engn & Engn Management, Shatin, Hong Kong, Peoples R China.	jjli@se.cuhk.edu.hk; hsen@se.cuhk.edu.hk; manchoso@se.cuhk.edu.hk	So, Anthony Man-Cho/AFR-0815-2022; So, Anthony Man-Cho/F-6001-2011	So, Anthony Man-Cho/0000-0003-2588-7851; So, Anthony Man-Cho/0000-0003-2588-7851				Blanchet J., 2018, ARXIV181002403; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Cheng WY, 2014, OPTIM LETT, V8, P763, DOI 10.1007/s11590-013-0609-6; Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x; Gao R., 2017, ARXIV171206050; Hong MY, 2017, MATH PROGRAM, V162, P165, DOI 10.1007/s10107-016-1034-2; Hsieh C.-J., 2008, P 25 INT C MACH LEAR, P408, DOI [10.1145/1390156.1390208, DOI 10.1145/1390156.1390208]; Li M, 2016, SIAM J OPTIMIZ, V26, P922, DOI 10.1137/140999025; Li XD, 2018, SIAM J OPTIMIZ, V28, P433, DOI 10.1137/16M1097572; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Luo FQ, 2019, EUR J OPER RES, V278, P20, DOI 10.1016/j.ejor.2019.03.008; Mehrotra S, 2015, DISTRIBUTIONAL UNPUB; Namkoong H, 2016, ADV NEURAL INFORM PR, P2208; Namkoong Hongseok, 2017, NEURIPS, P2971; Shafieezadeh-Abadeh S., 2019, J MACHINE LEARNING R, V20, P1; Sinha Aman, 2017, ARXIV PREPRINT ARXIV; Sra S., 2012, NEURAL INFORM PROCES; Vapnik V., 2000, STAT ENG INFORM SCI, V2; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Yang JF, 2013, MATH COMPUT, V82, P301; Yin WT, 2010, SIAM J IMAGING SCI, V3, P856, DOI 10.1137/090760350; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157; Zhou ZR, 2017, MATH PROGRAM, V165, P689, DOI 10.1007/s10107-016-1100-9	28	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303088
C	Lim, JH; Pinheiro, PO; Rostamzadeh, N; Pal, C; Ahn, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lim, Jae Hyun; Pinheiro, Pedro O.; Rostamzadeh, Negar; Pal, Christopher; Ahn, Sungjin			Neural Multisensory Scene Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INTEGRATION	For embodied agents to infer representations of the underlying 3D physical world they inhabit, they should efficiently combine multisensory cues from numerous trials, e.g., by looking at and touching objects. Despite its importance, multisensory 3D scene representation learning has received less attention compared to the unimodal setting. In this paper, we propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities. We also introduce a novel method, called the Amortized Product-of-Experts, to improve the computational efficiency and the robustness to unseen combinations of modalities at test time. Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation. To perform this exploration, we also develop the Multisensory Embodied 3D-Scene Environment (MESE).	[Lim, Jae Hyun; Pinheiro, Pedro O.; Rostamzadeh, Negar; Pal, Christopher] Element AI, Montreal, PQ, Canada; [Lim, Jae Hyun; Pal, Christopher] Mila, Montreal, PQ, Canada; [Lim, Jae Hyun; Pal, Christopher] Univ Montreal, Montreal, PQ, Canada; [Pal, Christopher] Polytech Montreal, Montreal, PQ, Canada; [Ahn, Sungjin] Rutgers State Univ, New Brunswick, NJ 08901 USA; [Lim, Jae Hyun] Element AI, JHL, Montreal, PQ, Canada	Universite de Montreal; Universite de Montreal; Polytechnique Montreal; Rutgers State University New Brunswick	Lim, JH (corresponding author), Element AI, Montreal, PQ, Canada.; Lim, JH (corresponding author), Mila, Montreal, PQ, Canada.; Lim, JH (corresponding author), Univ Montreal, Montreal, PQ, Canada.; Ahn, S (corresponding author), Rutgers State Univ, New Brunswick, NJ 08901 USA.; Lim, JH (corresponding author), Element AI, JHL, Montreal, PQ, Canada.	jae.hyun.lim@umontreal.ca; sungjin.ahn@cs.rutgers.edu						Amos B., 2018, INT C LEARN REPR; [Anonymous], 2008, 2008 IEEE Hot Chips 20 Symposium (HCS), DOI 10.1109/HOTCHIPS.2008.7476516; Barsalou LW, 2008, ANNU REV PSYCHOL, V59, P617, DOI 10.1146/annurev.psych.59.103006.093639; Brockman G., 2016, OPENAI GYM; Burda Yuri, 2016, 4 INT C LEARN REPR I; Chetlur S., 2014, ARXIV; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Deneve S, 2004, J PHYSIOLOGY-PARIS, V98, P249, DOI 10.1016/j.jphysparis.2004.03.011; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Garnelo M, 2018, ARXIV180701622; Garnelo Marta, 2018, ARXIV180701613, P1704; Higgins I., 2017, P INT C LEARN REPR T; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hsu Wei-Ning, 2018, ARXIV180511264; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Johannes Matthew S, 2011, J HOPKINS APL TECHNI; Kim Hyunjik, 2019, P INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Konecn J., 2016, ARXIV161005492; Kumar Ananya, 2018, ARXIV180702033; Kumar Vikash, 2015, INT C HUM ROB HUM; Kurle Richard, 2018, ARXIV181104451; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Le Tuan Anh, 2018, NEURIPS BAYES WORKSH; Mescheder L., 2018, ARXIV181203828; MikeWu Noah, 2018, NEURIPS; Murray MM, 2011, NEURAL BASEMULTISE; Nguyen-Phuoc Thu, 2019, ARXIV190401326; Pascual-Leone A, 2001, PROG BRAIN RES, V134, P427; Paszke A., 2017, AUTOMATIC DIFFERENTI; Quiroga Rodrigo Quian, 2012, NATURE REV NEUROSCIE; Rajeswar Sai, 2019, PREPRINT; Ramachandran VS, 1998, BRAIN, V121, P1603, DOI 10.1093/brain/121.9.1603; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rohe T, 2016, CURR BIOL, V26, P509, DOI 10.1016/j.cub.2015.12.056; Rosenbaum Dan, 2018, ARXIV180703149; Saha O, 2018, ROBOTICS, V7, DOI 10.3390/robotics7030047; Shams L, 2008, TRENDS COGN SCI, V12, P411, DOI 10.1016/j.tics.2008.07.006; Shin JG, 2018, ACM INT CONF PR SER, DOI 10.1145/3174910.3174930; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Suzuki Masahiro, 2016, ARXIV161101891; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wu JJ, 2017, ADV NEUR IN, V30; Yildirim Ilker, 2014, PERCEPTION CONCEPTIO	46	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900057
C	Lin, ZC; Zhao, L; Yang, DR; Qin, T; Yang, GW; Liu, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lin, Zichuan; Zhao, Li; Yang, Derek; Qin, Tao; Yang, Guangwen; Liu, Tie-Yan			Distributional Reward Decomposition for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many reinforcement learning (RL) tasks have specific properties that can be lever-aged to modify existing RL algorithms to adapt to those tasks and further improve performance, and a general class of such properties is the multiple reward channel. In those environments the full reward can be decomposed into sub-rewards obtained from different channels. Existing work on reward decomposition either requires prior knowledge of the environment to decompose the full reward, or decomposes reward without prior knowledge but with degraded performance. In this paper, we propose Distributional Reward Decomposition for Reinforcement Learning (DRDRL), a novel reward decomposition algorithm which captures the multiple reward channel structure under distributional setting. Empirically, our method captures the multi-channel structure and discovers meaningful reward decomposition, without any requirements on prior knowledge. Consequently, our agent achieves better performance than existing methods on environments with multiple reward channels.	[Lin, Zichuan; Yang, Guangwen] Tsinghua Univ, Beijing, Peoples R China; [Lin, Zichuan; Zhao, Li; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Redmond, WA USA; [Yang, Derek] Univ Calif San Diego, La Jolla, CA USA	Tsinghua University; Microsoft; University of California System; University of California San Diego	Lin, ZC (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	linzc16@mails.tsinghua.edu.cn; lizo@microsoft.com; dyang1206@gmail.com; taoqin@microsoft.com; ygw@tsinghua.edu.cn; tyliu@microsoft.com		Qin, Tao/0000-0002-9095-0776	National Key Research & Development Plan of China [2016YFA0602200, 2017YFA0604500]; Center for High Performance Computing and System Simulation, Pilot National Laboratory for Marine Science and Technology (Qingdao)	National Key Research & Development Plan of China; Center for High Performance Computing and System Simulation, Pilot National Laboratory for Marine Science and Technology (Qingdao)	This work was supported in part by the National Key Research & Development Plan of China (grant No. 2016YFA0602200 and 2017YFA0604500), and by Center for High Performance Computing and System Simulation, Pilot National Laboratory for Marine Science and Technology (Qingdao).	Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Castro Pablo Samuel, 2018, ARXIV181206110; Dabney W, 2018, AAAI CONF ARTIF INTE, P2892; Grimm C., 2019, ARXIV190108649; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Laversanne-Finot A., 2018, ARXIV180701521; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Seijen HV, 2017, NIPS, P5392; Sprague N, 2003, MULTIPLE GOAL REINFO; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Thomas Valentin, 2017, ARXIV170801289; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Zimdars, 2003, P 20 INT C MACH LEAR, P656	17	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306024
C	Liu, JZ; Paisley, J; Kioumourtzoglou, MA; Coull, BA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Jeremiah Zhe; Paisley, John; Kioumourtzoglou, Marianthi-Anna; Coull, Brent A.			Accurate Uncertainty Estimation and Decomposition in Ensemble Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POSTERIOR; PM2.5; MODEL; REGRESSION	Ensemble learning is a standard approach to building machine learning systems that capture complex phenomena in real-world data. An important aspect of these systems is the complete and valid quantification of model uncertainty. We introduce a Bayesian nonparametric ensemble (BNE) approach that augments an existing ensemble model to account for different sources of model uncertainty. BNE augments a model's prediction and distribution functions using Bayesian nonparametric machinery. It has a theoretical guarantee in that it robustly estimates the uncertainty patterns in the data distribution, and can decompose its overall predictive uncertainty into distinct components that are due to different sources of noise and error. We show that our method achieves accurate uncertainty estimates under complex observational noise, and illustrate its real-world utility in terms of uncertainty decomposition and model bias detection for an ensemble in predict air pollution exposures in Eastern Massachusetts, USA.	[Liu, Jeremiah Zhe] Google Res, Mountain View, CA 94043 USA; [Liu, Jeremiah Zhe; Coull, Brent A.] Harvard Univ, Cambridge, MA 02138 USA; [Paisley, John; Kioumourtzoglou, Marianthi-Anna] Columbia Univ, New York, NY 10027 USA	Google Incorporated; Harvard University; Columbia University	Liu, JZ (corresponding author), Google Res, Mountain View, CA 94043 USA.; Liu, JZ (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	zhl112@mail.harvard.edu; jpaisley@columbia.edu; mk3961@cumc.columbia.edu; bcoull@hsph.harvard.edu			USEPA grant [RD-83587201]; NIH [ES030616, ES000002]	USEPA grant; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Authors would like to thank Lorenzo Trippa, Jeff Miller, Boyu Ren at Harvard Biostatistics, Yoon Kim at Harvard CS and Ge Liu at MIT EECS for the insightful comments and fruitful discussion. This publication was made possible by USEPA grant RD-83587201. Its contents are solely the responsibility of the grantee and do not necessarily represent the official views of the USEPA. Further, USEPA does not endorse the purchase of any commercial products or services mentioned in the publication. Funding was also provided by NIH grants ES030616 and ES000002.	Andrieu C, 2008, STAT COMPUT, V18, P343, DOI 10.1007/s11222-008-9110-y; Bach F., 2014, ARXIV14128690; Beyer HG, 2007, COMPUT METHOD APPL M, V196, P3190, DOI 10.1016/j.cma.2007.03.003; Billingsley P., 2012, PROBABILITY MEASURE; Bishop CH, 2008, MON WEATHER REV, V136, P4641, DOI 10.1175/2008MWR2565.1; Bobb JF, 2015, BIOSTATISTICS, V16, P493, DOI 10.1093/biostatistics/kxu058; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Breiman L, 1996, MACH LEARN, V24, P49; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; BUJA A, 1989, ANN STAT, V17, P453, DOI 10.1214/aos/1176347115; Casella G, 2001, STAT INFERENCE, V2nd; Chen YC, 2018, ARXIV180704431; Depeweg S., 2017, ARXIV170608495STAT; Depeweg S, 2018, PR MACH LEARN RES, V80; Di Q, 2016, ATMOS ENVIRON, V131, P390, DOI 10.1016/j.atmosenv.2016.02.002; Durrande N., 2011, ARXIV11116233STAT; Gal Y., 2016, U CAMBRIDGE; Gneiting T, 2007, J R STAT SOC B, V69, P243, DOI 10.1111/j.1467-9868.2007.00587.x; Guiso L, 1999, Q J ECON, V114, P185, DOI 10.1162/003355399555981; Gupta S., 2015, P 32 INT C MACH LEAR, V37, P1737; Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020; Kloog I, 2014, ATMOS ENVIRON, V95, P581, DOI 10.1016/j.atmosenv.2014.07.014; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Lee K, 2017, TRAINING CONFIDENCE; Li C, 2017, BIOMETRIKA, V104, P665, DOI 10.1093/biomet/asx033; Li MT, 2019, J AM GERIATR SOC, V67, pS506, DOI 10.1111/jgs.15641; Li Q, 2004, STAT SINICA, V14, P485; Liu J. Z., 2019, ARXIV190409632; Lorenzi M., 2018, ARXIV180205680STAT; MacEachern SN, 2007, BAYESIAN ANAL, V2, P483, DOI 10.1214/07-BA219C; Malinin A., 2018, ARXIV180210501; Mane D, 2016, ARXIV160606565CS; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Pati, 2017, OPEN J STAT, V7, P567, DOI DOI 10.4236/OJS.2017.74039; Qian Y, 2016, B AM METEOROL SOC, V97, P821, DOI 10.1175/BAMS-D-15-00297.1; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Reich BJ, 2006, BIOMETRICS, V62, P1197, DOI 10.1111/j.1541-0420.2006.00617.x; Riihimaki J, 2010, J MACH LEARN RES, V9, P645; Ruppert D., 2003, SEMIPARAMETRIC REGRE; S. D. Team, 2018, STAN USERS GUIDE; Sargent RG, 2010, WINT SIMUL C PROC, P166, DOI 10.1109/WSC.2010.5679166; Scott DW, 2015, WILEY SER PROBAB ST, P217; Sullivan T. J., 2014, TEXTS APPL MATH, V63; T. F. R. B. of Governors in Washington DC, 2011, GUIDANCE MODEL RISK; van der Vaart AW, 2009, ANN STAT, V37, P2655, DOI 10.1214/08-AOS678; van der Vaart A. W., 2011, J MACHINE LEARNING R, V12; Van Der Wart AW, 2008, ANN STAT, V36, P1435, DOI 10.1214/009053607000000613; van Donkelaar A, 2015, ENVIRON SCI TECHNOL, V49, P10482, DOI 10.1021/acs.est.5b02076; Vandal T, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2377, DOI 10.1145/3219819.3219996; Waterhouse S, 1996, ADV NEUR IN, V8, P351; Yao YL, 2018, BAYESIAN ANAL, V13, P917, DOI 10.1214/17-BA1091	51	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900053
C	Liu, X; Zou, XL; Ji, ZL; Tian, GS; Mi, YY; Huang, TJ; Wong, KYM; Wu, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Xiao; Zou, Xiaolong; Ji, Zilong; Tian, Gengshuo; Mi, Yuanyuan; Huang, Tiejun; Wong, K. Y. Michael; Wu, Si			Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEURAL-NETWORKS; VISUAL-CORTEX; REPRESENTATIONS; INTEGRATION; RESPONSES; MEMORIES; CONTOURS	Experimental data has revealed that in addition to feedforward connections, there exist abundant feedback connections in a neural pathway. Although the importance of feedback in neural information processing has been widely recognized in the field, the detailed mechanism of how it works remains largely unknown. Here, we investigate the role of feedback in hierarchical information retrieval. Specifically, we consider a hierarchical network storing the hierarchical categorical information of objects, and information retrieval goes from rough to fine, aided by dynamical push-pull feedback from higher to lower layers. We elucidate that the push (positive) and pull (negative) feedbacks suppress the interferences due to neural correlations between different and the same categories, respectively, and their joint effect improves retrieval performance significantly. Our model agrees with the push-pull phenomenon observed in neural data and sheds light on our understanding of the role of feedback in neural information processing.	[Liu, Xiao; Zou, Xiaolong; Huang, Tiejun; Wu, Si] Peking Univ, Peking Tsinghua Ctr Life Sci, Acad Adv Interdisciplinary Studies, Sch Elect Engn & Comp Sci,IDG McGovern Inst Brain, Beijing, Peoples R China; [Ji, Zilong] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing, Peoples R China; [Tian, Gengshuo] Beijing Normal Univ, Dept Math, Beijing, Peoples R China; [Mi, Yuanyuan] Chongqing Univ, Ctr Neurointelligence, Chongqing, Peoples R China; [Wong, K. Y. Michael] Hong Kong Univ Sci & Technol, Dept Phys, Hong Kong, Peoples R China	Peking University; Beijing Normal University; Beijing Normal University; Chongqing University; Hong Kong University of Science & Technology	Liu, X (corresponding author), Peking Univ, Peking Tsinghua Ctr Life Sci, Acad Adv Interdisciplinary Studies, Sch Elect Engn & Comp Sci,IDG McGovern Inst Brain, Beijing, Peoples R China.	xiaoliu23@pku.edu.cn; xiaolz@pku.edu.cn; jizilong@mail.bnu.edu.cn; gengshuo_tian@163.com; miyuanyuan0102@cqu.edu.cn; tjhuang@pku.edu.cn; phkywong@ust.hk; siwu@pku.edu.cn			BMSTC (Beijing municipal science and technology commission) [Z171100000117007]; National Natural Science Foundation of China [31771146, 11734004, 61425025]; Beijing Nova Program [Z181100006218118]; Research Grants Council of Hong Kong [16322616, 16306817, 16302419]; Huawei Technology Co., Ltd.; Guangdong Province [2018B030338001]	BMSTC (Beijing municipal science and technology commission); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission); Research Grants Council of Hong Kong(Hong Kong Research Grants Council); Huawei Technology Co., Ltd.(Huawei Technologies); Guangdong Province	This work was supported by BMSTC (Beijing municipal science and technology commission) under grant No: Z171100000117007 (D.H. Wang & Y.Y. Mi), the National Natural Science Foundation of China (N0: 31771146, 11734004, Y.Y. Mi),the National Natural Science Foundation of China (N0: 61425025, T.J. Huang) Beijing Nova Program (N0: Z181100006218118, Y. Y. Mi), Guangdong Province with Grant (No. 2018B030338001, Si Wu & Y.Y. Mi) and grants from the Research Grants Council of Hong Kong (grant numbers 16322616, 16306817 and 16302419, K. Y. Michael Wong). This work received support from Huawei Technology Co., Ltd..	AMARI S, 1988, NEURAL NETWORKS, V1, P63, DOI 10.1016/0893-6080(88)90022-6; Blumenfeld B, 2006, NEURON, V52, P383, DOI 10.1016/j.neuron.2006.08.016; Chandar S., 2016, ARXIV160507427; CHEN L, 1982, SCIENCE, V218, P699, DOI 10.1126/science.7134969; Chen L., 2015, VIS COGN, V12, P553; Chen MG, 2014, NEURON, V82, P682, DOI 10.1016/j.neuron.2014.03.023; Chen RJ, 2017, NEURON, V96, P1388, DOI 10.1016/j.neuron.2017.11.004; Gilad A, 2013, NEURON, V78, P389, DOI 10.1016/j.neuron.2013.02.013; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Kandel E.R., 2000, PRINCIPLES NEURAL SC; Kropff E, 2007, HFSP J, V1, P249, DOI 10.2976/1.2793335; Kumar S., 2005, COMP VIS 2005 ICCV 2, V2; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434; Liu L, 2017, PLOS BIOL, V15, DOI 10.1371/journal.pbio.2003646; McFadyen J, 2017, J NEUROSCI, V37, P3864, DOI 10.1523/JNEUROSCI.3525-16.2017; Morin F., 2005, PROC INT WORKSHOP AR, P246; Okada M, 2006, NEW GENERAT COMPUT, V24, P185, DOI 10.1007/BF03037297; PARGA N, 1986, J PHYS-PARIS, V47, P1857, DOI 10.1051/jphys:0198600470110185700; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Sillito AM, 2006, TRENDS NEUROSCI, V29, P307, DOI 10.1016/j.tins.2006.05.001; Simonyan K., 2015, ARXIV PREPRINT ARXIV; SOURLAS N, 1988, EUROPHYS LETT, V7, P749, DOI 10.1209/0295-5075/7/8/014; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	26	1	1	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305067
C	Lu, AX; Lu, AX; Schormann, W; Ghassemi, M; Andrews, DW; Moses, AM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lu, Alex X.; Lu, Amy X.; Schormann, Wiebke; Ghassemi, Marzyeh; Andrews, David W.; Moses, Alan M.			The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LOCALIZATION	Understanding if classifiers generalize to out-of-sample datasets is a central problem in machine learning. Microscopy images provide a standardized way to measure the generalization capacity of image classifiers, as we can image the same classes of objects under increasingly divergent, but controlled factors of variation. We created a public dataset of 132,209 images of mouse cells, COOS-7 (Cells Out Of Sample 7-Class). COOS-7 provides a classification setting where four test datasets have increasing degrees of covariate shift: some images are random subsets of the training data, while others are from experiments reproduced months later and imaged by different instruments. We benchmarked a range of classification models using different representations, including transferred neural network features, end-to-end classification with a supervised deep CNN, and features from a self-supervised CNN. While most classifiers perform well on test datasets similar to the training dataset, all classifiers failed to generalize their performance to datasets with greater covariate shifts. These baselines highlight the challenges of covariate shifts in image data, and establish metrics for improving the generalization capacity of image classifiers.	[Lu, Alex X.] Univ Toronto, Comp Sci, Toronto, ON, Canada; [Lu, Amy X.] Univ Toronto, Vector Inst, Comp Sci, Toronto, ON, Canada; [Schormann, Wiebke; Andrews, David W.] Sunnybrook Res Inst, Biol Sci, Toronto, ON, Canada; [Ghassemi, Marzyeh] Univ Toronto, Vector Inst, CIFAR AI Chair, Toronto, ON, Canada; [Andrews, David W.] Univ Toronto, Biochem & Med Biophys, Toronto, ON, Canada; [Moses, Alan M.] Univ Toronto, Cell & Syst Biol, Comp Sci, CAGEF, Toronto, ON, Canada	University of Toronto; University of Toronto; University of Toronto; Sunnybrook Research Institute; University Toronto Affiliates; Sunnybrook Health Science Center; University of Toronto; University of Toronto; University of Toronto	Lu, AX (corresponding author), Univ Toronto, Comp Sci, Toronto, ON, Canada.	alexlu@cs.toronto.edu; amyxlu@cs.toronto.edu; wiebke.schormann@sri.utoronto.ca; marzyeh@cs.toronto.edu; david.andrews@sri.utoronto.ca; alan.moses@utoronto.ca		Lu, Alex/0000-0001-9568-3155	NSERC; CIHR Foundation [FDN143312]; Microsoft Research; CIFAR AI Chair at the Vector Institute; Canada Research Council Chair	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIHR Foundation(Canadian Institutes of Health Research (CIHR)); Microsoft Research(Microsoft); CIFAR AI Chair at the Vector Institute; Canada Research Council Chair	Alex X. Lu is funded by a pre-doctoral award from NSERC. Amy X. Lu is funded by a Master's award from NSERC. Alan M. Moses holds a Tier II Canada Research Chair. Wiebke Schormann and David W. Andrews are funded by CIHR Foundation Grant FDN143312. David W. Andrews holds a Tier 1 Canada Research Chair in Membrane Biogenesis. Maryzeh Ghassemi is funded in part by Microsoft Research, a CIFAR AI Chair at the Vector Institute, a Canada Research Council Chair, and an NSERC Discovery Grant. This work was partially performed on a GPU donated by Nvidia.	Ando D. M., 2017, IMPROVING PHENOTYPIC, DOI DOI 10.1101/161422; Bray MA, 2017, GIGASCIENCE, V6, DOI 10.1093/gigascience/giw014; Caicedo JC, 2017, NAT METHODS, V14, P849, DOI [10.1038/nmeth.4397, 10.1038/NMETH.4397]; Chen JY, 2014, J NEUROSCI METH, V230, P37, DOI 10.1016/j.jneumeth.2014.04.023; Collins TJ, 2015, ASSAY DRUG DEV TECHN, V13, P547, DOI 10.1089/adt.2015.661; Grys BT, 2017, J CELL BIOL, V216, P65, DOI 10.1083/jcb.201610026; Hung MC, 2011, J CELL SCI, V124, P3381, DOI 10.1242/jcs.089110; Jung K, 2015, J BIOMED INFORM, V58, P168, DOI 10.1016/j.jbi.2015.10.006; Koh JLY, 2015, G3-GENES GENOM GENET, V5, P1223, DOI 10.1534/g3.115.017830; Kothari S, 2014, IEEE J BIOMED HEALTH, V18, P765, DOI 10.1109/JBHI.2013.2276766; Kraus OZ, 2017, MOL SYST BIOL, V13, DOI 10.15252/msb.20177551; Lu AX, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1007348; Lu Alex X, 2019, BIOINFORMATICS; Nestor B., 2019, P 4 MACHINE LEARNING, P381; Pawlowski N., 2016, BIORXIV, DOI DOI 10.1101/085118; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Raj A, 2008, CELL, V135, P216, DOI 10.1016/j.cell.2008.09.050; Recht B, 2019, PR MACH LEARN RES, V97; Schoenauer-Sebag Alice, 2019, ICLR 2019; Shamir L, 2011, J MICROSC-OXFORD, V243, P284, DOI 10.1111/j.1365-2818.2011.03502.x; Singh S, 2014, J MICROSC-OXFORD, V256, P231, DOI 10.1111/jmi.12178; Snijder B, 2011, NAT REV MOL CELL BIO, V12, P119, DOI 10.1038/nrm3044; Tabak Gil, 2017, CORRECTING NUISANCE; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Xu YY, 2018, FRONT COMPUT SCI-CHI, V12, P26, DOI 10.1007/s11704-016-6309-5; Zech JR, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002683	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301080
C	Mahloujifar, S; Zhang, X; Mahmoody, M; Evans, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mahloujifar, Saeed; Zhang, Xiao; Mahmoody, Mohammad; Evans, David			Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many recent works have shown that adversarial examples that fool classifiers can be found by minimally perturbing a normal input. Recent theoretical results, starting with Gilmer et al. (2018b), show that if the inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable. A concentrated space has the property that any subset with Omega(1) (e.g., 1/100) measure, according to the imposed distribution, has small distance to almost all (e.g., 99/100) of the points in the space. It is not clear, however, whether these theoretical results apply to actual distributions such as images. This paper presents a method for empirically measuring and bounding the concentration of a concrete dataset which is proven to converge to the actual concentration. We use it to empirically estimate the intrinsic robustness to l(infinity) and l(2) perturbations of several image classification benchmarks.	[Mahloujifar, Saeed; Zhang, Xiao; Mahmoody, Mohammad; Evans, David] Univ Virginia, Charlottesville, VA 22903 USA	University of Virginia	Mahloujifar, S (corresponding author), Univ Virginia, Charlottesville, VA 22903 USA.	saeed@virginia.edu; shawn@virginia.edu; mohammad@virginia.edu; evans@virginia.edu			National Science Foundation SaTC program (Center for Trustworth Machine Learning) [1804603]; NSF CAREER [CCF-1350939]; Baidu; Intel; Amazon	National Science Foundation SaTC program (Center for Trustworth Machine Learning); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Baidu; Intel(Intel Corporation); Amazon	This work was partially funded by an award from the National Science Foundation SaTC program (Center for Trustworth Machine Learning, #1804603), an NSF CAREER award (CCF-1350939), and support from Baidu, Intel, and Amazon.	[Anonymous], 2001, MATH SURVEYS MONOGRA; [Anonymous], 2018, INT C LEARN REPR; Attias Idan, 2019, ALGORITHMIC LEARNING; Bhagoji A. N., 2019, ADV NEURAL INFORM PR; Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cullina D., 2018, ADV NEURAL INFORM PR; Devroye L., 2013, PROBABILISTIC THEORY; Diochnos Dimitrios, 2018, ADV NEURAL INFORM PR; Eisenstat D, 2007, INFORM PROCESS LETT, V101, P181, DOI 10.1016/j.ipl.2006.10.004; Fawzi Alhussein, 2018, ADV NEURAL INFORM PR; Gilmer J., 2018, INT C LEARN REPR WOR; Gilmer Justin, 2018, ARXIV180706732; Goodfellow I. J., 2015, P ICLR; Gowal Sven, 2019, IEEE INT C COMP VIS; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krusinga R., 2019, ARXIV190101499; LeCun Y, 2010, ATT LAB; Levy Paul, 1951, PROBLEMES CONCRETS A, V6; Madry Aleksander, 2018, ICLR; Mahloujifar Saeed, 2019, AAAI C ART INT; Milman V.D., 1986, ASYMPTOTIC THEORY FI; Montasser Omar, 2019, P MACHINE LEARNING R, V99, P1; Netzer Yuval, 2011, NEURIPS WORKSH DEEP; Neyshabur Behnam, 2017, ADV NEURAL INFORM PR; Omohundro S. M., 1989, 5 BALLTREE CONSTRUCT; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Pedregosa F., 2011, J MACH LEARN RES; Raghunathan A., 2019, ARXIV190606032; Schott Lukas, 2019, INT C LEARN REPR; Scott CD, 2006, J MACH LEARN RES, V7, P665; Shafahi Ali, 2019, INT C LEARN REPR; Sinha A., 2018, ICLR; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Talagrand M, 1995, PUBL MATH-PARIS, P73; Turner A., 2019, INT C LEARN REPR; WAGNER D, 2018, INT C MACH LEARN; Wang S., 2018, ARXIV181102625; Wong E., 2018, INT C MACH LEARN; Wong E., 2018, ADV NEURAL INFORM PR; Xiao H., 2017, ARXIV 170807747; Yin Dong, 2019, INT C MACH LEARN; Zhang H., 2019, ARXIV190606316, V2018, P4939	44	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305023
C	Manino, E; Tran-Thanh, L; Jennings, NR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Manino, Edoardo; Tran-Thanh, Long; Jennings, Nicholas R.			Streaming Bayesian Inference for Crowdsourced Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A key challenge in crowdsourcing is inferring the ground truth from noisy and unreliable data. To do so, existing approaches rely on collecting redundant information from the crowd, and aggregating it with some probabilistic method. However, oftentimes such methods are computationally inefficient, are restricted to some specific settings, or lack theoretical guarantees. In this paper, we revisit the problem of binary classification from crowdsourced data. Specifically we propose Streaming Bayesian Inference for Crowdsourcing (SBIC), a new algorithm that does not suffer from any of these limitations. First, SBIC has low complexity and can be used in a real-time online setting. Second, SBIC has the same accuracy as the best state-of-the-art algorithms in all settings. Third, SBIC has provable asymptotic guarantees both in the online and offline settings.	[Manino, Edoardo; Tran-Thanh, Long] Univ Southampton, Southampton, Hants, England; [Jennings, Nicholas R.] Imperial Coll, London, England	University of Southampton; Imperial College London	Manino, E (corresponding author), Univ Southampton, Southampton, Hants, England.	E.Manino@soton.ac.uk; tran-thanh@soton.ac.uk; n.jennings@imperial.ac.uk	Jennings, Nick/HGU-8308-2022; Manino, Edoardo/ABG-1744-2021	Jennings, Nick/0000-0003-0166-248X; 	UK Research Council project ORCHID [EP/I01 1587/1]	UK Research Council project ORCHID	This research is funded by the UK Research Council project ORCHID, grant EP/I01 1587/1. The authors acknowledge the use of the IRIDIS High Performance Computing Facility, and associated support services at the University of Southampton.	Barowy DW, 2012, ACM SIGPLAN NOTICES, V47, P639, DOI 10.1145/2398857.2384663; Bonald T, 2017, P 30 INT C NEUR INF, P4355; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Downs JS, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2399; Ipeirotis Panagiotis G., 2010, P ACM SIGKDD WORKSH, DOI [10.1145/1837885.1837906, DOI 10.1145/1837885.1837906]; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Kim H.-C., 2012, P 15 INT C ARTIFICIA, V22, P619; Lease Matthew, 2011, P TREC 2011; Liu Q., 2012, NIPS, P692; Manino E, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1568; Simpson Edwin, 2014, SCALABLE DECISION MA, P1; Snow Rion, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751; WELINDER P, 2010, P WORKSH ADV COMP VI, P1; Welinder P., 2010, 2010 IEEE COMPUTER S, P25, DOI [10.1109/CVPRW.2010.5543189, DOI 10.1109/CVPRW.2010.5543189]; Zhang Y., 2016, J MACHINE LEARNING R, V17, P3537; Zhou D., 2016, P INT C MACH LEARN I, P603	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904043
C	Mansouri, F; Chen, YX; Vartanian, A; Zhu, XJ; Singla, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mansouri, Farnam; Chen, Yuxin; Vartanian, Ara; Zhu, Xiaojin; Singla, Adish			Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SAMPLE	Algorithmic machine teaching studies the interaction between a teacher and a learner where the teacher selects labeled examples aiming at teaching a target hypothesis. In a quest to lower teaching complexity and to achieve more natural teacher-learner interactions, several teaching models and complexity measures have been proposed for both the batch settings (e.g., worst-case, recursive, preferencebased, and non-clashing models) as well as the sequential settings (e.g., local preference-based model). To better understand the connections between these different batch and sequential models, we develop a novel framework which captures the teaching process via preference functions Sigma. In our framework, each function sigma is an element of Sigma induces a teacher-learner pair with teaching complexity as TD(sigma). We show that the above-mentioned teaching models are equivalent to specific types/families of preference functions in our framework. This equivalence, in turn, allows us to study the differences between two important teaching models, namely sigma functions inducing the strongest batch (i.e., non-clashing) model and sigma functions inducing a weak sequential (i.e., local preference-based) model. Finally, we identify preference functions inducing a novel family of sequential models with teaching complexity linear in the VC dimension of the hypothesis class: this is in contrast to the best known complexity result for the batch models which is quadratic in the VC dimension.	[Mansouri, Farnam; Singla, Adish] Max Planck Inst Software Syst MPI SWS, Saarbrucken, Germany; [Chen, Yuxin] Univ Chicago, Chicago, IL 60637 USA; [Vartanian, Ara; Zhu, Xiaojin] Univ Wisconsin, Madison, WI 53706 USA	Max Planck Society; University of Chicago; University of Wisconsin System; University of Wisconsin Madison	Mansouri, F (corresponding author), Max Planck Inst Software Syst MPI SWS, Saarbrucken, Germany.	mfarnam@mpi-sws.org; chenyuxin@uchicago.edu; aravart@cs.wisc.edu; jerryzhu@cs.wisc.edu; adishs@mpi-sws.org	Singla, Adish/ABG-8960-2021	Chen, Yuxin/0000-0003-2133-140X	NSF [1545481, 1561512, 1623605, 1704117, 1836978]; MADLab AF CoE [FA9550-18-1-0166]	NSF(National Science Foundation (NSF)); MADLab AF CoE	This work was done in part when Yuxin Chen was at Caltech. Xiaojin Zhu is supported by NSF 1545481, 1561512, 1623605, 1704117, 1836978 and the MADLab AF CoE FA9550-18-1-0166.	Angluin D., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P285, DOI 10.1145/267460.267515; [Anonymous], [No title captured]; Bonawitz E, 2014, COGNITIVE PSYCHOL, V74, P35, DOI 10.1016/j.cogpsych.2014.06.003; Cakmak M., 2012, 26 AAAI C ART INT; Chen Yuxin, 2018, ADV NEURAL INFORM PR, P1476; Doliwa T, 2014, J MACH LEARN RES, V15, P3107; Floyd S, 1995, MACH LEARN, V21, P269; Gao ZY, 2017, J MACH LEARN RES, V18, P1; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Goldman SA, 1996, J COMPUT SYST SCI, V52, P255, DOI 10.1006/jcss.1996.0020; Gu XN, 2016, ASIA-PAC POWER ENERG, P2164, DOI 10.1109/APPEEC.2016.7779870; Haug Luis, 2018, ADV NEURAL INFORM PR, P8464; Hu L., 2017, P MACHINE LEARNING R, V65, P1147; Hunziker A, 2019, ADV NEURAL INFORM PR; Kamalaruban P, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2692; Kirkpatrick David, 2019, P 30 INT C ALG LEARN, V98, P506; Kuhlmann C, 1999, LECT NOTES ARTIF INT, V1572, P168; Lessard L., 2019, 22 INT CON ART INT S, P2495; Liu WY, 2018, PR MACH LEARN RES, V80; Liu WY, 2017, PR MACH LEARN RES, V70; Ott M, 1999, LECT NOTES ARTIF INT, V1572, P183; Singla A., 2013, NIPS WORKSH DAT DRIV; Singla A, 2014, PR MACH LEARN RES, V32, P154; Tschiatschek S., 2019, ADV NEURAL INFORM PR; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Zhu X., 2013, ADV NEURAL INFORM PR, P1905; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083; Zhu Xiaojin, 2018, ARXIV181104422; Zhu Xiaojin, 2018, ABS180105927 CORR; Zilles S, 2011, J MACH LEARN RES, V12, P349; Zilles Sandra, 2008, P 21 ANN C LEARN THE, P135	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900075
C	Mendonca, R; Gupta, A; Kralev, R; Abbeel, P; Levine, S; Finn, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mendonca, Russell; Gupta, Abhishek; Kralev, Rosen; Abbeel, Pieter; Levine, Sergey; Finn, Chelsea			Guided Meta-Policy Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reinforcement learning (RL) algorithms have demonstrated promising results on complex tasks, yet often require impractical numbers of samples since they learn from scratch. Meta-RL aims to address this challenge by leveraging experience from previous tasks so as to more quickly solve new tasks. However, in practice, these algorithms generally also require large amounts of on-policy experience during the meta-training process, making them impractical for use in many problems. To this end, we propose to learn a reinforcement learning procedure in a federated way, where individual off-policy learners can solve the individual meta-training tasks, and then consolidate these solutions into a single meta-learner. Since the central meta-learner learns by imitating the solutions to the individual tasks, it can accommodate either the standard meta-RL problem setting, or a hybrid setting where some or all tasks are provided with example demonstrations. The former results in an approach that can leverage policies learned for previous tasks without significant amounts of on-policy data during meta-training, whereas the latter is particularly useful in cases where demonstrations are easy for a person to provide. Across a number of continuous control meta-RL problems, we demonstrate significant improvements in meta-RL sample efficiency in comparison to prior work as well as the ability to scale to domains with visual observations.	[Mendonca, Russell; Gupta, Abhishek; Kralev, Rosen; Abbeel, Pieter; Levine, Sergey; Finn, Chelsea] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Mendonca, R (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	russellm@berkeley.edu; abhigupta@eecs.berkeley.edu; rdkralev@gmail.com; pabbeel@eecs.berkeley.edu; svlevine@eecs.berkeley.edu; cbfinn@berkeley.edu			Intel; National Science Foundation; JP Morgan	Intel(Intel Corporation); National Science Foundation(National Science Foundation (NSF)); JP Morgan	The authors would like to thank Tianhe Yu for contributions on an early version of the paper. This work was supported by Intel, JP Morgan and a National Science Foundation Graduate Research Fellowship for Abhishek Gupta.	[Anonymous], 2019, ARXIV190308254; Bengio Y., LEARNING SYNAPTIC LE; Bojarski M., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/IVS.2017.7995975; Brockman Greg, 2017, ARXIV160601540; Brys T., 2015, IJCAI; Clavera I., 2018, ARXIV180311347; Duan Y., 2016, RL2 FAST REINFORCEME; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Finn C., 2017, INT C MACH LEARN; Finn C., 2018, INT C LEARN REPR ICL; Finn Chelsea, 2017, C ROB LEARN PMLR, P357; Ghosh D., 2018, INT C LEARN REPR ICL; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Gupta A., 2018, P 32 INT C NEUR INF, P5307; Haarnoja T., 2018, P 35 INT C MACH LEAR; Hester Todd, 2018, AAAI; Houthooft R, 2018, ARXIV180204821; Kahn Gregory, 2016, ABS160300622 CORR; Kober J., 2013, INT J ROBOTICS RES; Kober Jens, 2009, NEURAL INFORM PROCES; Kormushev Petar, 2010, INT C INT ROB SYST I; Levine S, 2016, J MACH LEARN RES, V17; Mishra N., 2018, INT C LEARN REPR ICL; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair Ashvin, 2018, INT C ROB AUT ICRA; Norouzi Mohammad, 2016, ABS160900150 CORR; Omidshafiei Shayegan, 2017, INT C MACH LEARN ICM; Parisotto Emilio, 2016, 4 INT C LEARN REPR I; Peters Jan, 2006, INT C INT ROB SYST I; Pinto L, 2017, ARXIV171006542; Rajeswaran Aravind, 2018, ROBOTICS SCI SYSTEMS; Rakelly Kate, 2019, INT C MACH LEARN ICM; Ross S., 2011, INT C ART INT STAT; Rothfuss Jonas, 2018, ABS181006784 CORR; Rusu A. A., 2016, POLICY DISTILLATION; Saemundsson Steindor, 2018, ABS180307551 CORR; Sascha L., 2012, 2012 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2012.6252823; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Silver D., 2016, NATURE; Stadie Bradly C, 2018, ARXIV180301118; Subramanian K., 2016, INT C AUT AG MULT SY; Sun W., 2018, INT C LEARN REPR ICL; Sung F., 2017, ARXIV170609529; Taylor M., 2011, P INT C AUT AG MULT; Teh Yee, 2017, NEURAL INFORM PROCES; Thrun S., 2012, LEARNING LEARN; Wang J.X., 2016, ARXIV161105763; Williams Ronald J, 1992, REINFORCEMENT LEARNI; Zhang J, 2017, PROCEEDINGS OF THE ASME 11TH INTERNATIONAL CONFERENCE ON ENERGY SUSTAINABILITY, 2017	50	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901030
C	Meng, C; Ke, Y; Zhang, JY; Zhang, M; Zhong, WX; Ma, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meng, Cheng; Ke, Yuan; Zhang, Jingyi; Zhang, Mengrui; Zhong, Wenxuan; Ma, Ping			Large-scale optimal transport map estimation using projection pursuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SLICED INVERSE REGRESSION; DIMENSION REDUCTION; OCCUPATION MEASURES; CONVERGENCE	This paper studies the estimation of large-scale optimal transport maps (OTM), which is a well known challenging problem owing to the curse of dimensionality. Existing literature approximates the large-scale OTM by a series of one-dimensional OTM problems through iterative random projection. Such methods, however, suffer from slow or none convergence in practice due to the nature of randomly selected projection directions. Instead, we propose an estimation method of large-scale OTM by combining the idea of projection pursuit regression and sufficient dimension reduction. The proposed method, named projection pursuit Monge map (PPMM), adaptively selects the most "informative" projection direction in each iteration. We theoretically show the proposed dimension reduction method can consistently estimate the most "informative" projection direction in each iteration. Furthermore, the PPMM algorithm weakly convergences to the target large-scale OTM in a reasonable number of steps. Empirically, PPMM is computationally easy and converges fast. We assess its finite sample performance through the applications of Wasserstein distance estimation and generative models.	[Meng, Cheng; Ke, Yuan; Zhang, Jingyi; Zhang, Mengrui; Zhong, Wenxuan; Ma, Ping] Univ Georgia, Dept Stat, Athens, GA 30602 USA	University System of Georgia; University of Georgia	Meng, C (corresponding author), Univ Georgia, Dept Stat, Athens, GA 30602 USA.	cheng.meng25@uga.edu; yuan.ke@uga.edu; jingyi.zhang25@uga.edu; mengrui.zhang@uga.edu; wenxuan@uga.edu; pingma@uga.edu	Ke, Yuan/AFK-3195-2022	Zhang, Mengrui/0000-0002-7082-3753	National Science Foundation [DMS-1440037, DMS-1440038, DMS-1438957]; NIH [R01GM113242, R01GM122080]	National Science Foundation(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We would like to thank Xiaoxiao Sun, Rui Xie, Xinlian Zhang, Yiwen Liu, and Xing Xin for many fruitful discussions. We would also like to thank Dr. Xianfeng David Gu for his insightful blog about the Optimal transportation theory. Also, we would like to thank the UC Irvine Machine Learning Repository for dataset assistance. This work was partially supported by National Science Foundation grants DMS-1440037, DMS-1440038, DMS-1438957, and NIH grants R01GM113242, R01GM122080.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Benamou JD, 2002, INT J NUMER METH FL, V40, P21, DOI 10.1002/fld.264; Bertsekas D. P., 1992, COMPUT OPTIM APPL, V1, P7, DOI DOI 10.1007/BF00247653; Blaauw M, 2016, INTERSPEECH, P1770, DOI 10.21437/Interspeech.2016-1183; Boissard E, 2014, ANN I H POINCARE-PR, V50, P539, DOI 10.1214/12-AIHP517; Boissard E, 2011, ELECTRON J PROBAB, V16, P2296, DOI 10.1214/EJP.v16-958; Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3; Brenier Y, 1997, ARCH RATION MECH AN, V138, P319, DOI 10.1007/s002050050044; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; COOK RD, 1991, J AM STAT ASSOC, V86, P328, DOI 10.2307/2290564; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dosovitskiy Alexey, 2016, NEURIPS; Engel J, 2017, PR MACH LEARN RES, V70; Ferradans S, 2014, SIAM J IMAGING SCI, V7, P1853, DOI 10.1137/130929886; FRIEDMAN JH, 1987, J AM STAT ASSOC, V82, P249, DOI 10.2307/2289161; FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; Gerber S, 2017, J MACH LEARN RES, V18; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Guo Y., 2019, ARXIV190202934; Hensel M, 2017, ADV NEUR IN, V30; HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519; Ifarraguerri A, 2000, IEEE T GEOSCI REMOTE, V38, P2529, DOI 10.1109/36.885200; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kolouri S, 2018, ARXIV180401947; Li B, 2007, J AM STAT ASSOC, V102, P997, DOI 10.1198/016214507000000536; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.1080/01621459.1992.10476258; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194; Liu Y., 2017, ARXIV E PRINTS; Luenberger D.G, 2016, LINEAR NONLINEAR PRO, DOI 10.1007/978-3-319-18842-3; Merigot Q, 2011, COMPUT GRAPH FORUM, V30, P1583, DOI 10.1111/j.1467-8659.2011.02032.x; Papadakis N, 2014, SIAM J IMAGING SCI, V7, P212, DOI 10.1137/130920058; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Pitie F, 2005, IEEE I CONF COMP VIS, P1434; Pitie F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011; Rabin J, 2014, IEEE IMAGE PROC, P4852, DOI 10.1109/ICIP.2014.7025983; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Reich S, 2013, SIAM J SCI COMPUT, V35, pA2013, DOI 10.1137/130907367; Rigollet, 2019, ARXIV190505828; Rubner Yossi, 1997, IM UND WORKSH, P661; Salimans T., 2018, ARXIV180305573; Schuhmacher D., 2019, TRANSPORT COMPUTATIO; Seguy Vivien, 2017, ARXIV PREPRINT ARXIV; Solomon J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601175; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Su ZY, 2015, IEEE T PATTERN ANAL, V37, P2246, DOI 10.1109/TPAMI.2015.2408346; Taylor PN, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114316; Tolstikhin Ilya, 2017, ARXIV171101558; Villani C., 2008, OPTIMAL TRANSPORT OL; Weed J, 2017, ARXIV170700087	54	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308017
C	Meng, YX; Wu, W; Wang, F; Li, XY; Nie, P; Yin, F; Li, MY; Han, QH; Sun, XF; Li, JW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meng, Yuxian; Wu, Wei; Wang, Fei; Li, Xiaoya; Nie, Ping; Yin, Fan; Li, Muyu; Han, Qinghong; Sun, Xiaofei; Li, Jiwei			Glyce: Glyph-vectors for Chinese Character Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8% on the Fudan corpus for text classification. (1 2)	[Meng, Yuxian; Wu, Wei; Wang, Fei; Li, Xiaoya; Nie, Ping; Yin, Fan; Li, Muyu; Han, Qinghong; Sun, Xiaofei; Li, Jiwei] Shannon AI, Beijing, Peoples R China		Meng, YX (corresponding author), Shannon AI, Beijing, Peoples R China.	yuxian_meng@shannonai.com; wei_wu@shannonai.com; fei_wang@shannonai.com; xiaoya_li@shannonai.com; ping_nie@shannonai.com; fan_yin@shannonai.com; muyu_li@shannonai.com; qinghong_han@shannonai.com; xiaofei_sun@shannonai.com; jiwei_li@shannonai.com						Ballesteros M., 2016, ARXIV160303793; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Cao S., 2018, CW2VEC LEARNING CHIN; Chen D., 2014, P 2014 C EMPIRICAL M, P740, DOI DOI 10.3115/V1/D14-1082; Cheng H., 2016, ARXIV160802076; Dai F Z, 2017, ARXIV170900028; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dozat T, 2016, ARXIV161101734; Dyer C, 2015, ARXIV150508075; He SX, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2061; Huang Weipeng, 2019, ABS190304190 CORR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kiperwasser Eliyahu, 2016, ARXIV160304351; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Ronglu, 2011, FUDAN CORPUS TEXT CL; Li Y, 2015, ARXIV150806669; Liu F., 2017, ARXIV170404859; Ma Ji, 2018, ABS180806511 CORR; Ma N., 2018, ARXIV180711164, P5; Ma X., 2016, ELECTRON J DIFFER EQ, V2016, P1, DOI DOI 10.2147/DDDT.S102541; Marcheggiani D, 2017, P EMNLP SEP, DOI DOI 10.18653/V1/D17-1159; Peters Matthew, 2018, DEEP CONTEXTUALIZED, P2227, DOI [10.18653/v1/N18-1202, DOI 10.18653/V1/N18-1202]; Roth Michael, 2016, ARXIV160507515; Shao Y., 2017, P C 8 INT JOINT C NA, P173; Shi XL, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P594; Su T., 2017, ARXIV170804755; Sun Y, 2014, LECT NOTES COMPUT SC, V8835, P279, DOI 10.1007/978-3-319-12640-1_34; Sutskever I., 2018, IMPROVING LANGUAGE U; Szegedy C., 2016, IEEE C COMP VIS PATT; Tan Mi Xue, 2018, ARXIV180503330; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wang Z, 2017, IEEE IJCNN, P1411, DOI 10.1109/IJCNN.2017.7966018; Williams Adina, MULTIGENRE NLI CORPU; Yang J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P839, DOI 10.18653/v1/P17-1078; Yang Jie, 2018, ABS181012594 CORR; Yang Q, 2018, BIOMED RES INT, V2018, DOI 10.1155/2018/6751952; Yin R, 2016, P 2016 C EMP METH NA, P981, DOI DOI 10.18653/V1/D16-1100; Zhang T, 2017, COMPUTER VISION PATT; Zhang Xiang, 2017, WHICH ENCODING IS BE; Zhang Y, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1554	41	1	1	6	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302071
C	Mercado, P; Tudisco, F; Hein, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mercado, Pedro; Tudisco, Francesco; Hein, Matthias			Generalized Matrix Means for Semi-Supervised Learning with Multilayer Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LABEL PROPAGATION	We study the task of semi-supervised learning on multilayer graphs by taking into account both labeled and unlabeled observations together with the information encoded by each individual graph layer. We propose a regularizer based on the generalized matrix mean, which is a one-parameter family of matrix means that includes the arithmetic, geometric and harmonic means as particular cases. We analyze it in expectation under a Multilayer Stochastic Block Model and verify numerically that it outperforms state of the art methods. Moreover, we introduce a matrix-free numerical scheme based on contour integral quadratures and Krylov subspace solvers that scales to large sparse multilayer graphs.	[Mercado, Pedro; Hein, Matthias] Univ Tubingen, Tubingen, Germany; [Tudisco, Francesco] Gran Sasso Sci Inst, Laquila, Italy	Eberhard Karls University of Tubingen; Gran Sasso Science Institute (GSSI)	Mercado, P (corresponding author), Univ Tubingen, Tubingen, Germany.				DFG Cluster of Excellence "Machine Learning - New Perspectives for Science" [EXC 2064/1, 390727645]	DFG Cluster of Excellence "Machine Learning - New Perspectives for Science"(German Research Foundation (DFG))	P.M and M.H are supported by the DFG Cluster of Excellence "Machine Learning - New Perspectives for Science", EXC 2064/1, project number 390727645	Argyriou A., 2006, NEURIPS; Belkin M, 2004, COLT; BHAGWAT KV, 1978, MATH PROC CAMBRIDGE, V83, P393, DOI 10.1017/S0305004100054670; Chapelle O., 2010, SEMISUPERVISED LEARN; Craven M., 2011, AAAI; Eswaran Dhivya, 2017, VLDB; Greene, 2009, ECML PKDD; Greene D., 2005, PKDD; Gujral E, 2018, SDM; Hale N, 2008, SIAM J NUMER ANAL, V46, P2505, DOI 10.1137/070700607; Heimlicher S., 2012, ARXIV12092910; Kanade V, 2016, IEEE T INFORM THEORY, V62, P5906, DOI 10.1109/TIT.2016.2516564; Karasuyama M, 2013, IEEE T NEUR NET LEAR, V24, P1999, DOI 10.1109/TNNLS.2013.2271327; Kato T, 2009, IEEE T NEURAL NETWOR, V20, P35, DOI 10.1109/TNN.2008.2003354; Kipf Thomas N., 2017, INT C LEARNING REPRE; Liu J., 2013, SDM; Lu Q., 2003, ICML; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988; Mercado P, 2019, ICML; Mercado P., 2016, NEURIPS; Mercado P., 2018, AISTATS; Mossel E., 2016, ITCS; Nie F., 2016, IJCAI; Rasiwasia N., 2010, ACM MULTIMEDIA; SAAD Y, 1986, SIAM J SCI STAT COMP, V7, P856, DOI 10.1137/0907058; Saade A., 2018, J PHYS C SERIES, V1036; Subramanya A., 2014, GRAPH BASED SEMI SUP; Tsuda K, 2005, BIOINFORMATICS, V21, P59, DOI 10.1093/bioinformatics/bti1110; Viswanathan K., 2019, AISTATS; Yang Z., 2016, ICML; Ye J., 2018, PKDD; Zhou D., 2007, ICML; Zhou Dengyong, 2003, NEURIPS; Zhu X., 2003, ICML; Zhu Xiaojin, 2002, TECHNICAL REPORT	35	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906052
C	Milne, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Milne, Tristan			Piecewise Strong Convexity of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the loss surface of a feed-forward neural network with ReLU non-linearities, regularized with weight decay. We show that the regularized loss function is piecewise strongly convex on an important open set which contains, under some conditions, all of its global minimizers. This is used to prove that local minima of the regularized loss function in this set are isolated, and that every differentiable critical point in this set is a local minimum, partially addressing an open problem given at the Conference on Learning Theory (COLT) 2015; our result is also applied to linear neural networks to show that with weight decay regularization, there are no non-zero critical points in a norm ball obtaining training error below a given threshold. We also include an experimental section where we validate our theoretical work and show that the regularized loss function is almost always piecewise strongly convex when restricted to stochastic gradient descent trajectories for three standard image classification problems.	[Milne, Tristan] Univ Toronto, Dept Math, Toronto, ON, Canada	University of Toronto	Milne, T (corresponding author), Univ Toronto, Dept Math, Toronto, ON, Canada.	tmilne@math.toronto.edu			NSERC PGS D; Mackenzie King Open Scholarship	NSERC PGS D(Natural Sciences and Engineering Research Council of Canada (NSERC)); Mackenzie King Open Scholarship	We would like to thank the anonymous referees for their thoughtful reviews. Thanks also to Professor Adam Stinchcombe for the use of his GPUs. This work was partially supported by an NSERC PGS D and by the Mackenzie King Open Scholarship.	Allen-Zhu Zeyuan, 2018, ARXIV18103962; Choromanska A., 2015, COLT, P1756; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Finlay Chris, 2018, ARXIV181000953; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hardt M., 2016, ARXIV161104231; He K., 2015, CORR; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J. D., 2016, C LEARN THEOR, P1246; McOwen R., 2003, PARTIAL DIFFERENTIAL, Vsecond; Mityagin Boris, 2015, ARXIV15207276; Oymak S., 2019, ARXIV190204674; Safran I, 2016, PR MACH LEARN RES, V48; Santurkar S., 2018, P ADV NEURAL INFORM, P2483; Saxe A., 2014, INT C LEARNING REPRE; Soudry D., 2016, ARXIV PREPRINT ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wang T., 2017, CORR; Zhang Chiyuan, 2016, ARXIV161103530	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904059
C	Minderer, M; Sun, C; Villegas, R; Cole, F; Murphy, K; Lee, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Minderer, Matthias; Sun, Chen; Villegas, Ruben; Cole, Forrester; Murphy, Kevin; Lee, Honglak			Unsupervised Learning of Object Structure and Dynamics from Videos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Extracting and predicting object structure and dynamics from videos without supervision is a major challenge in machine learning. To address this challenge, we adopt a keypoint-based image representation and learn a stochastic dynamics model of the keypoints. Future frames are reconstructed from the keypoints and a reference frame. By modeling dynamics in the keypoint coordinate space, we achieve stable learning and avoid compounding of errors in pixel space. Our method improves upon unstructured representations both for pixel-level video prediction and for downstream tasks requiring object-level understanding of motion dynamics. We evaluate our model on diverse datasets: a multi-agent sports dataset, the Human3.6M dataset, and datasets based on continuous control tasks from the DeepMind Control Suite. The spatially structured representation outperforms unstructured representations on a range of motion-related tasks such as object tracking, action recognition and reward prediction.	[Minderer, Matthias; Sun, Chen; Villegas, Ruben; Cole, Forrester; Murphy, Kevin; Lee, Honglak] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Minderer, M (corresponding author), Google Res, Mountain View, CA 94043 USA.	mjlm@google.com; chensun@google.com; rubville@google.com; fcole@google.com; kpmurphy@google.com; honglak@google.com						Aberman Kfir, 2019, SIGGRAPH; Alemi A., 2018, ICML; Babaeizadeh M., 2018, ICLR; Bhattacharyya A., 2018, CVPR; Chan C., 2018, CORR; Chung Junyoung, 2015, NEURIPS; Denton Emily, 2018, ICML; Denton Emily L, 2017, NEURIPS; Finn Chelsea, 2016, NIPS; Hafner D., 2019, ICML; Ionescu C., 2014, PAMI; Jakab Tomas, 2018, NEURIPS; Kulkarni T., 2019, ARXIV190611883; Lee Alex X., 2018, CORR; Liu G., 2018, NEURIPS; Lotter W., 2017, ICLR; Mathieu Michael, 2016, P INT C LEARN REPR I; Oh J., 2015, NEURIPS; Ranzato M., 2014, 14126604 ARXIV; Simonyan K., 2014, 3 INT C LEARN REPR I; Srivastava Nitish, 2015, ICML; Sun C, 2019, ICLR; Tulyakov S., 2018, CVPR; Unterthiner Thomas, 2018, CORR, V7, P20; Villegas Ruben, 2017, ICML; Villegas Ruben, 2017, ICLR; Walker J., 2018, NEURIPS; Wichers Nevan, 2018, ICML; Xu Zhenjia, 2019, ICLR; Xue Tianfan, 2016, NEURIPS; Yan Xinchen, 2018, ECCV; Zhan Eric, 2019, ICLR; Zhang Y., 2018, CVPR	33	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300009
C	Mukkamala, MC; Ochs, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mukkamala, Mahesh Chandra; Ochs, Peter			Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				1ST-ORDER METHODS; NONNEGATIVE MATRIX; LINEARIZED MINIMIZATION; THRESHOLDING ALGORITHM; NONCONVEX; CONVERGENCE; OPTIMIZATION; CONTINUITY; IPIANO	Matrix Factorization is a popular non-convex optimization problem, for which alternating minimization schemes are mostly used. They usually suffer from the major drawback that the solution is biased towards one of the optimization variables. A remedy is non-alternating schemes. However, due to a lack of Lipschitz continuity of the gradient in matrix factorization problems, convergence cannot be guaranteed. A recently developed approach relies on the concept of Bregman distances, which generalizes the standard Euclidean distance. We exploit this theory by proposing a novel Bregman distance for matrix factorization problems, which, at the same time, allows for simple/closed form update steps. Therefore, for non-alternating schemes, such as the recently introduced Bregman Proximal Gradient (BPG) method and an inertial variant Convex-Concave Inertial BPG (CoCaIn BPG), convergence of the whole sequence to a stationary point is proved for Matrix Factorization. In several experiments, we observe a superior performance of our non-alternating schemes in terms of speed and objective value at the limit point.	[Mukkamala, Mahesh Chandra; Ochs, Peter] Saarland Univ, Math Optimizat Grp, Saarbrucken, Germany	Saarland University	Mukkamala, MC (corresponding author), Saarland Univ, Math Optimizat Grp, Saarbrucken, Germany.	mukkamala@math.uni-sb.de; ochs@math.uni-sb.de						Ablin P, 2019, INT CONF ACOUST SPEE, P700, DOI 10.1109/ICASSP.2019.8683291; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Attouch H, 2009, MATH PROGRAM, V116, P5, DOI 10.1007/s10107-007-0133-5; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Birnbaum Benjamin, 2011, P 12 ACM C EL COMM, P127, DOI DOI 10.1145/1993574.1993594; Bolte J, 2007, SIAM J OPTIMIZ, V18, P556, DOI 10.1137/060670080; Bolte J, 2018, SIAM J OPTIMIZ, V28, P2131, DOI 10.1137/17M1138558; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Bonettini S, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/aa5bfd; Bonettini S, 2016, SIAM J OPTIMIZ, V26, P891, DOI 10.1137/15M1019325; Brunet JP, 2004, P NATL ACAD SCI USA, V101, P4164, DOI 10.1073/pnas.0308531101; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; CENSOR Y, 1981, J OPTIMIZ THEORY APP, V34, P321, DOI 10.1007/BF00934676; Chaudhuri S, 2016, BLIND IMAGE DECONVOL; Cichocki A, 2007, LECT NOTES COMPUT SC, V4666, P169; Davis D., 2018, ARXIV180700255; Dragomir R. A., 2019, ARXIV190110791; Esposito F, 2019, J ULTRASOUND, P1; GILLIS N., 2015, REGULARIZATION OPTIM, P257, DOI [10.1201/b17558, DOI 10.1201/B17558]; Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gower RM, 2019, PR MACH LEARN RES, V97; Haeffele B.D., 2019, IEEE T PATTERN ANAL; Hanzely Filip, 2018, MSRTR201822; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Hsieh C.J., 2011, P 17 ACM SIGKDD INT, P1064; Hsieh CJ, 2014, PR MACH LEARN RES, V32; Jawanpuria P., 2018, P MACHINE LEARNING, V80, P2254; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee DD, 2001, ADV NEUR IN, V13, P556; Lu CQ, 2018, BIOINFORMATICS, V34, P3357, DOI 10.1093/bioinformatics/bty327; Lu HH, 2018, SIAM J OPTIMIZ, V28, P333, DOI 10.1137/16M1099546; Luss R, 2013, SIAM REV, V55, P65, DOI 10.1137/110839072; Maddison C. J., 2019, ARXIV190202257; Mnih A., 2007, ADV NEURAL INFORM PR, V20, P1257; Moitra A, 2016, SIAM J COMPUT, V45, P156, DOI 10.1137/140990139; Mukkamala M., 2017, P 34 INT C MACH LEAR, P2545; Mukkamala M.C, 2019, ARXIV190403537; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Nguyen LM, 2018, PR MACH LEARN RES, V80; Ochs P, 2019, J OPTIMIZ THEORY APP, V181, P244, DOI 10.1007/s10957-018-01452-0; Ochs P, 2018, J OPTIMIZ THEORY APP, V177, P153, DOI 10.1007/s10957-018-1272-y; Ochs P, 2014, SIAM J IMAGING SCI, V7, P1388, DOI 10.1137/130942954; Pock T, 2016, SIAM J IMAGING SCI, V9, P1756, DOI 10.1137/16M1064064; Powell MJ., 1973, MATH PROGRAM, V4, P193, DOI [DOI 10.1007/BF01584660, 10.1007/BF01584660]; Nguyen QV, 2017, VIETNAM J MATH, V45, P519, DOI 10.1007/s10013-016-0238-3; Rockafellar RT, 1998, FUNDAMENTAL PRINCIPL, V317; Sra S., 2006, ADV NEURAL INFORM PR, V18, P283; Srebro N., 2005, P ADV NEURAL INFORM; Starck JL., 2010, SPARSE IMAGE SIGNAL, DOI DOI 10.1017/CBO9780511730344; Teboulle M, 2018, MATH PROGRAM, V170, P67, DOI 10.1007/s10107-018-1284-2; Thung KH, 2018, MED IMAGE ANAL, V45, P68, DOI 10.1016/j.media.2018.01.002; Wen B, 2017, SIAM J OPTIMIZ, V27, P124, DOI 10.1137/16M1055323; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795; Xu Y, 2017, IEEE ACCESS, V5, P8502, DOI 10.1109/ACCESS.2017.2695239; Yang L, 2018, SIAM J OPTIMIZ, V28, P3402, DOI 10.1137/17M1130113; Yao Q., 2018, P INT C NEUR INF PRO, P5061; Yu A. W., 2014, P ADV NEUR INF PROC, P1350; Zhang X., 2019, ARXIV190411295; Zhu ZH, 2018, ADV NEUR IN, V31	68	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304029
C	Narasimhan, H; Cotter, A; Gupta, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Narasimhan, Harikrishna; Cotter, Andrew; Gupta, Maya			Optimizing Generalized Rate Metrics with Three Players	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a general framework for solving a large class of learning problems with non-linear functions of classification rates. This includes problems where one wishes to optimize a non-decomposable performance metric such as the F-measure or G-mean, and constrained training problems where the classifier needs to satisfy non-linear rate constraints such as predictive parity fairness, distribution divergences or churn ratios. We extend previous two-player game approaches for constrained optimization to an approach with three players to decouple the classifier rates from the non-linear objective, and seek to find an equilibrium of the game. Our approach generalizes many existing algorithms, and makes possible new algorithms with more flexibility and tighter handling of non-linear rate constraints. We provide convergence guarantees for convex functions of rates, and show how our methodology can be extended to handle sums-of-ratios of rates. Experiments on different fairness tasks confirm the efficacy of our approach.	[Narasimhan, Harikrishna; Cotter, Andrew; Gupta, Maya] Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA	Google Incorporated	Narasimhan, H (corresponding author), Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	hnarasimhan@google.com; acotter@google.com; mayagupta@google.com						Abernethy J. D., 2017, NIPS; Agarwal A, 2018, PR MACH LEARN RES, V80; Alabi D., 2018, ARXIV180404503; Angwin J., 2016, MACHINE BIAS; Busa-Fekete R., 2015, NIPS; Celis LE, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P160, DOI 10.1145/3287560.3287601; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Cotter A., 2019, ALGORITHMIC LEARNING; Cotter A, 2019, J MACH LEARN RES, V20; Daskalaki S, 2006, APPL ARTIF INTELL, V20, P381, DOI 10.1080/08839510500313653; Dixon L., 2018, ALES; Donini M., 2018, NEURIPS, P2791; Eban E, 2017, PR MACH LEARN RES, V54, P832; Elkan C., 2001, INT JOINT C ART INT, V17, P973, DOI DOI 10.5555/1642194.1642224; Esuli A, 2015, ACM T KNOWL DISCOV D, V9, DOI 10.1145/2700406; Fard M. M., 2016, NIPS; Frank A., 2010, UCI MACHINE LEARNING; Gao W., 2015, ASONAM; Goh G, 2016, ADV NEUR IN, V29; Gupta M., 2019, NEURIPS; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kar P, 2014, ADV NEUR IN, V27; Kar P, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1625, DOI 10.1145/2939672.2939832; Kearns M, 2018, PR MACH LEARN RES, V80; Kennedy K., 2009, ICAICS; Kim J., 2013, ACL; Koyejo O., 2014, ADV NEURAL INFORM PR, V3, P2744; Kubat M., 1997, INT CAR MOT LTD, P179, DOI DOI 10.1007/S13398-014-0173-7.2; Lawrence S, 1998, LECT NOTES COMPUT SC, V1524, P299; Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3; Lewis D.D., 1991, HLT WORKSH SPEECH NA; Liu M., 2018, NEURIPS; Manning C.D., 2008, INTRO INFORM RETRIEV; Narasimhan H, 2015, PR MACH LEARN RES, V37, P199; Narasimhan Harikrishna, 2018, INT C ART INT STAT, P1646; Pan W., 2016, ICDM; Parambath S., 2014, ADV NEURAL INF PROCE, V27, P2123; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Sanyal A, 2018, MACH LEARN, V107, P1597, DOI 10.1007/s10994-018-5736-y; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Sun Y., 2006, ICDM; Wang H., 2015, NIPS; Wang S, 2012, IEEE T SYST MAN CY B, V42, P1119, DOI 10.1109/TSMCB.2012.2187280; Wightman Linda F, 1998, LSAC NATL LONGITUDIN; Yan BW, 2018, PR MACH LEARN RES, V80; Zhou Xingyu, 2018, ARXIV180306573; Zinkevich M., 2003, P 20 INT C MACH LEAR, P928, DOI 10.5555/3041838	50	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902038
C	Naseer, M; Khan, S; Khan, MH; Khan, FS; Porikli, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Naseer, Muzammal; Khan, Salman; Khan, Muhammad Haris; Khan, Fahad Shahbaz; Porikli, Fatih			Cross-Domain Transferability of Adversarial Perturbations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings, where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods, whether learning an instance-specific or an instance-agnostic perturbation, is the direct or indirect reliance on the original domain-specific data distribution. In this work, for the first time, we demonstrate the existence of domain-invariant adversaries, thereby showing common adversarial space among different datasets and models. To this end, we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on entirely different domains. For instance, an adversarial function learned on Paintings, Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier, with success rates as high as similar to 99% (l(infinity) <= 10). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates, both under the white-box and black-box scenarios. Furthermore, despite being an instance-agnostic perturbation function, our attack outperforms the conventionally much stronger instance-specific attack methods.	[Naseer, Muzammal; Khan, Salman; Porikli, Fatih] Australian Natl Univ, Canberra, ACT, Australia; [Naseer, Muzammal; Khan, Salman; Khan, Muhammad Haris; Khan, Fahad Shahbaz] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Khan, Fahad Shahbaz] Linkoping Univ, Dept Elect Engn, CVL, Linkoping, Sweden	Australian National University; Linkoping University	Naseer, M (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.; Naseer, M (corresponding author), Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.	muzammal.naseer@anu.edu.au; salman.khan@inceptioniai.org; muhammad.haris@inceptioniai.org; fahad.khan@inceptioniai.org; fatih.porikli@anu.edu.au	Naseer, Muzammal/AAA-3446-2022; Khan, Fahad Shahbaz/ABD-6646-2021	Khan, Fahad Shahbaz/0000-0002-4263-3143				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2017, NEURIPS; [Anonymous], 2017, KAGGLE; Baluja S., 2017, ARXIV170309387; Dong Yinpeng, 2019, P IEEE COMP SOC C CO; Fawzi A., 2015, ARXIV150202590; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Geirhos R., 2019, IMAGENET TRAINED CNN; Goodfellow I. J., 2014, ARXIV14126572; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Johnson J, 2016, ECCV; Jolicoeur-Martineau Alexia, 2019, P INT C LEARN REPR I; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Kurakin A, 2016, INT C LEARN REPR SAN; Li Yingwei, 2019, ARXIV190400979; Liu Y., 2017, P 5 INT C LEARN REPR; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Mopuri Konda Reddy, 2017, P BRIT MACH VIS C 20, DOI DOI 10.5244/C.31.30; Mopuri Konda Reddy, 2018, ECCV; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Rajpurkar P., 2017, CORR; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Song Y., 2018, ADV NEURAL INFORM PR, P8312; Szegedy C., 2017, P 31 AAAI C ART INT, V4, P12; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy Christian, 2014, INT C LEARN REPR ICR; Trambr Florian, 2018, INT C LEARN REPR; Tramr F., 2017, ARXIV170403453; Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905; Xie C., 2018, CORR	35	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904053
C	Negrinho, R; Patil, D; Le, N; Ferreira, D; Gormley, MR; Gordon, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Negrinho, Renato; Patil, Darshan; Le, Nghia; Ferreira, Daniel; Gormley, Matthew R.; Gordon, Geoffrey			Towards modular and programmable architecture search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural architecture search methods are able to find high performance deep learning architectures with minimal effort from an expert [1]. However, current systems focus on specific use-cases (e.g. convolutional image classifiers and recurrent language models), making them unsuitable for general use-cases that an expert might wish to write. Hyperparameter optimization systems [2, 3, 4] are general-purpose but lack the constructs needed for easy application to architecture search. In this work, we propose a formal language for encoding search spaces over general computational graphs. The language constructs allow us to write modular, composable, and reusable search space encodings and to reason about search space design. We use our language to encode search spaces from the architecture search literature. The language allows us to decouple the implementations of the search space and the search algorithm, allowing us to expose search spaces to search algorithms through a consistent interface. Our experiments show the ease with which we can experiment with different combinations of search spaces and search algorithms without having to implement each combination from scratch. We release an implementation of our language with this paper(2).	[Negrinho, Renato; Patil, Darshan; Le, Nghia; Gormley, Matthew R.; Gordon, Geoffrey] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Ferreira, Daniel] TU Wien, Vienna, Austria; [Gordon, Geoffrey] Microsoft Res Montreal, Montreal, PQ, Canada; [Negrinho, Renato] Petuum, Pittsburgh, PA USA	Carnegie Mellon University; Technische Universitat Wien	Negrinho, R (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.				Google; NSF [IIS 1822831]	Google(Google Incorporated); NSF(National Science Foundation (NSF))	We thank the anonymous reviewers for helpful comments and suggestions. We thank Graham Neubig, Barnabas Poczos, Ruslan Salakhutdinov, Eric Xing, Xue Liu, Carolyn Rose, Zhiting Hu, Willie Neiswanger, Christoph Dann, Kirielle Singajarah, and Zejie Ai for helpful discussions. We thank Google for generous TPU and GCP grants. This work was funded in part by NSF grant IIS 1822831.	Abadi M., TENSORFLOW LARGE SCA; Bergstra J., 2013, HYPEROPT PYTHON LIB; Bergstra J, 2013, JMLR; Bergstra James, 2011, NEURIPS; Bergstra James, 2012, JMLR; Browne C. B., 2012, IEEE T COMPUTATIONAL; Falkner Stefan, 2018, ICML; Feurer M., 2015, NEURIPS; Hutter F, 2011, LION; Jin H., 2018, ARXIV180610282; Kandasamy Kirthevasan, 2018, NEURIPS; Li Lisha, 2017, JMLR; Lindauer Marius, 2017, SMACV3 ALGORITHM CON; Liu Chenxi, 2018, ECCV; Liu Hsueh-Ti Derek, 2018, ICLR; Negrinho R., 2017, ARXIV170408792; Olson Randal, 2016, WORKSH AUT MACH LEAR; Olson RS, 2016, EUR C APPL EV COMP; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pham Hieu, 2018, ICML; Real Esteban, 2019, AAAI; Shahriari Bobak, 2016, P IEEE; Snoek J., 2012, NEURIPS; Xie L., 2017, ICCV; Ying Chris, 2019, ICML; Zoph B., 2018, CVPR; Zoph B., 2017, ICLR	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905039
C	O'Donoghue, B; Maddison, CJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		O'Donoghue, Brendan; Maddison, Chris J.			Hamiltonian descent for composite objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION	In optimization the duality gap between the primal and the dual problems is a measure of the suboptimality of any primal-dual point. In classical mechanics the equations of motion of a system can be derived from the Hamiltonian function, which is a quantity that describes the total energy of the system. In this paper we consider a convex optimization problem consisting of the sum of two convex functions, sometimes referred to as a composite objective, and we identify the duality gap to be the 'energy' of the system. In the Hamiltonian formalism the energy is conserved, so we add a contractive term to the standard equations of motion so that this energy decreases linearly (i.e., geometrically) with time. This yields a continuous-time ordinary differential equation (ODE) in the primal and dual variables which converges to zero duality gap, i.e., optimality. This ODE has several useful properties: it induces a natural operator splitting; at convergence it yields both the primal and dual solutions; and it is invariant to affine transformation despite only using first order information. We provide several discretizations of this ODE, some of which are new algorithms and others correspond to known techniques, such as the alternating direction method of multipliers (ADMM). We conclude with some numerical examples that show the promise of our approach. We give an example where our technique can solve a convex quadratic minimization problem orders of magnitude faster than several commonly-used gradient methods, including conjugate gradient, when the conditioning of the problem is poor. Our framework provides new insights into previously known algorithms in the literature as well as providing a technique to generate new primal-dual algorithms.	[O'Donoghue, Brendan; Maddison, Chris J.] DeepMind, London, England; [Maddison, Chris J.] Univ Oxford, Oxford, England	University of Oxford	O'Donoghue, B (corresponding author), DeepMind, London, England.	bodonoghue@google.com; cmaddis@google.com						[Anonymous], 1991, APPL NONLINEAR CONTR; Attouch H, 2019, ESAIM CONTR OPTIM CA, V25, DOI 10.1051/cocv/2017083; Balduzzi D, 2018, PR MACH LEARN RES, V80; Bauschke H. H., 1997, J CONVEX ANAL, V4, P27; Betancourt M., 2018, ARXIV180203653; Bianchi Pascal, 2017, ARXIV170204144; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyd S, 2004, CONVEX OPTIMIZATION; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Condat L, 2013, J OPTIMIZ THEORY APP, V158, P460, DOI 10.1007/s10957-012-0245-9; Esser E., 2009, GEN FRAMEWORK CLASS; Franca G., 2018, INT C MACH LEARN INT C MACH LEARN, P1554; Franqa Guilherme, 2019, ARXIV190304100; Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219; Gronwall TH, 1918, ANN MATH, V20, P292; Hairer E., 2006, SPRINGER SERIES COMP, V31; He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936; Maddison Chris J, 2018, ARXIV180905042; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; O'Donoghue B, 2016, J OPTIMIZ THEORY APP, V169, P1042, DOI 10.1007/s10957-016-0892-3; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; ODonoghue B., 2017, SCS SPLITTING CONIC; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Peypouquet Juan, 2009, ARXIV09051270; Rockafellar RT, 1973, J OPTIMIZ THEORY APP, V12, P367, DOI 10.1007/BF00940418; Rockafellar R. T., 1970, CONVEX ANAL; Su WJ, 2016, J MACH LEARN RES, V17; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Wilson A., 2019, ARXIV190208825; Wilson A. C., 2016, ARXIV161102635; ZHU M, 2008, UCLA CAM REPORT, V34; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	36	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906016
C	Pai, M; Kumar, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pai, Meera; Kumar, Animesh			Distribution Learning of a Random Spatial Field with a Location-Unaware Mobile Sensor	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NETWORK	Measurement of spatial fields is of interest in environment monitoring. Recently mobile sensing has been proposed for spatial field reconstruction, which requires a smaller number of sensors when compared to the traditional paradigm of sensing with static sensors. A challenge in mobile sensing is to overcome the location uncertainty of its sensors. While GPS or other localization methods can reduce this uncertainty, we address a more fundamental question: can a location-unaware mobile sensor recording samples on a directed non-uniform random walk, learn the statistical distribution (as a function of space) of an underlying random process (spatial field)? The answer is in the affirmative for Lipschitz continuous fields, where the accuracy of our distribution-learning method increases with the number of observed field samples (sampling rate). To validate our distribution-learning method, we have created a dataset with 43 experimental trials by measuring sound-level along a fixed path using a location-unaware mobile sound-level meter.	[Pai, Meera; Kumar, Animesh] Indian Inst Technol, Elect Engn, Mumbai 400076, Maharashtra, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Bombay	Pai, M (corresponding author), Indian Inst Technol, Elect Engn, Mumbai 400076, Maharashtra, India.	meeravpai@ee.iitb.ac.in; animesh@ee.iitb.ac.in						Atakan B, 2014, IEEE T VEH TECHNOL, V63, P403, DOI 10.1109/TVT.2013.2271359; Che XH, 2009, IEEE INT CON DIS, P460, DOI 10.1109/ICDCSW.2009.36; Durrett R., 2010, CAMBRIDGE SERIES STA, V4th; Elhami G, 2018, IEEE T SIGNAL PROCES, V66, P5862, DOI 10.1109/TSP.2018.2872019; Grimmett G. R., 2001, PROBABILITY RANDOM P; Hu L., 2004, P 10 ANN INT C MOB C, P45; JERRI AJ, 1977, P IEEE, V65, P1565, DOI 10.1109/PROC.1977.10771; Kumar A, 2017, IEEE T INFORM THEORY, V63, P2188, DOI 10.1109/TIT.2017.2651878; Kumar A, 2016, IEEE INT SYMP INFO, P1257, DOI 10.1109/ISIT.2016.7541500; Kumar A, 2011, IEEE T INFORM THEORY, V57, P476, DOI 10.1109/TIT.2010.2090194; Kumar A, 2010, IEEE T SIGNAL PROCES, V58, P2654, DOI 10.1109/TSP.2010.2041276; Marco D, 2003, LECT NOTES COMPUT SC, V2634, P1; Morselli F, 2018, 2018 IEEE 29TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS (PIMRC), P1382, DOI 10.1109/PIMRC.2018.8580783; Pacholska M, 2017, 2017 INTERNATIONAL CONFERENCE ON SAMPLING THEORY AND APPLICATIONS (SAMPTA), P364, DOI 10.1109/SAMPTA.2017.8024451; Unnikrishnan J, 2013, IEEE T SIGNAL PROCES, V61, P2328, DOI 10.1109/TSP.2013.2247599; Wang YC, 2017, IEEE T VEH TECHNOL, V66, P7234, DOI 10.1109/TVT.2017.2655084	16	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904016
C	Richards, D; Rebeschini, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Richards, Dominic; Rebeschini, Patrick			Optimal Statistical Rates for Decentralised Non-Parametric Regression with Linear Speed-Up	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION; CONSENSUS; ALGORITHM	We analyse the learning performance of Distributed Gradient Descent in the context of multi-agent decentralised non-parametric regression with the square loss function when i.i.d. samples are assigned to agents. We show that if agents hold sufficiently many samples with respect to the network size, then Distributed Gradient Descent achieves optimal statistical rates with a number of iterations that scales, up to a threshold, with the inverse of the spectral gap of the gossip matrix divided by the number of samples owned by each agent raised to a problem-dependent power. The presence of the threshold comes from statistics. It encodes the existence of a "big data" regime where the number of required iterations does not depend on the network topology. In this regime, Distributed Gradient Descent achieves optimal statistical rates with the same order of iterations as gradient descent run with all the samples in the network. Provided the communication delay is sufficiently small, the distributed protocol yields a linear speed-up in runtime compared to the single-machine protocol. This is in contrast to decentralised optimisation algorithms that do not exploit statistics and only yield a linear speed-up in graphs where the spectral gap is bounded away from zero. Our results exploit the statistical concentration of quantities held by agents and shed new light on the interplay between statistics and communication in decentralised methods. Bounds are given in the standard non-parametric setting with source/capacity assumptions.	[Richards, Dominic; Rebeschini, Patrick] Univ Oxford, Dept Stat, 24-29 St Giles, Oxford OX1 3LB, England	University of Oxford	Richards, D (corresponding author), Univ Oxford, Dept Stat, 24-29 St Giles, Oxford OX1 3LB, England.	dominic.richards@spc.ox.ac.uk; patrick.rebeschini@stats.ox.ac.uk		Rebeschini, Patrick/0000-0001-7772-4160	EPSRC through the OxWaSP CDT programme [EP/L016710/1]; MRC through the OxWaSP CDT programme [EP/L016710/1]; Alan Turing Institute under the EPSRC grant [EP/N510129/1]	EPSRC through the OxWaSP CDT programme(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); MRC through the OxWaSP CDT programme(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); Alan Turing Institute under the EPSRC grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Dominic Richards is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1). Patrick Rebeschini is supported in part by the Alan Turing Institute under the EPSRC grant EP/N510129/1. We would like to thank Francis Bach, Lorenzo Rosasco and Alessandro Rudi for helpful discussions.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; [Anonymous], 2018, ADV NEURAL INFORM PR; [Anonymous], 2015, ADV NEURAL INFORM PR; Arjevani Y., 2015, ADV NEURAL INFORM PR, V28, P1756; Assran M, 2018, STOCHASTIC GRADIENT; Benezit F, 2010, IEEE T INFORM THEORY, V56, P5150, DOI 10.1109/TIT.2010.2060050; Berthier Raphael, 2018, ARXIV180508531; Bijral AS, 2017, IEEE T AUTOMAT CONTR, V62, P4483, DOI 10.1109/TAC.2017.2671377; Blanchard G, 2018, FOUND COMPUT MATH, V18, P971, DOI 10.1007/s10208-017-9359-7; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Cao M., 2006, 44 ANN ALLERTON C CO, P952; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Carratino L., 2018, ADV NEURAL INFORM PR, P10212; Cross V, 2009, CLIN OPHTHALMOL, V3, P1; Dieuleveut A., 2017, J MACHINE LEARNING R, V18, P3520; Dimakis ADG, 2008, IEEE T SIGNAL PROCES, V56, P1205, DOI 10.1109/TSP.2007.908946; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Fan RK., 1997, SPECTRAL GRAPH THEOR, V92; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Guo ZC, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/aa72b2; Johansson B, 2009, SIAM J OPTIMIZ, V20, P1157, DOI 10.1137/08073038X; Lian X, 2017, P ADV NEUR INF PROC, P5330; Lin J., 2017, J MACH LEARN RES, V18, P1; Lin Junhong, 2018, ARXIV180107226; Lin SB., 2017, J MACH LEARN RES, V18, P3202; Lobel I, 2011, IEEE T AUTOMAT CONTR, V56, P1291, DOI 10.1109/TAC.2010.2091295; Matei I, 2011, IEEE J-STSP, V5, P754, DOI 10.1109/JSTSP.2011.2120593; Mokhtari A, 2016, J MACH LEARN RES, V17; Mucke N, 2018, J MACH LEARN RES, V19; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P2506, DOI 10.1109/TAC.2009.2031203; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Olsson T, 2007, IEEE INT CONF ROBOT, P4770, DOI 10.1109/ROBOT.2007.364214; Peres Yuval, 2017, MARKOV CHAINS MIXING, V107; Pillaud-Vivien Loucas, 2018, ADV NEURAL INFORM PR, P8114; PINELIS IF, 1986, THEOR PROBAB APPL+, V30, P143, DOI 10.1137/1130013; Rabbat M, 2015, 2015 IEEE 6TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP), P517, DOI 10.1109/CAMSAP.2015.7383850; Richards D., 2018, GRAPH DEPENDENT IMPL; Robbins H., 1985, H ROBBINS SELECTED P, P102, DOI DOI 10.1007/978-1-4612-5110-1_9; Sayed AH, 2014, P IEEE, V102, P460, DOI 10.1109/JPROC.2014.2306253; Scaman K, 2017, PR MACH LEARN RES, V70; Shamir O., 2014, ADV NEURAL INFORM PR, P163; Shamir O, 2014, ANN ALLERTON CONF, P850, DOI 10.1109/ALLERTON.2014.7028543; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Steinwart I., 2008, SUPPORT VECTOR MACHI; Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531; Tsianos K., 2012, P ADV NEUR INF PROC, V25, P1943; Tsianos KI, 2016, IEEE T SIGNAL INF PR, V2, P489, DOI 10.1109/TSIPN.2016.2620440; Tsitsiklis J. N, 1984, PROBLEMS DECENTRALIZ; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Xiao L, 2004, SYST CONTROL LETT, V53, P65, DOI 10.1016/j.sysconle.2004.02.022; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008; Zhang Y., 2012, ADV NEURAL INFORM PR, P1502; Zhang YC, 2015, J MACH LEARN RES, V16, P3299	62	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301024
C	Rooshenas, A; Zhang, DX; Sharma, G; McCallum, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rooshenas, Amirmohammad; Zhang, Dongxu; Sharma, Gopal; McCallum, Andrew			Search-Guided, Lightly-Supervised Training of Structured Prediction Energy Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In structured output prediction tasks, labeling ground-truth training output is often expensive. However, for many tasks, even when the true output is unknown, we can evaluate predictions using a scalar reward function, which may be easily assembled from human knowledge or non-differentiable pipelines. But searching through the entire output space to find the best output with respect to this reward function is typically intractable. In this paper, we instead use efficient truncated randomized search in this reward function to train structured prediction energy networks (SPENs), which provide efficient test-time inference using gradient-based search on a smooth, learned representation of the score landscape, and have previously yielded state-of-the-art results in structured prediction. In particular, this truncated randomized search in the reward function yields previously unknown local improvements, providing effective supervision to SPENs, avoiding their traditional need for labeled training data.	[Rooshenas, Amirmohammad; Zhang, Dongxu; Sharma, Gopal; McCallum, Andrew] Univ Massachusetts, Coll Informat Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Rooshenas, A (corresponding author), Univ Massachusetts, Coll Informat Comp Sci, Amherst, MA 01003 USA.	pedram@cs.umass.edu; dongxuzhang@cs.umass.edu; gopalsharma@cs.umass.edu; mccallum@cs.umass.edu			DARPA [FA8750-17-C-0106]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This research was funded by DARPA grant FA8750-17-C-0106. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government.	Bahdanau Dzmitry, 2017, 5 INT C LEARN REPR I; Belanger D., 2017, THESIS; Belanger D, 2017, PR MACH LEARN RES, V70; Belanger D, 2016, PR MACH LEARN RES, V48; Chang KW, 2015, PR MACH LEARN RES, V37, P2058; Chang M.-W., 2010, P INT C MACH LEARN; Chang Ming-Wei, 2007, ACL, P280; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Gygli M, 2017, PR MACH LEARN RES, V70; Ho WSW., 1992, MEMBRANE HDB, DOI [10.1007/978-1-4615-3548-5, DOI 10.1007/978]; Hoang Cong Duy Vu, 2017, P 2017 C EMP METH NA, P146; Hu Z., 2016, P C EMP METH NAT LAN, P1670, DOI DOI 10.18653/V1/D16-1173; Huang Liang, 2012, P C N AM CHAPT ASS C, P142; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Langford J., 2018, P INT C LEARN REPR; LeCun Yann, 2006, PREDICTING STRUCTURE, P2; Liang Percy, 2009, P 26 ANN INT C MACH, P641; LWick Michael, 2011, P 28 INT C MACH LEAR, P777; Maes F, 2009, MACH LEARN, V77, P271, DOI 10.1007/s10994-009-5140-8; Mann GS, 2010, J MACH LEARN RES, V11, P955; Norouzi M, 2016, ADV NEUR IN, V29; Peng Haoruo, 2017, P 2017 C EMP METH NA; Ranzato M, 2016, ICLR; Rooshenas A., 2018, P N AM CHAPT ASS COM; Seymore K., 1999, AAAI 99 WORKSHOP MAC, P37; Sharma G, 2018, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2018.00578; Stewart R., 2017, AAAI, P2576	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905022
C	Russel, RH; Petrik, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Russel, Reazul Hasan; Petrik, Marek			Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MARKOV DECISION-PROCESSES	Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set-the set of plausible transition probabilities-which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge, our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method, and the empirical results demonstrate its practical promise.	[Russel, Reazul Hasan; Petrik, Marek] Univ New Hampshire, Dept Comp Sci, Durham, NH 03824 USA	University System Of New Hampshire; University of New Hampshire	Russel, RH (corresponding author), Univ New Hampshire, Dept Comp Sci, Durham, NH 03824 USA.	rrussel@cs.unh.edu; mpetrik@cs.unh.edu			NSF [1815275, 1717368]	NSF(National Science Foundation (NSF))	We would like to thank Vishal Gupta and the anonymous referees for their insightful comments and suggestions. This work was supported by NSF under grants number 1815275 and 1717368.	Bagnell J. A., 2001, C MELLON RES SHOWCAS, P948; BenTal A, 2009, PRINC SER APPL MATH, P1; Bertsimas D., 2017, DATA DRIVEN ROBUST O; Boyd S, 2004, CONVEX OPTIMIZATION; Castro PS, 2010, LECT NOTES ARTIF INT, V6321, P200, DOI 10.1007/978-3-642-15880-3_19; Delage E, 2010, OPER RES, V58, P203, DOI 10.1287/opre.1080.0685; Delgado KV, 2016, ARTIF INTELL, V230, P192, DOI 10.1016/j.artint.2015.09.005; Dietterich T. G., 2013, AAAI; Gelman A., 2021, BAYESIAN DATA ANAL; Goyal V., 2018, TECHNICAL REPORT; Gupta V, 2015, NEAR OPTIMAL BAYESIA; Hanasusanto Grani A, 2013, ADV NEURAL INFORM PR; Ho C. P., 2018, INT C MACH LEARN ICM, P1979; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jiang Nan, 2015, INT C MACH LEARN ICM; Kalyanasundaram S, 2002, IEEE DECIS CONTR P, P3799; Lange S, 2012, ADAPT LEARN OPTIM, V12, P45; Laroche R., 2018, SAFE POLICY IMPROVEM; Le Tallec Y, 2007, THESIS; Li L., 2015, INT C ART INT STAT A; MANNOR S., 2012, P 29 INT C MACH LEAR; Mannor S, 2016, MATH OPER RES, V41, P1484, DOI 10.1287/moor.2016.0786; Munos R., 2016, C NEUR INF PROC SYST; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Petrik M., 2012, INT C MACH LEARN ICM, P207; Petrik M., 2014, NEURAL INFORM PROCES; Petrik M., 2016, ICML WORKSH REL MACH, P1; Petrik M., 2016, ADV NEURAL INFORM PR; Puterman M. L., 1994, MARKOV DECISION PROC; Shapiro A, 2014, LECT STOCHASTIC PROG; Strehl A. L., 2007, THESIS; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Taleghan MA, 2015, J MACH LEARN RES, V16, P3877; Tamar A., 2014, INT C MACH LEARN ICM; Thomas P. S., 2016, INT C MACH LEARN ICM; Thomas P. S., 2015, ANN C AAAI; Tirinzoni A., 2018, NEURAL INFORM PROCES; Weissman T., 2003, INEQUALITIES L1 DEVI; Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566; Xu H., 2006, ADV NEURAL INFORM PR; Xu H, 2009, IEEE DECIS CONTR P, P3606, DOI 10.1109/CDC.2009.5400796	46	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307010
C	Sabato, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sabato, Sivan			epsilon-Best-Arm Identification in Pay-Per-Reward Multi-Armed Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EXPLORATION; COMPLEXITY	We study epsilon-best-arm identification, in a setting where during the exploration phase, the cost of each arm pull is proportional to the expected future reward of that arm. We term this setting Pay-Per-Reward. We provide an algorithm for this setting, that with a high probability returns an epsilon-best arm, while incurring a cost that depends only linearly on the total expected reward of all arms, and does not depend at all on the number of arms. Under mild assumptions, the algorithm can be applied also to problems with infinitely many arms.	[Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, IL-8410501 Beer Sheva, Israel	Ben Gurion University	Sabato, S (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, IL-8410501 Beer Sheva, Israel.	sabatos@cs.bgu.ac.il		Sabato, Sivan/0000-0002-7975-0044	Israel Science Foundation [555/15]	Israel Science Foundation(Israel Science Foundation)	Sivan Sabato was supported in part by the Israel Science Foundation (grant No. 555/15).	Agrawal D, 2009, IEEE SARNOFF SYMPOS, P93; Audibert Jean-Yves, 2010, COLT 23 C LEARN THEO, P13; Aziz M., 2018, ARXIV180304665; BANKS JS, 1992, ECONOMETRICA, V60, P1071, DOI 10.2307/2951539; Berry DA, 1997, ANN STAT, V25, P2103, DOI 10.1214/aos/1069362389; Brownlees C, 2015, ANN STAT, V43, P2507, DOI 10.1214/15-AOS1350; Carpentier A., 2016, C LEARN THEOR, P590; Carpentier A, 2015, PR MACH LEARN RES, V37, P1133; Chen L., 2017, P 30 COLT, V65, P535; Ding W., 2013, 27 AAAI C ART INT; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Hsu D, 2016, J MACH LEARN RES, V17; Hu JH, 2017, ACTA POLYM SIN, P811, DOI 10.11777/j.issn1000-3304.2017.16269; Joly E, 2017, ELECTRON J STAT, V11, P440, DOI 10.1214/17-EJS1228; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Li H., 2017, 31 AAAI C ART INT; Lugosi G., 2016, ARXIV160800757; Mannor S, 2004, J MACH LEARN RES, V5, P623; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; Pandey S, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P216; Tran-Thanh L., 2012, 26 AAAI C ART INT; Tran-Thanh L., 2010, 24 AAAI C ART INT; Vanchinathan HP, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1195, DOI 10.1145/2783258.2783360; Wang Y., 2009, ADV NEURAL INFORM PR, P1729; Xia Y., 2016, PROC 25 INT JOINT C, P2210; Xia YC, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P758; Yu XT, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P937	32	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302083
C	Sarpatwar, KK; Shanmugam, K; Ganapavarapu, VS; Jagmohan, A; Vaculin, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sarpatwar, Kanthi K.; Shanmugam, Karthikeyan; Ganapavarapu, Venkata Sitaramagiridharganesh; Jagmohan, Ashish; Vaculin, Roman			Differentially Private Distributed Data Summarization under Covariate Shift	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We envision Artificial Intelligence marketplaces to be platforms where consumers, with very less data for a target task, can obtain a relevant model by accessing many private data sources with vast number of data samples. One of the key challenges is to construct a training dataset that matches a target task without compromising on privacy of the data sources. To this end, we consider the following distributed data summarizataion problem. Given K private source datasets denoted by [D-i](i is an element of[K]) and a small target validation set D-nu which may involve a considerable covariate shift with respect to the sources, compute a summary dataset D-s subset of boolean OR(i is an element of[K]) D-i such that its statistical distance from the validation dataset D-nu is minimized. We use the popular Maximum Mean Discrepancy as the measure of statistical distance. The non-private problem has received considerable attention in prior art, for example in prototype selection (Kim et al., NIPS 2016). Our work is the first to obtain strong differential privacy guarantees while ensuring the quality guarantees of the non-private version. We study this problem in a Parsimonious Curator Privacy Model, where a trusted curator coordinates the summarization process while minimizing the amount of private information accessed. Our central result is a novel protocol that (a) ensures the curator accesses at most O(K-1/3 vertical bar D vertical bar + vertical bar D-nu vertical bar) points (b) has formal privacy guarantees on the leakage of information between the data owners and (c) closely matches the best known non-private greedy algorithm. Our protocol uses two hash functions, one inspired by the Rahimi-Recht random features method and the second leverages state of the art differential privacy mechanisms. Further, we introduce a novel "noiseless" differentially private auctioning protocol for winner notification, which may be of independent interest. Apart from theoretical guarantees, we demonstrate the efficacy of our protocol using real-world datasets.	[Sarpatwar, Kanthi K.; Ganapavarapu, Venkata Sitaramagiridharganesh; Jagmohan, Ashish; Vaculin, Roman] IBM Res, Yorktown Hts, NY 10598 USA; [Shanmugam, Karthikeyan] IBM Res AI, Yorktown Hts, NY USA	International Business Machines (IBM)	Sarpatwar, KK (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	sarpatwa@us.ibm.com; karthikeyan.shanmugan2@ibm.com; giridhar.ganapavarapu@ibm.com; ashishja@us.ibm.com; vaculin@us.ibm.com						Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; [Anonymous], 2009, DIRECTIONAL STAT; [Anonymous], 2017, ARXIV170704766; [Anonymous], 2014, ABS14123474 CORR; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork Cynthia, 2014, SIGACT SIGGRAPH S CO, P261; Ganin Yaroslav, 2014, ARXIV14097495; Giraud B. G., 2015, ARXIV150902373; Gottlieb J., 2017, FUELING GROWTH DATA; Gretton A., 2008, ABS08052368 CORR; Hamm J, 2016, PR MACH LEARN RES, V48; Hardt M, 2012, P 26 ANN C NEUR INF, P2339; Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85; Jukna S, 2011, EXTREMAL COMBINATORI, VSecond; Kaggle, 2014, ALLST PURCH PRED CHA; Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Kifer D., 2012, C LEARN THEOR, P25; Kim B., 2016, ADV NEURAL INFORM PR, P2280; McMahan H. Brendan, 2016, ARXIV160205629; Mitrovic M, 2017, PR MACH LEARN RES, V70; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Papernot N., 2017, ICLR; Pathak M., 2010, ADV NEURAL INFORM PR, P1876; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rubinstein B. I. P., 2012, J PRIVACY CONFIDENTI, V4, P65, DOI DOI 10.29012/JPC.V4I1.612; Sarpatwar K.K., 2019, IEEE C COMP VIS PATT; Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687; Talwar K., 2015, NIPS 2015 P 28 INT C, V2, P3025; Thakurta Abhradeep Guha, 2013, DIFFERENTIALLY PRIVA; Wu Xi, 2016, ARXIV160604722	35	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906015
C	Scetbon, M; Varoquaux, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Scetbon, Meyer; Varoquaux, Gael			Comparing distributions: l(1) geometry improves kernel two-sample testing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Are two sets of observations drawn from the same distribution? This problem is a two-sample test. Kernel methods lead to many appealing properties. Indeed state-of-the-art approaches use the L-2 distance between kernel-based distribution representatives to derive their test statistics. Here, we show that L-p distances (with p >= 1) between these distribution representatives give metrics on the space of distributions that are well-behaved to detect differences between distributions as they metrize the weak convergence. Moreover, for analytic kernels, we show that the L-1 geometry gives improved testing power for scalable computational procedures. Specifically, we derive a finite dimensional approximation of the metric given as the l(1) norm of a vector which captures differences of expectations of analytic functions evaluated at spatial locations or frequencies (i.e, features). The features can be chosen to maximize the differences of the distributions and give interpretable indications of how they differs. Using an l(1) norm gives better detection because differences between representatives are dense as we use analytic kernels (non-zero almost everywhere). The tests are consistent, while much faster than state-of-the-art quadratic-time kernel-based tests. Experiments on artificial and real-world problems demonstrate improved power/time tradeoff than the state of the art, based on l(2) norms, and in some cases, better outright power than even the most expensive quadratic-time tests.	[Scetbon, Meyer] ENSAE, CREST, Palaiseau, France; [Scetbon, Meyer; Varoquaux, Gael] Univ Paris Saclay, INRIA, St Aubin, France	Institut Polytechnique de Paris; Inria; UDICE-French Research Universities; Universite Paris Saclay	Scetbon, M (corresponding author), ENSAE, CREST, Palaiseau, France.; Scetbon, M (corresponding author), Univ Paris Saclay, INRIA, St Aubin, France.				DirtyDATA ANR grant [ANR-17-CE23-0018]	DirtyDATA ANR grant(French National Research Agency (ANR))	This work was funded by the DirtyDATA ANR grant (ANR-17-CE23-0018). We also would like to thank Zoltn Szab6 from Ecole Polytechnique for crucial suggestions, and acknowledge hardware donations from NVIDIA Corporation.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chwialkowski K.P., 2015, P 28 INT C NEUR INF, P1981; Cucker F, 2002, B AM MATH SOC, V39, P1; DEVROYE L, 1985, NONPARAMETRIC DENSIT; Dharmawansa P, 2007, IEEE T COMMUN, V55, P1407, DOI 10.1109/TCOMM.2007.900621; Fromont M., 2012, C LEARN THEOR, P23; FUKUMIZU K, 2008, ADV NEURAL INFORM PR, V20, P489; Gretton A, 2012, ADV NEURAL INF PROCE; Gretton A, 2012, J MACH LEARN RES, V13, P723; Harchaoui Z., 2008, ADV NEURAL INFORM PR, P609; Huggins JH, 2018, ADV NEUR IN, V31; Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181; Lang Ken, 1995, MACHINE LEARNING P; Le Quoc V., 2013, INT C MACH LEARN, P244; Lichman M, 2013, UCI MACHINE LEARNING; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Ramdas A, 2015, AAAI CONF ARTIF INTE, P3571; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Simon B., 2005, AM MATH SOC; Simon-Gabriel C.-J., 2016, ARXIV160405251; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Szekely G.J., 2004, INTERSTAT, V5, P1249; ZAREMBA W, 2013, ADV NEURAL INFORM PR, V26, P755; Zhu C., 2019, 190207279 ARXIV	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904002
C	Scheidegger, F; Benini, L; Bekas, C; Malossi, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Scheidegger, Florian; Benini, Luca; Bekas, Costas; Malossi, Cristiano			Constrained deep neural network architecture search for IoT devices accounting for hardware calibration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep neural networks achieve outstanding results for challenging image classification tasks. However, the design of network topologies is a complex task, and the research community is conducting ongoing efforts to discover top-accuracy topologies, either manually or by employing expensive architecture searches. We propose a unique narrow-space architecture search that focuses on delivering low-cost and rapidly executing networks that respect strict memory and time requirements typical of Internet-of-Things (IoT) near-sensor computing platforms. Our approach provides solutions with classification latencies below 10 ms running on a low-cost device with 1 GB RAM and a peak performance of 5.6 GFLOPS. The narrow-space search of floating-point models improves the accuracy on CIFAR10 of an established IoT model from 70.64% to 74.87% within the same memory constraints. We further improve the accuracy to 82.07% by including 16-bit half types and obtain the highest accuracy of 83.45% by extending the search with model-optimized IEEE 754 reduced types. To the best of our knowledge, this is the first empirical demonstration of more than 3000 trained models that run with reduced precision and push the Pareto optimal front by a wide margin. Within a given memory constraint, accuracy is improved by more than 7% points for half and more than 1% points for the best individual model format.	[Scheidegger, Florian; Benini, Luca] Swiss Fed Inst Technol, Ramistr 101, CH-8092 Zurich, Switzerland; [Scheidegger, Florian; Bekas, Costas; Malossi, Cristiano] IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland; [Benini, Luca] Univ Bologna, Via Zamboni 33, I-40126 Bologna, Italy	Swiss Federal Institutes of Technology Domain; ETH Zurich; International Business Machines (IBM); University of Bologna	Scheidegger, F (corresponding author), Swiss Fed Inst Technol, Ramistr 101, CH-8092 Zurich, Switzerland.; Scheidegger, F (corresponding author), IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.			Scheidegger, Florian/0000-0003-0430-3634; Malossi, Cristiano/0000-0002-8201-1533	European Union's H2020 research and innovation program, project OPRECOMP [732631]	European Union's H2020 research and innovation program, project OPRECOMP	This work was funded by the European Union's H2020 research and innovation program under grant agreement No 732631, project OPRECOMP.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2019, IEEE STD 1619 2018 R, P1, DOI [10.1109/IEEESTD.2019.8766229, DOI 10.1109/IEEESTD.2019.8766229]; Ashiquzzaman A, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (ICAIIC 2019), P82, DOI 10.1109/ICAIIC.2019.8669031; Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cavigelli L., 2018, CORR; Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Conti F, 2016, J SIGNAL PROCESS SYS, V84, P339, DOI 10.1007/s11265-015-1070-9; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fenza G, 2019, IEEE ACCESS, V7, P9645, DOI 10.1109/ACCESS.2019.2891315; Flegar G., 2019, FLOAT X C LIB CUSTOM; Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906; Gaber MM, 2019, WIRES DATA MIN KNOWL, V9, DOI 10.1002/widm.1292; Goldberg David E, 1991, FDN GENETIC ALGORITH, DOI DOI 10.1016/B978-0-08-050684-5.50008-2; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, IEEE C COMP VIS REC; Hill P., 2018, CORR; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Istrate R., 2019, TAPAS TRAIN LESS ACC; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li WJ, 2020, IEEE INTERNET THINGS, V7, P5882, DOI 10.1109/JIOT.2019.2949352; Loroch D. M., 2017, P MLHPC; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Mittal S, 2019, J SYST ARCHITECT, V99, DOI 10.1016/j.sysarc.2019.101635; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Nordrum A, 2016, IEEE SPECTRUM, V53, P12, DOI 10.1109/MSPEC.2016.7572524; Pedapati, 2019, ARXIV190501392; Pham H, 2018, PR MACH LEARN RES, V80; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Real E, 2017, PR MACH LEARN RES, V70; Rybalkin V, 2017, DES AUT TEST EUROPE, P1390, DOI 10.23919/DATE.2017.7927210; Scheidegger F., 2019, EFFICIENT IMAGE DATA; Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395; Stella X Yu, 2003, ICCV, V2, P313; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tan M., 2018, CORR; Weng Y, 2019, IEEE ACCESS, V7, P38495, DOI 10.1109/ACCESS.2019.2906369; Wistuba M., 2018, JOINT EUR C MACH LEA, P243; Wu B., 2018, CORR; Xiao H., 2017, FASHION MNIST NOVEL; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zoph B., 2016, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	53	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306010
C	Shah, NP; Madugula, S; Hottowy, P; Sher, A; Litke, A; Paninski, L; Chichilnisky, EJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shah, Nishal P.; Madugula, Sasidhar; Hottowy, Pawel; Sher, Alexander; Litke, Alan; Paninski, Liam; Chichilnisky, E. J.			Efficient characterization of electrically evoked responses for neural interfaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IN-VIVO; MOTION; BRAIN	Future neural interfaces will read and write population neural activity with high spatial and temporal resolution, for diverse applications. For example, an artificial retina may restore vision to the blind by electrically stimulating retinal ganglion cells. Such devices must tune their function, based on stimulating and recording, to match the function of the circuit. However, existing methods for characterizing the neural interface scale poorly with the number of electrodes, limiting their practical applicability. This work tests the idea that using prior information from previous experiments and closed-loop measurements may greatly increase the efficiency of the neural interface. Large-scale, high-density electrical recording and stimulation in primate retina were used as a lab prototype for an artificial retina. Three key calibration steps were optimized: spike sorting in the presence of stimulation artifacts, response modeling, and adaptive stimulation. For spike sorting, exploiting the similarity of electrical artifact across electrodes and experiments substantially reduced the number of required measurements. For response modeling, a joint model that captures the inverse relationship between recorded spike amplitude and electrical stimulation threshold from previously recorded retinas resulted in greater consistency and efficiency. For adaptive stimulation, choosing which electrodes to stimulate based on probability estimates from previous measurements improved efficiency. Similar improvements resulted from using either non-adaptive stimulation with a joint model across cells, or adaptive stimulation with an independent model for each cell. Finally, image reconstruction revealed that these improvements may translate to improved performance of an artificial retina.	[Shah, Nishal P.; Madugula, Sasidhar; Chichilnisky, E. J.] Stanford Univ, Stanford, CA 94305 USA; [Hottowy, Pawel] AGH Univ Sci & Technol, Krakow, Poland; [Sher, Alexander; Litke, Alan] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA; [Paninski, Liam] Columbia Univ, New York, NY 10027 USA	Stanford University; AGH University of Science & Technology; University of California System; University of California Santa Cruz; Columbia University	Shah, NP (corresponding author), Stanford Univ, Stanford, CA 94305 USA.		Shah, Nishal/AAX-5999-2020	Shah, Nishal/0000-0002-1275-0381; Chichilnisky, E.J./0000-0002-5613-0248; Sher, Alexander/0000-0001-6655-6456				Atkinson A., 2007, OPTIMUM EXPT DESIGNS, V34; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Dragas J, 2017, IEEE J SOLID-ST CIRC, V52, P1576, DOI 10.1109/JSSC.2017.2686580; Fan V. H., 2018, J NEURAL ENG; Frechette ES, 2005, J NEUROPHYSIOL, V94, P119, DOI 10.1152/jn.01175.2004; Humayun MS, 2012, OPHTHALMOLOGY, V119, P779, DOI 10.1016/j.ophtha.2011.09.028; Jepson LH, 2013, J NEUROSCI, V33, P7194, DOI 10.1523/JNEUROSCI.4967-12.2013; Jun JJ, 2017, NATURE, V551, P232, DOI 10.1038/nature24636; Kerr JND, 2008, NAT REV NEUROSCI, V9, P195, DOI 10.1038/nrn2338; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kipke DR, 2008, J NEUROSCI, V28, P11830, DOI 10.1523/JNEUROSCI.3879-08.2008; Lewi J, 2009, NEURAL COMPUT, V21, P619, DOI 10.1162/neco.2008.08-07-594; Litke AM, 2004, IEEE T NUCL SCI, V51, P1434, DOI 10.1109/TNS.2004.832706; Lorach H, 2015, NAT MED, V21, P476, DOI 10.1038/nm.3851; Madugula S., 2017, TEATC; Mena GE, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005842; O'Shea DJ, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aaa365; Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0; Parthasarathy N., 2017, NEURAL NETWORKS EFFI, P6434; Richard E., 2015, ADV NEURAL INF PROCE, P2476; Salas MA, 2018, ELIFE, V7, DOI 10.7554/eLife.32904; SALZMAN CD, 1990, NATURE, V346, P174, DOI 10.1038/346174a0; Schwartz AB, 2004, ANNU REV NEUROSCI, V27, P487, DOI 10.1146/annurev.neuro.27.070203.144233; Shababo B., 2013, ADV NEURAL INFORM PR, V26, P1304; Shah N. P, 2019, INT IEEE EMBS C NEUR; Stanley GB, 1999, J NEUROSCI, V19, P8036, DOI 10.1523/jneurosci.19-18-08036.1999; Stingl K, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2013.0077; Stosiek C, 2003, P NATL ACAD SCI USA, V100, P7319, DOI 10.1073/pnas.1232232100; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Warland DK, 1997, J NEUROPHYSIOL, V78, P2336, DOI 10.1152/jn.1997.78.5.2336; WILSON BS, 1991, NATURE, V352, P236, DOI 10.1038/352236a0	32	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													15	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906014
C	Simchi-Levi, D; Xu, YZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Simchi-Levi, David; Xu, Yunzong			Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the classical stochastic multi-armed bandit problem with a constraint on the total cost incurred by switching between actions. Under the unit switching cost structure, where the constraint limits the total number of switches, we prove matching upper and lower bounds on regret and provide near-optimal algorithms for this problem. Surprisingly, we discover phase transitions and cyclic phenomena of the optimal regret. That is, we show that associated with the multi-armed bandit problem, there are equal-length phases defined by the number of arms and switching costs, where the regret upper and lower bounds in each phase remain the same and drop significantly between phases. The results enable us to fully characterize the trade-off between regret and incurred switching cost in the stochastic multi-armed bandit problem, contributing new insights to this fundamental problem. Under the general switching cost structure, our analysis reveals a surprising connection between the bandit problem and the shortest Hamiltonian path problem.	[Simchi-Levi, David; Xu, Yunzong] MIT, Inst Data Syst & Soc, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Simchi-Levi, D (corresponding author), MIT, Inst Data Syst & Soc, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	dslevi@mit.edu; yxu@mit.edu						AGRAWAL R, 1988, IEEE T AUTOMAT CONTR, V33, P899, DOI 10.1109/9.7243; Agrawal R., 1990, Stochastics and Stochastics Reports, V29, P437, DOI 10.1080/17442509008833627; Altschuler J., 2018, C LEARN THEOR, P1569; Asawa M, 1996, IEEE T AUTOMAT CONTR, V41, P328, DOI 10.1109/9.486316; BANKS JS, 1994, ECONOMETRICA, V62, P687, DOI 10.2307/2951664; Bergemann D, 2001, J ECON DYN CONTROL, V25, P1585, DOI 10.1016/S0165-1889(99)00064-0; Brezzi M, 2002, J ECON DYN CONTROL, V27, P87, DOI 10.1016/S0165-1889(01)00028-8; Cesa-Bianchi N., 2013, P 26 INT C NEUR INF, P1160; Chen B., 2019, IISE TRANS, P1; Gao Z., 2019, ARXIV190401763; Jun TS, 2004, ECONOMIST-NETHERLAND, V152, P513, DOI 10.1007/s10645-004-2477-z; Perchet V, 2016, ANN STAT, V44, P660, DOI 10.1214/15-AOS1381; Simchi-Levi D., 2019, ARXIV190510825	14	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307053
C	Sobolev, A; Vetrov, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sobolev, Artem; Vetrov, Dmitry			Importance Weighted Hierarchical Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Variational Inference is a powerful tool in the Bayesian modeling toolkit, however, its effectiveness is determined by the expressivity of the utilized variational distributions in terms of their ability to match the true posterior distribution. In turn, the expressivity of the variational family is largely limited by the requirement of having a tractable density function. To overcome this roadblock, we introduce a new family of variational upper bounds on a log marginal density in the case of hierarchical models (also known as latent variable models). We then derive a family of increasingly tighter variational lower bounds on the otherwise intractable standard evidence lower bound for hierarchical variational distributions, enabling the use of more expressive approximate posteriors. We show that previously known methods, such as Hierarchical Variational Models, Semi-Implicit Variational Inference and Doubly Semi-Implicit Variational Inference can be seen as special cases of the proposed approach, and empirically demonstrate superior performance of the proposed method in a set of experiments.	[Sobolev, Artem; Vetrov, Dmitry] Samsung AI Ctr Moscow, Moscow, Russia; [Vetrov, Dmitry] NRU HSE, Moscow, Russia; [Vetrov, Dmitry] Natl Res Univ Higher Sch Econ, Samsung HSE Lab, Moscow, Russia	HSE University (National Research University Higher School of Economics); HSE University (National Research University Higher School of Economics)	Sobolev, A (corresponding author), Samsung AI Ctr Moscow, Moscow, Russia.	asobolev@bayesgroup.ru			Russian Science Foundation [17-71-20072]	Russian Science Foundation(Russian Science Foundation (RSF))	Authors would like to thank Aibek Alanov, Dmitry Molchanov, Oleg Ivanov and Anonymous Reviewer 3 for valuable discussions and feedback. Results on multisample extensions, shown in Section 4 have been obtained by Dmitry Vetrov and are supported by the Russian Science Foundation grant no.similar to 17-71-20072.	Abadi M, 2015, P 12 USENIX S OPERAT; Agakov FV, 2004, LECT NOTES COMPUT SC, V3316, P561; Angelova A., 2012, INT J PURE APPL MATH, V79; Atanov A., 2019, INT C LEARN REPR; Burda Yuri, 2015, ARXIV150900519; Chen Xi, 2016, ARXIV161102731; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2732; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Dillon J. V., 2017, CORR; Dinh L, 2016, ARXIV PREPRINT ARXIV; Domke J, 2018, ADV NEUR IN, V31; Gershman S.J., 2012, P 29 INT C MACH LEAR, P235; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grosse R. B., 2015, CORR; Guo F., 2016, ARXIV161105559; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55; Huszar F., 2017, ARXIV170208235; Jebara T, 2001, ADV NEUR IN, V13, P231; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kuleshov V., 2017, ADV NEURAL INFORM PR, V30, P6734; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Le T. A., 2017, ARXIV170510306; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Louizos C, 2017, ADV NEUR IN, V30; MACKAY DJC, 1995, NUCL INSTRUM METH A, V354, P73, DOI 10.1016/0168-9002(94)00931-7; Mescheder L, 2017, PR MACH LEARN RES, V70; Mohamed Shakir, 2016, ARXIV161003483; Molchanov D., 2018, ARXIV181002789; Naesseth C. A., 2017, ARXIV170511140; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Nowozin S., 2018, INT C LEARN REPR; Nowozin S, 2016, ADV NEUR IN, V29; Oliphant T.E., 2010, NUMPY GUIDE NUMPY, V1, P378, DOI [10.1016/j.jmoldx.2015.02.001, DOI 10.1016/J.JMOLDX.2015.02.001]; Papamakarios George, 2017, ARXIV170507057; Poole B., 2018, NEURIPS WORKSH BAYES; Rainforth T, 2018, PR MACH LEARN RES, V80; Ranganath R, 2016, PR MACH LEARN RES, V48; Reddi Sashank J., 2018, INT C LEARN REPR; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; SHAROT T, 1976, J AM STAT ASSOC, V71, P451, DOI 10.2307/2285332; Shi J., 2017, ARXIV170510119; Titsias M. K., 2018, ARXIV180802078; Tran D, 2017, ADV NEUR IN, V30; Tran Dustin, 2015, P NIPS 15, P3564; Tucker George, 2019, INT C LEARN REPR; Uehara M., 2016, ARXIV PREPRINT ARXIV; Virtanen P., 2019, ARXIV E PRINTS; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Waterhouse S, 1996, ADV NEUR IN, V8, P351; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yin MZ, 2018, PR MACH LEARN RES, V80; Zhang C, 2018, IEEE INT C INTELL TR, P2678, DOI 10.1109/ITSC.2018.8569807	60	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300055
C	Song, GL; Fan, Z; Lafferty, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Ganlin; Fan, Zhou; Lafferty, John			Surfing: Iterative Optimization Over Incrementally Trained Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We investigate a sequential optimization procedure to minimize the empirical risk functional f((theta) over cap)(x) = 1/2 parallel to I G((theta) over cap)(x) - y parallel to(2) for certain families of deep networks G(theta)(x). The approach is to optimize a sequence of objective functions that use network parameters obtained during different stages of the training process. When initialized with random parameters theta(0), we show that the objective f(theta 0) (x) is "nice" and easy to optimize with gradient descent. As learning is carried out, we obtain a sequence of generative networks x -> G(theta t) (x) and associated risk functions f(theta t) (x), where t indicates a stage of stochastic gradient descent during training. Since the parameters of the network do not change by very much in each step, the surface evolves slowly and can be incrementally optimized. The algorithm is formalized and analyzed for a family of expansive networks. We call the procedure surfing since it rides along the peak of the evolving (negative) empirical risk function, starting from a smooth surface at the beginning of learning and ending with a wavy nonconvex surface after learning is complete. Experiments show how surfing can be used to find the global optimum and for compressed sensing even when direct gradient descent on the final learned network fails.	[Song, Ganlin; Fan, Zhou; Lafferty, John] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA	Yale University	Song, GL (corresponding author), Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA.	ganlin.song@yale.edu; zhou.fan@yale.edu; john.lafferty@yale.edu			NSF [DMS-1513594, CCF-1839308, DMS-1916198]; J.E Morgan Faculty Research Award	NSF(National Science Foundation (NSF)); J.E Morgan Faculty Research Award	Research supported in part by NSF grants DMS-1513594, CCF-1839308, DMS-1916198, and a J.E Morgan Faculty Research Award.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Bora A, 2017, PR MACH LEARN RES, V70; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Chen T.Q., 2018, ADV NEURAL INFORM PR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Davis D, 2020, FOUND COMPUT MATH, V20, P119, DOI 10.1007/s10208-018-09409-5; Dinh L, 2017, 5 INT C LEARN REPR I; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Du SS, 2018, ARXIV181103804; Du SS., 2019, P 7 INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hand P., 2019, IEEE T INFORM THEORY; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2018, P ADV NEUR INF PROC, P10215; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Virmaux A., 2018, ADV NEURAL INFORM PR, V31, P3835; WAN WF, 2018, ADV NEURAL INFORM PR, P3815	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906066
C	Sorscher, B; Mel, GC; Ganguli, S; Ocko, SA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sorscher, Ben; Mel, Gabriel C.; Ganguli, Surya; Ocko, Samuel A.			A unified theory for the origin of grid cells through the lens of pattern formation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Grid cells in the brain fire in strikingly regular hexagonal patterns across space. There are currently two seemingly unrelated frameworks for understanding these patterns. Mechanistic models account for hexagonal firing fields as the result of pattern-forming dynamics in a recurrent neural network with hand-tuned center-surround connectivity. Normative models specify a neural architecture, a learning rule, and a navigational task, and observe that grid-like firing fields emerge due to the constraints of solving this task. Here we provide an analytic theory that unifies the two perspectives by casting the learning dynamics of neural networks trained on navigational tasks as a pattern forming dynamical system. This theory provides insight into the optimal solutions of diverse formulations of the normative task, and shows that symmetries in the representation of space correctly predict the structure of learned firing fields in trained neural networks. Further, our theory proves that a nonnegativity constraint on firing rates induces a symmetry-breaking mechanism which favors hexagonal firing fields. We extend this theory to the case of learning multiple grid maps and demonstrate that optimal solutions consist of a hierarchy of maps with increasing length scales. These results unify previous accounts of grid cell firing and provide a novel framework for predicting the learned representations of recurrent neural networks.	[Sorscher, Ben; Ganguli, Surya; Ocko, Samuel A.] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA; [Mel, Gabriel C.] Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA	Stanford University; Stanford University	Sorscher, B (corresponding author), Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.			Sorscher, Ben/0000-0002-0339-1356	Simons foundation; NSF Career [1845166]; Stanford Graduate Fellowship; James S. McDonnell foundation	Simons foundation; NSF Career(National Science Foundation (NSF)NSF - Office of the Director (OD)); Stanford Graduate Fellowship(Stanford University); James S. McDonnell foundation	S.G. thanks the Simons, and James S. McDonnell foundations, and NSF Career 1845166 for funding. B.S. thanks the Stanford Graduate Fellowship for financial support.	Aronov D, 2017, NATURE, V543, P719, DOI 10.1038/nature21692; Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6; Brun VH, 2008, HIPPOCAMPUS, V18, P1200, DOI 10.1002/hipo.20504; Burak Y, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/ journal. pcbi1000291; Butler WN, 2019, SCIENCE, V363, P1447, DOI 10.1126/science.aav5297; Campbell MG, 2018, NAT NEUROSCI, V21, P1096, DOI 10.1038/s41593-018-0189-y; Constantinescu AO, 2016, SCIENCE, V352, P1464, DOI 10.1126/science.aaf0941; CROSS MC, 1993, REV MOD PHYS, V65, P851, DOI 10.1103/RevModPhys.65.851; Cueva Christopher J, ARXIV180307770V1; Dordek Y, 2016, ELIFE, V5, DOI 10.7554/eLife.10094; Erdem UM, 2012, EUR J NEUROSCI, V35, P916, DOI 10.1111/j.1460-9568.2012.08015.x; Fuhs Mark C, 2006, SPIN GLASS MODEL PAT, DOI [10.1523/ JNEUROSCI. 4353- 05.2006, DOI 10.1523/JNEUROSCI.4353-05.2006]; Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721; Hales JB, 2014, CELL REP, V9, P893, DOI 10.1016/j.celrep.2014.10.009; Hardcastle K, 2015, NEURON, V86, P827, DOI 10.1016/j.neuron.2015.03.039; Hardcastle Kiah, 2017, MULTIPLEXED HETEROGE, DOI [10.1016/ j. neuron. 2017.03.025, DOI 10.1016/J.NEURON.2017.03.025]; Ismakov R, 2017, CURR BIOL, V27, P2337, DOI 10.1016/j.cub.2017.06.034; Krupic J, 2015, NATURE, V518, P232, DOI 10.1038/nature14153; Mathis A, 2015, ELIFE, V4, DOI 10.7554/eLife.05979; MCNAUGHTON BL, 1983, EXP BRAIN RES, V52, P41; Ocko SA, 2018, P NATL ACAD SCI USA, V115, pE11798, DOI 10.1073/pnas.1805959115; Saxe AM, 2019, P NATL ACAD SCI USA, V116, P11537, DOI 10.1073/pnas.1820226116; Skaggs William E, TECH REP; Stachenfeld KL, 2017, NAT NEUROSCI, V20, P1643, DOI 10.1038/nn.4650; Wei XX, 2015, ELIFE, V4, DOI 10.7554/eLife.08362; Whittington James C R, TECH REP; Zhang K, 1996, J NEUROSCI, V16, P2112	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901061
C	Subramani, N; Bowman, SR; Cho, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Subramani, Nishant; Bowman, Samuel R.; Cho, Kyunghyun			Can Unconditional Language Models Recover Arbitrary Sentences?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural network-based generative language models like ELMo and BERT can work effectively as general purpose sentence encoders in text classification without further fine-tuning. Is it possible to adapt them in a similar way for use as general-purpose decoders? For this to be possible, it would need to be the case that for any target sentence of interest, there is some continuous representation that can be passed to the language model to cause it to reproduce that sentence. We set aside the difficult problem of designing an encoder that can produce such representations and, instead, ask directly whether such representations exist at all. To do this, we introduce a pair of effective, complementary methods for feeding representations into pretrained unconditional language models and a corresponding set of methods to map sentences into and out of this representation space, the reparametrized sentence space. We then investigate the conditions under which a language model can be made to generate a sentence through the identification of a point in such a space and find that it is possible to recover arbitrary sentences nearly perfectly with language models and representations of moderate size without modifying any model parameters.	[Subramani, Nishant; Bowman, Samuel R.; Cho, Kyunghyun] NYU, New York, NY 10003 USA; Facebook AI Res, Menlo Pk, CA USA	New York University; Facebook Inc	Subramani, N (corresponding author), NYU, New York, NY 10003 USA.	nishant@nyu.edu			Samsung Electronics (Improving Deep Learning using Latent Structure)	Samsung Electronics (Improving Deep Learning using Latent Structure)	This work was supported by Samsung Electronics (Improving Deep Learning using Latent Structure). We gratefully acknowledge the support of NVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research.	[Anonymous], 2015, NIPS; [Anonymous], 2014, SCIPY OPEN SOURCE SC; [Anonymous], 2019, ARXIV190309722; Ba J., 2017, P 3 INT C LEARN REPR; Bahdanau Dzmitry, 2015, ICLR; Bird Steven, 2004, ACL; Bojanowski Piotr, 2018, ICML; Bowman Samuel R, 2016, CONLL 2016; Chen Yubo, 2018, EMNLP; Cho Kyunghyun, 2016, ARXIV160503835; Choi Heeyoul, 2017, COMPUTER SPEECH LANG; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171, DOI [10.18653/v1/N19-1423, DOI 10.18653/V1/N19-1423]; Dong L., 2019, ARXIV190503197; Graff D., 2003, LINGUISTIC DATA CONS; Graves Alex, 2012, COMPUT SCI; Graves Alex, 2013, ARXIV13080850 CORR; Gulcehre Caglar, 2015, USING MONOLINGUAL CO; Hochreiter S, 1997, NEURAL COMPUTATION; Koehn P., 2007, ACL; Koehn Philipp, 2017, ACL 2017; Lample G, 2019, ARXIV190107291; Li C., 2018, P 6 INT C LEARN REPR; Li J., 2016, NAACL; Merrienboer B., 2014, P SSST 8 8 WORKSH SY; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mu Jiaqi, 2017, ACL; Ofir Press and Lior Wolf, 2017, ACL; Olshausen B. A., 1997, VISION RES; Papineni K., 2002, P ANN M ASS COMP LIN; Pascanu Razvan, 2013, P INT C MACH LEAR; Peters Matthew, 2017, ACL; Peters Matthew E., 2018, P 2018 C N AM CHAPT; Radford Alec, 2018, THESIS; Ruder Sebastian, 2018, ACL; Sennrich R., 2016, ACL; Song Kaitao, 2019, ICML; Sriram A, 2018, INTERSPEECH, P387; Srivastava N., 2014, JMLR; Sutskever I., 2014, NEURIPS; Wright S. J., 1999, NUMERICAL OPTIMIZATI; Yang Zhilin, 2019, NIPS; Yu L., 2017, AAAI; Zoph B., 2016, EMNLP	43	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906087
C	Sun, J; Xu, ZB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Jian; Xu, Zongben			Neural Diffusion Distance for Image Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Diffusion distance is a spectral method for measuring distance among nodes on graph considering global data structure. In this work, we propose a spec-diff-net for computing diffusion distance on graph based on approximate spectral decomposition. The network is a differentiable deep architecture consisting of feature extraction and diffusion distance modules for computing diffusion distance on image by end-to-end training. We design low resolution kernel matching loss and high resolution segment matching loss to enforce the network's output to be consistent with human-labeled image segments. To compute high-resolution diffusion distance or segmentation mask, we design an up-sampling strategy by feature-attentional interpolation which can be learned when training spec-diff-net. With the learned diffusion distance, we propose a hierarchical image segmentation method outperforming previous segmentation methods. Moreover, a weakly supervised semantic segmentation network is designed using diffusion distance and achieved promising results on PASCAL VOC 2012 segmentation dataset.	[Sun, Jian; Xu, Zongben] Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Peoples R China	Xi'an Jiaotong University	Sun, J (corresponding author), Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Peoples R China.	jiansun@xjtu.edu.cn; zbxu@xjtu.edu.cn			National Natural Science Foundation of China [11971373, 11622106, 11690011, 61721002, U1811461]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by National Natural Science Foundation of China under Grants 11971373, 11622106, 11690011, 61721002, U1811461.	[Anonymous], 2014, ICLR; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bach FR, 2004, ADV NEUR IN, V16, P305; Barnes Matt, 2018, ICLR WORKSH; Bau III D, 1997, NUMERICAL LINEAR ALG; Chandra S., 2017, IEEE I CONF COMP VIS, P5103, DOI DOI 10.1109/ICCV.2017.546; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130; Ci Hai, 2018, P EUR C COMP VIS ECC, P501; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fan R., 2018, ECCV; Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171; FRANCIS JGF, 1962, COMPUT J, V4, P332, DOI 10.1093/comjnl/4.4.332; Hariharan Bharath, 2011, P IEEE C COMP VIS PA; Harley AW, 2017, IEEE I CONF COMP VIS, P5048, DOI 10.1109/ICCV.2017.539; Huang Zilong, 2018, CVPR; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jiang Peng, 2018, NEURIPS, P1637; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S, 2017, NEURAL INFORM PROCES; Mishne Gal, 2017, APPL COMPUTAIONAL HA; Nadler B., 2006, ADV NEURAL INFORM PR, P955; Ng AY, 2002, ADV NEUR IN, V14, P849; Oh Seong Joon, 2017, CVPR; Pinheiro P.O., 2015, CVPR; Shaham U, 2018, ICLR; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Vernaza P., 2017, CVPR; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Yu Ruixuan, 2018, ECCV GMDL, P377; Zagoruyko S., 2015, CVPR; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhu Xiaojin., 2003, P ICLR, P912	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301043
C	Sun, T; Sun, YJ; Li, DS; Liao, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Tao; Sun, Yuejiao; Li, Dongsheng; Liao, Qing			General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALTERNATING MINIMIZATION; CONVERGENCE; NONCONVEX	The incremental aggregated gradient algorithm is popular in network optimization and machine learning research. However, the current convergence results require the objective function to be strongly convex. And the existing convergence rates are also limited to linear convergence. Due to the mathematical techniques, the stepsize in the algorithm is restricted by the strongly convex constant, which may make the stepsize be very small (the strongly convex constant may be small). In this paper, we propose a general proximal incremental aggregated gradient algorithm, which contains various existing algorithms including the basic incremental aggregated gradient method. Better and new convergence results are proved even with the general scheme. The novel results presented in this paper, which have not appeared in previous literature, include: a general scheme, nonconvex analysis, the sublinear convergence rates of the function values, much larger stepsizes that guarantee the convergence, the convergence when noise exists, the line search strategy of the proximal incremental aggregated gradient algorithm and its convergence.	[Sun, Tao; Li, Dongsheng] Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China; [Sun, Yuejiao] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90095 USA; [Liao, Qing] Harbin Inst Technol Shenzhen, Dept Comp Sci & Technol, Shenzhen 518055, Guangdong, Peoples R China	National University of Defense Technology - China; University of California System; University of California Los Angeles; Harbin Institute of Technology	Sun, T (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.	nudtsuntao@163.com; sunyj@math.ucla.edu; dsli@nudt.edu.cn; liaoqing@hit.edu.cn			National Key R&D Program of China [2018YFB0204300]; National Natural Science Foundation of China [61932001, 61906200]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is sponsored in part by the National Key R&D Program of China under Grant No. 2018YFB0204300 and the National Natural Science Foundation of China under Grants (61932001 and 61906200).	Agarwal, 2014, ARXIV14100723; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bertsekas Dp, 2011, OPTIM MACH LEARN, V2010, P3; Blatt D, 2007, SIAM J OPTIMIZ, V18, P29, DOI 10.1137/040615961; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Chen Tianyi, 2018, NIPS 2018; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Gurbuzbalaban M, 2015, MATH PROGRAM, V151, P283, DOI 10.1007/s10107-015-0897-y; Gurbuzbalaban M, 2017, SIAM J OPTIMIZ, V27, P1035, DOI 10.1137/15M1049695; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Kurdyka K, 1998, ANN I FOURIER, V48, P769, DOI 10.5802/aif.1638; Lai MJ, 2013, SIAM J IMAGING SCI, V6, P1059, DOI 10.1137/120863290; Lian X., 2017, ARXIV170509056; LOJASIEWICZ S, 1993, ANN I FOURIER, V43, P1575; Mordukhovich B.S, 2006, VARIATIONAL ANAL GEN, V330; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Robbins H., 1985, HERBERT ROBBINS SELE, P111; Rockafellar R.T., 2015, CONVEX ANAL; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; Ryu Ernest K, 2017, ARXIV170806908; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Solodov MV, 1998, COMPUT OPTIM APPL, V11, P23, DOI 10.1023/A:1018366000512; Sun T., 2017, ARXIV170904072; Sun Tao, 2017, ADV NEURAL INFORM PR, P6183; Tseng P, 2014, J OPTIMIZ THEORY APP, V160, P832, DOI 10.1007/s10957-013-0409-2; Vanli N. D., 2016, ARXIV160801713; YAN BD, 2013, ADV NEURAL INFORM PR, P91	31	1	1	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301004
C	Sun, Y; Flammarion, N; Fazel, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Yue; Flammarion, Nicolas; Fazel, Maryam			Escaping from saddle points on Riemannian manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SECTIONAL CURVATURES	We consider minimizing a nonconvex, smooth function f on a Riemannian manifold M. We show that a perturbed version of Riemannian gradient descent algorithm converges to a second-order stationary point (and hence is able to escape saddle points on the manifold). The rate of convergence depends as 1/epsilon(2) on the accuracy c, which matches a rate known only for unconstrained smooth minimization. The convergence rate depends polylogarithmically on the manifold dimension d, hence is almost dimension -free. The rate also has a polynomial dependence on the parameters describing the curvature of the manifold and the smoothness of the function. While the unconstrained problem (Euclidean setting) is well -studied, our result is the first to prove such a rate for nonconvex, manifold-constrained problems.	[Sun, Yue; Fazel, Maryam] Univ Washington, Seattle, WA 98105 USA; [Flammarion, Nicolas] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	University of Washington; University of Washington Seattle; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Sun, Y (corresponding author), Univ Washington, Seattle, WA 98105 USA.	yuesun@uw.edu; nicolas.flammarion@epfl.ch; mfazel@uw.edu						Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Agarwal N., 2018, ARXIV180600065; Becigneul Gary, 2019, INT C LEARNING REPRE; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Boumal N., 2014, JMLR; BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Boumal N., 2016, IMA J NUMERICAL ANAL; Boumal N, 2019, IMA J NUMER ANAL, V39, P1, DOI 10.1093/imanum/drx080; Carmon Y., 2017, ARXIV161200547; Cheeger J., 2008, COMP THEOREMS RIEMAN; do Carmo MP, 2016, DIFFERENTIAL GEOMETR; Du S.S., 2017, ADV NEURAL INFORM PR, P1067; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Hu J, 2018, SIAM J MATRIX ANAL A, V39, P1181, DOI 10.1137/17M1142478; Ishteva M, 2011, SIAM J MATRIX ANAL A, V32, P115, DOI 10.1137/090764827; Jin C., 2018, C LEARN THEOR COLT 2; Jin C., 2017, ICML; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Kasai H, 2018, ADV NEURAL INFORM PR, V31, P4254; Khuzani M.B., 2017, ARXIV170308167; Kim SJ, 2016, PORTL INT CONF MANAG, P2460, DOI 10.1109/PICMET.2016.7806794; Lee J.M, 1997, GRADUATE TEXTS MATH, V176; Lee Jason D, 2017, ARXIV171007406; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Mangoubi O, 2018, ANN APPL PROBAB, V28, P2501, DOI 10.1214/17-AAP1365; Miolane Nina, 2018, ARXIV180508308; Mokhtari A, 2018, ARXIV180902162; N Tripuraneni, 2018, ARXIV180209128; Nouiehed M., 2018, ARXIV181002024; PEMANTLE R, 1990, ANN PROBAB, V18, P698, DOI 10.1214/aop/1176990853; Rapcsak T, 2008, J GLOBAL OPTIM, V40, P375, DOI 10.1007/s10898-007-9212-7; Sakai T., 1996, TRANSLATIONS MATH MO, V149; Sun Y., 2018, WORKSH MOD TRENDS NO; WONG YC, 1968, P NATL ACAD SCI USA, V60, P75, DOI 10.1073/pnas.60.1.75; Zhang H., 2016, C LEARN THEOR; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592; Zhang J., 2018, ARXIV180505565	40	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307031
C	Tan, FW; Cascante-Bonilla, P; Guo, XX; Wu, H; Feng, S; Ordonez, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tan, Fuwen; Cascante-Bonilla, Paola; Guo, Xiaoxiao; Wu, Hui; Feng, Song; Ordonez, Vicente			Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. Moreover, our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down, an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore, we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.	[Tan, Fuwen; Cascante-Bonilla, Paola; Ordonez, Vicente] Univ Virginia, Charlottesville, VA 22903 USA; [Guo, Xiaoxiao; Wu, Hui; Feng, Song] IBM Res AI, Albany, NY USA	University of Virginia	Tan, FW (corresponding author), Univ Virginia, Charlottesville, VA 22903 USA.	fuwen.tan@virginia.edu; pc9za@virginia.com; xiaoxiao.guo@ibm.com; wuhu@us.ibm.com; sfeng@us.ibm.com; vicente@virginia.edu			SAP Research	SAP Research	We thank our anonymous reviewers for helpful feedback. This work was funded by a research grant from SAP Research and generous gift funding from SAP Research. We thank Tassilo Klein and Moin Nabi from SAP Research for their support.	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Arandjelovic R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.92; Cascante-Bonilla Paola, 2019, C N AM ASS COMP LING; Chang Ning-San, 1980, PICTORIAL INFORM SYS; CHANG NS, 1980, IEEE T SOFTWARE ENG, V6, P519, DOI 10.1109/TSE.1980.230801; Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Faghri Fartash, 2018, BMVC, P12; Guo XX, 2018, ADV NEUR IN, V31; Gupta Tanmay, 2017, IEEE INT C COMP VIS; Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Kiddon Chloe, 2016, EMPIRICAL METHODS NA; Kingma D.P, P 3 INT C LEARNING R; Kiros R, 2014, PR MACH LEARN RES, V32, P595; Kovashka A, 2015, INT J COMPUT VISION, V115, P185, DOI 10.1007/s11263-015-0814-0; Kovashka Adriana, 2013, IEEE INT C COMP VIS; Krishna R., 2016, VISUAL GENOME CONNEC; Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8; Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P801, DOI 10.1145/3240508.3240605; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413; Siddiquie B, 2011, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2011.5995329; Sordoni Alessandro, 2015, P 24 ACM INT C INF K, P553, DOI DOI 10.1145/2806416.2806493; Sukhbaatar S, 2015, ADV NEUR IN, V28; Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Weinberger KQ, 2014, ADV NEURAL INFORM PR, P1889; West JG, 2015, SCI SYNTH, P1, DOI 10.1055/sos-SD-220-00001; Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677; Xu K, 2015, PR MACH LEARN RES, V37, P2048	37	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302063
C	Tanaka, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tanaka, Akinori			Discriminator optimal transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Within a broad class of generative adversarial networks, we show that discriminator optimization process increases a lower bound of the dual cost function for the Wasserstein distance between the target distribution p and the generator distribution p(G). It implies that the trained discriminator can approximate optimal transport (OT) from p(G) to p. Based on some experiments and a bit of OT theory, we propose discriminator optimal transport (DOT) scheme to improve generated images. We show that it improves inception score and FID calculated by unconditional GAN trained by CIFAR-10, STL-10 and a public pre-trained model of conditional GAN trained by ImageNet.	[Tanaka, Akinori] RIKEN, Ctr Adv Intelligence Project AIP, Math Sci Team, Chuo Ku, 1-4-1 Nihonbashi, Tokyo 1030027, Japan; [Tanaka, Akinori] RIKEN, Interdisciplinary Theoret & Math Sci Program iTHE, 2-1 Hirosawa, Wako, Saitama 3510198, Japan; [Tanaka, Akinori] Keio Univ, Fac Sci & Technol, Dept Math, Kouhoku Ku, 3-14-1 Hiyoshi, Yokohama, Kanagawa 2238522, Japan	RIKEN; RIKEN; Keio University	Tanaka, A (corresponding author), RIKEN, Ctr Adv Intelligence Project AIP, Math Sci Team, Chuo Ku, 1-4-1 Nihonbashi, Tokyo 1030027, Japan.; Tanaka, A (corresponding author), RIKEN, Interdisciplinary Theoret & Math Sci Program iTHE, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.; Tanaka, A (corresponding author), Keio Univ, Fac Sci & Technol, Dept Math, Kouhoku Ku, 3-14-1 Hiyoshi, Yokohama, Kanagawa 2238522, Japan.	akinori.tanaka@riken.jp						Agustsson Eirikur, 2019, 7 INT C LEARN REPR I; Arjovsky M., 2017, ARXIV170107875; Azadi Samaneh, 2019, 7 INT C LEARN REPR I; Bellemare M G, 2017, CORR; Brock A., 2019, 7 INT C LEARN REPR I; Caffarelli LA, 2002, J AM MATH SOC, V15, P1, DOI 10.1090/S0894-0347-01-00376-9; Fedus William, 2018, 6 INT C LEARN REPR I; Fergus R, 2014, INT C LEARN REPR; Flamary R<prime>emi, 2017, POT PYTHON OPTIMAL T; Gangbo W, 1996, ACTA MATH-DJURSHOLM, V177, P113, DOI 10.1007/BF02392620; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Karras T., 2018, 6 INT C LEARN REPR I; Kingma D. P., 2014, P INT C LEARN REPR; Lim Jae Hyun, 2017, CORR; Lukovnikov D., 2018, 6 INT C LEARN REPR I; Metz L., 2017, 5 INT C LEARN REPR I; Mirza M., 2014, ARXIV PREPRINT ARXIV; Miyato T., 2018, 6 INT C LEARN REPR I; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Radford A., 2016, 4 INT C LEARN REPR I; Salimans T., 2018, 6 INT C LEARN REPR I; Salimans T, 2016, ADV NEUR IN, V29; Shlens Jonathon, 2015, INT C LEARN REPR; Turner R, 2019, PR MACH LEARN RES, V97; Unterthiner Thomas, 2018, 6 INT C LEARN REPR I; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wei Xiang, 2018, 6 INT C LEARN REPR I; Zhang H, 2019, PR MACH LEARN RES, V97; Zhao J. J., 2016, CORR	32	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306078
C	Tanaka, Y; Tanaka, T; Iwata, T; Kurashima, T; Okawa, M; Akagi, Y; Toda, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tanaka, Yusuke; Tanaka, Toshiyuki; Iwata, Tomoharu; Kurashima, Takeshi; Okawa, Maya; Akagi, Yasunori; Toda, Hiroyuki			Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a probabilistic model for inferring the multivariate function from multiple areal data sets with various granularities. Here, the areal data are observed not at location points but at regions. Existing regression-based models can only utilize the sufficiently fine-grained auxiliary data sets on the same domain (e.g., a city). With the proposed model, the functions for respective areal data sets are assumed to be a multivariate dependent Gaussian process (GP) that is modeled as a linear mixing of independent latent GPs. Sharing of latent GPs across multiple areal data sets allows us to effectively estimate the spatial correlation for each areal data set; moreover it can easily be extended to transfer learning across multiple domains. To handle the multivariate areal data, we design an observation model with a spatial aggregation process for each areal data set, which is an integral of the mixed GP over the corresponding region. By deriving the posterior GP, we can predict the data value at any location point by considering the spatial correlations and the dependences between areal data sets, simultaneously. Our experiments on real-world data sets demonstrate that our model can 1) accurately refine coarse-grained areal data, and 2) offer performance improvements by using the areal data sets from multiple domains.	[Tanaka, Yusuke; Kurashima, Takeshi; Okawa, Maya; Akagi, Yasunori; Toda, Hiroyuki] NTT Serv Evolut Labs, Tokyo, Japan; [Iwata, Tomoharu] NTT Commun Sci Labs, Tokyo, Japan; [Tanaka, Yusuke; Tanaka, Toshiyuki] Kyoto Univ, Kyoto, Japan	Nippon Telegraph & Telephone Corporation; Kyoto University	Tanaka, Y (corresponding author), NTT Serv Evolut Labs, Tokyo, Japan.; Tanaka, Y (corresponding author), Kyoto Univ, Kyoto, Japan.	yusuke.tanaka.rh@hco.ntt.co.jp; tt@i.kyoto-u.ac.jp; tomoharu.iwata.gy@hco.ntt.co.jp; takeshi.kurashima.uf@hco.ntt.co.jp; maya.ookawa.af@hco.ntt.co.jp; yasunori.akagi.cu@hco.ntt.co.jp; hiroyuki.toda.xb@hco.ntt.co.jp	Tanaka, Toshiyuki/C-2749-2011					Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Bishop C.M, 2006, PATTERN RECOGN; Bogomolov A., 2014, P 16 INT C MULTIMODA, P427, DOI DOI 10.1145/2663204.2663254; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Boyle P., 2005, ADV NEURAL INFORM PR, P217; Burgess T. M., 1980, J SOIL SCI, V31; Gibbs M., 1997, TECHNICAL REPORT; Goovaerts P, 2010, MATH GEOSCI, V42, P535, DOI 10.1007/s11004-010-9286-5; Gotway CA, 2002, J AM STAT ASSOC, V97, P632, DOI 10.1198/016214502760047140; Hamelijnck O., 2019, NEURIPS; Higdon D, 2002, QUANTITATIVE METHODS, P37, DOI DOI 10.1007/978-1-4471-0657-9_2; Howitt R, 2003, EUR REV AGRIC ECON, V30, P359, DOI 10.1093/erae/30.3.359; Jerrett M, 2013, AM J RESP CRIT CARE, V188, P593, DOI 10.1164/rccm.201303-0609OC; Keil P, 2013, METHODS ECOL EVOL, V4, P82, DOI 10.1111/j.2041-210x.2012.00264.x; Law H. C. L., 2018, NEURIPS, P6084; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Luttinen J., 2009, ADV NEURAL INFORM PR, P1177; Micchelli Charles A, 2004, ADV NEURAL INFORM PR, P921; Murakami D, 2011, PROCD SOC BEHV, V21, DOI 10.1016/j.sbspro.2011.07.034; Murray-Smith R., 2004, DSMML, P110; Myers D.E., 1984, GEOSTATISTICS NATURA, P295, DOI [10.1007/978-94-009-3699-7_18, DOI 10.1007/978-94-009-3699-718]; Park NW, 2013, ADV METEOROL, V2013, DOI 10.1155/2013/237126; Pupasingha A., 2007, J SOCIO-ECON, V36, P650, DOI [DOI 10.1016/J.SOCEC.2006.12.021, 10.1016/j.socec.2006.12.021]; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Smith M. T., 2018, GAUSSIAN PROCESS REG; Smith-Clarke C, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P511, DOI 10.1145/2556288.2557358; Snelson E, 2004, ADV NEUR IN, V16, P337; Sturrock HJW, 2014, MALARIA J, V13, DOI 10.1186/1475-2875-13-421; Tanaka Y, 2019, AAAI CONF ARTIF INTE, P5091; Tang LA, 2012, PROC INT CONF DATA, P186, DOI 10.1109/ICDE.2012.33; Taylor B. M, 2018, J ROYAL STAT SOC A, P12347; Teh Y.W., 2005, WORKSH ART INT STAT, V2005, P333; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Wang HJ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P635, DOI 10.1145/2939672.2939736; Wilby R, 2004, GUIDELINES USE CLIMA; Wilson K, 2018, BIOSTATISTICS; Wotling G, 2000, J HYDROL, V233, P86, DOI 10.1016/S0022-1694(00)00232-8; Xavier A., 2016, SPATIAL STAT, V23, P91; Yousefi F., 2019, NEURIPS	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303004
C	Tessler, C; Tennenholtz, G; Mannor, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tessler, Chen; Tennenholtz, Guy; Mannor, Shie			Distributional Policy Optimization: An Alternative Approach for Continuous Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We identify a fundamental problem in policy gradient-based methods in continuous control. As policy gradient methods require the agent's underlying probability distribution, they limit policy representation to parametric distribution classes. We show that optimizing over such sets results in local movement in the action space and thus convergence to sub-optimal solutions. We suggest a novel distributional framework, able to represent arbitrary distribution functions over the continuous action space. Using this framework, we construct a generative scheme, trained using an off-policy actor-critic paradigm, which we call the Generative Actor Critic (GAC). Compared to policy gradient methods, GAC does not require knowledge of the underlying probability distribution, thereby overcoming these limitations. Empirical evaluation shows that our approach is comparable and often surpasses current state-of-the-art baselines in continuous domains.	[Tessler, Chen; Tennenholtz, Guy; Mannor, Shie] Technion Israel Inst Technol, Haifa, Israel	Technion Israel Institute of Technology	Tessler, C (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	chen.tessler@campus.technion.ac.il; guytenn@gmail.com; shie@ee.technion.ac.il		Mannor, Shie/0000-0003-4439-7647				Arjovsky M, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bhatnagar S, 2012, J OPTIMIZ THEORY APP, V153, P688, DOI 10.1007/s10957-012-9989-5; Borkar V. S., 2009, STOCHASTIC APPROXIMA, V48; Chou PW, 2017, PR MACH LEARN RES, V70; Chow Y., 2017, J MACHINE LEARNING R, V18, P6070; Dabney W., 2017, ARXIV171010044; Dabney W, 2018, AAAI CONF ARTIF INTE, P2892; Fujimoto S, 2018, PR MACH LEARN RES, V80; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; Hesse C., 2017, OPENAI BASELINES; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Kallenberg O., 2006, FDN MODERN PROBABILI; Koenker R, 2006, J AM STAT ASSOC, V101, P980, DOI 10.1198/016214506000000672; Konda VR, 2000, ADV NEUR IN, V12, P1008; Korenkevych Dmytro, 2019, ARXIV190311524; Lazaric A, 2016, J MACH LEARN RES, V17; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Ostrovski G, 2018, PR MACH LEARN RES, V80; Peng Xue Bin, 2018, ACM T GRAPHIC, V37, DOI DOI 10.1145/3197517.3201311; Polyak BT, 1990, AUTOMAT TELEMEKH, V7, P2; Puterman M. L., 1979, Mathematics of Operations Research, V4, P60, DOI 10.1287/moor.4.1.60; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Qu C., 2018, ARXIV180507732; Riedmiller M, 2018, PR MACH LEARN RES, V80; Salimans T., 2017, ARXIV170303864; Salimans T, 2016, ADV NEUR IN, V29; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2014, PR MACH LEARN RES, V32; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tang Yunhao, 2019, ARXIV190110500; Teh, 2018, P MACHINE LEARNING R, P29; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893	44	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301035
C	Tikka, S; Hyttinen, A; Karvanen, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tikka, Santtu; Hyttinen, Antti; Karvanen, Juha			Identifying Causal Effects via Context-specific Independence Relations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPUTATIONAL-COMPLEXITY; PROBABILISTIC INFERENCE; DIAGRAMS	Causal effect identification considers whether an interventional probability distribution can be uniquely determined from a passively observed distribution in a given causal structure. If the generating system induces context-specific independence (CSI) relations, the existing identification procedures and criteria based on do-calculus are inherently incomplete. We show that deciding causal effect non-identifiability is NP-hard in the presence of CSIs. Motivated by this, we design a calculus and an automated search procedure for identifying causal effects in the presence of CSIs. The approach is provably sound and it includes standard do-calculus as a special case. With the approach we can obtain identifying formulas that were unobtainable previously, and demonstrate that a small number of CSI-relations may be sufficient to turn a previously non-identifiable instance to identifiable.	[Tikka, Santtu; Karvanen, Juha] Univ Jyvaskyla, Dept Math & Stat, Jyvaskyla, Finland; [Hyttinen, Antti] Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland	University of Jyvaskyla; Aalto University; University of Helsinki	Tikka, S (corresponding author), Univ Jyvaskyla, Dept Math & Stat, Jyvaskyla, Finland.	santtu.tikka@jyu.fi; antti.hyttinen@helsinki.fi; juha.t.karvanen@jyu.fi		Tikka, Santtu/0000-0003-4039-4342	Academy of Finland [311877, 295673]	Academy of Finland(Academy of Finland)	ST was supported by Academy of Finland grant 311877 (Decision analytics utilizing causal models and multiobjective optimization). AH was supported by Academy of Finland grant 295673.	Aleksandrowicz G, 2017, J ARTIF INTELL RES, V58, P431, DOI 10.1613/jair.5229; Barash Y, 2002, J COMPUT BIOL, V9, P169, DOI 10.1089/10665270252935403; Bareinboim E., 2014, ADV NEURAL INFORM PR, V27, P280; Bareinboim E., 2012, P 28 C UNC ART INT, P113; Bareinboim E, 2015, AAAI CONF ARTIF INTE, P3475; Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115; Butz CJ, 2016, P 8 INT C PROB GRAPH, P74; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Chickering David Maxwell, 1997, P 13 C UNC ART INT P, P80; COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D; Corander J, 2019, ANN PURE APPL LOGIC, V170, P975, DOI 10.1016/j.apal.2019.04.004; Dal G. H., 2018, P MACH LEARN RES; Dawid AP, 2002, INT STAT REV, V70, P161; Galles D., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P185; Georgi B, 2007, LECT NOTES ARTIF INT, V4702, P79; Halpern JY, 2000, J ARTIF INTELL RES, V12, P317, DOI 10.1613/jair.648; Huang Y., 2006, P 21 NAT C ART INT A, P1149; Hyttinen A., 2018, INT C PROB GRAPH MOD; Hyttinen A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P395; Koller D., 2009, PROBABILISTIC GRAPHI; Mohan K, 2013, ADV NEUTRAL INFORM P, V26, P1277; Nyman H, 2014, BAYESIAN ANAL, V9, P883, DOI 10.1214/14-BA882; Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.1093/biomet/82.4.669; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Pensar J, 2015, DATA MIN KNOWL DISC, V29, P503, DOI 10.1007/s10618-014-0355-0; Shimony S. E., 1991, AAAI-91. Proceedings Ninth National Conference on Artificial Intelligence, P482; Shpitser I, 2006, P 22 C UNCERTAINTY A, P437, DOI [10.48550/ARXIV.1206.6876, DOI 10.48550/ARXIV.1206.6876]; Shpitser I, 2008, J MACH LEARN RES, V9, P1941; Tikka S., 2019, DOSEARCH CAUSAL EFFE; Tikka S., 2019, CAUSAL EFFECT IDENTI; Tikka S, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i12; Visscher S, 2007, LECT NOTES ARTIF INT, V4594, P87	33	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302076
C	Tobin, J; Abbeel, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tobin, Josh; Abbeel, Pieter		OpenAl Robotics	Geometry-Aware Neural Rendering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RECONSTRUCTION	Understanding the 3-dimensional structure of the world is a core challenge in computer vision and robotics. Neural rendering approaches learn an implicit 3D model by predicting what a camera would see from an arbitrary viewpoint. We extend existing neural rendering to more complex, higher dimensional scenes than previously possible. We propose Epipolar Cross Attention (ECA), an attention mechanism that leverages the geometry of the scene to perform efficient non-local operations, requiring only O(n) comparisons per spatial dimension instead of O(n(2)). We introduce three new simulated datasets inspired by real-world robotics and demonstrate that ECA significantly improves the quantitative and qualitative performance of Generative Query Networks (GQN) [7].	[Tobin, Josh; OpenAl Robotics] OpenAI, San Francisco, CA 94110 USA; [Tobin, Josh; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Abbeel, Pieter] Covariant AI, Berkeley, CA USA	University of California System; University of California Berkeley	Tobin, J (corresponding author), OpenAI, San Francisco, CA 94110 USA.; Tobin, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	josh@openai.com; pabbeel@cs.berkeley.edu						Abadi M, 2015, P 12 USENIX S OPERAT; Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; [Anonymous], ARXIV180800177 OPENA; Brockman G., 2016, OPENAI GYM; Chang Angel X., 2015, ARXIV151203012CSGR P; Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022; Eisert P, 1999, INT CONF ACOUST SPEE, P3509, DOI 10.1109/ICASSP.1999.757599; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Faugeras O., 2002, VARIATIONAL PRINCIPL; Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Gupta, 2017, ARXIV171208125; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kumar Ananya, 2018, ARXIV180702033; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Levine S, 2016, J MACH LEARN RES, V17; Liu ZH, 2016, 9TH INTERNATIONAL CONFERENCE ON MICROWAVE AND MILLIMETER WAVE TECHNOLOGY PROCEEDINGS, VOL. 1, (ICMMT 2016), P286, DOI 10.1109/ICMMT.2016.7761750; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Morris DD, 2000, PROC CVPR IEEE, P332, DOI 10.1109/CVPR.2000.855837; Plappert, 2018, ARXIV180209464, P1; Reed Scott, 2017, ARXIV171010304; Rezende DJ, 2016, ADV NEUR IN, V29; Rosenbaum Dan, 2018, ARXIV180703149; Seitz S., 2006, 2006 IEEE COMP SOC C, V1, P519, DOI [10.1109/CVPR.2006.19, DOI 10.1109/CVPR.2006.19]; Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526; SHEPARD RN, 1971, SCIENCE, V171, P701, DOI 10.1126/science.171.3972.701; Soatto S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P974; Szeliski R., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P157, DOI 10.1109/CVPR.1999.786933; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Tulsiani S, 2018, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR.2018.00306; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Zamir AR, 2016, LECT NOTES COMPUT SC, V9907, P535, DOI 10.1007/978-3-319-46487-9_33	47	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903022
C	Tziavelis, N; Giannakopoulos, I; Doka, K; Koziris, N; Karras, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tziavelis, Nikolaos; Giannakopoulos, Ioannis; Doka, Katerina; Koziris, Nectarios; Karras, Panagiotis			Equitable Stable Matchings in Quadratic Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COLLEGE ADMISSIONS; COMPLEXITY	Can we reach a stable matching that achieves high equity among the two sides of a market in quadratic time? The Deferred Acceptance (DA) algorithm finds a stable matching that is biased in favor of one side; optimizing apt equity measures is strongly NP-hard. A proposed approximation algorithm offers a guarantee only with respect to the DA solutions. Recent work introduced Deferred Acceptance with Compensation Chains (DACC), a class of algorithms that can reach any stable matching in O(n(4)) time, but did not propose a way to achieve good equity. In this paper, we propose an alternative that is computationally simpler and achieves high equity too. We introduce Monotonic Deferred Acceptance (MDA), a class of algorithms that progresses monotonically towards a stable matching; we couple MDA with a mechanism we call Strongly Deferred Acceptance (SDA), to build an algorithm that reaches an equitable stable matching in quadratic time; we amend this algorithm with a few low-cost local search steps to build Deferred Local Search (DLS), which, as we demonstrate experimentally, outperforms previous solutions in terms of equity measures and matches the most efficient ones in runtime.	[Tziavelis, Nikolaos] Northeastern Univ, Boston, MA 02115 USA; [Giannakopoulos, Ioannis; Doka, Katerina; Koziris, Nectarios] NTU Athens, Athens, Greece; [Karras, Panagiotis] Aarhus Univ, Aarhus, Denmark	Northeastern University; Aarhus University	Tziavelis, N (corresponding author), Northeastern Univ, Boston, MA 02115 USA.		Karras, Panagiotis/AFS-4578-2022	Tziavelis, Nikolaos/0000-0001-8342-2177				Abdulkadiroglu A, 2005, AM ECON REV, V95, P368, DOI 10.1257/000282805774669637; Abdulkadiroglu A, 2005, AM ECON REV, V95, P364, DOI 10.1257/000282805774670167; Abdulkadiroglu A, 2003, AM ECON REV, V93, P729, DOI 10.1257/000282803322157061; Agoston KC, 2016, J COMB OPTIM, V32, P1371, DOI 10.1007/s10878-016-0085-x; Aldershof B., 1999, Constraints, V4, P281, DOI 10.1023/A:1026453915989; Antonio R. M., 1998, REV ECON DES, V3, P137; Balinski M, 1999, J ECON THEORY, V84, P73, DOI 10.1006/jeth.1998.2469; Biro P, 2015, CENT EUR J OPER RES, V23, P727, DOI 10.1007/s10100-013-0320-9; Biro Peter, 2008, TR2008291 U GLASG; Braun S, 2010, BE J ECON ANAL POLI, V10; Brozovsky Lukas, 2007, P C ZNAL 2007 OSTR; Dworczak P., 2019, COMMUNICATION; Dworczak P, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P65, DOI 10.1145/2940716.2940727; Everaere Patricia, 2013, AAMAS; FEDER T, 1992, J COMPUT SYST SCI, V45, P233, DOI 10.1016/0022-0000(92)90048-N; Feder Tomas, 1995, STABLE NETWORKS PROD, V555; GALE D, 1962, AM MATH MON, V69, P9, DOI 10.2307/2312726; Giannakopoulos I, 2015, PROC INT C TOOLS ART, P989, DOI 10.1109/ICTAI.2015.142; GUSFIELD D, 1987, SIAM J COMPUT, V16, P111, DOI 10.1137/0216010; Gusfield D., 1989, STABLE MARRIAGE PROB; Hassidim Avinatan, 2017, NEED VS MERIT LARGE; Viet HH, 2016, 2016 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTING AND APPLICATIONS (ACOMP), P18, DOI [10.1109/ACOMP.2016.17, 10.1109/ACOMP.2016.013]; Viet HH, 2016, LECT NOTES COMPUT SC, V9810, P556, DOI 10.1007/978-3-319-42911-3_46; Irving Robert W., 2016, ENCY ALGORITHMS, P1470; IRVING RW, 1987, J ACM, V34, P532, DOI 10.1145/28869.28871; IRVING RW, 1986, SIAM J COMPUT, V15, P655, DOI 10.1137/0215048; Iwama K, 2010, ACM T ALGORITHMS, V7, DOI 10.1145/1868237.1868239; Iwama Kazuo, 1999, ICALP; Kato A., 1993, JAPAN J IND APPL MAT, V10, P1, DOI DOI 10.1007/BF03167200; Klaus B, 2006, ECON THEOR, V27, P431, DOI 10.1007/s00199-004-0602-5; Klaus B, 2009, INT GAME THEORY REV, V11, P181, DOI 10.1142/S0219198909002248; Knuth DE, 1997, AM MATH SOC; KNUTH DE, 1976, MARIAGES STABLES LEU; Liebowitz J., 2005, Electronic Government, V2, P384, DOI 10.1504/EG.2005.008330; Manlove D.F., 2013, ALGORITHMICS MATCHIN; McDermid E, 2014, ALGORITHMICA, V68, P545, DOI 10.1007/s00453-012-9672-0; MCVITIE DG, 1971, COMMUN ACM, V14, P486, DOI 10.1145/362619.362631; Piette E, 2013, SEPT JOURN FRANC MOD; Pini MS, 2011, AUTON AGENT MULTI-AG, V22, P183, DOI 10.1007/s10458-010-9121-x; Romero-Medina A, 2005, THEOR DECIS, V58, P305, DOI 10.1007/s11238-005-6846-0; Romero-Medina A, 2001, THEOR DECIS, V50, P197, DOI 10.1023/A:1010311325241; Roth A. E., 2018, COMMUNICATION; Roth A. E., 2007, 13225 NAT BUR EC RES; ROTH AE, 1984, J POLIT ECON, V92, P991, DOI 10.1086/261272; Sotomayor M, 1990, ECONOMETRIC SOC MONO, V18; Teo CP, 2001, MANAGE SCI, V47, P1252, DOI 10.1287/mnsc.47.9.1252.9784	46	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300042
C	Uesato, J; Alayrac, JB; Huang, PS; Stanforth, R; Fawzi, A; Kohli, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Uesato, Jonathan; Alayrac, Jean-Baptiste; Huang, Po-Sen; Stanforth, Robert; Fawzi, Alhussein; Kohli, Pushmeet			Are Labels Required for Improving Adversarial Robustness?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR-10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7% over using 4K supervised examples alone, and captures over 95% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4% over the previous state-of-the-art on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.	[Uesato, Jonathan; Alayrac, Jean-Baptiste; Huang, Po-Sen; Stanforth, Robert; Fawzi, Alhussein; Kohli, Pushmeet] DeepMind, London, England		Uesato, J (corresponding author), DeepMind, London, England.	juesato@google.com; jalayrac@google.com; posenhuang@google.com						Athalye A., 2018, P 35 INT C MACH LEAR; Attias I., 2018, ALT, P2; Bachman Phil, 2014, NEURIPS; Berthelot D., 2019, ARXIV190502249; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bojarski M., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/IVS.2017.7995975; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carmon Yair, 2019, NEURIPS; Chapelle O., 2009, SEMISUPERVISED LEARN; Chen D.-D., 2018, IJCAI; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; De Fauw J, 2018, NAT MED, V24, P1342, DOI 10.1038/s41591-018-0107-6; Douze M, 2009, P ACM INT C IM VID R; Finlayson S. G., 2019, SCIENCE, V9; Goodfellow I. J., 2014, ARXIV14126572; Gowal S., 2019, ALTERNATIVE SURROGAT, p[5, 15]; Gowal Sven, 2018, EFFECTIVENESS INTERV; Gu S., 2014, ARXIV14125068; Hendrycks Dan, 2019, ICML; Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333; Khiim J., 2018, ARXIV181009519, P2; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kurakin A., 2017, ICLR; Lane S., 2017, ICLR, P3; Laurent B, 2000, ANN STAT, V28, P1302; Liu Yanpei, 2016, ARXIV161102770; Madry Aleksander, 2018, ICLR; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903079
C	Vanunts, A; Drutsa, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vanunts, Arsenii; Drutsa, Alexey			Optimal Pricing in Repeated Posted-Price Auctions with Different Patience of the Seller and the Buyer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DYNAMIC MECHANISM; COMMITMENT; DESIGN; AGENT	We study revenue optimization pricing algorithms for repeated posted-price auctions where a seller interacts with a single strategic buyer that holds a fixed private valuation. When the participants non-equally discount their cumulative utilities, we show that the optimal constant pricing (which offers the Myerson price) is no longer optimal. In the case of more patient seller, we propose a novel multidimensional optimization functional - a generalization of the one used to determine Myerson's price. This functional allows to find the optimal algorithm and to boost revenue of the optimal static pricing by an efficient low-dimensional approximation. Numerical experiments are provided to support our results.	[Vanunts, Arsenii; Drutsa, Alexey] Yandex, Moscow, Russia; [Drutsa, Alexey] MSU, Moscow, Russia	Lomonosov Moscow State University	Vanunts, A (corresponding author), Yandex, Moscow, Russia.	avanunts@yandex.ru; adrutsa@yandex.ru						Agarwal D, 2014, P 20 ACM SIGKDD INT, P1613; Aggarwal G, 2006, P ACM C ELECT COMMER, V7, P1; Aggarwal G., 2009, P 18 INT C WORLD WID, P241, DOI 10.1145/1526709.1526742; Aggarwal G, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P235; Amin K., 2011, COLT; Amin K, 2012, P 28 C UNC ART INT; Amin K., 2014, NIPS 2014; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; [Anonymous], 2007, ALG GAM THEOR; [Anonymous], 2009, AUCTION THEORY; Ashlagi I., 2013, WORKING PAPER; Ashlagi Itai, 2016, EC 2016; Babaioff Moshe, 2015, ACM Transactions on Economics and Computation, V3, DOI 10.1145/2559152; Bachrach Yoram, 2014, P 15 ACM C EC COMP, P75, DOI DOI 10.1145/2600057.2602879; Balseiro S., 2016, EC 2016; Balseiro SR, 2015, MANAGE SCI, V61, P864, DOI 10.1287/mnsc.2014.2022; Bergemann D., 2010, WILEY ENCY OPERATION; BESANKO D, 1985, ECON LETT, V17, P33, DOI 10.1016/0165-1765(85)90122-3; Bester H, 2001, ECONOMETRICA, V69, P1077, DOI 10.1111/1468-0262.00231; Borgs C., 2005, P 6 ACM C EL COMM, P44; Borgs C, 2007, P 16 INT C WORLD WID, P531, DOI DOI 10.1145/1242572.1242644; Celis L.E., 2011, P 20 INT C WORLD WID, P147; Cesa-Bianchi N, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1190; Charles D, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P417, DOI 10.1145/2835776.2835822; Chawla S, 2016, P 27 ANN ACM SIAM S, P1476; Chen Y, 2015, International Conference on Mechanics, Building Material and Civil Engineering (MBMCE 2015), P771; Chhabra M., P 10 INT C AUT AG MU, VVolume 1, P63; Cohen M. C., 2016, EC 2016; den Boer A.V., 2015, SURV OPER RES MANAG, V20, P1; Devanur N. R., 2015, SODA 2015; DRUTSA A, 2017, CORR; Drutsa A., 2019, CORR; Drutsa A, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P33, DOI 10.1145/3038912.3052700; Drutsa Alexey, 2018, INT C MACH LEARN, P1318; Dutting P., 2011, P 20 INT C WORLD WID, P127; Edelman B, 2007, AM ECON REV, V97, P242, DOI 10.1257/aer.97.1.242; Edelman B, 2007, DECIS SUPPORT SYST, V43, P192, DOI 10.1016/j.dss.2006.08.008; Feldman M., 2016, ADV NEURAL INFORM PR, P3864; Goel G., 2014, WWW 2014; Goel Gagan., 2015, P 16 ACM C EC COMP, P149; Gomes R, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P19, DOI 10.1145/2566486.2568029; Gonen R, 2007, PODC'07: PROCEEDINGS OF THE 26TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P362; Greenwald A., 2012, NIPS 2012, P2321; HART OD, 1988, REV ECON STUD, V55, P509, DOI 10.2307/2297403; He D., 2013, IJCAI 2013, P206; Heavlin W, 2012, P 21 INT C WORLD WID, P91; Heidari H, 2016, PR MACH LEARN RES, V48; Hummel P, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P7; Immorlica N, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P167, DOI 10.1145/3033274.3085130; Iyer K, 2011, ACM SIGECOM EXCH, V10, P10; Kakade SM, 2013, OPER RES, V61, P837, DOI 10.1287/opre.2013.1194; Kanoria Y., 2017, SSRN; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; Leme Renato Paes, 2016, WWW 2016; Lin T., 2015, NIPS 2015; Lucier B., 2012, P 21 WORLD WIDE WEB, P361; Medina A. M., 2017, NIPS 2017; Mehta A, 2005, ANN IEEE SYMP FOUND, P264, DOI 10.1109/SFCS.2005.12; Michael Ostrovsky, 2011, P 12 ACM C ELECT COM, P59, DOI DOI 10.1145/1993574.1993585; Mirrokni Vahab, 2017, NONCLAIRVOYANT DYNAM; Mohri M., 2016, J MACHINE LEARNING R, V17, P1; MOHRI M, 2015, ADV NEURAL INFORM PR, P2530; Mohri M., 2015, UAI 2015; Mohri M, 2014, PR MACH LEARN RES, V32; Mohri Mehryar, 2014, ADV NEURAL INFORM PR, P1871; Morgenstern J. H., 2015, NIPS 2015; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Noti G, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P619, DOI 10.1145/2566486.2568004; Pandey S., 2007, P 19 ADV NEUR INF PR, P1065; Pavan A., 2009, DYNAMIC MECH DESIGN; Pavan A, 2014, ECONOMETRICA, V82, P601, DOI 10.3982/ECTA10269; Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P601; Rudolph MR, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1113, DOI 10.1145/2872427.2883051; SCHMIDT KM, 1993, J ECON THEORY, V60, P114, DOI 10.1006/jeth.1993.1037; Sun Y, 2014, ELECTRON COMMER R A, V13, P178, DOI 10.1016/j.elerap.2014.02.003; Thompson D. R. M., 2013, P 14 ACM C EL COMM, P837; Vainsencher D., 2011, P 28 INT C MACH LEAR, P1137; Varian HR, 2007, INT J IND ORGAN, V25, P1163, DOI 10.1016/j.ijindorg.2006.10.002; Varian HR, 2014, AM ECON REV, V104, P442, DOI 10.1257/aer.104.5.442; Varian HR, 2009, AM ECON REV, V99, P430, DOI 10.1257/aer.99.2.430; VICKREY W, 1961, J FINANC, V16, P8, DOI 10.2307/2977633; Wang Z, 2016, SSRN; Weed  J., 2016, JMLR, V49, P1; Yuan S, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1897, DOI 10.1145/2623330.2623357; Zhu YZ, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P588, DOI 10.1145/1571941.1572042; Zoghi M., 2015, NIPS 2015	86	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300085
C	Vitale, F; Rajagopalan, A; Gentile, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vitale, Fabio; Rajagopalan, Anand; Gentile, Claudio			Flattening a Hierarchical Clustering through Active Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	We investigate active learning by pairwise similarity over the leaves of trees originating from hierarchical clustering procedures. In the realizable setting, we provide a full characterization of the number of queries needed to achieve perfect reconstruction of the tree cut. In the non-realizable setting, we rely on known important-sampling procedures to obtain regret and query complexity bounds. Our algorithms come with theoretical guarantees on the statistical error and, more importantly, lend themselves to linear-time implementations in the relevant parameters of the problem. We discuss such implementations, prove running time guarantees for them, and present preliminary experiments on real-world datasets showing the compelling practical performance of our algorithms as compared to both passive learning and simple active learning baselines.	[Vitale, Fabio] INRIA Lille, Dept Comp Sci, Lille, France; [Vitale, Fabio] Sapienza Univ Rome, Rome, Italy; [Rajagopalan, Anand; Gentile, Claudio] Google Res NY, New York, NY USA	Sapienza University Rome; Google Incorporated	Vitale, F (corresponding author), INRIA Lille, Dept Comp Sci, Lille, France.; Vitale, F (corresponding author), Sapienza Univ Rome, Rome, Italy.	fabio.vitale@inria.fr; anandbr@google.com; cgentile@google.com			Google Focused Award "ALL4AI"; ERC Starting Grant [DMAP 680153]	Google Focused Award "ALL4AI"(Google Incorporated); ERC Starting Grant(European Research Council (ERC))	Fabio Vitale acknowledges support from the Google Focused Award "ALL4AI" and the ERC Starting Grant "DMAP 680153", awarded to the Department of Computer Science of Sapienza University.	ARKIN EM, 1993, P 9 ANN S COMP GEOM, P369; Ashtiani H., 2016, P 30 NIPS; Awasthi P, 2017, J MACH LEARN RES, V18; Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27; Beygelzimer A., 2009, P 26 ANN INT C MACH, P49; Beygelzimer A., 2010, ADV NEURAL INFORM PR, V23, P199; Chen Y., 2015, C LEARN THEOR, P338; Chen YC, 2017, ASPA SER PUB ADM PUB, P20; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Cortes C, 2019, PR MACH LEARN RES, V89; Dasgupta S, 2008, P 25 INT C MACH LEAR; Dasgupta S., 2005, ADV NEURAL INFORM PR, V18, P235; Davidson S, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2684066; Firmani D, 2016, PROC VLDB ENDOW, V9, P384; Golovin Daniel, 2017, ARXIV10033967; Gonen A, 2013, J MACH LEARN RES, V14, P2583; Hanneke S., 2007, P 24 INT C MACH LEAR, P353, DOI [10.1145/1273496.1273541, DOI 10.1145/1273496.1273541]; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; HAREL D, 1984, SIAM J COMPUT, V13, P338, DOI 10.1137/0213024; Kosaraju SR, 1999, LECT NOTES COMPUT SC, V1663, P157; Kpotufe Samory, 2015, C LEARN THEOR, P1176; Mazumdar A., 2017, ARXIV170607510VL; Meila M, 2012, MACH LEARN, V86, P369, DOI 10.1007/s10994-011-5267-2; Mussmann Stephen, 2018, P 21 INT C ART INT S; Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Tosh C., 2017, 24 INT 34 INT C MACH; Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982; Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906090
C	Wang, C; Hu, H; Lu, YM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Chuang; Hu, Hong; Lu, Yue M.			A Solvable High-Dimensional Model of GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a theoretical analysis of the training process for a single-layer GAN fed by high-dimensional input data. The training dynamics of the proposed model at both microscopic and macroscopic scales can be exactly analyzed in the high-dimensional limit. In particular, we prove that the macroscopic quantities measuring the quality of the training process converge to a deterministic process characterized by an ordinary differential equation (ODE), whereas the microscopic states containing all the detailed weights remain stochastic, whose dynamics can be described by a stochastic differential equation (SDE). This analysis provides a new perspective different from recent analyses in the limit of small learning rate, where the microscopic state is always considered deterministic, and the contribution of noise is ignored. From our analysis, we show that the level of the background noise is essential to the convergence of the training process: setting the noise level too strong leads to failure of feature recovery, whereas setting the noise too weak causes oscillation. Although this work focuses on a simple copy model of GAN, we believe the analysis methods and insights developed here would prove useful in the theoretical understanding of other variants of GANs with more advanced training algorithms.	[Wang, Chuang] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, 95 Zhong Guan Cun Dong Lu, Beijing 100190, Peoples R China; [Wang, Chuang; Hu, Hong; Lu, Yue M.] Harvard Univ, John A Paulson Sch Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA	Chinese Academy of Sciences; Institute of Automation, CAS; Harvard University	Wang, C (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, 95 Zhong Guan Cun Dong Lu, Beijing 100190, Peoples R China.; Wang, C (corresponding author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA.	wangchuang@ia.ac.cn; honghu@g.harvard.edu; yuelu@seas.harvard.edu			US Army Research Office [W911NF-16-1-0265]; US National Science Foundation [CCF-1319140, CCF-1718698, CCF-1910410]	US Army Research Office; US National Science Foundation(National Science Foundation (NSF))	This work was supported by the US Army Research Office under contract W911NF-16-1-0265 and by the US National Science Foundation under grants CCF-1319140, CCF-1718698, and CCF-1910410.	Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; BIEHL M, 1995, J PHYS A-MATH GEN, V28, P643, DOI 10.1088/0305-4470/28/3/018; Chhabra A, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON COLLABORATION AND INTERNET COMPUTING (CIC), P243, DOI 10.1109/CIC.2017.00040; Chuang Wang, 2016, 2016 IEEE Information Theory Workshop (ITW), P186, DOI 10.1109/ITW.2016.7606821; Feizi S., 2017, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hensel M, 2017, ADV NEUR IN, V30; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121; Jourdain B, 2014, BERNOULLI, V20, P1930, DOI 10.3150/13-BEJ546; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li J, 2017, ARXIV170609884; Mazumdar Eric V, 2019, ARXIV190100838; Mei S., 2018, ARXIV180406561; Mescheder L, 2018, PR MACH LEARN RES, V80; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Nagarajan V, 2017, ADV NEUR IN, V30; Nguyen P-M., 2019, MEAN FIELD LIMIT LEA; Reed S, 2016, PR MACH LEARN RES, V48; Roberts GO, 1997, ANN APPL PROBAB, V7, P110, DOI 10.1214/aoap/1034625254; SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337; Sonderby C.K., 2017, ICLR, P1; Wang C, 2017, ARXIV171204332; Wang C., 2017, ADV NEURAL INFORM PR, P6641; Wang C, 2018, IEEE J-STSP, V12, P1240, DOI 10.1109/JSTSP.2018.2877405	27	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905043
C	Wang, Z; Ji, KY; Zhou, Y; Liang, YB; Tarokh, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Zhe; Ji, Kaiyi; Zhou, Yi; Liang, Yingbin; Tarokh, Vahid			SpiderBoost and Momentum: Faster Stochastic Variance Reduction Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					SARAH and SPIDER are two recently developed stochastic variance-reduced algorithms, and SPIDER has been shown to achieve a near-optimal first-order oracle complexity in smooth nonconvex optimization. However, SPIDER uses an accuracy-dependent stepsize that slows down the convergence in practice, and cannot handle objective functions that involve nonsmooth regularizers. In this paper, we propose SpiderBoost as an improved scheme, which allows to use a much larger constant-level stepsize while maintaining the same near-optimal oracle complexity, and can be extended with proximal mapping to handle composite optimization (which is nonsmooth and nonconvex) with provable convergence guarantee. In particular, we show that proximal SpiderBoost achieves an oracle complexity of O(min{n(1/2) epsilon(-2), epsilon(-3)}) in composite nonconvex optimization, improving the state-of-the-art result by a factor of O(min{n(1/6), epsilon(-1/3)}). We further develop a novel momentum scheme to accelerate SpiderBoost for composite optimization, which achieves the near-optimal oracle complexity in theory and substantial improvement in experiments.	[Wang, Zhe; Ji, Kaiyi; Liang, Yingbin] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA; [Zhou, Yi] Univ Utah, Dept ECE, Salt Lake City, UT 84112 USA; [Tarokh, Vahid] Duke Univ, Dept ECE, Durham, NC 27706 USA	University System of Ohio; Ohio State University; Utah System of Higher Education; University of Utah; Duke University	Wang, Z (corresponding author), Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.	wang.10982@osu.edu; ji.367@osu.edu; yi.zhou@utah.edu; liang.889@osu.edu; vahid.tarokh@duke.edu	Poor, H. Vincent/S-5027-2016; 哲, 王/GYJ-1551-2022	Poor, H. Vincent/0000-0002-2062-131X; 	U.S. National Science Foundation [CCF-1761506, CCF-1909291, CCF-1900145]	U.S. National Science Foundation(National Science Foundation (NSF))	The work of Z. Wang, K. Ji, and Y. Liang was supported in part by the U.S. National Science Foundation under the grants CCF-1761506, CCF-1909291, and CCF-1900145.	Allen-Zhu Z., 2018, P INT C MACH LEARN I, V80, P179; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; [Anonymous], 2018, APPL COMPUTATIONAL H; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Fang C, 2018, ADV NEUR IN, V31; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Huang F., 2019, P INT C INT C MACH L; Huang F., 2019, ARXIV190713463; Ji K., 2019, FASTER STOCHASTIC AL; Ji KY, 2019, PR MACH LEARN RES, V97; Jin, 2018, P MACH LEARN RES, P815; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Lei LH, 2017, ADV NEUR IN, V30; Li QW, 2017, PR MACH LEARN RES, V70; Li Z., 2019, ARXIV190409265; Li Z., 2018, P ADV NEUR INF PROC; Nesterov Y., 2004, APPL OPTIM; Nguyen L. M., 2017, ARXIV170507261; Nguyen LM, 2017, PR MACH LEARN RES, V70; Nguyen Lam M., 2019, ARXIV190107648; Nitanda A, 2016, JMLR WORKSH CONF PRO, V51, P195; Pham N. H., 2019, ARXIV190205679; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi SJ, 2016, PR MACH LEARN RES, V48; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Xu Y., 2019, ARXIV190207672; Zhang J., 2018, ARXIV81104194; Zhang Junyu, 2019, ARXIV190610186; Zhou DR, 2018, ADV NEUR IN, V31; Zhou M, 2018, 2018 IEEE 29TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS (PIMRC), P1242; Zhou P., 2019, P INT C ART INT STAT; Zhou Y, 1965, CHARACTERIZATION GRA; Zhou Y, 2016, ANN ALLERTON CONF, P331, DOI 10.1109/ALLERTON.2016.7852249	40	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302041
C	Weber, RS; Eyal, M; Detlefsen, NS; Shriki, O; Freifeld, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Weber, Ron Shapira; Eyal, Matan; Detlefsen, Nicki Skafte; Shriki, Oren; Freifeld, Oren			Diffeomorphic Temporal Alignment Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Time-series analysis is confounded by nonlinear time warping of the data. Traditional methods for joint alignment do not generalize: after aligning a given signal ensemble, they lack a mechanism, that does not require solving a new optimization problem, to align previously-unseen signals. In the multi-class case, they must also first classify the test data before aligning it. Here we propose the Diffeomorphic Temporal Alignment Net (DTAN), a learning-based method for time-series joint alignment. Via flexible temporal transformer layers, DTAN learns and applies an input-dependent nonlinear time warping to its input signal. Once learned, DTAN easily aligns previously-unseen signals by its inexpensive forward pass. In a single-class case, the method is unsupervised: the ground-truth alignments are unknown. In the multi-class case, it is semi-supervised in the sense that class labels (but not the ground-truth alignments) are used during learning; in test time, however, the class labels are unknown. As we show, DTAN not only outperforms existing joint-alignment methods in aligning training data but also generalizes well to test data. Our code is available at https://github.com/BGU-CS-VIL/dtan.	[Weber, Ron Shapira; Eyal, Matan; Shriki, Oren; Freifeld, Oren] Ben Gurion Univ Negev, Beer Sheva, Israel; [Detlefsen, Nicki Skafte] Tech Univ Denmark, Lyngby, Denmark	Ben Gurion University; Technical University of Denmark	Weber, RS (corresponding author), Ben Gurion Univ Negev, Beer Sheva, Israel.	ronsha@post.bgu.ac.il; mataney@post.bgu.ac.il; nsde@dtu.dk; shrikio@bgu.ac.il; orenfr@cs.bgu.ac.il	Shriki, Oren/R-8638-2019	Shriki, Oren/0000-0003-1129-4799	VILLUM FONDEN [15334]	VILLUM FONDEN(Villum Fonden)	NSD was supported by research grant #15334 from the VILLUM FONDEN.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Abid A., 2018, ARXIV181010107; Allassonniere S., 2015, SIAM J IMAGING SCI; [Anonymous], [No title captured]; [Anonymous], 2010, PATTERN THEORY STOCH; Arsigny V., 2006, BIR; Bagnall A., 2014, ARXIV14064757; Bahdanau D., 2015, P 3 INT C LEARNING R; Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964; Chen Y, 2015, UCR TIME SERIES CLAS; Cox M., 2008, P IEEE C COMP VIS PA, P1; Cox M, 2009, IEEE I CONF COMP VIS, P1949, DOI 10.1109/ICCV.2009.5459430; Cuturi M, 2017, PR MACH LEARN RES, V70; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Dalca A. V., 2019, ADV NEURAL INFORM PR; de Vos BD, 2017, LECT NOTES COMPUT SC, V10553, P204, DOI 10.1007/978-3-319-67558-9_24; Detlefsen N. S., 2018, CVPR; Detlefsen N. S., 2018, LIBCPAB; Durrleman S., 2013, IJCV; Freifeld O., 2017, IEEE TPAMI; Freifeld O., 2015, ICCV; Freifeld O., 2018, TECHNICAL REPORT; Grabocka J., 2018, ARXIV181208306; Gupta L, 1996, IEEE T BIO-MED ENG, V43, P348, DOI 10.1109/10.486255; Gusfield D., 1997, ALGORITHMS STRINGS T; Hauberg S., 2016, AISTATS; Huang G., 2012, ADV NEURAL INFORM PR, P764; Huang GB, 2007, IEEE I CONF COMP VIS, P237, DOI 10.1109/iccv.2007.4408858; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Joshi S, 2004, NEUROIMAGE, V23, pS151, DOI 10.1016/j.neuroimage.2004.07.068; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kingma D.P., 2015, 3 INT C LEARN REPR I, P1, DOI DOI 10.1007/S11390-017-1754-7; Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34; Lin C.-H., 2017, IEEE C COMP VIS PATT, P2568; Listgarten J., 2005, P 18 ANN C NEUR INF, P817; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Lohit S., 2019, P IEEE C COMP VIS PA, P12426; Mattar MA, 2009, INT CONF ACOUST SPEE, P3457, DOI 10.1109/ICASSP.2009.4960369; Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856; Nair V., 2010, ICML, P807; O'Shea TJ, 2016, CONF REC ASILOMAR C, P662, DOI 10.1109/ACSSC.2016.7869126; Oh J., 2018, ARXIV180806725; Petitjean F, 2014, IEEE DATA MINING, P470, DOI 10.1109/ICDM.2014.27; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rumelhart D. E., 1985, TECHNICAL REPORT; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Sakoe H., 1971, 1971 P INT C AC BUD; SUN G, 1993, ADV NEURAL INFORMATI, V5, P180; Tavenard R., 2017, TSLEARN MACHINE LEAR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang L., 1994, J COMPUTATIONAL BIOL; WIGLEY TML, 1984, J CLIM APPL METEOROL, V23, P201, DOI 10.1175/1520-0450(1984)023<0201:OTAVOC>2.0.CO;2; Xi X., 2006, P 23 INT C MACHINE L, P1033, DOI 10.1145/1143844.1143974; Xiong JX, 2015, Proceedings of the 11th Euro-Asia Conference on Environment and CSR: Tourism, Society and Education Session (Part II), P2; Zhang Miaomiao, 2018, IJCV; Zhang T, 2018, INTERSPEECH, P1349, DOI 10.21437/Interspeech.2018-1152	57	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306056
C	Weiler, M; Cesa, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Weiler, Maurice; Cesa, Gabriele			General E(2) - Equivariant Steerable CNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of E(2)-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group E(2) and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. E(2)-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as drop in replacement for non-equivariant convolutions.	[Weiler, Maurice] Univ Amsterdam, QUVA Lab, Amsterdam, Netherlands; [Cesa, Gabriele] Univ Amsterdam, Amsterdam, Netherlands	University of Amsterdam; University of Amsterdam	Weiler, M (corresponding author), Univ Amsterdam, QUVA Lab, Amsterdam, Netherlands.	m.weiler@uva.nl; cesa.gabriele@gmail.com						Anderson B., 2019, ADV NEURAL INFORM PR, V2019, P14510; Bekkers Erik J., 2018, INT C MED IM COMP CO; Boscaini Davide, 2015, COMPUTER GRAPHICS FO; Bruna J, 2013, PROC INT C LEARN REP; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Cohen Taco, 2018, ARXIV181102017; Cohen TS, 2019, PR MACH LEARN RES, V97; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen Taco S., 2017, 5 INT C LEARN REPR I, P2; Cohen Taco S, 2018, ARXIV180310743; Cohen Taco S, 2018, ICLR; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Diaconu N, 2019, PR MACH LEARN RES, V97; Dieleman S, 2016, PR MACH LEARN RES, V48; Djork-Arn, ICLR 2016; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Hinton Geoffrey E., 2018, INT C LEARN REPR; Hoogeboom E., 2018, INT C LEARN REPR; Jiang C.M., 2019, PROC INT C LEARN REP, P1; Kondor R., 2018, ARXIV PREPRINT ARXIV; Kondor R, 2018, PR MACH LEARN RES, V80; Kondor Risi, 2018, C NEUR INF PROC SYST; Laptev D, 2016, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.2016.38; Mallat S., 2012, ESANN, V44, P68; Marcos D, 2018, ISPRS J PHOTOGRAMM, V145, P96, DOI 10.1016/j.isprsjprs.2018.01.021; Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540; Marcos D, 2016, INT C PATT RECOG, P2012, DOI 10.1109/ICPR.2016.7899932; Masci J., 2015, P IEEE INT C COMP VI, P37; Oyallon Edouard, 2015, C COMP VIS PATT REC; Perraudin Nathanael, 2018, ARXIV181012186ASTROP; Poulenard A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275102; Sabour Sara, 2017, PROC 31 INT C NEURAL; Serre J-P, 1977, GRADUATE TEXTS MATH, V42; Sifre Laurent, 2014, ARXIV14031687; Sifre Laurent, 2013, C COMP VIS PATT REC; Thomas Nathaniel, 2018, ARXIV180208219; Veeling BS, 2018, LECT NOTES COMPUT SC, V11071, P210, DOI 10.1007/978-3-030-00934-2_24; Weiler M., 2018, C COMP VIS PATT REC; Weiler Maurice, 2018, C NEUR INF PROC SYST; Winkels Marysia, 2018, C MED IM DEEP LEARN; Worrall D, 2018, LECT NOTES COMPUT SC, V11209, P585, DOI 10.1007/978-3-030-01228-1_35; Worrall Daniel E., 2017, C COMP VIS PATT REC; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	45	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906006
C	Wendler, C; Alistarh, D; Puschel, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wendler, Chris; Alistarh, Dan; Puschel, Markus			Powerset Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIGNAL-PROCESSING THEORY	We present a novel class of convolutional neural networks (CNNs) for set functions, i.e., data indexed with the powerset of a finite set. The convolutions are derived as linear, shift-equivariant functions for various notions of shifts on set functions. The framework is fundamentally different from graph convolutions based on the Laplacian, as it provides not one but several basic shifts, one for each element in the ground set. Prototypical experiments with several set function classification tasks on synthetic datasets and on datasets derived from real-world hypergraphs demonstrate the potential of our new powerset CNNs.	[Wendler, Chris; Puschel, Markus] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Alistarh, Dan] IST Austria, Klosterneuburg, Austria	Swiss Federal Institutes of Technology Domain; ETH Zurich; Institute of Science & Technology - Austria	Wendler, C (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	chris.wendler@inf.ethz.ch; dan.alistarh@ist.ac.at; pueschel@inf.ethz.ch		Alistarh, Dan/0000-0003-3650-940X	European Research Council (ERC) under the European Union [805223]	European Research Council (ERC) under the European Union(European Research Council (ERC))	We thank Max Horn for insightful discussions and his extensive feedback, and Razvan Pascanu for feedback on an earlier draft. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 805223).	Abadi M, 2015, P 12 USENIX S OPERAT; Badanidiyuru A., 2012, SODA; Balcan M. F., 2012, LEARNING VALUATION F; Balcan MF, 2011, ACM S THEORY COMPUT, P793; Benson AR, 2018, P NATL ACAD SCI USA, V115, pE11221, DOI 10.1073/pnas.1800683115; Berge C., 1973, GRAPHS HYPERGRAPHS; Bilmes J., 2017, ARXIV170108939; Branzei R, 2008, MODELCOOPERATIVE G, V556; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Choi SS, 2011, J COMPUT SYST SCI, V77, P1039, DOI 10.1016/j.jcss.2010.08.011; Cohen T., 2017, ARXIV170904893; Cohen TS, 2016, PR MACH LEARN RES, V48; de Vries S, 2003, INFORMS J COMPUT, V15, P284, DOI 10.1287/ijoc.15.3.284.16077; de Wolf R, 2008, THEORY COMPUT, V1, P1; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Djolonga J, 2017, ADV NEUR IN, V30; Gilmer J, 2017, PR MACH LEARN RES, V70; Golovin D., 2014, ARXIV14071082; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hackel T., 2018, PROC GERMAN C PATTER, P597; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Krause A., 2007, AAAI, V7, P1650; Krause A, 2014, TRACTABILITY, P71; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lu Y, 2016, PROCEEDINGS OF 2016 FUTURE TECHNOLOGIES CONFERENCE (FTC), P1230, DOI 10.1109/FTC.2016.7821757; Mossel E., 2003, P 35 ACM S THEOR COM, P206; Murphy R. L., 2018, ARXIV181101900; Nair V, 2010, P 27 INT C MACHINE L, P807; ODonnell Ryan, 2014, ANAL BOOLEAN FUNCTIO; Osokin A, 2015, IEEE T PATTERN ANAL, V37, P1347, DOI 10.1109/TPAMI.2014.2369046; Puschel M, 2008, IEEE T SIGNAL PROCES, V56, P3572, DOI 10.1109/TSP.2008.925261; Puschel M, 2007, IEEE T IMAGE PROCESS, V16, P1506, DOI 10.1109/TIP.2007.896626; Puschel M, 2019, INT CONF ACOUST SPEE, P5371, DOI 10.1109/ICASSP.2019.8682729; Puschel M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4359; Ravanbakhsh S, 2017, PR MACH LEARN RES, V70; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rosenblatt Frank, 1961, TECHNICAL REPORT, DOI DOI 10.1001/ARCHPSYC.1962.01720030064010; Sandryhaila A, 2014, IEEE T SIGNAL PROCES, V62, P3042, DOI 10.1109/TSP.2014.2321121; Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935; Sandryhaila A, 2012, IEEE T SIGNAL PROCES, V60, P2247, DOI 10.1109/TSP.2012.2186133; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Scheibler R, 2015, IEEE T INFORM THEORY, V61, P2115, DOI 10.1109/TIT.2015.2404441; Seifert B, 2019, INT CONF ACOUST SPEE, P5023, DOI 10.1109/ICASSP.2019.8682222; Selsam D., 2018, ARXIV180203685; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41; Stankovic RS, 2005, FOURIER ANALYSIS ON FINITE GROUPS WITH APPLICATIONS IN SIGNAL PROCESSING AND SYSTEM DESIGN, P1; Stobbe Peter, 2012, P 15 INT C ART INT S, P1125; Such FP, 2017, IEEE J-STSP, V11, P884, DOI 10.1109/JSTSP.2017.2726981; Sutton AM, 2012, THEOR COMPUT SCI, V425, P58, DOI 10.1016/j.tcs.2011.02.006; Tschiatschek S, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2731; Wang H., 2018, C ART INT; Wang PW, 2019, PR MACH LEARN RES, V97; Wendler C, 2019, IEEE GLOB CONF SIG; Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391	62	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300084
C	Wenliang, LK; Sahani, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wenliang, Li Kevin; Sahani, Maneesh			A neurally plausible model for online recognition and postdiction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Humans and other animals are frequently near-optimal in their ability to integrate noisy and ambiguous sensory data to form robust percepts, which are informed both by sensory evidence and by prior experience about the causal structure of the environment. It is hypothesized that the brain establishes these structures using an internal model of how the observed patterns can be generated from relevant but unobserved causes. In dynamic environments, such integration often takes the form of postdiction, wherein later sensory evidence affects inferences about earlier percepts. As the brain must operate in current time, without the luxury of acausal propagation of information, how does such postdictive inference come about? Here, we propose a general framework for neural probabilistic inference in dynamic models based on the distributed distributional code (DDC) representation of uncertainty, naturally extending the underlying encoding to incorporate implicit probabilistic beliefs about both present and past. We show that, as in other uses of the DDC, an inferential model can be learned efficiently using samples from an internal model of the world. Applied to stimuli used in the context of psychophysics experiments, the framework provides an online and plausible mechanism for inference, including postdictive effects.	[Wenliang, Li Kevin; Sahani, Maneesh] UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England	University of London; University College London	Wenliang, LK (corresponding author), UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England.	kevinli@gatsby.ucl.ac.uk; maneesh@gatsby.ucl.ac.uk			Gatsby Charitable Foundation	Gatsby Charitable Foundation	This work is supported by the Gatsby Charitable Foundation.	Alais D., 2004, CURRENT BIOL; Amigoni F., 2003, AMS C ART INT APPL E; [Anonymous], 2014, ICLR; Battaglia P. W., 2003, J OPT SOC AM A; Beck J., 2007, PROGR BRAIN RES; Beierholm U., 2008, NEURIPS; Bregman A.S., 1994, AUDITORY SCENE ANAL; Charles A. S., 2017, JMLR; choi hoon h., 2006, PERCEPTION; Churchland A. K., 2011, NEURON; Churchland MM, 2010, NATURE NEUROSCIENCE; Dayan P., 1995, NEURAL COMPUTATION; de Xivry J.-J. O., 2013, J NEUROSCIENCE; Deneve S., 2007, J NEUROSCIENCE; Eagleman D. M., 2000, SCIENCE; Eliasmith C., 2004, NEURAL ENG COMPUTATI; Ernst M. O., 2002, NATURE; Funamizu A., 2016, NATURE NEUROSCIENCE; Ganguli S., 2008, PNAS; Geldard F. A., 1972, SCIENCE; Grunewalder S., 2012, ICML; Hinton G. E., 1995, SCIENCE; Hoyer P. O., 2003, NEURIPS; Kording KP, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000943; Kording K. P., 2004, J NEUROPHYSIOLOGY; Kutschireiter A., 2017, SCI REPORTS; Legenstein R., 2014, PLOS COMPUTATIONAL B; Ma W. J., 2006, NATURE NEUROSCIENCE; Mackay D. M., 1958, NATURE; Makin J. G., 2015, PLOS COMPUTATIONAL B; Miller G. A., 1950, J ACOUSTICAL SOC AM; Mohsenzadeh Y., 2016, FRONTIERS SYSTEMS NE; Nijhawan R., 1994, NATURE; Orban G., 2011, CURRENT OPINION NEUR; Orban G., 2016, NEURON; Oseledets I. V., 2011, SIAM J SCI COMPUTING; Purushothaman G., 1998, NATURE; Rao R. P., 2001, NEURAL COMPUTATION; Rezende D.J., 2014, ICML; Riecke L., 2008, PERCEPTION PSYCHOPHY; Sahani M., 2003, NEURAL COMPUTATION; Shimojo S, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00196; Sokoloski S., 2017, NEURAL COMPUTATION; Song L., 2013, IEEE SIGNAL PROCESSI; Vertes E., 2018, NEURIPS; Vertes E., 2019, NEURIPS; Wainwright M., 2008, FDN TRENDS MACHINE L; Wenliang L., 2019, COSYNE ABSTRACTS; Whiteley L, 2008, J VISION, V8, DOI 10.1167/8.3.2; Whitney D., 1998, NATURE NEUROSCIENCE; Zemel R. S., 1998, NEURAL COMPUTATION; Zemel R. S., 1999, NEURIPS	52	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901029
C	Wigren, A; Risuleo, RS; Murray, L; Lindsten, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wigren, Anna; Risuleo, Riccardo Sven; Murray, Lawrence; Lindsten, Fredrik			Parameter elimination in particle Gibbs sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STATE-SPACE MODELS	Bayesian inference in state-space models is challenging due to high-dimensional state trajectories. A viable approach is particle Markov chain Monte Carlo, combining MCMC and sequential Monte Carlo to form "exact approximations" to otherwise intractable MCMC methods. The performance of the approximation is limited to that of the exact method. We focus on particle Gibbs and particle Gibbs with ancestor sampling, improving their performance beyond that of the underlying Gibbs sampler (which they approximate) by marginalizing out one or more parameters. This is possible when the parameter prior is conjugate to the complete data likelihood. Marginalization yields a non-Markovian model for inference, but we show that, in contrast to the general case, this method still scales linearly in time. While marginalization can be cumbersome to implement, recent advances in probabilistic programming have enabled its automation. We demonstrate how the marginalized methods are viable as efficient inference backends in probabilistic programming, and demonstrate with examples in ecology and epidemiology.	[Wigren, Anna; Risuleo, Riccardo Sven] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden; [Murray, Lawrence] Uber AI, San Francisco, CA USA; [Lindsten, Fredrik] Linkoping Univ, Div Stat & Machine Learning, Linkoping, Sweden	Uppsala University; Linkoping University	Wigren, A (corresponding author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.	anna.wigren@it.uu.se; riccardo.risuleo@it.uu.se; lawrence.murray@uber.com; fredrik.lindsten@liu.se			Swedish Research Council [2016-04278, 2016-06079]; Swedish Foundation for Strategic Research (SSF) [ICA16-0015, RIT15-0012]; Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation	Swedish Research Council(Swedish Research CouncilEuropean Commission); Swedish Foundation for Strategic Research (SSF)(Swedish Foundation for Strategic Research); Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation	This research is financially supported, in part, by the Swedish Research Council via the projects Learning of Large-Scale Probabilistic Dynamical Models (contract number: 2016-04278) and New Directions in Learning Dynamical Systems (NewLEADS) (contract number:2016-06079), by the Swedish Foundation for Strategic Research (SSF) via the projects Probabilistic Modeling and Inference for Machine Learning (contract number: ICA16-0015) and ASSEMBLE (contract number: RIT15-0012), and by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Andrieu C, 2009, ANN STAT, V37, P697, DOI 10.1214/07-AOS574; Calafat FM, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04898-y; Cappe O, 2007, P IEEE, V95, P899, DOI 10.1109/JPROC.2007.893250; Carvalho CM, 2010, STAT SCI, V25, P88, DOI 10.1214/10-STS325; Chen R, 2000, J R STAT SOC B, V62, P493, DOI 10.1111/1467-9868.00246; Chopin N., 2010, ARXIV PREPRINT ARXIV; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; Fearnhead P, 2016, STAT COMPUT, V26, P1293, DOI 10.1007/s11222-015-9603-4; Funk S, 2016, PLOS NEGLECT TROP D, V10, DOI 10.1371/journal.pntd.0005173; Ge H., 2018, P INT C ARTIFICIAL I, P1682; Goodman N. D., 2014, DESIGN IMPLEMENTATIO; Hoffman M. D., 2018, ADV NEURAL INFORM PR, V31, P10716; Lande R., 2003, STOCHASTIC POPULATIO; Lee A., 2018, ARXIV180605852; Linderman SW, 2014, ADV NEUR IN, V27; Lindsten F, 2014, J MACH LEARN RES, V15, P2145; Mansinghka V., 2014, ARXIV14040099; Marcos M, 2015, J GEOPHYS RES-OCEANS, V120, P8115, DOI 10.1002/2015JC011173; Murray L., 2018, INT C ART INT STAT, P1037; Murray LM, 2018, ANNU REV CONTROL, V46, P29, DOI 10.1016/j.arcontrol.2018.10.013; Murray LM, 2015, J STAT SOFTW, V67, P1; Murray LM, 2013, SIAM-ASA J UNCERTAIN, V1, P494, DOI 10.1137/130915376; Nadeem K, 2012, OIKOS, V121, P1656, DOI 10.1111/j.1600-0706.2011.20010.x; Obermeyer F., 2019, 36 INT C MACH LEARN; Parslow J, 2013, ECOL APPL, V23, P679, DOI 10.1890/12-0312.1; Pfeffer A., 2016, PRACTICAL PROBABILIS; Rasmussen DA, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002136; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Saether BE, 2000, P ROY SOC B-BIOL SCI, V267, P621, DOI 10.1098/rspb.2000.1047; Singh SS, 2017, BIOMETRIKA, V104, P953, DOI 10.1093/biomet/asx051; Stanaway JD, 2016, LANCET INFECT DIS, V16, P712, DOI 10.1016/S1473-3099(16)00026-8; Storvik G, 2002, IEEE T SIGNAL PROCES, V50, P281, DOI 10.1109/78.978383; Todeschini A, 2014, ARXIV14123779STATCO; Tolpin David, 2016, P 28 S IMPLEMENTATIO, DOI [10.1145/3064899.3064910, DOI 10.1145/3064899.3064910]; Valera I, 2015, ADV NEUR IN, V28; van de Meent J.-W., 2015, P 18 INT C ART INT S; van Dyk DA, 2008, J AM STAT ASSOC, V103, P790, DOI 10.1198/016214508000000409	41	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900050
C	Wiseman, S; Kim, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wiseman, Sam; Kim, Yoon			Amortized Bethe Free Energy Minimization for Learning MRFs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				HIDDEN MARKOV-MODELS; PROPAGATION	We propose to learn deep undirected graphical models (i.e., MRFs) with a non-ELBO objective for which we can calculate exact gradients. In particular, we optimize a saddle-point objective deriving from the Bethe free energy approximation to the partition function. Unlike much recent work in approximate inference, the derived objective requires no sampling, and can be efficiently computed even for very expressive MRFs. We furthermore amortize this optimization with trained inference networks. Experimentally, we find that the proposed approach compares favorably with loopy belief propagation, but is faster, and it allows for attaining better held out log likelihood than other recent approximate inference schemes.	[Wiseman, Sam] Toyota Technol Inst Chicago, Chicago, IL 60637 USA; [Kim, Yoon] Harvard Univ, Cambridge, MA 02138 USA	Toyota Technological Institute - Chicago; Harvard University	Wiseman, S (corresponding author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	swiseman@ttic.edu; yoonkim@seas.harvard.edu		Wiseman, Sam/0000-0003-0923-1086	Google AI PhD Fellowship	Google AI PhD Fellowship(Google Incorporated)	We are grateful to Alexander M. Rush and Justin Chiu for insightful conversations and suggestions. YK is supported by a Google AI PhD Fellowship.	Alimoglu Fevzi, 1996, COMBINING MULTIPLE C; [Anonymous], 1935, P R SOC LONDON A, DOI DOI 10.1098/RSPA.1935.0122; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; Burda Yuri, 2015, ICLR; Chib S, 1996, J ECONOMETRICS, V75, P79, DOI 10.1016/0304-4076(95)01770-4; Courant R., 1943, B AM MATH SOC, V49, P1, DOI 10.1090/S0002-9904-1943-07818-4; Dai HJ, 2016, PR MACH LEARN RES, V48; Deng Zhiwei, 2016, P CVPR; Elidan G., 2012, ARXIV12066837; Frey B. J., 1997, PROC ANN ALLERTON C, P666; Gonzalez J., 2009, INT C ART INT STAT F, P177; Gormley Matthew, 2014, P 52 ANN M ASS COMP, P9; Grathwohl Will, 2018, ICLR; Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742; Heess Nicolas, 2013, P NIPS; Heskes T., 2003, ADV NEURAL INFORM PR, P359; Hinton, 2016, ARXIV PREPRINT ARXIV; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jang E., 2017, ICLR; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Koller D., 2009, PROBABILISTIC GRAPHI; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Kuleshov V., 2017, ADV NEURAL INFORM PR, V30, P6734; Li Y., 2014, ARXIV14105884; Li Yujia, 2015, ARXIV151105493; Lin G., 2015, P NIPS; Maddison Chris J, 2017, ICLR; Marcus M., 1993, COMPUT LINGUIST, V19, P331; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Meshi O., 2009, UAI, P402; Mikolov T., 2011, P INTERSPEECH; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mnih Andriy, 2016, P ICML; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; PEARL J, 1986, ARTIF INTELL, V29, P241, DOI 10.1016/0004-3702(86)90072-X; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Ross Stephane, 2011, P AISTATS; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Scott SL, 2002, J AM STAT ASSOC, V97, P337, DOI 10.1198/016214502753479464; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Sontag David, 2007, TECHNICAL REPORT; Srikumar V, 2012, P 2012 JOINT C EMP M, P1114; Stoyanov Veselin, 2011, P AISTATS; Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Tran Ke, 2016, P WORKSH STRUCT PRED; Tu Lifu, 2018, ICLR; Tucker G, 2017, ADV NEUR IN, V30; van den Oord A, 2016, PR MACH LEARN RES, V48; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weller A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P858; Weller A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P868; Welling M., 2001, P 17 C UNC ART INT, P554; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Willsky Alan S, 2008, ADV NEURAL INFORM PR, P1425; Yedidia J., 2003, EXPLORING ARTIFICIAL, V8, P236; Yedidia JS, 2001, ADV NEUR IN, V13, P689; Yoon K., 2018, ARXIV180307710; Zucchini W., 2016, HIDDEN MARKOV MODELS, V2; 2006, SPRING S OPERAT RES, P497	68	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907023
C	Wu, C; Miller, J; Chang, Y; Sznaier, M; Dy, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Chieh; Miller, Jared; Chang, Yale; Sznaier, Mario; Dy, Jennifer			Solving Interpretable Kernel Dimension Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS; OPTIMIZATION; DEPENDENCE	Kernel dimensionality reduction (KDR) algorithms find a low dimensional representation of the original data by optimizing kernel dependency measures that are capable of capturing nonlinear relationships. The standard strategy is to first map the data into a high dimensional feature space using kernels prior to a projection onto a low dimensional space. While KDR methods can be easily solved by keeping the most dominant eigenvectors of the kernel matrix, its features are no longer easy to interpret. Alternatively, Interpretable KDR (IKDR) is different in that it projects onto a subspace before the kernel feature mapping, therefore, the projection matrix can indicate how the original features linearly combine to form the new features. Unfortunately, the IKDR objective requires a non-convex manifold optimization that is difficult to solve and can no longer be solved by eigendecomposition. Recently, an efficient iterative spectral (eigendecomposition) method (ISM) has been proposed for this objective in the context of alternative clustering. However, ISM only provides theoretical guarantees for the Gaussian kernel. This greatly constrains ISM's usage since any kernel method using ISM is now limited to a single kernel. This work extends the theoretical guarantees of ISM to an entire family of kernels, thereby empowering ISM to solve any kernel method of the same objective. In identifying this family, we prove that each kernel within the family has a surrogate Phi matrix and the optimal projection is formed by its most dominant eigenvectors. With this extension, we establish how a wide range of IKDR applications across different learning paradigms can be solved by ISM. To support reproducible results, the source code is made publicly available on https://github.com/chieh-neu/ISM_supervised_DR.	[Wu, Chieh; Miller, Jared; Chang, Yale; Sznaier, Mario; Dy, Jennifer] Northeastern Univ, Elect & Comp Engn Dept, Boston, MA 02115 USA	Northeastern University	Wu, C (corresponding author), Northeastern Univ, Elect & Comp Engn Dept, Boston, MA 02115 USA.				NSF [IIS-1546428]	NSF(National Science Foundation (NSF))	We would like to acknowledge support for this project from NSF grant IIS-1546428. We would also like to thank Zulqarnain Khan for his insightful discussions.	[Anonymous], 2011, PRINCIPAL COMPONENT; Barshan E, 2011, PATTERN RECOGN, V44, P1357, DOI 10.1016/j.patcog.2010.12.015; Bay S.D., 2000, ACM SIGKDD EXPLOR NE, V2, P81; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Buitinck L., 2013, API DESIGN MACHINE L, DOI DOI 10.48550/ARXIV.1309.0238; Chang Yale, 2017, P 2017 SIAM INT C DA, P207, DOI DOI 10.1137/1.9781611974973.24; Chen FH, 2007, PRINCIPLES OF TISSUE ENGINEERING, 3RD EDITION, P823, DOI 10.1016/B978-012370615-7/50059-7; Cortes C, 2012, J MACH LEARN RES, V13, P795; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906; Gangeh MJ, 2016, LECT NOTES COMPUT SC, V9730, P12, DOI 10.1007/978-3-319-41501-7_2; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Gretton A, 2012, J MACH LEARN RES, V13, P723; James I. M., 1976, TOPOLOGY STIEFEL MAN, V24; Jones E., 2001, SCIPY OPEN SOURCE SC; Kambhatla N, 1997, NEURAL COMPUT, V9, P1493, DOI 10.1162/neco.1997.9.7.1493; Knyazev Andrew V, 2012, PRINCIPAL ANGLES SUB; Masaeli M., 2010, P 27 INT C MACH LEAR, P751; Nishimori Y, 2005, NEUROCOMPUTING, V67, P106, DOI 10.1016/j.neucom.2004.11.035; Niu D., 2010, PROC 27 INT C MACH L, P831; Niu D., 2011, P 14 INT C ART INT S, P552; Niu DL, 2014, IEEE T PATTERN ANAL, V36, P1340, DOI 10.1109/TPAMI.2013.180; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Song L., 2008, P 20 INT C NEUR INF, P1385; Song L, 2012, J MACH LEARN RES, V13, P1393; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Sugiyama M., 2010, JMLR WORKSHOP C P, V9, P804; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Theis FJ, 2009, LECT NOTES COMPUT SC, V5441, P354, DOI 10.1007/978-3-642-00599-2_45; Vladymyrov Max, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P256, DOI 10.1007/978-3-642-40994-3_17; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; Wickelmaier F., 2003, SOUND QUALITY RES UN, V46, P1; Wolberg W. H., 1992, WISCONSIN BREAST CAN; Wu C., 2018, INT C ART INT STAT, P115	42	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307088
C	Xia, ZH; Chakrabarti, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xia, Zhihao; Chakrabarti, Ayan			Training Image Estimators without Image Ground-Truth	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep neural networks have been very successful in image estimation applications such as compressive-sensing and image restoration, as a means to estimate images from partial, blurry, or otherwise degraded measurements. These networks are trained on a large number of corresponding pairs of measurements and ground-truth images, and thus implicitly learn to exploit domain-specific image statistics. But unlike measurement data, it is often expensive or impractical to collect a large training set of ground-truth images in many application settings. In this paper, we introduce an unsupervised framework for training image estimation networks, from a training set that contains only measurements-with two varied measurements per image-but no ground-truth for the full images desired as output. We demonstrate that our framework can be applied for both regular and blind image estimation tasks, where in the latter case parameters of the measurement model (e.g., the blur kernel) are unknown: during inference, and potentially, also during training. We evaluate our method for training networks for compressive-sensing and blind deconvolution, considering both non-blind and blind training for the latter. Our unsupervised framework yields models that are nearly as accurate as those from fully supervised training, despite not having access to any ground-truth images.	[Xia, Zhihao; Chakrabarti, Ayan] Washington Univ, 1 Brookings Dr, St Louis, MO 63130 USA	Washington University (WUSTL)	Xia, ZH (corresponding author), Washington Univ, 1 Brookings Dr, St Louis, MO 63130 USA.	zhihao.xia@wustl.edu; ayan@wustl.edu			NSF [IIS-1820693]	NSF(National Science Foundation (NSF))	This work was supported by the NSF under award no. IIS-1820693.	Aila T., 2018, ARXIV180304189; Ambient GAN, 2018, ICLR; Anirudh R., 2018, ARXIV180507281; Ba Le Vuong, 2012, ECCV; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Chakrabarti A, 2016, LECT NOTES COMPUT SC, V9907, P221, DOI 10.1007/978-3-319-46487-9_14; Chang Jen-Hao Rick, 2017, P ICCV; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55; Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1; Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942; Liu Ziwei, 2015, P INT C COMP VIS ICC; Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728; Ma W., 2018, P ECCV, P201; Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683; Metzler Christopher A, 2018, ARXIV180510531; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Ronneberger O., 2015, P INT C MED IMAG COM, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, DOI 10.48550/ARXIV.1505.04597]; Roth Stefan, 2009, IJCV; Russakovsky O, 2015, IMAGENET LARGE SCALE, V115, P211; Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452; Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196; Zhang Kai, 2017, P CVPR; Zhou T., 2017, P IEEE C COMP VIS PA, P1851; Zhussip Magauiya, 2019, P CVPR; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302044
C	Xu, LY; Honda, J; Niu, G; Sugiyama, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Liyuan; Honda, Junya; Niu, Gang; Sugiyama, Masashi			Uncoupled Regression from Pairwise Comparison Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Uncoupled regression is the problem to learn a model from unlabeled data and the set of target values while the correspondence between them is unknown. Such a situation arises in predicting anonymized targets that involve sensitive information, e.g., one's annual income. Since existing methods for uncoupled regression often require strong assumptions on the true target function, and thus, their range of applications is limited, we introduce a novel framework that does not require such assumptions in this paper. Our key idea is to utilize pairwise comparison data, which consists of pairs of unlabeled data that we know which one has a larger target value. Such pairwise comparison data is easy to collect, as typically discussed in the learning-to-rank scenario, and does not break the anonymity of data. We propose two practical methods for uncoupled regression from pairwise comparison data and show that the learned regression model converges to the optimal model with the optimal parametric convergence rate when the target variable distributes uniformly. Moreover, we empirically show that for linear models the proposed methods are comparable to ordinary supervised regression with labeled data.	[Xu, Liyuan; Honda, Junya; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan; [Xu, Liyuan; Honda, Junya; Niu, Gang; Sugiyama, Masashi] RIKEN, Wako, Saitama, Japan; [Xu, Liyuan] Gatsby Computat Neurosci Unit, London, England	University of Tokyo; RIKEN; University of London; University College London	Xu, LY (corresponding author), Univ Tokyo, Tokyo, Japan.; Xu, LY (corresponding author), RIKEN, Wako, Saitama, Japan.; Xu, LY (corresponding author), Gatsby Computat Neurosci Unit, London, England.	liyuan@ms.k.u-tokyo.ac.jp; honda@stat.t.u-tokyo.ac.jp; gang.niu@riken.jp; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	KAKENHI [18K17998]; JST CREST [JPMJCR18A2]	KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	LX utilized the facility provided by Masason Foundation. JH acknowledges support by KAKENHI 18K17998, and MS was supported by JST CREST Grant Number JPMJCR18A2.	Bao H., 2018, P 35 INT C MACH LEAR; Carpentier A., 2016, P 19 INT C ART INT S; Cohen W. W., 2002, P 18 ACM SIGKDD INT; Cortes C., 2007, P 19 ADV NEUR INF PR; Cortes C, 2008, MACHINE LEARN SIGN P, P2, DOI 10.1109/MLSP.2008.4685446; du Plessis M., 2015, P 32 INT C MACH LEAR; du Plessis M. C., 2014, P 27 ADV NEUR INF PR; Duh K, 2011, COMPUT SPEECH LANG, V25, P261, DOI 10.1016/j.csl.2010.05.002; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Egusa R, 2017, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON COMPUTER SUPPORTED EDUCATION (CSEDU), VOL 2, P340, DOI 10.5220/0006377203400344; Frigyik BA, 2008, IEEE T INFORM THEORY, V54, P5130, DOI 10.1109/TIT.2008.929943; Georgios K, 2018, J INTELL FUZZY SYST, V35, P1; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; Herbrich R, 2000, ADV NEUR IN, P115; Hsu D. J., 2017, ADV NEURAL INFORM PR, P1531; Jebara T., 2004, KLUWER INT SER ENG C, DOI 10.1007/978-1-4419-9011-2; Jitta A., 2017, P 3 INT WORKSH ADV M; Lu N., 2019, P 7 INT C LEARN REPR; Mann GS, 2010, J MACH LEARN RES, V11, P955; Mendelson S, 2008, IEEE T INFORM THEORY, V54, P3797, DOI 10.1109/TIT.2008.926323; Mohri M., 2018, FDN MACHINE LEARNING; Niu G, 2016, ADV NEUR IN, V29; Pananjady A, 2018, IEEE T INFORM THEORY, V64, P3286, DOI 10.1109/TIT.2017.2776217; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Quadrianto N., 2009, P 21 ADV NEUR INF PR; Ray S., 2001, P 18 INT C MACH LEAR; RIGOLLET P., 2019, INFORM INFERENCE J I; Rudin C., 2005, P 18 ANN C LEARN THE; Walter V, 1999, INT J GEOGR INF SCI, V13, P445, DOI 10.1080/136588199241157; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137; Yamada M., 2011, P 14 INT C ART INT S; Zhang Q., 2002, P 15 ADV NEUR INF PR; Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106	34	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304004
C	Yun, SY; Proutiere, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yun, Se-Young; Proutiere, Alexandre			Optimal Sampling and Clustering in the Stochastic Block Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMMUNITY DETECTION	This paper investigates the design of joint adaptive sampling and clustering algorithms in networks whose structure follows the celebrated Stochastic Block Model (SBM). To extract hidden clusters, the interaction between edges (pairs of nodes) may be sampled sequentially, in an adaptive manner. After gathering samples, the learner returns cluster estimates. We derive information-theoretical upper bounds on the cluster recovery rate. These bounds actually reveal the optimal sequential edge sampling strategy, and interestingly, the latter does not depend on the sampling budget, but on the parameters of the SBM only. We devise a joint sampling and clustering algorithm matching the recovery rate upper bounds. The algorithm initially uses a fraction of the sampling budget to estimate the SBM parameters, and to learn the optimal sampling strategy. This strategy then guides the remaining sampling process, which confers the optimality of the algorithm. We show both analytically and numerically that adaptive edge sampling yields important improvements over random sampling (traditionally used in the SBM analysis). For example, we prove that adaptive sampling significantly enlarges the region of the SBM parameters where asymptotically exact cluster recovery is feasible.	[Yun, Se-Young] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Proutiere, Alexandre] KTH, Stockholm, Sweden	Korea Advanced Institute of Science & Technology (KAIST); Royal Institute of Technology	Yun, SY (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.	yunseyoung@kaist.ac.kr; alepro@kth.se			Korea Electric Power Corporation [R18XA05]; Wallenberg Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation	Korea Electric Power Corporation; Wallenberg Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation	S. Yun was supported by Korea Electric Power Corporation. (Grant number:R18XA05). A. Proutiere was partially supported by the Wallenberg Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.	Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Abbe Emmanuel, 2014, ABS14053267 CORR; Decelle A., 2011, PHYS REV LETT, V107; Deshpande Y, 2016, IEEE INT SYMP INFO, P185, DOI 10.1109/ISIT.2016.7541286; Gao Chao, 2017, J MACHINE LEARNING R, V18, P1980; Hajek B, 2016, IEEE T INFORM THEORY, V62, P5918, DOI 10.1109/TIT.2016.2594812; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Massoulie L, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P694, DOI 10.1145/2591796.2591857; Mossel E., 2016, P C LEARN THEOR, P1319; Mossel E, 2015, ACM S THEORY COMPUT, P69, DOI 10.1145/2746539.2746603; Yun S.Y., 2016, ADV NEURAL INFORM PR, V29, P965; Yun S.-Y., 2014, COLT, P138; Zhang AY, 2016, ANN STAT, V44, P2252, DOI 10.1214/15-AOS1428	17	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905013
C	Zhang, LP; Tang, K; Yao, X		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Liangpeng; Tang, Ke; Yao, Xin			Explicit Planning for Efficient Exploration in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INTERVAL ESTIMATION	Efficient exploration is crucial to achieving good performance in reinforcement learning. Existing systematic exploration strategies (R-MAX, MBIE, UCRL, etc.), despite being promising theoretically, are essentially greedy strategies that follow some predefined heuristics. When the heuristics do not match the dynamics of Markov decision processes (MDPs) well, an excessive amount of time can be wasted in travelling through already-explored states, lowering the overall efficiency. We argue that explicit planning for exploration can help alleviate such a problem, and propose a Value Iteration for Exploration Cost (VIEC) algorithm which computes the optimal exploration scheme by solving an augmented MDP. We then present a detailed analysis of the exploration behaviour of some popular strategies, showing how these strategies can fail and spend O(n(2)md) or O(n(2)m + nmd) steps to collect sufficient data in some tower-shaped MDPs, while the optimal exploration scheme, which can be obtained by VIEC, only needs O(nmd), where n, m are the numbers of states and actions and d is the data demand. The analysis not only points out the weakness of existing heuristic-based strategies, but also suggests a remarkable potential in explicit planning for exploration.	[Zhang, Liangpeng; Yao, Xin] Univ Birmingham, CERCIA, Sch Comp Sci, Birmingham, W Midlands, England; [Tang, Ke; Yao, Xin] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Univ Key Lab Evolving Intelligent Syst Guangdong, Shenzhen Key Lab Computat Intelligence, Shenzhen 518055, Peoples R China	University of Birmingham; Southern University of Science & Technology	Yao, X (corresponding author), Univ Birmingham, CERCIA, Sch Comp Sci, Birmingham, W Midlands, England.; Yao, X (corresponding author), Southern Univ Sci & Technol, Dept Comp Sci & Engn, Univ Key Lab Evolving Intelligent Syst Guangdong, Shenzhen Key Lab Computat Intelligence, Shenzhen 518055, Peoples R China.	L.Zhang.7@pgr.bham.ac.uk; tangk3@sustc.edu.cn; xiny@sustc.edu.cn			EPSRC [EP/J017515/1, EP/P005578/1]; Royal Society (Newton Advanced Fellowship); Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X386]; Shenzhen Peacock Plan [KQTD2016112514355531]; Program for University Key Laboratory of Guangdong Province [2017KSYS008]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Royal Society (Newton Advanced Fellowship); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Shenzhen Peacock Plan; Program for University Key Laboratory of Guangdong Province	This work was supported by EPSRC (Grant Nos. EP/J017515/1 and EP/P005578/1), the Royal Society (through a Newton Advanced Fellowship to Ke Tang and hosted by Xin Yao), the Program for Guangdong Introducing Innovative and Enterpreneurial Teams (Grant No. 2017ZT07X386), Shenzhen Peacock Plan (Grant No. KQTD2016112514355531) and the Program for University Key Laboratory of Guangdong Province (Grant No. 2017KSYS008).	[Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Auer P, 2006, ADV NEURAL INFORM PR, V19, P49; Bellemare M., 2016, NEURIPS; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; EISELT HA, 1995, OPER RES, V43, P399, DOI 10.1287/opre.43.3.399; Kakade Sham M., 2003, THESIS; Kolter J. Z., 2009, P 26 ANN INT C MACHI, P513, DOI DOI 10.1145/1553374.1553441; Lattimore T, 2014, THEOR COMPUT SCI, V558, P125, DOI 10.1016/j.tcs.2014.09.029; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rao Karun, 2012, P 11 INT C AUT AG MU, V1, P375; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Strehl AL, 2004, PROC INT C TOOLS ART, P128; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Szita Istvan, 2010, INT C MACH LEARN ICM, P1031; Tang H, 2017, NIPS, V30, P2753; van Seijen H., 2013, INT C MACH LEARN, P361; Whitehead S. D., 1991, AAAI-91. Proceedings Ninth National Conference on Artificial Intelligence, P607; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893; Zhang LP, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4033	20	1	1	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307050
C	Zhang, ZH; Yu, C; Crandall, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Zehua; Yu, Chen; Crandall, David			A Self Validation Network for Object-Level Human Attention Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EYE-HAND COORDINATION; MOVEMENTS	Due to the foveated nature of the human vision system, people can focus their visual attention on only a small region of their visual field at a time, which usually contains a single object. Estimating this object of attention in first-person (egocentric) videos is useful for many human-centered real-world applications such as augmented reality and driver assistance systems. A straightforward solution for this problem is to first estimate the gaze with a traditional gaze estimator and generate object candidates from an off-the-shelf object detector, and then pick the object that the estimated gaze falls in. However, such an approach can fail because it addresses the where and the what problems separately, despite that they are highly related, chicken-and-egg problems. In this paper, we propose a novel unified model that incorporates both spatial and temporal evidence in identifying as well as locating the attended object in first-person videos. It introduces a novel Self Validation Module that enforces and leverages consistency of the where and the what concepts. We evaluate on two public datasets, demonstrating that the Self Validation Module significantly benefits both training and testing and that our model outperforms the state-of-the-art.	[Zhang, Zehua; Crandall, David] Indiana Univ, Luddy Sch Informat Comp & Engn, Bloomington, IN 47405 USA; [Yu, Chen] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA	Indiana University System; Indiana University Bloomington; Indiana University System; Indiana University Bloomington	Zhang, ZH (corresponding author), Indiana Univ, Luddy Sch Informat Comp & Engn, Bloomington, IN 47405 USA.	zehzhang@indiana.edu; chenyu@indiana.edu; djcran@indiana.edu			National Science Foundation [CAREER IIS-1253549]; National Institutes of Health [R01 HD074601, R01 HD093792]; NVidia; Google; IU Office of the Vice Provost for Research; College of Arts and Sciences; School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children"	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NVidia; Google(Google Incorporated); IU Office of the Vice Provost for Research; College of Arts and Sciences; School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children"	This work was supported in part by the National Science Foundation (CAREER IIS-1253549), the National Institutes of Health (R01 HD074601, R01 HD093792), NVidia, Google, and the IU Office of the Vice Provost for Research, the College of Arts and Sciences, and the School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children."	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], [No title captured]; Ba SO, 2011, IEEE T PATTERN ANAL, V33, P101, DOI 10.1109/TPAMI.2010.69; Baradel F, 2018, LECT NOTES COMPUT SC, V11217, P106, DOI 10.1007/978-3-030-01261-8_7; Bertasius G, 2017, IEEE I CONF COMP VIS, P1974, DOI 10.1109/ICCV.2017.216; Bertasius Gedas, 2016, ARXIV160304908; Borji Ali, 2012, CVPR; Bowman M, 2009, EXP BRAIN RES, V195, P273, DOI 10.1007/s00221-009-1781-x; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044; Chollet F., 2017, R INTERFACE TO KERAS; Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Furnari A, 2017, J VIS COMMUN IMAGE R, V49, P401, DOI 10.1016/j.jvcir.2017.10.004; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009; Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563; Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Huang YF, 2018, LECT NOTES COMPUT SC, V11208, P789, DOI 10.1007/978-3-030-01225-0_46; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kaiming He, 2020, IEEE Transactions on Pattern Analysis and Machine Intelligence, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Karessli N, 2017, PROC CVPR IEEE, P6412, DOI 10.1109/CVPR.2017.679; Karthikeyan S, 2013, IEEE I CONF COMP VIS, P625, DOI 10.1109/ICCV.2013.83; Kay W., 2017, ARXIV PREPRINT ARXIV; Lazzari S, 2009, J MOTOR BEHAV, V41, P294, DOI 10.3200/JMBR.41.4.294-304; Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820; Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342; Li X, 2018, IEEE INT VAC ELECT C, P335; Li Y, 2018, LECT NOTES COMPUT SC, V11209, P639, DOI 10.1007/978-3-030-01228-1_38; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Li Y, 2013, IEEE I CONF COMP VIS, P2432, DOI 10.1109/ICCV.2013.302; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu D, 2010, IEEE T PATTERN ANAL, V32, P2178, DOI 10.1109/TPAMI.2010.31; Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80; Luong M., 2015, ARXIV150804025; Ma MH, 2016, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2016.209; Mozer M. C., ATTENTION, V9, P341; Niklaus S., 2018, REIMPLEMENTATION PWC; Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71; Papadopoulos DP, 2014, LECT NOTES COMPUT SC, V8693, P361, DOI 10.1007/978-3-319-10602-1_24; Perone S, 2008, DEV PSYCHOL, V44, P1242, DOI 10.1037/0012-1649.44.5.1242; Pirsiavash H, 2012, PROC CVPR IEEE, P2847, DOI 10.1109/CVPR.2012.6248010; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; RUTISHAUSER U, 2004, IEEE C COMP VIS PATT, V2; SATTAR H, 2015, PROC CVPR IEEE, P981, DOI DOI 10.1109/CVPR.2015.7298700; Shen Y, 2018, LECT NOTES COMPUT SC, V11206, P202, DOI 10.1007/978-3-030-01216-8_13; Song H, 2018, JOINT INT CONF SOFT, P718, DOI 10.1109/SCIS-ISIS.2018.00119; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vidoni ED, 2009, EXP BRAIN RES, V195, P611, DOI 10.1007/s00221-009-1833-2; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yamada K, 2011, LECT NOTES COMPUT SC, V7087, P277, DOI 10.1007/978-3-642-25367-6_25; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhang MM, 2017, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2017.377; ZHANG PP, 2017, IEEE I CONF COMP VIS, P202, DOI DOI 10.1109/ICCV.2017.31; Zhang Z., 2018, BRIT MACH VIS C BMVC; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zhu CB, 2017, IEEE INT CONF COMP V, P2860, DOI 10.1109/ICCVW.2017.337	69	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906039
C	Zhao, BX; Wang, YS; Kolar, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Boxin; Wang, Y. Samuel; Kolar, Mladen			Direct Estimation of Differential Functional Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SELECTION; ALGORITHM; NETWORKS	We consider the problem of estimating the difference between two functional undirected graphical models with shared structures. In many applications, data are naturally regarded as high-dimensional random function vectors rather than multivariate scalars. For example, electroencephalography (EEG) data are more appropriately treated as functions of time. In these problems, not only can the number of functions measured per sample be large, but each function is itself an infinite dimensional object, making estimation of model parameters challenging. We develop a method that directly estimates the difference of graphs, avoiding separate estimation of each graph, and show it is consistent in certain high-dimensional settings. We illustrate finite sample properties of our method through simulation studies. Finally, we apply our method to EEG data to uncover differences in functional brain connectivity between alcoholics and control subjects.	[Zhao, Boxin] Unveristy Chicago, Dept Stat, Chicago, IL 60637 USA; [Wang, Y. Samuel; Kolar, Mladen] Unveristy Chicago, Booth Sch Business, Chicago, IL 60637 USA		Zhao, BX (corresponding author), Unveristy Chicago, Dept Stat, Chicago, IL 60637 USA.	boxinz@uchicago.edu; swang24@uchicago.edu; mkolar@chicagobooth.edu		Wang, Y. Samuel/0000-0001-8069-3430				Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bosq D, 2000, LINEAR PROCESSES FUN, V149; Cai TT, 2017, ANNU REV STAT APPL, V4, P423, DOI 10.1146/annurev-statistics-060116-053754; Drton M, 2017, ANNU REV STAT APPL, V4, P365, DOI 10.1146/annurev-statistics-060116-053803; Fang SQ, 2016, ADV COMPUT SCI TECH, P29; Hsing T., 2015, THEORETICAL FDN FUNC; Ingber L, 1997, PHYS REV E, V55, P4578, DOI 10.1103/PhysRevE.55.4578; Kim B., 2019, ARXIV190500466; Knyazev GG, 2007, NEUROSCI BIOBEHAV R, V31, P377, DOI 10.1016/j.neubiorev.2006.10.004; Kolar M, 2010, ANN APPL STAT, V4, P94, DOI 10.1214/09-AOAS308; Lauritzen S., 1996, GRAPH MODELS; Li B, 2018, J AM STAT ASSOC, V113, P1637, DOI 10.1080/01621459.2017.1356726; Liu S, 2014, NEURAL COMPUT, V26, P1169, DOI 10.1162/NECO_a_00589; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Na S., 2019, ARXIV190905892; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Qiao XH, 2019, J AM STAT ASSOC, V114, P211, DOI 10.1080/01621459.2017.1390466; Tibshirani R., 2010, LECT NOTES; Wang JL, 2016, ANNU REV STAT APPL, V3, P257, DOI 10.1146/annurev-statistics-041715-033624; Wu P, 2018, IEEE INT C INTELL TR, P3749, DOI 10.1109/ITSC.2018.8569582; Xu P., 2016, ADV NEURAL INFORM PR, P1064; Yao F, 2006, J R STAT SOC B, V68, P3, DOI 10.1111/j.1467-9868.2005.00530.x; Yu M., 2019, ARXIV190506261; Yuan HL, 2017, BIOMETRIKA, V104, P755, DOI 10.1093/biomet/asx049; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; zHANG C, 2018, ARXIV PREPRINT ARXIV; ZHANG XL, 1995, BRAIN RES BULL, V38, P531, DOI 10.1016/0361-9230(95)02023-5; Zhao SD, 2014, BIOMETRIKA, V101, P253, DOI 10.1093/biomet/asu009; Zhu HX, 2016, J MACH LEARN RES, V17	31	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302056
C	Zhong, YQ; Wu, CY; You, SY; Neumann, U		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhong, Yiqi; Wu, Cho-Ying; You, Suya; Neumann, Ulrich			Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we propose our Correlation For Completion Network (CFCNet), an end-to-end deep learning model that uses the correlation between two data sources to perform sparse depth completion. CFCNet learns to capture, to the largest extent, the semantically correlated features between RGB and depth information. Through pairs of image pixels and the visible measurements in a sparse depth map, CFCNet facilitates feature-level mutual transformation of different data sources. Such a transformation enables CFCNet to predict features and reconstruct data of missing depth measurements according to their corresponding, transformed RGB features. We extend canonical correlation analysis to a 2D domain and formulate it as one of our training objectives (i.e. 2d deep canonical correlation, or "2D(2)CCA loss"). Extensive experiments validate the ability and flexibility of our CFCNet compared to the state-of-the-art methods on both indoor and outdoor scenes with different real-life sparse patterns.	[Zhong, Yiqi; Wu, Cho-Ying] Univ Calif Los Angeles, Los Angeles, CA 90095 USA; [You, Suya] US Army, Res Lab, Playa Vista, CA USA; [Neumann, Ulrich] Univ Southern Calif, Los Angeles, CA 90007 USA	University of California System; University of California Los Angeles; United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL); University of Southern California	Zhong, YQ (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.	yiqizhon@usc.edu; choyingw@usc.edu; suya.you.civ@mail.mil; uneumann@usc.edu						Aleotti F, 2019, LECT NOTES COMPUT SC, V11129, P337, DOI 10.1007/978-3-030-11009-3_20; Anderson T. W, 1984, INTRO MULTIVARIATE S; Andrew Galen, 2013, ICML; Cai-rong Z., 2007, IEEE WORKSH APPL COM, P43; Chen Z, 2018, LECT NOTES COMPUT SC, V11208, P176, DOI 10.1007/978-3-030-01225-0_11; Cheng XJ, 2018, LECT NOTES COMPUT SC, V11220, P108, DOI 10.1007/978-3-030-01270-0_7; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089; Forsyth David A, 2012, COMPUTER VISION MODE; Geiger A., 2012, P IEEE COMP SOC C CO; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Imran Saif, 2019, ARXIV190305421; Jaritz M, 2018, INT CONF 3D VISION, P52, DOI 10.1109/3DV.2018.00017; Kan MN, 2016, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2016.524; Katsurai M, 2016, INT CONF ACOUST SPEE, P2837, DOI 10.1109/ICASSP.2016.7472195; Kokkinos Iasonas, 2017, IEEE INT C COMP VIS; Kukharev G., 2010, Pattern Recognition and Image Analysis, V20, P210, DOI 10.1134/S1054661810020136; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lee SH, 2007, IEEE SIGNAL PROC LET, V14, P735, DOI 10.1109/LSP.2007.896438; Li M, 2004, 2004 7TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS 1-3, P1419; Liu Y, 2017, SECUR COMMUN NETW, DOI 10.1155/2017/1461520; Mal Fangchang, 2018, IEEE INT C ROB AUT I, P1; Mroueh Y, 2015, INT CONF ACOUST SPEE, P2130, DOI 10.1109/ICASSP.2015.7178347; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Pizzoli M, 2014, IEEE INT CONF ROBOT, P2609, DOI 10.1109/ICRA.2014.6907233; Qiu J., 2018, CORRABS181200488; Shivakumar SS, 2019, IEEE INT C INTELL TR, P13, DOI 10.1109/ITSC.2019.8917294; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695; Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012; Wang Tsun-Hsuan, 2019, IEEE INT C ROB AUT I; Wang WR, 2015, PR MACH LEARN RES, V37, P1083; Wang WR, 2015, INT CONF ACOUST SPEE, P4590, DOI 10.1109/ICASSP.2015.7178840; Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9; Wang WY, 2019, ADV NEUR IN, V32; Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966; Yang Jian, 2004, IEEE T PATTERN ANAL; Yang XH, 2017, INFORM SCIENCES, V385, P338, DOI 10.1016/j.ins.2017.01.011; Yang Yanchao, 2019, ARXIV190110034; Yiyi Liao, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5059, DOI 10.1109/ICRA.2017.7989590; Zhang Yilun, 2019, ARXIV190306397; Zhang YD, 2018, PROC CVPR IEEE, P175, DOI 10.1109/CVPR.2018.00026; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	47	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305034
C	Zhu, LG; Liu, ZJ; Han, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhu, Ligeng; Liu, Zhijian; Han, Song			Deep Leakage from Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODEL	Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradients exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. Thereby we want to raise people's awareness to rethink the gradient's safety. We also discuss several possible strategies to prevent such deep leakage. Without changes on training setting, the most effective defense method is gradient pruning.	[Zhu, Ligeng; Liu, Zhijian; Han, Song] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Zhu, LG (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	ligeng@mit.edu; zhijian@mit.edu; songhan@mit.edu	Liu, Zhijian/ABF-4061-2020	Liu, Zhijian/0000-0003-3632-9986	MIT-IBM Watson Al lab; Intel; AWS; Facebook	MIT-IBM Watson Al lab(International Business Machines (IBM)); Intel(Intel Corporation); AWS; Facebook(Facebook Inc)	We sincerely thank MIT-IBM Watson Al lab, Intel, Facebook and AWS for supporting this work. We sincerely thank John Cohn for the discussions.	Abadi Martin, 2015, TENSORFLOW LARGE SCA; Akiba T, 2017, IEEE CPMT SYMP JAP, P1; An GC, 2014, INT SYM COMPUT INTEL, P3, DOI 10.1109/ISCID.2014.41; [Anonymous], NIPS WORKSH PRIV MUL; [Anonymous], 2017, P ICLR; [Anonymous], 2015, ARXIV151201274; Barney B., INTRO PARALLEL COMPU; Bonawitz K., 2019, SYSML C; Bonawitz Keith, 2016, CORR; Dean J., 2012, NIPS 12, V1, P1223; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Fredrikson M, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1322, DOI 10.1145/2810103.2813677; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hitaj Briland, 2017, CORR; Huang G.B., 2008, WORKSHOP FACESREAL L; Iandola FN, 2016, PROC CVPR IEEE, P2592, DOI 10.1109/CVPR.2016.284; Jia Xianyan, 2018, ARXIV180711205; Jochems A, 2017, INT J RADIAT ONCOL, V99, P344, DOI 10.1016/j.ijrobp.2017.04.021; Jochems A, 2016, RADIOTHER ONCOL, V121, P459, DOI 10.1016/j.radonc.2016.10.002; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kim H., 2016, ARXIV160208191; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kyrola A., 2017, ABS170602677 ARXIV; LeCun Yann, MNIST DATABASE HANDW; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; McMahan H. Brendan, 2016, ARXIV160205629; Melis Luca, 2018, CORR; Netzer Y., 2011, P NIPS WORKSH DEEP L; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Patarasuk P, 2009, J PARALLEL DISTR COM, V69, P117, DOI 10.1016/j.jpdc.2008.09.002; Phong LT, 2018, IEEE T INF FOREN SEC, V13, P1333, DOI 10.1109/TIFS.2017.2787987; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Sergeev Alexander, 2018, ARXIV180205799; Shokri R, 2017, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2017.41; Tagliavini Giuseppe, 2017, CORR; Tsuzuku Y., 2018, ARXIV180206058	36	1	1	6	31	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906043
C	Ziyin, L; Wang, ZT; Liang, PP; Salakhutdinov, R; Morency, LP; Ueda, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ziyin, Liu; Wang, Zhikang T.; Liang, Paul Pu; Salakhutdinov, Ruslan; Morency, Louis-Philippe; Ueda, Masahito			Deep Gamblers: Learning to Abstain with Portfolio Theory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We deal with the selective classification problem (supervised-learning problem with a rejection option), where we want to achieve the best performance at a certain level of coverage of the data. We transform the original rn-class classification problem to (m + 1)-class where the (m + 1)-th class represents the model abstaining from making a prediction due to disconfidence. Inspired by portfolio theory, we propose a loss function for the selective classification problem based on the doubling rate of gambling. Minimizing this loss function corresponds naturally to maximizing the return of a horse race, where a player aims to balance between betting on an outcome (making a prediction) when confident and reserving one's winnings (abstaining) when not confident. This loss function allows us to train neural networks and characterize the disconfidence of prediction in an end-to-end fashion. In comparison with previous methods, our method requires almost no modification to the model inference algorithm or model architecture. Experiments show that our method can identify uncertainty in data points, and achieves strong results on SVHN and CIFAR1O at various coverages of the data.	[Ziyin, Liu; Wang, Zhikang T.; Ueda, Masahito] Univ Tokyo, Inst Phys Intelligence, Tokyo, Japan; [Ziyin, Liu; Wang, Zhikang T.; Ueda, Masahito] Univ Tokyo, Dept Phys, Tokyo, Japan; [Liang, Paul Pu; Salakhutdinov, Ruslan] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Morency, Louis-Philippe] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA	University of Tokyo; University of Tokyo; Carnegie Mellon University; Carnegie Mellon University	Ziyin, L (corresponding author), Univ Tokyo, Inst Phys Intelligence, Tokyo, Japan.; Ziyin, L (corresponding author), Univ Tokyo, Dept Phys, Tokyo, Japan.	zliu@cat.phys.s.u-tokyo.ac.jp; wang@cat.phys.s.u-tokyo.ac.jp; pliang@cs.cmu.edu; rsalakhu@cs.cmu.edu; morency@cs.cmu.edu; ueda@phys.s.u-tokyo.ac.jp	Ueda, Masahito/G-3046-2012	Ueda, Masahito/0000-0002-5367-1436	GSSS scholarship at the University of Tokyo; Global Science Graduate Course (GSGC) program of the University of Tokyo; National Science Foundation [1734868, 1722822]; National Institutes of Health; KAKENHI [18H01145]; Japan Society for the Promotion of Science [JP15H05855]	GSSS scholarship at the University of Tokyo; Global Science Graduate Course (GSGC) program of the University of Tokyo; National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Japan Society for the Promotion of Science(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	Liu Ziyin thanks Mr. Zongping Gong for buying him drink sometimes, during the writing of this paper; he also thanks the GSSS scholarship at the University of Tokyo for supporting his graduate study. Z. T. Wang is supported by Global Science Graduate Course (GSGC) program of the University of Tokyo. This material is based upon work partially supported by the National Science Foundation (Awards #1734868, #1722822) and National Institutes of Health. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or National Institutes of Health, and no official endorsement should be inferred. Also, This work was supported by KAKENHI Grant No. JP 18H01145 and a Grant-in-Aid for Scientific Research on Innovative Areas "Topological Materials Science" (KAKENHI Grant No. JP15H05855) from the Japan Society for the Promotion of Science.	Abe K, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.181802; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Chelba Ciprian, 2013, ARXIV13123005; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605; Elsayed G., 2018, ADV NEURAL INFORM PR, P842; Gal Y., 2015, ARXIV PREPRINT ARXIV; Gal Y., 2016, THESIS, V1, P3; Geifman Y, 2019, PR MACH LEARN RES, V97; Geifman Y, 2017, ADV NEUR IN, V30; Goh GB, 2017, J COMPUT CHEM, V38, P1291, DOI 10.1002/jcc.24764; Hastie T, 2009, ELEMENTS STAT LEARNI; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hechtlinger Y., 2018, ARXIV PREPRINT ARXIV; Hinton G, 2018, JAMA-J AM MED ASSOC, V320, P1101, DOI 10.1001/jama.2018.11100; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Liang Paul Pu., 2018, P 2018 C EMPIRICAL M, P150, DOI [10.18653/v1/d18-1014, DOI 10.18653/V1/D18-1014, 10.18653/v1/D18-1014]; Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Neubig G., 2017, ARXIV PREPRINT ARXIV; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Pearce Tim, 2018, ARXIV181005546; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Rolnick D., 2017, ARXIV170510694; Ros AS, 2018, AAAI CONF ARTIF INTE, P1660; Sacramento J, 2018, ADV NEUR IN, V31; Szepesvfri Csaba, 2009, ALGORITHMS FOR REINF; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Vandat A, 2017, ADV NEUR IN, V30; Vaswani A., 2017, ADV NEURAL INFORM PR, V30	41	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902027
C	Anari, N; Daskalakis, C; Maass, W; Papadimitriou, CH; Saberi, A; Vempala, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Anari, Nima; Daskalakis, Constantinos; Maass, Wolfgang; Papadimitriou, Christos H.; Saberi, Amin; Vempala, Santosh			Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon Bhaskara et al. [3] but allows for a wider range of perturbation models, including discrete ones. We give an application to recovering assemblies of neurons. Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies. This suggests that an animal's memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data. Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their l-wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram.	[Anari, Nima] Stanford Univ, Comp Sci, Stanford, CA 94305 USA; [Daskalakis, Constantinos] MIT, EECS, Cambridge, MA 02139 USA; [Maass, Wolfgang] Graz Univ Technol, Theoret Comp Sci, Graz, Austria; [Papadimitriou, Christos H.] Columbia Univ, Comp Sci, New York, NY 10027 USA; [Saberi, Amin] Stanford Univ, MS&E, Stanford, CA 94305 USA; [Vempala, Santosh] Georgia Tech, Comp Sci, Atlanta, GA USA	Stanford University; Massachusetts Institute of Technology (MIT); Graz University of Technology; Columbia University; Stanford University; University System of Georgia; Georgia Institute of Technology	Anari, N (corresponding author), Stanford Univ, Comp Sci, Stanford, CA 94305 USA.	anari@cs.stanford.edu; costis@csail.mit.edu; maass@igi.tugraz.at; christos@cs.columbia.edu; saberi@stanford.edu; vempala@gatech.edu		Anari, Nima/0000-0002-4394-3530				[Anonymous], 2012, PROC 25 ANN C LEARN; Barak B, 2015, ACM S THEORY COMPUT, P143, DOI 10.1145/2746539.2746605; Bhaskara A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P594, DOI 10.1145/2591796.2591881; Buzsaki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023; Chang JT, 1996, MATH BIOSCI, V137, P51, DOI 10.1016/S0025-5564(96)00075-2; De Falco E, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13408; De Lathauwer L, 2007, IEEE T SIGNAL PROCES, V55, P2965, DOI 10.1109/TSP.2007.893943; Ge R., 2015, ARXIV150405287; Goyal N, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P584, DOI 10.1145/2591796.2591875; HASTAD J, 1990, J ALGORITHMS, V11, P644, DOI 10.1016/0196-6774(90)90014-6; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Hopkins SB, 2016, ACM S THEORY COMPUT, P178, DOI 10.1145/2897518.2897529; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Ison MJ, 2015, NEURON, V87, P220, DOI 10.1016/j.neuron.2015.06.016; Kolda TG, 2011, SIAM J MATRIX ANAL A, V32, P1095, DOI 10.1137/100801482; LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071; Ma, 2017, ADV NEURAL INFORM PR, P3656; Mossel Elchanan, 2005, P 37 ANN ACM S THEOR, P366; PITOWSKY I, 1991, MATH PROGRAM, V50, P395, DOI 10.1007/BF01594946; Quiroga RQ, 2012, NAT REV NEUROSCI, V13, P587, DOI 10.1038/nrn3251; Tengyu Ma, 2016, 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS). Proceedings, P438, DOI 10.1109/FOCS.2016.54	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005044
C	Bai, WR; Noble, WS; Bilmes, JA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bai, Wenruo; Noble, William S.; Bilmes, Jeff A.			Submodular Maximization via Gradient Ascent: The Case of Deep Submodular Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FUNCTION SUBJECT; ALGORITHMS; LOCATION	We study the problem of maximizing deep submodular functions (DSFs) [13, 3] subject to a matroid constraint. DSFs are an expressive class of submodular functions that include, as strict subfamilies, the facility location, weighted coverage, and sums of concave composed with modular functions. We use a strategy similar to the continuous greedy approach [6], but we show that the multilinear extension of any DSF has a natural and computationally attainable concave relaxation that we can optimize using gradient ascent. Our results show a guarantee of max(0<delta<1)(1-epsilon-delta-e(-delta 2 Omega(k))) with a running time of O(n(2) /epsilon(2) ) plus time for pipage rounding [6] to recover a discrete solution, where k is the rank of the matroid constraint. This bound is often better than the standard 1 - 1/e guarantee of the continuous greedy algorithm, but runs much faster. Our bound also holds even for fully curved (c = 1) functions where the guarantee of 1 - c/e degenerates to 1 - 1/e where c is the curvature of f [37]. We perform computational experiments that support our theoretical results.	[Bai, Wenruo; Bilmes, Jeff A.] Dept Elect & Comp Engn, Seattle, WA 98195 USA; [Noble, William S.; Bilmes, Jeff A.] Dept Comp Sci & Engn, Seattle, WA 98195 USA; [Noble, William S.] Dept Genome Sci, Seattle, WA 98195 USA		Bai, WR (corresponding author), Dept Elect & Comp Engn, Seattle, WA 98195 USA.	wrbai@uw.edu; wnoble@uw.edu; bilmes@uw.edu			National Science Foundation [IIS-1162606]; National Institutes of Health [R01GM103544]; Google; Microsoft; Intel; CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program - DARPA	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Google(Google Incorporated); Microsoft(Microsoft); Intel(Intel Corporation); CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program - DARPA	This material is based upon work supported by the National Science Foundation under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft, and an Intel research award. This research is also supported by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.	Abadi M, 2015, P 12 USENIX S OPERAT; Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2; Bilmes J., 2017, ARXIV170108939; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bubeck S., 2015, FDN TRENDS MACHINE L; Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Chakrabarty Deeparnab, 2016, ARXIV161009800; Chekuri C, 2004, LECT NOTES COMPUT SC, V3122, P72; Chekuri Chandra, 2009, ARXIV09094348; CORNUEJOLS G, 1977, MANAGE SCI, V23, P789, DOI 10.1287/mnsc.23.8.789; CUNNINGHAM WH, 1985, COMBINATORICA, V5, P185, DOI 10.1007/BF02579361; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; Dughmi Shaddin, 2009, ARXIV09120322; Fujito T, 2000, IEICE T FUND ELECTR, VE83A, P480; Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928; Hassani H, 2017, ADV NEURAL INFORM PR, P5841; Hui Lin, 2011, P 49 ANN M ASS COMP, V2, P170; Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589; Kai Wei, 2015, INT C MACH LEARN ICM; Kai Wei, 2014, P ICASSP; Kai Wei, 2016, BIORXIV; Karimi M.R., 2017, NIPS, P6856; Kirchhoff K., 2014, P 2014 C EMPIRICAL M, P131, DOI [10.3115/v1/D14-1014, DOI 10.3115/V1/D14-1014]; Kohli P, 2009, IEEE T PATTERN ANAL, V31, P1645, DOI 10.1109/TPAMI.2008.217; Krause A, 2006, IPSN 2006: THE FIFTH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P2; Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68; Libbrecht M. W., 2018, PROTEIN-STRUCT FUNCT; Lin Hui, 2009, ASRU; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Mokhtari Aryan, 2018, AISTATS, P1886; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; RAGHAVAN P, 1988, J COMPUT SYST SCI, V37, P130, DOI 10.1016/0022-0000(88)90003-7; Soma T., 2015, P ADV NEURAL INFORM, P847; Stobbe P., 2010, ADV NEURAL INFORM PR, P2208; Sviridenko M., 2015, PROC 26 ACM SIAM S D, P1134; Vetta A, 2002, ANN IEEE SYMP FOUND, P416, DOI 10.1109/SFCS.2002.1181966; WDolhansky Brian, 2016, ADV NEURAL INFORM PR, P3396	41	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA	30705579				2022-12-19	WOS:000461852002052
C	Baker, J; Fearnhead, P; Fox, EB; Nemeth, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Baker, Jack; Fearnhead, Paul; Fox, Emily B.; Nemeth, Christopher			Large-Scale Stochastic Sampling from the Probability Simplex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DIRICHLET; LANGEVIN	Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference. These methods are based on sampling a discrete-time approximation to a continuous time process, such as the Langevin diffusion. When applied to distributions defined on a constrained space the time-discretization error can dominate when we are near the boundary of the space. We demonstrate that because of this, current SGMCMC methods for the simplex struggle with sparse simplex spaces; when many of the components are close to zero. Unfortunately, many popular large-scale Bayesian models, such as network or topic models, require inference on sparse simplex spaces. To avoid the biases caused by this discretization error, we propose the stochastic Cox-Ingersoll-Ross process (SCIR), which removes all discretization error and we prove that samples from the SCIR process are asymptotically unbiased. We discuss how this idea can be extended to target other constrained spaces. Use of the SCIR process within a SGMCMC algorithm is shown to give substantially better performance for a topic model and a Dirichlet process mixture model than existing SGMCMC approaches.	[Baker, Jack] Univ Lancaster, STOR I CDT, Math & Stat, Lancaster, England; [Fearnhead, Paul; Nemeth, Christopher] Univ Lancaster, Math & Stat, Lancaster, England; [Fox, Emily B.] Univ Washington, Comp Sci & Engn & Stat, Seattle, WA 98195 USA	Lancaster University; Lancaster University; University of Washington; University of Washington Seattle	Baker, J (corresponding author), Univ Lancaster, STOR I CDT, Math & Stat, Lancaster, England.	j.baker1@lancaster.ac.uk; p.fearnhead@lancaster.ac.uk; ebfox@uw.edu; c.nemeth@lancaster.ac.uk	Nemeth, Christopher/I-2881-2019; Fearnhead, Paul/A-1653-2008	Nemeth, Christopher/0000-0002-9084-3866; Fearnhead, Paul/0000-0002-9386-2341; Fox, Emily/0000-0003-3188-9685	EPSRC [EP/L015692/1, EP/K014463/1, EP/R018561/1, EP/S00159X/1, EP/R01860X/1]; ONR [N00014-15-1-2380]; NSF CAREER Award [IIS-1350133]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ONR(Office of Naval Research); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	Jack Baker gratefully acknowledges the support of the EPSRC funded EP/L015692/1 STOR-i Centre for Doctoral Training Paul Fearnhead was supported by EPSRC grants EP/K014463/1 and EP/R018561/1. Christopher Nemeth acknowledges the support of EPSRC grants EP/S00159X/1 and EP/R01860X/1. Emily Fox acknowledges the support of ONR Grant N00014-15-1-2380 and NSF CAREER Award IIS-1350133.	Baker J., 2017, CONTROL VARIATES STO; BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Breese J. S., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P43; Chatterji N. S., 2018, THEORY VARIANCE REDU; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; COX JC, 1985, ECONOMETRICA, V53, P385, DOI 10.2307/1911242; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154; Dunson DB, 2009, J AM STAT ASSOC, V104, P1042, DOI 10.1198/jasa.2009.tm08439; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Kalli M, 2011, STAT COMPUT, V21, P93, DOI 10.1007/s11222-009-9150-y; Li WZ, 2016, JMLR WORKSH CONF PRO, V51, P723; Liverani S, 2015, J STAT SOFTW, V64, P1; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; NAGAPETYAN T., 2017, TRUE COST STOCHASTIC; Papaspiliopoulos O., 2008, TECHNICAL REPORT; Papaspiliopoulos O, 2008, BIOMETRIKA, V95, P169, DOI 10.1093/biomet/asm086; Patterson S., 2013, P 26 INT C NEUR INF, P3102; ROSENBLATT M, 1952, ANN MATH STAT, V23, P470, DOI 10.1214/aoms/1177729394; Sato I, 2014, PR MACH LEARN RES, V32, P982; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Walker SG, 2007, COMMUN STAT-SIMUL C, V36, P45, DOI 10.1080/03610910601096262; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zygalakis KC, 2011, SIAM J SCI COMPUT, V33, P102, DOI 10.1137/090762336	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001028
C	Balkanski, E; Breuer, A; Singer, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Balkanski, Eric; Breuer, Adam; Singer, Yaron			Non-monotone Submodular Maximization in Exponentially Fewer Iterations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper we consider parallelization for applications whose objective can be expressed as maximizing a non-monotone submodular function under a cardinality constraint. Our main result is an algorithm whose approximation is arbitrarily close to 1/2e in O( log(2) n) adaptive rounds, where n is the size of the ground set. This is an exponential speedup in parallel running time over any previously studied algorithm for constrained non-monotone submodular maximization. Beyond its provable guarantees, the algorithm performs well in practice. Specifically, experiments on traffic monitoring and personalized data summarization applications show that the algorithm finds solutions whose values are competitive with state-of-the-art algorithms while running in exponentially fewer parallel iterations.	[Balkanski, Eric; Breuer, Adam; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Balkanski, E (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	ericbalkanski@g.harvard.edu; breuer@g.harvard.edu; yaron@seas.harvard.edu			Google PhD Fellowship; NSF grant CAREER [CCF 1452961]; NSF [CCF 1301976]; BSF grant [2014389]; NSF USICCS proposal [1540428]; Google Research award; Facebook research award	Google PhD Fellowship(Google Incorporated); NSF grant CAREER; NSF(National Science Foundation (NSF)); BSF grant(US-Israel Binational Science Foundation); NSF USICCS proposal; Google Research award(Google Incorporated); Facebook research award(Facebook Inc)	This research was supported by a Google PhD Fellowship, NSF grant CAREER CCF 1452961, NSF CCF 1301976, BSF grant 2014389, NSF USICCS proposal 1540428, a Google Research award, and a Facebook research award.	Agarwal A., 2017, COLT, V65, P39; Balkanski Eric, 2018, ARXIV180406355; Balkanski Eric, 2018, ICML; Balkanski Eric, 2018, STOC; Barbosa RD, 2016, ANN IEEE SYMP FOUND, P645, DOI 10.1109/FOCS.2016.74; BERGER B, 1989, ANN IEEE SYMP FOUND, P54, DOI 10.1109/SFCS.1989.63455; Blelloch G. E., 1998, SPAA '98. Tenth Annual ACM Symposium on Parallel Algorithms and Architectures, P16, DOI 10.1145/277651.277660; Blelloch GE, 1996, COMMUN ACM, V39, P85, DOI 10.1145/227234.227246; Blelloch GE, 2012, P 24 ANN ACM S PAR A, P82; Blelloch GE, 2011, SPAA 11: PROCEEDINGS OF THE TWENTY-THIRD ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P23; Braverman M, 2016, ACM S THEORY COMPUT, P851, DOI 10.1145/2897518.2897642; Buchbinder N., 2014, P 25 ANN ACM SIAM S, P1433; Buhrman H, 2012, NON ADAPTIVE QUERY C; CalTrans, PEMS CAL PERF MEAS S; Canonne C., 2017, ARXIV170205678; Chekuri C, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P201, DOI 10.1145/2688073.2688086; Chen X., 2017, ARXIV170406314; Chierichetti, 2010, P 19 INT C WORLD WID, P231, DOI 10.1145/1772690.1772715.; COLE R, 1988, SIAM J COMPUT, V17, P770, DOI 10.1137/0217049; Duris P., 1984, P 16 ANN ACM S THEOR, P81; Ene A, 2016, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2016.34; Ene Alina, 2018, ARXIV180405379; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46; Feldman Moran, 2015, KNOWLEDGE INFORM SYS, V42; Gharan SO, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1098; Gupta A, 2010, LECT NOTES COMPUT SC, V6484, P246, DOI 10.1007/978-3-642-17572-5_20; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Haupt Jarvis D., 2009, 2009 43rd Asilomar Conference on Signals, Systems and Computers, P1551, DOI 10.1109/ACSSC.2009.5470138; Haupt J, 2009, 2009 IEEE 13TH DIGITAL SIGNAL PROCESSING WORKSHOP & 5TH IEEE PROCESSING EDUCATION WORKSHOP, VOLS 1 AND 2, PROCEEDINGS, P702, DOI 10.1109/DSP.2009.4786013; Indyk P, 2011, ANN IEEE SYMP FOUND, P285, DOI 10.1109/FOCS.2011.83; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kumar R., 2015, TOPC, V2, p14:1, DOI DOI 10.1145/2809814; Lee J, 2009, ACM S THEORY COMPUT, P323; Mirrokni V, 2015, ACM S THEORY COMPUT, P153, DOI 10.1145/2746539.2746624; Mirzasoleiman B, 2016, PR MACH LEARN RES, V48; Namkoong H, 2017, PR MACH LEARN RES, V70; Nisan N., 1991, P 23 ANN ACM S THEOR, P419; PAPADIMITRIOU CH, 1984, J COMPUT SYST SCI, V28, P260, DOI 10.1016/0022-0000(84)90069-2; Rajagopalan S, 1998, SIAM J COMPUT, V28, P526; Valiant L. G., 1975, SIAM Journal on Computing, V4, P348, DOI 10.1137/0204030	42	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302037
C	Bernacchia, A; Lengyel, M; Hennequin, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bernacchia, Alberto; Lengyel, Mate; Hennequin, Guillaume			Exact natural gradient in deep linear networks and application to the nonlinear case	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	Stochastic gradient descent (SGD) remains the method of choice for deep learning, despite the limitations arising for ill-behaved objective functions. In cases where it could be estimated, the natural gradient has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function, but little is known theoretically about its convergence properties, and it has yet to find a practical implementation that would scale to very deep and large networks. Here, we derive an exact expression for the natural gradient in deep linear networks, which exhibit pathological curvature similar to the nonlinear case. We provide for the first time an analytical solution for its convergence rate, showing that the loss decreases exponentially to the global minimum in parameter space. Our expression for the natural gradient is surprisingly simple, computationally tractable, and explains why some approximations proposed previously work well in practice. This opens new avenues for approximating the natural gradient in the nonlinear case, and we show in preliminary experiments that our online natural gradient descent outperforms SGD on MNIST autoencoding while sharing its computational simplicity.	[Bernacchia, Alberto; Lengyel, Mate; Hennequin, Guillaume] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England; [Lengyel, Mate] Cent European Univ, Dept Cognit Sci, H-1051 Budapest, Hungary	University of Cambridge; Central European University	Bernacchia, A (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.	ab2347@cam.ac.uk; m.lengyel@eng.cam.ac.uk; g.hennequin@eng.cam.ac.uk	Lengyel, Mate/A-6665-2013	Lengyel, Mate/0000-0001-7266-0049	Wellcome Trust [202111/Z/16/Z]; Wellcome Trust Investigator Award [095621/Z/11/Z]	Wellcome Trust(Wellcome TrustEuropean Commission); Wellcome Trust Investigator Award(Wellcome Trust)	We thank Richard Turner and James Martens for discussions. This work was supported by Wellcome Trust Seed Award 202111/Z/16/Z (G.H.) and Wellcome Trust Investigator Award 095621/Z/11/Z (A.B., M.L.).	Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], 2014, ARXIV14107455; Ba J., 2016, P 5 INT C LEARN REPR; Bishop CM, 2016, PATTERN RECOGN; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Desjardins G., 2015, P 28 C NEUR PROC SYS, P2071; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fujimoto Y, 2018, LECT NOTES ARTIF INT, V10841, P47, DOI 10.1007/978-3-319-91253-0_5; Grosse R, 2016, PR MACH LEARN RES, V48; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Heskes T, 2000, NEURAL COMPUT, V12, P881, DOI 10.1162/089976600300015637; Kingma D. P., 2014, ADAM METHOD STOCHAST; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Luo H., 2017, ARXIV170207097; Martens J., 2010, P 27 INT C MACH LEAR, P735; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Martens James, 2014, NEW INSIGHTS PERSPEC; Ollivier Y, 2015, INF INFERENCE, V4, P108, DOI 10.1093/imaiai/iav006; Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4; Pascanu R., 2013, P 2 INT C LEARN REPR; ROUx N. L., 2008, ADV NEURAL INFORM PR, V20, P849; Saxe A. M., 2013, P 2 INT C LEARN REPR; Vinyals Oriol, 2012, ARTIF INTELL, P1261; Yang HH, 1998, NEURAL COMPUT, V10, P2137, DOI 10.1162/089976698300017007; Zeiler M.D, 2012, CORR ABS12125701	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000044
C	Branzei, S; Mehta, R; Nisan, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Branzei, Simina; Mehta, Ruta; Nisan, Noam			Universal Growth in Production Economies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA						[Branzei, Simina] Purdue Univ, W Lafayette, IN 47907 USA; [Mehta, Ruta] Univ Illinois, Urbana, IL 61801 USA; [Nisan, Noam] Hebrew Univ Jerusalem, Jerusalem, Israel; [Nisan, Noam] Microsoft Res, Redmond, WA USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; University of Illinois System; University of Illinois Urbana-Champaign; Hebrew University of Jerusalem; Microsoft	Branzei, S (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	simina@purdue.edu; rutamehta@illinois.edu; noam@cs.huji.ac.il			European Research Council (ERC) under the European Union [740282]; ISF [1435/14]; Israel-USA Bi-national Science Foundation (BSF) grant [2014389]; NSF [CCF 1750436]	European Research Council (ERC) under the European Union(European Research Council (ERC)); ISF(Israel Science Foundation); Israel-USA Bi-national Science Foundation (BSF) grant; NSF(National Science Foundation (NSF))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 740282), from the ISF grant 1435/14 administered by the Israeli Academy of Sciences and Israel-USA Bi-national Science Foundation (BSF) grant 2014389, and from the NSF grant CCF 1750436.		0	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													1	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302001
C	Casale, FP; Dalca, AV; Saglietti, L; Listgarten, J; Fusi, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Casale, Francesco Paolo; Dalca, Adrian V.; Saglietti, Luca; Listgarten, Jennifer; Fusi, Nicolo			Gaussian Process Prior Variational Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Variational autoencoders (VAE) are a powerful and widely-used class of models to learn complex data distributions in an unsupervised fashion. One important limitation of VAEs is the prior assumption that latent sample representations are independent and identically distributed. However, for many important datasets, such as time-series of images, this assumption is too strong: accounting for covariances between samples, such as those in time, can yield to a more appropriate model specification and improve performance in downstream tasks. In this work, we introduce a new model, the Gaussian Process (GP) Prior Variational Autoencoder (GPPVAE), to specifically address this issue. The GPPVAE aims to combine the power of VAEs with the ability to model correlations afforded by GP priors. To achieve efficient inference in this new class of models, we leverage structure in the covariance matrix, and introduce a new stochastic backpropagation strategy that allows for computing stochastic gradients in a distributed and low-memory fashion. We show that our method outperforms conditional VAEs (CVAEs) and an adaptation of standard VAEs in two image data applications.	[Casale, Francesco Paolo; Saglietti, Luca; Fusi, Nicolo] Microsoft Res New England, Cambridge, MA 02142 USA; [Dalca, Adrian V.] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Dalca, Adrian V.] HMS, MGH, Martinos Ctr Biomed Imaging, Boston, MA USA; [Saglietti, Luca] Italian Inst Genom Med, Turin, Italy; [Listgarten, Jennifer] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA	Microsoft; Massachusetts Institute of Technology (MIT); Harvard University; Harvard Medical School; Italian Institute for Genomic Medicine (IIGM); University of California System; University of California Berkeley	Casale, FP (corresponding author), Microsoft Res New England, Cambridge, MA 02142 USA.	frcasale@microsoft.com	Dalca, Adrian/U-4037-2019		NSF [0339122]	NSF(National Science Foundation (NSF))	Stimulus images courtesy of Michael J. Tarr, Center for the Neural Basis of Cognition and Department of Psychology, Carnegie Mellon University, http://www.tarrlab.org.Funding provided by NSF award 0339122.	[Anonymous], 2007, J MACHINE LEARNING R; [Anonymous], 1997, MATRIX ALGEBRA STAT; Ba J., 2017, P 3 INT C LEARN REPR; Bauer M., 2016, NEURIPS, P1533; Casale FP, 2017, PLOS GENET, V13, DOI 10.1371/journal.pgen.1006693; Casale FP, 2015, NAT METHODS, V12, P755, DOI [10.1038/NMETH.3439, 10.1038/nmeth.3439]; Chen Xi, 2016, ARXIV161102731; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Dalca AV, 2015, LECT NOTES COMPUT SC, V9351, P519, DOI 10.1007/978-3-319-24574-4_62; Durrande Nicolas, 2011, ARXIV11116233; Gal Yarin, 2014, ADV NEURAL INF PROCE, V2, P3257; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HENDERSON HV, 1981, SIAM REV, V23, P53, DOI 10.1137/1023004; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hoffman Matthew D, 2016, WORKSH ADV APPR BAY; Hou XX, 2017, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2017.131; Jiang Z., 2016, ARXIV PREPRINT ARXIV; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kingma DP, 2 INT C LEARN REPR I, P1; Kulkarni TD, 2015, ADV NEUR IN, V28; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2; Lonsdale J, 2013, NAT GENET, V45, P580, DOI 10.1038/ng.2653; Maaloe L., 2016, ARXIV160205473; Nalisnick E., 2016, NIPS WORKSH BAY DEEP, V2; Pandey G, 2017, IEEE IJCNN, P308, DOI 10.1109/IJCNN.2017.7965870; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rakitsch B., 2013, PROC 26 INT C NEURAL, V26, P1466; Ranganath R, 2016, PR MACH LEARN RES, V48; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rezende D.J., 2014, PROC INT CONFER ENCE; Rezende Danilo Jimenez, 2015, ARXIV150505770; Righi G, 2012, VIS COGN, V20, P143, DOI 10.1080/13506285.2012.654624; Shu Rui, 2016, ECCV WORKSH ACT ANT, V2; Siddharth N, 2017, ARXIV E PRINTS; Siddharth N., 2016, ARXIV161107492; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Sohn Kihyuk, 2015, NEURAL INFORM PROCES; Stegle Oliver, 2011, ADV NEURAL INFORM PR, P630; Suzuki Masahiro, 2016, ARXIV161101891; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Tomczak J. M., 2017, ARXIV170507120; Tran D., 2015, ARXIV151106499; Vedantam Ramakrishna, 2017, ARXIV1705010762; Wang Weiran, 2016, 161003454 ARXIV; Wilson A., 2013, INT C MACH LEARN, P1067; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wu M., 2018, ARXIV180205335	51	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004088
C	Chang, S; Yang, J; Choi, J; Kwak, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chang, Simyung; Yang, John; Choi, Jaeseok; Kwak, Nojun			Genetic-Gated Networks for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce the Genetic-Gated Networks (G2Ns), simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer(s) of networks. Our method can take both advantages of gradient-free optimization and gradient-based optimization methods, of which the former is effective for problems with multiple local minima, while the latter can quickly find local minima. In addition, multiple chromosomes can define different models, making it easy to construct multiple models and can be effectively applied to problems that require multiple models. We show that this G2N can be applied to typical reinforcement learning algorithms to achieve a large improvement in sample efficiency and performance.	[Chang, Simyung] Seoul Natl Univ, Samsung Elect, Seoul, South Korea; [Yang, John; Choi, Jaeseok; Kwak, Nojun] Seoul Natl Univ, Seoul, South Korea	Samsung; Samsung Electronics; Seoul National University (SNU); Seoul National University (SNU)	Chang, S (corresponding author), Seoul Natl Univ, Samsung Elect, Seoul, South Korea.	timelighter@snu.ac.kr; yjohn@snu.ac.kr; jaeseok.choi@snu.ac.kr; nojunk@snu.ac.kr			Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) [2017M3C4A7077582]	Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF)	This work was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) (2017M3C4A7077582).	[Anonymous], 2017, CORR; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Brockman G., 2016, OPENAI GYM; Fortunato M., 2017, NOISY NETWORKS EXPLO; Hausknecht M, 2014, IEEE T COMP INTEL AI, V6, P355, DOI 10.1109/TCIAIG.2013.2294713; HOLLAND JH, 1992, SCI AM, V267, P66, DOI 10.1038/scientificamerican0792-66; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Osband Ian, 2017, ARXIV170307608; Plappert Matthias, 2017, ARXIV170601905; Ruckstiess T, 2008, LECT NOTES ARTIF INT, V5212, P234, DOI 10.1007/978-3-540-87481-2_16; Ruffio E, 2011, TUTORIAL 2 ZERO ORDE; Salimans T., 2017, ARXIV170303864; Schulman J., 2017, ABS170706347 CORR; Silver D, 2014, ICML ICML 14, P387; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wang Z, 2015, FINANCIAL REPORTING; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu Y., 2017, NIPS 2017, P5279	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301071
C	Chen, MS; Yang, LF; Wang, MD; Zhao, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Minshuo; Yang, Lin F.; Wang, Mengdi; Zhao, Tuo			Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Stochastic optimization naturally arises in machine learning. Efficient algorithms with provable guarantees, however, are still largely missing, when the objective function is nonconvex and the data points are dependent. This paper studies this fundamental challenge through a streaming PCA problem for stationary time series data. Specifically, our goal is to estimate the principle component of time series data with respect to the covariance matrix of the stationary distribution. Computationally, we propose a variant of Oja's algorithm combined with downsampling to control the bias of the stochastic gradient caused by the data dependency. Theoretically, we quantify the uncertainty of our proposed stochastic algorithm based on diffusion approximations. This allows us to prove the asymptotic rate of convergence and further implies near optimal asymptotic sample complexity. Numerical experiments are provided to support our analysis.	[Chen, Minshuo; Zhao, Tuo] Georgia Inst Technol, Atlanta, GA 30332 USA; [Yang, Lin F.; Wang, Mengdi] Princeton Univ, Princeton, NJ 08544 USA	University System of Georgia; Georgia Institute of Technology; Princeton University	Chen, MS (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	mchen393@gatech.edu; lin.yang@princeton.edu; mengdiw@princeton.edu; tourzhao@gatech.edu		Wang, Mengdi/0000-0002-2101-9507				ALLEN- ZHU Z., 2016, ARXIV160707837; ALLEN- ZHU Z., 2017, ARXIV170808694; [Anonymous], 2016, ARXIV161209296; Bottou L, 1998, NEURAL NETWORKS, V17, P142; Bradley RC, 2005, PROBAB SURV, V2, P107, DOI 10.1214/154957805100000104; CHEN Z., 2017, INT C MACH LEARN; CHUNG K. L., 2004, CHANCE CHOICE MEMORA, P79; De Vito S, 2008, SENSOR ACTUAT B-CHEM, V129, P750, DOI 10.1016/j.snb.2007.09.060; Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659; Durrett R., 2010, PROBABILITY THEORY E, V4th ed., DOI 10.1017/CBO9780511779398; Ethier S.N., 2009, MARKOV PROCESSES CHA, V282, DOI [10.1002/9780470316658, DOI 10.1002/9780470316658]; Freidlin M I, 1998, GRUND MATH WISS, V260, DOI 10.1007/978-1-4612-0611-82; Ge R., 2016, NEURIPS, P2973; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Glynn P.W., 1990, HDB OPERATIONS RES, V2, P145; HALL E. C., 2016, ARXIV160502693; Homem-De-Mello T, 2008, SIAM J OPTIMIZ, V19, P524, DOI 10.1137/060657418; HSU D. J., 2015, ADV NEURAL INFORM PR; JAIN P., 2016, C LEARN THEOR; Lee JD, 2019, MATH PROGRAM, V176, P311, DOI 10.1007/s10107-019-01374-3; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Reddi S., 2016, ADV NEURAL INFORM PR; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; SACKS J, 1958, ANN MATH STAT, V29, P373, DOI 10.1214/aoms/1177706619; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shamir Ohad, 2013, INT C MACH LEARN; SREBRO N., 2004, ADV NEURAL INFORM PR; SUN J., 2015, SAMPL THEOR APPL SAM; SUN J., 2016, INF THEOR ISIT 2016; TJOSTHEIM D, 1990, ADV APPL PROBAB, V22, P587, DOI 10.2307/1427459	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303049
C	Chen, XW; Huang, WR; Chen, W; Lui, JCS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Xiaowei; Huang, Weiran; Chen, Wei; Lui, John C. S.			Community Exploration: From Offline Optimization to Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FINITE-TIME ANALYSIS; SUMS	We introduce the community exploration problem that has many real-world applications such as online advertising. In the problem, an explorer allocates limited budget to explore communities so as to maximize the number of members he could meet. We provide a systematic study of the community exploration problem, from offline optimization to online learning. For the offline setting where the sizes of communities are known, we prove that the greedy methods for both of non-adaptive exploration and adaptive exploration are optimal. For the online setting where the sizes of communities are not known and need to be learned from the multi-round explorations, we propose an "upper confidence" like algorithm that achieves the logarithmic regret bounds. By combining the feedback from different rounds, we can achieve a constant regret bound.	[Chen, Xiaowei; Lui, John C. S.] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Huang, Weiran] Huawei Noahs Ark Lab, Hong Kong, Peoples R China; [Chen, Wei] Microsoft Res, San Diego, CA USA	Chinese University of Hong Kong; Huawei Technologies; Microsoft	Chen, XW (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	xwchen@cse.cuhk.edu.hk; huang.inbox@outlook.com; weic@microsoft.com; cslui@cse.cuhk.edu.hk		Huang, Weiran/0000-0003-1193-6157; Chen, Wei/0000-0003-0065-3610	National Natural Science Foundation of China [61433014]; GRF [14208816]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); GRF	We thank Jing Yu from School of Mathematical Sciences at Fudan University for her insightful discussion on the offline problems, especially, we thank Jing Yu for her method to find a good initial allocation, which leads to a faster greedy method. Wei Chen is partially supported by the National Natural Science Foundation of China (Grant No. 61433014). The work of John C.S. Lui is supported in part by the GRF Grant 14208816.	Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Berry Donald A., 1985, MONOGRAPHS STAT APPL, V5, P71; Bressan Marco, 2015, ARXIV151207901; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck S, 2013, J MACH LEARN RES, V14, P601; Chen W., 2016, P ADV NEUR INF PROC, P1651; Chen Weigen, 2013, T CHINA ELECTROTECHN, V01, P50, DOI DOI 10.1145/2463209.2488881; CHRISTMAN MC, 1994, STAT SINICA, V4, P335; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1; Finkelstein M, 1998, STAT PROBABIL LETT, V37, P423, DOI 10.1016/S0167-7152(97)00146-6; Gabillon V., 2013, ADV NEURAL INFORM PR, P2697; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Janson S, 2004, RANDOM STRUCT ALGOR, V24, P234, DOI 10.1002/rsa.20008; Katzir L., 2011, WWW; Kveton B, 2015, JMLR WORKSH CONF PRO, V38, P535; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Wang Q., 2017, NIPS 2017, P1161	20	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000002
C	Cheung, YK		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cheung, Yun Kuen			Multiplicative Weights Updates with Constant Step-Size in Graphical Constant-Sum Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REPLICATOR; COMPLEXITY	Since Multiplicative Weights (MW) updates are the discrete analogue of the continuous Replicator Dynamics (RD), some researchers had expected their qualitative behaviours would be similar. We show that this is false in the context of graphical constant-sum games, which include two-person zero-sum games as special cases. In such games which have a fully-mixed Nash Equilibrium (NE), it was known that RD satisfy the permanence and Poincare recurrence properties, but we show that MW updates with any constant step-size epsilon > 0 converge to the boundary of the state space, and thus do not satisfy the two properties. Using this result, we show that MW updates have a regret lower bound of Omega(1/(epsilon T)), while it was known that the regret of RD is upper bounded by O(1/T). Interestingly, the regret perspective can be useful for better understanding of the behaviours of MW updates. In a two-person zero-sum game, if it has a unique NE which is fully mixed, then we show, via regret, that for any sufficiently small epsilon, there exist at least two probability densities and a constant Z > 0, such that for any arbitrarily small z > 0, each of the two densities fluctuates above Z and below z infinitely often.	[Cheung, Yun Kuen] Singapore Univ Technol & Design, Singapore, Singapore	Singapore University of Technology & Design	Cheung, YK (corresponding author), Singapore Univ Technol & Design, Singapore, Singapore.	yunkuen_cheung@sutd.edu.sg			Singapore NRF 2018 Fellowship [NRF-NRFF2018-07]; MOE AcRF [2016-T2-1-170]	Singapore NRF 2018 Fellowship(National Research Foundation, Singapore); MOE AcRF(Ministry of Education, Singapore)	The author would like to acknowledge Singapore NRF 2018 Fellowship NRF-NRFF2018-07 and MOE AcRF Tier 2 Grant 2016-T2-1-170. The author thanks the anonymous reviewers for their helpful suggestions and comments, and for pointing out the prior work about continuous replicator/FTRL dynamics and the interplay between them and their discrete counterparts.	Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Bailey JP, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P321, DOI 10.1145/3219166.3219235; Benaim M, 1999, LECT NOTES MATH, V1709, P1; Cai Y, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P217; Chastain E., 2013, ITCS, P57; Chen XH, 2009, J MECH PHYS SOLIDS, V57, P1, DOI 10.1016/j.jmps.2008.10.008; Daskalakis C, 2015, GAME ECON BEHAV, V92, P327, DOI 10.1016/j.geb.2014.01.003; Daskalakis C, 2009, LECT NOTES COMPUT SC, V5556, P423, DOI 10.1007/978-3-642-02930-1_35; Daskalakis C, 2009, SIAM J COMPUT, V39, P195, DOI 10.1137/070699652; Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163; GAUNERSDORFER A, 1995, GAME ECON BEHAV, V11, P279, DOI 10.1006/game.1995.1052; GAUNERSDORFER A, 1992, SIAM J APPL MATH, V52, P1476, DOI 10.1137/0152085; Hart S, 2010, GAME ECON BEHAV, V69, P107, DOI 10.1016/j.geb.2007.12.002; Hofbauer J., 1987, LECT NOTES EC MATH S, V287, p70?92; Hofbauer J., 1998, EVOLUTIONARY GAMES P; Hofbauer J, 2009, MATH OPER RES, V34, P263, DOI 10.1287/moor.1080.0359; Kwon J, 2017, J DYN GAMES, V4, P125, DOI 10.3934/jdg.2017008; Mertikopoulos P, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2703; Nagarajan SG, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P685; NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529; Palaiopanos G., 2017, ADV NEURAL INFORM PR, P5874; Piliouras G., 2014, P 25 ANN ACM SIAM S, P861; Piliouras Georgios, 2018, ITCS; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Sato Y, 2002, P NATL ACAD SCI USA, V99, P4748, DOI 10.1073/pnas.032086299; SCHUSTER P, 1981, BIOL CYBERN, V40, P1, DOI 10.1007/BF00326675; Sorin S, 2009, MATH PROGRAM, V116, P513, DOI 10.1007/s10107-007-0111-y; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989; Zeeman EC, 1980, LECT NOTES MATH, V819, P472	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303052
C	Desai, N; Critch, A; Russell, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Desai, Nishant; Critch, Andrew; Russell, Stuart			Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					It is commonly believed that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a Pareto optimal policy, i.e. a policy that cannot be improved upon for one principal without making sacrifices for another. Harsanyi's theorem shows that when the principals have a common prior on the outcome distributions of all policies, a Pareto optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals' utilities. In this paper, we derive a more precise generalization for the sequential decision setting in the case of principals with different priors on the dynamics of the environment. We refer to this generalization as the Negotiable Reinforcement Learning (NRL) framework. In this more general case, the relative weight given to each principal's utility should evolve over time according to how well the agent's observations conform with that principal's prior. To gain insight into the dynamics of this new framework, we implement a simple NRL agent and empirically examine its behavior in a simple environment.	[Desai, Nishant] Univ Calif Berkeley, Ctr Human Compatible AI, Berkeley, CA 94720 USA; [Critch, Andrew] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA; [Russell, Stuart] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of California System; University of California Berkeley	Desai, N (corresponding author), Univ Calif Berkeley, Ctr Human Compatible AI, Berkeley, CA 94720 USA.	nishantdesai@berkeley.edu; critch@berkeley.edu; russell@cs.berkeley.edu						Abbeel P., 2004, P 21 INT C MACHINE L, P1; [Anonymous], 2014, ADV NEURAL INFORM PR; Armstrong S, 2016, AI SOC, V31, P201, DOI 10.1007/s00146-015-0590-y; Baum Seth D, 2016, AI SOC, P1; Bellman RE, 1957, DYNAMIC PROGRAMMING; Bostrom N., 2014, SUPERINTELLIGENCE PA; Ghavamzadeh Mohammad, 2016, ARXIV E PRINTS; Harsanyi John C, 1980, ESSAYS ETHICS SOCIAL, P6; Roijers DM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1666; Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964; Russell S., 2016, COOPERATIVE INVERSE; SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071; Soh H, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P713; Thrun S., 2003, IJCAI, P1025, DOI DOI 10.5555/1630659.1630806; Tzeng GH, 2011, MULTIPLE ATTRIBUTE DECISION MAKING: METHODS AND APPLICATIONS, P1; Wang W., 2014, THESIS	18	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304070
C	Deudon, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Deudon, Michel			Learning semantic similarity in a continuous space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric. Our work naturally extends Word Mover's Distance (WMD) [1] by representing text documents as normal distributions instead of bags of embedded words. Our learned metric measures the dissimilarity between two questions as the minimum amount of distance the intent (hidden representation) of one question needs to "travel" to match the intent of another question. We first learn to repeat, reformulate questions to infer intents as normal distributions with a deep generative model [2] (variational auto encoder). Semantic similarity between pairs is then learned discriminatively as an optimal transport distance metric (Wasserstein 2) with our novel variational siamese framework. Among known models that can read sentences individually, our proposed framework achieves competitive results on Quora duplicate questions dataset. Our work sheds light on how deep generative models can approximate distributions (semantic representations) to effectively measure semantic similarity with meaningful distance metrics from Information Theory.	[Deudon, Michel] Ecole Polytech, Palaiseau, France	Institut Polytechnique de Paris	Deudon, M (corresponding author), Ecole Polytech, Palaiseau, France.	michel.deudon@polytechnique.edu			Ecole Polytechnique	Ecole Polytechnique	We would like to thank Ecole Polytechnique for financial support and Telecom Paris-Tech for GPU resources. We are grateful to Professor Chloe Clavel, Professor Gabriel Peyre, Constance Noziere and Paul Bertin for their critical reading of the paper. Special thanks go to Magdalena Fuentes for helping running the code on Telecom's clusters. We also thank Professor Francis Bach and Professor Guillaume Obozinski for their insightful course on probabilistic graphical models at ENS Cachan, as well as Professor Michalis Vazirgiannis for his course on Text Mining and NLP at Ecole Polytechnique. We also thank the reviewers for their valuable comments and feedback.	Abadi M, 2015, P 12 USENIX S OPERAT; Beaulieu M. M., 1997, Fifth Text REtrieval Conference (TREC-5) (NIST SP 500-238), P143; Bird S., 2009, NATURAL LANGUAGE PRO; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Conneau A, 2017, PROC 2017 C EMPIR ME, P670, DOI [10.18653/v1/d17-1070, DOI 10.18653/V1/D17-1070]; Dadashov Elkhan, CS224N; Dhingra Bhuwan, 2018, ARXIV PREPRINT ARXIV, P59, DOI DOI 10.18653/V1/W18-1708; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ganitkevitch Juri, 2013, P 2013 C N AM CHAPT, P758; Glorot X., 2010, PROC MACH LEARN RES, P249; Gong Yichen, 2018, INT C LEARN REPR ICL; Hill Felix, 2016, P 2016 C N AM CHAPT, P1367, DOI DOI 10.18653/V1/N16-1162; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Homma Yushi, CS224N; Kantorovich L., 1942, DOKL AKAD NAUK+, V37, P227; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Li JW, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1106, DOI 10.3115/v1/p15-1107; Lin Z., 2017, ARXIV PREPRINT ARXIV; Ma Mingbo, 2015, P 53 ANN M ASS COMP, P1106; Marelli M, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION; Miao YS, 2016, PR MACH LEARN RES, V48; Mikolov T., 2013, ARXIV; Monge G., 1781, MEM MATH PHYS ACAD R, P666; Mu Jiaqi, 2018, 6 INT C LEARN REPR I; Mueller J, 2017, PR MACH LEARN RES, V70; Nickel M, 2017, ADV NEUR IN, V30; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Rehrek R, 2010, P LREC 2010 WORKSH N, DOI DOI 10.13140/2.1.2393.1847; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Sanborn Adrian, 2015, CS224D; Shen Dinghan, 2018, 32 AAAI C ART INT AA; Shen Dinghan, 2018, P 56 ANN M ASS COMP; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tomar Gaurav Singh, 2017, P 1 WORKSH SUBW CHAR, P142, DOI DOI 10.18653/V1/W17-4121; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Verges- Llahi Jaume, 2005, PATTERN RECOGN, P13; Villani C., 2003, TOPICS OPTIMAL TRANS, V58; Vilnis Luke, 2015, INT C LEARN REPR ILC; Wang Z, 2017, IEEE IJCNN, P1411, DOI 10.1109/IJCNN.2017.7966018; Wieting J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P451; Wieting John, 2016, ICLR; Williams A., 2018, P C N AM CHAPT ASS C, V1, P1112, DOI DOI 10.18653/V1/N18-1101	50	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301002
C	Dezfouli, A; Morris, R; Ramos, F; Dayan, P; Balleine, BW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dezfouli, Amir; Morris, Richard; Ramos, Fabio; Dayan, Peter; Balleine, Bernard W.			Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DECISION-MAKING; REWARD; SIGNALS; FMRI	Neuroscience studies of human decision-making abilities commonly involve subjects completing a decision-making task while BOLD signals are recorded using fMRI. Hypotheses are tested about which brain regions mediate the effect of past experience, such as rewards, on future actions. One standard approach to this is model-based fMRI data analysis, in which a model is fitted to the behavioral data, i.e., a subject's choices, and then the neural data are parsed to find brain regions whose BOLD signals are related to the model's internal signals. However, the internal mechanics of such purely behavioral models are not constrained by the neural data, and therefore might miss or mischaracterize aspects of the brain. To address this limitation, we introduce a new method using recurrent neural network models that are flexible enough to be jointly fitted to the behavioral and neural data. We trained a model so that its internal states were suitably related to neural activity during the task, while at the same time its output predicted the next action a subject would execute. We then used the fitted model to create a novel visualization of the relationship between the activity in brain regions at different times following a reward and the choices the subject subsequently made. Finally, we validated our method using a previously published dataset. We found that the model was able to recover the underlying neural substrates that were discovered by explicit model engineering in the previous work, and also derived new results regarding the temporal pattern of brain activity.	[Dezfouli, Amir; Balleine, Bernard W.] UNSW Sydney, Sydney, NSW, Australia; [Dezfouli, Amir] CSIRO, Data61, Canberra, ACT, Australia; [Morris, Richard; Ramos, Fabio] Univ Sydney, Sydney, NSW, Australia; [Dayan, Peter] UCL, Gatsby Unit, London, England	University of New South Wales Sydney; Commonwealth Scientific & Industrial Research Organisation (CSIRO); University of Sydney; University of London; University College London	Dezfouli, A (corresponding author), UNSW Sydney, Sydney, NSW, Australia.; Dezfouli, A (corresponding author), CSIRO, Data61, Canberra, ACT, Australia.	akdezfuli@gmail.com; richardumorris@gmail.com; p.dayan@ucl.ac.uk; fabio.ramos@sydney.edu.au; bernard.balleine@unsw.edu.au	Morris, Richard W/ABF-1934-2020	Morris, Richard W/0000-0002-5018-1239; Balleine, Bernard/0000-0001-8618-7950; Dezfouli, Amir/0000-0002-7633-9225	UNSW Sydney; National Health and Medical Research Council of Australia [GNT1079561]; Gatsby Charitable Foundation	UNSW Sydney; National Health and Medical Research Council of Australia(National Health and Medical Research Council (NHMRC) of Australia); Gatsby Charitable Foundation	AD and BWB were supported by funding from UNSW Sydney and the National Health and Medical Research Council of Australia GNT1079561. PD was funded by the Gatsby Charitable Foundation. Part of this work was conducted whilst PD was at Uber Technologies. Neither body played a part in its design, execution or communication. PD is affiliated with Max Planck Institute for Biological Cybernetics, Tubingen, Germany(peter.dayan@tuebingen.mpg.de).	Abadi M, 2015, P 12 USENIX S OPERAT; Balleine BW, 2010, NEUROPSYCHOPHARMACOL, V35, P48, DOI 10.1038/npp.2009.131; Bengio Y., 2014, ARXIV14061078; Breakspear M, 2017, NAT NEUROSCI, V20, P340, DOI 10.1038/nn.4497; Cohen JD, 2017, NAT NEUROSCI, V20, P304, DOI 10.1038/nn.4499; Daw ND, 2006, NATURE, V441, P876, DOI 10.1038/nature04766; Dayan P, 2002, NEURON, V36, P285, DOI 10.1016/S0896-6273(02)00963-7; Doya K, 2008, NAT NEUROSCI, V11, P410, DOI 10.1038/nn2077; Gold Joshua I, 2007, ANN REV NEUROSCIENCE, V30; Halpern David, 2018, P 11 INT C ED DAT MI; Hare TA, 2011, P NATL ACAD SCI USA, V108, P18120, DOI 10.1073/pnas.1109322108; Henson R, 2007, STATISTICAL PARAMETRIC MAPPING: THE ANALYSIS OF FUNCTIONAL BRAIN IMAGES, P178, DOI 10.1016/B978-012372560-8/50014-0; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kingma D.P, P 3 INT C LEARNING R; O'Doherty J, 2004, SCIENCE, V304, P452, DOI 10.1126/science.1094285; O'Doherty JP, 2007, ANN NY ACAD SCI, V1104, P35, DOI 10.1196/annals.1390.022; Rangel A, 2010, CURR OPIN NEUROBIOL, V20, P262, DOI 10.1016/j.conb.2010.03.001; Seo H, 2007, J NEUROSCI, V27, P8366, DOI 10.1523/JNEUROSCI.2369-07.2007; SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013; Song HF, 2017, ELIFE, V6, DOI [10.7554/elife.21492, 10.7554/eLife.21492]; Sunnaker M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002803; Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Turner BM, 2013, NEUROIMAGE, V72, P193, DOI 10.1016/j.neuroimage.2013.01.048; Walton ME, 2004, NAT NEUROSCI, V7, P1259, DOI 10.1038/nn1339; Wunderlich K, 2009, P NATL ACAD SCI USA, V106, P17199, DOI 10.1073/pnas.0901077106	27	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304026
C	Dimakopoulou, M; Osband, I; Van Roy, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dimakopoulou, Maria; Osband, Ian; Van Roy, Benjamin			Scalable Coordinated Exploration in Concurrent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling [1] and randomized value function learning [11]. We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods [1]. With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.	[Dimakopoulou, Maria; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA; [Osband, Ian] Google DeepMind, London, England	Stanford University; Google Incorporated	Dimakopoulou, M (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	madima@stanford.edu; iosband@google.com; bvr@stanford.edu						Dimakopoulou Maria, 2018, ICML; Glorot X., 2010, PROC MACH LEARN RES, P249; Gu S., 2016, DEEP REINFORCEMENT L; Guo ZH, 2015, AAAI CONF ARTIF INTE, P2624; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kim Michael Jong, 2017, IEEE T AUTOMATIC CON; Kingma D.P, P 3 INT C LEARNING R; Osband I., 2018, ARXIV180603335; Osband I, 2017, PR MACH LEARN RES, V70; Osband I, 2016, PR MACH LEARN RES, V48; Osband Ian, 2017, MULT C REINF LEARN D; Osband Ian, 2016, ARXIV160802731; Pazis Jason, 2016, AAAI; Pazis Jason, 2013, P ANN AAAI C ARTIFIC, P774; Russo Daniel, 2017, ARXIV170702038; Silver D., 2013, P INT C MACH LEARN I, P924; Strens, 2000, P 17 INT C MACH LEAR, P943; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Tassa Y., 2018, ARXIV180100690; Van Roy, 2013, ADV NEURAL INFORM PR, P3003	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304025
C	Dvurechensky, P; Dvinskikh, D; Gasnikov, A; Uribe, CA; Nedic, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dvurechensky, Pavel; Dvinskikh, Darina; Gasnikov, Alexander; Uribe, Cesar A.; Nedic, Angelia			Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISTANCE	We study the decentralized distributed computation of discrete approximations for the regularized Wasserstein barycenter of a finite set of continuous probability measures distributedly stored over a network. We assume there is a network of agents/machines/computers, and each agent holds a private continuous probability measure and seeks to compute the barycenter of all the measures in the network by getting samples from its local measure and exchanging information with its neighbors. Motivated by this problem, we develop, and analyze, a novel accelerated primal-dual stochastic gradient method for general stochastic convex optimization problems with linear equality constraints. Then, we apply this method to the decentralized distributed optimization setting to obtain a new algorithm for the distributed semi-discrete regularized Wasserstein barycenter problem. Moreover, we show explicit non-asymptotic complexity for the proposed algorithm. Finally, we show the effectiveness of our method on the distributed computation of the regularized Wasserstein barycenter of univariate Gaussian and von Mises distributions, as well as some applications to image aggregation.(1)	[Dvurechensky, Pavel; Dvinskikh, Darina] RAS, Inst Informat Transmiss Problems, Weierstrass Inst Appl Anal & Stochast, Moscow, Russia; [Gasnikov, Alexander] RAS, Inst Informat Transmiss Problems, Moscow Inst Phys & Technol, Moscow, Russia; [Uribe, Cesar A.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Nedic, Angelia] Arizona State Univ, Moscow Inst Phys & Technol, Tempe, AZ 85287 USA	Kharkevich Institute for Information Transmission Problems of the RAS; Russian Academy of Sciences; Kharkevich Institute for Information Transmission Problems of the RAS; Moscow Institute of Physics & Technology; Russian Academy of Sciences; Massachusetts Institute of Technology (MIT); Arizona State University; Arizona State University-Tempe	Dvurechensky, P (corresponding author), RAS, Inst Informat Transmiss Problems, Weierstrass Inst Appl Anal & Stochast, Moscow, Russia.	pavel.dvurechensky@wias-berlin.de; darina.dvinskikh@wias-berlin.de; gasnikov@yandex.ru; cauribe@mit.edu; angelia.nedich@asu.edu	Dvurechensky, Pavel E./P-7295-2015; Uribe, Cesar A./AAW-5779-2020; Dvinskikh, Darina/U-5035-2019	Dvurechensky, Pavel E./0000-0003-1201-2343; Uribe, Cesar A./0000-0002-7080-9724; Dvinskikh, Darina/0000-0003-1757-1021	National Science Foundation; CPS [15-44953]; Russian Science Foundation [18-71-10108]	National Science Foundation(National Science Foundation (NSF)); CPS; Russian Science Foundation(Russian Science Foundation (RSF))	The work of A. Nedic and C.A. Uribe in Sect. 5 is supported by the National Science Foundation under grant no. CPS 15-44953. The research by P. Dvurechensky, D. Dvinskikh, and A. Gasnikov in Sect. 3 and Sect. 4 was funded by the Russian Science Foundation (project 18-71-10108).	Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Anikin AS, 2017, COMP MATH MATH PHYS+, V57, P1262, DOI 10.1134/S0965542517080048; [Anonymous], 2014, ICML; Beiglbock M, 2013, FINANC STOCH, V17, P477, DOI 10.1007/s00780-013-0205-8; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Bigot J, 2017, ANN I H POINCARE-PR, V53, P1, DOI 10.1214/15-AIHP706; Bottou L., 2017, ARXIV170107875STATML; Buttazzo G, 2012, PHYS REV A, V85, DOI 10.1103/PhysRevA.85.062502; Chernov A, 2016, LECT NOTES COMPUT SC, V9869, P391, DOI 10.1007/978-3-319-44914-2_31; Claici S, 2018, PR MACH LEARN RES, V80; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Del Barrio E, 1999, ANN PROBAB, V27, P1009, DOI 10.1214/aop/1022677394; Dvurechensky P., 2018, ARXIV180603915; Dvurechensky P., 2016, P 9 INT C DISCR OPT, P584; Dvurechensky P., 2017, ARXIV170607622; Dvurechensky P, 2018, PR MACH LEARN RES, V80; Dvurechensky P, 2016, J OPTIMIZ THEORY APP, V171, P121, DOI 10.1007/s10957-016-0999-6; Ebert Johannes, 2017, ARXIV170303658; Gasnikov AV, 2016, COMP MATH MATH PHYS+, V56, P514, DOI 10.1134/S0965542516040084; Genevay A., 2016, P NEUR INF PROC SYST, P3440; Guigues V, 2017, OPTIM METHOD SOFTW, V32, P1033, DOI 10.1080/10556788.2017.1350177; Ho N., 2017, P MACHINE LEARNING R, P1501; Kantorovitch L, 1942, CR ACAD SCI URSS, V37, P199; Kolouri S, 2017, IEEE SIGNAL PROC MAG, V34, P43, DOI 10.1109/MSP.2017.2695801; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Lan G., 2018, MATH PROGRAMMING; Monge G., 1781, MEMOIRE THEORIE DEBL; Nedi~c A., 2017, ARXIV170402718; Nedic A, 2017, SIAM J OPTIMIZ, V27, P2597, DOI 10.1137/16M1084316; Nedic A, 2017, IEEE T AUTOMAT CONTR, V62, P5538, DOI 10.1109/TAC.2017.2690401; Nedic A, 2017, P AMER CONTR CONF, P3950, DOI 10.23919/ACC.2017.7963560; Olfati-Saber R., 2006, BELIEF CONSENSUS DIS, P169; Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387; Rogozin A., 2018, ARXIV180506045; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18; Scaman K, 2017, PR MACH LEARN RES, V70; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Staib M, 2017, ADV NEURAL INFORM PR, P2644; Uribe C. A., 2018, ARXIV180900710; Uribe C. A., 2017, ARXIV171200232; Uribe CA, 2018, IEEE DECIS CONTR P, P6544, DOI 10.1109/CDC.2018.8619160	45	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005035
C	Ellis, K; Morales, L; Sable-Meyer, M; Solar-Lezama, A; Tenenbaum, JB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ellis, Kevin; Morales, Lucas; Sable-Meyer, Mathias; Solar-Lezama, Armando; Tenenbaum, Joshua B.			Library Learning for Neurally-Guided Bayesian Program Induction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Successful approaches to program induction require a hand-engineered domain-specific language (DSL), constraining the space of allowed programs and imparting prior knowledge of the domain. We contribute a program induction algorithm called EC2 that learns a DSL while jointly training a neural network to efficiently search for programs in the learned DSL. We use our model to synthesize functions on lists, edit text, and solve symbolic regression problems, showing how the model learns a domain-specific library of program components for expressing solutions to problems in the domain.	[Ellis, Kevin; Morales, Lucas; Solar-Lezama, Armando; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA; [Sable-Meyer, Mathias] ENS Paris Saclay, Cachan, France	Massachusetts Institute of Technology (MIT); UDICE-French Research Universities; Universite Paris Saclay	Ellis, K (corresponding author), MIT, Cambridge, MA 02139 USA.	ellisk@mit.edu; lucasem@mit.edu; mathsm@mit.edu; asolar@csail.mit.edu; jbt@mit.edu			NSF GRFP; AFOSR [FA9550-16-1-0012]; MIT-IBM Watson AI Lab; MUSE program (Darpa) [FA8750-14-2-0242]; AWS ML Research Award; Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]	NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); MIT-IBM Watson AI Lab(International Business Machines (IBM)); MUSE program (Darpa); AWS ML Research Award; Center for Brains, Minds and Machines (CBMM) - NSF STC award	We are grateful for collaborations with Eyal Dechter, whose EC algorithm directly inspired this work, and for funding from the NSF GRFP, AFOSR award FA9550-16-1-0012, the MIT-IBM Watson AI Lab, the MUSE program (Darpa grant FA8750-14-2-0242), and an AWS ML Research Award. This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216.	Allamanis M, 2014, 22ND ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (FSE 2014), P472, DOI 10.1145/2635868.2635901; Alur Rajeev, 2016, ARXIV161107627; [Anonymous], PLDI; Balog Matej, 2016, ICLR; Bishop C.M, 2006, PATTERN RECOGN; Cho K, 2014, C EMP METH NAT LANG, DOI 10.3115/v1/D14-1179; Cohn Trevor, JMLR; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Dechter Eyal, 2013, IJCAI; Devlin J., 2017, ICML; Devlin Jacob, 2017, NIPS; Ellis Kevin, NIPS; Ellis Kevin, 2018, NIPS; Ellis Kevin, 2016, ADV NEURAL INFORM PR; Gulwani S, 2011, POPL 11: PROCEEDINGS OF THE 38TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P317, DOI 10.1145/1926385.1926423; Henderson Robert John, 2013, THESIS; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hwang Irvin, 2011, ARXIV11105667; Johnson Justin, CVPR; Kalyan A., 2018, ICLR; Katayama S, 2015, LECT NOTES ARTIF INT, V9205, P111, DOI 10.1007/978-3-319-21365-1_12; Koza J. R., 1993, GENETIC PROGRAMMING; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lau T, 2001, THESIS; Le T. A., 2017, AISTATS; Lezama A. S., 2008, THESIS; Liang Percy, 2010, ICML; Lin Dianhuan, 2014, ECAI 2014; Menon A., 2013, P 30 INT C INT C MAC, P187; Muggleton SH, 2015, MACH LEARN, V100, P49, DOI 10.1007/s10994-014-5471-y; O'Donnell T., 2015, PRODUCTIVITY REUSE L; Osera PM, 2015, ACM SIGPLAN NOTICES, V50, P619, DOI [10.1145/2737924.2738007, 10.1145/2813885.2738007]; Pierce B. C., 2002, TYPES PROGRAMMING LA; Polozov O, 2015, ACM SIGPLAN NOTICES, V50, P107, DOI [10.1145/2858965.2814310, 10.1145/2814270.2814310]; Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150; Schmid U, 2011, COGN SYST RES, V12, P237, DOI 10.1016/j.cogsys.2010.12.002; Schmidhuber J, 2004, MACH LEARN, V54, P211, DOI 10.1023/B:MACH.0000015880.99707.b2; Shin Richard, 2018, PROGRAM SYNTHESIS LE; Solomonoff R. J., 1989, 6 ISR C ART INT COMP; Stuhlmuller Andreas, 2013, NIPS	41	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002036
C	Fehr, M; Buffett, O; Thomas, V; Dibangoye, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fehr, Mathieu; Buffett, Olivier; Thomas, Vincent; Dibangoye, Junes			rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many state-of-the-art algorithms for solving Partially Observable Markov Decision Processes (POMDPs) rely on turning the problem into a "fully observable" problem-a belief MDP-and exploiting the piece-wise linearity and convexity (PWLC) of the optimal value function in this new state space (the belief simplex Delta). This approach has been extended to solving rho-POMDPs-i.e., for information-oriented criteria-when the reward rho is convex in Delta. General rho-POMDPs can also be turned into "fully observable" problems, but with no means to exploit the PWLC property. In this paper, we focus on POMDPs and rho-POMDPs with lambda rho-Lipschitz reward function, and demonstrate that, for finite horizons, the optimal value function is Lipschitz-continuous. Then, value function approximators are proposed for both upper- and lower-bounding the optimal value function, which are shown to provide uniformly improvable bounds. This allows proposing two algorithms derived from HSVI which are empirically evaluated on various benchmark problems.	[Fehr, Mathieu] Ecole Normale Super, Rue Ulm, Paris, France; [Buffett, Olivier; Thomas, Vincent] Univ Lorraine, CNRS, INRIA, LORIA, Nancy, France; [Dibangoye, Junes] Univ Lyon, INSA Lyon, INRIA, CITI, Lyon, France	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Lorraine; Inria; Institut National des Sciences Appliquees de Lyon - INSA Lyon	Fehr, M (corresponding author), Ecole Normale Super, Rue Ulm, Paris, France.	mathieu.fehr@ens.fr; olivier.buffet@loria.fr; vincent.thomas@loria.fr; jilles.dibangoye@inria.fr						Araya- Lopez M., 2010, ADV NEURAL INFORM PR; ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X; Dibangoye J., 2013, P 23 INT JOINT C ART; Dibangoye J., 2016, J ARTIFICIAL INTELLI, V55; Dufour F, 2012, J MATH ANAL APPL, V388, P1254, DOI 10.1016/j.jmaa.2011.11.015; Egorov M., 2016, P 30 AAAI C ART INT; Fonteneau R., 2009, P IEEE S APPR DYN PR; Fox D, 1998, ROBOT AUTON SYST, V25, P195, DOI 10.1016/S0921-8890(98)00049-9; Hansen E. A., 2004, P 19 NAT C ART INT A; Hinderer K, 2005, MATH METHOD OPER RES, V62, P3, DOI 10.1007/s00186-005-0438-1; Ieong S., 2007, P NAT C ART INT AAAI; Kurniawati H., 2008, ROBOTICS SCI SYSTEMS; Laraki R., 2004, MATH OPERATIONS RES, V29; Mihaylova L., 2006, NATO SCI SERIES DATA, V198; Mnih V, 2013, PLAYING ATARI DEEP R; Pineau J., 2006, J ARTIFICIAL INTELLI, V27; PINEAU J, 2003, P 18 INT JOINT C ART; Platzman L. K., 1977, THESIS; Poupart P., 2011, P 21 INT C AUT PLANN; Rachelson E., 2004, P INT S ART INT MATH; Satsangi Y., 2015, IASUVA1501; Smallwood R., 1973, OPERATION RES, V21; Smith T., 2007, THESIS; SMITH T, 2005, P 21 C UNC ART INT U; Smith T., 2004, P ANN C UNC ART INT; Sondik E.J., 1971, THESIS; Spaan M. T., 2015, AUTONOMOUS AGENTS MU, V29; Zhang NL, 2001, J ARTIF INTELL RES, V14, P29, DOI 10.1613/jair.761; Zhang Z., 2014, P 31 INT C MACH LEAR	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001047
C	Feldman, V; Vondrak, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Feldman, Vitaly; Vondrak, Jan			Generalization Bounds for Uniformly Stable Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				STABILITY	Uniform stability of a learning algorithm is a classical notion of algorithmic stability introduced to derive high-probability bounds on the generalization error (Bousquet and Elisseeff, 2002). Specifically, for a loss function with range bounded in [0, 1], the generalization error of a gamma-uniformly stable learning algorithm on n samples is known to be within O((gamma + 1/n), root n log(1/delta)) of the empirical error with probability at least 1 - delta. Unfortunately, this bound does not lead to meaningful generalization bounds in many common settings where gamma >= 1/root n. At the same time the bound is known to be tight only when gamma = O(1/n). We substantially improve generalization bounds for uniformly stable algorithms without making any additional assumptions. First, we show that the bound in this setting is O(root(gamma + 1/n) log(1/delta)) with probability at least 1 - delta. In addition, we prove a tight bound of O(gamma(2) + 1/n) on the second moment of the estimation error. The best previous bound on the second moment is O(gamma + 1/n). Our proofs are based on new analysis techniques and our results imply substantially stronger generalization guarantees for several well-studied algorithms.	[Feldman, Vitaly] Google Brain, Mountain View, CA 94043 USA; [Vondrak, Jan] Stanford Univ, Stanford, CA 94305 USA	Google Incorporated; Stanford University	Feldman, V (corresponding author), Google Brain, Mountain View, CA 94043 USA.							AbouMoustafa K. T., 2018, ISAIM; [Anonymous], P 30 INT C MACH LEAR; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P202, DOI 10.1109/TIT.1979.1056032; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork Cynthia, 2014, CORR; Dwork Cynthia, 2018, CORR; Elisseeff A, 2005, J MACH LEARN RES, V6, P55; Feldman Vitaly, 2016, CORR; Hardt M, 2016, PR MACH LEARN RES, V48; Kakade Sham M., 2008, NIPS; Kale S., 2011, P 2 S INN COMP SCI I, P487; Liu  Tongliang, 2017, P INT C MACH LEARN, P2159; London Ben, 2017, NIPS, P2935; Maurer A., 2017, C LEARN THEOR, P1461; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Nissim K., 2015, CORR; Nissim Kobbi, 2017, CORR; ROGERS WH, 1978, ANN STAT, V6, P506, DOI 10.1214/aos/1176344196; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Steinke T., 2017, ARXIV170103493; Wibisono Andre, 2009, MITCSAILTR2009060; Wu X, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1307, DOI 10.1145/3035918.3064047; Zhang T, 2003, NEURAL COMPUT, V15, P1397, DOI 10.1162/089976603321780326	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004032
C	Figueiredo, F; Borges, G; de Melo, POSV; Assuncao, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Figueiredo, Flavio; Borges, Guilherme; Vaz de Melo, Pedro O. S.; Assuncao, Renato			Fast Estimation of Causal Interactions using Wold Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We here focus on the task of learning Granger causality matrices for multivariate point processes. In order to accomplish this task, our work is the first to explore the use of Wold processes. By doing so, we are able to develop asymptotically fast MCMC learning algorithms. With N being the total number of events and K the number of processes, our learning algorithm has a O(N ( log (N) + log (K))) cost per iteration. This is much faster than the O((NK2)-K-3) or O(K-3) for the state of the art. Our approach, called GRANGER-BUSCA, is validated on nine datasets. This is an advance in relation to most prior efforts which focus mostly on subsets of the Memetracker data. Regarding accuracy, GRANGER-BUSCA is three times more accurate (in Precision @10) than the state of the art for the commonly explored subsets Memetracker. Due to GRANGER-BUSCA's much lower training complexity, our approach is the only one able to train models for larger, full, sets of data.	[Figueiredo, Flavio; Borges, Guilherme; Vaz de Melo, Pedro O. S.; Assuncao, Renato] Univ Fed Minas Gerais, Belo Horizonte, MG, Brazil	Universidade Federal de Minas Gerais	Figueiredo, F (corresponding author), Univ Fed Minas Gerais, Belo Horizonte, MG, Brazil.	flaviovdf@dcc.ufmg.br; guilherme.borges@dcc.ufmg.br; olmo@dcc.ufmg.br; assuncao@dcc.ufmg.br	de Melo, Pedro Olmo Vaz/AAE-3933-2019	de Melo, Pedro Olmo Vaz/0000-0002-9749-0151	project ATMOSPHERE (atmosphere-eubrazil.eu) - Brazilian Ministry of Science, Technology and Innovation [51119]; European Commission under the Cooperation Programme, Horizon 2020 [777154]; CNPq; CAPES; Fapemig; Microsoft Azure [CRM:0740801]	project ATMOSPHERE (atmosphere-eubrazil.eu) - Brazilian Ministry of Science, Technology and Innovation; European Commission under the Cooperation Programme, Horizon 2020; CNPq(Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)); CAPES(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); Fapemig(Fundacao de Amparo a Pesquisa do Estado de Minas Gerais (FAPEMIG)); Microsoft Azure(Microsoft)	We thank Fabricio Murai and the anonymous reviewers for providing comments. We also thank Gabriel Coutinho for discussions on the mathematical properties of GRANGER-BUSCA, as well as Alexandre Souza for providing pointers to prior studies. This work has been partially supported by the project ATMOSPHERE (atmosphere-eubrazil.eu), funded by the Brazilian Ministry of Science, Technology and Innovation (Project 51119 -MCTI/RNP 4th Coordinated Call) and by the European Commission under the Cooperation Programme, Horizon 2020 grant agreement no 777154. Funding was also provided by the authors' individual grants from CNPq, CAPES and Fapemig. Computational resources were provided by the Microsoft Azure for Data Science Research Award (CRM:0740801).	Achab M., 2017, ICML; Alves R., 2016, KDD; [Anonymous], 2011, NIPS; Arpaci-Dusseau Remzi H., 2015, OPERATING SYSTEMS 3, V1.0; Bacry E., 2015, MARKET MICROSTRUCTUR, V1, P1; Bao Y., 2017, ML FOR HC; Barabasi A.-L., 2016, NETW SCI; Blundell Charles, 2012, NIPS; Chen S., 2017, ARXIV170704928; Chevallier J., 2015, MATH MODELS METHODS, V25; Choi E., 2015, ICDM; Cox D. R., 1955, J ROYAL STAT SOC B; Daley D., 1982, J APPL PROBABILITY, V19; de Melo P. O. S. Vaz, 2013, WWW; Dhamala M, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.018701; Didelez V., 2008, J ROYAL STAT SOC B, V70; Du N., 2015, KDD; Etesami J., 2016, UAI; Fenwick P. M., 1994, SOFTWARE PRACTICE EX, V24; Granger C. W., 1969, ECONOMETRICA J ECONO; Guttorp P, 2012, INT STAT REV, V80, P253, DOI 10.1111/j.1751-5823.2012.00181.x; Hallac David, 2017, KDD; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Hawkes A. G., 1971, J ROYAL STAT SOC B; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Horn R.A., 2013, MATRIX ANAL, P321; Isham V., 1977, J APPL PROBABILITY, V14; Kumar S., 2016, ICDM; LESKOVEC J, 2010, ICWSM; Leskovec J., 2009, KDD; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Li A. Q., 2014, KDD; Li S., 2017, CIKM; Linderman S. W, 2014, ICML; Linderman SW, 2015, ARXIV150703228; Monti RP, 2014, NEUROIMAGE, V103, P427, DOI 10.1016/j.neuroimage.2014.07.033; Namaki A., 2011, PHYSICA A, V390; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Panzarasa P., 2009, J ASS INFORM SCI TEC, V60; Paranjape A., 2017, WSDM; Rizoiu M.-A., 2018, FRONTIERS MULTIMEDIA; Silva J, 2009, IEEE T PATTERN ANAL, V31, P563, DOI 10.1109/TPAMI.2008.232; Steyvers M., 2007, HDB LATENT SEMANTIC, V427; Terenin A., 2017, NIPS; Vaz de Melo P. O. S., 2015, ACM T KNOWLEDGE DISC, V9; Vere-Jones, 2003, INTRO THEORY POINT P; Wold H., 1948, SCANDINAVIAN ACTUARI, V1948; Xu H., 2016, ICML; Yang Y., 2017, NIPS; Yin H., 2017, KDD; Yu H. - F., 2015, WWW; Yuan J., 2015, WWW; Zhou K., 2013, AISTATS	53	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303001
C	Fischer, V; Kohler, J; Pfeil, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fischer, Volker; Koehler, Jan; Pfeil, Thomas			The streaming rollout of deep networks - towards fully model-parallel execution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURAL-NETWORKS	Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the network's architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally during inference, the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. We prove that certain rollouts, also for networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and enables a fully parallel execution of the network reducing runtime on massively parallel devices. Finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts.	[Fischer, Volker; Koehler, Jan; Pfeil, Thomas] Bosch Ctr Artificial Intelligence, Renningen, Germany		Fischer, V (corresponding author), Bosch Ctr Artificial Intelligence, Renningen, Germany.	volker.fischer@de.bosch.com; jan.koehler@de.bosch.com; thomas.pfeil@de.bosch.com						Abadi M, 2015, P 12 USENIX S OPERAT; Amodei D, 2016, PR MACH LEARN RES, V48; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Breuel TM, 2013, PROC INT CONF DOC, P683, DOI 10.1109/ICDAR.2013.140; Brueckner Raymond, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4823, DOI 10.1109/ICASSP.2014.6854518; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Campos Victor, 2018, INT C LEARN REPR ICL; Carreira J, 2018, LECT NOTES COMPUT SC, V11208, P680, DOI 10.1007/978-3-030-01225-0_40; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung Junyoung, 2017, INT C LEARN REPR ICL; diaeresis>el Mathieu Micha<spacing, 2014, 2 INT C LEARN REPR I, P3; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Duan Y, 2016, PR MACH LEARN RES, V48; Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113; Fernandez S, 2007, LECT NOTES COMPUT SC, V4669, P220; Figurnov M, 2017, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2017.194; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Graves A., 2014, ARXIV14105401; Graves A., 2008, ADV NEURAL INFORM PR, P545, DOI DOI 10.1007/978-1-4471-4072-6; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Greff Klaus, 2017, INT C LEARN REPR ICL; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Han S., 2016, P 4 INT C LEARN REPR, P1; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Zhiheng, 2015, ARXIV150801991; Kim Yong-Deok, 2015, COMPRESSION DEEP CON; Koutnik J, 2014, PR MACH LEARN RES, V32, P1863; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liao Q., 2016, CORR; Lin CM, 2014, NEURAL COMPUT APPL, V24, P487, DOI 10.1007/s00521-012-1242-5; Little WA, 1996, FROM HIGH-TEMPERATURE SUPERCONDUCTIVITY TO MICROMINIATURE REFRIGERATION, P145; Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642; Minsky M., 1969, PERCEPTRONS INTRO CO, DOI 10.7551/mitpress/11301.001.0001; Neil Daniel, 2016, ADV NEURAL INFORM PR; Pascanu R., 2013, ARXIV13126026; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pundak G, 2017, INTERSPEECH, P1303, DOI 10.21437/Interspeech.2017-429; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Shelhamer Evan, 2016, ECCV WORKSH; Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Teerapittayanon S, 2016, INT C PATT RECOG, P2464, DOI 10.1109/ICPR.2016.7900006; Theano Development Team, 2016, ARXIV160502688 THEAN; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X; Williams R.J., 1995, BACKPROPAGATION THEO, V1, P433; Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376; Zamir A. R., 2016, COMPUTER SCI COMPUTE; Zilly J.G., 2016, ARXIV PREPRINT ARXIV	60	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304008
C	Fletcher, AK; Pandit, P; Rangan, S; Sarkar, S; Schniter, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fletcher, Alyson K.; Pandit, Parthe; Rangan, Sundeep; Sarkar, Subrata; Schniter, Philip			Plug-in Estimation in High-Dimensional Linear Inverse Problems: A Rigorous Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MESSAGE-PASSING ALGORITHMS; RECOVERY	Estimating a vector x from noisy linear measurements Ax + w often requires use of prior knowledge or structural constraints on x for accurate reconstruction. Several recent works have considered combining linear least-squares estimation with a generic or "plug-in" denoiser function that can be designed in a modular manner based on the prior knowledge about x. While these methods have shown excellent performance, it has been difficult to obtain rigorous performance guarantees. This work considers plug-in denoising combined with the recently-developed Vector Approximate Message Passing (VAMP) algorithm, which is itself derived via Expectation Propagation techniques. It shown that the mean squared error of this "plug-and-play" VAMP can be exactly predicted for high-dimensional right-rotationally invariant random A and Lipschitz denoisers. The method is demonstrated on applications in image recovery and parametric bilinear estimation.	[Fletcher, Alyson K.] UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Pandit, Parthe] UC Los Angeles, Dept ECE, Los Angeles, CA USA; [Rangan, Sundeep] NYU, Dept ECE, New York, NY 10003 USA; [Sarkar, Subrata; Schniter, Philip] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; New York University; University System of Ohio; Ohio State University	Fletcher, AK (corresponding author), UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.	akfletcher@ucla.edu; parthepandit@ucla.edu; srangan@nyu.edu; sarkar.51@osu.edu; schniter.1@osu.edu	Schniter, Philip/X-3346-2019; Rangan, Sundeep/AAH-2526-2020	Schniter, Philip/0000-0003-0939-7545; 	National Science Foundation [1738285, 1738286, 1116589, 1302336, 1547332, CCF-1527162]; Office of Naval Research [N00014-15-1-2677]; industrial affiliates of NYU WIRELESS	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); industrial affiliates of NYU WIRELESS	A. K. Fletcher and P. Pandit were supported in part by the National Science Foundation under Grants 1738285 and 1738286 and the Office of Naval Research under Grant N00014-15-1-2677. S. Rangan was supported in part by the National Science Foundation under Grants 1116589, 1302336, and 1547332, and the industrial affiliates of NYU WIRELESS. The work of P. Schniter was supported in part by the National Science Foundation under Grant CCF-1527162.	Andersen M. R., 2014, ADV NEURAL INFORM PR, P1745; [Anonymous], 2016, ARXIV161003082; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Berthier R., 2017, ARXIV170803950; Borgerding M, 2017, IEEE T SIGNAL PROCES, V65, P4293, DOI 10.1109/TSP.2017.2708040; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Caltagirone F, 2014, IEEE INT SYMP INFO, P1812, DOI 10.1109/ISIT.2014.6875146; Chen S, 2017, PR ELECTROMAGN RES S, P2035; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100; Demeure C., 1991, STAT SIGNAL PROCESSI, V63; Deshpande Y, 2014, IEEE INT SYMP INFO, P2197, DOI 10.1109/ISIT.2014.6875223; Donoho DL, 2013, IEEE T INFORM THEORY, V59, P3396, DOI 10.1109/TIT.2013.2239356; Fletcher A. K., 2017, P NEUR INF PROC SYST, P2542; Fletcher A. K., 2018, 180610466 ARXIV; Fletcher A, 2016, IEEE INT SYMP INFO, P190, DOI 10.1109/ISIT.2016.7541287; Haykin S., 1994, BLIND DECONVOLUTION; Junyuan X, 2012, ADV NEURAL INF PROCE, P341; Kamilov US, 2014, IEEE T INFORM THEORY, V60, P2969, DOI 10.1109/TIT.2014.2309005; Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733; Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002; Ma JJ, 2017, IEEE ACCESS, V5, P2020, DOI 10.1109/ACCESS.2017.2653119; Ma YT, 2017, IEEE INT SYMP INFO, P231, DOI 10.1109/ISIT.2017.8006524; Matsushita R., 2013, ADV NEURAL INFORM PR, P917; Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683; Opper M, 2005, J MACH LEARN RES, V6, P2177; Parker JT, 2016, IEEE J-STSP, V10, P795, DOI 10.1109/JSTSP.2016.2539123; Rangan S., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1246, DOI 10.1109/ISIT.2012.6283056; RANGAN S, 2011, P IEEE INT S INF THE, P2174; Rangan S, 2017, IEEE INT SYMP INFO, P1588, DOI 10.1109/ISIT.2017.8006797; Rangan S, 2017, IEEE T SIGNAL PROCES, V65, P4577, DOI 10.1109/TSP.2017.2713759; Rangan S, 2014, IEEE INT SYMP INFO, P236, DOI 10.1109/ISIT.2014.6874830; Rangan S, 2013, IEEE INT SYMP INFO, P664, DOI 10.1109/ISIT.2013.6620309; Schniter P., 2017, P BASP WORKSHOP, P77; Sun P, 2018, IEEE T SIGNAL PROCES, V66, P2772, DOI 10.1109/TSP.2018.2812720; Taeb A., 2013, ARXIV13032389; Takeuchi K, 2017, IEEE INT SYMP INFO, P501, DOI 10.1109/ISIT.2017.8006578; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Vila J, 2015, INT CONF ACOUST SPEE, P2021, DOI 10.1109/ICASSP.2015.7178325; Wang XR, 2017, INT CONF ACOUST SPEE, P1323, DOI 10.1109/ICASSP.2017.7952371; Xu L., 2014, INT C NEUR INF PROC, V27, P1790; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhu H, 2011, IEEE T SIGNAL PROCES, V59, P2002, DOI 10.1109/TSP.2011.2109956	48	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002003
C	Fromm, J; Patel, S; Philipose, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fromm, Josh; Patel, Shwetak; Philipose, Matthai			Heterogeneous Bitwidth Binarization in Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent work has shown that fast, compact low-bitwidth neural networks can be surprisingly accurate. These networks use homogeneous binarization: all parameters in each layer or (more commonly) the whole model have the same low bitwidth (e.g., 2 bits). However, modern hardware allows efficient designs where each arithmetic instruction can have a custom bitwidth, motivating heterogeneous binarization, where every parameter in the network may have a different bitwidth. In this paper, we show that it is feasible and useful to select bitwidths at the parameter granularity during training For instance a heterogeneously quantized version of modern networks such as AlexNet and MobileNet, with the right mix of 1-, 2- and 3-bit parameters that average to just 1.4 bits can equal the accuracy of homogeneous 2-bit versions of these networks. Further, we provide analyses to show that the heterogeneously binarized systems yield FPGA- and ASIC-based implementations that are correspondingly more efficient in both circuit area and energy efficiency than their homogeneous counterparts.	[Fromm, Josh] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA; [Patel, Shwetak] Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA; [Philipose, Matthai] Microsoft Res, Redmond, WA 98052 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle; Microsoft	Fromm, J (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.	jwfromm@uw.edu; shwetak@cs.washington.edu; matthaip@microsoft.com						Alemdar H, 2017, IEEE IJCNN, P2547, DOI 10.1109/IJCNN.2017.7966166; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Courbariaux M., 2015, ADV NEUR IN, P3123; Dong Y., 2017, BMVC; Ehliar A, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY (FPT), P131, DOI 10.1109/FPT.2014.7082765; HAN S., 2015, ARXIV151000149; Howard A.G., 2017, ARXIV170404861; Hubara I., 2016, ARXIV160907061; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Fengfu, 2016, ARXIV; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tang W, 2017, AAAI CONF ARTIF INTE, P2625; Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744; Zhou Shuchang, 2016, P IEEE C COMP VIS PA	17	1	1	3	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304005
C	Fujii, K; Soma, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fujii, Kaito; Soma, Tasuku			Fast greedy algorithms for dictionary selection with generalized sparsity constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OVERCOMPLETE DICTIONARIES	In dictionary selection, several atoms are selected from finite candidates that successfully approximate given data points in the sparse representation. We propose a novel efficient greedy algorithm for dictionary selection. Not only does our algorithm work much faster than the known methods, but it can also handle more complex sparsity constraints, such as average sparsity. Using numerical experiments, we show that our algorithm outperforms the known methods for dictionary selection, achieving competitive performances with dictionary learning algorithms in a smaller running time.	[Fujii, Kaito; Soma, Tasuku] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan	University of Tokyo	Fujii, K (corresponding author), Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan.	kaito_fujii@mist.i.u-tokyo.ac.jp; tasuku_soma@mist.i.u-tokyo.ac.jp	Soma, Tasuku/AAI-5374-2020	Soma, Tasuku/0000-0001-9519-2487	JSPS KAKENHI [JP 18J12405]; ACT-I, JST; JST CREST, Japan [JPMJCR14D2]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); ACT-I, JST; JST CREST, Japan(Core Research for Evolutional Science and Technology (CREST))	The authors would thank Taihei Oki and Nobutaka Shimizu for their stimulating discussions. K.F. was supported by JSPS KAKENHI Grant Number JP 18J12405. T.S. was supported by ACT-I, JST. This work was supported by JST CREST, Grant Number JPMJCR14D2, Japan.	Agarwal A, 2016, SIAM J OPTIMIZ, V26, P2775, DOI 10.1137/140979861; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Arora S., 2014, C LEARNING THEORY, P779; Balkanski E., 2016, P 33 INT C MACH LEAR, P2207; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Cevher V, 2011, IEEE J-STSP, V5, P979, DOI 10.1109/JSTSP.2011.2161862; Chen L, 2018, PROC INTERNAT C ARTI, P1896; Cong Y, 2017, IEEE T IMAGE PROCESS, V26, P185, DOI 10.1109/TIP.2016.2619260; Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951; Das A., 2011, P 28 INT C MACH LEAR, P1057; Dumitrescu B., 2018, DICT LEARNING ALGORI; Elenberg E. R., 2016, P NIPS WORKSH LEARN; Elenberg ER, 2017, ADV NEUR IN, V30; Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624; Everingham M., PASCAL VISUAL OBJECT; Foucart S., 2013, MATH INTRO COMPRESSI, P1, DOI DOI 10.1007/978-0-8176-4948-7; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Huang J., 2009, J MACHINE LEARNING R, V12, P3371; Kale S., 2017, P 35 INT C MACH LEAR, P1; Khanna R., 2017, ICML, P1837; Krause Andreas, 2010, ICML, P567; Liberty E., 2017, APPROXIMATION RANDOM, V81; Mairal J, 2010, J MACH LEARN RES, V11, P19; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477; Rusu C, 2014, IEEE SIGNAL PROC LET, V21, P6, DOI 10.1109/LSP.2013.2288788; Stan S, 2017, INT C MACH LEARN, P3241; Streeter M., 2009, P ADV NEUR INF PROC, P1577; Zhou M, 2009, ADV NEURAL INFORM PR, P2295	32	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304073
C	Geffner, T; Domke, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Geffner, Tomas; Domke, Justin			Using Large Ensembles of Control Variates for Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient's variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.	[Geffner, Tomas; Domke, Justin] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Geffner, T (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	tgeffner@cs.umass.edu; domke@cs.umass.edu						Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Challis Edward, 2011, P 14 INT C ART INT S, P199; Furmston Thomas, 2010, P AISTATS, P241; Garnett, 2016, ADV NEURAL INFORM PR, V29, P1804; Grathwohl W., 2017, ARXIV171100123; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jang Eric, 2017, P 5 INT C LEARN REPR; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Jordan Michael I., EXPONENTIAL FAMILY C; Kingma DP, 2 INT C LEARN REPR I, P1; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Lawson J., 2017, ADV NEURAL INFORM PR, P2627; Liang Percy, 2007, P 2007 JOINT C EMP M; Maddison Chris J, 2016, ARXIV161100712; Miller Andrew C, 2017, ARXIV170507880; Mnih Andriy, 2014, INT C MACH LEARN; Paisley J., 2012, ARXIV12066430; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rezende D.J., 2014, PROC INT CONFER ENCE; Roeder Geoffrey, 2017, ARXIV170309194; Ruiz Francisco JR, 2016, ARXIV160301140; Ruiz Francisco R, 2016, ADV NEURAL INFORM PR, P460; Titsias MK, 2015, ADV NEUR IN, V28; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; van Amersfoort J. R., 2014, ARXIV14126581; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang C, 2013, INT CONF SMART GRID, P181, DOI 10.1109/SmartGridComm.2013.6687954; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Winn J, 2005, J MACH LEARN RES, V6, P661; Zhang C., 2017, ARXIV171105597	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004051
C	Gillenwater, J; Kulesza, A; Mariet, Z; Vassilvitskii, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gillenwater, Jennifer; Kulesza, Alex; Mariet, Zelda; Vassilvitskii, Sergei			Maximizing Induced Cardinality Under a Determinantal Point Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Determinantal point processes (DPPs) are well-suited to recommender systems where the goal is to generate collections of diverse, high-quality items. In the existing literature this is usually formulated as finding the mode of the DPP (the so-called MAP set). However, the MAP objective inherently assumes that the DPP models "optimal" recommendation sets, and yet obtaining such a DPP is nontrivial when there is no ready source of example optimal sets. In this paper we advocate an alternative framework for applying DPPs to recommender systems. Our approach assumes that the DPP simply models user engagements with recommended items, which is more consistent with how DPPs for recommender systems are typically trained. With this assumption, we are able to formulate a metric that measures the expected number of items that a user will engage with. We formalize this optimization of this metric as the Maximum Induced Cardinality (MIC) problem. Although the MIC objective is not submodular, we show that it can be approximated by a submodular function, and that empirically it is well-optimized by a greedy algorithm.	[Gillenwater, Jennifer; Kulesza, Alex; Vassilvitskii, Sergei] Google Res NYC, New York, NY 10011 USA; [Mariet, Zelda] MIT, Cambridge, MA 02139 USA	Google Incorporated; Massachusetts Institute of Technology (MIT)	Gillenwater, J (corresponding author), Google Res NYC, New York, NY 10011 USA.	jengi@google.com; kulesza@google.com; zelda@csail.mit.edu; sergeiv@google.com						Chen Laming, 2017, LARG SCAL REC SYST W; Dupuy C., 2018, C ART INT STAT AISTA; Feige U., 2009, SIAM J COMPUTING SIC, V39; Friedland S., 2013, LINEAR ALGEBRA ITS A, V438; Gartrell M., 2017, AAAI C ART INT; Gillenwater J., 2012, NEURAL INFORM PROCES; Gillenwater J. A., 2014, P NEURIPS, P3149; HAGER WW, 1989, SIAM REV, V31, P221, DOI 10.1137/1031049; Hurley N, 2011, ACM T INTERNET TECHN, V10, DOI 10.1145/1944339.1944341; Kathuria T., 2017, SAMPLING GREEDY MAP; Kulesza A., 2012, FDN TRENDS MACHINE L, V5; Kulesza Alex, 2011, C UNC ART INT UAI; Mariet Zelda, 2015, INT C MACH LEARN ICM; Nemhauser G. L., 1978, MATH PROGRAMMING, V14; Nikolov A., 2016, S THEOR COMP STOC; Smyth Barry, 2001, INT C CAS BAS REAS; Suhubi E. S., 2003, FUNCTIONAL ANAL; Swaminathan A., 2017, NEURAL INFORM PROCES; Wilhelm M., 2018, C INF KNOWL MAN CIKM; Zhang M., 2016, STAT SIGN PROC WORKS; Ziegler C., 2005, INT C WORLD WID WEB; [No title captured]	22	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001045
C	Ha, JS; Park, YJ; Chae, HJ; Park, SS; Choi, HL		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ha, Jung-Su; Park, Young-Jin; Chae, Hyeok-Joo; Park, Soon-Seo; Choi, Han-Lim			Adaptive Path-Integral Autoencoder: Representation Learning and Planning for Dynamical Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a representation learning algorithm that learns a low-dimensional latent dynamical system from high-dimensional sequential raw data, e.g., video. The framework builds upon recent advances in amortized inference methods that use both an inference network and a refinement procedure to output samples from a variational distribution given an observation sequence, and takes advantage of the duality between control and inference to approximately solve the intractable inference problem using the path integral control approach. The learned dynamical model can be used to predict and plan the future states; we also present the efficient planning method that exploits the learned low-dimensional latent dynamics. Numerical experiments show that the proposed path-integral control based variational inference method leads to tighter lower bounds in statistical model learning of sequential data. The supplementary video(1) and the implementation code(2) are available online.	[Ha, Jung-Su; Park, Young-Jin; Chae, Hyeok-Joo; Park, Soon-Seo; Choi, Han-Lim] Korea Adv Inst Sci & Technol, Dept Aerosp Engn & KI Robot, Daejeon 305701, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Ha, JS (corresponding author), Korea Adv Inst Sci & Technol, Dept Aerosp Engn & KI Robot, Daejeon 305701, South Korea.	jsha@lics.kaist.ac.kr; yjpark@lics.kaist.ac.kr; hjchae@lics.kaist.ac.kr; sspark@lics.kaist.ac.kr; hanlimc@kaist.ac.kr		Bhirud, Sachin/0000-0003-2897-4731	Agency for Defense Development [UD150047JD]	Agency for Defense Development(Agency of Defense Development (ADD), Republic of Korea)	This work was supported by the Agency for Defense Development under contract UD150047JD.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Banijamali E, 2018, PR MACH LEARN RES, V84; Bellman R., 2013, DYNAMIC PROGRAMMING; Burda Yuri, 2016, 4 INT C LEARN REPR I; Chen NT, 2016, IEEE-RAS INT C HUMAN, P629, DOI 10.1109/HUMANOIDS.2016.7803340; Cremer C, 2018, PR MACH LEARN RES, V80; Cremer Chris, 2017, ARXIV170402916; Gardiner C, 1985, HDB STOCHASTIC METHO, V4; Genewein T, 2015, FRONT ROBOT AI, DOI 10.3389/frobt.2015.00027; Ha JS, 2018, IEEE ROBOT AUTOM LET, V3, P3892, DOI 10.1109/LRA.2018.2856915; Hjelm D., 2016, ADV NEURAL INFORM PR, P4691; Ichter B, 2018, IEEE INT CONF ROBOT, P7087; Jonschkowski R, 2015, AUTON ROBOT, V39, P407, DOI 10.1007/s10514-015-9459-7; Kappen HJ, 2016, J STAT PHYS, V162, P1244, DOI 10.1007/s10955-016-1446-7; Karkus P, 2017, ADV NEUR IN, V30; Karl M., 2017, PROC INT C LEARN REP, P1; Kim Y, 2018, PR MACH LEARN RES, V80; Kingma D. P, 2014, ARXIV13126114; Krishnan RG, 2018, PR MACH LEARN RES, V84; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Le Tuan Anh, 2018, ICLR; Lesort T, 2018, NEURAL NETWORKS, V108, P379, DOI 10.1016/j.neunet.2018.07.006; Mnih A, 2016, PR MACH LEARN RES, V48; Naesseth CA, 2018, PR MACH LEARN RES, V84; Okada M., 2017, ARXIV170609597; Rainforth Tom, 2018, ARXIV180204537; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Ruiz HC, 2017, IEEE T SIGNAL PROCES, V65, P3191, DOI 10.1109/TSP.2017.2686340; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Tassa Y., 2018, ARXIV180100690; Thijssen S, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.032104; Todorov E, 2008, IEEE DECIS CONTR P, P4286, DOI 10.1109/CDC.2008.4739438; Todorov E, 2009, P NATL ACAD SCI USA, V106, P11478, DOI 10.1073/pnas.0710743106; Vernaza P, 2012, INT J ROBOT RES, V31, P1739, DOI 10.1177/0278364912457436; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Zhang C, 2018, IEEE INT C INT ROBOT, P3654, DOI 10.1109/IROS.2018.8594028; Ziebart B. D., 2008, AAAI, V8, P1433	41	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003048
C	Haris, A; Simon, N; Shojaie, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Haris, Asad; Simon, Noah; Shojaie, Ali			Wavelet regression and additive models for irregularly spaced data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ORTHONORMAL BASES; SHRINKAGE	We present a novel approach for nonparametric regression using wavelet basis functions. Our proposal, waveMesh, can be applied to non-equispaced data with sample size not necessarily a power of 2. We develop an efficient proximal gradient descent algorithm for computing the estimator and establish adaptive minimax convergence rates. The main appeal of our approach is that it naturally extends to additive and sparse additive models for a potentially large number of covariates. We prove minimax optimal convergence rates under a weak compatibility condition for sparse additive models. The compatibility condition holds when we have a small number of covariates. Additionally, we establish convergence rates for when the condition is not met. We complement our theoretical results with empirical studies comparing waveMesh to existing methods.	[Haris, Asad; Simon, Noah; Shojaie, Ali] Univ Washington, Dept Biostat, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Haris, A (corresponding author), Univ Washington, Box 357232, Seattle, WA 98195 USA.	aharis@uw.edu; nrsimon@uw.edu; ashojaie@uw.edu			National Institutes of Health; National Science Foundation	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation(National Science Foundation (NSF))	We thank three anonymous referees for insightful comments that substantially improved the manuscript. We thank Professor Sylvain Sardy for providing software. This work was partially supported by National Institutes of Health grants to A.S. and N.S., and National Science Foundation grants to A.S.	Amato U, 2001, STAT COMPUT, V11, P373, DOI 10.1023/A:1011929305660; [Anonymous], 2012, ESSENTIAL WAVELETS S; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Cai TT, 1998, ANN STAT, V26, P1783; Cai TT, 1999, STAT PROBABIL LETT, V42, P313, DOI 10.1016/S0167-7152(98)00223-5; Cencov N. N., 1962, SOVIET MATH, V3, P1559; Chui Charles K, 1992, INTRO WAVELETS, P38; DAUBECHIES I, 1993, SIAM J MATH ANAL, V24, P499, DOI 10.1137/0524031; DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199; DAUBECHIES I, 1988, COMMUN PUR APPL MATH, V41, P909, DOI 10.1002/cpa.3160410705; Daubechies Ingrid, 1992, 10 LECT WAVELETS, V61; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626; Hall P, 1997, ANN STAT, V25, P1912; Kovac A, 2000, J AM STAT ASSOC, V95, P172, DOI 10.2307/2669536; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; Meyer Yves, 1985, SEMIN BOURBAKI, V662, P1985; Nason G., 2010, WAVELET METHODS STAT; Nesterov Y., 2007, TECHNICAL REPORT; Nunes MA, 2006, STAT COMPUT, V16, P143, DOI 10.1007/s11222-006-6560-y; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Pensky M, 2001, ANN I STAT MATH, V53, P681, DOI 10.1023/A:1014640632666; Percival D. B., 2006, WAVELET METHODS TIME, V4; Petersen A, 2016, J COMPUT GRAPH STAT, V25, P1005, DOI 10.1080/10618600.2015.1073155; Ravikumar P, 2009, J R STAT SOC B, V71, P1009, DOI 10.1111/j.1467-9868.2009.00718.x; Sardy S, 2004, J COMPUT GRAPH STAT, V13, P283, DOI 10.1198/1061860043434; Sardy S, 1999, STAT COMPUT, V9, P65, DOI 10.1023/A:1008818328241; Schnaidt Grez German A, 2018, ARXIV E PRINTS; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; Strang G., 1996, WAVELETS FILTER BANK; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vidakovic B., 2009, STAT MODELING WAVELE, V503; Wahba G., 1990, SPLINE MODELS OBSERV; Zhang SL, 2003, ANN STAT, V31, P152	38	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003052
C	Hu, HX; Chen, LY; Gong, BQ; Sha, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hu, Hexiang; Chen, Liyu; Gong, Boqing; Sha, Fei			Synthesized Policies for Transfer and Adaptation across Tasks and Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments (epsilon) and tasks (tau), probably more importantly, by learning from only sparse (epsilon, tau) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from environment and task embeddings. Notably, one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GRIDWORLD and THOR, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (epsilon, tau) pairs after learning from only 40% of them.	[Hu, Hexiang; Chen, Liyu] Univ Southern Calif, Los Angeles, CA 90089 USA; [Gong, Boqing] Tencent AI Lab, Bellevue, WA 98004 USA; [Sha, Fei] Netflix, Los Angeles, CA 90028 USA	University of Southern California; Netflix, Inc.	Hu, HX (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	hexiangh@usc.edu; liyuc@usc.edu; boqinggo@outlook.com; fsha@netflix.com	Hu, Hexiang/GNW-4536-2022		DARPA [FA8750-18-2-0117]; NSF [IIS-1065243, 1451412, 1513966/1632803/1833137, 1208500, CCF-1139148]; Google Research Award; Alfred P. Sloan Research Fellowship; ARO [W911NF-12-1-0241, W911NF-15-1-0484]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated); Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); ARO	We appreciate the feedback from the reviewers. This work is partially supported by DARPA#FA8750-18-2-0117, NSF IIS-1065243, 1451412, 1513966/1632803/1833137, 1208500, CCF-1139148, a Google Research Award, an Alfred P. Sloan Research Fellowship, gifts from Facebook and Netflix, and ARO#W911NF-12-1-0241 and W911NF-15-1-0484.	Andreas J, 2017, PR MACH LEARN RES, V70; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Devin Coline, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2169, DOI 10.1109/ICRA.2017.7989250; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow Ian, 2017, 5 INT C LEARN REPR I; Hinton G., 2015, ARXIV150302531; Ho  J., 2016, ADV NEURAL INFORM PR, P4565; Kolve Eric, 2017, ARXIV171205474; Kulkarni T.D., 2016, ABS160602396 CORR; Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Oh J, 2017, PR MACH LEARN RES, V70; Parisotto Emilio, 2015, ARXIV151106342; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Silver David, 2017, CORR; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Teh YW, 2017, ADV NEUR IN, V30; Vinyals Oriol, 2017, ARXIV170804782; Wilson A., 2007, PROC INT C MACH LEAR, P1015, DOI DOI 10.1145/1273496; Zhang C., 2018, ARXIV180406893; Zhu Y., 2017, P IEEE INT C COMP VI, V2, P7	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301018
C	Imani, M; Ghoreishi, SF; Braga-Neto, UM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Imani, Mahdi; Ghoreishi, Seyede Fatemeh; Braga-Neto, Ulisses M.			Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a Bayesian decision making framework for control of Markov Decision Processes (MDPs) with unknown dynamics and large, possibly continuous, state, action, and parameter spaces in data-poor environments. Most of the existing adaptive controllers for MDPs with unknown dynamics are based on the reinforcement learning framework and rely on large data sets acquired by sustained direct interaction with the system or via a simulator. This is not feasible in many applications, due to ethical, economic, and physical constraints. The proposed framework addresses the data poverty issue by decomposing the problem into an offline planning stage that does not rely on sustained direct interaction with the system or simulator and an online execution stage. In the offline process, parallel Gaussian process temporal difference (GPTD) learning techniques are employed for near-optimal Bayesian approximation of the expected discounted reward over a sample drawn from the prior distribution of unknown parameters. In the online stage, the action with the maximum expected return with respect to the posterior distribution of the parameters is selected. This is achieved by an approximation of the posterior distribution using a Markov Chain Monte Carlo (MCMC) algorithm, followed by constructing multiple Gaussian processes over the parameter space for efficient prediction of the means of the expected return at the MCMC sample. The effectiveness of the proposed framework is demonstrated using a simple dynamical system model with continuous state and action spaces, as well as a more complex model for a metastatic melanoma gene regulatory network observed through noisy synthetic gene expression data.	[Imani, Mahdi; Ghoreishi, Seyede Fatemeh; Braga-Neto, Ulisses M.] Texas A&M Univ, College Stn, TX 77843 USA	Texas A&M University System; Texas A&M University College Station	Imani, M (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.	m.imani88@tamu.edu; f.ghoreishi88@tamu.edu; ulisses@ece.tamu.edu	Braga-Neto, Ulisses/ABI-2677-2020		National Science Foundation [CCF-1718924]	National Science Foundation(National Science Foundation (NSF))	The authors acknowledge the support of the National Science Foundation, through NSF award CCF-1718924.	Antos A., 2008, ADV NEURAL INFORM PR, P9; Asmuth J., 2012, ARXIV12023699; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1; Bittner M, 2000, NATURE, V406, P536, DOI 10.1038/35020115; Braga-Neto U, 2011, CONF REC ASILOMAR C, P1050, DOI 10.1109/ACSSC.2011.6190172; Busoniu L, 2010, AUTOM CONTROL ENG SE, P1, DOI 10.1201/9781439821091-f; Drougard N, 2014, AAAI CONF ARTIF INTE, P2257; Engel Y., 2005, P 22 INT C MACH LEAR, P201, DOI DOI 10.1145/1102351.1102377; Fonteneau R, 2013, IEEE SYMP ADAPT DYNA, P77, DOI 10.1109/ADPRL.2013.6614992; Gasic M, 2014, IEEE-ACM T AUDIO SPE, V22, P28, DOI 10.1109/TASL.2013.2282190; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Gilks W. R., 1995, MARKOV CHAIN MONTE C, DOI 10.1201/b14835; Guez A., 2012, ADV NEURAL INFORM PR; Guez A, 2013, J ARTIF INTELL RES, V48, P841, DOI 10.1613/jair.4117; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Imani M., 2018, IEEE T CONTROL SYSTE; Imani M, 2017, IEEE T SIGNAL PROCES, V65, P359, DOI 10.1109/TSP.2016.2614798; Imani M, 2018, AUTOMATICA, V95, P172, DOI 10.1016/j.automatica.2018.05.028; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Vien NA, 2013, APPL INTELL, V39, P345, DOI 10.1007/s10489-012-0416-2; Poupart P., 2006, ICML, P697; Powell W.B, 2012, OPTIMAL LEARNING, V841; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ross Stephane, 2008, Uncertain Artif Intell, V2008, P476; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Trevizan FW, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2023; Wang T., 2005, P 22 INT C MACH LEAR, P956, DOI DOI 10.1145/1102351.1102472; Wang Y., 2012, INT C MACH LEARN; Weeraratna AT, 2002, CANCER CELL, V1, P279, DOI 10.1016/S1535-6108(02)00045-4	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002067
C	Ito, S; Hatano, D; Sumita, H; Yabe, A; Fukunaga, T; Kakimura, N; Kawarabayashi, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ito, Shinji; Hatano, Daisuke; Sumita, Hanna; Yabe, Akihiro; Fukunaga, Takuro; Kakimura, Naonori; Kawarabayashi, Ken-ichi			Regret Bounds for Online Portfolio Selection with a Cardinality Constraint	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Online portfolio selection is a sequential decision-making problem in which a learner repetitively selects a portfolio over a set of assets, aiming to maximize long-term return. In this paper, we study the problem with the cardinality constraint that the number of assets in a portfolio is restricted to be at most k, and consider two scenarios: (i) in the full-feedback setting, the learner can observe price relatives (rates of return to cost) for all assets, and (ii) in the bandit-feedback setting, the learner can observe price relatives only for invested assets. We propose efficient algorithms for these scenarios, which achieve sublinear regrets. We also provide regret (statistical) lower bounds for both scenarios which nearly match the upper bounds when k is a constant. In addition, we give a computational lower bound, which implies that no algorithm maintains both computational efficiency, as well as a small regret upper bound.	[Ito, Shinji; Yabe, Akihiro] NEC Corp Ltd, Minato, Japan; [Hatano, Daisuke; Fukunaga, Takuro] RIKEN AIP, Tokyo, Japan; [Sumita, Hanna] Tokyo Metropolitan Univ, Tokyo, Japan; [Fukunaga, Takuro] JST PRESTO, Tokyo, Japan; [Kakimura, Naonori] Keio Univ, Tokyo, Japan; [Kawarabayashi, Ken-ichi] Natl Inst Informat, Tokyo, Japan	NEC Corporation; RIKEN; Tokyo Metropolitan University; Japan Science & Technology Agency (JST); Keio University; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Ito, S (corresponding author), NEC Corp Ltd, Minato, Japan.				JST ERATO, Japan [JPMJER1201]; JSPS KAKENHI [JP18H05291]	JST ERATO, Japan(Japan Science & Technology Agency (JST)); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by JST ERATO Grant Number JPMJER1201, Japan, and JSPS KAKENHI Grant Number JP18H05291.	Agarwal A., 2006, P 23 INT C MACHINE L, P9, DOI 10.1145/1143844.1143846; Alon N., 2004, PROBABILISTIC METHOD; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen W., 2013, ICML 2013, P151; Combes R., 2015, P 28 INT C NEUR INF, P2116; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; Das Puja, 2014, 28 AAAI C ART INT; FENWICK PM, 1994, SOFTWARE PRACT EXPER, V24, P327, DOI 10.1002/spe.4380240306; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gentle JE, 2009, STAT COMPUT SER, P3; Hakansson N.H., 1995, HBK OPERAT RES MANAG, V9, P65; Kalai A., 2002, J MACHINE LEARNING R, V3, P423; Karp RM., 1972, COMPLEXITY COMPUTER, P85; KELLY JL, 1956, BELL SYST TECH J, V35, P917, DOI 10.1002/j.1538-7305.1956.tb03809.x; Li B., 2012, ARXIV PREPRINT ARXIV, P273; Li B, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2512962; Lobo MS, 1998, LINEAR ALGEBRA APPL, V284, P193, DOI 10.1016/S0024-3795(98)10032-0; Matousek J., 2001, LECT NOTES; Ordentlich E, 1998, MATH OPER RES, V23, P960, DOI 10.1287/moor.23.4.960; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Ye Y., 2018, P 21 INT C ART INT S, P2008; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005019
C	Jeon, W; Seo, S; Kim, KE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jeon, Wonseok; Seo, Seokin; Kim, Kee-Eung			A Bayesian Approach to Generative Adversarial Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generative adversarial training for imitation learning has shown promising results on high-dimensional and continuous control tasks. This paradigm is based on reducing the imitation learning problem to the density matching problem, where the agent iteratively refines the policy to match the empirical state-action visitation frequency of the expert demonstration. Although this approach can robustly learn to imitate even with scarce demonstration, one must still address the inherent challenge that collecting trajectory samples in each iteration is a costly operation. To address this issue, we first propose a Bayesian formulation of generative adversarial imitation learning (GAIL), where the imitation policy and the cost function are represented as stochastic neural networks. Then, we show that we can significantly enhance the sample efficiency of GAIL leveraging the predictive density of the cost, on an extensive set of imitation learning tasks with high-dimensional states and actions.	[Jeon, Wonseok; Seo, Seokin; Kim, Kee-Eung] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea; [Kim, Kee-Eung] PROWLER Io, Cambridge, England	Korea Advanced Institute of Science & Technology (KAIST)	Jeon, W (corresponding author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.	wsjeon@ai.kaist.ac.kr; siseo@ai.kaist.ac.kr; kekim@cs.kaist.ac.kr			ICT R&D program of MSIT/IITP [2017-0-01778]; Ministry of Trade, Industry & Energy (MOTIE, Korea) [10063424]	ICT R&D program of MSIT/IITP; Ministry of Trade, Industry & Energy (MOTIE, Korea)	This work was supported by the ICT R&D program of MSIT/IITP. (No. 2017-0-01778, Development of Explainable Human-level Deep Machine Learning Inference Framework) and the Ministry of Trade, Industry & Energy (MOTIE, Korea) under Industrial Technology Innovation Program (No. 10063424, Development of Distant Speech Recognition and Multi-task Dialog Processing Technologies for In-door Conversational Robots).	Abadi M., TENSORFLOW LARGE SCA; Abdolmaleki A., 2018, P 6 INT C LEARN REPR; Ba J., 2017, P 3 INT C LEARN REPR; Bagnell J Andrew, 2015, TECHNICAL REPORT; Choi J, 2011, ADV NEURAL INFORM PR, P1989, DOI DOI 10.1109/IGARSS.2011.6049518; Finn C., 2016, ARXIV161103852; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hesse C., 2017, OPENAI BASELINES; Ho J, 2016, PR MACH LEARN RES, V48; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kim Kee- Eung, 2018, P 33 AAAI C ART INT; Levine S, 2018, P 6 INT C LEARN REPR; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Li Yunzhu, 2017, ADV NEURAL INFORM PR, V30, P3815; Liu Q., 2016, NEURIPS; Liu Q, 2016, PR MACH LEARN RES, V48; Liu Y., 2017, ARXIV170402399; Neu G., 2007, P 23 C UNC ART INT, P295; Neumann G, 2011, P 28 INT C MACH LEAR, P817; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964; Saatchi Y., 2017, ADV NEURAL INFO P SY, V30, P3622; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Toussaint M., 2009, INT C MACH LEARN ICM, P1049; Wang Z., 2017, P NEUR INF PROC SYST; Ziebart B. D., 2008, AAAI, V8, P1433	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002002
C	Ji, KY; Liang, YB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ji, Kaiyi; Liang, Yingbin			Minimax Estimation of Neural Net Distance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					An important class of distance metrics proposed for training generative adversarial networks (GANs) is the integral probability metric (IPM), in which the neural net distance captures the practical GAN training via two neural networks. This paper investigates the minimax estimation problem of the neural net distance based on samples drawn from the distributions. We develop the first known minimax lower bound on the estimation error of the neural net distance, and an upper bound tighter than an existing bound on the estimator error for the empirical neural net distance. Our lower and upper bounds match not only in the order of the sample size but also in terms of the norm of the parameter matrices of neural networks, which justifies the empirical neural net distance as a good approximation of the true neural net distance for training GANs in practice.	[Ji, Kaiyi; Liang, Yingbin] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Ji, KY (corresponding author), Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.	ji.367@osu.edu; liang.889@osu.edu		Liang, Yingbin/0000-0002-8635-2992	U.S. National Science Foundation [CCF-1801855]	U.S. National Science Foundation(National Science Foundation (NSF))	The work was supported in part by U.S. National Science Foundation under the grant CCF-1801855.	[Anonymous], P INT C LEARN REPR I; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Du SS, 2018, PR MACH LEARN RES, V80; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Golowich N., 2018, P C LEARN THEOR COLT; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1; Kontorovich A, 2014, PR MACH LEARN RES, V32, P28; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liang Tengyuan, 2017, ARXIV171208244; Liu S, 2017, ADV NEUR IN, V30; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; MONTGOMERYSMITH SJ, 1990, P AM MATH SOC, V109, P517, DOI 10.2307/2048015; Muandet K., 2012, PROC 25 INT C NEURAL, P10; Oymak S, 2018, PR MACH LEARN RES, V80; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhang P., 2018, P INT C LEARN REPR I; Zou S., 2015, IEEE T SIGNAL PROCES, V65, P5034	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303081
C	Kadmon, J; Ganguli, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kadmon, Jonathan; Ganguli, Surya			Statistical mechanics of low-rank tensor decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE; MODEL	Often, large, high dimensional datasets collected across multiple modalities can be organized as a higher order tensor. Low-rank tensor decomposition then arises as a powerful and widely used tool to discover simple low dimensional structures underlying such data. However, we currently lack a theoretical understanding of the algorithmic behavior of low-rank tensor decompositions. We derive Bayesian approximate message passing (AMP) algorithms for recovering arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic mean field theory to precisely characterize their performance. Our theory reveals the existence of phase transitions between easy, hard and impossible inference regimes, and displays an excellent match with simulations. Moreover it reveals several qualitative surprises compared to the behavior of symmetric, cubic tensor decomposition. Finally, we compare our AMP algorithm to the most commonly used algorithm, alternating least squares (ALS), and demonstrate that AMP significantly outperforms ALS in the presence of noise.	[Kadmon, Jonathan; Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA; [Ganguli, Surya] Google Brain, Mountain View, CA USA	Stanford University; Google Incorporated	Kadmon, J (corresponding author), Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.	kadmonj@stanford.edu; sganguli@stanford.edu		Ganguli, Surya/0000-0002-9264-7551	Center for Theory of Deep Learning at the Hebrew University; Simons Foundation; Office of Naval Research; National Institutes of Health; Burroughs-Wellcome Foundation; McKnight Foundation; James S. McDonnell Foundation	Center for Theory of Deep Learning at the Hebrew University; Simons Foundation; Office of Naval Research(Office of Naval Research); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Burroughs-Wellcome Foundation(Burroughs Wellcome Fund); McKnight Foundation; James S. McDonnell Foundation	We thank Alex Williams for useful discussions. We thank the Center for Theory of Deep Learning at the Hebrew University (J.K), and the Burroughs-Wellcome, McKnight, James S. McDonnell, and Simons Foundations, and the Office of Naval Research and the National Institutes of Health (S.G) for support.	Acar E, 2007, BIOINFORMATICS, V23, pI10, DOI 10.1093/bioinformatics/btm210; Advani M., 2016, ADV NEURAL INFORM PR, P3378; Advani M, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03014; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], 1935, P R SOC LONDON A, DOI DOI 10.1098/RSPA.1935.0122; Barbier J, 2017, ANN ALLERTON CONF, P1056; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; CRISANTI A, 1992, Z PHYS B CON MAT, V87, P341, DOI 10.1007/BF01309287; CRISANTI A, 1995, J PHYS I, V5, P805, DOI 10.1051/jp1:1995164; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ganguli S., 2010, ADV NEURAL INFORM PR, V23, P667; Ganguli S, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.188701; Harshman R.A., 1970, WORKING PAPERS PHONE; Hunyadi B., 2017, WILEY INTERDISCIPLIN, V7; Kabashima Y, 2004, LECT NOTES ARTIF INT, V3244, P479; Kabashima Y, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/09/L09003; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Lesieur T, 2015, ANN ALLERTON CONF, P680, DOI 10.1109/ALLERTON.2015.7447070; Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733; MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2181, DOI 10.1088/0305-4470/22/12/018; MEZARD M, 1985, J PHYS LETT-PARIS, V46, pL771, DOI 10.1051/jphyslet:019850046017077100; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Nishimori H, 2001, STAT PHYS SPIN GLASS, V111; Rabinowitz NC, 2015, ELIFE, V4, DOI 10.7554/eLife.08998; Rangan S., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1246, DOI 10.1109/ISIT.2012.6283056; Rangan S., 2009, P ADV NEUR INF PROC, V22, P1545; Richard E., 2014, P ADV NEUR INF PROC, P2897; Seely JS, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005164; Sidiropoulos ND, 2017, IEEE T SIGNAL PROCES, V65, P3551, DOI 10.1109/TSP.2017.2690524; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Williams AH, 2018, NEURON, V98, P1099, DOI 10.1016/j.neuron.2018.05.015; Yedidia J. S., 2001, TR200116 MITS EL RES	38	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002072
C	Kaplan, H; Stemmer, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kaplan, Haim; Stemmer, Uri			Differentially Private k-Means with Constant Multiplicative Error	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We design new differentially private algorithms for the Euclidean k-means problem, both in the centralized model and in the local model of differential privacy. In both models, our algorithms achieve significantly improved error guarantees than the previous state-of-the-art. In addition, in the local model, our algorithm significantly reduces the number of interaction rounds. Although the problem has been widely studied in the context of differential privacy, all of the existing constructions achieve only super constant approximation factors. We present-for the first time-efficient private algorithms for the problem with constant multiplicative error. Furthermore, we show how to modify our algorithms so they compute private coresets for k-means clustering in both models.	[Kaplan, Haim] Tel Aviv Univ, Tel Aviv, Israel; [Kaplan, Haim] Google, Mountain View, CA 94043 USA; [Stemmer, Uri] Ben Gurion Univ Negev, Beer Sheva, Israel	Tel Aviv University; Google Incorporated; Ben Gurion University	Kaplan, H (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.; Kaplan, H (corresponding author), Google, Mountain View, CA 94043 USA.	haimk@post.tau.ac.il; u@uri.co.il			Koshland fellowship; Israel Science Foundation [950/16, 5219/17]	Koshland fellowship; Israel Science Foundation(Israel Science Foundation)	Work done while the second author was a postdoctoral researcher at the Weizmann Institute of Science, supported by a Koshland fellowship, and by the Israel Science Foundation (grants 950/16 and 5219/17).	Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736; Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15; Arthur D., 2007, P 18 ANN ACMS S, P1027, DOI DOI 10.1145/1283383.1283494; Arthur D, 2009, ANN IEEE SYMP FOUND, P405, DOI 10.1109/FOCS.2009.14; Arya V, 2004, SIAM J COMPUT, V33, P544, DOI 10.1137/S0097539702416402; Balcan MF, 2017, PR MACH LEARN RES, V70; Barger A., 2016, P 2016 SIAM INT C DA; Blum A., 2005, P 24 ACM SIGMOD SIGA, P128, DOI DOI 10.1145/1065167.1065184; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Chen K, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1177, DOI 10.1145/1109557.1109687; Cohen E., 2018, P 32 AAAI C ART INT; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Feldman D., 2007, P 23 ACM S COMP GEOM, P11, DOI DOI 10.1145/1247069.1247072; Feldman D, 2017, 2017 16TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN), P3, DOI 10.1145/3055031.3055090; Feldman D, 2011, ACM S THEORY COMPUT, P569; Feldman D, 2009, ACM S THEORY COMPUT, P361; Gupta A, 2010, PROC APPL MATH, V135, P1106; Har-Peled S., 2004, P 36 ANN ACM S THEOR, P291, DOI DOI 10.1145/1007352.1007400; Har-Peled S, 2007, DISCRETE COMPUT GEOM, V37, P3, DOI 10.1007/s00454-006-1271-x; Huang ZY, 2018, PODS'18: PROCEEDINGS OF THE 37TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P395, DOI 10.1145/3196959.3196977; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Johnson W. B., 1984, EXTENSIONS LIPSCHITZ; Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mohan P., 2012, P SIGMOD INT C MAN D, P349, DOI [10.1145/2213836.2213876, DOI 10.1145/2213836.2213876]; Nissim K., 2018, ALGORITHMIC LEARNING, P619; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Nissim K, 2016, PODS'16: PROCEEDINGS OF THE 35TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P413, DOI 10.1145/2902251.2902296; Nock, 2016, P 33 INT C MACH LEAR, V48, P145; Su D, 2016, CODASPY'16: PROCEEDINGS OF THE SIXTH ACM CONFERENCE ON DATA AND APPLICATION SECURITY AND PRIVACY, P26, DOI 10.1145/2857705.2857708; The California Water Boards, 2015, STAT WAT RES CONTR B, P81	33	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305045
C	Karvonen, T; Oates, CJ; Sarkka, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Karvonen, Toni; Oates, Chris. J.; Sarkka, Simo			A Bayes-Sard Cubature Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				HILBERT-SPACES; QUADRATURE; APPROXIMATION; INTERPOLATION	This paper focusses on the formulation of numerical integration as an inferential task. To date, research effort has largely focussed on the development of Bayesian cubature, whose distributional output provides uncertainty quantification for the integral. However, the point estimators associated to Bayesian cubature can be inaccurate and acutely sensitive to the prior when the domain is high-dimensional. To address these drawbacks we introduce Bayes-Sard cubature, a probabilistic framework that combines the flexibility of Bayesian cubature with the robustness of classical cubatures which are well-established. This is achieved by considering a Gaussian process model for the integrand whose mean is a parametric regression model, with an improper prior on each regression coefficient. The features in the regression model consist of test functions which are guaranteed to be exactly integrated, with remaining degrees of freedom afforded to the non-parametric part. The asymptotic convergence of the Bayes-Sard cubature method is established and the theoretical results are numerically verified. In particular, we report two orders of magnitude reduction in error compared to Bayesian cubature in the context of a high-dimensional financial integral.	[Karvonen, Toni; Sarkka, Simo] Aalto Univ, Espoo, Finland; [Oates, Chris. J.] Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England; [Oates, Chris. J.] Alan Turing Inst, London, England	Aalto University; Newcastle University - UK	Karvonen, T (corresponding author), Aalto Univ, Espoo, Finland.	toni.karvonen@aalto.fi; chris.oates@ncl.ac.uk; simo.sarkka@aalto.fi			Aalto ELEC Doctoral School; Lloyd's Register Foundation programme on data-centric engineering; Academy of Finland [266940, 304087, 313708]; National Science Foundation [DMS-1127914]	Aalto ELEC Doctoral School; Lloyd's Register Foundation programme on data-centric engineering; Academy of Finland(Academy of Finland); National Science Foundation(National Science Foundation (NSF))	The authors are grateful for discussion with Aretha Teckentrup, Catherine Powell, Fred Hickernell and Filip Tronarp. TK was supported by the Aalto ELEC Doctoral School. CJO was supported by the Lloyd's Register Foundation programme on data-centric engineering. SS was supported by the Academy of Finland projects 266940, 304087 and 313708. This material was based upon work partially supported by the National Science Foundation under Grant DMS-1127914 to the Statistical and Applied Mathematical Sciences Institute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.	BACH F, 2017, J MACHINE LEARNING R, V18; Berlinet A., 2011, REPRODUCING KERNEL H; Bezhaev AYu., 1991, SOVIET J NUMER ANAL, V6, P95, DOI [10.1515/rnam.1991.6.2.95, DOI 10.1515/RNAM.1991.6.2.95]; BOGACHEV VI, 1998, MATH SURVEYS MONOGRA, V62; Briol F.X., 2018, STAT SCI; Chai H., 2018, ARXIV180204782; Cockayne  J., 2017, ARXIV170203673V2; Davis P.J., 2007, METHODS NUMERICAL IN; DeVore  R., 2018, CONSTRUCTIVE APPROXI; Diaconis P., 1988, STAT DECISION THEORY, V1, P163, DOI DOI 10.1007/978-1-4613-8768-8_20; Gautschi W., 2004, ORTHOGONAL POLYNOMIA, DOI DOI 10.1093/OSO/9780198506720.001.0001, Patent No. [2205.12815, 220512815]; Genz A, 1996, J COMPUT APPL MATH, V71, P299, DOI 10.1016/0377-0427(95)00232-4; Gunter T., 2014, ADV NEURAL INFORM PR, P2789; Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142; Hensman J, 2018, J MACH LEARN RES, V18, P1; Hickernell FJ, 1998, MATH COMPUT, V67, P299, DOI 10.1090/S0025-5718-98-00894-1; Holtz M., 2011, LECT NOTES COMPUTATI, V77; Jagadeeswaran  R., 2018, ARXIV180909803V1; Kanagawa  M., 2017, ARXIV170900147V1; KANAGAWA M, 2016, ADV NEURAL INFORM PR, V29, P3288; Karvonen  T., 2018, ARXIV180910227V1; Karvonen T, 2017, IEEE INT WORKS MACH; Karvonen T, 2018, SIAM J SCI COMPUT, V40, pA697, DOI 10.1137/17M1121779; Kennedy M, 1998, STAT COMPUT, V8, P365, DOI 10.1023/A:1008832824006; Kennedy MC, 2001, J R STAT SOC B, V63, P425, DOI 10.1111/1467-9868.00294; Larkin F. M., 1972, ROCKY MT J MATH, V2, P379; Larkin F.M, 1974, INFORM PROCESSING, V74, P605; LARKIN FM, 1970, MATH COMPUT, V24, P911, DOI 10.2307/2004625; Minka T., 2000, TECHNICAL REPORT; Mosamam AM, 2010, J NONPARAMETR STAT, V22, P711, DOI 10.1080/10485250903388886; O'Hagan  A., 1988, TECHNICAL REPORT; O'Hagan A., 1992, BAYESIAN STAT, P345; Oates C., 2017, ADV NEURAL INFORM PR, V30, P109; Oettershagen J., 2017, THESIS U BONN; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; OHAGAN A, 1978, J R STAT SOC B, V40, P1; Osborne M., 2012, ADV NEURAL INF PROCE, V25, P1; Owhadi  H., 2015, ARXIV150604208V2; PATTERSON TN, 1968, MATH COMPUT, V22, P847, DOI 10.2307/2004583; Portier F., 2018, ARXIV180101797V3; Pronzato  L., 2018, ARXIV180810722V1; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; RICHTERDYN N, 1971, SIAM J NUMER ANAL, V8, P583, DOI 10.1137/0708056; SANTNER T. J., 2003, DESIGN ANAL COMPUTER; SARD A, 1949, AM J MATH, V71, P80, DOI 10.2307/2372095; Sarkka S., 2016, J ADV INFORM FUSION, V11, P31; SCHOENBERG IJ, 1964, B AM MATH SOC, V70, P143, DOI 10.1090/S0002-9904-1964-11054-5; WAHBA G, 1978, J ROY STAT SOC B MET, V40, P364; Wendland H., 2005, CAMBRIDGE MONOGRAPH; Xu WT, 2017, SIAM-ASA J UNCERTAIN, V5, P138, DOI 10.1137/15M105358X; Zhou QP, 2018, INVERSE PROBL, V34, DOI 10.1088/1361-6420/aac287	51	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000039
C	Kasai, H; Mishra, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kasai, Hiroyuki; Mishra, Bamdev			Inexact trust-region algorithms on Riemannian manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RANK MATRIX COMPLETION; OPTIMIZATION METHODS	We consider an inexact variant of the popular Riemannian trust-region algorithm for structured big-data minimization problems. The proposed algorithm approximates the gradient and the Hessian in addition to the solution of a trust-region sub-problem. Addressing large-scale finite-sum problems, we specifically propose sub-sampled algorithms with a fixed bound on sub-sampled Hessian and gradient sizes, where the gradient and Hessian are computed by a random sampling technique. Numerical evaluations demonstrate that the proposed algorithms outperform state-of-the-art Riemannian deterministic and stochastic gradient algorithms across different applications.	[Kasai, Hiroyuki] Univ Electrocommun, Chofu, Tokyo, Japan; [Mishra, Bamdev] Microsoft, Hyderabad, India	University of Electro-Communications - Japan	Kasai, H (corresponding author), Univ Electrocommun, Chofu, Tokyo, Japan.	kasai@is.uec.ac.jp; bamdevm@microsoft.com		Mishra, Bamdev/0000-0001-7430-2843	JSPS KAKENHI [JP16K00031, JP17H01732]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	H. Kasai was partially supported by JSPS KAKENHI Grant Numbers JP16K00031 and JP17H01732. We thank Nicolas Boumal and Hiroyuki Sato for insight discussions and also express our sincere appreciation to Jonas Moritz Kohler for sharing his expertise on sub-sampled algorithms in the Euclidean case.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Balzano L., 2010, ALLERTON; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Boumal N., 2018, IMA J NUMER ANAL; Boumal N, 2015, LINEAR ALGEBRA APPL, V475, P200, DOI 10.1016/j.laa.2015.02.027; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X; Cartis C, 2011, MATH PROGRAM, V127, P245, DOI 10.1007/s10107-009-0286-5; Conn A., 2000, MPS SIAM SERIES OPTI; Da Silva C, 2015, LINEAR ALGEBRA APPL, V481, P131, DOI 10.1016/j.laa.2015.04.015; DEFAZIO A, 2014, NIPS; Erdogdu M., 2015, NIPS; GABAY D, 1982, J OPTIMIZ THEORY APP, V37, P177, DOI 10.1007/BF00934767; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Huang W., 2016, ENUMATH 2015; Huang W, 2015, SIAM J OPTIMIZ, V25, P1660, DOI 10.1137/140955483; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; JOHNSON R., 2013, NIPS; Kasai  H., 2016, ICML; Kasai H., 2018, AISTATS; Kasai H., 2018, ICML; KASAI H, 2018, JMLR, V18; Kohler J. M., 2017, ICML; Kressner D, 2014, BIT, V54, P447, DOI 10.1007/s10543-013-0455-z; Kueng R, 2014, LINEAR ALGEBRA APPL, V441, P110, DOI 10.1016/j.laa.2013.04.018; Le Roux N., 2012, NIPS; LUENBERGER DG, 1972, MANAGE SCI, V18, P620, DOI 10.1287/mnsc.18.11.620; Meyer G., 2011, ICML; Mishra B., 2019, MACHINE LEARNING; Mishra B, 2014, IEEE DECIS CONTR P, P1137, DOI 10.1109/CDC.2014.7039534; Nguyen Lam M., 2017, ICML; Nimishakavi M., 2018, NEURIPS; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108; Porikli F., 2006, ICIP; Reddi S. J., 2016, ICML; Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Sato H., 2017, ARXIV170205594; Shalit U, 2012, J MACH LEARN RES, V13, P429; Theis F. J., 2009, ICA; Toint P. L., 1981, SPARSE MATRICES THEI, P1981; Tuzel O., 2006, ECCV; Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768; Xu Peng, 2017, ARXIV170807164; Yang WH, 2014, PAC J OPTIM, V10, P415; Yao Z., 2018, ARXIV180206925; ZHANG H., 2016, P ADV NEURAL INFORM, P4599	51	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304028
C	Law, HCL; Sejdinovic, D; Cameron, E; Lucas, TCD; Flaxman, S; Battle, K; Fukumizu, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Law, Ho Chung Leon; Sejdinovic, Dino; Cameron, Ewan; Lucas, Tim C. D.; Flaxman, Seth; Battle, Katherine; Fukumizu, Kenji			Variational Learning on Aggregate Outputs with Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PLASMODIUM-FALCIPARUM; AFRICA	While a typical supervised learning framework assumes that the inputs and the outputs are measured at the same levels of granularity, many applications, including global mapping of disease, only have access to outputs at a much coarser level than that of the inputs. Aggregation of outputs makes generalization to new inputs much more difficult. We consider an approach to this problem based on variational learning with a model of output aggregation and Gaussian processes, where aggregation leads to intractability of the standard evidence lower bounds. We propose new bounds and tractable approximations, leading to improved prediction accuracy and scalability to large datasets, while explicitly taking uncertainty into account. We develop a framework which extends to several types of likelihoods, including the Poisson model for aggregated count data. We apply our framework to a challenging and important problem, the fine-scale spatial modelling of malaria incidence, with over 1 million observations.	[Law, Ho Chung Leon; Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England; [Cameron, Ewan; Lucas, Tim C. D.; Battle, Katherine] Univ Oxford, Big Data Inst, Oxford, England; [Flaxman, Seth] Imperial Coll London, Dept Math, London, England; [Flaxman, Seth] Imperial Coll London, Data Sci Inst, London, England; [Fukumizu, Kenji] Inst Stat Math, Tachikawa, Tokyo, Japan; [Sejdinovic, Dino] Alan Turing Inst, London, England	University of Oxford; University of Oxford; Imperial College London; Imperial College London; Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan	Law, HCL (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	ho.law@stats.ox.ac.uk; dino.sejdinovic@stats.ox.ac.uk; dr.ewan.cameron@gmail.com; timcdlucas@gmail.com; s.flaxman@imperial.ac.uk; kather-ine.battle@bdi.ox.ac.uk; fukumizu@ism.ac.jp		Fukumizu, Kenji/0000-0002-3488-2625; Sejdinovic, Dino/0000-0001-5547-9213	EPSRC; MRC through the OxWaSP CDT programme [EP/L016710/1]; JSPS KAKENHI [26280009]; ERC [FP7/617071]; Alan Turing Institute [EP/N510129/1]; Bill and Melinda Gates Foundation;  [OPP1152978];  [OPP1132415];  [OPP1106023]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); MRC through the OxWaSP CDT programme(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); ERC(European Research Council (ERC)European Commission); Alan Turing Institute; Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); ; ; 	We thank Kaspar Martens for useful discussions, and Dougal Sutherland for providing the code base in which this work was based on. HCLL is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1). HCLL and KF are supported by JSPS KAKENHI 26280009. EC and KB are supported by OPP1152978, TL by OPP1132415 and the MAP database by OPP1106023. DS is supported in part by the ERC (FP7/617071) and by The Alan Turing Institute (EP/N510129/1). The data were provided by the Malaria Atlas Project supported by the Bill and Melinda Gates Foundation.	Ancarani LU, 2008, J MATH PHYS, V49, DOI 10.1063/1.2939395; Ba J., 2017, P 3 INT C LEARN REPR; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bhatt S, 2015, NATURE, V526, P207, DOI 10.1038/nature15535; Chang, 2014, ARXIV14025902; Cheplygina V, 2015, PATTERN RECOGN LETT, V59, P11, DOI 10.1016/j.patrec.2015.03.008; de Freitas N., 2005, P 21 C UNC ART INT, P332; Gething PW, 2016, NEW ENGL J MED, V375, P2435, DOI 10.1056/NEJMoa1606701; Goovaerts P, 2010, MATH GEOSCI, V42, P535, DOI 10.1007/s11004-010-9286-5; Hamprecht Fred A, 2017, P IEEE C COMP VIS PA, P6570; Hensman J, 2013, GAUSSIAN PROCESSES B; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Howitt R, 2003, EUR REV AGRIC ECON, V30, P359, DOI 10.1093/erae/30.3.359; Keil P, 2013, METHODS ECOL EVOL, V4, P82, DOI 10.1111/j.2041-210x.2012.00264.x; Kotzias D, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P597, DOI 10.1145/2783258.2783380; Law H. C. L., 2018, INT C ART INT STAT A, P1167; Law H. C. L., 2017, NIPS; Lloyd C, 2015, PR MACH LEARN RES, V37, P1814; Melnikov V., 2016, MACHINE LEARNING KNO, P756, DOI DOI 10.1007/978-3-319-46227-1_47; Muandet K., 2016, ARXIV PREPRINT ARXIV, P133; Musicant DR, 2007, IEEE DATA MINING, P252, DOI 10.1109/ICDM.2007.50; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Patrini Giorgio, 2014, NIPS; Quadrianto N, 2009, J MACH LEARN RES, V10, P2349; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Smola AJ, 2001, ADV NEUR IN, V13, P619; Szabo Z, 2016, J MACH LEARN RES, V17; Teh Y. W., 2007, P ADV NEUR INF PROC, P1353; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Warrel DA, 2017, OXFORD TXB MED; Weiss DJ, 2015, MALARIA J, V14, DOI 10.1186/s12936-015-0574-x; Xavier A, 2018, SPAT STAT-NETH, V23, P91, DOI 10.1016/j.spasta.2017.11.005; Yu F. X., 2013, ARXIV13060886; Zaheer Manzil, 2017, NIPS	36	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000057
C	Lee, J; Kim, GH; Poupart, P; Kim, KE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Jongmin; Kim, Geon-Hyeong; Poupart, Pascal; Kim, Kee-Eung			Monte-Carlo Tree Search for Constrained POMDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MARKOV DECISION-PROCESSES	Monte-Carlo Tree Search (MCTS) has been successfully applied to very large POMDPs, a standard model for stochastic sequential decision-making problems. However, many real-world problems inherently have multiple goals, where multi-objective formulations are more natural. The constrained POMDP (CPOMDP) is such a model that maximizes the reward while constraining the cost, extending the standard POMDP model. To date, solution methods for CPOMDPs assume an explicit model of the environment, and thus are hardly applicable to large-scale real-world problems. In this paper, we present CC-POMCP (Cost-Constrained POMCP), an online MCTS algorithm for large CPOMDPs that leverages the optimization of LP-induced parameters and only requires a black-box simulator of the environment. In the experiments, we demonstrate that CC-POMCP converges to the optimal stochastic action selection in CPOMDP and pushes the state-of-the-art by being able to scale to very large problems.	[Lee, Jongmin; Kim, Geon-Hyeong; Kim, Kee-Eung] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea; [Poupart, Pascal] Univ Waterloo, Waterloo AI Inst, Waterloo, ON, Canada; [Poupart, Pascal] Vector Inst, Toronto, ON, Canada; [Kim, Kee-Eung] PROWLER Io, Cambridge, England	Korea Advanced Institute of Science & Technology (KAIST); University of Waterloo	Lee, J (corresponding author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.	jmlee@ai.kaist.ac.kr; ghkim@ai.kaist.ac.kr; ppoupart@uwaterloo.ca; kekim@cs.kaist.ac.kr			ICT R&D program of MSIT/IITP of Korea [2017-0-01778]; DAPA/ADD of Korea [UD170018CD]; NRF of Korea	ICT R&D program of MSIT/IITP of Korea; DAPA/ADD of Korea; NRF of Korea	This work was supported by the ICT R&D program of MSIT/IITP of Korea (No. 2017-0-01778) and DAPA/ADD of Korea (UD170018CD). J. Lee acknowledges the Global Ph.D. Fellowship Program by NRF of Korea (NRF-2018-Global Ph.D. Fellowship Program).	Altman E., 1999, STOCH MODEL SER; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Feinberg EA, 2002, J MATH ANAL APPL, V273, P93, DOI 10.1016/S0022-247X(02)00213-5; Gelly S, 2011, ARTIF INTELL, V175, P1856, DOI 10.1016/j.artint.2011.03.007; Guez A, 2013, J ARTIF INTELL RES, V48, P841, DOI 10.1613/jair.4117; Isom J. D., 2008, AAAI, P291; Katt S, 2017, PR MACH LEARN RES, V70; Kim D., 2011, P 22 INT JOINT C ART, P1968; Kocsis L., 2006, 1 U TART; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Lee J., 2017, P 26 INT JOINT C ART, P2088; Oh Eunsoo, 2011, P 27 C UNC ART INT U, P565; PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441; Piunovskiy AB, 2000, OPER RES LETT, V27, P119, DOI 10.1016/S0167-6377(00)00039-0; Poupart P, 2015, AAAI CONF ARTIF INTE, P3342; Silver D., 2010, NIPS, P2164; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sondik E.J., 1971, THESIS; Trey Smith, 2004, P 20 C UNC ART INT, P520; Undurti A, 2010, IEEE INT CONF ROBOT, P3966, DOI 10.1109/ROBOT.2010.5509743; Williams JD, 2007, COMPUT SPEECH LANG, V21, P393, DOI 10.1016/j.csl.2006.06.008	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002047
C	Lei, YW; Tang, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lei, Yunwen; Tang, Ke			Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				APPROXIMATION; REGULARIZATION; ALGORITHMS	We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives with non-smooth loss functions, for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to study the generalization performance of multi-pass stochastic gradient descent (SGD) in a non-parametric setting. Our high-probability generalization bounds enjoy a logarithmical dependency on the number of passes provided that the step size sequence is square-summable, which improves the existing bounds in expectation with a polynomial dependency and therefore gives a strong justification on the ability of multi-pass SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our theoretical findings.	[Lei, Yunwen; Tang, Ke] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen Key Lab Computat Intelligence, Shenzhen 518055, Peoples R China	Southern University of Science & Technology	Tang, K (corresponding author), Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen Key Lab Computat Intelligence, Shenzhen 518055, Peoples R China.	leiyw@sustc.edu.cn; tangk3@sustc.edu.cn	Lei, Yunwen/V-2782-2018	Lei, Yunwen/0000-0002-5383-467X	National Key Research and Development Program of China [2017YFB1003102]; National Natural Science Foundation of China [61806091, 61672478]; Science and Technology Innovation Committee Foundation of Shenzhen [ZDSYS201703031748284]; Shenzhen Peacock Plan [KQTD2016112514355531]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Innovation Committee Foundation of Shenzhen; Shenzhen Peacock Plan	This work is supported in part by the National Key Research and Development Program of China (Grant No. 2017YFB1003102), the National Natural Science Foundation of China (Grant Nos. 61806091 and 61672478), the Science and Technology Innovation Committee Foundation of Shenzhen (Grant No. ZDSYS201703031748284) and Shenzhen Peacock Plan (Grant No. KQTD2016112514355531).	Agarwal A, 2009, IMMUNE INFERTILITY, P155, DOI 10.1007/978-3-642-01379-9_3.2; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; BACH F., 2011, ADV NEURAL INFORM PR, P451; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Bottou Leon, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Duchi J. C., 2010, COLT, P14; Duchi Y. J., 2009, ADV NEURAL INFORM PR, P495; Hardt M, 2016, PR MACH LEARN RES, V48; Lacoste-Julien Simon, 2012, ARXIV12122002; Lei YW, 2020, APPL COMPUT HARMON A, V48, P343, DOI 10.1016/j.acha.2018.05.005; Lei YW, 2018, J MACH LEARN RES, V18; Lei YW, 2018, SIAM J IMAGING SCI, V11, P547, DOI 10.1137/17M1136225; Lin JH, 2016, PR MACH LEARN RES, V48; Lin Junhong, 2016, ADV NEURAL INFORM PR, P4556; London B, 2017, ADV NEUR IN, V30; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nguyen LM, 2018, PR MACH LEARN RES, V80; Orabona F, 2014, ADV NEUR IN, V27; RAKHLIN A., 2012, P INT C MACH LEARN, P1571; Rosasco L., 2015, ADV NEURAL INFORM PR, V28, P1630; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Shamir O., 2013, INT C MACH LEARN, P71; Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089; Steinwart I, 2007, ANN STAT, V35, P575, DOI 10.1214/009053606000001226; Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Ying YM, 2006, IEEE T INFORM THEORY, V52, P4775, DOI 10.1109/TIT.2006.883632; Ying YM, 2017, APPL COMPUT HARMON A, V42, P224, DOI 10.1016/j.acha.2015.08.007; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332; Zhaoguang Pan, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7285868; Zhou ZY, 2017, ADV NEUR IN, V30; Zinkevich, 2003, P 20 INT C MACH LEAR, P928	44	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301050
C	Lindenbaum, O; Stanley, JS; Wolf, G; Krishnaswamy, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lindenbaum, Ofir; Stanley, Jay S., III; Wolf, Guy; Krishnaswamy, Smita			Geometry Based Data Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DENSITY ESTIMATORS; DIFFUSION	We propose a new type of generative model for high-dimensional data that learns a manifold geometry of the data, rather than density, and can generate points evenly along this manifold. This is in contrast to existing generative models that represent data density, and are strongly affected by noise and other artifacts of data collection. We demonstrate how this approach corrects sampling biases and artifacts, thus improves several downstream data analysis tasks, such as clustering and classification. Finally, we demonstrate that this approach is especially useful in biology where, despite the advent of single-cell technologies, rare subpopulations and gene-interaction relationships are affected by biased sampling. We show that SUGAR can generate hypothetical populations, and it is able to reveal intrinsic patterns and mutual-information relationships between genes on a single-cell RNA sequencing dataset of hematopoiesis.	[Lindenbaum, Ofir; Wolf, Guy] Yale Univ, Appl Math Program, New Haven, CT 06511 USA; [Stanley, Jay S., III] Yale Univ, Computat Biol & Bioinformat Program, New Haven, CT 06510 USA; [Krishnaswamy, Smita] Yale Univ, Dept Genet & Comp Sci, New Haven, CT 06510 USA	Yale University; Yale University; Yale University	Krishnaswamy, S (corresponding author), Yale Univ, Dept Genet & Comp Sci, New Haven, CT 06510 USA.	ofir.lindenbaum@yale.edu; jay.stanley@yale.edu; guy.wolf@yale.edu; smita.krishnawamy@yale.edu			Chan-Zuckerberg Initiative [182702]	Chan-Zuckerberg Initiative	This research was partially funded by grant from the Chan-Zuckerberg Initiative (ID: 182702).	Alcala-Fdez J, 2009, SOFT COMPUT, V13, P307, DOI 10.1007/s00500-008-0323-y; [Anonymous], 2003, ADV NEURAL INFORM PR; Beal M. J., 2003, BAYESIAN STAT, V7; Bengio Y., 2006, IMAGINE M, P115; Bengio Y., 2005, ADV NEURAL INFORM PR, V17, P129; Bermanis A, 2016, APPL COMPUT HARMON A, V41, P190, DOI 10.1016/j.acha.2015.07.005; Bermanis A, 2016, APPL COMPUT HARMON A, V40, P207, DOI 10.1016/j.acha.2015.02.001; Brubaker M., 2012, P 15 INT C ART INT S, P161; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Doersch Carl, 2016, ARXIV160605908V2; GaryWeiss M., 2004, SIGKDD EXPLOR NEWSL, V6, P7, DOI DOI 10.1145/1007730.1007734; Gine E, 2002, ANN I H POINCARE-PR, V38, P907, DOI 10.1016/S0246-0203(02)01128-7; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gorodkin J, 2004, COMPUT BIOL CHEM, V28, P367, DOI 10.1016/j.compbiolchem.2004.09.006; Grun D, 2015, NATURE, V525, P251, DOI 10.1038/nature14966; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; Hensman P., 2015, THESIS; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Keller Y, 2010, IEEE T SIGNAL PROCES, V58, P403, DOI 10.1109/TSP.2009.2030861; Kim JK, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms9687; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689; Li X, 2013, PROCEEDINGS OF THE 2013 8TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE & EDUCATION (ICCSE 2013), P89; Li YQ, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms10836; Lindenbaum Ofir, 2017, ARXIV170701093; Lopez V, 2013, INFORM SCIENCES, V250, P113, DOI 10.1016/j.ins.2013.07.007; Moon K. R., 2019, BIORXIV, DOI [10.1101/120378, DOI 10.1101/120378]; Moon Kevin R., 2017, CURRENT OPINION SYST; Oztireli AC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866190; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; Scott DW, 2015, WILEY SER PROBAB ST, P125; SCOTT DW, 1985, ANN STAT, V13, P1024, DOI 10.1214/aos/1176349654; Seaman DE, 1996, ECOLOGY, V77, P2075, DOI 10.2307/2265701; Seiffert C, 2010, IEEE T SYST MAN CY A, V40, P185, DOI 10.1109/TSMCA.2009.2029559; Singer A, 2009, P NATL ACAD SCI USA, V106, P16090, DOI 10.1073/pnas.0905547106; VARANASI MK, 1989, J ACOUST SOC AM, V86, P1404, DOI 10.1121/1.398700; Velten L, 2017, NAT CELL BIOL, V19, P271, DOI 10.1038/ncb3493; Wu J, 2012, ADV K MEANS CLUSTERI, P17, DOI DOI 10.1007/978-3-642-29807-3_2; Zelnik-Manor Lihi, 2005, P ADV NEUR INF PROC, P1601	42	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301039
C	Liu, MR; Zhang, XX; Zhou, X; Yang, TB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Mingrui; Zhang, Xiaoxuan; Zhou, Xun; Yang, Tianbao			Faster Online Learning of Optimal Threshold for Consistent F-measure Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GRADIENT; BOUNDS	In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (e.g., classification error rate), F-measure is non-decomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack statistical consistency guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is a novel stochastic algorithm with low memory and computational costs, which can enjoy a convergence rate of (O) over tilde (1/root n) for learning the optimal threshold under a mild condition on the convergence of the posterior probability, where n is the number of processed examples. It is provably faster than its predecessor based on a heuristic for updating the threshold. The experiments verify the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms.	[Liu, Mingrui; Zhang, Xiaoxuan; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Zhou, Xun] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA	University of Iowa; University of Iowa	Liu, MR (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	mingrui-liu@uiowa.edu; tianbao-yang@uiowa.edu		Liu, Mingrui/0000-0002-5181-3429	National Science Foundation [IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	The authors thank the anonymous reviewers for their helpful comments. M. Liu, X. Zhang and T. Yang are partially supported by National Science Foundation (IIS-1545995).	Agarwal A., 2012, NIPS, P1547; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Busa-Fekete R., 2015, PROC INT C NEURAL IN, P595; CesaBianchi N, 1996, IEEE T NEURAL NETWOR, V7, P604, DOI 10.1109/72.501719; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Grunwald P. D., 2007, MINIMUM DESCRIPTION; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kar P., 2014, ADV NEURAL INFORM PR, P694; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Liu MR, 2018, PR MACH LEARN RES, V80; Liu Mingrui, 2018, ADV NEURAL INFORM PR; Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031; Narasimhan H, 2015, PR MACH LEARN RES, V37, P199; Natarajan Nagarajan, 2014, NEURAL INFORM PROCES; Parambath S., 2014, ADV NEURAL INF PROCE, V27, P2123; RAKHLIN A., 2012, P INT C MACH LEARN, P1571; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699; Tang Y, 2006, 2006 INTERNATIONAL CONFERENCE ON COLLABORATIVE COMPUTING: NETWORKING, APPLICATIONS AND WORKSHARING, P18; van Erven Tim, 2015, J MACHINE LEARNING R; Xu Y, 2017, PR MACH LEARN RES, V70; Yan Y, 2017, AAAI CONF ARTIF INTE, P2817; Ye N., 2012, P 29 INT C MACH LEAR; Zhang T, 2006, ANN STAT, V34, P2180, DOI 10.1214/009053606000000704; Zhao MJ, 2013, J MACH LEARN RES, V14, P1033; Zhao PL, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P919; Zinkevich, 2003, P 20 INT C MACH LEAR, P928	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303085
C	Lui, KYC; Ding, GW; Huang, RT; McCann, RJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lui, Kry Yik Chau; Ding, Gavin Weiguang; Huang, Ruitong; McCann, Robert J.			Dimensionality Reduction has Quantifiable Imperfections: Two Geometric Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				VISUAL ANALYSIS; WAIST	In this paper, we investigate Dimensionality reduction (DR) maps in an information retrieval setting from a quantitative topology point of view. In particular, we show that no DR maps can achieve perfect precision and perfect recall simultaneously. Thus a continuous DR map must have imperfect precision. We further prove an upper bound on the precision of Lipschitz continuous DR maps. While precision is a natural measure in an information retrieval setting, it does not measure "how" wrong the retrieved data is. We therefore propose a new measure based on Wasserstein distance that comes with similar theoretical guarantee. A key technical step in our proofs is a particular optimization problem of the L-2-Wasserstein distance over a constrained set of distributions. We provide a complete solution to this optimization problem, which can be of independent interest on the technical side.	[Lui, Kry Yik Chau; Ding, Gavin Weiguang; Huang, Ruitong] Borealis AI, Toronto, ON, Canada; [McCann, Robert J.] Univ Toronto, Dept Math, Toronto, ON, Canada	University of Toronto	Lui, KYC (corresponding author), Borealis AI, Toronto, ON, Canada.	yikchau.y.lui@borealisai.com; gavin.ding@borealisai.com; ruitong.huang@borealisai.com; mccann@math.toronto.edu						Akopyan A, 2020, INT MATH RES NOTICES, V2020, P669, DOI 10.1093/imrn/rny037; Akopyan A, 2017, B LOND MATH SOC, V49, P690, DOI 10.1112/blms.12062; Alpert H, 2015, J TOPOL ANAL, V7, P73, DOI 10.1142/S1793525315500053; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Arora Sanjeev, 2018, P MACHINE LEARNING R, P1455; Bonneel N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024192; Boutsidis C., 2010, ADV NEURAL INFORM PR, V23, P298; Caffarelli LA, 2010, ANN MATH, V171, P673, DOI 10.4007/annals.2010.171.673; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Figalli A, 2010, ARCH RATION MECH AN, V195, P533, DOI 10.1007/s00205-008-0212-7; Flamary R<prime>emi, 2017, POT PYTHON OPTIMAL T; Granata D, 2016, SCI REP-UK, V6, DOI 10.1038/srep31377; Gromov M, 2003, GEOM FUNCT ANAL, V13, P178, DOI 10.1007/s000390300004; Guillemin V, 2010, DIFFERENTIAL TOPOLOG, V370; Guth  LARRY, 2012, THE ABEL PRIZE 2008, P181; Hjaltason GR, 2003, IEEE T PATTERN ANAL, V25, P530, DOI 10.1109/TPAMI.2003.1195989; Klartag B, 2017, GEOM FUNCT ANAL, V27, P130, DOI 10.1007/s00039-017-0397-8; Korman J, 2013, P NATL ACAD SCI USA, V110, P10064, DOI 10.1073/pnas.1221333110; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lespinats S, 2011, COMPUT GRAPH FORUM, V30, P113, DOI 10.1111/j.1467-8659.2010.01835.x; Martins RM, 2014, COMPUT GRAPH-UK, V41, P26, DOI 10.1016/j.cag.2014.01.006; McCann RJ, 2004, NONLINEARITY, V17, P1891, DOI 10.1088/0951-7715/17/5/017; McQueen J., 2016, ADV NEURAL INFORM PR, P2631; Muger M., 2015, MATH SEMESTERBERICHT, V62, P59; Narayanan H., 2010, NEURIPS, P1786; PAYNE LE, 1967, SIAM REV, V9, P453, DOI 10.1137/1009070; Roeer  Malte, 2013, ARXIV13051529; Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217; Schreck T, 2010, INFORM VISUAL, V9, P181, DOI 10.1057/ivs.2010.2; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Der Maaten Laurens, 2009, ARTIF INTELL, P384; Venna J, 2010, J MACH LEARN RES, V11, P451; Verma N, 2013, J MACH LEARN RES, V14, P2415; Wang X., 2005, MATH MAG, V78, P390, DOI [10.2307/30044198, DOI 10.2307/30044198]	36	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003005
C	Magdon-Ismail, M; Xia, LR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Magdon-Ismail, Malik; Xia, Lirong			A Mathematical Model For Optimal Decisions In A Representative Democracy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONDORCET JURY THEOREM; MONOTONICITY; QUALITY	Direct democracy, where each voter casts one vote, fails when the average voter competence falls below 50%. This happens in noisy settings when voters have limited information. Representative democracy, where voters choose representatives to vote, can be an elixir in both these situations. We introduce a mathematical model for studying representative democracy, in particular understanding the parameters of a representative democracy that gives maximum decision making capability. Our main result states that under general and natural conditions, 1. for fixed voting cost, the optimal number of representatives is linear; 2. for polynomial cost, the optimal number of representatives is logarithmic.	[Magdon-Ismail, Malik; Xia, Lirong] Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Magdon-Ismail, M (corresponding author), Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.	magdon@cs.rpi.edu; xial@cs.rpi.edu		Xia, Lirong/0000-0002-9800-6691	NSF [1453542]; ONR [N00014-17-1-2621]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	We thank all anonymous reviewers for helpful comments and suggestions. LX is supported by NSF #1453542 and ONR #N00014-17-1-2621.	Auriol E, 2012, PUBLIC CHOICE, V153, P419, DOI 10.1007/s11127-011-9801-3; Azari Soufiani H., 2014, P ADV NEURAL INFORM; Ben-Yashar R, 2000, SOC CHOICE WELFARE, V17, P189, DOI 10.1007/s003550050014; Ben-Yashar R., 2003, J PUBLIC ECON THEORY, V5, P527; Ben-Yashar R, 2011, PUBLIC CHOICE, V148, P435, DOI 10.1007/s11127-010-9663-0; Berend D, 2005, SOC CHOICE WELFARE, V24, P83, DOI 10.1007/s00355-003-0293-z; Berend D, 1998, SOC CHOICE WELFARE, V15, P481, DOI 10.1007/s003550050118; Berend D, 2007, SOC CHOICE WELFARE, V28, P507, DOI 10.1007/s00355-006-0179-y; Besley T, 1997, Q J ECON, V112, P85, DOI 10.1162/003355397555136; Bruce Bartlett, 2014, ENL HOUS REPR; Caragiannis Ioannis, 2016, ACM T EC COMPUTATION, V4; Cohensius G, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P858; Condorcet N, 1785, ESSAI APPL ANAL PROB; Elkind E, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P182; FELD SL, 1984, PUBLIC CHOICE, V42, P273, DOI 10.1007/BF00124946; Fey M, 2003, SOC CHOICE WELFARE, V20, P27, DOI 10.1007/s003550200157; Flynn Brian, 2012, WHATS WRONG C ITS NO; Gentle JE, 2009, STAT COMPUT SER, P3; GRADSTEIN M, 1987, ECON LETT, V23, P335, DOI 10.1016/0165-1765(87)90140-6; GROFMAN B, 1983, THEOR DECIS, V15, P261, DOI 10.1007/BF00125672; Humphreys Keith, 2016, WHY WE MIGHT WANT GR; Kahng A., 2018, P 32 AAAI C ART INT, P1087; Kanazawa S, 1998, MATH SOC SCI, V35, P69, DOI 10.1016/S0165-4896(97)00028-0; Karotkin D, 2003, SOC CHOICE WELFARE, V20, P429, DOI 10.1007/s003550200190; Karotkin Drora, 1995, LABOUR ECON, V2, P41; MILLER NR, 1986, INFORMATION POOLING, P173; Mulkhopadhaya K, 2003, J LAW ECON ORGAN, V19, P24; NITZAN S, 1984, THEOR DECIS, V17, P47, DOI 10.1007/BF00140055; NITZAN S, 1980, INT ECON REV, V21, P547, DOI 10.2307/2526351; Nitzan Shmuel, 2017, OXFORD HDB LAW EC; OWEN G, 1989, MATH SOC SCI, V17, P1, DOI 10.1016/0165-4896(89)90012-7; PAROUSH J, 1989, SOC CHOICE WELFARE, V6, P127, DOI 10.1007/BF00303167; Paroush J, 1998, SOC CHOICE WELFARE, V15, P15; Pivato Marcus, SOCIAL CHOICE WELFAR, V40, P581; Procaccia Ariel D., 2012, P 28 C UNC ART INT; Sandholm T., 2005, P UAI 05, P145; Sapir L, 2005, MATH SOC SCI, V50, P83, DOI 10.1016/j.mathsocsci.2005.01.002; Stone Peter, 2013, OPTIMAL COMMITTEE PE; Xia L., 2016, P UAI; Xia L, 2011, IJCAI 11, P446; YOUNG HP, 1988, AM POLIT SCI REV, V82, P1231, DOI 10.2307/1961757	41	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304069
C	Mainsah, BO; Kalika, D; Collins, LM; Liu, SY; Throckmorton, CS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mainsah, Boyla O.; Kalika, Dmitry; Collins, Leslie M.; Liu, Siyuan; Throckmorton, Chandra S.			Information-based Adaptive Stimulus Selection to Optimize Communication Efficiency in Brain-Computer Interfaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Stimulus-driven brain-computer interfaces (BCIs), such as the P300 speller, rely on using a sequence of sensory stimuli to elicit specific neural responses as control signals, while a user attends to relevant target stimuli that occur within the sequence. In current BCIs, the stimulus presentation schedule is typically generated in a pseudo-random fashion. Given the non-stationarity of brain electrical signals, a better strategy could be to adapt the stimulus presentation schedule in real-time by selecting the optimal stimuli that will maximize the signal-to-noise ratios of the elicited neural responses and provide the most information about the user's intent based on the uncertainties of the data being measured. However, the high-dimensional stimulus space limits the development of algorithms with tractable solutions for optimized stimulus selection to allow for real-time decision-making within the stringent time requirements of BCI processing. We derive a simple analytical solution of an information-based objective function for BCI stimulus selection by transforming the high-dimensional stimulus space into a one-dimensional space that parameterizes the objective function - the prior probability mass of the stimulus under consideration, irrespective of its contents. We demonstrate the utility of our adaptive stimulus selection algorithm in improving BCI performance with results from simulation and real-time human experiments.	[Mainsah, Boyla O.; Collins, Leslie M.; Liu, Siyuan; Throckmorton, Chandra S.] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Kalika, Dmitry] Johns Hopkins Univ, Appl Phys Lab, Laurel, MD USA	Duke University; Johns Hopkins University; Johns Hopkins University Applied Physics Laboratory	Collins, LM (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	leslie.collins@duke.edu						Clearwater J. M., 2008, Journal of Integrative Neuroscience, V7, P529, DOI 10.1142/S0219635208002003; Cover TM, 2006, ELEMENTS INFORM THEO; Cowley B, 2017, ADV NEURAL INFORM PR, V30, P1395; DiMattina C, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00101; FARWELL LA, 1988, ELECTROEN CLIN NEURO, V70, P510, DOI 10.1016/0013-4694(88)90149-6; Hill J, 2009, ADV NEURAL INFORM PR, V21, P665; Kalika D, 2017, IEEE SYS MAN CYBERN, P1405, DOI 10.1109/SMC.2017.8122810; Kastella K, 1997, IEEE T SYST MAN CY A, V27, P112, DOI 10.1109/3468.553230; Krusienski DJ, 2008, J NEUROSCI METH, V167, P15, DOI 10.1016/j.jneumeth.2007.07.017; Lees S, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aa9817; Mainsah BO, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2552/aa7525; Mainsah BO, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/6/066007; Mainsah BO, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016013; Mak JN, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/2/025003; Martens SMM, 2011, NEURAL COMPUT, V23, P160, DOI 10.1162/NECO_a_00066; Martens SMM, 2009, J NEURAL ENG, V6, DOI 10.1088/1741-2560/6/2/026003; Moghimi S, 2013, ASSIST TECHNOL, V25, P99, DOI 10.1080/10400435.2012.723298; Park J, 2012, IEEE T NEUR SYS REH, V20, P584, DOI 10.1109/TNSRE.2012.2191979; Ramadan RA, 2017, NEUROCOMPUTING, V223, P26, DOI 10.1016/j.neucom.2016.10.024; Schalk G, 2010, PRACTICAL GUIDE TO BRAIN-COMPUTER INTERFACING WITH BCI2000, P1, DOI 10.1007/978-1-84996-092-2; SQUIRES K, 1977, PERCEPT PSYCHOPHYS, V22, P31, DOI 10.3758/BF03206077; Throckmorton CS, 2013, IEEE T NEUR SYS REH, V21, P508, DOI 10.1109/TNSRE.2013.2253125; Townsend G, 2010, CLIN NEUROPHYSIOL, V121, P1109, DOI 10.1016/j.clinph.2010.01.030; Varona P, 2016, CLOSED LOOP NEUROSCIENCE, P81, DOI 10.1016/B978-0-12-802452-2.00006-8; Verhoeven T, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/6/066027; Wolpaw J.R., 2012, BRAIN COMPUTER INTER, DOI [10.1093/acprof:oso/9780195388855.001.0001, DOI 10.1093/ACPROF:OSO/9780195388855.001.0001]	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304080
C	Marques, AN; Lam, RR; Willcox, KE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Marques, Alexandre N.; Lam, Remi R.; Willcox, Karen E.			Contour location via entropy reduction leveraging multiple information sources	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				IDENTIFICATION; DESIGN	We introduce an algorithm to locate contours of functions that are expensive to evaluate. The problem of locating contours arises in many applications, including classification, constrained optimization, and performance analysis of mechanical and dynamical systems (reliability, probability of failure, stability, etc.). Our algorithm locates contours using information from multiple sources, which are available in the form of relatively inexpensive, biased, and possibly noisy approximations to the original function. Considering multiple information sources can lead to significant cost savings. We also introduce the concept of contour entropy, a formal measure of uncertainty about the location of the zero contour of a function approximated by a statistical surrogate model. Our algorithm locates contours efficiently by maximizing the reduction of contour entropy per unit cost.	[Marques, Alexandre N.] MIT, Dept Aeronaut & Astronaut, Cambridge, MA 02139 USA; [Lam, Remi R.] MIT, Ctr Computat Engn, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Willcox, Karen E.] Univ Texas Austin, Inst Computat Engn & Sci, Austin, TX 78712 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); University of Texas System; University of Texas Austin	Marques, AN (corresponding author), MIT, Dept Aeronaut & Astronaut, Cambridge, MA 02139 USA.	noll@mit.edu; rlam@mit.edu; kwillcox@ices.utexas.edu	Willcox, Karen E/AAH-4519-2021		U.S. Air Force Center of Excellence on Multi-Fidelity Modeling of Rocket Combustor Dynamics [FA9550-17-1-0195]; AFOSR MURI on managing multiple information sources of multi-physics systems [FA9550-15-1-0038, FA9550-18-1-0023]	U.S. Air Force Center of Excellence on Multi-Fidelity Modeling of Rocket Combustor Dynamics; AFOSR MURI on managing multiple information sources of multi-physics systems	This work was supported in part by the U.S. Air Force Center of Excellence on Multi-Fidelity Modeling of Rocket Combustor Dynamics, Award FA9550-17-1-0195, and by the AFOSR MURI on managing multiple information sources of multi-physics systems, Awards FA9550-15-1-0038 and FA9550-18-1-0023.	[Anonymous], 2017, ADV NEURAL INFORM PR; Basudhar A, 2008, PROBABILIST ENG MECH, V23, P1, DOI 10.1016/j.probengmech.2007.08.004; Basudhar A, 2010, STRUCT MULTIDISCIP O, V42, P517, DOI 10.1007/s00158-010-0511-0; Bect J, 2012, STAT COMPUT, V22, P773, DOI 10.1007/s11222-011-9241-4; Bichon BJ, 2008, AIAA J, V46, P2459, DOI 10.2514/1.34321; Chevalier C, 2014, TECHNOMETRICS, V56, P455, DOI 10.1080/00401706.2013.860918; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cover T.M, 2006, WILEY SERIES TELECOM; Dribusch C, 2010, STRUCT MULTIDISCIP O, V42, P693, DOI 10.1007/s00158-010-0516-8; Forrester AIJ, 2007, P R SOC A, V463, P3251, DOI 10.1098/rspa.2007.1900; HEINEMANN RF, 1981, CHEM ENG SCI, V36, P1411, DOI 10.1016/0009-2509(81)80175-3; Lam R, 2015, 56 AIAA ASCE AHS ASC; Lam RR, 2016, ADV NEUR IN, V29; Lecerf M, 2015, AIAA J, V53, P3073, DOI 10.2514/1.J053893; Peherstorfer B, 2016, SIAM J SCI COMPUT, V38, pA3163, DOI 10.1137/15M1046472; Picheny V, 2010, J MECH DESIGN, V132, DOI 10.1115/1.4001873; Ranjan P, 2008, TECHNOMETRICS, V50, P527, DOI 10.1198/004017008000000541; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schohn G., 2000, P 17 INT C MACH LEAR, P839; Stroh R, 2017, FIRE SAFETY J, V91, P1016, DOI 10.1016/j.firesaf.2017.03.059; Surjanovic S., VIRTUAL LIB SIMULATI; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Wang HQ, 2016, J COMPUT PHYS, V313, P247, DOI 10.1016/j.jcp.2016.02.053; Warmuth MK, 2002, ADV NEUR IN, V14, P1449; Zhou Y., 2010, THESIS BOWLING GREEN	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305025
C	Moniz, JRA; Beckham, C; Rajotte, S; Honari, S; Pal, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Moniz, Joel Ruben Antony; Beckham, Christopher; Rajotte, Simon; Honari, Sina; Pal, Christopher			Unsupervised Depth Estimation, 3D Face Rotation and Replacement	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present an unsupervised approach for learning to estimate three dimensional (3D) facial structure from a single image while also predicting 3D viewpoint transformations that match a desired pose and facial geometry. We achieve this by inferring the depth of facial keypoints of an input image in an unsupervised manner, without using any form of ground-truth depth information. We show how it is possible to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3D affine transformation matrix that maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on a desired target facial geometry or pose. Our resulting approach, called DepthNets, can therefore be used to infer plausible 3D transformations from one face pose to another, allowing faces to be frontalized, transformed into 3D models or even warped to another pose and facial geometry. Lastly, we identify certain shortcomings with our formulation, and explore adversarial image translation techniques as a post-processing step to re-synthesize complete head shots for faces re-targeted to different poses or identities. (1)	[Moniz, Joel Ruben Antony] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Beckham, Christopher; Rajotte, Simon; Honari, Sina; Pal, Christopher] Mila Univ Montreal, Montreal, PQ, Canada; [Beckham, Christopher; Rajotte, Simon; Pal, Christopher] Polytech Montreal, Montreal, PQ, Canada; [Pal, Christopher] Element AI, Montreal, PQ, Canada	Carnegie Mellon University; Universite de Montreal; Polytechnique Montreal	Moniz, JRA (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	jrmoniz@andrew.cmu.edu; christopher.beckham@polymtl.ca; simon.rajotte@polymtl.ca; honaris@iro.umontreal.ca; christopher.pal@polymtl.ca			Samsung; Google	Samsung(Samsung); Google(Google Incorporated)	We would like to thank Samsung and Google for partially funding this project. We are also thankful to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal for helpful discussions.	Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Eigen David, 2014, NEURIPS; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Godard C., 2017, P IEEE C COMP VIS PA; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058; Hassner T, 2013, IEEE I CONF COMP VIS, P3607, DOI 10.1109/ICCV.2013.448; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Honari S, 2016, PROC CVPR IEEE, P5743, DOI 10.1109/CVPR.2016.619; Honari Sina, 2018, P IEEE C COMP VIS PA; Huang Rui, 2017, P IEEE INT C COMP VI; Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117; Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Parkhi O.M., 2015, BRIT MACH VIS C BMVC, P1; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Sun YH, 2017, IEEE ICC; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tewari Ayush, 2017, P IEEE INT C COMP VI, V2; Thewlis James, 2017, P IEEE INT C COMP VI, V1, P5; Tran Luan, 2017, P IEEE C COMP VIS PA; Tung H.Y., 2017, ADV NEURAL INFORM PR, P5242; Tung Hsiao-Yu Fish, 2017, P IEEE INT C COMP VI, V2; Yin XX, 2017, HEALTH INFOR SCI, P1, DOI 10.1007/978-3-319-57027-3_1; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhao J., 2017, ADV NEURAL INFORM PR, P66; Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235; Zhou T., 2017, P IEEE C COMP VIS PA	30	1	1	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004031
C	Ohnishi, M; Yukawa, M; Johansson, M; Sugiyama, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ohnishi, Motoya; Yukawa, Masahiro; Johansson, Mikael; Sugiyama, Masashi			Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Motivated by the success of reinforcement learning (RL) for discrete-time tasks such as AlphaGo and Atari games, there has been a recent surge of interest in using RL for continuous-time control of physical systems (cf. many challenging tasks in OpenAI Gym and DeepMind Control Suite). Since discretization of time is susceptible to error, it is methodologically more desirable to handle the system dynamics directly in continuous time. However, very few techniques exist for continuous-time RL and they lack flexibility in value function approximation. In this paper, we propose a novel framework for model-based continuous-time value function approximation in reproducing kernel Hilbert spaces. The resulting framework is so flexible that it can accommodate any kind of kernel-based approach, such as Gaussian processes and kernel adaptive filters, and it allows us to handle uncertainties and nonstationarity without prior knowledge about the environment or what basis functions to employ. We demonstrate the validity of the presented framework through experiments.	[Ohnishi, Motoya] Keio Univ, KTH, RIKEN, Tokyo, Japan; [Yukawa, Masahiro] Keio Univ, RIKEN, Tokyo, Japan; [Johansson, Mikael] KTH, Stockholm, Sweden; [Sugiyama, Masashi] Univ Tokyo, RIKEN, Tokyo, Japan	Keio University; RIKEN; Keio University; RIKEN; Royal Institute of Technology; RIKEN; University of Tokyo	Ohnishi, M (corresponding author), Keio Univ, KTH, RIKEN, Tokyo, Japan.	motoya.ohnishi@riken.jp; yukawa@elec.keio.ac.jp; mikaelj@ee.kth.se; masashi.sugiyama@riken.jp	Yukawa, Masahiro/G-5504-2014; Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	KAKENHI [17H00757, 18H01446, 15H02757]; Swedish Research Council; Knut and Allice Wallenberg Foundation	KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Swedish Research Council(Swedish Research CouncilEuropean Commission); Knut and Allice Wallenberg Foundation(Knut & Alice Wallenberg Foundation)	This work was partially conducted when M. Ohnishi was at the GRITS Lab, Georgia Institute of Technology; M. Ohnishi thanks the members of the GRITS Lab, including Dr. Li Wang, and Prof. Magnus Egerstedt for discussions regarding barrier functions. M. Yukawa was supported in part by KAKENHI 18H01446 and 15H02757, M. Johansson was supported in part by the Swedish Research Council and by the Knut and Allice Wallenberg Foundation, and M. Sugiyama was supported in part by KAKENHI 17H00757. Lastly, the authors thank all of the anonymous reviewers for their very insightful comments.	Agrawal A., 2017, P RSS; Ames A. D., 2016, ARXIV160906408; [Anonymous], 1996, ROBUST OPTIMAL CONTR; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; BAIRD LC, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P2448, DOI 10.1109/ICNN.1994.374604; Brockman G., 2016, OPENAI GYM; Doya K, 2000, NEURAL COMPUT, V12, P219, DOI 10.1162/089976600300015961; Engel Y., 2005, P 22 INT C MACH LEAR, P201, DOI DOI 10.1145/1102351.1102377; Fleming W., 2006, CONTROLLED MARKOV PR, V25; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Glotfelter P, 2017, IEEE CONTR SYST LETT, V1, P310, DOI 10.1109/LCSYS.2017.2710943; Grunewalder S., 2012, P ICML; Khalil H. K, 1996, NONLINEAR SYSTEMS; Khasminskii, 2012, STOCHASTIC STABILITY; Konda VR, 2004, ANN APPL PROBAB, V14, P796, DOI 10.1214/105051604000000116; Koppel A., 2017, IEEE T AUTOMATIC CON; Koppel A, 2017, INT CONF ACOUST SPEE, P4671, DOI 10.1109/ICASSP.2017.7953042; Krylov N.V., 2008, CONTROLLED DIFFUSION, V14; Lewis Frank L., 2009, IEEE CIRCUITS SYSTEM, V9; Liberzon D., 2011, CALCULUS VARIATIONS, DOI DOI 10.2307/J.CTVCM4G0S; Liu W., 2010, ICML; Minh HQ, 2010, CONSTR APPROX, V32, P307, DOI 10.1007/s00365-009-9080-0; Morris B, 2013, IEEE DECIS CONTR P, P2920, DOI 10.1109/CDC.2013.6760327; Munos R, 1998, ADV NEUR IN, V10, P1029; Nishiyama Y., 2012, ARXIV12104887; Ohnishi M., 2018, ARXIV180109627; Oksendal B., 2013, STOCHASTIC DIFFERENT; Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829; Pan Y., 2014, ADV NEURAL INFORM PR, P1907; Rasmussen C., 2006, GAUSSIAN PROCESSES M, V1; Richard C, 2009, IEEE T SIGNAL PROCES, V57, P1058, DOI 10.1109/TSP.2008.2009895; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Sugiyama M., 2015, STAT REINFORCEMENT L; Sun W., 2016, P IJCAI; Sutton R. S., 2009, ADV NEURAL INFORM PR, V21, P1609; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Takizawa M, 2015, IEEE T SIGNAL PROCES, V63, P4257, DOI 10.1109/TSP.2015.2437835; Tassa Y., 2018, ARXIV180100690; Taylor Gavin, 2009, P 26 ANN INT C MACHI, P1017; Theodorou E, 2010, P AMER CONTR CONF, P1125; Theodorou E, 2010, IEEE INT CONF ROBOT, P2397, DOI 10.1109/ROBOT.2010.5509336; TSITSIKLIS JN, 1997, P ADV NEUR INF PROC, V9, P1075; Vamvoudakis KG, 2010, AUTOMATICA, V46, P878, DOI 10.1016/j.automatica.2010.02.018; Wang L., 2017, IEEE T ROBOTICS; Wang L., 2017, ARXIV171005472; Wieland P., 2007, IFAC P, V40, P462; Xiangru Xu, 2015, IFAC - Papers Online, V48, P54, DOI 10.1016/j.ifacol.2015.11.152; Xu X, 2007, IEEE T NEURAL NETWOR, V18, P973, DOI 10.1109/TNN.2007.899161; Yukawa M, 2012, IEEE T SIGNAL PROCES, V60, P4672, DOI 10.1109/TSP.2012.2200889; Zhou DX, 2008, J COMPUT APPL MATH, V220, P456, DOI 10.1016/j.cam.2007.08.023	52	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302080
C	Parmas, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Parmas, Paavo			Total stochastic gradient algorithms and applications in reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Backpropagation and the chain rule of derivatives have been prominent; however, the total derivative rule has not enjoyed the same amount of attention. In this work we show how the total derivative rule leads to an intuitive visual framework for creating gradient estimators on graphical models. In particular, previous "policy gradient theorems" are easily derived. We derive new gradient estimators based on density estimation, as well as a likelihood ratio gradient, which "jumps" to an intermediate node, not directly to the objective function. We evaluate our methods on model-based policy gradient algorithms, achieve good performance, and present evidence towards demystifying the success of the popular PILCO algorithm [5].	[Parmas, Paavo] Okinawa Inst Sci & Technol Grad Univ, Neural Computat Unit, Onna, Okinawa, Japan	Okinawa Institute of Science & Technology Graduate University	Parmas, P (corresponding author), Okinawa Inst Sci & Technol Grad Univ, Neural Computat Unit, Onna, Okinawa, Japan.	paavo.parmas@oist.jp			OIST Graduate School; JSPS KAKENHI [JP16H06563, JP16K21738]	OIST Graduate School; JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	We thank the anonymous reviewers for useful comments. This work was supported by OIST Graduate School funding and by JSPS KAKENHI Grant Number JP16H06563 and JP16K21738.	Asadi Kavosh, 2017, ARXIV170900503; Ciosek K., 2017, ARXIV170605374; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; McHutchon A., 2014, THESIS; Mnih A, 2016, PR MACH LEARN RES, V48; Murray I., 2016, ARXIV160207527; Naumann U, 2008, MATH PROGRAM, V112, P427, DOI 10.1007/s10107-006-0042-z; Parmas P, 2018, PR MACH LEARN RES, V80; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Silver D, 2014, PR MACH LEARN RES, V32; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tieleman T, 2012, COURSERA NEURAL NETW, V4, P26; Titsias MK, 2015, ADV NEUR IN, V28; Tokui S, 2017, PR MACH LEARN RES, V70; van Seijen H, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P177	24	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004073
C	Pimentel-Alarcon, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pimentel-Alarcon, Daniel			Mixture Matrix Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Completing a data matrix X has become an ubiquitous problem in modern data science, with motivations in recommender systems, computer vision, and networks inference, to name a few. One typical assumption is that X is low-rank. A more general model assumes that each column of X corresponds to one of several low-rank matrices. This paper generalizes these models to what we call mixture matrix completion (MMC): the case where each entry of X corresponds to one of several low-rank matrices. MMC is a more accurate model for recommender systems, and brings more flexibility to other completion and clustering problems. We make four fundamental contributions about this new model. First, we show that MMC is theoretically possible (well-posed). Second, we give its precise information-theoretic identifiability conditions. Third, we derive the sample complexity of MMC. Finally, we give a practical algorithm for MMC with performance comparable to the state-of-the-art for simpler related problems, both on synthetic and real data.	[Pimentel-Alarcon, Daniel] Georgia State Univ, Dept Comp Sci, Atlanta, GA 30303 USA	University System of Georgia; Georgia State University	Pimentel-Alarcon, D (corresponding author), Georgia State Univ, Dept Comp Sci, Atlanta, GA 30303 USA.	pimentel@gsu.edu						Alderson  D., 2005, IEEE ACM T NETWORKIN; Ashraphijuo  M., 2017, IEEE INT S INF THEOR; Balzano  L., 2010, ALL C COMM CONTR COM; Balzano L., 2012, IEEE STAT SIGNAL PRO; Barford  P., 2001, P ACM INT MEAS WORKS; Basri R., 2003, IEEE T PATTERN ANAL; Bouwmans  T., 2016, COMPUTER SCI REV; Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes E., 2010, IEEE T INFORM THEORY; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen YX, 2014, PR MACH LEARN RES, V32, P100; Chen Yuxin, 2015, IEEE T INFORM THEORY; Chunikhina  E., 2014, IEEE STAT SIGNAL PRO; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ding XH, 2011, IEEE T IMAGE PROCESS, V20, P3419, DOI 10.1109/TIP.2011.2156801; Elhamifar E., 2016, ADV NEURAL INFORM PR; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Eriksson B, 2010, IEEE INFOCOM SER; Eriksson Brian, 2012, ARTIFICIAL INTELLIGE; Eriksson Brian, 2008, ACM SIGCOMM; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; GOVINDAN R, IEEE INFOCOM 2000; Highlander  S., 2012, ANIMAL HLTH RES REV; Hu H., 2015, IEEE PATTERN ANAL MA; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kiraly  F., 2015, J MACHINE LEARNING R; Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169; Lin Z., 2009, UILUENG092215; Lin Z., 2009, INT WORKSH COMP ADV; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Ma S., 2011, MATH PROGRAMMING; Ma  Y., LOW RANK MATRIX RECO; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Mande  S., 2012, BRIEFINGS BIOINFORMA; Marcais  G., 2018, PLOS COMPUTATIONAL B; Nguyen  N., 2016, BIOFILMS MICROBIOMES; Ongie G, 2017, PR MACH LEARN RES, V70; Peng X, 2015, AAAI CONF ARTIF INTE, P3827; Pimentel-Alarcon D, 2016, 2016 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP); Pimentel-Alarcon  D., 2016, IEEE J SELECTED TOPI; Pimentel-Alarcon D., 2017, ELECT J STAT; Pimentel-Alarcon D., 2014, IEEE STAT SIGNAL PRO; Pimentel-Alarcon DL, 2016, PR MACH LEARN RES, V48; Pimentel-Alarcon DL, 2016, IEEE INT SYMP INFO, P96, DOI 10.1109/ISIT.2016.7541268; Pimentel-Alarcon DL, 2015, IEEE INT SYMP INFO, P2191, DOI 10.1109/ISIT.2015.7282844; Qu C., 2015, ADV NEURAL INFORM PR; Ranjan  R., 2016, BIOCH BIOPHYSICAL RE; Schumacher H, 2013, IEEE MTT S INT MICR; Shen  Y., 2014, INT C NUM OPT NUM LI; Sherwood  R., 2008, ACM SIGCOMM; Shu XB, 2014, PROC CVPR IEEE, P3874, DOI 10.1109/CVPR.2014.495; Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199; Spring  N., 2004, IEEE ACM T NETWORKIN; Tipping M.E., 1999, NEURAL COMPUTATION; Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228; Tron R, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940399; Wang Y., 2015, ADV NEURAL INFORM PR; Wang Y.-X., 2013, ARXIV13091233; Wen  Z., 2012, MATH PROGRAMMING COM; Yang CY, 2015, PR MACH LEARN RES, V37, P2463; Yang  Y., 2014, PREPRINT; Yi  X., 2014, INT C MACH LEARN; Yuan X, 2009, SPARSE LOW RANK MATR	68	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302022
C	Ryali, CK; Reddy, G; Yu, AJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ryali, Chaitanya K.; Reddy, Gautam; Yu, Angela J.			Demystifying excessively volatile human learning: A Bayesian persistent prior and a neural approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DYNAMICS	Understanding how humans and animals learn about statistical regularities in stable and volatile environments, and utilize these regularities to make predictions and decisions, is an important problem in neuroscience and psychology. Using a Bayesian modeling framework, specifically the Dynamic Belief Model (DBM), it has previously been shown that humans tend to make the default assumption that environmental statistics undergo abrupt, unsignaled changes, even when environmental statistics are actually stable. Because exact Bayesian inference in this setting, an example of switching state space models, is computationally intensive, a number of approximately Bayesian and heuristic algorithms have been proposed to account for learning/prediction in the brain. Here, we examine a neurally plausible algorithm, a special case of leaky integration dynamics we denote as EXP (for exponential filtering), that is significantly simpler than all previously suggested algorithms except for the delta-learning rule, and which far outperforms the delta rule in approximating Bayesian prediction performance. We derive the theoretical relationship between DBM and EXP, and show that EXP gains computational efficiency by foregoing the representation of inferential uncertainty (as does the delta rule), but that it nevertheless achieves near-Bayesian performance due to its ability to incorporate a "persistent prior" influence unique to DBM and absent from the other algorithms. Furthermore, we show that EXP is comparable to DBM but better than all other models in reproducing human behavior in a visual search task, suggesting that human learning and prediction also incorporates an element of persistent prior. More broadly, our work demonstrates that when observations are information-poor, detecting changes or modulating the learning rate is both difficult and thus unnecessary for making Bayes-optimal predictions.	[Ryali, Chaitanya K.] Univ Calif San Diego, Dept Comp Sci & Engn, 9500 Gilman Dr, La Jolla, CA 92093 USA; [Reddy, Gautam] Univ Calif San Diego, Dept Phys, 9500 Gilman Dr, La Jolla, CA 92093 USA; [Yu, Angela J.] Univ Calif San Diego, Dept Cognit Sci, 9500 Gilman Dr, La Jolla, CA 92093 USA	University of California System; University of California San Diego; University of California System; University of California San Diego; University of California System; University of California San Diego	Ryali, CK (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, 9500 Gilman Dr, La Jolla, CA 92093 USA.	rckrishn@eng.ucsd.edu; gnallama@physics.ucsd.edu; ajyu@ucsd.edu	Reddy, Gautam/AHD-8256-2022		NSF CRCNS grant [BCS-1309346]	NSF CRCNS grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank He Huang for assistance with data collection, Samer Sabri for helpful input with the writing, and the anonymous reviewers for helpful comments. This work was in part funded by an NSF CRCNS grant (BCS-1309346) to AJY.	Adams R. P., 2007, ARXIV07103742STAT; Ahmad S, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00955; Behrens TEJ, 2007, NAT NEUROSCI, V10, P1214, DOI 10.1038/nn1954; Bialek W, 2001, NEURAL COMPUT, V13, P2409, DOI 10.1162/089976601753195969; Daw ND, 2006, NATURE, V441, P876, DOI 10.1038/nature04766; Ghahramani Z, 2000, NEURAL COMPUT, V12, P831, DOI 10.1162/089976600300015619; Guo D, 2018, ADV NEURAL INFORM PR; HERRNSTEIN RJ, 1961, J EXP ANAL BEHAV, V4, P267, DOI 10.1901/jeab.1961.4-267; Ide JS, 2013, J NEUROSCI, V33, P2039, DOI 10.1523/JNEUROSCI.2201-12.2013; Ma N, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01046; Meyniel F, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004305; Nassar MR, 2012, NAT NEUROSCI, V15, P1040, DOI 10.1038/nn.3130; Nassar MR, 2010, J NEUROSCI, V30, P12366, DOI 10.1523/JNEUROSCI.0822-10.2010; Rescorla RA., 1972, CLASSICAL CONDITION, pp. 64, DOI DOI 10.1101/GR.110528.110; Sugrue LP, 2004, SCIENCE, V304, P1782, DOI 10.1126/science.1094765; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Wilder M, 2009, ADV NEURAL INFORM PR, P2053; Wilson RC, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003150; Yu A, 2003, ADV NEURAL INF PROCE, P173; Yu A J., 2014, DECISION, V1, P275, DOI [10.1037/dec0000013, DOI 10.1037/DEC0000013]; Yu A. J., 2013, ADV NEURAL INFORM PR, V26, P2607; Yu AJ, 2005, NEURON, V46, P681, DOI 10.1016/j.neuron.2005.04.026; Yu AJ, 2009, ADV NEURAL INF PROCE, V21, P1873, DOI DOI 10.1371/JOURNAL.PONE.0099909	23	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF	34366637				2022-12-19	WOS:000461823302077
C	Saparbayeva, B; Zhang, MM; Lin, LZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Saparbayeva, Bayan; Zhang, Michael Minyi; Lin, Lizhen			Communication Efficient Parallel Algorithms for Optimization on Manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EXTRINSIC SAMPLE MEANS; ROBUST	The last decade has witnessed an explosion in the development of models, theory and computational algorithms for "big data" analysis. In particular, distributed computing has served as a natural and dominating paradigm for statistical inference. However, the existing literature on parallel inference almost exclusively focuses on Euclidean data and parameters. While this assumption is valid for many applications, it is increasingly more common to encounter problems where the data or the parameters lie on a non-Euclidean space, like a manifold for example. Our work aims to fill a critical gap in the literature by generalizing parallel inference algorithms to optimization on manifolds. We show that our proposed algorithm is both communication efficient and carries theoretical convergence guarantees. In addition, we demonstrate the performance of our algorithm to the estimation of Frechet means on simulated spherical data and the low-rank matrix completion problem over Grassmann manifolds applied to the Netflix prize data set.	[Saparbayeva, Bayan; Lin, Lizhen] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA; [Zhang, Michael Minyi] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA	University of Notre Dame; Princeton University	Saparbayeva, B (corresponding author), Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.	bsaparba@nd.edu; mz8@cs.princeton.edu; lizhen.lin@nd.edu			DARPA [N66001-17-1-4041]; NSF [1447721, IIS 1663870]; DMS Career [1654579]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); DMS Career	Bayan Saparbayeva was partially supported by DARPA N66001-17-1-4041. Michael Zhang was supported by NSF grant 1447721. Lizhen Lin acknowledges the support from NSF grants IIS 1663870, DMS Career 1654579 and a DARPA grant N66001-17-1-4041.	Alexander AL, 2007, NEUROTHERAPEUTICS, V4, P316, DOI 10.1016/j.nurt.2007.05.011; Bhattacharya A, 2012, INST MATH STAT MG, P209; Bhattacharya R, 2005, ANN STAT, V33, P1225, DOI 10.1214/009053605000000093; Bhattacharya R, 2003, ANN STAT, V31, P1; Bhattacharya R, 2017, P AM MATH SOC, V145, P413, DOI 10.1090/proc/13216; BOUMAL N., 2014, THESIS; Dalcin L, 2005, J PARALLEL DISTR COM, V65, P1108, DOI 10.1016/j.jpdc.2005.03.010; Downs T, 1971, VECTORCARDIOGRAPHY, V2, P216; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Frechet M., 1948, ANN I H POINCARE, V10, P215; Ho J, 2004, PROC CVPR IEEE, P782; Jordan M. I., 2018, J AM STAT ASS; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kolaczyk Eric, 2017, ARXIV E PRINTS; Kurtek S, 2012, J AM STAT ASSOC, V107, P1152, DOI 10.1080/01621459.2012.699770; Kurtek S, 2012, IEEE T PATTERN ANAL, V34, P1717, DOI 10.1109/TPAMI.2011.233; Lee J., 2015, ARXIV150304337; Lin LZ, 2017, STAT SINICA, V27, P535, DOI 10.5705/ss.202016.0017; Mackey L, 2015, J MACH LEARN RES, V16, P913; Minsker S, 2017, J MACH LEARN RES, V18; Neiswanger Willie, P 30 C UNC ART INT, P623; Nemeth C, 2018, BAYESIAN ANAL, V13, P507, DOI 10.1214/17-BA1063; Salehian H., 2015, MATH FDN COMPUTATION, V3, P143; Scott SL, 2016, INT J MANAG SCI ENG, V11, P78, DOI 10.1080/17509653.2016.1142191; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Teja G. Prabhu, 2012, 2012 International Conference on Recent Trends in Information Technology (ICRTIT), P103, DOI 10.1109/ICRTIT.2012.6206780; Wang XY, 2015, ADV NEUR IN, V28; Zhang MM, 2018, COMPUT STAT DATA AN, V127, P229, DOI 10.1016/j.csda.2018.05.016; Zhang YC, 2013, J MACH LEARN RES, V14, P3321	30	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303056
C	Schmidt, F; Hofmann, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Schmidt, Florian; Hofmann, Thomas			Deep State Space Models for Unconditional Word Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Autoregressive feedback is considered a necessity for successful unconditional text generation using stochastic sequence models. However, such feedback is known to introduce systematic biases into the training process and it obscures a principle of generation: committing to global information and forgetting local nuances. We show that a non-autoregressive deep state space model with a clear separation of global and local uncertainty can be built from only two ingredients: An independent noise source and a deterministic transition function. Recent advances on flow-based variational inference can be used to train an evidence lower-bound without resorting to annealing, auxiliary losses or similar measures. The result is a highly interpretable generative model on par with comparable auto-regressive models on the task of word generation.	[Schmidt, Florian; Hofmann, Thomas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Schmidt, F (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	florian.schmidt@inf.ethz.ch; thomas.hofmann@inf.ethz.ch						Bandanau D., 2014, ABS14090473 CORR; Bayer J, 2014, ARXIV14117610; Bengio Samy, 2015, ABS150603099 CORR; Bowman Samuel R., 2015, ABS151106349 CORR; Burda Yuri, 2015, ABS150900519 CORR; Che Tong, 2017, ABS170207983 CORR; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Dinh Laurent, 2016, ABS160508803 CORR; Fedus William, 2018, ABS180107736 CORR; Filippova K., 2015, C P EMNLP 2015 C EMP, P360; Fraccaro Marco, 2016, SEQUENTIAL NEURAL MO, P2199; Goyal A., 2017, ADV NEURAL INFORM PR, P6716; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Graves Alex, 2013, ARXIV13080850 CORR; Gregor K., 2015, ABS150204623 CORR; Hu Z., 2017, INT C MACH LEARN ICM; Karl Maximilian, 2017, ARXIV160506432; Kingma D. P., 2016, ABS160604934 CORR; Kingma Diederik P., 2014, ABS14126980 CORR; Kiros R., 2015, ARXIV150606726; Kusner M.J., 2016, GANS SEQUENCES DISCR; Leblond Remi, 2017, ABS170604499 CORR; Maddison Chris J., 2017, ABS170509279 CORR; Mikolov T., 2010, INTERSPEECH 2010; Naesseth C. A., 2017, ARXIV170511140; Ranzato Marc'Aurelio, 2015, ABS151106732 CORR; Ranzato Marc'Aurelio, 2015, ARXIV151106732; Rezende D., 2015, ICML, P1530; Rush A. M., 2015, ABS150900685 CORR; Shabanian Samira, 2017, VARIATIONAL BI LSTMS, P11; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P5998; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Wiseman Sam, 2016, ABS160602960 CORR; Yu Lantao, 2016, ABS160905473 CORR; Zhu Y., 2015, ARXIV150606724	38	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000064
C	Shao, H; Yu, XT; King, I; Lyu, MR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shao, Han; Yu, Xiaotian; King, Irwin; Lyu, Michael R.			Almost Optimal Algorithms for Linear Stochastic Bandits with Heavy-Tailed Payoffs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGRET	In linear stochastic bandits, it is commonly assumed that payoffs are with sub-Gaussian noises. In this paper, under a weaker assumption on noises, we study the problem of linear stochastic bandits with heavy-tailed payoffs (LinBET), where the distributions have finite moments of order 1 + epsilon, for some epsilon is an element of (0, 1]. We rigorously analyze the regret lower bound of LinBET as Omega(T1/1+epsilon), implying that finite moments of order 2 (i.e., finite variances) yield the bound of Omega(root T), with T being the total number of rounds to play bandits. The provided lower bound also indicates that the state-of-the-art algorithms for LinBET are far from optimal. By adopting median of means with a well-designed allocation of decisions and truncation based on historical information, we develop two novel bandit algorithms, where the regret upper bounds match the lower bound up to polylogarithmic factors. To the best of our knowledge, we are the first to solve LinBET optimally in the sense of the polynomial order on T. Our proposed algorithms are evaluated based on synthetic datasets, and outperform the state-of-the-art results.	[Shao, Han; Yu, Xiaotian; King, Irwin; Lyu, Michael R.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China	Chinese University of Hong Kong	Shao, H (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	hshao@cse.cuhk.edu.hk; xtyu@cse.cuhk.edu.hk; king@cse.cuhk.edu.hk; lyu@cse.cuhk.edu.hk	King, Irwin/C-9681-2015	King, Irwin/0000-0001-8106-6447	Research Grants Council of the Hong Kong Special Administrative Region, China [CUHK 14208815, CUHK 14210717]; Microsoft Research Asia (2018 Microsoft Research Asia Collaborative Research Award)	Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council); Microsoft Research Asia (2018 Microsoft Research Asia Collaborative Research Award)	The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14210717 of the General Research Fund), and Microsoft Research Asia (2018 Microsoft Research Asia Collaborative Research Award).	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934; Agrawal S., 2012, C LEARN THEOR, P39; Audibert JY, 2011, ANN STAT, V39, P2766, DOI 10.1214/11-AOS918; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bubeck S., 2010, THESIS; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Carpentier A, 2014, ADV NEUR IN, V27; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Cont R, 2000, MACROECON DYN, V4, P170; Dani V, 2008, P C LEARN THEOR COLT, P355; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; Gittins J. C., 2011, MULTIARMED BANDIT AL, V2nd; Hsu D, 2014, PR MACH LEARN RES, V32, P37; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore T., 2017, ADV NEURAL INFORM PR, p1583{1592; Lattimore T, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P477; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Liebeherr J, 2012, IEEE T INFORM THEORY, V58, P1010, DOI 10.1109/TIT.2011.2173713; Medina AM, 2016, PR MACH LEARN RES, V48; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Roberts JA, 2015, CURR OPIN NEUROBIOL, V31, P164, DOI 10.1016/j.conb.2014.10.014; Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334; SHAO M, 1993, P IEEE, V81, P986, DOI 10.1109/5.231338; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Vakili S, 2013, IEEE J-STSP, V7, P759, DOI 10.1109/JSTSP.2013.2263494; Yu XT, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P937	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003002
C	Shim, H; Hwang, SJ; Yang, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shim, Hajin; Hwang, Sung Ju; Yang, Eunho			Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of active feature acquisition where the goal is to sequentially select the subset of features in order to achieve the maximum prediction performance in the most cost-effective way at test time. In this work, we formulate this active feature acquisition as a joint learning problem of training both the classifier (environment) and the reinforcement learning (RL) agent that decides either to 'stop and predict' or 'collect a new feature' at test time, in a cost-sensitive manner. We also introduce a novel encoding scheme to represent acquired subsets of features by proposing an order-invariant set encoding at the feature level, which also significantly reduces the search space for our agent. We evaluate our model on a carefully designed synthetic dataset for the active feature acquisition as well as several medical datasets. Our framework shows meaningful feature acquisition process for diagnosis that complies with human knowledge, and outperforms all baselines in terms of prediction performance as well as feature acquisition cost.	[Shim, Hajin; Hwang, Sung Ju; Yang, Eunho] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Hwang, Sung Ju; Yang, Eunho] AItrics, Seoul, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Shim, H (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.	shimazing@kaist.ac.kr; sjhwang82@kaist.ac.kr; eunhoy@kaist.ac.kr	Yang, Eunho/K-8395-2016		Institute for Information & communications Technology Promotion(IITP) grant [2017-0-01779, 2017-0-00537]; Engineering Research Center Program through the National Research Foundation of Korea (NRF) - Korea government(MSIT) [NRF-2018R1A5A1059921]; NAVER Corporation (Research on deep and structurally constrained machine learning)	Institute for Information & communications Technology Promotion(IITP) grant; Engineering Research Center Program through the National Research Foundation of Korea (NRF) - Korea government(MSIT); NAVER Corporation (Research on deep and structurally constrained machine learning)	This work was supported by Institute for Information & communications Technology Promotion(IITP) grant (2017-0-01779, A machine learning and statistical inference framework for explainable artificial intelligence, 2017-0-00537, Development of Autonomous IoT Collaboration Framework for Space Intelligence), the Engineering Research Center Program through the National Research Foundation of Korea (NRF) (NRF-2018R1A5A1059921) funded by the Korea government(MSIT) and NAVER Corporation (Research on deep and structurally constrained machine learning).	Ba J., 2014, ARXIV; Ba J, 2015, ADV NEUR IN, V28; Bahdanau D., 2014, ARXIV14090473, p1409.0473; Bilgic  Mustafa, 2007, P NAT C ART INT; Denoyer  L., 2011, JOINT EUR C MACH LEA; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215; Greiner R, 2002, ARTIF INTELL, V139, P137, DOI 10.1016/S0004-3702(02)00209-6; Hasselt H., 2010, ADV NEURAL INFORM PR, V23; He, 2012, ADV NEURAL INFORM PR, P3149; He H., 2016, ARXIV160202181; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Kanani P., 2012, P 5 ACM INT C WEB SE; Kanani Pallika, 2008, ADV NEURAL INFORM PR; Kapoor A, 2009, ADV NEURAL INFORM PR; Kendall A., 2017, WHAT UNCERTAINTIES W, V3, P4; Kingma D.P, P 3 INT C LEARNING R; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; Nan F, 2014, INT CONF ACOUST SPEE; Narasimhan K., 2016, EMNLP; Ruckstiess  T., 2011, AUSTR C ART INT; Ruckstiess T, 2013, INT J MACH LEARN CYB, V4, P235, DOI 10.1007/s13042-012-0092-x; Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347; Sheng V. S., 2006, P 23 INT C MACH LEAR, P809; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Trapeznikov K, 2013, ARTIF INTELL, P581; Vinyals O., 2016, P ICLR; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu Z., 2013, ICML, P133	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301036
C	Sun, HT; Bing, LD; Cohen, WW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sun, Haitian; Bing, Lidong; Cohen, William W.			Semi-Supervised Learning with Declaratively Specified Entropy Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). SSL methods based on different assumptions perform differently on different tasks, which leads to difficulties applying them in practice. In this paper, we propose to use entropy to unify many types of constraints. Our method can be used to easily specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics. Besides, our model is flexible as to the underlying learning mechanism. Compared to prior frameworks for specifying SSL techniques, our technique achieves consistent improvements on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.	[Sun, Haitian; Cohen, William W.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Bing, Lidong] Alibaba DAMO Acad, Machine Intelligence Technol, R&D Ctr Singapore, Hangzhou, Zhejiang, Peoples R China	Carnegie Mellon University	Bing, LD (corresponding author), Alibaba DAMO Acad, Machine Intelligence Technol, R&D Ctr Singapore, Hangzhou, Zhejiang, Peoples R China.	haitians@cs.cmu.edu; l.bing@alibaba-inc.com; wcohen@cs.cmu.edu						[Anonymous], 2003, ADV NEURAL INFORM PR; [Anonymous], 2016, ARXIV160308861; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bing LD, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1454; Bing LD, 2017, AAAI CONF ARTIF INTE, P3408; Bing Lidong, 2015, P 2015 C EMPIRICAL M, P524; Bing Lidong, 2016, 30 AAAI C ART INT, P2899; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Carlson A, 2010, AAAI CONF ARTIF INTE, P1306; Cohen W.W., 2017, CORR; GALE WA, 1992, SPEECH AND NATURAL LANGUAGE, P233; Grandvalet Y., 2005, CAP, P529; Hoffmann R., 2011, P 49 ANN M ASS COMP, V1, P541, DOI DOI 10.5555/2002472; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Kipf TN, 2016, P INT C LEARN REPR; Kouki P, 2015, P 9 ACM C RECOMMENDE, P99; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; PLASTINO AR, 1993, PHYS LETT A, V177, P177, DOI 10.1016/0375-9601(93)90021-Q; Pujara J, 2013, LECT NOTES COMPUT SC, V8218, P542, DOI 10.1007/978-3-642-41335-3_34; Ratner A., 2017, ARXIV171110160; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Surdeanu M, 2012, P 2012 JOINT C EMP M, V2012, P455; Talukdar PP, 2009, LECT NOTES ARTIF INT, V5782, P442, DOI 10.1007/978-3-642-04174-7_29; Velickovic P., 2017, STAT-US, V1050, P20; Wang WY, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2129, DOI 10.1145/2505515.2505573; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu Xiaojin., 2003, P ICLR, P912	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304044
C	Synnaeve, G; Lin, ZM; Gehring, J; Gant, D; Mella, V; Khalidov, V; Carion, N; Usunier, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Synnaeve, Gabriel; Lin, Zeming; Gehring, Jonas; Gant, Dan; Mella, Vegard; Khalidov, Vasil; Carion, Nicolas; Usunier, Nicolas			Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We formulate the problem of defogging as state estimation and future state prediction from previous, partial observations in the context of real-time strategy games. We propose to employ encoder-decoder neural networks for this task, and introduce proxy tasks and baselines for evaluation to assess their ability of capturing basic game rules and high-level dynamics. By combining convolutional neural networks and recurrent networks, we exploit spatial and sequential correlations and train well-performing models on a large dataset of human games of StarCraft (R) : Brood War (R)(dagger). Finally, we demonstrate the relevance of our models to downstream tasks by applying them for enemy unit prediction in a state-of-the-art, rule-based StarCraft bot. We observe improvements in win rates against several strong community bots.	[Synnaeve, Gabriel; Lin, Zeming; Gant, Dan] Facebook, New York, NY 10003 USA; [Gehring, Jonas; Mella, Vegard; Khalidov, Vasil; Carion, Nicolas; Usunier, Nicolas] Facebook, Paris, France	Facebook Inc; Facebook Inc	Synnaeve, G (corresponding author), Facebook, New York, NY 10003 USA.	gab@fb.com; zlin@fb.com; jgehring@fb.com; danielgant@fb.com; vegardmella@fb.com; vkhalidov@fb.com; alcinos@fb.com; usunier@fb.com			Blizzard Entertainment, Inc.	Blizzard Entertainment, Inc.	StarCraft is a trademark or registered trademark of Blizzard Entertainment, Inc., in the U.S. and/or other countries. Nothing in this paper should be construed as approval, endorsement, or sponsorship by Blizzard Entertainment, Inc.	Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Foerster J, 2017, ARXIV170508926; Jozefowicz Rafal, 2016, ARXIV160202410; Justesen Niels, 2017, 2017 IEEE Conference on Computational Intelligence and Games (CIG), P162, DOI 10.1109/CIG.2017.8080430; Ke JT, 2017, TRANSPORT RES C-EMER, V85, P591, DOI 10.1016/j.trc.2017.10.016; KNESER R, 1995, INT CONF ACOUST SPEE, P181, DOI 10.1109/ICASSP.1995.479394; Lerer A, 2016, PR MACH LEARN RES, V48; Lin Z., 2017, AAAI C ART INT INT D; Luc P., 2017, ICCV 2017 INT C COMP, P10; Mathieu M., 2015, ARXIV151105440CSSTAT; Nardelli  Nantas, 2016, ARXIV161100625CS; Ontanon S, 2013, IEEE T COMP INTEL AI, V5, P293, DOI 10.1109/TCIAIG.2013.2286295; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Ranzato M., 2015, INT C LEARN REPR ICL; Shi X., 2015, ABS150604214 CORR; Shi  X., 2017, ADV NEURAL INFORM PR, P5622; Synnaeve G., 2012, 2012 IEEE Conference on Computational Intelligence and Games (CIG 2012), P409, DOI 10.1109/CIG.2012.6374184; Synnaeve G., 2012, THESIS GRENOBLE U; Uriarte Alberto, 2015, 11 ART INT INT DIG E; Usunier N., 2017, P INT C LEARN REPR I; Weber B. G., 2011, AIIDE; Yu HY, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17071501	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005033
C	Thewlis, J; Bilen, H; Vedaldi, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Thewlis, James; Bilen, Hakan; Vedaldi, Andrea			Modelling and unsupervised learning of symmetric deformable object categories	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SHAPE	We propose a new approach to model and learn, without manual supervision, the symmetries of natural objects, such as faces or flowers, given only images as input. It is well known that objects that have a symmetric structure do not usually result in symmetric images due to articulation and perspective effects. This is often tackled by seeking the intrinsic symmetries of the underlying 3D shape, which is very difficult to do when the latter cannot be recovered reliably from data. We show that, if only raw images are given, it is possible to look instead for symmetries in the space of object deformations. We can then learn symmetries from an unstructured collection of images of the object as an extension of the recently-introduced object frame representation, modified so that object symmetries reduce to the obvious symmetry groups in the normalized space. We also show that our formulation provides an explanation of the ambiguities that arise in recovering the pose of symmetric objects from their shape or images and we provide a way of discounting such ambiguities in learning.	[Thewlis, James; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England; [Bilen, Hakan] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Oxford; University of Edinburgh	Thewlis, J (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford, England.	jdt@robots.ox.ac.uk; hbilen@ed.ac.uk; vedaldi@robots.ox.ac.uk	Bilen, Hakan/AAG-3202-2022; Bilen, Hakan/ACY-3128-2022; Bilen, Hakan/H-9130-2016	Bilen, Hakan/0000-0002-6947-6918	AIMS CDT [EPSRC EP/L015897/1]; ERC [638009-IDIU]	AIMS CDT; ERC(European Research Council (ERC)European Commission)	This work acknowledges the support of the AIMS CDT (EPSRC EP/L015897/1) and ERC 638009-IDIU. We thank Almut Sophia Koepke for feedback and corrections.	ALT H, 1988, DISCRETE COMPUT GEOM, V3, P237, DOI 10.1007/BF02187910; Anselmi F, 2019, PATTERN RECOGN, V86, P201, DOI 10.1016/j.patcog.2018.07.025; Bagon S, 2008, LECT NOTES COMPUT SC, V5305, P30, DOI 10.1007/978-3-540-88693-8_3; Bilen H, 2014, P BMVC 2014, P1997; Boiman O., 2007, ADV NEURAL INFORM PR, V19, P177; Cootes T. F., 1995, CVIU; Coumans Erwin, 2010, BULLET PHYS ENGINE; Dalal N., 2005, P CVPR; Ecins A, 2016, IEEE INT CONF ROBOT, P2271, DOI 10.1109/ICRA.2016.7487376; Felzenszwalb P. F., 2010, PAMI; FERGUS R, 2003, P CVPR; Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507; Goslin M, 2004, COMPUTER, V37, P112, DOI 10.1109/MC.2004.180; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Han Kai, 2017, P ICCV; Jaderberg Max, 2015, P NEURIPS; Kanazawa A., 2016, P CVPR; Kemelmacher-Shlizerman Ira, 2012, P CVPR; Koffka K., 2013, PRINCIPLES GESTALT P, V44; Learned-Miller Erik G, 2006, IEEE T PATTERN ANAL; Leibe B, 2004, WORKSH STAT LEARN CO; Liu Z., 2015, P ICCV; Long J.L., 2014, P C NEUR INF PROC SY, V27, P1601; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu YX, 2009, FOUND TRENDS COMPUT, V5, P1, DOI 10.1561/0600000008; MAROLA G, 1989, IEEE T PATTERN ANAL, V11, P104, DOI 10.1109/34.23119; Mitra NJ, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239514, 10.1145/1276377.1276456]; Mobahi Hossein, 2014, P CVPR; Naber GL, 2012, APPL MATH SCI, V92, P1, DOI 10.1007/978-1-4419-7838-7; Novotny D., 2017, P ICCV; Parkhi O. M., 2012, P CVPR; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Raviv D, 2010, INT J COMPUT VISION, V89, P18, DOI 10.1007/s11263-010-0320-3; Rocco I., 2017, P CVPR; Shimshoni I, 2000, INT J COMPUT VISION, V39, P97, DOI 10.1023/A:1008118909580; Si Zhangzhang, PAMI; Sun CM, 1997, IEEE T PATTERN ANAL, V19, P164, DOI 10.1109/34.574800; Thewlis J., 2017, P NEURIPS; Thewlis J., 2017, P ICCV; Thrun S, 2005, IEEE I CONF COMP VIS, P1824; Vetter T, 1997, IEEE T PATTERN ANAL, V19, P733, DOI 10.1109/34.598230; WATSON JD, 1953, NATURE, V171, P737, DOI 10.1038/171737a0; Wilbur JD, 2010, DEV CELL, V18, P841, DOI 10.1016/j.devcel.2010.04.007; Yang H, 2015, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2015.7299100; Zhang W., 2008, P ECCV; Zhang Zhanpeng, 2016, PAMI; Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871	47	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002070
C	Tirinzoni, A; Chen, XL; Petrik, M; Ziebart, BD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tirinzoni, Andrea; Chen, Xiangli; Petrik, Marek; Ziebart, Brian D.			Policy-Conditioned Uncertainty Sets for Robust Markov Decision Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					What policy should be employed in a Markov decision process with uncertain parameters? Robust optimization's answer to this question is to use rectangular uncertainty sets, which independently reflect available knowledge about each state, and then to obtain a decision policy that maximizes the expected reward for the worst-case decision process parameters from these uncertainty sets. While this rectangularity is convenient computationally and leads to tractable solutions, it often produces policies that are too conservative in practice, and does not facilitate knowledge transfer between portions of the state space or across related decision processes. In this work, we propose non-rectangular uncertainty sets that bound marginal moments of state-action features defined over entire trajectories through a decision process. This enables generalization to different portions of the state space while retaining appropriate uncertainty of the decision process. We develop algorithms for solving the resulting robust decision problems, which reduce to finding an optimal policy for a mixture of decision processes, and demonstrate the benefits of our approach experimentally.	[Tirinzoni, Andrea] Politecn Milan, Milan, Italy; [Chen, Xiangli] Amazon Robot, North Reading, MA USA; [Petrik, Marek] Univ New Hampshire, Durham, NH 03824 USA; [Ziebart, Brian D.] Univ Illinois, Chicago, IL USA	Polytechnic University of Milan; University System Of New Hampshire; University of New Hampshire; University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Tirinzoni, A (corresponding author), Politecn Milan, Milan, Italy.	andrea.tirinzoni@polimi.it; cxiangli@amazon.com; mpetrik@cs.unh.edu; bziebart@uic.edu		Ziebart, Brian/0000-0003-4041-6871	National Science Foundation [1652530, 1717368]; Future of Life Institute FLI-RFP-AI1 program	National Science Foundation(National Science Foundation (NSF)); Future of Life Institute FLI-RFP-AI1 program	We thank the anonymous reviewers whose comments helped to improve the paper significantly. This work was supported, in part, by the National Science Foundation under Grant No. 1652530 and Grant No. 1717368, and by the Future of Life Institute (futureoflife.org) FLI-RFP-AI1 program.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Analui B, 2014, COMPUT MANAG SCI, V11, P197, DOI 10.1007/s10287-014-0213-y; [Anonymous], 1990, P 1990 INT S INF THE; Bagnell J. A., 2004, THESIS; Bagnell J. D., 2001, CMURITR0125; Boyd S, 2004, CONVEX OPTIMIZATION; Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741; Dudik M, 2007, J MACH LEARN RES, V8, P1217; Ernst D, 2005, J MACH LEARN RES, V6, P503; Hu L. J., 2013, AVAILABLE OPTIM ONLI, P1695; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Kazama J, 2003, PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P137; Kery M, 2012, BAYESIAN POPULATION ANALYSIS USING WINBUGS: A HIERARCHICAL PERSPECTIVE, P1; Kramer G, 2003, IEEE T INFORM THEORY, V49, P4, DOI 10.1109/TIT.2002.806135; Kramer G., 1998, THESIS SWISS FEDERAL; Le Tallec Y, 2007, THESIS; Mannor Shie, 2012, P 29 INT C MACH LEAR; MARKO H, 1973, IEEE T COMMUN, VCO21, P1345, DOI 10.1109/TCOM.1973.1091610; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Permuter HH, 2008, IEEE INT SYMP INFO, P1403, DOI 10.1109/ISIT.2008.4595218; Petrik M., 2014, NEURAL INFORM PROCES; Petrik M., 2016, ADV NEURAL INFORM PR; Philpott AB, 2012, EUR J OPER RES, V218, P470, DOI 10.1016/j.ejor.2011.10.056; Puterman M. L., 1994, MARKOV DECISION PROC; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tatikonda S. C., 2000, THESIS; Wiesemann W, 2014, OPER RES, V62, P1358, DOI 10.1287/opre.2014.1314; Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566; Xu Huan, 2010, ADV NEURAL INFORM PR, P2505; Ziebart B. D., 2010, P 27 INT C INT C MAC, P1255; Ziebart B. D., 2008, AAAI, V8, P1433; Ziebart B. D., 2010, THESIS	33	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003049
C	Tukuljac, HP; Deleforge, A; Gribonval, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tukuljac, Helena Peic; Deleforge, Antoine; Gribonval, Remi			MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				IDENTIFICATION	This paper addresses the general problem of blind echo retrieval, i.e., given M sensors measuring in the discrete-time domain M mixtures of K delayed and attenuated copies of an unknown source signal, can the echo locations and weights be recovered? This problem has broad applications in fields such as sonars, seismology, ultrasounds or room acoustics. It belongs to the broader class of blind channel identification problems, which have been intensively studied in signal processing. Existing methods in the literature proceed in two steps: (i) blind estimation of sparse discrete-time filters and (ii) echo information retrieval by peak-picking on filters. The precision of these methods is fundamentally limited by the rate at which the signals are sampled: estimated echo locations are necessary on-grid, and since true locations never match the sampling grid, the weight estimation precision is impacted. This is the so-called basis-mismatch problem in compressed sensing. We propose a radically different approach to the problem, building on the framework of finite-rate-of-innovation sampling. The approach operates directly in the parameter-space of echo locations and weights, and enables near-exact blind and off-grid echo retrieval from discrete-time measurements. It is shown to outperform conventional methods by several orders of magnitude in precision.	[Tukuljac, Helena Peic] Ecole Polytech Fed Lausanne, Dept Comp & Commun Sci, Lausanne, Switzerland; [Deleforge, Antoine] Univ Lorraine, CNRS, INRIA, LORIA, F-54000 Nancy, France; [Gribonval, Remi] Univ Rennes, CNRS, INRIA, IRISA, F-35000 Rennes, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Lorraine; Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Rennes	Tukuljac, HP (corresponding author), Ecole Polytech Fed Lausanne, Dept Comp & Commun Sci, Lausanne, Switzerland.	helena.peictukuljac@epfl.ch; antoine.deleforge@inria.fr; remi.gribonval@inria.fr						AbedMeraim K, 1997, IEEE T INFORM THEORY, V43, P499, DOI 10.1109/18.556108; Achim A, 2010, IEEE ENG MED BIO, P4304, DOI 10.1109/IEMBS.2010.5626210; Aissa-El-Bey A, 2008, 2008 IEEE 9TH WORKSHOP ON SIGNAL PROCESSING ADVANCES IN WIRELESS COMMUNICATIONS, VOLS 1 AND 2, P271, DOI 10.1109/SPAWC.2008.4641612; ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599; Antonello N, 2014, 2014 14TH INTERNATIONAL WORKSHOP ON ACOUSTIC SIGNAL ENHANCEMENT (IWAENC), P114, DOI 10.1109/IWAENC.2014.6953349; Bertin N, 2016, INT CONF ACOUST SPEE, P6340, DOI 10.1109/ICASSP.2016.7472897; Blu T, 2008, IEEE SIGNAL PROC MAG, V25, P31, DOI 10.1109/MSP.2007.914998; Candes EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731; Chi YJ, 2016, IEEE J-STSP, V10, P782, DOI 10.1109/JSTSP.2016.2543462; Chi YJ, 2011, IEEE T SIGNAL PROCES, V59, P2182, DOI 10.1109/TSP.2011.2112650; Choudhary S, 2018, IEEE T SIGNAL PROCES, V66, P3696, DOI 10.1109/TSP.2018.2815014; Crocco M, 2016, INT CONF ACOUST SPEE, P3201, DOI 10.1109/ICASSP.2016.7472268; Crocco M, 2014, EUR SIGNAL PR CONF, P910; Dokmanic I, 2015, IEEE J-STSP, V9, P825, DOI 10.1109/JSTSP.2015.2415761; Dokmanic I, 2013, P NATL ACAD SCI USA, V110, P12186, DOI 10.1073/pnas.1221464110; El Kharroubi A., 2010, SIGN PROC ADV WIR CO, P1; Garofolo J. S., 1993, SPACE TERR INTEGR NE, V93, P27; Hua YB, 1996, IEEE T SIGNAL PROCES, V44, P661, DOI 10.1109/78.489039; Kleeman L, 2016, SPRINGER HANDBOOK OF ROBOTICS, P753; Kowalczyk K, 2013, IEEE SIGNAL PROC LET, V20, P653, DOI 10.1109/LSP.2013.2261059; Lee K, 2018, IEEE T INFORM THEORY, V64, P4792, DOI 10.1109/TIT.2018.2840711; Li XF, 2018, IEEE-ACM T AUDIO SPE, V26, P1755, DOI 10.1109/TASLP.2018.2839362; Lin Y., 2008, ADV NEURAL INFORM PR, V20, P921; PRICE R, 1958, P IRE, V46, P555, DOI 10.1109/JRPROC.1958.286870; Sato H., 2012, SEISMIC WAVE PROPAGA, V496; Scheibler R., 2018, IEEE INT C AC SPEECH; Scheibler Robin, 2017, CORR; Stoica P, 1997, SPECTRAL ANAL SIGNAL, V1st; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; van den Boomgaard R, 2001, LECT NOTES COMPUT SC, V2106, P205; Vetterli M, 2002, IEEE T SIGNAL PROCES, V50, P1417, DOI 10.1109/TSP.2002.1003065; Xu GH, 1995, IEEE T SIGNAL PROCES, V43, P2982, DOI 10.1109/78.476442; Ye JC, 2017, IEEE T INFORM THEORY, V63, P777, DOI 10.1109/TIT.2016.2629078	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302021
C	Udwani, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Udwani, Rajan			Multi-objective Maximization of Monotone Submodular Functions with Cardinality Constraint	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTICOMMODITY FLOW; FRACTIONAL PACKING; APPROXIMABILITY; APPROXIMATION; ALGORITHMS	We consider the problem of multi-objective maximization of monotone submodular functions subject to cardinality constraint, often formulated as max(vertical bar A vertical bar=k) min(i is an element of{1,...,m} )f(i) (A). While it is widely known that greedy methods work well for a single objective, the problem becomes much harder with multiple objectives. In fact, Krause et al. (2008) showed that when the number of objectives m grows as the cardinality k i.e., m = Omega (k), the problem is inapproximable (unless P = NP). On the other hand, when m is constant Chekuri et al. (2010) showed a randomized (1 -1/e) - epsilon approximation with runtime (number of queries to function oracle) n(m/epsilon 3). We focus on finding a fast and practical algorithm that has (asymptotic) approximation guarantees even when m is super constant. We first modify the algorithm of Chekuri et al. (2010) to achieve a (1 - 1/e) - epsilon approximation for m = o(k/log(3) k), with epsilon -> 0 as k -> infinity. This demonstrates a steep transition from constant factor approximability to inapproximability around m = Omega (k). Then using Multiplicative-Weight-Updates (MWU), we find a much faster ((O) over tilden/(n/delta(3)) time asymptotic (1 - 1/e)(2) - delta approximation. While the above results are all randomized, we also give a simple deterministic (1 - 1/e) - epsilon approximation with runtime kn(m/epsilon 4). Finally, we run synthetic experiments using Kronecker graphs and find that our MWU inspired heuristic outperforms existing heuristics.	[Udwani, Rajan] MIT, Ctr Operat Res, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Udwani, R (corresponding author), MIT, Ctr Operat Res, Cambridge, MA 02139 USA.	rudwani@alum.mit.edu			ONR [N00014-17-1-2194]	ONR(Office of Naval Research)	The author gratefully acknowledges partial support from ONR Grant N00014-17-1-2194. The author would also like to thank James B. Orlin and anonymous referees for their insightful comments and feedback on early drafts of this work.	Arora S, 2007, ACM S THEORY COMPUT, P227, DOI 10.1145/1250790.1250823; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Azar Y, 2012, LECT NOTES COMPUT SC, V7391, P38, DOI 10.1007/978-3-642-31594-7_4; Badanidiyuru A., 2014, P 25 ANN ACM SIAM S, P1497; Bienstock D., 2006, POTENTIAL FUNCTION M, V53; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Chekuri C., 2015, P 2015 C INN THEOR C, P201; Chekuri C, 2011, ACM S THEORY COMPUT, P783; Chekuri C, 2010, ANN IEEE SYMP FOUND, P575, DOI 10.1109/FOCS.2010.60; Dobzinski S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1107; El-Arini K, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P289; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46; Filmus Y, 2014, SIAM J COMPUT, V43, P514, DOI 10.1137/130920277; Garg N, 2004, LECT NOTES COMPUT SC, V3221, P371; Garg N, 2007, SIAM J COMPUT, V37, P630, DOI 10.1137/S0097539704446232; Globerson Amir, 2006, P 23 INT C MACH LEAR, P353, DOI DOI 10.1145/1143844.1143889; GRIGORIADIS MD, 1994, SIAM J OPTIMIZ, V4, P86, DOI 10.1137/0804004; Guestrin C., 2005, P 22 INT C MACH LEAR, P265, DOI DOI 10.1145/1102351.1102385; He XR, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P885, DOI 10.1145/2939672.2939760; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Koufogiannakis C, 2007, ANN IEEE SYMP FOUND, P494, DOI 10.1109/FOCS.2007.62; Krause A., 2005, P 21 C UNCERTAINTY A, P324, DOI DOI 10.5555/3020336.3020377; Krause A, 2008, J WATER RES PL-ASCE, V134, P516, DOI 10.1061/(ASCE)0733-9496(2008)134:6(516); Krause A, 2006, IPSN 2006: THE FIFTH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P2; Krause A, 2008, J MACH LEARN RES, V9, P2761; Leskovec J, 2010, J MACH LEARN RES, V11, P985; Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Mirzasoleiman B., 2015, AAAI; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Orlin JB, 2018, MATH PROGRAM, V172, P505, DOI 10.1007/s10107-018-1320-2; Ostfeld A, 2008, J WATER RESOURCES PL; Papadimitriou CH, 2000, ANN IEEE SYMP FOUND, P86; PLOTKIN SA, 1991, PROCEEDINGS - 32ND ANNUAL SYMPOSIUM ON FOUNDATIONS OF COMPUTER SCIENCE, P495, DOI 10.1109/SFCS.1991.185411; Singer Yaron, 2017, ADV NEURAL INFORM PR, P4705; Singla A, 2014, PR MACH LEARN RES, V32, P154; Thoma M., 2009, P SIAM INT C DAT MIN, P1076; Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318; Vondrak J, 2008, ACM S THEORY COMPUT, P67; YOUNG NE, 1995, PROCEEDINGS OF THE SIXTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P170; Young NE, 2001, ANN IEEE SYMP FOUND, P538, DOI 10.1109/SFCS.2001.959930	44	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004009
C	Vankadara, LC; von Luxburg, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Vankadara, Leena Chennuru; von Luxburg, Ulrike			Measures of distortion for machine learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply machine learning algorithms to analyze the data. The quality of such an embedding is typically described in terms of a distortion measure. In this paper, we show that many of the existing distortion measures behave in an undesired way, when considered from a machine learning point of view. We investigate desirable properties of distortion measures and formally prove that most of the existing measures fail to satisfy these properties. These theoretical findings are supported by simulations, which for example demonstrate that existing distortion measures are not robust to noise or outliers and cannot serve as good indicators for classification accuracy. As an alternative, we suggest a new measure of distortion, called sigma-distortion. We can show both in theory and in experiments that it satisfies all desirable properties and is a better candidate to evaluate distortion in the context of machine learning.	[Vankadara, Leena Chennuru; von Luxburg, Ulrike] Univ Tubingen, Max Planck Inst Intelligent Syst, Tubingen, Germany	Eberhard Karls University of Tubingen; Max Planck Society	Vankadara, LC (corresponding author), Univ Tubingen, Max Planck Inst Intelligent Syst, Tubingen, Germany.	leena.chennuru@tuebingen.mpg.de; luxburg@informatik.uni-tuebingen.de			Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, DFG) [ZUK 63]; International Max Planck Research School for Intelligent Systems (IMPRS-IS)	Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, DFG)(German Research Foundation (DFG)); International Max Planck Research School for Intelligent Systems (IMPRS-IS)	This work has been supported by the Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, DFG, ZUK 63) and the International Max Planck Research School for Intelligent Systems (IMPRS-IS).	Abraham I, 2005, ANN IEEE SYMP FOUND, P83, DOI 10.1109/SFCS.2005.51; Abraham I, 2011, ADV MATH, V228, P3026, DOI 10.1016/j.aim.2011.08.003; Abraham I, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P875; Abraham I, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P363; Abraham I, 2007, ACM S THEORY COMPUT, P631, DOI 10.1145/1250790.1250883; Bartal Y, 2015, SIAM J DISCRETE MATH, V29, P1207, DOI 10.1137/140977655; Chan THH, 2010, J ACM, V57, DOI 10.1145/1734213.1734215; Cox T., 2000, MULTIDIMENSIONAL SCA; Gupta A, 2003, ANN IEEE SYMP FOUND, P534, DOI 10.1109/SFCS.2003.1238226; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Semmes S, 1996, REV MAT IBEROAM, V12, P337; Semmes S., 1999, PUBL MAT, V43, P571; Shaw B, 2009, INT C MACH LEARN, P937, DOI DOI 10.1145/1553374.1553494; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Weinberger K. Q., 2006, AAAI, V6, P1683	20	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304086
C	Wang, F; Decker, J; Wu, XL; Essertel, G; Rompf, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Fei; Decker, James; Wu, Xilun; Essertel, Gregory; Rompf, Tiark			Backpropagation with Continuation Callbacks: Foundations for Efficient and Expressive Differentiable Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Training of deep learning models depends on gradient descent and end-to-end differentiation. Under the slogan of differentiable programming, there is an increasing demand for efficient automatic gradient computation for emerging network architectures that incorporate dynamic control flow, especially in NLP. In this paper we propose an implementation of backpropagation using functions with callbacks, where the forward pass is executed as a sequence of function calls, and the backward pass as a corresponding sequence of function returns. A key realization is that this technique of chaining callbacks is well known in the programming languages community as continuation-passing style (CPS). Any program can be converted to this form using standard techniques, and hence, any program can be mechanically converted to compute gradients. Our approach achieves the same flexibility as other reverse-mode automatic differentiation (AD) techniques, but it can be implemented without any auxiliary data structures besides the function call stack, and it can easily be combined with graph construction and native code generation techniques through forms of multi-stage programming, leading to a highly efficient implementation that combines the performance benefits of define-then-run software frameworks such as TensorFlow with the expressiveness of define-by-run frameworks such as PyTorch.	[Wang, Fei; Decker, James; Wu, Xilun; Essertel, Gregory; Rompf, Tiark] Purdue Univ, W Lafayette, IN 47906 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Wang, F (corresponding author), Purdue Univ, W Lafayette, IN 47906 USA.	wang603@purdue.edu; decker31@purdue.edu; wu636@purdue.edu; gesserte@purdue.edu; tiark@purdue.edu			NSF [1553471, 1564207]; DOE [DE-SC0018050]; Google Faculty Research Award	NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE)); Google Faculty Research Award(Google Incorporated)	This work was supported in part by NSF awards 1553471 and 1564207, DOE award DE-SC0018050, and a Google Faculty Research Award.	Abadi M, 2015, P 12 USENIX S OPERAT; Ackermann Stefan, 2012, JET EMBEDDED DSL HIG; Amin Nada, 2018, PACMPL, V2; Appel Andrew, 1992, COMPILING CONTINUATI; Baydin Atilim Gunes, 2016, ABS161103423 CORR; Breuleux Olivier, 2017, NIPS AUTODIFF WORKSH; Chuang Jason, 2013, STANFORD SENTIMENT T; Clinger W. D., 1999, Higher-Order and Symbolic Computation, V12, P7, DOI 10.1023/A:1010016816429; Danvy O, 2005, FUND INFORM, V66, P397; DANVY O, 1990, PROCEEDINGS OF THE 1990 ACM CONFERENCE ON LISP AND FUNCTIONAL PROGRAMMING, P151, DOI 10.1145/91556.91622; Distributed (Deep) Machine Learning Community, 2018, NNVM OP COMP AI FRAM; Elliott Conal, 2018, PACMPL, V2; Elliott CM, 2009, ICFP'09: PROCEEDINGS OF THE 2009 ACM SIGPLAN INTERNATIONAL CONFERENCE ON FUNCTIONAL PROGRAMMING, P191; Essertel GM, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P799; FELLEISEN M, 1988, P 15 ANN ACM S PRINC, P180; Fischer M. J., 1993, LISP and Symbolic Computation, V6, P259, DOI 10.1007/BF01019461; Fischer Michael J., 1972, P ACM C PROV ASS PRO; JORRING U, 1986, 13TH ANN ACM S PRINC, P86, DOI DOI 10.1145/512644.512652; Karczmarczuk J., 2001, Higher-Order and Symbolic Computation, V14, P35, DOI 10.1023/A:1011501232197; KOSSAKOWSKI G, 2012, ECOOP, V7313, P409; LeCun Yann, 2018, DEEP LEARNING EST MO; Looks M., 2017, ICLR; Moldovan Dan, 2018, ARXIVCSMS181008061; Neubig Graham, 2017, NIPS, P3974; Ofenbeck G, 2017, PROCEEDINGS OF THE 16TH ACM SIGPLAN INTERNATIONAL CONFERENCE ON GENERATIVE PROGRAMMING: CONCEPTS AND EXPERIENCES (GPCE'17), P15, DOI 10.1145/3136040.3136060; Ofenbeck G, 2014, ACM SIGPLAN NOTICES, V49, P125, DOI [10.1145/2637365.2517228, 10.1145/2517208.2517228]; Olah C., 2015, NEURAL NETWORKS TYPE; Paszke Adam, 2017, PYTORCH TENSORS DYNA, P6; Pearlmutter BA, 2008, ACM T PROGR LANG SYS, V30, DOI 10.1145/1330017.1330018; Reynolds J. C., 1993, LISP and Symbolic Computation, V6, P233, DOI 10.1007/BF01019459; Rompf Tiark, 2016, A List of Successes that can Change the World. Essays Dedicated to Philip Wadler on the Occasion of his 60th Birthday. LNCS 9600, P318, DOI 10.1007/978-3-319-30936-1_17; Rompf T., 2012, THESIS; Rompf T, 2011, ACM SIGPLAN NOTICES, V46, P127, DOI 10.1145/1942788.1868314; Rompf T, 2016, SCALA'16: PROCEEDINGS OF THE 2016 7TH ACM SIGPLAN SYMPOSIUM ON SCALA, P41, DOI 10.1145/2998392.2998399; Rompf T, 2015, PROCEEDINGS OF THE 20TH ACM SIGPLAN INTERNATIONAL CONFERENCE ON FUNCTIONAL PROGRAMMING (ICFP'15), P2, DOI 10.1145/2784731.2784760; Rompf T, 2012, COMMUN ACM, V55, P121, DOI 10.1145/2184319.2184345; Rompf T, 2009, ICFP'09: PROCEEDINGS OF THE 2009 ACM SIGPLAN INTERNATIONAL CONFERENCE ON FUNCTIONAL PROGRAMMING, P317; Rompf Tiark, 2013, OPTIMIZING DATA STRU; Rompf Tiark, 2012, HIGH ORDER SYMB COMP, V25, P165; Rompf Tiark, 2015, SNAPL LIPICS, V32, P238, DOI 10.4230/LIPIcs.SNAPL.2015.238; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Siskind Jeffrey Mark, 2008, Higher-Order and Symbolic Computation, V21, P361, DOI 10.1007/s10990-008-9037-1; Speelpenning B., 1980, THESIS; Stojanov A, 2018, INT SYM CODE GENER, P2, DOI 10.1145/3168810; Sujeeth A., 2011, P ANN IEEE ACM INT S, P609; Sujeeth AK, 2014, ACM T EMBED COMPUT S, V13, DOI 10.1145/2584665; Taha W, 2000, THEOR COMPUT SCI, V248, P211, DOI 10.1016/S0304-3975(00)00053-0; Tahboub RY, 2018, INT CONF MANAGE DATA, P307, DOI 10.1145/3183713.3196893; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; TensorFlow team, 2018, XLA OV; van Merrienboer Bart, 2018, NIPS; VANWIJNGAARDEN A, 1966, FORMAL LANGUAGE DESC, P13; Wang F., 2018, ICLR WORKSH TRACK; WENGERT RE, 1964, COMMUN ACM, V7, P463, DOI 10.1145/355586.364791	54	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004071
C	Wang, ST; Li, D; Cheng, Y; Geng, JK; Wang, YS; Wang, S; Xia, ST; Wu, JP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Songtao; Li, Dan; Cheng, Yang; Geng, Jinkun; Wang, Yanshu; Wang, Shuai; Xia, Shutao; Wu, Jianping			BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In distributed machine learning (DML), the network performance between machines significantly impacts the speed of iterative training. In this paper we propose BML, a new gradient synchronization algorithm with higher network performance and lower network cost than the current practice. BML runs on BCube network, instead of using the traditional Fat-Tree topology. BML algorithm is designed in such a way that, compared to the parameter server (PS) algorithm on a Fat-Tree network connecting the same number of server machines, BML achieves theoretically 1/k of the gradient synchronization time, with k/5 of switches (the typical number of k is 2 similar to 4). Experiments of LeNet-5 and VGG-19 benchmarks on a testbed with 9 dual-GPU servers show that, BML reduces the job completion time of DML training by up to 56.4%.	[Wang, Songtao; Li, Dan; Cheng, Yang; Geng, Jinkun; Wang, Yanshu; Wang, Shuai; Xia, Shutao; Wu, Jianping] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China; [Wang, Songtao; Xia, Shutao] Tsinghua Univ, Grad Sch Shenzhen, Beijing, Peoples R China	Tsinghua University; Tsinghua University; University Town of Shenzhen; Tsinghua Shenzhen International Graduate School	Wang, ST (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.; Wang, ST (corresponding author), Tsinghua Univ, Grad Sch Shenzhen, Beijing, Peoples R China.		Geng, Jinkun/ABC-6089-2021		National Key Basic Research Program of China (973 program) [2014CB347800]; National Natural Science Foundation of China [61522205, 61772305, 61432002, 61771273]	National Key Basic Research Program of China (973 program)(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The work was supported by the National Key Basic Research Program of China (973 program) under Grant 2014CB347800, and the National Natural Science Foundation of China under Grant No. 61522205, No. 61772305, No. 61432002, No. 61771273. Dan Li is the corresponding author of this paper.	Abadi M., 2016, USENIX OSDI 16; Al-Fares M., 2008, ACM SIGCOMM 08; [Anonymous], 2015, ARXIV151201274; Cheng H.-T., 2016, DLRS 16; Couto R. D. S., 2015, CORR; Dai W, 2015, AAAI CONF ARTIF INTE, P79; Dean J., NIPS 12; Guo C., 2009, ACM SIGCOMM 09; Guo C., 2008, ACM SIGCOMM 08; Hazelwood K., 2018, IEEE HPCA 18; Kyrola A., 2017, ABS170602677 ARXIV; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li D, 2011, IEEE ACM T NETWORK, V19, P102, DOI 10.1109/TNET.2010.2053718; Li M., 2014, USENIX OSDI 14; MCDONALD R, 2010, HUMAN LANGUAGE TECHN, V2010, P456; Patarasuk P, 2009, J PARALLEL DISTR COM, V69, P117, DOI 10.1016/j.jpdc.2008.09.002; Rumelhart D. E., 1988, NEUROCOMPUTING FDN R, P696; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Smola A., 2017, SPEECH AI WORLD 2017; You Y., 2017, ARXIV170905011; Zhang H., 2017, USENIX ATC 17; Zhao H., SIAM SDM 13, P785	23	1	1	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304027
C	Wang, WB; Qiao, XY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Wenbo; Qiao, Xingye			Learning Confidence Sets using Support Vector Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEYMAN-PEARSON CLASSIFICATION; CONVEXITY; KERNEL	The goal of confidence-set learning in the binary classification setting [14] is to construct two sets, each with a specific probability guarantee to cover a class. An observation outside the overlap of the two sets is deemed to be from one of the two classes, while the overlap is an ambiguity region which could belong to either class. Instead of plug-in approaches, we propose a support vector classifier to construct confidence sets in a flexible manner. Theoretically, we show that the proposed learner can control the non-coverage rates and minimize the ambiguity with high probability. Efficient algorithms are developed and numerical studies illustrate the effectiveness of the proposed method.	[Wang, Wenbo; Qiao, Xingye] SUNY Binghamton, Dept Math Sci, Binghamton, NY 13902 USA	State University of New York (SUNY) System; State University of New York (SUNY) Binghamton	Qiao, XY (corresponding author), SUNY Binghamton, Dept Math Sci, Binghamton, NY 13902 USA.	wang2@math.binghamton.edu; qiao@math.binghamton.edu	Qiao, Xingye/P-6321-2019	Qiao, Xingye/0000-0003-0937-9822				ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bengio Y., 2009, ADV NEURAL INFORM PR, V21; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Denis Christophe, 2016, ARXIV160808783; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Furnkranz J, 2010, PREFERENCE LEARNING, P1, DOI 10.1007/978-3-642-14125-6_1; Herbei R, 2006, CAN J STAT, V34, P709, DOI 10.1002/cjs.5550340410; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Liu YF, 2011, J AM STAT ASSOC, V106, P166, DOI 10.1198/jasa.2011.tm10319; Platt J.C, 1999, ADV LARGE MARGIN CLA, P61; Rigollet P, 2011, J MACH LEARN RES, V12, P2831; Sadinle Mauricio, 2017, J AM STAT ASS; Shafer G, 2008, J MACH LEARN RES, V9, P371; Tong X, 2016, WIRES COMPUT STAT, V8, P64, DOI 10.1002/wics.1376; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vovk Vladimir, 2017, ANN MATH ARTIFICIAL, P1; Wang JH, 2008, BIOMETRIKA, V95, P149, DOI 10.1093/biomet/asm077; Wu YC, 2013, J COMPUT GRAPH STAT, V22, P416, DOI 10.1080/10618600.2012.680866; Wu YC, 2010, J AM STAT ASSOC, V105, P424, DOI 10.1198/jasa.2010.tm09107; Zhang C, 2013, J MACH LEARN RES, V14, P1349; Zhang Chong, 2017, J AM STAT ASS; Zhang HH, 2008, ELECTRON J STAT, V2, P149, DOI 10.1214/08-EJS122; Zhang T, 2004, ANN STAT, V32, P56; Zhu J, 2005, J COMPUT GRAPH STAT, V14, P185, DOI 10.1198/106186005X25619	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304090
C	Welleck, S; Yao, ZX; Gai, Y; Mao, JL; Zhang, Z; Cho, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Welleck, Sean; Yao, Zixin; Gai, Yu; Mao, Jialin; Zhang, Zheng; Cho, Kyunghyun			Loss Functions for Multiset Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.	[Welleck, Sean; Yao, Zixin; Gai, Yu; Mao, Jialin; Zhang, Zheng] New York Univ Shanghai, Shanghai, Peoples R China; [Welleck, Sean; Cho, Kyunghyun] NYU, New York, NY 10003 USA; [Cho, Kyunghyun] CIFAR Azrieli Global Scholar, Toronto, ON, Canada	NYU Shanghai; New York University	Welleck, S (corresponding author), New York Univ Shanghai, Shanghai, Peoples R China.; Welleck, S (corresponding author), NYU, New York, NY 10003 USA.	wellecks@nyu.edu; zixin.yao@nyu.edu; yg1246@nyu.edu; jialin.mao@nyu.edu; zz@nyu.edu; kyunghyun.cho@nyu.edu			eBay; TenCent; NVIDIA; CIFAR; Samsung Electronics (Improving Deep Learning using Latent Structure); STCSM [17JC1404101]	eBay; TenCent; NVIDIA; CIFAR(Canadian Institute for Advanced Research (CIFAR)); Samsung Electronics (Improving Deep Learning using Latent Structure)(Samsung); STCSM(Science & Technology Commission of Shanghai Municipality (STCSM))	KC thanks support by eBay, TenCent, NVIDIA and CIFAR. This work was supported by Samsung Electronics (Improving Deep Learning using Latent Structure) and 17JC1404101 STCSM.	[Anonymous], 2013, ARXIV13124894; Chang KW, 2015, PR MACH LEARN RES, V37, P2058; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Dembczynski K., 2010, P 27 INT C MACH LEAR, P279; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ehrenfeld W, 2011, J PHYS CONF SER, V331, DOI 10.1088/1742-6596/331/3/032007; He K., 2017, ARXIV170306870, P2980, DOI [10.1109/ICCV.2017.322, DOI 10.1109/ICCV.2017.322]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Leblond Remi, 2017, SEARNN TRAINING RNNS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lempitsky V., 2010, NIPS, V23, P1324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Nam J, 2017, ADV NEUR IN, V30; Onoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5; Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39; Rezatofighi SH, 2017, IEEE I CONF COMP VIS, P5257, DOI 10.1109/ICCV.2017.561; Romera-Paredes Bernardino, 2015, RECURRENT INSTANCE S; Ross St<prime>ephane, 2011, AISTATS; Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255; Tsoumakas G., 2007, INT J DATA WAREHOUSI, V3, P1; Vinyals Oriol, 2015, ARXIV151106391; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Welleck Sean, 2017, ADV NEURAL INFORM PR; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000030
C	Wimalawarne, K; Mamitsuka, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wimalawarne, Kishan; Mamitsuka, Hiroshi			Efficient Convex Completion of Coupled Tensors using Coupled Nuclear Norms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Coupled norms have emerged as a convex method to solve coupled tensor completion. A limitation with coupled norms is that they only induce low-rankness using the multilinear rank of coupled tensors. In this paper, we introduce a new set of coupled norms known as coupled nuclear norms by constraining the CP rank of coupled tensors. We propose new coupled completion models using the coupled nuclear norms as regularizers, which can be optimized using computationally efficient optimization methods. We derive excess risk bounds for proposed coupled completion models and show that proposed norms lead to better performance. Through simulation and real-data experiments, we demonstrate that proposed norms achieve better performance for coupled completion compared to existing coupled norms.	[Wimalawarne, Kishan; Mamitsuka, Hiroshi] Kyoto Univ, Bioinformat Ctr, Kyoto, Japan; [Mamitsuka, Hiroshi] Aalto Univ, Dept Comp Sci, Espoo, Finland	Kyoto University; Aalto University	Wimalawarne, K (corresponding author), Kyoto Univ, Bioinformat Ctr, Kyoto, Japan.	kishanwn@gmail.com; mami@kuicr.kyoto-u.ac.jp	Mamitsuka, Hiroshi/R-1110-2016	Mamitsuka, Hiroshi/0000-0002-6607-5617	MEXT KAKENHI [16H02868]; Business Finland; AIPSE Academy of Finland; ACCEL JST [JPMJAC1503]	MEXT KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Business Finland; AIPSE Academy of Finland(Academy of Finland); ACCEL JST	This work has been partially supported by MEXT KAKENHI Grant Number 16H02868, Grant Number JPMJAC1503 ACCEL JST, FiDiPro Tekes (currently Business Finland) and AIPSE Academy of Finland.	Acar E, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-239; Anandkumar A., 2017, J MACHINE LEARNING R, V18; [Anonymous], 2001, MATH SURVEYS MONOGRA; Bouchard G, 2013, AISTATS, V31, P144; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; El-Yaniv R, 2007, LECT NOTES COMPUT SC, V4539, P157, DOI 10.1007/978-3-540-72927-3_13; Ermis B, 2015, DATA MIN KNOWL DISC, V29, P203, DOI 10.1007/s10618-013-0341-y; Harshman R.A., 1970, MULTIMODAL FACTOR AN; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Li C, 2015, AAAI CONF ARTIF INTE, P2743; Lim LH, 2014, IEEE T INFORM THEORY, V60, P1260, DOI 10.1109/TIT.2013.2291876; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Nguyen NH, 2015, INF INFERENCE, V4, P195, DOI 10.1093/imaiai/iav004; Shamir O, 2014, J MACH LEARN RES, V15, P3401; Sorber L, 2015, IEEE J-STSP, V9, P586, DOI 10.1109/JSTSP.2015.2400415; Tomioka R., 2013, NIPS; Vershynin R, 2011, PROBAB THEORY REL, V150, P471, DOI 10.1007/s00440-010-0281-z; Wimalawarne K., 2014, NIPS; Wimalawarne K, 2018, NEURAL COMPUT, V30, P3095, DOI 10.1162/neco_a_01123; Yang YN, 2015, IEEE SIGNAL PROC LET, V22, P1633, DOI 10.1109/LSP.2015.2420592; Yuan M, 2016, FOUND COMPUT MATH, V16, P1031, DOI 10.1007/s10208-015-9269-5; Zheng Vincent W, 2010, AAAI	24	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001044
C	Yang, LF; Arora, R; Braverman, V; Zhao, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yang, Lin F.; Arora, Raman; Braverman, Vladimir; Zhao, Tuo			The Physical Systems Behind Optimization Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We use differential equations based approaches to provide some physics insights into analyzing the dynamics of popular optimization algorithms in machine learning. In particular, we study gradient descent, proximal gradient descent, coordinate gradient descent, proximal coordinate gradient, and Newton's methods as well as their Nesterov's accelerated variants in a unified framework motivated by a natural connection of optimization algorithms to physical systems. Our analysis is applicable to more general algorithms and optimization problems beyond convexity and strong convexity, e.g. Polyak-Lojasiewicz and error bound conditions (possibly nonconvex).	[Yang, Lin F.] Princeton Univ, Princeton, NJ 08544 USA; [Yang, Lin F.; Arora, Raman; Braverman, Vladimir] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Zhao, Tuo] Georgia Inst Technol, Atlanta, GA 30332 USA	Princeton University; Johns Hopkins University; University System of Georgia; Georgia Institute of Technology	Zhao, T (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	lin.yang@princeton.edu; arora@cs.jhu.edu; vova@cs.jhu.edu; tourzhao@gatech.edu			National Science Foundation [1546482, 1447639, 1650041, 1652257]; ONR Award [N00014-18-1-2364]; Israel Science Foundation [897/13]; Minerva Foundation; DARPA award [W911NF1820267]	National Science Foundation(National Science Foundation (NSF)); ONR Award; Israel Science Foundation(Israel Science Foundation); Minerva Foundation; DARPA award	Work was done while the author was at Johns Hopkins University. This work is partially supported by the National Science Foundation under grant numbers 1546482, 1447639, 1650041 and 1652257, the ONR Award N00014-18-1-2364, the Israel Science Foundation grant #897/13, a Minerva Foundation grant, and by DARPA award W911NF1820267.	Ethier S. N., 2009, MARKOV PROCESSES CHA, V282; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Gong P., 2014, ARXIV PREPRINT ARXIV; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Lu ZS, 2015, MATH PROGRAM, V152, P615, DOI 10.1007/s10107-014-0800-2; Necoara  Ion, 2015, ARXIV150406298; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Rockafellar R.T., 2015, CONVEX ANAL; Wibisono Andre, 2016, ARXIV160304245; Wilson A. C., 2016, ARXIV161102635; Zhang H, 2016, ARXIV160600269; Zhang H., 2013, ARXIV13034645; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304039
C	Zhang, ZJ; Zhang, YN; Li, ZP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Zijun; Zhang, Yining; Li, Zongpeng			Removing the Feature Correlation Effect of Multiplicative Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multiplicative noise, including dropout, is widely used to regularize deep neural networks (DNNs), and is shown to be effective in a wide range of architectures and tasks. From an information perspective, we consider injecting multiplicative noise into a DNN as training the network to solve the task with noisy information pathways, which leads to the observation that multiplicative noise tends to increase the correlation between features, so as to increase the signal-to-noise ratio of information pathways. However, high feature correlation is undesirable, as it increases redundancy in representations. In this work, we propose non-correlating multiplicative noise (NCMN), which exploits batch normalization to remove the correlation effect in a simple yet effective way. We show that NCMN significantly improves the performance of standard multiplicative noise on image classification tasks, providing a better alternative to dropout for batch-normalized networks. Additionally, we present a unified view of NCMN and shake-shake regularization, which explains the performance gain of the latter.	[Zhang, Zijun; Zhang, Yining] Univ Calgary, Calgary, AB, Canada; [Li, Zongpeng] Wuhan Univ, Wuhan, Hubei, Peoples R China	University of Calgary; Wuhan University	Zhang, ZJ (corresponding author), Univ Calgary, Calgary, AB, Canada.	zijun.zhang@ucalgary.ca; yining.zhang1@ucalgary.ca; zongpeng@whu.edu.cn			NSFC [61628209]; Hubei Science Foundation [2016CFA030, 2017AAA125]; Wuhan Science Tech Program [2018010401011288]	NSFC(National Natural Science Foundation of China (NSFC)); Hubei Science Foundation; Wuhan Science Tech Program	This work was supported by NSFC 61628209, Hubei Science Foundation 2016CFA030, 2017AAA125, and Wuhan Science & Tech Program 2018010401011288.	Batra, 2016, ICLR, P1; Bengio Y., 2009, ADV NEURAL INFORM PR, V22, P99; Gastaldi  Xavier, 2017, WORKSH INT C LEARN R; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 2016, ICLR, DOI DOI 10.1109/WCNC.2016.7564824; Rodriguez  Pau, 2017, INT C LEARN REPR; Semeniuta S., 2016, P COLING 2016 26 INT, P1757; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang SH, 2013, APPL MECH MATER, V422, P118, DOI 10.4028/www.scientific.net/AMM.422.118; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Z., 2017, ARXIV170904546	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300058
C	Zhou, DR; Xu, P; Gu, QQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Dongruo; Xu, Pan; Gu, Quanquan			Stochastic Nested Variance Reduction for Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MINIMIZATION	We study finite-sum nonconvex optimization problems, where the objective function is an average of n nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses K + 1 nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, the proposed algorithm converges to an epsilon-approximate first-order stationary point (i.e., parallel to del F(x)parallel to(2) <= epsilon) within O(n boolean AND epsilon(-)(2) + epsilon(-3) boolean AND n(1/2)epsilon(-2))(1) number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG O(n + n(2/3)epsilon(-)(2)) and that of SCSG O(n boolean AND epsilon(-2 )+ epsilon(-10/3) boolean AND n(2/3)epsilon(-2)). For gradient dominated functions, our algorithm also achieves better gradient complexity than the state-of-the-art algorithms. Thorough experimental results on different nonconvex optimization problems back up our theory.	[Zhou, Dongruo; Xu, Pan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Zhou, DR (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	drzhou@cs.ucla.edu; panxu@cs.ucla.edu; qgu@cs.ucla.edu	Xu, Pan/AAH-3620-2019; Zhou, Dongruo/GYJ-3503-2022; X, Pan/GVS-4402-2022	Xu, Pan/0000-0002-2559-8622; 	National Science Foundation [IIS-1652539]; BIGDATA [IIS-1855099]	National Science Foundation(National Science Foundation (NSF)); BIGDATA	We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation IIS-1652539 and BIGDATA IIS-1855099. We also thank AWS for providing cloud computing credits associated with the NSF BIGDATA award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Agarwal N., 2017, FINDING APPROXIMATE; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Allen-Zhu Zeyuan, 2018, ARXIV180203866; Bietti A., 2017, ADV NEURAL INFORM PR, P1622; Carmon Y., 2017, LOWER BOUNDS FINDING; Carmon Y., 2016, ACCELERATED METHODS; Carmon Y, 2017, PR MACH LEARN RES, V70; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio AJ, 2014, PR MACH LEARN RES, V32, P1125; Garber D., 2015, ARXIV150905647; Harikandeh R., 2015, PROC NEURAL INF PROC, P2251; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Hu C., 2009, ADV NEURAL INF PROCE, P781; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lan GH, 2018, MATH PROGRAM, V171, P167, DOI 10.1007/s10107-017-1173-0; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lei LH, 2017, ADV NEUR IN, V30; Li H., 2015, PROC 28 INT C NEURAL, P379; Li Qunwei, 2017, ARXIV170504925; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2014, INTRO LECT CONVEX OP; Ng AY, READING DIGITS NATUR; PAQUETTE C, 2017, ARXIV170310993; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Reddi S. J., 2016, STOCHASTIC VARIANCE, P314; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi SJ, 2016, IEEE DECIS CONTR P, P1971, DOI 10.1109/CDC.2016.7798553; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Scholkopf B., 2001, LEARNING KERNELS SUP; Shalev-Shwartz S, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz Shai, 2015, ARXIV PREPRINT ARXIV; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zeyuan Allen-Zhu, 2017, ARXIV170808694	46	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303088
C	Zhou, TY; Wang, SJ; Bilmes, JA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Tianyi; Wang, Shengjie; Bilmes, Jeff A.			Diverse Ensemble Evolution: Curriculum Data-Model Marriage	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DYNAMIC CLASSIFIER SELECTION; NEURAL-NETWORKS; MIXTURES	We study a new method "Diverse Ensemble Evolution (DivE(2))" to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model's current expertise and an intra-and inter-model diversity reward. DivE(2) schedules, over the course of training epochs, the relative importance of these characteristics; it starts by selecting easy samples for each model, and then gradually adjusts towards the models having specialized and complementary expertise on subsets of the training data, thereby encouraging high accuracy of the ensemble. We utilize an intra-model diversity term on data assigned to each model, and an inter-model diversity term on data assigned to pairs of models, to penalize both within-model and cross-model redundancy. We formulate the data-model marriage problem as a generalized bipartite matching, represented as submodular maximization subject to two matroid constraints. DivE(2) solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data-model marriage while the continuous part updates model parameters based on the assignments. In experiments, DivE(2) outperforms other ensemble training methods under a variety of model aggregation techniques, while also maintaining competitive efficiency.	[Zhou, Tianyi] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA; Univ Washington, Dept Elect & Comp Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Zhou, TY (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	tianyizh@uw.edu; wangsj@uw.edu; bilmes@uw.edu		Zhou, Tianyi/0000-0001-5348-0632	National Science Foundation [IIS-1162606]; National Institutes of Health [R01GM103544]; Google; Microsoft; Intel research award; CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program - DARPA	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Google(Google Incorporated); Microsoft(Microsoft); Intel research award; CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program - DARPA	This material is based upon work supported by the National Science Foundation under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft, and an Intel research award. This research is also supported by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.	Bai W., 2016, 7 ACM C BIOINF COMP; Basu S, 2013, 27 AAAI C ART INT; Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1; Bengio Y., 2014, GROWING ADAPTIVE MAC, P109; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Caruana Rich, 2004, ICML, DOI DOI 10.1145/1015330.1015432; Cavalin PR, 2013, NEURAL COMPUT APPL, V22, P673, DOI 10.1007/s00521-011-0737-9; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; CONFORTI M, 1984, DISCRETE APPL MATH, V7, P251, DOI 10.1016/0166-218X(84)90003-9; Cornuejols G., 1977, ANN DISCRET MATH, V1, P163, DOI DOI 10.1016/S0167-5060(08)70732-5; Cotter A, 2018, PR MACH LEARN RES, V80; Cruz RMO, 2018, INFORM FUSION, V41, P195, DOI 10.1016/j.inffus.2017.09.010; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duchi J, 2011, J MACH LEARN RES, V12, P2121; EFRON B, 1979, ANN STAT, V7, P1, DOI 10.1214/aos/1176344552; Elsayed G.F., 2018, ADVERSARIAL EXAMPLES; Fisher M. L., 1978, MATH PROGRAMMING STU, V8; Fiterau M., 2012, ADV NEURAL INFORM PR, P3023; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gao Huang, 2017, INT C LEARN REPR ICL; Gillenwater J., 2012, ADV NEURAL INFORM PR; Guzm<prime>an-rivera Abner, 2012, ADV NEURAL INFORM PR; Han S, 2016, ADV NEURAL INF PROCE, V29, DOI 10.5555/3157096.3157109; HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hui Lin, 2009, P IEEE AUT SPEECH RE; Hui Lin, 2011, P 49 ANN M ASS COMP, V2, P170; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Khan Faisal, 2011, ADV NEURAL INFORM PR, P1449; Kingma D.P, P 3 INT C LEARNING R; Ko AHR, 2008, PATTERN RECOGN, V41, P1718, DOI 10.1016/j.patcog.2007.10.015; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Lee S., 2016, ADV NEURAL INFORM PR, V29, P2119; Lee Stefan, 2015, ABS151106314 ARXIV; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Merz C., 1996, DYNAMICAL SELECTION, P281; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Mirzasoleiman B, 2015, AAAI CONF ARTIF INTE, P1812; Moghimi M., 2016, BRIT MACH VIS C BMVC; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Partalas I, 2008, FRONT ARTIF INTEL AP, V178, P117, DOI 10.3233/978-1-58603-891-5-117; Prasad Adarsh, 2014, ADV NEURAL INFORM PR, V2, P2645; RAJPURKAR P, 2016, P 2016 C EMP METH NA, V2016, P2383, DOI DOI 10.18653/V1/D16-1264; Sandler Mark, 2018, ARXIV180104381, DOI DOI 10.1109/CVPR.2018.00474; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Singh S., 2016, ADV NEURAL INFORM PR, V29; Spitkovsky Valentin I., 2009, NIPS 2009 WORKSH GRA; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tang K., 2012, ADV NEURAL INFORM PR; Tang Y., 2012, ACM INT C MULT, P833, DOI DOI 10.1145/2393347.2396324; Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994; Tramer Florian, 2018, INT C LEARN REPR ICL; Veit A, 2016, ADV NEUR IN, V29; Woods K, 1997, IEEE T PATTERN ANAL, V19, P405, DOI 10.1109/34.588027; Xiao H., 2017, ARXIV 170807747; Zhou T., 2018, INT C LEARN REPR; Zhou Tianyi, 2018, NIPS; Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X; Zhu XQ, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P305, DOI 10.1109/ICDM.2004.10091	73	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000041
C	Zhou, Y; Wang, Z; Liang, YB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Yi; Wang, Zhe; Liang, Yingbin			Convergence of Cubic Regularization for Nonconvex Optimization under KL Property	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Cubic-regularized Newton's method (CR) is a popular algorithm that guarantees to produce a second-order stationary solution for solving nonconvex optimization problems. However, existing understandings of the convergence rate of CR are conditioned on special types of geometrical properties of the objective function. In this paper, we explore the asymptotic convergence rate of CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of nonconvex objective functions. In specific, we characterize the asymptotic convergence rate of various types of optimality measures for CR including function value gap, variable distance gap, gradient norm and least eigenvalue of the Hessian matrix. Our results fully characterize the diverse convergence behaviors of these optimality measures in the full parameter regime of the KL property. Moreover, we show that the obtained asymptotic convergence rates of CR are order-wise faster than those of first-order gradient descent algorithms under the KL property.	[Zhou, Yi; Wang, Zhe; Liang, Yingbin] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Zhou, Y (corresponding author), Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.	zhou.1172@osu.edu; wang.10982@osu.edu; liang.889@osu.edu	哲, 王/GYJ-1551-2022	Liang, Yingbin/0000-0002-8635-2992	U.S. National Science Foundation [CCF-1761506, ECCS-1818904]	U.S. National Science Foundation(National Science Foundation (NSF))	This work was supported in part by U.S. National Science Foundation under the grants CCF-1761506 and ECCS-1818904.	Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; Allen-Zhu Z., 2017, ARXIV170808694V3; [Anonymous], 2016, ADV NEURAL INFORM PR; Arias-Castro E, 2017, J MACH LEARN RES, V18, P1; Attouch H, 2009, MATH PROGRAM, V116, P5, DOI 10.1007/s10107-007-0133-5; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Bhojanapalli S., 2016, P ADV NEUR INF PROC; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; CARMON Y, 2016, ARXIV161200547; Cartis C., 2011, MATH PROGRAMMING; Cartis C, 2011, MATH PROGRAM, V130, P295, DOI 10.1007/s10107-009-0337-y; Frankel P, 2015, J OPTIMIZ THEORY APP, V165, P874, DOI 10.1007/s10957-014-0642-3; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; GHADIMI S, 2017, ARXIV171005782; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hartman P., 2002, SOC IND APPL MATH; Jiang B., 2017, ARXIV171004788; Karimi H., 2016, MACHINE LEARNING KNO, P795, DOI 10.1007/978-3-319-46128-1_50; Kohler JM, 2017, PR MACH LEARN RES, V70; Li  Qunwei, 2017, P 34 INT C MACH LEAR, P2111; Liu M., 2017, ARXIV170908571V2; LOJASIEWICZ S., 1963, EQUATIONS DERIVEES P, P87; Lojasiewicz S., 1965, ENSEMBLES SEMIANALYT; Nesterov Y., 2006, MATH PROGRAMMING; Nesterov Y, 2008, MATH PROGRAM, V112, P159, DOI 10.1007/s10107-006-0089-x; Noll D, 2013, SPRINGER P MATH STAT, V50, P593, DOI 10.1007/978-1-4614-7621-4_27; Sun J., 2015, ARXIV151006096V2; Sun J., 2017, FDN COMPUTATIONAL MA, P1, DOI DOI 10.1007/S10208-017-9365-9; Tripuraneni N., 2017, ARXIV71102838; Wang Z., 2018, ARXIV180207372V1; Xu Peng, 2017, ARXIV170807164; Yue M., 2018, ARXIV180109387V1; Zhou Y, 1965, CHARACTERIZATION GRA; Zhou Y, 2018, J MACH LEARN RES, V19; Zhou Y, 2016, JMLR WORKSH CONF PRO, V51, P713; Zhou Y, 2016, ANN ALLERTON CONF, P331, DOI 10.1109/ALLERTON.2016.7852249; Zhou Yi, 2018, P INT C LEARN REPR I, P13	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303073
C	Zhou, ZY; Mertikopoulos, P; Athey, S; Bambos, N; Glynn, P; Ye, YY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Zhengyuan; Mertikopoulos, Panayotis; Athey, Susan; Bambos, Nicholas; Glynn, Peter; Ye, Yinyu			Learning in Games with Lossy Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS; DYNAMICS	We consider a game-theoretical multi-agent learning problem where the feedback information can be lost during the learning process and rewards are given by a broad class of games known as variationally stable games. We propose a simple variant of the classical online gradient descent algorithm, called reweighted online gradient descent (ROGD) and show that in variationally stable games, if each agent adopts ROGD, then almost sure convergence to the set of Nash equilibria is guaranteed, even when the feedback loss is asynchronous and arbitrarily corrrelated among agents. We then extend the framework to deal with unknown feedback loss probabilities by using an estimator (constructed from past data) in its replacement. Finally, we further extend the framework to accomodate both asynchronous loss and stochastic rewards and establish that multi-agent ROGD learning still converges to the set of Nash equilibria in such settings. Together, these results contribute to the broad lanscape of multi-agent online learning by significantly relaxing the feedback information that is required to achieve desirable outcomes.	[Zhou, Zhengyuan; Athey, Susan; Bambos, Nicholas; Glynn, Peter; Ye, Yinyu] Stanford Univ, Stanford, CA 94305 USA; [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, INRIA, LIG, Grenoble, France	Stanford University; Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS)	Zhou, ZY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	zyzhou@stanford.edu; panayotis.mertikopoulos@imag.fr; athey@stanford.edu; bambos@stanford.edu; glynn@stanford.edu; yinyu-ye@stanford.edu		Athey, Susan/0000-0001-6934-562X; Bambos, Nicholas/0000-0001-9250-4553				ANAND  A., 2015, 24 INT JOINT C ART I; Anand A, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1901; [Anonymous], 2014, ARXIV14025481; [Anonymous], THESIS; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; BALANDAT  M., 2016, ARXIV160601261; Bervoets S., 2018, LEARNING MINIMAL INF; Bloembergen D, 2015, J ARTIF INTELL RES, V53, P659, DOI 10.1613/jair.4818; Blum A, 1998, LECT NOTES COMPUT SC, V1442, P306, DOI 10.1007/BFb0029575; Blum A, 2007, J MACH LEARN RES, V8, P1307; Bravo M., 2018, P 32 INT C NEUR INF, P5666; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Busoniu L, 2010, STUD COMPUT INTELL, V310, P183; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; DE SOUZA  P., 2012, PROBLEM BOOKS MATH; Dimakopoulou M, 2018, PR MACH LEARN RES, V80; Fudenberg D., 1998, THEORY LEARNING GAME, V2; Goodfellow I., 2016, DEEP LEARNING; Grover A., 2018, INT C AUT AG MULT SY; Grover Aditya, 2018, INT C ART INT STAT, P833; Grover Aditya, 2018, INT C MACH LEARN; Hazan E., 2016, FDN TRENDS R OPTIMIZ; Heliou A., 2017, P ADV NEUR INF PROC, V30, P6369; Joulani P., 2013, ICML, P1453; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Krichene S, 2015, ANN ALLERTON CONF, P480, DOI 10.1109/ALLERTON.2015.7447043; Lam Kiet, 2016, 2016 ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS), P1, DOI 10.1109/ICCPS.2016.7479108; Menache I., 2011, SYNTH LECT COMMUN NE, V4, P1; Mertikopoulos P., 2018, MATH PROGRAM, P1; Mertikopoulos P, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2703; Mertikopoulos P, 2017, IEEE T SIGNAL PROCES, V65, P2277, DOI 10.1109/TSP.2017.2656847; Mnih V., 2013, ARXIV PREPRINT ARXIV; Monnot B, 2017, KNOWL ENG REV, V32, DOI 10.1017/S0269888917000133; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; PALAIOPANOS  G., 2017, ADV NEURAL INFORM PR, V30, P5872, DOI DOI 10.5555/3295222.3295337; Perkins S, 2017, IEEE T AUTOMAT CONTR, V62, P379, DOI 10.1109/TAC.2015.2511930; Pike-Burke  Ciara, 2018, P 35 INT C MACH LEAR, P4102; Quanrud K., 2015, P ADV NEUR INF PROC, P1270; ROUGHGARDEN  T., SELFISH ROUTING PRIC, V174; Salehisadaghiani F, 2017, IFAC PAPERSONLINE, V50, P6166, DOI 10.1016/j.ifacol.2017.08.983; Shalev-Shwartz S., 2007, ADV NEURAL INFORM PR, P1265; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shoham Y., 2008, MULTIAGENT SYSTEMS A, DOI DOI 10.1017/CBO9780511811654; Viossat Y, 2013, J ECON THEORY, V148, P825, DOI 10.1016/j.jet.2012.07.003; Yin Z, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2131, DOI 10.1145/3097983.3098148; Yin Zi., 2018, ADV NEURAL INFORM PR; Zhou Z., 2018, ARXIV181004778; Zhou Z., 2017, 2017 IEEE 56 ANN C D, P5776, DOI [10.1109/CDC.2017.8264532, DOI 10.1109/CDC.2017.8264532]; Zhou Z., 2017, NIPS 17; Zhou ZY, 2016, LECT NOTES COMPUT SC, V9996, P114, DOI 10.1007/978-3-319-47413-7_7; Zhou ZY, 2016, P AMER CONTR CONF, P3802, DOI 10.1109/ACC.2016.7525505; Zhu MH, 2016, AUTOMATICA, V63, P82, DOI 10.1016/j.automatica.2015.10.012; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	55	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305017
C	Alon, N; Reichman, D; Shinkar, I; Wagner, T; Musslick, S; Cohen, JD; Griffiths, TL; Dey, B; Ozcimder, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alon, Noga; Reichman, Daniel; Shinkar, Igor; Wagner, Tal; Musslick, Sebastian; Cohen, Jonathan D.; Griffiths, Thomas L.; Dey, Biswadip; Ozcimder, Kayhan			A graph-theoretic approach to multitasking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A key feature of neural network architectures is their ability to support the simultaneous interaction among large numbers of units in the learning and processing of representations. However, how the richness of such interactions trades off against the ability of a network to simultaneously carry out multiple independent processes - a salient limitation in many domains of human cognition - remains largely unexplored. In this paper we use a graph-theoretic analysis of network architecture to address this question, where tasks are represented as edges in a bipartite graph G = (A boolean OR B, E). We define a new measure of multitasking capacity of such networks, based on the assumptions that tasks that need to be multitasked rely on independent resources, i.e., form a matching, and that tasks can be multitasked without interference if they form an induced matching. Our main result is an inherent tradeoff between the multitasking capacity and the average degree of the network that holds regardless of the network architecture. These results are also extended to networks of depth greater than 2. On the positive side, we demonstrate that networks that are random-like (e.g., locally sparse) can have desirable multitasking properties. Our results shed light into the parallel-processing limitations of neural systems and provide insights that may be useful for the analysis and design of parallel architectures.	[Alon, Noga] Tel Aviv Univ, Tel Aviv, Israel; [Reichman, Daniel; Shinkar, Igor; Griffiths, Thomas L.] Univ Calif Berkeley, Berkeley, CA USA; [Wagner, Tal] MIT, Cambridge, MA 02139 USA; [Musslick, Sebastian; Cohen, Jonathan D.; Dey, Biswadip; Ozcimder, Kayhan] Princeton Univ, Princeton, NJ 08544 USA	Tel Aviv University; University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT); Princeton University	Alon, N (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.		Musslick, Sebastian/AAW-8385-2021; Alon, Noga/GOV-5970-2022; Jeong, Yongwook/N-7413-2016	Alon, Noga/0000-0003-1332-4883; Dey, Biswadip/0000-0003-1140-1363; Griffiths, Thomas/0000-0002-5138-7255	DARPA [N66001-15-2-4048]; Value Alignment in Autonomous Systems; William and Flora Hewlett Foundation [2014-1600]; John Templeton Foundation	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Value Alignment in Autonomous Systems; William and Flora Hewlett Foundation; John Templeton Foundation	Supported by DARPA contract N66001-15-2-4048, Value Alignment in Autonomous Systems and Grant: 2014-1600, Sponsor: William and Flora Hewlett Foundation, Project Title: Cybersecurity and Internet Policy; This publication was made possible through the support of a grant from the John Templeton Foundation. The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the John Templeton Foundation	Alon N, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1079; Alon Noga, 2017, ARXIV161102400; AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530; Arora S, 2014, PR MACH LEARN RES, V32; BIRK Y, 1993, IEEE T INFORM THEORY, V39, P186, DOI 10.1109/18.179355; Bregman LM, 1973, SOV MATH DOKL, V14, P945; CHLAMTAC I, 1985, IEEE T COMMUN, V33, P1240, DOI 10.1109/TCOM.1985.1096245; EGORYCHEV GP, 1981, ADV MATH, V42, P299, DOI 10.1016/0001-8708(81)90044-X; FALIKMAN DI, 1981, MATH NOTES+, V29, P475, DOI 10.1007/BF01163285; Feige Uriel, 2016, GEN GIRTH PROBLEMS G; Feng SF, 2014, COGN AFFECT BEHAV NE, V14, P129, DOI 10.3758/s13415-013-0236-9; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; KOMLOS J, 1988, NEURAL NETWORKS, V1, P239, DOI 10.1016/0893-6080(88)90029-9; Musslick S, 2016, P 38 ANN C COGN SCI, P1547; Musslick Sebastian, 2017, 39 COGN SCI SOC C LO; Neisser U., 1967, COGNITIVE PSYCHOL; PYBER L, 1985, COMBINATORICA, V5, P347, DOI 10.1007/BF02579250; PYBER L, 1995, J COMB THEORY B, V63, P41, DOI 10.1006/jctb.1995.1004; Rumelhart D. E., 1988, PARALLEL DISTRIBUTED; SCHNEIDER W, 1977, PSYCHOL REV, V84, P1, DOI 10.1037/0033-295X.84.1.1; Schrijver A, 1998, J COMB THEORY B, V72, P122, DOI 10.1006/jctb.1997.1798; Valiant Leslie G, 2000, CIRCUITS OF THE MIND	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402015
C	Balcan, MF; Zhang, HY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Balcan, Maria-Florina; Zhang, Hongyang			Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				HALFSPACES	We provide new results for noise-tolerant and sample-efficient learning algorithms under s-concave distributions. The new class of s-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and t-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown about the geometry of this class of distributions and their applications in the context of learning. The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of s-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and the disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passive learning of intersections of halfspaces. Our analysis of geometric properties of s-concave distributions might be of independent interest to optimization more broadly.	[Balcan, Maria-Florina; Zhang, Hongyang] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Zhang, HY (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	ninamf@cs.cmu.edu; hongyanz@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		Sloan Fellowship; Microsoft Research Fellowship;  [NSF-CCF 1535967];  [NSF CCF-1422910];  [NSF CCF-1451177]	Sloan Fellowship(Alfred P. Sloan Foundation); Microsoft Research Fellowship(Microsoft); ; ; 	This work was supported in part by grants NSF-CCF 1535967, NSF CCF-1422910, NSF CCF-1451177, a Sloan Fellowship, and a Microsoft Research Fellowship.	Applegate David, 1991, P 23 STOC, P156, DOI [10.1145/103418.103439]5, DOI 10.1145/103418.103439]5]; Awasthi P., 2016, C LEARN THEOR COLT, P152; Awasthi P, 2017, J ACM, V63, DOI 10.1145/3006384; Awasthi P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P449, DOI 10.1145/2591796.2591839; Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Balcan Maria-Florina, 2016, ADV NEURAL INFORM PR, P2955; Balcan P. Long, 2013, C LEARNING THEORY CO, P288; Baum EB, 1990, NEURAL COMPUT, V2, P510, DOI 10.1162/neco.1990.2.4.510; Beygelzimer A., 2009, P 26 ANN INT C MACH, P49; BRASCAMP HJ, 1976, J FUNCT ANAL, V22, P366, DOI 10.1016/0022-1236(76)90004-5; Caramanis C, 2007, IEEE T INFORM THEORY, V53, P1043, DOI 10.1109/TIT.2006.890699; Chandrasekaran K, 2009, LECT NOTES COMPUT SC, V5687, P420, DOI 10.1007/978-3-642-03685-9_32; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Daniely A, 2016, ACM S THEORY COMPUT, P105, DOI 10.1145/2897518.2897520; Dasgupta S., 2004, ADV NEURAL INFORM PR, P337; Dasgupta S., 2007, C ADV NEUR INF PROC, P353; Friedman E., 2009, ANN C LEARN THEOR; Hanneke S., 2007, P 24 INT C MACH LEAR, P353, DOI [10.1145/1273496.1273541, DOI 10.1145/1273496.1273541]; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; Kalai AT, 2006, MATH OPER RES, V31, P253, DOI 10.1287/moor.1060.0194; Kalai AT, 2008, SIAM J COMPUT, V37, P1777, DOI 10.1137/060649057; Kane DM, 2017, ANN IEEE SYMP FOUND, P355, DOI 10.1109/FOCS.2017.40; Klivans A.R., 2014, LIPICS, P793; Klivans AR, 2009, LECT NOTES COMPUT SC, V5687, P588, DOI 10.1007/978-3-642-03685-9_44; Servedio R. A., 2001, THESIS; Wang LW, 2011, J MACH LEARN RES, V12, P2269; Welling M., 2014, ADV NEURAL INFORM PR, V27; Xu Y., 2017, ADV NEURAL INFORM PR, P2428; Yan S., 2017, ARXIV170205581	32	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404084
C	Balkanski, E; Syed, U; Vassilvitskii, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Balkanski, Eric; Syed, Umar; Vassilvitskii, Sergei			Statistical Cost Sharing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MECHANISMS	We study the cost sharing problem for cooperative games in situations where the cost function C is not available via oracle queries, but must instead be learned from samples drawn from a distribution, represented as tuples (S, C(S)), for different subsets S of players. We formalize this approach, which we call STATISTICAL COST SHARING, and consider the computation of the core and the Shapley value. Expanding on the work by Balcan et al. [2015], we give precise sample complexity bounds for computing cost shares that satisfy the core property with high probability for any function with a non-empty core. For the Shapley value, which has never been studied in this setting, we show that for submodular cost functions with bounded curvature kappa it can be approximated from samples from the uniform distribution to a root 1-kappa factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value and that these can be approximated arbitrarily well from samples from any distribution and for any function.	[Balkanski, Eric] Harvard Univ, Cambridge, MA 02138 USA; [Syed, Umar; Vassilvitskii, Sergei] Google NYC, New York, NY USA	Harvard University	Balkanski, E (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	ericbalkanski@g.harvard.edu; usyed@google.com; sergeiv@google.com						Anshelevich E, 2008, SIAM J COMPUT, V38, P1602, DOI 10.1137/070680096; Bachrach Y, 2010, AUTON AGENT MULTI-AG, V20, P105, DOI 10.1007/s10458-009-9078-9; Badanidiyuru A., 2012, SODA; Balcan M. F., 2012, COLT, P4; Balcan MF, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P475; Balcan MF, 2011, ACM S THEORY COMPUT, P793; Balcan Maria-Florina, 2016, ARXIV150500039V2; Balkanski Eric, 2015, P 16 ACM C EC COMP E, P529, DOI DOI 10.1145/2764468.2764505; Bondareva O.N., 1963, PROBL KYBERN, V10, P119; Deng XT, 1999, MATH OPER RES, V24, P751, DOI 10.1287/moor.24.3.751; Devanur NR, 2005, DECIS SUPPORT SYST, V39, P11, DOI 10.1016/j.dss.2004.08.004; Fatima SS, 2008, ARTIF INTELL, V172, P1673, DOI 10.1016/j.artint.2008.05.003; FEIGENBAUM J, 2000, P 32 ANN ACM S THEOR, P218; Feldman V., 2014, C LEARNING THEORY, P679; Feldman Vitaly, 2014, INF THEOR APPL WORKS, P1; Gillies D.B., 1959, CONTRIBUTIONS THEORY, V4, P47; Goemans MX, 2004, J ALGORITHM, V50, P194, DOI 10.1016/S0196-6774(03)00098-1; Goodfellow IJ, 2014, ABS14126572 CORR; Immorlica N, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1361192.1361201; Iyer Rishabh K., 2013, P 8 C WORKSHOP NEURA, P2436; Iyer Rishabh K., 2013, ADV NEURAL INFORM PR, P2742; JAIN K, 2002, P 34 ANN ACM S THEOR, P313; Liben-Nowell David, 2012, Computing and Combinatorics. Proceedings of the 18th Annual International Conference COCOON 2012, P568, DOI 10.1007/978-3-642-32241-9_48; Mann I., 1960, VALUES LARGE GAMES; Moulin H, 1999, SOC CHOICE WELFARE, V16, P279, DOI 10.1007/s003550050145; Moulin H, 2001, ECON THEOR, V18, P511, DOI 10.1007/PL00004200; Pal M, 2003, ANN IEEE SYMP FOUND, P584, DOI 10.1109/SFCS.2003.1238231; Roth A.E., 1988, SHAPLEY VALUE ESSAYS; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; SHAPLEY LS, 1967, NAV RES LOGIST Q, V14, P453, DOI 10.1002/nav.3800140404; Shapley LS., 1953, CONTRIB THEORY GAMES, V2, P307; Sviridenko M., 2015, PROC 26 ACM SIAM S D, P1134; Szegedy C., 2013, ABS13126199 CORR; Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559; Vondrak Jan, 2010, RIMS KOKYUROKU BES B, VB23, P253; Winter E., 2002, HDB GAME THEORY EC A, V3, P2025, DOI DOI 10.1016/S1574-0005(02)03016-3	36	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406029
C	Barbos, AC; Caron, F; Giovannelli, JF; Doucet, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Barbos, Andrei-Cristian; Caron, Francois; Giovannelli, Jean-Francois; Doucet, Arnaud			Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DISTRIBUTIONS	We propose a generalized Gibbs sampler algorithm for obtaining samples approximately distributed from a high-dimensional Gaussian distribution. Similarly to Hogwild methods, our approach does not target the original Gaussian distribution of interest, but an approximation to it. Contrary to Hogwild methods, a single parameter allows us to trade bias for variance. We show empirically that our method is very flexible and performs well compared to Hogwild-type algorithms.	[Barbos, Andrei-Cristian; Giovannelli, Jean-Francois] Univ Bordeaux, IMS Lab, CNRS, BINP, Bordeaux, France; [Caron, Francois; Doucet, Arnaud] Univ Oxford, Dept Stat, Oxford, England	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite de Bordeaux; University of Oxford	Barbos, AC (corresponding author), Univ Bordeaux, IMS Lab, CNRS, BINP, Bordeaux, France.	andbarbos@u-bordeaux.fr; caron@stats.ox.ac.uk; giova@ims-bordeaux.fr; doucet@stats.ox.ac.uk						ADLER SL, 1981, PHYS REV D, V23, P2901, DOI 10.1103/PhysRevD.23.2901; Barone P., 1990, PROBAB ENG INF SCI, V4, P369, DOI [10.1017/S0269964800001674, DOI 10.1017/S0269964800001674]; Fox C, 2017, BERNOULLI, V23, P3711, DOI 10.3150/16-BEJ863; Gel Y, 2004, J AM STAT ASSOC, V99, P575, DOI 10.1198/016214504000000872; Gilavert C, 2015, IEEE T SIGNAL PROCES, V63, P70, DOI 10.1109/TSP.2014.2367457; Golub G., 2013, MATRIX COMPUTATIONS; JOHNSON M, 2013, ADV NEURAL INFORM PR, P2715; Newman D., 2008, ADV NEURAL INFORM PR, P1081; Orieux F, 2012, IEEE SIGNAL PROC LET, V19, P251, DOI 10.1109/LSP.2012.2189104; Papandreou G., 2010, ADV NEURAL INF PROCE, V23, P1858; Roberts GO, 1997, J ROY STAT SOC B MET, V59, P291, DOI 10.1111/1467-9868.00070; Rue H, 2001, J ROY STAT SOC B, V63, P325, DOI 10.1111/1467-9868.00288	13	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405010
C	Ben-Porat, O; Tennenholtz, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ben-Porat, Omer; Tennenholtz, Moshe			Best Response Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In a regression task, a predictor is given a set of instances, along with a real value for each point. Subsequently, she has to identify the value of a new instance as accurately as possible. In this work, we initiate the study of strategic predictions in machine learning. We consider a regression task tackled by two players, where the payoff of each player is the proportion of the points she predicts more accurately than the other player. We first revise the probably approximately correct learning framework to deal with the case of a duel between two predictors. We then devise an algorithm which finds a linear regression predictor that is a best response to any (not necessarily linear) regression algorithm. We show that it has linearithmic sample complexity, and polynomial time complexity when the dimension of the instances domain is fixed. We also test our approach in a high-dimensional setting, and show it significantly defeats classical regression algorithms in the prediction duel. Together, our work introduces a novel machine learning task that lends itself well to current competitive online settings, provides its theoretical foundations, and illustrates its applicability.	[Ben-Porat, Omer; Tennenholtz, Moshe] Technion Israel Inst Technol, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Ben-Porat, O (corresponding author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.	omerbp@campus.technion.ac.il; moshet@ie.technion.ac.il			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [740435]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	We thank Gili Baumer and Argyris Deligkas for helpful discussions, and anonymous reviewers for their useful suggestions. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 740435).	AMALDI E, 1995, THEOR COMPUT SCI, V147, P181, DOI 10.1016/0304-3975(94)00254-G; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Dekel O, 2010, J COMPUT SYST SCI, V76, P759, DOI 10.1016/j.jcss.2010.03.003; Gurobi Optimization, 2019, GUROBI OPTIMIZER REF; HARRISON D, 1978, J ENVIRON ECON MANAG, V5, P81, DOI 10.1016/0095-0696(78)90006-2; Immorlica N, 2011, ACM S THEORY COMPUT, P215; Meir R, 2012, ARTIF INTELL, V186, P123, DOI 10.1016/j.artint.2012.03.008; Nisan N., 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P129, DOI 10.1145/301250.301287; Pechyony D., 2010, ADV NEURAL INFORM PR, P1894; Sauer N., 1972, J COMB THEORY A, V13, P145, DOI [10.1016/0097-3165(72)90019-2, DOI 10.1016/0097-3165(72)90019-2]; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 2008, P NATO WORKSH MIN MA, P3; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	15	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401052
C	Berthet, Q; Perchet, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Berthet, Quentin; Perchet, Vianney			Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the problem of bandit optimization, inspired by stochastic optimization and online learning problems with bandit feedback. In this problem, the objective is to minimize a global loss function of all the actions, not necessarily a cumulative loss. This framework allows us to study a very general class of problems, with applications in statistics, machine learning, and other fields. To solve this problem, we analyze the Upper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and convex optimization. We give theoretical guarantees for the performance of this algorithm over various classes of functions, and discuss the optimality of these results.	[Berthet, Quentin] Univ Cambridge, Cambridge, England; [Perchet, Vianney] ENS Paris Saclay, Paris, France; [Perchet, Vianney] Criteo Res, Paris, France	University of Cambridge; UDICE-French Research Universities; Universite Paris Saclay	Berthet, Q (corresponding author), Univ Cambridge, Cambridge, England.	q.berthet@statslab.cam.ac.uk; vianney.perchet@normalesup.org	Jeong, Yongwook/N-7413-2016		Isaac Newton Trust Early Career Support Scheme; Alan Turing Institute under the EPSRC [EP/N510129/1]; ANR [ANR-13-JS01-0004-01]; FMJH Program Gaspard Monge in Optimization and operations research (EDF); Labex LMH	Isaac Newton Trust Early Career Support Scheme; Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ANR(French National Research Agency (ANR)); FMJH Program Gaspard Monge in Optimization and operations research (EDF); Labex LMH	Supported by an Isaac Newton Trust Early Career Support Scheme and by The Alan Turing Institute under the EPSRC grant EP/N510129/1.; Supported by the ANR (grant ANR-13-JS01-0004-01), and the FMJH Program Gaspard Monge in Optimization and operations research (supported in part by EDF) and from the Labex LMH.	Agarwal A., 2011, P 24 INT C NEUR INF; Agrawal S, 2014, P 15 ACM C EC COMPUT, P989, DOI [10.1145/2600057.2602844, DOI 10.1145/2600057.2602844.URL]; Agrawal S., 2016, P ANN C LEARN THEOR; Audibert J.Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bach F., 2013, ADV NIPS; Bach F., 2016, COLT 2016; Berthet Q, 2016, P IEEE, V104, P111, DOI 10.1109/JPROC.2015.2494098; Bubeck S., 2016, CORR; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Carpentier A., 2015, PREPRINT; Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Dippon J, 2003, ANN STAT, V31, P1260, DOI 10.1214/aos/1059655913; Duchi J., 2016, LOCAL ASYMPTOTICS SO; Even-Dar E., 2009, P COLT; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Hazan E., 2014, ADV NIPS; Hazan E., 2014, P C LEARN THEOR COLT; Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287; Jaggi Martin, 2011, THESIS; Jamieson K.G., 2012, ADV NEURAL INF PROCE, V25; Lacoste-Julien S., 2013, NIPS 2013; Lafond J., 2015, ONLINE FRANK WOLFE A; Mannor S., 2014, P COLT; Mas-Colell A., 1995, MICROECONOMIC THEORY; Nesterov Y., 2003, INTRO LECT CONVEX OP; Perchet V, 2016, ANN STAT, V44, P660, DOI 10.1214/15-AOS1381; POLYAK B. T., 1990, PROBL INFORM TRANSM+, V26, P45; Rakhlin A., 2011, P 24 ANN C LEARN THE, P559; Saha A., 2011, P INT C ART INT STAT; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shamir O., 2013, P C LEARNG THEOR	35	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402027
C	Chen, S; Banerjee, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Sheng; Banerjee, Arindam			Alternating Estimation for Structured High-Dimensional Multi-Response Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				REGRESSION; LASSO	We consider the problem of learning high-dimensional multi-response linear models with structured parameters. By exploiting the noise correlations among different responses, we propose an alternating estimation (AltEst) procedure to estimate the model parameters based on the generalized Dantzig selector (GDS). Under suitable sample size and resampling assumptions, we show that the error of the estimates generated by AltEst, with high probability, converges linearly to certain minimum achievable level, which can be tersely expressed by a few geometric measures, such as Gaussian width of sets related to the parameter structure. To the best of our knowledge, this is the first non-asymptotic statistical guarantee for such AltEst-type algorithm applied to estimation with general structures.	[Chen, Sheng; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Chen, S (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	shengc@cs.umn.edu; banerjee@cs.umn.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	The research was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo.	Agarwal A., 2013, CORR; Anderson T. W., 2003, INTRO MULTIVARIATE S; [Anonymous], 2011, ECONOMETRIC ANAL; [Anonymous], 2015, ADV NEURAL INFORM PR; Argyriou A., 2012, NIPS; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bach F., 2011, OPTIMIZATION MACHINE, V5; Banerjee A., 2014, ADV NEURAL INFORM PR; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chatterjee S., 2014, ADV NEURAL INFORM PR; Chen S., 2016, ADV NEURAL INFORM PR; Chen S., 2015, NIPS, P2908; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Goncalves Andre R, 2014, P 23 ACM INT C C INF, P451, DOI DOI 10.1145/2661829.2662091; GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761; Izenman A., 2008, MODERN MULTIVARIATE; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1; Jacob L., 2009, ICML; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Jalali A., 2010, ADV NEURAL INF PROCE, V23, P964; Jenatton R, 2011, J MACH LEARN RES, V12, P2297; Kar P., 2014, ADV NEURAL INFORM PR, P685; Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549; Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013; Liu H., 2009, P 26 ANN INT C MACHI, P649, DOI DOI 10.1145/1553374.1553458; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Netrapalli P., 2013, NIPS; Rai Piyush, 2012, P 25 INT C NEUR INF, V25, P3185; Rao N., 2012, INT C ART INT STAT A; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Sohn K.-A., 2012, P 15 INT C ARTIFICIA, P1081; Sun R., 2015, FOCS; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp J. A., 2015, CONVEX RECOVERY STRU, P67; Wytock M, 2013, P 30 INT C MACH LEAR, P1265; Yi XY, 2014, PR MACH LEARN RES, V32, P613; Yuan XT, 2014, IEEE T INFORM THEORY, V60, P1673, DOI 10.1109/TIT.2013.2296784	42	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402086
C	Chevallier, J; Oudard, S; Allassonniere, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chevallier, Juliette; Oudard, Stephan; Allassonniere, Stephanie			Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MAXIMUM-LIKELIHOOD; GUIDELINES; MODELS	We introduce a hierarchical model which allows to estimate a group-average piecewise-geodesic trajectory in the Riemannian space of measurements and individual variability. This model falls into the well defined mixed-effect models. The subject-specific trajectories are defined through spatial and temporal transformations of the group-average piecewise-geodesic path, component by component. Thus we can apply our model to a wide variety of situations. Due to the non-linearity of the model, we use the Stochastic Approximation Expectation-Maximization algorithm to estimate the model parameters. Experiments on synthetic data validate this choice. The model is then applied to the metastatic renal cancer chemotherapy monitoring: we run estimations on RECIST scores of treated patients and estimate the time they escape from the treatment. Experiments highlight the role of the different parameters on the response to treatment.	[Chevallier, Juliette] Ecole Polytech, CMAP, Palaiseau, France; [Oudard, Stephan] USPC, AP HP, HEGP, Oncol Dept, Paris, France; [Allassonniere, Stephanie] Univ Paris 05, CRC, Paris, France	Institut Polytechnique de Paris; Assistance Publique Hopitaux Paris (APHP); Hopital Universitaire Europeen Georges-Pompidou - APHP; UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; Universite Paris Cite	Chevallier, J (corresponding author), Ecole Polytech, CMAP, Palaiseau, France.	juliette.chevallier@polytechnique.edu; atephanie.allassonniere@parisdescartes.fr	Chevallier, Juliette/AAK-1021-2021	Chevallier, Juliette/0000-0002-4736-3904	Investissement d'avenir [ANR-11-LABX-0056-LMH]; Fondation of Medical Research [DBI20131228564]	Investissement d'avenir(French National Research Agency (ANR)); Fondation of Medical Research	Ce travail beneficie d'un financement public Investissement d'avenir, reference ANR-11-LABX-0056-LMH. This work was supported by a public grant as part of the Investissement d'avenir, project reference ANR-11-LABX-0056-LMH.; Travail realise dans le cadre d'un projet finance par la Fondation de la Recherche Medicale, "DBI20131228564". Work performed as a part of a project funded by the Fondation of Medical Research, grant number "DBI20131228564".	Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229; Burotto M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096316; Delyon B, 1999, ANN STAT, V27, P94; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Escudier B, 2016, ANN ONCOL, V27, pv58, DOI 10.1093/annonc/mdw328; Gallot S., 2004, RIEMANNIAN GEOMETRY; Kuhn E, 2005, COMPUT STAT DATA AN, V49, P1020, DOI 10.1016/j.csda.2004.07.002; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Robert C.P., 1999, MONTE CARLO STAT MET; Rothermundt C, 2017, WORLD J UROL, V35, P641, DOI 10.1007/s00345-016-1903-6; Rothermundt C, 2015, ONCOLOGIST, V20, P1028, DOI 10.1634/theoncologist.2015-0145; Schiratti JB, 2015, ADV NEUR IN, V28; Stein WD, 2008, ONCOLOGIST, V13, P1046, DOI 10.1634/theoncologist.2008-0075; Therasse P, 2000, J NATL CANCER I, V92, P205, DOI 10.1093/jnci/92.3.205	14	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401019
C	Choudhury, S; Javdani, S; Srinivasa, S; Scherer, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Choudhury, Sanjiban; Javdani, Shervin; Srinivasa, Siddhartha; Scherer, Sebastian			Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Robotic motion-planning problems, such as a UAV flying fast in a partially-known environment or a robot arm moving around cluttered objects, require finding collision-free paths quickly. Typically, this is solved by constructing a graph, where vertices represent robot configurations and edges represent potentially valid movements of the robot between these configurations. The main computational bottlenecks are expensive edge evaluations to check for collisions. State of the art planning methods do not reason about the optimal sequence of edges to evaluate in order to find a collision free path quickly. In this paper, we do so by drawing a novel equivalence between motion planning and the Bayesian active learning paradigm of decision region determination (DRD). Unfortunately, a straight application of existing methods requires computation exponential in the number of edges in a graph. We present BISECT, an efficient and near-optimal algorithm to solve the DRD problem when edges are independent Bernoulli random variables. By leveraging this property, we are able to significantly reduce computational complexity from exponential to linear in the number of edges. We show that BISECT outperforms several state of the art algorithms on a spectrum of planning problems for mobile robots, manipulators, and real flight data collected from a full scale helicopter. Open-source code and details can be found here: https://github.com/sanjibac/matlab_learning_collision_checking	[Choudhury, Sanjiban; Javdani, Shervin; Srinivasa, Siddhartha; Scherer, Sebastian] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Choudhury, S (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	sanjiban@cmu.edu; sjavdani@cmu.edu; siddh@cs.cmu.edu; basti@cs.cmu.edu	Jeong, Yongwook/N-7413-2016; Choudhury, Sanjiban/AAE-9760-2021	Choudhury, Sanjiban/0000-0003-2762-8888	ONR [N000141310821]	ONR(Office of Naval Research)	We would like to acknowledge the support from ONR grant N000141310821. We would like to thank Shushman Choudhury for insightful discussions and the 7D arm planning datasets. We would like to thank Oren Salzaman, Mohak Bhardwaj, Vishal Dugar and Paloma Sodhi for feedback on the paper.	Bohlin R., 2000, ICRA; Burns B., 2005, ICRA; Canny, 1988, COMPLEXITY ROBOT MOT; Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939; Chen Yuxin, 2015, AAAI; Choudhury Sanjiban, 2016, ICRA; Choudhury Shushman, 2016, IROS; Cover Hugh, 2013, ICRA; Dasgupta S, 2004, NIPS; Dellin Christopher M, 2016, EXPT ROBOTICS; Dellin Christopher M, 2016, ICAPS; Frieze A., 2015, INTRO RANDOM GRAPHS, DOI DOI 10.1017/CBO9781316339831; Gammell J. D., 2015, ICRA; Golovin D., 2011, J ARTIFICIAL INTELLI; Golovin D., 2010, NIPS; Hart P. E., 1968, IEEE T SYSTEMS SCI C; Howard R. A., 1966, IEEE T SYSTEMS SCI C; Javdani Shervin, 2014, AISTATS; Karaman S, 2011, INT J ROBOT RES, V30, P846, DOI 10.1177/0278364911406761; Kononenko Igor, 2001, ARTIFICIAL INTELLIGE; Krause A, 2009, J ARTIF INTELL RES, V35, P557, DOI 10.1613/jair.2737; LaValle S. M., 2001, IJRR; LaValle S. M., 2006, PLANNING ALGORITHMS; Likhachev M., 2015, 8 ANN S COMB SEARCH; Narayanan Venkatraman, 2017, ICAPS; Nielsen C.L., 2000, IROS; Pivtoraiko Mihail, 2009, J FIELD ROBOTICS; Yoshizumi T, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P923	28	1	1	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404068
C	Cohen, J; Heliou, A; Mertikopoulos, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cohen, Johanne; Heliou, Amelie; Mertikopoulos, Panayotis			Learning with Bandit Feedback in Potential Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DYNAMICS	This paper examines the equilibrium convergence properties of no-regret learning with exponential weights in potential games. To establish convergence with minimal information requirements on the player's side, we focus on two frameworks: the semi-bandit case (where players have access to a noisy estimate of their payoff vectors, including strategies they did not play), and the bandit case (where players are only able to observe their in-game, realized payoffs). In the semi-bandit case, we show that the induced sequence of play converges almost surely to a Nash equilibrium at a quasi-exponential rate. In the bandit case, the same result holds for epsilon-approximations of Nash equilibria if we introduce an exploration factor epsilon > 0 that guarantees that action choice probabilities never fall below epsilon. In particular, if the algorithm is run with a suitably decreasing exploration factor, the sequence of play converges to a bona fide Nash equilibrium with probability 1.	[Cohen, Johanne] Univ Paris Saclay, Univ Paris Sud, LRI CNRS, Paris, France; [Heliou, Amelie] Univ Paris Saclay, Inria, CNRS, AMIBio,LIX,Ecole Polytech, Paris, France; [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, Inria, LIG, F-38000 Grenoble, France	UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay; Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS)	Cohen, J (corresponding author), Univ Paris Saclay, Univ Paris Sud, LRI CNRS, Paris, France.	johanne.cohen@lri.fr; amelie.heliou@polytechnique.edu; panayotis.mertikopoulos@imag.fr	Jeong, Yongwook/N-7413-2016	Mertikopoulos, Panayotis/0000-0003-2026-9616	CNRS PEPS MASTODONS project ADOC; Huawei Innovation Research Program ULTRON; ANR JCJC project ORACLESS [ANR-16-CE33-0004-01]	CNRS PEPS MASTODONS project ADOC; Huawei Innovation Research Program ULTRON; ANR JCJC project ORACLESS(French National Research Agency (ANR))	Johanne Cohen was partially supported by the grant CNRS PEPS MASTODONS project ADOC 2017. Amelie Heliou and Panayotis Mertikopoulos gratefully acknowledge financial support from the Huawei Innovation Research Program ULTRON and the ANR JCJC project ORACLESS (grant no. ANR-16-CE33-0004-01).	Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; AUER P, 1995, P 36 ANN S FDN COMP; Benaim M., 1996, Journal of Dynamics and Differential Equations, V8, P141, DOI 10.1007/BF02218617; BENAIM M, 1999, SEMINAIRE PROBABILIT, V33; Blum A, 2007, ALGORITHMIC GAME THEORY, P79; Blum A, 2008, ACM S THEORY COMPUT, P373; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Coucheney P, 2015, MATH OPER RES, V40, P611, DOI 10.1287/moor.2014.0687; Foster D. J., 2016, ADV NEURAL INFORM PR, P4727; Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hofbauer J, 2003, B AM MATH SOC, V40, P479, DOI 10.1090/S0273-0979-03-00988-1; Kleinberg R, 2011, DISTRIB COMPUT, V24, P21, DOI 10.1007/s00446-011-0129-5; Kleinberg R, 2009, ACM S THEORY COMPUT, P533; Krichene W, 2015, SIAM J CONTROL OPTIM, V53, P1056, DOI 10.1137/140980685; Lasaulce Samson, 2010, GAME THEORY LEARNING; Mehta Ruta, 2015, ITCS 15 P 6 C INN TH; Mertikopoulos Panayotis, SODA 18 P 29 ANN ACM; Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044; Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481; Palaiopanos Gerasimos, 2017, ADV NEURAL INFORM PR, P5872; Roughgarden T, 2015, J ACM, V62, DOI 10.1145/2806883; Sandholm WilliamH., 2010, POPULATION GAMES EVO; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989; TAYLOR PD, 1978, MATH BIOSCI, V40, P145, DOI 10.1016/0025-5564(78)90077-9; Viossat Y, 2013, J ECON THEORY, V148, P825, DOI 10.1016/j.jet.2012.07.003; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406043
C	Colombo, N; Silva, R; Kang, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Colombo, Nicolo; Silva, Ricardo; Kang, Soong			Tomography of the London Underground: a Scalable Model for Origin-Destination Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MULTICAST-BASED INFERENCE; NETWORK TOMOGRAPHY; TRANSPORTATION	The paper addresses the classical network tomography problem of inferring local traffic given origin-destination observations. Focusing on large complex public transportation systems, we build a scalable model that exploits input-output information to estimate the unobserved link/station loads and the users' path preferences. Based on the reconstruction of the users' travel time distribution, the model is flexible enough to capture possible different path-choice strategies and correlations between users travelling on similar paths at similar times. The corresponding likelihood function is intractable for medium or large-scale networks and we propose two distinct strategies, namely the exact maximum-likelihood inference of an approximate but tractable model and the variational inference of the original intractable model. As an application of our approach, we consider the emblematic case of the London underground network, where a tap-in/tap-out system tracks the starting/exit time and location of all journeys in a day. A set of synthetic simulations and real data provided by Transport For London are used to validate and test the model on the predictions of observable and unobservable quantities.	[Colombo, Nicolo; Silva, Ricardo] UCL, Dept Stat Sci, London, England; [Silva, Ricardo] UCL, Alan Turing Inst, London, England; [Kang, Soong] UCL, Sch Management, London, England	University of London; University College London; University of London; University College London; University of London; University College London	Colombo, N (corresponding author), UCL, Dept Stat Sci, London, England.	nicolo.colombo@ucl.ac.uk; ricardo.silva@ucl.ac.uk; smkang@ucl.ac.uk	Jeong, Yongwook/N-7413-2016	colombo, nicolo/0000-0002-2822-4829	EPSRC [EP/N020723/1]; Alan Turing Institute under the EPSRC [EP/N510129/1]; Alan Turing Institute-Lloyd's Register Foundation programme on Data-Centric Engineering	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute-Lloyd's Register Foundation programme on Data-Centric Engineering	We thank Transport for London for kindly providing access to data. This work has been funded by a EPSRC grant EP/N020723/1. RS also acknowledges support by The Alan Turing Institute under the EPSRC grant EP/N510129/1 and the Alan Turing Institute-Lloyd's Register Foundation programme on Data-Centric Engineering.	Airoldi EM, 2013, J AM STAT ASSOC, V108, P149, DOI 10.1080/01621459.2012.756328; Banavar JR, 1999, NATURE, V399, P130, DOI 10.1038/20144; Bell MGH, 1997, TRANSPORTATION NETWO; BOELTER LMK, 1960, P NATL ACAD SCI USA, V46, P824, DOI 10.1073/pnas.46.6.824; Caceres R, 1999, IEEE T INFORM THEORY, V45, P2462, DOI 10.1109/18.796384; Canevet O, 2016, PR MACH LEARN RES, V48; Cao J, 2000, J AM STAT ASSOC, V95, P1063, DOI 10.2307/2669743; Castro R, 2004, STAT SCI, V19, P499, DOI 10.1214/088342304000000422; Chen M., 2016, ARXIV PREPRINT ARXIV; Christakis NA, 2013, STAT MED, V32, P556, DOI 10.1002/sim.5408; COATES M, 2000, ITC C IP TRAFF MOD M; Coates Mark, 2001, IEEE SIGNAL PROCESSI; Du JL, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1501; Eagle N, 2009, P NATL ACAD SCI USA, V106, P15274, DOI 10.1073/pnas.0900282106; Gershman Samuel, 2014, P COGNITIVE SCI SOC, V36; Gopalan Prem K, 2012, ADV NEURAL INFORM PR, P2249; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kumar Akshat, 2013, ARXIV13096841; Kurant M, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.138701; Lo Presti F, 2002, IEEE ACM T NETWORK, V10, P761, DOI 10.1109/TNET.2002.805026; Newman M., 2011, STRUCTURE DYNAMICS N, DOI 10.1515/9781400841356; Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480; Nielsen BF, 2014, TRANSPORTMETRICA A, V10, P502, DOI 10.1080/23249935.2013.795199; Nuzzolo A, 2013, IEEE INT C INTELL TR, P1894, DOI 10.1109/ITSC.2013.6728505; Rogers EM, 1981, COMMUNICATION NETWOR; Roth C, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0015923; Silva R, 2015, P NATL ACAD SCI USA, V112, P5643, DOI 10.1073/pnas.1412908112; Tebaldi C, 1998, J AM STAT ASSOC, V93, P557, DOI 10.2307/2670105; Tsang Y, 2002, INT CONF ACOUST SPEE, P2045; Vanderbei R. J., 1994, 9404 SOR PRINC U 9404 SOR PRINC U; Vandewiele G, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1469, DOI 10.1145/3041021.3051699; Vardi Y, 1996, J AM STAT ASSOC, V91, P365, DOI 10.2307/2291416; Wasserman S, 1994, SOCIAL NETWORK ANAL, DOI DOI 10.1017/CBO9780511815478; Willumsen L.G., 1978, ESTIMATION OD MATRIX; Xiaofang, 2011, COMPUTING SPATIAL TR; Yin HD, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0167126; Zhaoguang Pan, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7285868; Zheng Y, 2014, ACM T INTEL SYST TEC, V5, DOI 10.1145/2629592; ZHONG C, 2016, PLOS ONE, V11, DOI DOI 10.1371/JOURNAL.PONE.0149222	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403013
C	Derezinski, M; Warmuth, MK		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Derezinski, Michal; Warmuth, Manfred K.			Unbiased estimates for linear regression via volume sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				APPROXIMATION; MATRICES	Given a full rank matrix X with more columns than rows, consider the task of estimating the pseudo inverse X+ based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times X+inverted perpendicular X+. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of X. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from X. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.	[Derezinski, Michal; Warmuth, Manfred K.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA	University of California System; University of California Santa Cruz	Derezinski, M (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.	mderezin@ucsc.edu; manfred@ucsc.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1619271]	NSF(National Science Foundation (NSF))	Thanks to Daniel Hsu and Wojciech Kotlowski for many valuable discussions. This research was supported by NSF grant IIS-1619271.	Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287; BENTAL A, 1990, LINEAR ALGEBRA APPL, V139, P165, DOI 10.1016/0024-3795(90)90395-S; Boutsidis Christos, 2012, ABS12023505 CORR; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Deshpande A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1117, DOI 10.1145/1109557.1109681; Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; Gartrell M, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P349, DOI 10.1145/2959100.2959178; Guruswami V., 2012, P 23 ANN ACM SIAM S, P1207, DOI [10.1137/1.9781611973099.95, 10.1137/1.9781611973099, DOI 10.1137/1.9781611973099]; Hsu Daniel, 2017, ABS170507048 CORR; Kang B., 2013, ADV NEURAL INFORM PR, P2319; Kulesza Alex, 2011, ICML; Kulesza Alex, 2012, DETERMINANTAL POINT; Li Chunyuan, 2017, NIPS; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Sugiyama M, 2009, MACH LEARN, V75, P249, DOI 10.1007/s10994-009-5100-3	19	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403015
C	Diakonikolas, I; Grigorescu, E; Li, J; Natarajan, A; Onak, K; Schmidt, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Diakonikolas, Ilias; Grigorescu, Elena; Li, Jerry; Natarajan, Abhiram; Onak, Krzysztof; Schmidt, Ludwig			Communication-Efficient Distributed Learning of Discrete Probability Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DENSITY-ESTIMATION; MULTIVARIATE HISTOGRAMS; ALGORITHMS	We initiate a systematic investigation of distribution learning (density estimation) when the data is distributed across multiple servers. The servers must communicate with a referee and the goal is to estimate the underlying distribution with as few bits of communication as possible. We focus on non-parametric density estimation of discrete distributions with respect to the l(1) and ,l(2) norms. We provide the first non-trivial upper and lower bounds on the communication complexity of this basic estimation task in various settings of interest. Specifically, our results include the following: 1. When the unknown discrete distribution is unstructured and each server has only one sample, we show that any blackboard protocol (i.e., any protocol in which servers interact arbitrarily using public messages) that learns the distribution must essentially communicate the entire sample. 2. For the case of structured distributions, such as k-histograms and monotone distributions, we design distributed learning algorithms that achieve significantly better communication guarantees than the naive ones, and obtain tight upper and lower bounds in several regimes. Our distributed learning algorithms run in near-linear time and are robust to model misspecification. Our results provide insights on the interplay between structure and communication efficiency for a range of fundamental distribution estimation tasks.	[Diakonikolas, Ilias] USC, CS, Los Angeles, CA 90007 USA; [Grigorescu, Elena; Natarajan, Abhiram] Purdue Univ, CS, W Lafayette, IN 47907 USA; [Li, Jerry; Schmidt, Ludwig] MIT, EECS & CSAIL, Cambridge, MA 02139 USA; [Onak, Krzysztof] IBM Res Corp, Albany, NY USA	University of Southern California; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Massachusetts Institute of Technology (MIT); International Business Machines (IBM)	Diakonikolas, I (corresponding author), USC, CS, Los Angeles, CA 90007 USA.	diakonik@usc.edu; elena-g@purdue.edu; jerryzli@mit.edu; nataraj2@purdue.edu; konak@us.ibm.com; ludwigs@mit.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF-1652862, CCF-1618981, CCF-1649515]; Sloan Research Fellowship; NSF CAREER [CCF-1453261, CCF-1565235]; Google Faculty Research Award; NSF Graduate Research Fellowship; Purdue Research Foundation; Google PhD Fellowship	NSF(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Faculty Research Award(Google Incorporated); NSF Graduate Research Fellowship(National Science Foundation (NSF)); Purdue Research Foundation; Google PhD Fellowship(Google Incorporated)	The authors would like to thank the reviewers for their insightful and constructive comments. ID was supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. EG was supported by NSF Award CCF-1649515. JL was supported by NSF CAREER Award CCF-1453261, CCF-1565235, a Google Faculty Research Award, and an NSF Graduate Research Fellowship. AN was supported in part by a grant from the Purdue Research Foundation and NSF Awards CCF-1618981 and CCF-1649515. LS was funded by a Google PhD Fellowship.	Acharya J, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1278; Acharya J, 2015, PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P249, DOI 10.1145/2745754.2745772; [Anonymous], 2016, ABS160607384 CORR; Balcan M.F., 2012, P C LEARN THEOR, P26; BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488; Braverman M, 2016, ACM S THEORY COMPUT, P1011, DOI 10.1145/2897518.2897582; Chan SO, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P604, DOI 10.1145/2591796.2591848; Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380; Chan Siu-on, 2014, ADV NEURAL INFORM PR, P1844; Chaudhuri S., 1998, SIGMOD Record, V27, P436, DOI 10.1145/276305.276343; Daskalakis C., 2012, SODA, P1371; Daskalakis C, 2015, ALGORITHMICA, V72, P316, DOI 10.1007/s00453-015-9971-3; Daskalakis C, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1833; Daskalakis C, 2013, ANN IEEE SYMP FOUND, P217, DOI 10.1109/FOCS.2013.31; Daume Hal  III, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P154, DOI 10.1007/978-3-642-34106-9_15; Daume III H., 2012, ARTIF INTELL, P282; Devroye L, 2004, TEST, V13, P129, DOI 10.1007/BF02603004; Devroye L., 2012, COMBINATORIAL METHOD; Diakonikolas I, 2016, HDB OF BIG DATA, P267; Duchi J. C., 2014, ARXIV E PRINTS; FREEDMAN D, 1981, Z WAHRSCHEINLICHKEIT, V57, P453, DOI 10.1007/BF01025868; Fuller SH, 2011, FUTURE OF COMPUTING PERFORMANCE: GAME OVER OR NEXT LEVEL?, P1; Garg A., 2014, ADV NEURAL INFORM PR, V27, P2726, DOI [https://doi.org/10.1109/hipc.1997.634533, DOI 10.1109/HIPC.1997.634533]; Gilbert A. C., 2002, P 34 ANN ACM S THEOR, P389; Guha S, 2006, ACM T DATABASE SYST, V31, P396, DOI 10.1145/1132863.1132873; Hu Y, 2007, CIS: 2007 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY, PROCEEDINGS, P28, DOI 10.1109/CIS.2007.138; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Jordan M. I., 2016, CORR; Kannan R., 2014, P 27 C LEARN THEOR C, P1040; Klemela J, 2009, STAT SINICA, V19, P159; Koudas N., 2002, SIGMOD, P428; Kowalczyk W., 2004, ADV NEURAL INFORM PR, V17, P713; Liang Y., 2014, ADV NEURAL INFORM PR, P3113; Lugosi G, 1996, ANN STAT, V24, P687; N. R. Council, 2013, FRONT MASS DAT AN; Nowak R, 2003, INT CONF ACOUST SPEE, P836; Pearson K., 1895, PHILOS T ROYAL SOC A, P343, DOI [https://doi.org/10.1098/rsta.1895.0010, DOI 10.1098/RSTA.1895.0010]; Slavov V, 2014, VLDB J, V23, P51, DOI 10.1007/s00778-013-0314-1; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Zhang Y., 2013, NEURAL INFORM PROCES, P2328; Zhou MQ, 2012, PROC INT CONF DATA, P594, DOI 10.1109/ICDE.2012.19	41	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406045
C	Dimitrakakis, C; Parkes, DC; Radanovic, G; Tylkin, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dimitrakakis, Christos; Parkes, David C.; Radanovic, Goran; Tylkin, Paul			Multi-View Decision Processes: The Helper-AI Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider a two-player sequential game in which agents have the same reward function but may disagree on the transition probabilities of an underlying Markovian model of the world. By committing to play a specific policy, the agent with the correct model can steer the behavior of the other agent, and seek to improve utility. We model this setting as a multi-view decision process, which we use to formally analyze the positive effect of steering policies. Furthermore, we develop an algorithm for computing the agents' achievable joint policy, and we experimentally show that it can lead to a large utility increase when the agents' models diverge.	[Dimitrakakis, Christos] Chalmers Univ Technol, Gothenburg, Sweden; [Dimitrakakis, Christos] Univ Lille, Lille, France; [Parkes, David C.; Radanovic, Goran; Tylkin, Paul] Harvard Univ, Cambridge, MA 02138 USA	Chalmers University of Technology; Universite de Lille - ISITE; Universite de Lille; Harvard University	Dimitrakakis, C (corresponding author), Chalmers Univ Technol, Gothenburg, Sweden.; Dimitrakakis, C (corresponding author), Univ Lille, Lille, France.	christos.dimitrakakis@gmail.com; parkes@eecs.harvard.edu; gradanovic@g.harvard.edu; ptylkin@g.harvard.edu	Dimitrakakis, Christos/F-6404-2011	Dimitrakakis, Christos/0000-0002-5367-5189	People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA [608743]; Swedish national science foundation (VR); Future of Life Institute; SEAS TomKat fund; SNSF Early Postdoc Mobility fellowship	People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA; Swedish national science foundation (VR)(Swedish Research Council); Future of Life Institute; SEAS TomKat fund; SNSF Early Postdoc Mobility fellowship	The research has received funding from: the People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA grant agreement 608743, the Swedish national science foundation (VR), the Future of Life Institute, the SEAS TomKat fund, and a SNSF Early Postdoc Mobility fellowship.	Amir Ofra, 2016, IJCAI 2016; Bosansky B, 2016, ARTIF INTELL, V237, P1, DOI 10.1016/j.artint.2016.03.005; Bosansky Branislav, 2015, COMPUTATION STACKELB; Elmalech A, 2015, AAAI CONF ARTIF INTE, P1313; Even-Dar E, 2003, LECT NOTES ARTIF INT, V2777, P581, DOI 10.1007/978-3-540-45167-9_42; Gal Y, 2008, J ARTIF INTELL RES, V33, P109, DOI 10.1613/jair.2503; Guo X, 2013, ADV NEURAL INFORM PR, P2130; Letchford Joshua, 2012, P 26 AAAI C ART INT; Licklider JCR., 1960, IRE T HUM FACT ELECT, VHFE1, P4, DOI [10.1109/THFE2.1960.4503259, DOI 10.1109/THFE2.1960.4503259]; Littman M. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P394; Mansour Y, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P401; Russell S., 2016, COOPERATIVE INVERSE; Sorg J., 2010, P 27 INT C MACH LEAR, P1007; Zhang HQ, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P295; Zinkevich Martin, 2005, ADV NEURAL INFORM PR	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405051
C	Ding, Y; Kondor, R; Eskreis-Winkler, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ding, Yi; Kondor, Risi; Eskreis-Winkler, Jonathan			Multiresolution Kernel Approximation for Gaussian Process Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MATRIX	Gaussian process regression generally does not scale to beyond a few thousands data points without applying some sort of kernel approximation method. Most approximations focus on the high eigenvalue part of the spectrum of the kernel matrix, K, which leads to bad performance when the length scale of the kernel is small. In this paper we introduce Multiresolution Kernel Approximation (MKA), the first true broad bandwidth kernel approximation algorithm. Important points about MKA are that it is memory efficient, and it is a direct method, which means that it also makes it easy to approximate K-1 and det(K).	[Ding, Yi; Kondor, Risi] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA; [Kondor, Risi; Eskreis-Winkler, Jonathan] Univ Chicago, Dept Stat, Chicago, IL 60637 USA	University of Chicago; University of Chicago	Ding, Y (corresponding author), Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.	dingy@uchicago.edu; risi@uchicago.edu; eskreiswinkler@uchicago.edu	Jeong, Yongwook/N-7413-2016					Abou-Rjeili A, 2006, P 20 INT C PAR DISTR; Allard William K, 2012, APPL COMPUTATIONAL H; Ambikasaran Sivaram, 2015, ARXIV14036015V2; Berthet Q., 2013, C LEARN THEOR, P1046; Borm Steffen, 2007, ECML; Chandrasekaran S, 2005, CALCOLO, V42, P171, DOI 10.1007/s10092-005-0103-3; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Gittens A., 2013, INT C MACHINE LEARNI, P567; Greengard L., 1987, J COMPUT PHYS; Hackbusch W, 1999, COMPUTING, V62, P89, DOI 10.1007/s006070050015; Hackbusch W., 2000, LECT APPL MATH, P9; Jin Rong, 2013, IEEE T INF THEORY; Kondor Risi, 2014, ICML; Kuleshov V., 2013, J MACHINE LEARNING R, P1418; Kumar Sanjiv, 2009, NIPS; Liang Y., 2014, ADV NEURAL INFORM PR, P3113; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahimi A., 2008, NIPS; Rajani N., 2015, 1 HIGH PERF GRAPH MI; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Savas B., 2011, P SIAM INT C DAT MIN; Si S., 2014, ICML; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Snelson E., 2005, NIPS; Stein M. L., 1999, INTERPOLATION SPATIA; Sun SL, 2015, INFORM FUSION, V26, P36, DOI 10.1016/j.inffus.2015.03.001; Teneva Nedelina, 2016, P 19 INT C AR INT ST; Wang R., 2015, ARXIV150500398; Wang Shusen, 2014, AISTATS; Williams C, 2001, ADV NEURAL INFORM PR, V13; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Zhang YX, 2013, CHINESE PHYS LETT, V30, DOI 10.1088/0256-307X/30/4/043101; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	35	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403078
C	Dutil, F; Gulcehre, C; Trischler, A; Bengio, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dutil, Francis; Gulcehre, Caglar; Trischler, Adam; Bengio, Yoshua			Plan, Attend, Generate: Planning for Sequence-to-Sequence Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We investigate the integration of a planning mechanism into sequence-to-sequence models using attention. We develop a model which can plan ahead in the future when it computes its alignments between input and output sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the recently proposed strategic attentive reader and writer (STRAW) model for Reinforcement Learning. Our proposed model is end-to-end trainable using primarily differentiable operations. We show that it outperforms a strong baseline on character-level translation tasks from WMT' 15, the algorithmic task of finding Eulerian circuits of graphs, and question generation from the text. Our analysis demonstrates that the model computes qualitatively intuitive alignments, converges faster than the baselines, and achieves superior performance with fewer parameters.	[Dutil, Francis; Gulcehre, Caglar; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada; [Trischler, Adam] Microsoft Res Maluuba, Montreal, PQ, Canada	Universite de Montreal	Dutil, F (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.	frdutil@gmail.com; ca9lar@gmail.com; adam.trischler@microsoft.com; yoshua.umontreal@gmail.com	Jeong, Yongwook/N-7413-2016					[Anonymous], 2016, P ADV NEUR INF PROC; [Anonymous], 2013, COMPUT SCI; Ba L.J, 2016, P C WORKSH NEUR INF; Bahdanau D., 2016, ARXIV160707086; Bahdanau Dzmitry, 2015, ICLR 2015; Bengio Yoshua, 2013, ARXIV; Cho K., 2014, 8 WORKSH SYNT SEM ST, P103, DOI DOI 10.3115/V1/W14-4012; Cho K, 2014, C EMP METH NAT LANG, DOI 10.3115/v1/D14-1179; Chung J., 2016, ARXIV160306147; Dietterich Thomas G, 2000, HIERARCHICAL REINFOR; Gulcehre C, 2016, ARXIV160308148; Gulcehre Caglar, 2017, ARXIV170108718; Jang Eric, 2017, P 5 INT C LEARN REPR; Kalchbrenner Nal, 2016, ARXIV161010099; Lee Jason, 2016, ARXIV161003017; Luo Y., 2016, ICASSP; Luong Minh-Thang, 2016, ARXIV160400788; Maddison Chris J, 2016, ARXIV161100712; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Rajpurkar Pranav, 2016, P EMNLP; Sennrich Rico, 2015, ARXIV150807909; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang ZJ, 2016, ADV SOC SCI EDUC HUM, V64, P1480; Yuan X., 2017, P 2 WORKSH REPR LEAR, P15, DOI [10.18653/v1/W17-2603, DOI 10.18653/V1/W17-2603]	29	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405054
C	Falahatgar, M; Ohannessian, M; Orlitsky, A; Pichapati, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Falahatgar, Moein; Ohannessian, Mesrob; Orlitsky, Alon; Pichapati, Venkatadheeraj			The power of absolute discounting: all-dimensional distribution estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Categorical models are a natural fit for many problems. When learning the distribution of categories from samples, high-dimensionality may dilute the data. Minimax optimality is too pessimistic to remedy this issue. A serendipitously discovered estimator, absolute discounting, corrects empirical frequencies by subtracting a constant from observed categories, which it then redistributes among the unobserved. It outperforms classical estimators empirically, and has been used extensively in natural language modeling. In this paper, we rigorously explain the prowess of this estimator using less pessimistic notions. We show that (1) absolute discounting recovers classical minimax KL-risk rates, (2) it is adaptive to an effective dimension rather than the true dimension, (3) it is strongly related to the Good-Turing estimator and inherits its competitive properties. We use power-law distributions as the cornerstone of these results. We validate the theory via synthetic data and an application to the Global Terrorism Database.	[Falahatgar, Moein; Orlitsky, Alon; Pichapati, Venkatadheeraj] UCSD, La Jolla, CA 92093 USA; [Ohannessian, Mesrob] TTIC, Chicago, IL USA	University of California System; University of California San Diego	Falahatgar, M (corresponding author), UCSD, La Jolla, CA 92093 USA.	moein@ucsd.edu; mesrob@gmail.com; alon@ucsd.edu; dheerajpv7@ucsd.edu	Jeong, Yongwook/N-7413-2016		NSF [CIF-1564355, CIF-1619448]	NSF(National Science Foundation (NSF))	We thank Vaishakh Ravindrakumar for very helpful suggestions, and NSF for supporting this work through grants CIF-1564355 and CIF-1619448.	Acharya J., 2013, J MACH LEARN RES P T, P764; Allahverdyan AE, 2013, PHYS REV E, V88, DOI 10.1103/PhysRevE.88.062804; Boucheron S, 2015, IEEE T INFORM THEORY, V61, P4948, DOI 10.1109/TIT.2015.2455058; Braess D, 2004, J APPROX THEORY, V128, P187, DOI 10.1016/j.jat.2004.04.010; Chen S.F., 1996, P 34 ANN M ASS COMPU, P310, DOI DOI 10.3115/981863.981904; Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111; Falahatgar M., 2016, NIPS, P4860; Falahatgar M, 2015, IEEE INT SYMP INFO, P2001, DOI 10.1109/ISIT.2015.7282806; Favaro S, 2016, BIOMETRICS, V72, P136, DOI 10.1111/biom.12366; Gale William A., 1995, J QUANT LINGUIST, V2, P217, DOI DOI 10.1080/09296179508590051; Hamou Anna Ben, 2017, CONCENTRATION INEQUA; Jozefowicz Rafal, 2016, ARXIV160202410; Katz Slava M., 1987, ESTIMATION PROBABILI; KNESER R, 1995, INT CONF ACOUST SPEE, P181, DOI 10.1109/ICASSP.1995.479394; LaFree Gary, 2016, GLOB TERR DAT; Mercer R., 1985, IBM TECHNICAL DISCLO, V28, P2591; Natalini P, 2000, MATH INEQUAL APPL, V3, P69; Neuman Edward, 2013, RESULTS MATH, P1; NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001; Ohannessian M. I., 2012, J MACH LEARN RES TRA, P21; Orlitsky A, 2003, SCIENCE, V302, P427, DOI 10.1126/science.1088284; Orlitsky A., 2015, ADV NEURAL INFORM PR, P2143; Paninski L, 2004, P ADV NEUR INF PROC; Pitman J, 1997, ANN PROBAB, V25, P855; Smith FA, 2003, ECOLOGY, V84, P3403, DOI 10.1890/02-9003; Teh YW, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P985; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Valiant G., 2015, ARXIV150405321; Zipf George Kingsley, 1935, PSYCHOBIOLOGY LANGUA	30	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406070
C	Fang, L; Yang, F; Dong, W; Guan, T; Qiao, CM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fang, Le; Yang, Fan; Dong, Wen; Guan, Tong; Qiao, Chunming			Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INFERENCE	Technological breakthroughs allow us to collect data with increasing spatio-temporal resolution from complex interaction systems. The combination of high-resolution observations, expressive dynamic models, and efficient machine learning algorithms can lead to crucial insights into complex interaction dynamics and the functions of these systems. In this paper, we formulate the dynamics of a complex interacting network as a stochastic process driven by a sequence of events, and develop expectation propagation algorithms to make inferences from noisy observations. To avoid getting stuck at a local optimum, we formulate the problem of minimizing Be the free energy as a constrained primal problem and take advantage of the concavity of dual problem in the feasible domain of dual variables guaranteed by duality theorem. Our expectation propagation algorithms demonstrate better performance in inferring the interaction dynamics in complex transportation networks than competing models such as particle filter, extended Kalman filter, and deep neural networks.	[Fang, Le; Yang, Fan; Dong, Wen; Guan, Tong; Qiao, Chunming] Univ Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Fang, L (corresponding author), Univ Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.	lefang@buffalo.edu; fyang24@buffalo.edu; wendong@buffalo.edu; tongguan@buffalo.edu; qiao@buffalo.edu	Jeong, Yongwook/N-7413-2016; Dong, Wen/AAQ-6057-2021	Dong, Wen/0000-0001-8923-2227				Arkin A, 1998, GENETICS, V149, P1633; Balmer M., 2009, MULTIAGENT SYSTEMS T, P57, DOI [DOI 10.4018/978-1-60566-226-8.CH003, 10.4018/978-1-60566-226-8.ch003]; Boyd S, 2004, CONVEX OPTIMIZATION; Del Moral P., 1996, MARKOV PROCESSES REL, V2, P555; Doucet A., 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.; Friston KJ, 2003, NEURAL NETWORKS, V16, P1325, DOI 10.1016/j.neunet.2003.06.005; Gillespie DT, 2007, ANNU REV PHYS CHEM, V58, P35, DOI 10.1146/annurev.physchem.58.032806.104637; Golightly Andrew, 2013, Methods Mol Biol, V1021, P169, DOI 10.1007/978-1-62703-450-0_9; GRASSMANN WK, 1977, COMPUT OPER RES, V4, P47, DOI 10.1016/0305-0548(77)90007-7; Guan T., 2017, P 2017 IEEE WIR COMM, P1, DOI [10.1109/WCNC.2017.7925446, DOI 10.1109/WCNC.2017.7925446]; Heskes T., 2002, UNCERTAINTY ARTIFICI, P216; Julier SJ, 2004, P IEEE, V92, P401, DOI 10.1109/JPROC.2003.823141; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Minka T., 2001, EP ENERGY FUNCTION M; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rao V., 2012, ARXIV12023760; Tebaldi C, 1998, J AM STAT ASSOC, V93, P557, DOI 10.2307/2670105; Vrettas MD, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.012148; Welling M., 2001, P 17 C UNC ART INT, P554; Wen Dong, 2012, ARXIV12104864; Wilkinson DJ, 2011, STOCHASTIC MODELLING; Xu Zhen, 2016, ADV NEURAL INFORM PR, P2775; Yang F, 2017, LECT NOTES COMPUT SC, V10354, P193, DOI 10.1007/978-3-319-60240-0_23; Yedidia J., 2003, EXPLORING ARTIFICIAL, P236	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402008
C	Fletcher, AK; Sahraee-Ardakan, M; Rangan, S; Schniter, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fletcher, Alyson K.; Sahraee-Ardakan, Mojtaba; Rangan, Sundeep; Schniter, Philip			Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MESSAGE-PASSING ALGORITHMS; APPROXIMATE; INFERENCE	We consider the problem of estimating a random vector x from noisy linear measurements y = Ax + w in the setting where parameters theta on the distribution of x and w must be learned in addition to the vector x. This problem arises in a wide range of statistical learning and linear inverse problems. Our main contribution shows that a computationally simple iterative message passing algorithm can provably obtain asymptotically consistent estimates in a certain high-dimensional large system limit (LSL) under very general parametrizations. Importantly, this LSL applies to all right-rotationally random A - a much larger class of matrices than Lid. sub-Gaussian matrices to which many past message passing approaches are restricted. In addition, a simple testable condition is provided in which the mean square error (MSE) on the vector x matches the Bayes optimal MSE predicted by the replica method. The proposed algorithm uses a combination of Expectation-Maximization (EM) with a recently-developed Vector Approximate Message Passing (VAMP) technique. We develop an analysis framework that shows that the parameter estimates in each iteration of the algorithm converge to deterministic limits that can be precisely predicted by a simple set of state evolution (SE) equations. The SE equations, which extends those of VAMP without parameter adaptation, depend only on the initial parameter estimates and the statistical properties of the problem and can be used to predict consistency and precisely characterize other performance measures of the method.	[Fletcher, Alyson K.] UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Sahraee-Ardakan, Mojtaba] UC Los Angeles, Dept EE, Los Angeles, CA USA; [Rangan, Sundeep] NYU, Dept ECE, New York, NY 10003 USA; [Schniter, Philip] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; New York University; University System of Ohio; Ohio State University	Fletcher, AK (corresponding author), UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.	akfletcher@ucla.edu; msahraee@ucla.edu; srangan@nyu.edu; schniter@ece.osu.edu	Fletcher, Alyson K/C-3226-2015; Rangan, Sundeep/AAH-2526-2020; Schniter, Philip/X-3346-2019; Jeong, Yongwook/N-7413-2016	Fletcher, Alyson K/0000-0002-3756-6580; Schniter, Philip/0000-0003-0939-7545; 	National Science Foundation [1254204, 1738286, 1116589, 1302336, 1547332, CCF-1527162]; Office of Naval Research [N00014-15-1-2677]; NYU WIRELESS	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); NYU WIRELESS	A. K. Fletcher and M. Saharee-Ardakan were supported in part by the National Science Foundation under Grants 1254204 and 1738286 and the Office of Naval Research under Grant N00014-15-1-2677. S. Rangan was supported in part by the National Science Foundation under Grants 1116589, 1302336, and 1547332, and the industrial affiliates of NYU WIRELESS. The work of P. Schniter was supported in part by the National Science Foundation under Grant CCF-1527162.	Barbier J., 2016, ARXIV160702335; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Fletcher A., 2017, RIGOROUS DYNAMICS CO; Fletcher A. K., 2017, P IEEE ICASSP; Fletcher A, 2016, IEEE INT SYMP INFO, P190, DOI 10.1109/ISIT.2016.7541287; Ghahramani Z., 2014, ADV NEURAL INFORM PR, V27, P2843; Heskes T, 2004, ADV NEUR IN, V16, P353; Kamilov US, 2014, IEEE T INFORM THEORY, V60, P2969, DOI 10.1109/TIT.2014.2309005; Manoel A, 2015, PR MACH LEARN RES, V37, P1123; Opper M, 2005, J MACH LEARN RES, V6, P2177; Opper M., 2004, P NIPS, P1001; RANGAN S, 2011, P IEEE INT S INF THE, P2174; Rangan S., 2017, P IEEE ISIT JUN; Rangan S, 2017, IEEE T INFORM THEORY, V63, P676, DOI 10.1109/TIT.2016.2619373; Rangan S, 2014, IEEE INT SYMP INFO, P236, DOI 10.1109/ISIT.2014.6874830; Rangan S, 2012, IEEE T INFORM THEORY, V58, P1902, DOI 10.1109/TIT.2011.2177575; Reeves G., 2016, P IEEE ISIT; Seeger M., 2011, INT C ART INT STAT, P652; Seeger MW, 2008, J MACH LEARN RES, V9, P759; Shawe-Taylor J., 2011, P NEUR INF PROC SYST, P2555; Takeuchi K., 2017, P IEEE ISIT JUN; Tulino AM, 2013, IEEE T INFORM THEORY, V59, P4243, DOI 10.1109/TIT.2013.2250578; van den Berg E, 2008, SIAM J SCI COMPUT, V31, P890, DOI 10.1137/080714488; Vila J, 2015, INT CONF ACOUST SPEE, P2021, DOI 10.1109/ICASSP.2015.7178325; Vila JP, 2014, IEEE T SIGNAL PROCES, V62, P4689, DOI 10.1109/TSP.2014.2337841; Vila JP, 2013, IEEE T SIGNAL PROCES, V61, P4658, DOI 10.1109/TSP.2013.2272287	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402058
C	Futami, F; Sato, I; Sugiyama, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Futami, Futoshi; Sato, Issei; Sugiyama, Masashi			Expectation Propagation for t-Exponential Family Using q-Algebra	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Exponential family distributions are highly useful in machine learning since their calculation can be performed efficiently through natural parameters. The exponential family has recently been extended to the t-exponential family, which contains Student-t distributions as family members and thus allows us to handle noisy data well. However, since the t-exponential family is defined by the deformed exponential, an efficient learning algorithm for the t-exponential family such as expectation propagation (EP) cannot be derived in the same way as the ordinary exponential family. In this paper, we borrow the mathematical tools of q-algebra from statistical physics and show that the pseudo additivity of distributions allows us to perform calculation of t-exponential family distributions through natural parameters. We then develop an expectation propagation (EP) algorithm for the t-exponential family, which provides a deterministic approximation to the posterior or predictive distribution with simple moment matching. We finally apply the proposed EP algorithm to the Bayes point machine and Student-t process classification, and demonstrate their performance numerically.	[Futami, Futoshi; Sato, Issei; Sugiyama, Masashi] Univ Tokyo, RIKEN, Tokyo, Japan	RIKEN; University of Tokyo	Futami, F (corresponding author), Univ Tokyo, RIKEN, Tokyo, Japan.	futami@ms.k.u-tokyo.ac.jp; sato@k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022; Jeong, Yongwook/N-7413-2016	Sugiyama, Masashi/0000-0001-6658-6743; 	JST CREST [JPMJCR1403]; KAKENHI [17H00757]	JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	FF acknowledges support by JST CREST JPMJCR1403 and MS acknowledges support by KAKENHI 17H00757.	Bishop C.M, 2006, PATTERN RECOGN; Ding N., 2011, ADV NEURAL INFORM PR, P1494; Ding N., 2010, ADV NEURAL INF PROCE, V23, P514; Kim HC, 2008, LECT NOTES COMPUT SC, V5342, P896; Minka T., 2001, THESIS MIT CAMBRIDGE; Nivanen L, 2003, REP MATH PHYS, V52, P437, DOI 10.1016/S0034-4877(03)80040-X; Rasmussen C., 2006, GAUSSIAN PROCESSES M, V1; Seeger M. W., 2005, EXPECTATION PROPAGAT; Shah A, 2014, JMLR WORKSH CONF PRO, V33, P877; Suyari H, 2005, IEEE T INFORM THEORY, V51, P753, DOI 10.1109/TIT.2004.840862	11	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402029
C	Gong, CY; Huang, WB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gong, Chengyue; Huang, Win-bin			Deep Dynamic Poisson Factorization Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A new model, named as deep dynamic poisson factorization model, is proposed in this paper for analyzing sequential count vectors. The model based on the Poisson Factor Analysis method captures dependence among time steps by neural networks, representing the implicit distributions. Local complicated relationship is obtained from local implicit distribution, and deep latent structure is exploited to get the long-time dependence. Variational inference on latent variables and gradient descent based on the loss functions derived from variational distribution is performed in our inference. Synthetic datasets and real-world datasets are applied to the proposed model and our results show good predicting and fitting performance with interpretable latent structure.	[Gong, Chengyue; Huang, Win-bin] Peking Univ, Dept Informat Management, Beijing, Peoples R China	Peking University	Gong, CY (corresponding author), Peking Univ, Dept Informat Management, Beijing, Peoples R China.	cygong@pku.edu.cn; huangwb@pku.edu.cn	Jeong, Yongwook/N-7413-2016					Ahmed A., 2008, SDM; [Anonymous], NIPS; [Anonymous], 2014, ICLR; Ayan A., 2015, AISTATS; Blei D., 2004, NIPS; Bui T. D., ARXIV160507066; Bui T. D., 2016, ICML; Cong Y., 2017, ICML; Gan Z., 2015, AISTATS; Gan Z., 2015, ICML; Gong Yuyun, 2016, KDD; Gopalan P., ARXIV13111704; Gopalan Prem, 2015, UAI; Hoffman M., 2017, ICML; Lauly S., 2012, NIPS; Paisley J., 2015, PAMI; Ranganath R., 2014, AISTATS; Ricardo H., 2015, NIPS; Schein Aaron, 2016, NIPS; Zhao S., 2017, ICML; Zhou M., ARXIV160407464; Zhou M., 2012, AISTATS; Zhou M, 2012, ADV NEURAL INFORM PR; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211	25	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401068
C	Gorbach, NS; Bauer, S; Buhmann, JM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gorbach, Nico S.; Bauer, Stefan; Buhmann, Joachim M.			Scalable Variational Inference for Dynamical Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PARAMETER-ESTIMATION	Gradient matching is a promising tool for learning parameters and state dynamics of ordinary differential equations. It is a grid free inference approach, which, for fully observable systems is at times competitive with numerical integration. However, for many real-world applications, only sparse observations are available or even unobserved variables are included in the model description. In these cases most gradient matching methods are difficult to apply or simply do not provide satisfactory results. That is why, despite the high computational cost, numerical integration is still the gold standard in many applications. Using an existing gradient matching approach, we propose a scalable variational inference framework which can infer states and parameters simultaneously, offers computational speedups, improved accuracy and works well even under model misspecifications in a partially observable system.	[Gorbach, Nico S.; Bauer, Stefan; Buhmann, Joachim M.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Gorbach, NS (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	ngorbach@inf.ethz.ch; bauers@inf.ethz.ch; jbuhmann@inf.ethz.ch	Jeong, Yongwook/N-7413-2016; Buhmann, Joachim/AAU-4760-2020		Max Planck ETH Center for Learning Systems; SystemsX.ch project SignalX	Max Planck ETH Center for Learning Systems; SystemsX.ch project SignalX	This research was partially supported by the Max Planck ETH Center for Learning Systems and the SystemsX.ch project SignalX.	Archambeau C., 2008, ADV NEURAL INFORM PR; Babtie AC, 2014, P NATL ACAD SCI USA, V111, P18507, DOI 10.1073/pnas.1414026112; Barenco M, 2006, GENOME BIOL, V7, DOI 10.1186/gb-2006-7-3-r25; Calderhead Ben, 2008, NEURAL INFORM PROCES; Dondelinger Frank, 2013, INT C ART INT STAT A; Lorenz EN, 1998, J ATMOS SCI, V55, P399, DOI 10.1175/1520-0469(1998)055<0399:OSFSWO>2.0.CO;2; Lotka Alfred J, 1978, GOLDEN AGE THEORETIC, P274, DOI DOI 10.1007/978-3-642-50151-7_12; Lyons Simon, 2012, NEURAL INFORM PROCES; Macdonald B, 2015, FRONT BIOENG BIOTECH, V3, DOI 10.3389/fbioe.2015.00180; Macdonald Benn, 2015, INT C MACH LEARN ICM; Niu Mu, 2016, INT C MACH LEARN ICM; Ramsay JO, 2007, J ROY STAT SOC B, V69, P741, DOI 10.1111/j.1467-9868.2007.00610.x; Ruttor Andreas, 2013, NEURAL INFORM PROCES; Ruttor Andreas, 2010, AISTATS; Schillings C, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004457; Stephan KE, 2008, NEUROIMAGE, V42, P649, DOI 10.1016/j.neuroimage.2008.04.262; VARAH JM, 1982, SIAM J SCI STAT COMP, V3, P28, DOI 10.1137/0903003; Vrettas MD, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.012148; Vyshemirsky V, 2008, BIOINFORMATICS, V24, P833, DOI 10.1093/bioinformatics/btm607; Wang Yali, 2014, INT C MACH LEARN ICM	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404085
C	Gu, SX; Lillicrap, T; Ghahramani, Z; Turner, RE; Scholkopf, B; Levine, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gu, Shixiang; Lillicrap, Timothy; Ghahramani, Zoubin; Turner, Richard E.; Scholkopf, Bernhard; Levine, Sergey			Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning Shixiang	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.	[Gu, Shixiang] Univ Cambridge, Max Planck Inst, Cambridge, England; [Lillicrap, Timothy] DeepMind, London, England; [Ghahramani, Zoubin] Univ Cambridge, Uber AI Labs, Cambridge, England; [Turner, Richard E.] Univ Cambridge, Cambridge, England; [Scholkopf, Bernhard] Max Planck Inst, Munich, Bavaria, Germany; [Levine, Sergey] Univ Calif Berkeley, Berkeley, CA USA	Max Planck Society; University of Cambridge; University of Cambridge; University of Cambridge; Max Planck Society; University of California System; University of California Berkeley	Gu, SX (corresponding author), Univ Cambridge, Max Planck Inst, Cambridge, England.	sg717@cam.ac.uk; countzero@google.com; zoubin@eng.cam.ac.uk; ret26@cam.ac.uk; bs@tuebingen.mpg.de; sylevine@eecs.berkeley.edu			Cambridge-Tubingen PhD Fellowship; NSERC; Google Focused Research Award	Cambridge-Tubingen PhD Fellowship; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Google Focused Research Award(Google Incorporated)	This work is supported by generous sponsorship from Cambridge-Tubingen PhD Fellowship, NSERC, and Google Focused Research Award.	Brockman G., 2016, OPENAI GYM; Degris, 2012, ARXIV12054839, P179; Duan Y, 2016, PR MACH LEARN RES, V48; Gu S., 2017, ICLR; Jiang N, 2016, PR MACH LEARN RES, V48; Jie T., 2010, ADV NEURAL INFORM PR, P1000; Kakade S., 2002, PROC ICML, V2, P267; Levine S, 2016, J MACH LEARN RES, V17; Levine Sergey, 2013, ICML; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; ODonoghue B., 2017, ICLR; Peshkin L., 2002, P 19 INT C MACH LEAR; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Precup D., 2000, P 17 INT C MACH LEAR; Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317, DOI 10.1007/11564096_32; Ross SM., 2006, SIMULATION; Schneider J, 2003, P INT JOINT C ART IN; Schulman J., 2016, P INT C LEARNING REP; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2014, PR MACH LEARN RES, V32; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Thomas PS, 2014, PR MACH LEARN RES, V32; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wang Ziyu, 2017, 5 INT C LEARN REPR I; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	31	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403088
C	Hamilton, L; Koehler, F; Moitra, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hamilton, Linus; Koehler, Frederic; Moitra, Ankur			Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Markov random fields are a popular model for high-dimensional probability distributions. Over the years, many mathematical, statistical and algorithmic problems on them have been studied. Until recently, the only known algorithms for provably learning them relied on exhaustive search, correlation decay or various incoherence assumptions. Bresler [4] gave an algorithm for learning general Ising models on bounded degree graphs. His approach was based on a structural result about mutual information in Ising models. Here we take a more conceptual approach to proving lower bounds on the mutual information. Our proof generalizes well beyond Ising models, to arbitrary Markov random fields with higher order interactions. As an application, we obtain algorithms for learning Markov random fields on bounded degree graphs on n nodes with r-order interactions in n(r) time and log n sample complexity. Our algorithms also extend to various partial observation models.	[Hamilton, Linus; Koehler, Frederic] MIT, Dept Math, Cambridge, MA 02139 USA; [Moitra, Ankur] MIT, Dept Math & Comp Sci, Cambridge, MA 02139 USA; [Moitra, Ankur] MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Hamilton, L (corresponding author), MIT, Dept Math, Cambridge, MA 02139 USA.	luh@mit.edu; fkoehler@mit.edu; moitra@mit.edu	Jeong, Yongwook/N-7413-2016		Hertz Fellowship; NSF CAREER Award [CCF-1453261]; NSF [CCF-1565235]; David and Lucile Packard Fellowship; Alfred P. Sloan Fellowship	Hertz Fellowship; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); David and Lucile Packard Fellowship(The David & Lucile Packard Foundation); Alfred P. Sloan Fellowship(Alfred P. Sloan Foundation)	This work was supported in part by Hertz Fellowship.; This work was supported in part by NSF CAREER Award CCF-1453261, NSF Large CCF-1565235, a David and Lucile Packard Fellowship and an Alfred P. Sloan Fellowship.	Abbeel P, 2006, J MACH LEARN RES, V7, P1743; Anandkumar Anima, 2012, ADV NEURAL INFORM PR, P1052; Anandkumar A, 2012, ANN STAT, V40, P1346, DOI 10.1214/12-AOS1009; Bresler G, 2015, ACM S THEORY COMPUT, P771, DOI 10.1145/2746539.2746631; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Csiszar I, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P170; Dasarathy Gautam, 2016, J MACH LEARN RES; Dasgupta S, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P134; Jalali Ali, 2011, P 14 INT C ART INT S, P378; Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268; MARTINELLI F, 1994, COMMUN MATH PHYS, V161, P447, DOI 10.1007/BF02101929; Mossel E, 2009, PROBAB THEORY REL, V143, P401, DOI 10.1007/s00440-007-0131-9; O'Donnell R, 2014, ANAL BOOLEAN FUNCTIO; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Scholkopf B, 2006, ADV NEURAL INFORM PR, P817; Sly A, 2012, ANN IEEE SYMP FOUND, P361, DOI 10.1109/FOCS.2012.56; Sly A, 2010, ANN IEEE SYMP FOUND, P287, DOI 10.1109/FOCS.2010.34; Srebro N., 2001, P 17 C UNC ART INT U, P504; Valiant G, 2012, ANN IEEE SYMP FOUND, P11, DOI 10.1109/FOCS.2012.27; Vuffray M., 2016, ADV NEURAL INFORM PR, P2595	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402050
C	Haupt, J; Li, XG; Woodruff, DP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Haupt, Jarvis; Li, Xingguo; Woodruff, David P.			Near Optimal Sketching of Low-Rank Tensor Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DESCENT METHOD; DECOMPOSITIONS	We study the least squares regression problem min(Theta is an element of Rp1)(X...XpD)parallel to A(Theta) - b parallel to(2)(2), where Theta is a low-rank tensor, defined as Theta = Sigma(R)(r=1) theta((r))(1)circle...circle theta((r))(D), for vectors theta((r))(d)is an element of R-pd for all r is an element of [R] and d is an element of [D]. Here, circle denotes the outer product of vectors, and A(Theta) is a linear function on Theta. This problem is motivated by the fact that the number of parameters in Theta is only R. Sigma(D)(d=1)pd, which is significantly smaller than the Pi(D)(d=1)pd number of parameters in ordinary least squares regression.We consider the above CP decomposition model of tensors Theta, as well as the Tucker decomposition. For both models we show how to apply data dimensionality reduction techniques based on sparse random projections Phi is an element of R-mxn, with m << n, to reduce the problem to a much smaller problem min(Theta)parallel to Phi A(Theta) - Phi b parallel to(2)(2), for which parallel to Phi A(Theta) - Phi b parallel to(2 )(2)= (1 +/-epsilon)parallel to A(Theta) - b parallel to(2)(2) holds simultaneously for all Theta. We obtain a significantly smaller dimension and sparsity in the randomized linear mapping Phi than is possible for ordinary least squares regression. Finally, we give a number of numerical simulations supporting our theory.	[Haupt, Jarvis; Li, Xingguo] Univ Minnesota, Minneapolis, MN 55455 USA; [Li, Xingguo] Georgia Tech, Atlanta, GA 30332 USA; [Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of Minnesota System; University of Minnesota Twin Cities; University System of Georgia; Georgia Institute of Technology; Carnegie Mellon University	Li, XG (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.; Li, XG (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	jdhaupt@umn.edu; lixx1661@umn.edu; dwoodruf@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		University of Minnesota	University of Minnesota(University of Minnesota System)	The authors acknowledge support from University of Minnesota Startup Funding and Doctoral Dissertation Fellowship from University of Minnesota.	[Anonymous], 2006, GENERIC CHAINING UPP; Bahadori M.T., 2014, P 27 INT C NEURAL IN, V2, P3491; Bourgain J, 2015, GEOM FUNCT ANAL, V25, P1009, DOI 10.1007/s00039-015-0332-9; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Dasgupta A, 2010, ACM S THEORY COMPUT, P341; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Dirksen S., 2015, FDN COMPUTATIONAL MA, P1; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Guo WW, 2012, IEEE T IMAGE PROCESS, V21, P816, DOI 10.1109/TIP.2011.2165291; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Haupt Jarvis, 2017, ARXIX170907093; Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Li BX, 2013, CALCOLO, V50, P69, DOI 10.1007/s10092-012-0058-0; Li Xiaoshan, 2013, ARXIV13045637; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Nelson J, 2014, LECT NOTES COMPUT SC, V8572, P883; Park SW, 2007, IEEE T SYST MAN CY B, V37, P1156, DOI 10.1109/TSMCB.2007.904575; Raskutti Garvesh, 2015, ARXIV151201215; Romera-Paredes B., 2013, P 30 INT C MACHINE L, P1444; Rosset A, 2004, J DIGIT IMAGING, V17, P205, DOI 10.1007/s10278-004-1014-6; Shen Zhongmin, 2001, LECT FINSLER GEOMETR, V2001; Sidiropoulos Nicholas, 2017, IEEE T SIGNAL PROCES; Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0; Vershynin R., 2010, ARXIV10113027; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Yang Y, 2016, ADV NEUR IN, V29; Yu R, 2016, PR MACH LEARN RES, V48; Yu Rose, 2015, INT C MACH LEARN; Zhao QB, 2013, IEEE T PATTERN ANAL, V35, P1660, DOI 10.1109/TPAMI.2012.254; Zhou H, 2013, J AM STAT ASSOC, V108, P540, DOI 10.1080/01621459.2013.776499; Zhou Hua, 2013, MATLAB TENSORREG TOO	36	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403052
C	Hu, AJ; Negahban, SN		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hu, Addison J.; Negahban, Sahand N.			Minimax Estimation of Bandable Precision Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMAL RATES; COVARIANCE; CONVERGENCE; SELECTION	The inverse covariance matrix provides considerable insight for understanding statistical models in the multivariate setting. In particular, when the distribution over variables is assumed to be multivariate normal, the sparsity pattern in the inverse covariance matrix, commonly referred to as the precision matrix, corresponds to the adjacency matrix representation of the Gauss-Markov graph, which encodes conditional independence statements between variables. Minimax results under the spectral norm have previously been established for covariance matrices, both sparse and banded, and for sparse precision matrices. We establish minimax estimation bounds for estimating banded precision matrices under the spectral norm. Our results greatly improve upon the existing bounds; in particular, we find that the minimax rate for estimating banded precision matrices matches that of estimating banded covariance matrices. The key insight in our analysis is that we are able to obtain barely-noisy estimates of k X k subblocks of the precision matrix by inverting slightly wider blocks of the empirical covariance matrix along the diagonal. Our theoretical results are complemented by experiments demonstrating the sharpness of our bounds.	[Hu, Addison J.; Negahban, Sahand N.] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA	Yale University	Hu, AJ (corresponding author), Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA.	addison.hu@yale.edu; sahand.negahban@yale.edu	Jeong, Yongwook/N-7413-2016		NSF [DMS 1723128]	NSF(National Science Foundation (NSF))	We would like to thank Harry Zhou for stimulating discussions regarding matrix estimation problems. SN acknowledges funding from NSF Grant DMS 1723128.	Bickel PJ, 2011, J R STAT SOC B, V73, P711, DOI 10.1111/j.1467-9868.2011.00779.x; Boyd S, 2004, CONVEX OPTIMIZATION; Cai T. T., 2011, ARXIV11022233STAT; Cai TT, 2016, ELECTRON J STAT, V10, P1, DOI 10.1214/15-EJS1081; Cai TT, 2016, ANN STAT, V44, P455, DOI 10.1214/13-AOS1171; Cai TT, 2012, ANN STAT, V40, P2389, DOI 10.1214/12-AOS998; Cai TT, 2010, ANN STAT, V38, P2118, DOI 10.1214/09-AOS752; Friedman J., 2007, BIOSTATISTICS; Frith CD, 1995, HUM BRAIN MAPP, V3, P153, DOI 10.1002/hbm.460030209; Hosseini M. J., 2016, ADV NEURAL INFORM PR, P3808; Hu A. J., 2017, ARXIV171007006V1; Lauritzen S., 1996, OXFORD STAT SCI SERI; Lee K., 2017, ARXIV170701143STAT; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Padmanabhan N, 2016, MON NOT R ASTRON SOC, V460, P1567, DOI 10.1093/mnras/stw1042; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Saon G, 2011, INT CONF ACOUST SPEE, P5056; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; VISSER H, 1995, J CLIMATE, V8, P969, DOI 10.1175/1520-0442(1995)008<0969:TEARAI>2.0.CO;2; Woodbury M. A., 1950, 42 PRINC U STAT RES; Yu B., 1997, FESTSCHRIFT L LECAM, P423; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404093
C	Huang, WB; Harandi, M; Zhang, T; Fan, LJ; Sun, FC; Huang, JZ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Huang, Wenbing; Harandi, Mehrtash; Zhang, Tong; Fan, Lijie; Sun, Fuchun; Huang, Junzhou			Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				KERNELS	Linear Dynamical Systems (LDSs) are fundamental tools for modeling spatio-temporal data in various disciplines. Though rich in modeling, analyzing LDSs is not free of difficulty, mainly because LDSs do not comply with Euclidean geometry and hence conventional learning techniques can not be applied directly. In this paper, we propose an efficient projected gradient descent method to minimize a general form of a loss function and demonstrate how clustering and sparse coding with LDSs can be solved by the proposed method efficiently. To this end, we first derive a novel canonical form for representing the parameters of an LDS, and then show how gradient-descent updates through the projection on the space of LDSs can be achieved dexterously. In contrast to previous studies, our solution avoids any approximation in LDS modeling or during the optimization process. Extensive experiments reveal the superior performance of the proposed method in terms of the convergence and classification accuracy over state-of-the-art techniques.	[Huang, Wenbing; Huang, Junzhou] Tencent AI Lab, Bellevue, WA 98004 USA; [Harandi, Mehrtash; Zhang, Tong] CSIRO, Data61, Canberra, ACT, Australia; [Harandi, Mehrtash; Zhang, Tong] Australian Natl Univ, Canberra, ACT, Australia; [Huang, Wenbing; Fan, Lijie; Sun, Fuchun] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing, Peoples R China	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University; Tsinghua University	Huang, WB (corresponding author), Tencent AI Lab, Bellevue, WA 98004 USA.	helendhuang@tencent.com; mehrtash.harandi@data61.csiro.au; tong.zhang@anu.edu.cn; flj14@mails.tsinghua.edu.cn; fcsun@mail.tsinghua.edu.cn; joehhuang@tencent.com	Huang, Wenbing/AHB-1846-2022; Huang, Wenbing/AAI-7943-2021; Jeong, Yongwook/N-7413-2016; Harandi, Mehrtash/D-6586-2018	Huang, Wenbing/0000-0002-2566-4159; Huang, Wenbing/0000-0002-2566-4159; Harandi, Mehrtash/0000-0002-6937-6300	National Science Foundation of China (NSFC) [91420302, 91520201, 61210013, 61327809]; NSFC in project Crossmodal Learning [NSFC 61621136008/DFG TRR-169]; German Research of Foundation (DFG) in project Crossmodal Learning [NSFC 61621136008/DFG TRR-169]; National High-Tech Research and Development Plan [2015AA042306]; Australian Research Council's Discovery Projects funding scheme [DP150104645]	National Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); NSFC in project Crossmodal Learning; German Research of Foundation (DFG) in project Crossmodal Learning(German Research Foundation (DFG)); National High-Tech Research and Development Plan(National High Technology Research and Development Program of China); Australian Research Council's Discovery Projects funding scheme(Australian Research Council)	This research was supported in part by the National Science Foundation of China (NSFC) (Grant No: 91420302, 91520201,61210013 and 61327809), the NSFC and the German Research of Foundation (DFG) in project Crossmodal Learning (Grant No: NSFC 61621136008/DFG TRR-169), and the National High-Tech Research and Development Plan under Grant 2015AA042306. Besides, Tong Zhang was supported by Australian Research Council's Discovery Projects funding scheme (project DP150104645).	Afsari B., 2014, GEOMETRIC THEORY INF, P219; Afsari B, 2012, PROC CVPR IEEE, P2208, DOI 10.1109/CVPR.2012.6247929; BARRAUD AY, 1977, IEEE T AUTOMAT CONTR, V22, P883, DOI 10.1109/TAC.1977.1101604; Chan AB, 2005, PROC CVPR IEEE, P846; Chan AB, 2007, PROC CVPR IEEE, P208; Chan AB, 2010, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2010.5539878; Chan Antoni B., 2013, CLUSTERING DYNAMIC T, V35, P1606; De Cock K, 2002, SYST CONTROL LETT, V46, P265, DOI 10.1016/S0167-6911(02)00135-4; Derpanis KG, 2012, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2012.6247815; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; Duda R. O., 2012, PATTERN CLASSIFICATI; Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106; Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872; Ghanem B, 2010, LECT NOTES COMPUT SC, V6312, P223; Harandi M, 2015, INT J COMPUT VISION, V114, P113, DOI 10.1007/s11263-015-0833-x; Huang A., 2008, P 6 NZ COMP SCI RES, V4, P9; Huang Wenbing, 2016, IEEE C COMP VIS PATT; Huang Wenbing, 2016, P 25 INT JOINT C ART; Johansen S., 1995, LIKELIHOOD BASED INF, DOI DOI 10.1093/0198774508.001.0001; Kalman, 1996, COLL MATH J, V27, P2, DOI DOI 10.1080/07468342.1996.11973744; Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167; Martin RJ, 2000, IEEE T SIGNAL PROCES, V48, P1164, DOI 10.1109/78.827549; Mumtaz A, 2015, IEEE T PATTERN ANAL, V37, P697, DOI 10.1109/TPAMI.2014.2359432; Peteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009; Ravichandran A, 2013, IEEE T PATTERN ANAL, V35, P342, DOI 10.1109/TPAMI.2012.83; Saisan P, 2001, PROC CVPR IEEE, P58; Siddiqi S., 2007, ADV NEURAL INFORM PR; Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52; Vishwanathan SVN, 2007, INT J COMPUT VISION, V73, P95, DOI 10.1007/s11263-006-9352-0; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; Woolfe F, 2006, LECT NOTES COMPUT SC, V3952, P549; Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123; Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110	33	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403050
C	Imaizumi, M; Maehara, T; Hayashi, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Imaizumi, Masaaki; Maehara, Takanori; Hayashi, Kohei			On Tensor Train Rank Minimization: Statistical Efficiency and Scalable Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MATRIX COMPLETION	Tensor train (TT) decomposition provides a space-efficient representation for higher-order tensors. Despite its advantage, we face two crucial limitations when we apply the TT decomposition to machine learning problems: the lack of statistical theory and of scalable algorithms. In this paper, we address the limitations. First, we introduce a convex relaxation of the TT decomposition problem and derive its error bound for the tensor completion task. Next, we develop a randomized optimization method, in which the time complexity is as efficient as the space complexity is. In experiments, we numerically confirm the derived bounds and empirically demonstrate the performance of our method with a real higher-order tensor.	[Imaizumi, Masaaki] RIKEN Ctr Adv Intelligence Project, Inst Stat Math, Tokyo, Japan; [Maehara, Takanori] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan; [Hayashi, Kohei] RIKEN Ctr Adv Intelligence Project, Natl Inst Adv Ind Sci & Technol, Tokyo, Japan	Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan; RIKEN; RIKEN; National Institute of Advanced Industrial Science & Technology (AIST); RIKEN	Imaizumi, M (corresponding author), RIKEN Ctr Adv Intelligence Project, Inst Stat Math, Tokyo, Japan.	imaizumi@ism.ac.jp; takanori.maehara@riken.jp; hayashi.kohei@gmail.com	Jeong, Yongwook/N-7413-2016		JSPS [15J10206]; ONR [N62909-17-1-2138]	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); ONR(Office of Naval Research)	We thank Prof. Taiji Suzuki for comments that greatly improved the manuscript. M. Imaizumi is supported by Grant-in-Aid for JSPS Research Fellow (15J10206) from the JSPS. K. Hayashi is supported by ONR N62909-17-1-2138.	Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Chen J., 2017, ARXIV170104831; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Grasedyck L., 2015, ARXIV150900311; Hamilton James D., 1994, TIME SERIES ANAL, V2; Harshman R.A., 1970, MULTIMODAL FACTOR AN; Jurafsky D., 2014, SPEECH LANGUAGE PROC, V3; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Li P., 2006, P 12 ACM SIGKDD INT, P287, DOI DOI 10.1145/1150402.1150436; Lichman M., 2013, UCI MACHINE LEARNING; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Novikov A, 2016, ARXIV160503795; Novikov A, 2014, PR MACH LEARN RES, V32, P811; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; Oseledets I, 2010, LINEAR ALGEBRA APPL, V432, P70, DOI 10.1016/j.laa.2009.07.024; Phien H. N., 2016, ARXIV160101083; Suzuki T., 2016, P ADV NEUR INF PROC, V29, P3783; Tomioka R., 2011, ADV NEURAL INFORM PR; Tomioka R., 2010, ARXIV10100789; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; WANG W, 2016, ARXIV160905587; Yadong Mu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2609, DOI 10.1109/CVPR.2011.5995369; Zhang Z., 2016, T SIGNAL PROCESSING	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404001
C	Jang, PA; Loeb, AE; Davidow, MB; Wilson, AG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jang, Phillip A.; Loeb, Andrew E.; Davidow, Matthew B.; Wilson, Andrew Gordon			Scalable Levy Process Priors for Spectral Kernel Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Gaussian processes are rich distributions over functions, with generalization properties determined by a kernel function. When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a Levy process. The resulting distribution has support for all stationary covariances-including the popular RBF, periodic, and Matern kernels-combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of the art predictive performance. The proposed model also presents an approach to spectral regularization, as the Levy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for O (n) training and O (1) predictions. We perform extrapolations having reasonable uncertainty estimates on several benchmarks, show that the proposed model can recover flexible ground truth covariances and that it is robust to errors in initialization.	[Jang, Phillip A.; Loeb, Andrew E.; Davidow, Matthew B.; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Jang, PA (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.				Natural Sciences and Engineering Research Council of Canada [PGS-D 502888]; National Science Foundation [DGE 1144153, IIS-1563887]	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); National Science Foundation(National Science Foundation (NSF))	This work is supported in part by the Natural Sciences and Engineering Research Council of Canada (PGS-D 502888) and the National Science Foundation DGE 1144153 and IIS-1563887 awards.	Bochner Salomon, 1959, LECT FOURIER INTEGRA, V42; Clyde Merlise A, 2007, BAYESIAN STAT, V8, P91; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Green P. J., 2003, TRANSDIMENSIONAL MAR; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; HYNDMAN RJ, 2005, TIME SERIES DATA LIB; MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Turner RE, 2010, THESIS; Wilson A., 2013, P 30 INT C MACH LEAR, V28, P1075; Wilson A. G., 2013, INT C MACH LEARN ICM; Wilson A.G., 2014, COVARIANCE KERNELS F; Wilson Andrew Gordon, 2015, INT C MACH LEARN ICM; Wolpert RL, 2011, ANN STAT, V39, P1916, DOI 10.1214/11-AOS889	16	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404002
C	Jiang, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jiang, Heinrich			On the Consistency of Quick Shift	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MEAN SHIFT	Quick Shift is a popular mode-seeking and clustering algorithm. We present finite sample statistical consistency guarantees for Quick Shift on mode and cluster recovery under mild distributional assumptions. We then apply our results to construct a consistent modal regression algorithm.	[Jiang, Heinrich] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA	Google Incorporated	Jiang, H (corresponding author), Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	heinrich.jiang@gmail.com	Jeong, Yongwook/N-7413-2016					[Anonymous], 2010, ADV NEURAL INFORM PR; Arias-Castro E., 2015, J MACHINE LEARNING R; Chen YC, 2016, ANN STAT, V44, P489, DOI 10.1214/15-AOS1373; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Dasgupta S., 2014, ADV NEURAL INF PROCE, V27, P2555; Eldridge J., 2015, P 28 C LEARN THEOR, P588; Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226; HARTIGAN JA, 1981, J AM STAT ASSOC, V76, P388, DOI 10.2307/2287840; Jiang H, 2017, PR MACH LEARN RES, V54, P1197; Jiang H, 2017, PR MACH LEARN RES, V70; Jiang H, 2017, PR MACH LEARN RES, V70; KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225; Sriperumbudur B. K., 2012, J MACHINE LEARNING R, V22, P1090; TSYBAKOV A. B., 1990, PROBLEMY PEREDACHI I, V26, P38; Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52; Wang D., 2017, ARXIV170603113	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400005
C	Kuzborskij, I; Cesa-Bianchi, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kuzborskij, Ilja; Cesa-Bianchi, Nicolo			Nonparametric Online Regression while Learning the Metric	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GRADIENTS	We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix G of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret -on the same data sequence- in terms of the spectrum of G. As a preliminary step in our analysis, we extend a nonparametric online learning algorithm by Hazan and Megiddo enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.	[Kuzborskij, Ilja] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Cesa-Bianchi, Nicolo] Univ Milan, Dipartimento Informat, I-20135 Milan, Italy	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Milan	Kuzborskij, I (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	ilja.kuzborskij@gmail.com; nicolo.cesa-bianchi@unimi.it	Jeong, Yongwook/N-7413-2016; Cesa-Bianchi, Nicolò/C-3721-2013	Cesa-Bianchi, Nicolò/0000-0001-8477-4748	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [637076]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	Authors would like to thank Sebastien Gerchinovitz and Samory Kpotufe for useful discussions on this work. IK would like to thank Google for travel support. This work also was in parts funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 637076).	Allez R, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.046202; Bellet A., 2013, ARXIV PREPRINT ARXIV; Dong XM, 2008, J MATH ANAL APPL, V341, P1018, DOI 10.1016/j.jmaa.2007.10.044; Gaillard P., 2015, C LEARN THEOR COLT; Guo ZC, 2017, ADV COMPUT MATH, V43, P127, DOI 10.1007/s10444-016-9479-7; Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36; Jin R., 2009, C NEUR INF PROC SYST; Kpotufe S., 2016, J MACHINE LEARNING R, V17, P1; Kpotufe S., 2013, C NEUR INF PROC SYST; Krauthgamer R., 2004, P 15 ANN ACM SIAM S, P798; Mukherjee S, 2006, J MACH LEARN RES, V7, P519; Mukherjee S, 2006, J MACH LEARN RES, V7, P2481; Rakhlin A., 2014, C LEARN THEOR COLT; Trivedi S., 2014, C UNC ART INT UAI; Vovk V., 2006, INT C THEOR APPL MOD; Vovk V., 2006, CS0609045 ARXIV; Vovk V, 2007, MACH LEARN, V69, P193, DOI 10.1007/s10994-007-5021-y; Wang Y., 2012, C LEARN THEOR COLT; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; WILKINSON JH, 1965, ALGEBRAIC EIGENVALUE, V0087; Wu QA, 2010, J MACH LEARN RES, V11, P2175	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400064
C	Li, P; Slawski, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Ping; Slawski, Martin			Simple Strategies for Recovering Inner Products from Coarsely Quantized Random Projections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				JOHNSON-LINDENSTRAUSS	Random projections have been increasingly adopted for a diverse set of tasks in machine learning involving dimensionality reduction. One specific line of research on this topic has investigated the use of quantization subsequent to projection with the aim of additional data compression. Motivated by applications in nearest neighbor search and linear learning, we revisit the problem of recovering inner products (respectively cosine similarities) in such setting. We show that even under coarse scalar quantization with 3 to 5 bits per projection, the loss in accuracy tends to range from "negligible" to "moderate". One implication is that in most scenarios of practical interest, there is no need for a sophisticated recovery approach like maximum likelihood estimation as considered in previous work on the subject. What we propose herein also yields considerable improvements in terms of accuracy over the Hamming distance-based approach in Li et al. (ICML 2014) which is comparable in terms of simplicity.	[Li, Ping] Baidu Res, Sunnyvale, CA 94089 USA; [Li, Ping] Rutgers State Univ, New Brunswick, NJ 08901 USA; [Slawski, Martin] George Mason Univ, Dept Stat, Fairfax, VA 22030 USA	Baidu; Rutgers State University New Brunswick; George Mason University	Li, P (corresponding author), Baidu Res, Sunnyvale, CA 94089 USA.; Li, P (corresponding author), Rutgers State Univ, New Brunswick, NJ 08901 USA.	pingli98@gmail.com; mslawsk3@gmu.edu	Jeong, Yongwook/N-7413-2016		 [NSF-Bigdata-1419210];  [NSF-III-1360971]	; 	The work was partially supported by NSF-Bigdata-1419210, NSF-III-1360971. Ping Li also thanks Michael Mitzenmacher for helpful discussions.	Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Ailon N., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P557, DOI 10.1145/1132516.1132597; Ailon N, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2483699.2483701; Anderson T.W, 2003, INTRO MULTIVARIATE S; Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245, DOI 10.1145/502512.502546; Boufounos P., 2008, INFORM SCI SYSTEMS; Boutsidis C., 2010, ADV NEURAL INFORM PR, P298; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; Fradkin D., 2003, P 9 ACM SIGKDD INT C, P517; Gersho A., 1992, VECTOR QUANTIZATION; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jacques L, 2015, IEEE T INFORM THEORY, V61, P5012, DOI 10.1109/TIT.2015.2453355; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Kenthapadi Krishnaram, 2013, J PRIVACY CONFIDENTI, V5, P39; KIEFFER JC, 1983, IEEE T INFORM THEORY, V29, P42, DOI 10.1109/TIT.1983.1056622; Krahmer F, 2011, SIAM J MATH ANAL, V43, P1269, DOI 10.1137/100810447; Laska JN, 2012, IEEE T SIGNAL PROCES, V60, P3496, DOI 10.1109/TSP.2012.2194710; Li M, 2012, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2012.6343406; Li P, 2016, ADV NEURAL INFORM PR, P2756; Li P, 2006, LECT NOTES ARTIF INT, V4005, P635, DOI 10.1007/11776420_46; Li P, 2014, PR MACH LEARN RES, V32, P676; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Maillard O., 2009, ADV NEURAL INFORM PR, P1213; Matousek J, 2008, RANDOM STRUCT ALGOR, V33, P142, DOI 10.1002/rsa.20218; Rane S., 2013, SPIE OPTICAL ENG APP; Rane S, 2013, IEEE SIGNAL PROC MAG, V30, P18, DOI 10.1109/MSP.2012.2230221; Vempala S.S., 2005, RANDOM PROJECTION ME, DOI [10.1090/dimacs/065, DOI 10.1090/DIMACS/065]	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404062
C	Lim, CH; Wright, SJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lim, Cong Han; Wright, Stephen J.			k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VARIABLE SELECTION; MODEL SELECTION; REGRESSION	The k-support and OWL norms generalize the l(1) norm, providing better prediction accuracy and better handling of correlated variables. We study the norms obtained from extending the k-support norm and OWL norms to the setting in which there are overlapping groups. The resulting norms are in general NP-hard to compute, but they are tractable for certain collections of groups. To demonstrate this fact, we develop a dynamic program for the problem of projecting onto the set of vectors supported by a fixed number of groups. Our dynamic program utilizes tree decompositions and its complexity scales with the treewidth. This program can be converted to an extended formulation which, for the associated group structure, models the k-group support norms and an overlapping group variant of the ordered weighted l(1) norm. Numerical results demonstrate the efficacy of the new penalties.	[Lim, Cong Han; Wright, Stephen J.] Univ Wisconsin, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Lim, CH (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	clim9@wisc.edu; swright@cs.wisc.edu	Jeong, Yongwook/N-7413-2016		NSF [CMMI-1634597]; ONR Award [N00014-13-1-0129]; AFOSR [FA9550-13-1-0138]	NSF(National Science Foundation (NSF)); ONR Award; AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was supported by NSF award CMMI-1634597, ONR Award N00014-13-1-0129, and AFOSR Award FA9550-13-1-0138.	[Anonymous], 2016, TECHNICAL REPORT; Argyriou A., 2012, ADV NEURAL INFORM PR; AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Baldassarre L, 2016, IEEE T INFORM THEORY, V62, P6508, DOI 10.1109/TIT.2016.2602222; Dell Holger, 2016, P 11 INT S PAR EX CO, V30, P1, DOI [10.4230/LIPIcs.IPEC.2016.30, DOI 10.4230/LIPICS.IPEC.2016.30]; Downey R.G., 1999, MG COMP SCI; Downey R. G., 2013, TEXTS COMPUTER SCI; El Halabi M, 2015, JMLR WORKSH CONF PRO, V38, P223; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; Obozinski G., 2012, TECHNICAL REPORT; Oswal U, 2016, PR MACH LEARN RES, V48; Rao N., 2013, ADV NEURAL INFORM PR, P2202; Rao N., 2012, P 15 INT C ART INT S, P942; Rao N. S., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P1917, DOI 10.1109/ICIP.2011.6115845; Rao N, 2017, INT CONF ACOUST SPEE, P2402, DOI 10.1109/ICASSP.2017.7952587; Sankaran R, 2017, PR MACH LEARN RES, V54, P1123; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zeng X., 2014, ARXIV14094271	20	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400027
C	Liu, JZ; Coull, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Jeremiah Zhe; Coull, Brent			Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This work constructs a hypothesis test for detecting whether an data-generating function h : R-p -> R belongs to a specific reproducing kernel Hilbert space H-0, where the structure of H-0 is only partially known. Utilizing the theory of reproducing kernels, we reduce this hypothesis to a simple one-sided score test for a scalar parameter, develop a testing procedure that is robust against the mis-specification of kernel functions, and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method, we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test under different data-generating functions and estimation strategies for the null model. Our results reveal interesting connections between notions in machine learning (model underfit/overfit) and those in statistical inference (i.e. Type I error/power of hypothesis test), and also highlight unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference.	[Liu, Jeremiah Zhe; Coull, Brent] Harvard Univ, Dept Biostat, Cambridge, MA 02138 USA	Harvard University	Liu, JZ (corresponding author), Harvard Univ, Dept Biostat, Cambridge, MA 02138 USA.	zhl112@mail.harvard.edu; bcoull@hsph.harvard.edu	Jeong, Yongwook/N-7413-2016					ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Bodenham DA, 2016, STAT COMPUT, V26, P917, DOI 10.1007/s11222-015-9583-4; Cortes C., 2010, TWO STAGE LEARNING K; Cortes C., 2012, ARXIV12023712CSSTAT; Davies A., 2014, ARXIV14024293CSS; Elisseeff A., 2002, LEARNING THEORY PRAC; Evgeniou T, 2004, MACH LEARN, V55, P71, DOI 10.1023/B:MACH.0000019805.88351.60; Evgeniou T., 2000, P 17 INT C MACH LEAR, P271; Gu C., 2013, SMOOTHING SPLINE ANO; HARVILLE DA, 1977, J AM STAT ASSOC, V72, P320, DOI 10.2307/2286796; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Lin XH, 1997, BIOMETRIKA, V84, P309, DOI 10.1093/biomet/84.2.309; Liu D, 2007, BIOMETRICS, V63, P1079, DOI 10.1111/j.1541-0420.2007.00799.x; Maity A, 2011, BIOMETRICS, V67, P1271, DOI 10.1111/j.1541-0420.2011.01598.x; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Snoek J., 2012, ARXIV12062944CSSTAT; Wahba G., 1990, SPLINE MODELS OBSERV; Wilson A. G., 2015, ARXIV151102222CSSTAT; Zhang DW, 2003, BIOSTATISTICS, V4, P57, DOI 10.1093/biostatistics/4.1.57	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400076
C	McInerney, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		McInerney, James			An Empirical Bayes Approach to Optimizing Machine Learning Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					There is rapidly growing interest in using Bayesian optimization to tune model and inference hyperparameters for machine learning algorithms that take a long time to run. For example, Spearmint is a popular software package for selecting the optimal number of layers and learning rate in neural networks. But given that there is uncertainty about which hyperparameters give the best predictive performance, and given that fitting a model for each choice of hyperparameters is costly, it is arguably wasteful to "throw away" all but the best result, as per Bayesian optimization. A related issue is the danger of overfitting the validation data when optimizing many hyperparameters. In this paper, we consider an alternative approach that uses more samples from the hyperparameter selection procedure to average over the uncertainty in model hyperparameters. The resulting approach, empirical Bayes for hyperparameter averaging (EB-Hyp) predicts held-out data better than Bayesian optimization in two experiments on latent Dirichlet allocation and deep latent Gaussian models. EB-Hyp suggests a simpler approach to evaluating and deploying machine learning algorithms that does not require a separate validation data set and hyperparameter selection procedure.	[McInerney, James] Spotify Res, 45 W 18th St,7th Floor, New York, NY 10011 USA	Spotify	McInerney, J (corresponding author), Spotify Res, 45 W 18th St,7th Floor, New York, NY 10011 USA.	jamesm@spotify.com	Mcinerney, James/AAF-7234-2020; Jeong, Yongwook/N-7413-2016					Bergstra J, 2012, J MACH LEARN RES, V13, P281; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Brochu E, 2010, ARXIV PREPRINT ARXIV; Carlin BP, 2000, J AM STAT ASSOC, V95, P1286, DOI 10.2307/2669771; Choi T., 2004, POSTERIOR CONSISTENC; EFRON B, 1972, J AM STAT ASSOC, V67, P130, DOI 10.2307/2284711; Freund Y., 1999, JAPANESE SOC ART INT, V14, P1612; GPy, 2012, GAUSS PROC FRAM PYTH; Hensman J., 2013, P C UNC ART INT UAI, P282; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Huang G.B., 2008, WORKSHOP FACESREAL L; Li C., 2015, NIPS; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Osborne M., 2012, ADV NEURAL INF PROCE, V25, P1; Osborne M.A., 2010, THESIS OXFORD U OXFO; Rasmussen C. E., 2006, GAUSSIAN PROCESSES M, V2, P4; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Robbins H., 1955, H ROBBINS SELECTED P, P49; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Stein M.L., 1999, INTERPOLATION SPATIA; Swersky K., 2014, ARXIV14063896; Van Der Wart AW, 2008, ANN STAT, V36, P1435, DOI 10.1214/009053607000000613	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402074
C	Merdivan, E; Loghmani, MR; Geist, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Merdivan, Erinc; Loghmani, Mohammad Reza; Geist, Matthieu			Reconstruct & Crush Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This article introduces an energy-based model that is adversarial regarding data: it minimizes the energy for a given data distribution (the positive samples) while maximizing the energy for another given data distribution (the negative or unlabeled samples). The model is especially instantiated with autoencoders where the energy, represented by the reconstruction error, provides a general distance measure for unknown data. The resulting neural network thus learns to reconstruct data from the first distribution while crushing data from the second distribution. This solution can handle different problems such as Positive and Unlabeled (PU) learning or covariate shift, especially with imbalanced data. Using autoencoders allows handling a large variety of data, such as images, text or even dialogues. Our experiments show the flexibility of the proposed approach in dealing with different types of data in different settings: images with CIFAR-10 and CIFAR-100 (not-in-training setting), text with Amazon reviews (PU learning) and dialogues with Facebook bAbI (next response classification and dialogue completion).	[Merdivan, Erinc] AIT Austrian Inst Technol GmbH, Vienna, Austria; [Merdivan, Erinc] Univ Lorraine, LORIA, F-57070 Metz, France; [Merdivan, Erinc] Univ Paris Saclay, Cent Supelec, CNRS, F-57070 Metz, France; [Loghmani, Mohammad Reza] TU Wien, Vision4Robot Lab, ACIN, Vienna, Austria; [Geist, Matthieu] Univ Lorraine, F-57070 Metz, France; [Geist, Matthieu] CNRS, LIEC, UMR 7360, F-57070 Metz, France	Austrian Institute of Technology (AIT); Universite de Lorraine; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; Technische Universitat Wien; Universite de Lorraine; Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Earth Sciences & Astronomy (INSU); Universite de Lorraine	Merdivan, E (corresponding author), AIT Austrian Inst Technol GmbH, Vienna, Austria.; Merdivan, E (corresponding author), Univ Lorraine, LORIA, F-57070 Metz, France.; Merdivan, E (corresponding author), Univ Paris Saclay, Cent Supelec, CNRS, F-57070 Metz, France.	erinc.merdivan@ait.ac.at; loghmani@acin.tuwien.ac.at; matthieu.geist@univ-lorraine.fr			European Union Horizon2020 MSCA ITN ACROSSING project [616757]	European Union Horizon2020 MSCA ITN ACROSSING project	This work has been funded by the European Union Horizon2020 MSCA ITN ACROSSING project (GA no. 616757). The authors would like to thank the members of the project's consortium for their valuable inputs.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2006, IEEE T AUTOM SCI ENG; [Anonymous], 2003, IJCAI; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bing L, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P179, DOI 10.1109/icdm.2003.1250918; Bordes A., 2016, ABS160507683 CORR; BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Denis F, 1998, LECT NOTES ARTIF INT, V1501, P112; Geli F., 2015, EMNLP; GREENE WH, 1981, ECONOMETRICA, V49, P795, DOI 10.2307/1911523; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G, 1994, ADV NEURAL INFORM PR, V6, DOI DOI 10.1021/jp906511z; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Japkowicz N., 1995, IJCAI; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li X.-l., 2005, ECML; Liu C.-W., 2016, EMNLP; McAuley Julian, 2013, P 7 ACM C REC SYST A, DOI DOI 10.1145/2507157.2507163; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Yu H., 2002, KDD; Zhao J., 2017, ICLR	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404060
C	Messias, JV; Whiteson, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Messias, Joao, V; Whiteson, Shimon			Dynamic-Depth Context Tree Weighting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Reinforcement learning (RL) in partially observable settings is challenging because the agent's observations are not Markov. Recently proposed methods can learn variable-order Markov models of the underlying process but have steep memory requirements and are sensitive to aliasing between observation histories due to sensor noise. This paper proposes dynamic-depth context tree weighting (D2-CTW), a model-learning method that addresses these limitations. D2-CTW dynamically expands a suffix tree while ensuring that the size of the model, but not its depth, remains bounded. We show that D2-CTW approximately matches the performance of state-of-the-art alternatives at stochastic time-series prediction while using at least an order of magnitude less memory. We also apply D2-CTW to model-based RL, showing that, on tasks that require memory of past observations, D2-CTW can learn without prior knowledge of a good state representation, or even the length of history upon which such a representation should depend.	[Messias, Joao, V] Morpheus Labs, Oxford, England; [Whiteson, Shimon] Univ Oxford, Oxford, England	University of Oxford	Messias, JV (corresponding author), Morpheus Labs, Oxford, England.	jmessias@morpheuslabs.co.uk; shimon.whiteson@cs.ox.ac.uk	Jeong, Yongwook/N-7413-2016		European Commission [FP7-ICT-611153]	European Commission(European CommissionEuropean Commission Joint Research Centre)	This work was supported by the European Commission under the grant agreement FP7-ICT-611153 (TERESA).	Bakker B, 2001, INT C NEURAL INF PRO, V14, P1475; Begleiter R, 2004, J ARTIF INTELL RES, V22, P385, DOI 10.1613/jair.1491; BELL T, 1989, COMPUT SURV, V21, P557, DOI 10.1145/76894.76896; Bellemare MG, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3337; Bellemare MG, 2014, PR MACH LEARN RES, V32, P1458; Cleary JG, 1997, COMPUT J, V40, P67, DOI 10.1093/comjnl/40.2_and_3.67; Farias VF, 2010, IEEE T INFORM THEORY, V56, P2441, DOI 10.1109/TIT.2010.2043762; Hamilton W. L., 2013, ICML, P178; Holmes M. P., 2006, P 23 INT C MACH LEAR, P409; Hutter M., 2013, J MACHINE LEARNING R, V30; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; McCallum A. K., 1995, THESIS; Ron D, 1996, MACH LEARN, V25, P117, DOI 10.1007/BF00114008; Singh S., 2004, P 20 C UNCERTAINTY A, P512; Stone P, 2015, AAAI FALL S SERIES; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tjalkens T. J., 1993, Proceedings of the Fourteenth Symposium on Information Theory in the Benelux, P128; Veness J, 2012, IEEE DATA COMPR CONF, P327, DOI 10.1109/DCC.2012.39; Veness J, 2011, J ARTIF INTELL RES, V40, P95, DOI 10.1613/jair.3125; Volf P. A. J., 2002, WEIGHTING TECHNIQUES; Wierstra D, 2007, LECT NOTES COMPUT SC, V4668, P697; Willems FMJ, 1998, IEEE T INFORM THEORY, V44, P792, DOI 10.1109/18.661523; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012; Wood F, 2011, COMMUN ACM, V54, P91, DOI 10.1145/1897816.1897842	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403039
C	Mitrovic, S; Bogunovic, I; Norouzi-Fard, A; Tarnawski, J; Cevher, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mitrovic, Slobodan; Bogunovic, Ilija; Norouzi-Fard, Ashkan; Tarnawski, Jakub; Cevher, Volkan			Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the classical problem of maximizing a monotone submodular function subject to a cardinality constraint k, with two additional twists: (i) elements arrive in a streaming fashion, and (ii) m items from the algorithm's memory are removed after the stream is finished. We develop a robust submodular algorithm STAR-T. It is based on a novel partitioning structure and an exponentially decreasing thresholding rule. STAR-T makes one pass over the data and retains a short but robust summary. We show that after the removal of any m elements from the obtained summary, a simple greedy algorithm STAR-T-GREEDY that runs on the remaining elements achieves a constant-factor approximation guarantee. In two different data summarization tasks, we demonstrate that it matches or outperforms existing greedy and streaming methods, even if they are allowed the benefit of knowing the removed subset in advance.	[Mitrovic, Slobodan; Bogunovic, Ilija; Norouzi-Fard, Ashkan; Tarnawski, Jakub; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Mitrovic, S (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	slobodan.mitrovic@epfl.ch; ilija.bogunovic@epfl.ch; ashkan.norouzifard@epfl.ch; jakub.tarnawski@epfl.ch; volkan.cevher@epfl.ch			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program [725594]; Swiss National Science Foundation (SNF) [407540_167319/1]; NCCR MARVEL - Swiss National Science Foundation; Hasler Foundation Switzerland [16066]; Office of Naval Research (ONR) [N00014-16-R-BA01]; ERC [335288-OptApprox]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program(European Research Council (ERC)); Swiss National Science Foundation (SNF)(Swiss National Science Foundation (SNSF)); NCCR MARVEL - Swiss National Science Foundation(Swiss National Science Foundation (SNSF)); Hasler Foundation Switzerland; Office of Naval Research (ONR)(Office of Naval Research); ERC(European Research Council (ERC)European Commission)	IB and VC's work was supported in part by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement number 725594), in part by the Swiss National Science Foundation (SNF), project 407540_167319/1, in part by the NCCR MARVEL, funded by the Swiss National Science Foundation, in part by Hasler Foundation Switzerland under grant agreement number 16066 and in part by Office of Naval Research (ONR) under grant agreement number N00014-16-R-BA01. JT's work was supported by ERC Starting Grant 335288-OptApprox.	[Anonymous], 2011, P 17 ACM SIGKDD INT; Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Bogunovic I., 2017, INT C MACH LEARN ICM; Chen W, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P795, DOI 10.1145/2939672.2939745; Golovin D., 2011, J ARTIFICIAL INTELLI, V42; Gomes R., 2010, P 27 INT C MACHINE L, P391; Guillory A, 2010, ARXIV10023345; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Kempe D., 2003, INT C KNOWL DISC DAT; Krause A, 2008, J MACH LEARN RES, V9, P2761; Kumar R., 2015, TOPC, V2, p14:1, DOI DOI 10.1145/2809814; Lin H., 2011, ASS COMP LING HUMAN, V1; Lindgren EM, 2016, ADV NEUR IN, V29; Mcauley J., 2014, ACM T KNOWL DISCOV D; Mirzasoleiman B, 2017, PR MACH LEARN RES, V70; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Norouzi-Fard A., 2016, ADV NEUR INF P SYS N; Orlin J. B., 2016, INT C INT PROGR COMB; Staib M., 2017, INT C MACH LEARN ICM; Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520; Tschiatschek Sebastian, 2014, ADV NEURAL INFORM PR, P1413; Yang J, 2015, KNOWL INF SYST, V42, P181, DOI 10.1007/s10115-013-0693-z	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404061
C	Mourtada, J; Gaiffas, S; Scornet, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mourtada, Jaouad; Gaiffas, Stephane; Scornet, Erwan			Universal consistency and minimax rates for online Mondrian Forests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CLASSIFICATION; AGGREGATION	We establish the consistency of an algorithm of Mondrian Forests [LRT14, LRT16], a randomized classification algorithm that can be implemented online. First, we amend the original Mondrian Forest algorithm proposed in [LRT14], that considers a fixed lifetime parameter. Indeed, the fact that this parameter is fixed hinders the statistical consistency of the original procedure. Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters lambda(n), and uses an alternative updating rule, allowing to work also in an online fashion. Second, we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function, which is a strong extension of previous results [AG14] to an arbitrary dimension.	[Mourtada, Jaouad; Gaiffas, Stephane; Scornet, Erwan] Ecole Polytech, Ctr Math Appl, Palaiseau, France	Institut Polytechnique de Paris	Mourtada, J (corresponding author), Ecole Polytech, Ctr Math Appl, Palaiseau, France.	jaouad.mourtada@polytechnique.edu; stephane.gaiffas@polytechnique.edu; erwan.scornet@polytechnique.edu	Scornet, Erwan/AAE-7811-2020	Mourtada, Jaouad/0000-0002-7830-9783				Arlot S., 2014, ARXIV14073939; Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Balog Matej, 2016, 32 C UNC ART INT UAI; Biau G, 2016, TEST-SPAIN, V25, P197, DOI 10.1007/s11749-016-0481-7; Biau G, 2012, J MACH LEARN RES, V13, P1063; Biau G, 2008, J MACH LEARN RES, V9, P2015; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman Leo, 2004, 670 U CAL BERK; Breiman Leo, 2000, 577 U CAL BERK STAT; Denil M., 2013, INT C MACH LEARN, P1256; Denil M, 2014, PR MACH LEARN RES, V32; Devroye L., 1996, APPL MATH, V31; Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107; Freund Y., 1997, P 20 9 ANN ACM S THE, P334, DOI [10.1145/258533.258616, DOI 10.1145/258533.258616]; Genuer R, 2012, J NONPARAMETR STAT, V24, P543, DOI 10.1080/10485252.2012.677843; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Helmbold DP, 1997, MACH LEARN, V27, P51, DOI 10.1023/A:1007396710653; Lakshminarayanan Balaji, 2016, P 19 INT WORKSH ART; Lakshminarayanan Balaji, 2014, ADV NEURAL INFORM PR, V27, P3140; Lecue G, 2007, BERNOULLI, V13, P1000, DOI 10.3150/07-BEJ6044; Mammen E, 1999, ANN STAT, V27, P1808; Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Roy D. M., 2011, THESIS; Roy D. M., 2009, ADV NEURAL INFORM PR; Saffari Amir, 2009, 3 IEEE ICCV WORKSH O; Scornet E, 2015, ANN STAT, V43, P1716, DOI 10.1214/15-AOS1321; Taddy MA, 2011, J AM STAT ASSOC, V106, P109, DOI 10.1198/jasa.2011.ap09769; Tsybakov AB, 2004, ANN STAT, V32, P135; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012; Yang YH, 1999, IEEE T INFORM THEORY, V45, P2271	32	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403080
C	Murata, T; Suzuki, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Murata, Tomoya; Suzuki, Taiji			Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We develop a new accelerated stochastic gradient method for efficiently solving the convex regularized empirical risk minimization problem in mini-batch settings. The use of mini-batches has become a golden standard in the machine learning community, because the mini-batch techniques stabilize the gradient estimate and can easily make good use of parallel computing. The core of our proposed method is the incorporation of our new "double acceleration" technique and variance reduction technique. We theoretically analyze our proposed method and show that our method much improves the mini-batch efficiencies of previous accelerated stochastic methods, and essentially only needs size root n mini-batches for achieving the optimal iteration complexities for both non-strongly and strongly convex objectives, where n is the training set size. Further, we show that even in non-mini-batch settings, our method achieves the best known convergence rate for non-strongly convex and strongly convex objectives.	[Murata, Tomoya] NTT DATA Math Syst Inc, Tokyo, Japan; [Suzuki, Taiji] Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Math Informat, Tokyo, Japan; [Suzuki, Taiji] Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan; [Suzuki, Taiji] RIKEN, Ctr Adv Integrated Intelligence Res, Tokyo, Japan	University of Tokyo; Japan Science & Technology Agency (JST); RIKEN	Murata, T (corresponding author), NTT DATA Math Syst Inc, Tokyo, Japan.	murata@msi.co.jp; taiji@mist.i.u-tokyo.ac.jp	Jeong, Yongwook/N-7413-2016		MEXT kakenhi [25730013, 25120012, 26280009, 15H05707]; JST-PRESTO; JST-CREST	MEXT kakenhi(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST-PRESTO(Japan Science & Technology Agency (JST)); JST-CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	This work was partially supported by MEXT kakenhi (25730013, 25120012, 26280009 and 15H05707), JST-PRESTO and JST-CREST.	Allen-Zhu Z., 2017, 48 ANN ACM S THEOR C, P19; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Duchi Y. J., 2009, ADV NEURAL INFORM PR, P495; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Li H., 2015, PROC 28 INT C NEURAL, P379; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; Nitanda A, 2016, JMLR WORKSH CONF PRO, V51, P195; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S., 2007, TECHNICAL REPORT; Tseng P, 2008, TECHNICAL REPORT; Xiao L., 2009, ADV NEURAL INFORM PR, Vvol 22, P2116; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang YC, 2015, PR MACH LEARN RES, V37, P353	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400058
C	Ouyang, Y; Gagrani, M; Nayyar, A; Jain, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ouyang, Yi; Gagrani, Mukul; Nayyar, Ashutosh; Jain, Rahul			Learning Unknown Markov Decision Processes: A Thompson Sampling Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish (O) over tilde (HS root AT) bounds on expected regret under a Bayesian setting, where S and A are the sizes of the state and action spaces, T is time, and H is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.	[Ouyang, Yi] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Gagrani, Mukul; Nayyar, Ashutosh; Jain, Rahul] Univ Southern Calif, Los Angeles, CA USA	University of California System; University of California Berkeley; University of Southern California	Ouyang, Y (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	ouyangyi@berkeley.edu; mgagrani@usc.edu; ashutosn@usc.edu; rahul.jain@usc.edu	Jeong, Yongwook/N-7413-2016		NSF [1611574, 1446901]	NSF(National Science Foundation (NSF))	Yi Ouyang would like to thank Yang Liu from Harvard University for helpful discussions. Rahul Jain and Ashutosh Nayyar were supported by NSF Grants 1611574 and 1446901.	Abbasi-Yadkori Y., 2015, UAI; Bartlett P. L., 2009, UAI; Bertsekas D. P., 2012, APPROXIMATE DYNAMIC, V2; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Chapelle O., 2011, NIPS; Dann C., 2015, NIPS; Filippi S., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P115, DOI 10.1109/ALLERTON.2010.5706896; Fonteneau R., 2013, BAYESOPT2013; Gopalan A., 2015, COLT; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kumar P.R., 2015, STOCHASTIC SYSTEMS E; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Osband I., 2016, EWRL; Osband Ian, 2013, NIPS; Osband Ian, 2016, ARXIV160802731; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strens M., 2000, ICML; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401036
C	Peter, S; Kirschbaum, E; Both, M; Campbell, LA; Harvey, BK; Heins, C; Durstewitz, D; Andilla, FD; Hamprecht, FA		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Peter, Sven; Kirschbaum, Elke; Both, Martin; Campbell, Lee A.; Harvey, Brandon K.; Heins, Conor; Durstewitz, Daniel; Andilla, Ferran Diego; Hamprecht, Fred A.			Sparse convolutional coding for neuronal assembly detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CORTICAL ACTIVITY; SPIKING ACTIVITY; CELL ASSEMBLIES; MULTIPLE LEVELS; SYNFIRE CHAINS; UNITARY EVENTS; DECONVOLUTION; PATTERNS; REGULARIZATION; PROBABILITY	Cell assemblies, originally proposed by Donald Hebb (1949), are subsets of neurons firing in a temporally coordinated way that gives rise to repeated motifs supposed to underly neural representations and information processing. Although Hebb's original proposal dates back many decades, the detection of assemblies and their role in coding is still an open and current research topic, partly because simultaneous recordings from large populations of neurons became feasible only relatively recently. Most current and easy-to-apply computational techniques focus on the identification of strictly synchronously spiking neurons. In this paper we propose a new algorithm, based on sparse convolutional coding, for detecting recurrent motifs of arbitrary structure up to a given length. Testing of our algorithm on synthetically generated datasets shows that it outperforms established methods and accurately identifies the temporal structure of embedded assemblies, even when these contain overlapping neurons or when strong background noise is present. Moreover, exploratory analysis of experimental datasets from hippocampal slices and cortical neuron cultures have provided promising results.	[Peter, Sven; Kirschbaum, Elke; Hamprecht, Fred A.] Interdisciplinary Ctr Sci Comp IWR, Heidelberg, Germany; [Both, Martin] Inst Physiol & Pathophysiol, Heidelberg, Germany; [Campbell, Lee A.; Harvey, Brandon K.; Heins, Conor] NIDA, Baltimore, MD USA; [Heins, Conor] Max Planck Inst Dynam & Self Org, Gottingen, Germany; [Durstewitz, Daniel] Cent Inst Mental Hlth, Dept Theoret Neurosci, Mannheim, Germany; [Andilla, Ferran Diego] Robert Bosch GmbH, Hildesheim, Germany	Ruprecht Karls University Heidelberg; National Institutes of Health (NIH) - USA; NIH National Institute on Drug Abuse (NIDA); Max Planck Society; Central Institute of Mental Health; Bosch	Peter, S (corresponding author), Interdisciplinary Ctr Sci Comp IWR, Heidelberg, Germany.	sven.peter@iwr.uni-heidelberg.de; elke.kirschbaum@iwr.uni-heidelberg.de; mboth@physiologie.uni-heidelberg.de; lee.campbell@nih.gov; bharvey@mail.nih.gov; conor.heins@ds.mpg.de; daniel.durstewitz@zi-mannheim.de; ferran.diegoandilla@de.bosch.com; fred.hamprecht@iwr.uni-heidelberg.de	Harvey, Brandon Keith/AAY-4080-2021; Jeong, Yongwook/N-7413-2016	Harvey, Brandon Keith/0000-0002-4036-1383; 	Intramural Research Program of the NIH, NIDA; DFG [SFB 1134, Du 354/8-1]	Intramural Research Program of the NIH, NIDA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute on Drug Abuse (NIDA)); DFG(German Research Foundation (DFG))	SP and EK thank Eleonora Russo for sharing her knowledge on generating synthetic data and Fynn Bachmann for his support. LAC, BKH and CH thank Lowella Fortuno for technical assistance with cortical cultures and acknowledge the support by the Intramural Research Program of the NIH, NIDA. DD acknowledges partial financial support by DFG Du 354/8-1. SP, EK, MB, DD, FD and FAH gratefully acknowledge partial financial support by DFG SFB 1134.	Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/NMETH.2434, 10.1038/nmeth.2434]; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Billeh YN, 2014, J NEUROSCI METH, V236, P92, DOI 10.1016/j.jneumeth.2014.08.011; Buzsaki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233; Buzsaki G, 1998, J SLEEP RES, V7, P17, DOI 10.1046/j.1365-2869.7.s1.3.x; Carrillo-Reid L, 2015, J NEUROSCI, V35, P8813, DOI 10.1523/JNEUROSCI.5214-14.2015; Cichocki A, 2006, ELECTRON LETT, V42, P947, DOI 10.1049/el:20060983; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Cossart P, 2004, SCIENCE, V304, P242, DOI 10.1126/science.1090124; Diego F., 2014, ADV NEURAL INFORM PR, P64; Diego F., 2013, NIPS; Gerstein GL, 2012, J NEUROSCI METH, V206, P54, DOI 10.1016/j.jneumeth.2012.02.003; Girardeau G, 2011, CURR OPIN NEUROBIOL, V21, P452, DOI 10.1016/j.conb.2011.02.005; Girardeau G, 2009, NAT NEUROSCI, V12, P1222, DOI 10.1038/nn.2384; Grun S, 2002, NEURAL COMPUT, V14, P81, DOI 10.1162/089976602753284464; Grun S, 2002, NEURAL COMPUT, V14, P43, DOI 10.1162/089976602753284455; Hansen PC, 2002, NUMER ALGORITHMS, V29, P323, DOI 10.1023/A:1015222829062; Hebb D.O., 1949, ORG BEHAV NEUROPSYCH; Howard DB, 2008, VIROLOGY, V372, P24, DOI 10.1016/j.virol.2007.10.007; Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173; Lopes-dos-Santos V, 2013, J NEUROSCI METH, V220, P149, DOI 10.1016/j.jneumeth.2013.04.010; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323; Marr D., 1991, SIMPLE MEMORY THEORY; Mokeichev A, 2007, NEURON, V53, P413, DOI 10.1016/j.neuron.2007.01.017; Nicolelis MAL, 1997, NEURON, V19, P219, DOI 10.1016/S0896-6273(00)80932-0; NICOLELIS MAL, 1995, SCIENCE, V268, P1353, DOI 10.1126/science.7761855; O'Grady Paul D., 2006, Proceedings of the 2006 IEEE Signal Processing Society Workshop, P427; Pastalkova E, 2008, SCIENCE, V321, P1322, DOI 10.1126/science.1159775; Pfeiffer T, 2014, NEUROIMAGE, V94, P239, DOI 10.1016/j.neuroimage.2014.03.030; PIERSKALLA WP, 1968, OPER RES, V16, P422, DOI 10.1287/opre.16.2.422; Pnevmatikakis E. A., ARXIV14092903QBIOSTA; Pnevmatikakis E. A., 2013, NIPS; Pnevmatikakis E. A., 2013, COMPUTATIONAL SYSTEM; Protter M, 2009, IEEE T IMAGE PROCESS, V18, P27, DOI 10.1109/TIP.2008.2008065; Quiroga-Lombard CS, 2013, J NEUROPHYSIOL, V110, P562, DOI 10.1152/jn.00186.2013; Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477; Russo E, 2017, ELIFE, V6, DOI 10.7554/eLife.19428; Santos V. Lopes-dos, 2011, PLOS ONE, V6, P1; SINGER W, 1993, ANNU REV PHYSIOL, V55, P349, DOI 10.1146/annurev.physiol.55.1.349; Smaragdis P, 2004, LECT NOTES COMPUT SC, V3195, P494; Smith AC, 2006, NEURAL COMPUT, V18, P1197, DOI 10.1162/neco.2006.18.5.1197; Smith AC, 2010, NEURAL COMPUT, V22, P2522, DOI 10.1162/NECO_a_00020; Staude B, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00016; Staude B, 2010, J COMPUT NEUROSCI, V29, P327, DOI 10.1007/s10827-009-0195-x; Stevens CF, 2003, NEURON, V40, P381, DOI 10.1016/S0896-6273(03)00643-3; Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731; Szlam A., 2010, COMPUTER RES REPOSIT; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Torre E, 2016, J NEUROSCI, V36, P8329, DOI 10.1523/JNEUROSCI.4375-15.2016; Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009; Weiss R. J., 2010, ISMIR; Yuste R, 2005, NAT REV NEUROSCI, V6, P477, DOI 10.1038/nrn1686; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	54	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403072
C	Qian, C; Shi, JC; Yu, Y; Tang, K; Zhou, ZH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Qian, Chao; Shi, Jing-Cheng; Yu, Yang; Tang, Ke; Zhou, Zhi-Hua			Subset Selection under Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The problem of selecting the best k-element subset from a universe is involved in many applications. While previous studies assumed a noise-free environment or a noisy monotone submodular objective function, this paper considers a more realistic and general situation where the evaluation of a subset is a noisy monotone function (not necessarily submodular), with both multiplicative and additive noises. To understand the impact of the noise, we firstly show the approximation ratio of the greedy algorithm and POSS, two powerful algorithms for noise-free subset selection, in the noisy environments. We then propose to incorporate a noise-aware strategy into POSS, resulting in the new PONSS algorithm. We prove that PONSS can achieve a better approximation ratio under some assumption such as i.i.d. noise distribution. The empirical results on influence maximization and sparse regression problems show the superior performance of PONSS.	[Qian, Chao; Tang, Ke] USTC, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei, Anhui, Peoples R China; [Shi, Jing-Cheng; Yu, Yang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China; [Tang, Ke] SUSTech, Shenzhen Key Lab Computat Intelligence, Shenzhen, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Nanjing University; Southern University of Science & Technology	Qian, C (corresponding author), USTC, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei, Anhui, Peoples R China.	chaoqian@ustc.edu.cn; shijc@lamda.nju.edu.cn; yuy@lamda.nju.edu.cn; tangk3@sustc.edu.cn; zhouzh@lamda.nju.edu.cn	Jeong, Yongwook/N-7413-2016		NSFC [61333014, 61603367, 61672478]; YESS [2016QNRC001]; JiangsuSF [BK20160066, BK20170013]; Royal Society Newton Advanced Fellowship [NA150123]; Collaborative Innovation Center of Novel Software Technology and Industrialization	NSFC(National Natural Science Foundation of China (NSFC)); YESS; JiangsuSF; Royal Society Newton Advanced Fellowship; Collaborative Innovation Center of Novel Software Technology and Industrialization	The authors would like to thank reviewers for their helpful comments and suggestions. C. Qian was supported by NSFC (61603367) and YESS (2016QNRC001). Y. Yu was supported by JiangsuSF (BK20160066, BK20170013). K. Tang was supported by NSFC (61672478) and Royal Society Newton Advanced Fellowship (NA150123). Z.-H. Zhou was supported by NSFC (61333014) and Collaborative Innovation Center of Novel Software Technology and Industrialization.	[Anonymous], 2002, SUBSET SELECTION REG, DOI DOI 10.1201/9781420035933; Bian AA, 2017, PR MACH LEARN RES, V70; Chen W., 2010, KDD, P1029, DOI [10.1145/1835804.1835934, DOI 10.1145/1835804.1835934]; Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047; Chen Y., 2015, C LEARN THEOR, P338; Das A., 2011, P 28 INT C MACH LEAR, P1057; Das A, 2008, ACM S THEORY COMPUT, P45; Davis G, 1997, CONSTR APPROX, V13, P57, DOI 10.1007/BF02678430; Diekhoff G., 1992, STAT SOCIAL BEHAV SC; Elenberg Ethan R, 2016, ARXIV161200804; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Goldenberg J, 2001, MARKET LETT, V12, P211, DOI 10.1023/A:1011122126881; Goyal A., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P211, DOI 10.1109/ICDM.2011.132; Hassidim A., 2017, COLT, P1069; Horel T., 2016, P NIPS BARC, P3045; Johnson RA, 2007, APPL MULTIVARIATE ST; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Qian C., 2017, EVOLUTIONARY COMPUTA; Qian C., 2015, P ADV NEUR INF PROC, V28, P1765; Qian C., 2016, PROC INT JOINT C ART, P1939; Qian C, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2606; Qian C, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2613; Qian C, 2015, AAAI CONF ARTIF INTE, P2935; Singla A, 2016, AAAI CONF ARTIF INTE, P2037	26	1	1	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403061
C	Qu, Q; Zhang, YQ; Eldar, YC; Wright, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Qu, Qing; Zhang, Yuqian; Eldar, Yonina C.; Wright, John			Convolutional Phase Retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STABLE SIGNAL RECOVERY	We study the convolutional phase retrieval problem, which considers recovery of an unknown signal x is an element of C-n from m measurements consisting of the magnitude of its cyclic convolution with a known kernel a of length m. This model is motivated by applications to channel estimation, optics, and underwater acoustic communication, where the signal of interest is acted on by a given channel/filter, and phase information is difficult or impossible to acquire. We show that when a is random and m is sufficiently large, x can be efficiently recovered up to a global phase using a combination of spectral initialization and generalized gradient descent. The main challenge is coping with dependencies in the measurement operator; we overcome this challenge by using ideas from decoupling theory, suprema of chaos processes and the restricted isometry property of random circulant matrices, and recent analysis for alternating minimizing methods.	[Qu, Qing; Zhang, Yuqian; Wright, John] Columbia Univ, New York, NY 10027 USA; [Eldar, Yonina C.] Technion, Haifa, Israel	Columbia University	Qu, Q (corresponding author), Columbia Univ, New York, NY 10027 USA.	qq2105@columbia.edu; yz2409@columbia.edu; yonina@ee.technion.ac.il; jw2966@columbia.edu	Qu, Qing/AAA-8226-2019; Jeong, Yongwook/N-7413-2016	Qu, Qing/0000-0001-9136-558X; 	European Unions Horizon 2020 research and innovation program [646804-ERCCOGBNYQ]; Israel Science Foundation [335/14]; Microsoft graduate research fellowship;  [NSF CCF 1527809];  [NSF IIS 1546411]	European Unions Horizon 2020 research and innovation program; Israel Science Foundation(Israel Science Foundation); Microsoft graduate research fellowship(Microsoft); ; 	This work was partially supported by the grants NSF CCF 1527809 and NSF IIS 1546411, the grants from the European Unions Horizon 2020 research and innovation program under grant agreement No. 646804-ERCCOGBNYQ, and the grant from the Israel Science Foundation under grant no. 335/14. QQ thanks the generous support of the Microsoft graduate research fellowship. We would like to thank Shan Zhong for the helpful discussion for real applications and providing the antenna data for experiments, and we thank Ju Sun and Han-wen Kuo for helpful discussion and input regarding the analysis of this work.	[Anonymous], 2014, THESIS STANFORD U ST; Arik SO, 2016, OPT LETT, V41, P4265, DOI 10.1364/OL.41.004265; Bendory T., 2017, IEEE T INFORM THEORY, VPP, P1; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Candfes Emmanuel J., 2013, SIAM J IMAGING SCI, V6; Chen Yuxin, 2015, ARXIV150505114; de la Pena V., 2012, DECOUPLING DEPENDENC; Foucart S., 2013, MATH INTRO COMPRESSI, P1, DOI DOI 10.1007/978-0-8176-4948-7; Gagliardi R. M., 1976, OPTICAL COMMUNICATIO; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; GERCHBERG RW, 1972, OPTIK, V35, P237; Gross D., 2013, ARXIV13102267; Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149; Jaganathan Kishore, 2016, OPTICAL COMPRESSIVE; Krahmer F., 2014, GAMM MITT, V37, P217, DOI 10.1002/gamm.201410010; Krahmer F, 2014, COMMUN PUR APPL MATH, V67, P1877, DOI 10.1002/cpa.21504; Kreutz-Delgado K., 2009, ARXIV PREPRINT ARXIV; KWAPIEN S, 1987, ANN PROBAB, V15, P1062, DOI 10.1214/aop/1176992081; Mecozzi A, 2016, OPTICA, V3, P1220, DOI 10.1364/OPTICA.3.001220; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Rauhut H, 2010, THEORETICAL FDN NUME, V9, P1, DOI DOI 10.1515/9783110226157.1; Roman V., 2012, COMPRESSED SENSING T; Shahmansoori A., 2015, 2015 IEEE GLOB WORKS, P1; Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673; Soltanolkotabi Mahdi, 2017, ABS170206175 CORR; STOJANOVIC M, 1994, IEEE J OCEANIC ENG, V19, P100, DOI 10.1109/48.289455; Sun J., 2015, ARXIV151006096; Sun J., 2015, ARXIV150406785; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Waldspurger Irene, 2016, ARXIV160903088; Walk P., 2015, ASILOMAR 2015; WRIGHT J., 2016, ARXIV160206664; Zhang H., 2016, ADV NEURAL INF PROCE, V29, P2622; Zhang Yinda, 2017, P IEEE C COMP VIS PA	40	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406016
C	Regier, J; Jordan, MI; McAuliffe, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Regier, Jeffrey; Jordan, Michael, I; McAuliffe, Jon			Fast Black-box Variational Inference through Stochastic Trust-Region Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce TrustVI, a fast second-order algorithm for black-box variational inference based on trust-region optimization and the "reparameterization trick." At each iteration, TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implemented TrustVI in the Stan framework and compared it to two alternatives: Automatic Differentiation Variational Inference (ADVI) and Hessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is based on stochastic first-order optimization. The latter uses second-order information, but lacks convergence guarantees. TrustVI typically converged at least one order of magnitude faster than ADVI, demonstrating the value of stochastic second-order information. TrustVI often found substantially better variational distributions than HFSGVI, demonstrating that our convergence theory can matter in practice.				jregier@cs.berkeley.edu; jordan@cs.berkeley.edu; jon@stat.berkeley.edu	Jordan, Michael I/C-5253-2013; Jeong, Yongwook/N-7413-2016					[Anonymous], 2014, INT C MACH LEARN; Blei D. M., 2017, J AM STAT ASS; Carpenter B., 2016, J STAT SOFTWARE, V20; Cheng RQ, 2018, MULTIMED TOOLS APPL, V77, P20651, DOI 10.1007/s11042-017-5472-5; Deng G, 2009, MATH PROGRAM, V117, P81, DOI 10.1007/s10107-007-0164-y; EFRON B, 1981, ANN STAT, V9, P586, DOI 10.1214/aos/1176345462; Fan Kai, 2015, ADV NEURAL INFORM PR; Fike J. A., 2012, LECT NOTES COMPUTATI, V87, P163, DOI DOI 10.1007/978-3-642-30023-3_15; Gelman A., 2006, DATA ANAL USING REGR, DOI DOI 10.1017/CBO9780511790942.005; Gould NIM, 1999, SIAM J OPTIMIZ, V9, P504, DOI 10.1137/S1052623497322735; Kingma D., 2014, ADAM METHOD STOCHAST; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Lenders F., 2016, ARXIV161104718; Lunn D., 2012, BUGS BOOK PRACTICAL; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; OpenBugs developers, ALL MULT LOG REGR; OpenBugs developers, RATS NORM HIER MOD; OpenBugs developers, SEEDS RAND EFF LOG R; OpenBugs developers, DYES VAR COMP MOD; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Regier J, 2016, ARXIV161103404; Shashaani Sara, 2016, IEEE WINT SIM C; Spall, 2005, INTRO STOCHASTIC SEA; TITSIAS M. K., 2014, INT C MACH LEARN	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402044
C	Savinov, N; Ladicky, L; Pollefeys, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Savinov, Nikolay; Ladicky, Lubor; Pollefeys, Marc			Matching neural paths: transfer from recognition to correspondence search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Many machine learning tasks require finding per-part correspondences between objects. In this work we focus on low-level correspondences - a highly ambiguous matching problem. We propose to use a hierarchical semantic representation of the objects, coming from a convolutional neural network, to solve this ambiguity. Training it for low-level correspondence prediction directly might not be an option in some domains where the ground-truth correspondences are hard to obtain. We show how transfer from recognition can be used to avoid such training. Our idea is to mark parts as "matching" if their features are close to each other at all the levels of convolutional feature hierarchy (neural paths). Although the overall number of such paths is exponential in the number of layers, we propose a polynomial algorithm for aggregating all of them in a single backward pass. The empirical validation is done on the task of stereo correspondence and demonstrates that we achieve competitive results among the methods which do not use labeled target domain data.	[Savinov, Nikolay; Ladicky, Lubor; Pollefeys, Marc] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Pollefeys, Marc] Microsoft, Redmond, WA USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; Microsoft	Savinov, N (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	nikolay.savinov@nf.ethz.ch; lubor.ladicky@inf.ethz.ch; marc.pollefeys@inf.ethz.ch	Jeong, Yongwook/N-7413-2016; Pollefeys, Marc/I-7607-2013		Swiss NSF [163910]	Swiss NSF(Swiss National Science Foundation (SNSF))	We would like to thank Dmitry Laptev, Alina Kuznetsova and Andrea Cohen for their comments about the manuscript. We also thank Valery Vishnevskiy for running our code while our own cluster was down. This work is partially funded by the Swiss NSF project 163910 "Efficient Object-Centric Detection".	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Choy Christopher Bongsoo, 2016, ADV NEURAL INFORM PR, P2406; Donahue J, 2013, ABS13101531 CORR, P2013; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56; Kim Seungryong, 2017, ARXIV170200926; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Long J. L., 2014, ADV NEURAL INFORM PR, V27, P1601; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Menze Moritz, 2015, CVPR; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sermanet P., 2013, ARXIV PREPRINT ARXIV; van den Oord Aaron, 2016, ABS160903499 CORR; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345; Zbontar J, 2016, J MACH LEARN RES, V17; Zbontar Jure, 2016, MC CNN GITHUB REPOSI	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401024
C	Schroecker, Y; Isbell, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Schroecker, Yannick; Isbell, Charles			State Aware Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				AVERAGE	Imitation learning is the study of learning how to act given a set of demonstrations provided by a human expert. It is intuitively apparent that learning to take optimal actions is a simpler undertaking in situations that are similar to the ones shown by the teacher. However, imitation learning approaches do not tend to use this insight directly. In this paper, we introduce State Aware Imitation Learning (SAIL), an imitation learning algorithm that allows an agent to learn how to remain in states where it can confidently take the correct action and how to recover if it is lead astray. Key to this algorithm is a gradient learned using a temporal difference update rule which leads the agent to prefer states similar to the demonstrated states. We show that estimating a linear approximation of this gradient yields similar theoretical guarantees to online temporal difference learning approaches and empirically show that SAIL can effectively be used for imitation learning in continuous domains with non-linear function approximators used for both the policy representation and the gradient estimate.	[Schroecker, Yannick; Isbell, Charles] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Schroecker, Y (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	yannickschroecker@gatech.edu; isbell@cc.gatech.edu	Jeong, Yongwook/N-7413-2016		Office of Naval Research [N000141410003]	Office of Naval Research(Office of Naval Research)	This work was supported by the Office of Naval Research under grant N000141410003	Boularias Abdeslam, 2011, INT C ART INT STAT A, V15, P1; Brockman G., 2016, OPENAI GYM; Chernova Sonia, 2014, ROBOT LEARNING HUMAN, DOI [10.2200/S00568ED1V01Y201402AIM028, DOI 10.2200/S00568ED1V01Y201402AIM028]; Choi Jaedeug, 2011, NEURAL INFORM PROCES; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Finn C., 2016, INT C MACH LEARN ICM; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ha David, 2016, ARXIV160909106V4CSLG; Hester Todd, 2017, 170403732V1CSAI ARXI; Ho J, 2016, PR MACH LEARN RES, V48; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Jaderberg Max, 2016, 160805343V1CSLG ARXI; Klein Edouard, 2013, JOINT EUR C MACH LEA; Klein Edouard, 2012, NEURAL INFORM PROCES; Morimura T, 2010, NEURAL COMPUT, V22, P342, DOI 10.1162/neco.2009.12-08-922; Munos Remi, 2016, NEURAL INFORM PROCES; Ng A. Y., 2000, INT C MACH LEARN; Pomerleau Deana, 1989, NEURAL INFORM PROCES; Ross Stephane, 2011, AISTATS; Ross Stephane, 2010, INT C ART INT STAT A; Schaal S., 1997, NEURAL INFORM PROCES; Schmidhuber Juergen H., 1993, INT C ART NEUR NETW; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton RS., 2016, J MACH LEARN RES, V17, P2603; Tsitsiklis JN, 2002, MACH LEARN, V49, P179, DOI 10.1023/A:1017980312899; Tsitsiklis JN, 1999, AUTOMATICA, V35, P1799, DOI 10.1016/S0005-1098(99)00099-0; Ziebart Brian D, 2007, AAAI C ART INT AAAI	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402094
C	Scieur, D; Bach, F; d'Aspremont, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Scieur, Damien; Bach, Francis; d'Aspremont, Alexandre			Nonlinear Acceleration of Stochastic Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.	[Scieur, Damien; Bach, Francis] PSL Res Univ, ENS, INRIA, Paris, France; [d'Aspremont, Alexandre] PSL Res Univ, CNRS, ENS, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Scieur, D (corresponding author), PSL Res Univ, ENS, INRIA, Paris, France.	damien.scieur@inria.fr; francis.bach@inria.fr; aspremon@ens.fr	Jeong, Yongwook/N-7413-2016		European Research Council (ERC project SIPA); European Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN) [607290 SpaRTaN]; chaire Economie des nouvelles donnees with the data science joint research initiative; fonds AXA pour la recherche	European Research Council (ERC project SIPA); European Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN); chaire Economie des nouvelles donnees with the data science joint research initiative; fonds AXA pour la recherche(AXA Research Fund)	The authors would like to acknowledge support from a starting grant from the European Research Council (ERC project SIPA), from the European Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN) under grant agreement number 607290 SpaRTaN, as well as support from the chaire Economie des nouvelles donnees with the data science joint research initiative with the fonds AXA pour la recherche and a gift from Societe Generale Cross Asset Quantitative Research.	Allen-Zhu Z., 2016, ARXIV160305953; ANDERSON DG, 1965, J ACM, V12, P547, DOI 10.1145/321296.321305; BACH F., 2011, ADV NEURAL INFORM PR, P451; CABAY S, 1976, SIAM J NUMER ANAL, V13, P734, DOI 10.1137/0713060; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defossez A, 2015, JMLR WORKSH CONF PRO, V38, P205; Fercoq O., 2016, ARXIV160907358; Flammarion N., 2015, C LEARN THEOR, P658; Golub GH., 1961, NUMER MATH, V3, P147, DOI DOI 10.1007/BF01386013; Jain P., 2016, ARXIV161003774; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Mesina M., 1977, Computer Methods in Applied Mechanics and Engineering, V10, P165, DOI 10.1016/0045-7825(77)90004-4; Nedic A, 2001, APPL OPTIMIZAT, V54, P223; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6; Scieur D., 2016, ADV NEURAL INFORM PR, P712; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32	19	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404006
C	Song, Z; Muraoka, Y; Fujimaki, R; Carin, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Song, Zhao; Muraoka, Yusuke; Fujimaki, Ryohei; Carin, Lawrence			Scalable Model Selection for Belief Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORKS	We propose a scalable algorithm for model selection in sigmoid belief networks (SBNs), based on the factorized asymptotic Bayesian (FAB) framework. We derive the corresponding generalized factorized information criterion (gFIC) for the SBN, which is proven to be statistically consistent with the marginal log-likelihood. To capture the dependencies within hidden variables in SBNs, a recognition network is employed to model the variational distribution. The resulting algorithm, which we call FABIA, can simultaneously execute both model selection and inference by maximizing the lower bound of gFIC. On both synthetic and real data, our experiments suggest that FABIA, when compared to state-of-the-art algorithms for learning SBNs, (i) produces a more concise model, thus enabling faster testing; (ii) improves predictive performance; (iii) accelerates convergence; and (iv) prevents overfitting.	[Song, Zhao; Carin, Lawrence] Duke Univ, Dept ECE, Durham, NC 27708 USA; [Muraoka, Yusuke; Fujimaki, Ryohei] NEC Data Sci Res Labs, Cupertino, CA 95014 USA	Duke University	Song, Z (corresponding author), Duke Univ, Dept ECE, Durham, NC 27708 USA.	zhao.song@duke.edu; ymuraoka@nec-labs.com; rfujimaki@nec-labs.com; lcarin@duke.edu	Song, Zhao/AAW-4042-2020	Carin, Lawrence/0000-0001-6277-7948	ARO; DARPA; DOE; NGA; ONR; NSF; NEC Fellowship	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); NEC Fellowship	The authors would like to thank Ricardo Henao for helpful discussions, and the anonymous reviewers for their insightful comments and suggestions. Part of this work was done during the internship of the first author at NEC Laboratories America, Cupertino, CA. This research was supported in part by ARO, DARPA, DOE, NGA, ONR, NSF, and the NEC Fellowship.	Alvarez Jose M, 2016, ADV NEURAL INFORM PR, P2270; Armagan Artin, 2011, Adv Neural Inf Process Syst, V24, P523; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Bornschein J., 2015, INT C LEARN REPR; Carlson D, 2016, IEEE J-STSP, V10, P296, DOI 10.1109/JSTSP.2015.2505684; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fujimaki R, P 15 INT C ART INT, P400; Fujimaki R., 2012, P 29 INT C MACH LEAR, P799; Gan Z, 2015, JMLR WORKSH CONF PRO, V38, P268; Gan Z, 2015, PR MACH LEARN RES, V37, P1823; Hayashi K., 2013, ADV NEURAL INFORM PR, P1214; Hayashi K, 2015, PR MACH LEARN RES, V37, P1358; Henao R, 2015, ADV NEUR IN, V28; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Liu CC, 2015, PR MACH LEARN RES, V37, P1227; Liu CC, 2016, IEEE DATA MINING, P271, DOI [10.1109/ICDM.2016.101, 10.1109/ICDM.2016.0038]; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Maddison Chris J, 2017, ICLR; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Song Z, 2016, JMLR WORKSH CONF PRO, V51, P1347; Srivastava N., 2013, UAI 2013, P616; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Wan L., 2013, P INT C MACHINE LEAR, P1058; Zhou MY, 2015, ADV NEUR IN, V28	35	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404066
C	Srivastava, N; Vul, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Srivastava, Nisheeth; Vul, Edward			A simple model of recognition and recall memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COMPLEMENTARY LEARNING-SYSTEMS; RETRIEVAL-PROCESSES	We show that several striking differences in memory performance between recognition and recall tasks are explained by an ecological bias endemic in classic memory experiments - that such experiments universally involve more stimuli than retrieval cues. We show that while it is sensible to think of recall as simply retrieving items when probed with a cue - typically the item list itself - it is better to think of recognition as retrieving cues when probed with items. To test this theory, by manipulating the number of items and cues in a memory experiment, we show a crossover effect in memory performance within subjects such that recognition performance is superior to recall performance when the number of items is greater than the number of cues and recall performance is better than recognition when the converse holds. We build a simple computational model around this theory, using sampling to approximate an ideal Bayesian observer encoding and retrieving situational co-occurrence frequencies of stimuli and retrieval cues. This model robustly reproduces a number of dissociations in recognition and recall previously used to argue for dual-process accounts of declarative memory.	[Srivastava, Nisheeth] IIT Kanpur, Comp Sci, Kanpur 208016, Uttar Pradesh, India; [Vul, Edward] UCSD, Dept Psychol, 9500 Gilman Dr, La Jolla, CA 92093 USA	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kanpur; University of California System; University of California San Diego	Srivastava, N (corresponding author), IIT Kanpur, Comp Sci, Kanpur 208016, Uttar Pradesh, India.	nsrivast@cse.iitk.ac.in; evul@ucsd.edu	Jeong, Yongwook/N-7413-2016					Bauml KH, 1997, PSYCHON B REV, V4, P260, DOI 10.3758/BF03209403; Craik FIM, 1996, J EXP PSYCHOL GEN, V125, P159, DOI 10.1037/0096-3445.125.2.159; Gershman SJ, 2010, PSYCHOL REV, V117, P197, DOI 10.1037/a0017808; GILLUND G, 1984, PSYCHOL REV, V91, P1, DOI 10.1037/0033-295X.91.1.1; GLANZER M, 1990, J EXP PSYCHOL LEARN, V16, P5, DOI 10.1037/0278-7393.16.1.5; Gregg, 1976, WORD FREQUENCY RECOG; Kumaran D, 2016, TRENDS COGN SCI, V20, P512, DOI 10.1016/j.tics.2016.05.004; MANDLER G, 1980, PSYCHOL REV, V87, P252, DOI 10.1037/0033-295X.87.3.252; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; NICKERSON RS, 1984, MEM COGNITION, V12, P531, DOI 10.3758/BF03213342; RATCLIFF R, 1990, J EXP PSYCHOL LEARN, V16, P163, DOI 10.1037/0278-7393.16.2.163; Shiffrin RM, 1997, PSYCHON B REV, V4, P145, DOI 10.3758/BF03209391; Srivastava Nisheeth, 2014, P ANN M COGN SCI SOC; STERNBERG S, 1969, AM SCI, V57, P421; Todd PM, 2007, CURR DIR PSYCHOL SCI, V16, P167, DOI 10.1111/j.1467-8721.2007.00497.x; TULVING E, 1971, J EXP PSYCHOL, V87, P116, DOI 10.1037/h0030186; Vul E, 2014, COGNITIVE SCI, V38, P599, DOI 10.1111/cogs.12101; Wixted JT, 2007, PSYCHOL REV, V114, P152, DOI 10.1037/0033-295X.114.1.152; Yonelinas AP, 2002, J MEM LANG, V46, P441, DOI 10.1006/jmla.2002.2864	20	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400028
C	Tibshirani, RJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tibshirani, Ryan J.			Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DUAL ASCENT METHODS; RELAXATION METHODS; CONVERGENCE; DECOMPOSITION; PROJECTIONS; RULES; COSTS	We study connections between Dykstra's algorithm for projecting onto an intersection of convex sets, the augmented Lagrangian method of multipliers or ADMM, and block coordinate descent. We prove that coordinate descent for a regularized regression problem, in which the penalty is a separable sum of support functions, is exactly equivalent to Dykstra's algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent, in the special case of two sets, with one being a linear subspace. These connections, aside from being interesting in their own right, suggest new ways of analyzing and extending coordinate descent. For example, from existing convergence theory on Dykstra's algorithm over polyhedra, we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent, based on the Dykstra and ADMM connections.	[Tibshirani, Ryan J.] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA; [Tibshirani, Ryan J.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Tibshirani, RJ (corresponding author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.; Tibshirani, RJ (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	ryantibs@stat.cmu.edu	Jeong, Yongwook/N-7413-2016					Auslender A., 1976, OPTIMISATION METHODE; Bauschke H., 2000, OPTIMIZATION, V48, P409; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Bauschke Heinz H., 2013, ARXIV13014506; Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyle J. P., 1986, ADV ORDER RESTRICTED, P28, DOI DOI 10.1007/978-1-4613-9940-7_3; Censor Y, 1998, COMMUN APPL ANAL, V2, P407; Chang TH, 2016, IEEE T SIGNAL PROCES, V64, P3118, DOI 10.1109/TSP.2016.2537271; Chang TH, 2016, IEEE T SIGNAL PROCES, V64, P3131, DOI 10.1109/TSP.2016.2537261; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; DEUTSCH F, 1994, NUMER FUNC ANAL OPT, V15, P537, DOI 10.1080/01630569408816580; Deutsch F., 2001, BEST APPROXIMATION I; Douglas J., 1956, T AM MATH SOC, V82, P421, DOI DOI 10.1090/S0002-9947-1956-0084194-4; DYKSTRA RL, 1983, J AM STAT ASSOC, V78, P837, DOI 10.2307/2288193; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1; Gabay D., 1983, STUD MATH APPL, V15, P299, DOI [DOI 10.1016/S0168-2024(08)70034-1, 10.1016/S0168-2024(08)70034-1]; GAFFKE N, 1989, METRIKA, V36, P29, DOI [10.1007/BF02614077, DOI 10.1007/BF02614077]; GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41; Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219; Halperin I., 1962, ACTA SCI MATH SZEGED, V23, P96; HAN SP, 1988, MATH PROGRAM, V40, P1, DOI 10.1007/BF01580719; Hildreth C., 1957, NAVAL RES LOGIST Q, V4, P79; Hong MY, 2017, MATH PROGRAM, V162, P165, DOI 10.1007/s10107-016-1034-2; IUSEM AN, 1987, SIAM J CONTROL OPTIM, V25, P231, DOI 10.1137/0325014; IUSEM AN, 1990, MATH PROGRAM, V47, P37, DOI 10.1007/BF01580851; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Kadkhodaie M, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497, DOI 10.1145/2783258.2783400; Li XG, 2016, JMLR WORKSH CONF PRO, V51, P491; LIONS PL, 1979, SIAM J NUMER ANAL, V16, P964, DOI 10.1137/0716071; Luenberger D. G., 1973, INTRO LINEAR NONLINE; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; LUO ZQ, 1993, MATH OPER RES, V18, P846, DOI 10.1287/moor.18.4.846; Nishihara R, 2015, PR MACH LEARN RES, V37, P343; Ortega J. M., 1970, ITERATIVE SOLUTION N, V30; PIERRA G, 1984, MATH PROGRAM, V28, P96, DOI 10.1007/BF02612715; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; Sardy S, 2000, J COMPUT GRAPH STAT, V9, P361, DOI 10.2307/1390659; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; TSENG P, 1993, MATH PROGRAM, V59, P231, DOI 10.1007/BF01581245; TSENG P, 1990, SIAM J CONTROL OPTIM, V28, P214, DOI 10.1137/0328011; TSENG P, 1987, MATH PROGRAM, V38, P303, DOI 10.1007/BF02592017; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; von Neumman J., 1950, ANN MATH STUD, V22; Wang J, 2015, J MACH LEARN RES, V16, P1063; WARGA J, 1963, J SOC IND APPL MATH, V11, P588, DOI 10.1137/0111043; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Wu TT, 2008, ANN APPL STAT, V2, P224, DOI 10.1214/07-AOAS147	56	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400050
C	Van Buskirk, G; Raichel, B; Ruozzi, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Van Buskirk, Gregory; Raichel, Benjamin; Ruozzi, Nicholas			Sparse Approximate Conic Hulls	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MATRIX; ALGORITHMS	We consider the problem of computing a restricted nonnegative matrix factorization (NMF) of an m x n matrix X. Specifically, we seek a factorization X approximate to BC, where the k columns of B are a subset of those from X and C is an element of R(>= 0)(kxn)Equivalently, given the matrix X, consider the problem of finding a small subset, S, of the columns of X such that the conic hull of S epsilon-approximates the conic hull of the columns of X, i.e., the distance of every column of X to the conic hull of the columns of S should be at most an epsilon-fraction of the angular diameter of X. If k is the size of the smallest epsilon-approximation, then we produce an O(k/epsilon(2/3)) sized O(epsilon(1/3))-approximation, yielding the first provable, polynomial time epsilon-approximation for this class of NMF problems, where also desirably the approximation is independent of n and m. Furthermore, we prove an approximate conic Caratheodory theorem, a general sparsity result, that shows that any column of X can be epsilon-approximated with an O(1/epsilon(2)) sparse combination from S. Our results are facilitated by a reduction to the problem of approximating convex hulls, and we prove that both the convex and conic hull variants are d-SUM-hard, resolving an open problem. Finally, we provide experimental results for the convex and conic algorithms on a variety of feature selection tasks.	[Van Buskirk, Gregory; Raichel, Benjamin; Ruozzi, Nicholas] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA	University of Texas System; University of Texas Dallas	Van Buskirk, G (corresponding author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.	greg.vanbuskirk@utdallas.edu; benjamin.raichel@utdallas.edu; nicholas.ruozzi@utdallas.edu	Jeong, Yongwook/N-7413-2016		NSF CRII [1566137]; DARPA Explainable Artificial Intelligence Program [N66001-17-2-4032]; NSF [III-1527312]	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); DARPA Explainable Artificial Intelligence Program; NSF(National Science Foundation (NSF))	Greg Van Buskirk and Ben Raichel were partially supported by NSF CRII Award-1566137. Nicholas Ruozzi was partially supported by DARPA Explainable Artificial Intelligence Program under contract number N66001-17-2-4032 and NSF grant III-1527312	Arora S, 2016, SIAM J COMPUT, V45, P1582, DOI 10.1137/130913869; Barman S, 2015, ACM S THEORY COMPUT, P361, DOI 10.1145/2746539.2746566; Benson A. R, 2014, ADV NEURAL INFORM PR, P945; Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006; Blum A., 2016, SODA, P548; Civril A, 2012, THEOR COMPUT SCI, V421, P1, DOI 10.1016/j.tcs.2011.11.019; Clarkson K. L., 2010, CORESETS SPARSE GREE, V6; Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277; Donoho D. L., 2003, ADV NEURAL INFORM PR; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Gillis N, 2014, J MACH LEARN RES, V15, P1249; Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226; Greene D., 2006, P 23 INT C MACHINE L, P377, DOI DOI 10.1145/1143844.1143892; Guruswami V., 2012, P 23 ANN ACM SIAM S, P1207, DOI [10.1137/1.9781611973099.95, 10.1137/1.9781611973099, DOI 10.1137/1.9781611973099]; Kumar A., 2013, P INT C MACH LEARN, P231; Kumar A., 2015, NEAR SEPARABLE NONNE, P343; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Li J., 2016, ARXIV160107996; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Novikoff, 1962, P S MATH THEOR AUT, P615; Patrascu M, 2010, PROC APPL MATH, V135, P1065; Recht B., 2012, ADV NEURAL INFORM PR, V25, P1214; Zhou TY, 2014, ADV NEUR IN, V27	25	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402057
C	Xu, AL; Raginsky, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Aolin; Raginsky, Maxim			Information-theoretic analysis of generalization capability of learning algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STABILITY	We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.	[Xu, Aolin; Raginsky, Maxim] Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA; [Xu, Aolin; Raginsky, Maxim] Univ Illinois, Coordinated Sci Lab, 1101 W Springfield Ave, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Xu, AL (corresponding author), Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA.; Xu, AL (corresponding author), Univ Illinois, Coordinated Sci Lab, 1101 W Springfield Ave, Urbana, IL 61801 USA.	aolinxu2@illinois.edu; maxim@illinois.edu	Jeong, Yongwook/N-7413-2016		NSF CAREER award [CCF-1254041]; Center for Science of Information (CSoI); NSF Science and Technology Center [CCF-0939370]	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Center for Science of Information (CSoI); NSF Science and Technology Center(National Science Foundation (NSF))	This work was supported in part by the NSF CAREER award CCF-1254041 and in part by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.	Alabdulmohsin I., 2017, 20 INT C ART INT STA; Alabdulmohsin I., 2015, 28 ANN C NEUR INF PR; Bassily R., 2016, P 48 ANN ACM S THEOR; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Buescher KL, 1996, IEEE T AUTOMAT CONTR, V41, P545, DOI 10.1109/9.489275; Dwork C., 2014, FDN TRENDS THEORETIC, V9; Dwork C., 2015, P 47 ACM S THEOR COM; Dwork C., 2015, 28 ANN C NEUR INF PR; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; MCSHERRY F, 2007, P 48 ANN IEEE S FDN; Polyanskiy Y., 2012, LECT NOTES ECE563 UI; Raginsky M, 2016, 2016 IEEE INFORMATION THEORY WORKSHOP (ITW); Raginsky M, 2016, IEEE T INFORM THEORY, V62, P3355, DOI 10.1109/TIT.2016.2549542; Russo D., 2016, MUCH DOES YOUR DATA; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Verdu S., 1996, Problems of Information Transmission, V32, P86; Wang Y.-X., 2016, P INT C PRIV STAT DA; Zhang Chiyuan, 2017, INT C LEARN REPR ICL; Zhang T, 2006, IEEE T INFORM THEORY, V52, P1307, DOI 10.1109/TIT.2005.864439	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402056
C	Xu, Y; Lin, QH; Yang, TB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Yi; Lin, Qihang; Yang, Tianbao			Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONVERGENCE	Error bound, an inherent property of an optimization problem, has recently revived in the development of algorithms with improved global convergence without strong convexity. The most studied error bound is the quadratic error bound, which generalizes strong convexity and is satisfied by a large family of machine learning problems. Quadratic error bound have been leveraged to achieve linear convergence in many first-order methods including the stochastic variance reduced gradient (SVRG) method, which is one of the most important stochastic optimization methods in machine learning. However, the studies along this direction face the critical issue that the algorithms must depend on an unknown growth parameter (a generalization of strong convexity modulus) in the error bound. This parameter is difficult to estimate exactly and the algorithms choosing this parameter heuristically do not have theoretical convergence guarantee. To address this issue, we propose novel SVRG methods that automatically search for this unknown parameter on the fly of optimization while still obtain almost the same convergence rate as when this parameter is known. We also analyze the convergence property of SVRG methods under Holderian error bound, which generalizes the quadratic error bound.	[Xu, Yi; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Lin, Qihang] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA	University of Iowa; University of Iowa	Xu, Y (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	yi-xu@uiowa.edu; qihang-lin@uiowa.edu; tianbao-yan@uiowa.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [IIS-1463988, IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	We thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995).	Allen-Zhu Z., 2017, P 49 ANN ACM S THEOR; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Bolte Jerome, 2015, CORR; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; DRUSVYATSKIY D., 2016, ARXIV160206661; Gong P., 2014, CORR; Hou K., 2013, ADV NEURAL INFORM PR, P710; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Li GY, 2013, MATH PROGRAM, V137, P37, DOI 10.1007/s10107-011-0481-z; LIN Q., 2014, ICML, V32, P73; Liu J., 2016, CORR; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Liu M., 2017, CORR; LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Ma C., 2015, CORR; Murata T., 2017, CORR; Necoara I., 2015, CORR; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nguyen L., 2017, CORR; NYQUIST H, 1983, COMMUN STAT-THEOR M, V12, P2511, DOI 10.1080/03610928308828618; Rockafellar RT., 1970, CONVEX ANAL; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xu Y, 2017, PR MACH LEARN RES, V70; Xu Yi, 2016, ADV NEURAL INFORM PR, P1208; Yang T.-J., 2016, CORR; Zhang H., 2013, ARXIV13034645; Zhang Han, 2016, CORR; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157; Zhou ZR, 2015, PR MACH LEARN RES, V37, P1501; Zhou Zirui, 2015, ABS151203518 CORR	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403034
C	Yang, ZR; Balasubramanian, K; Wang, ZR; Liu, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yang, Zhuoran; Balasubramanian, Krishna; Wang, Zhaoran; Liu, Han			Learning Non-Gaussian Multi-Index Model via Second-Order Stein's Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SLICED INVERSE REGRESSION; DIMENSION REDUCTION; SIGNAL RECOVERY; SPARSE; CONVERGENCE; RATES	We consider estimating the parametric components of semiparametric multi-index models in high dimensions. To bypass the requirements of Gaussianity or elliptical symmetry of covariates in existing methods, we propose to leverage a second-order Stein's method with score function-based corrections. We prove that our estimator achieves a near-optimal statistical rate of convergence even when the score function or the response variable is heavy-tailed. To establish the key concentration results, we develop a data-driven truncation argument that may be of independent interest. We supplement our theoretical findings with simulations.	[Yang, Zhuoran; Balasubramanian, Krishna] Princeton Univ, Princeton, NJ 08544 USA; [Wang, Zhaoran; Liu, Han] Tencent AI Lab, Bellevue, WA USA; [Wang, Zhaoran; Liu, Han] Northwestern Univ, Evanston, IL 60208 USA	Princeton University; Northwestern University	Yang, ZR (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	zy6@princeton.edu; kb18@princeton.edu; zhaoranwang@gmail.com; hanliu.cmu@gmail.com	Wang, Zhaoran/P-7113-2018; Jeong, Yongwook/N-7413-2016					Ai Albert, 2014, LINEAR ALGEBRA ITS A; Anandkumar A., 2014, ARXIV14122863; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Boufounos PT, 2008, 2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3, P16, DOI 10.1109/CISS.2008.4558487; Cai, 2016, J MACHINE LEARNING R, V17, P1; Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chen X, 2010, ANN STAT, V38, P3696, DOI 10.1214/10-AOS826; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Fan JQ, 2011, ANNU REV ECON, V3, P291, DOI 10.1146/annurev-economics-061109-080451; Goldstein L., 2016, ARXIV160901025; Gross David, 2015, J FOURIER ANAL APPL; Janzamin M., 2015, ARXIV150608473; Jiang B, 2014, ANN STAT, V42, P1751, DOI 10.1214/14-AOS1233; Lecue G, 2015, ELECTRON J PROBAB, V20, P1, DOI 10.1214/EJP.v20-3525; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.1080/01621459.1992.10476258; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Lin Qian, 2015, ARXIV150703895; Lin Qian, 2017, ARXIV170106009; Mendelson S., 2014, C LEARN THEOR, P25; Neykov M., 2016, P ADV NEUR INF PROC, P4089; Oliveira R., 2013, ARXIV13122903; Plan Y, 2016, IEEE T INFORM THEORY, V62, P1528, DOI 10.1109/TIT.2016.2517008; Stein Charles, 2004, STEINS METHOD; Tan Kean Ming, 2016, ARXIV160408697; Tan Kean Ming, 2016, CONVEX FORMULA UNPUB; Vu VQ, 2013, ADV NEURAL INFORM PR, V26; Wang TY, 2016, ANN STAT, V44, P1896, DOI 10.1214/15-AOS1369; Wang Zhaoran, 2014, Adv Neural Inf Process Syst, V2014, P3383; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Wang Zhaoran, 2015, ARXIV151208861; WRIGHT J., 2016, ARXIV160206664; Yang Zhuoran, 2015, INT C MACH LEARN; Yi Xinyang, 2015, Adv Neural Inf Process Syst, V28, P1549; Zhu LX, 2006, J AM STAT ASSOC, V101, P630, DOI 10.1198/016214505000001285	40	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406017
C	Yu, Q; Maddah-Ali, MA; Avestimehr, AS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yu, Qian; Maddah-Ali, Mohammad Ali; Avestimehr, A. Salman			Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider a large-scale matrix multiplication problem where the computation is carried out using a distributed system with a master node and multiple worker nodes, where each worker can store parts of the input matrices. We propose a computation strategy that leverages ideas from coding theory to design intermediate computations at the worker nodes, in order to optimally deal with straggling workers. The proposed strategy, named as polynomial codes, achieves the optimum recovery threshold, defined as the minimum number of workers that the master needs to wait for in order to compute the output. This is the first code that achieves the optimal utilization of redundancy for tolerating stragglers or failures in distributed matrix multiplication. Furthermore, by leveraging the algebraic structure of polynomial codes, we can map the reconstruction problem of the final output to a polynomial interpolation problem, which can be solved efficiently. Polynomial codes provide order-wise improvement over the state of the art in terms of recovery threshold, and are also optimal in terms of several other metrics including computation latency and communication load. Moreover, we extend this code to distributed convolution and show its order-wise optimality.	[Yu, Qian; Avestimehr, A. Salman] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA; [Maddah-Ali, Mohammad Ali] Nokia Bell Labs, Holmdel, NJ USA	University of Southern California; Nokia Corporation; Nokia Bell Labs	Yu, Q (corresponding author), Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.		Jeong, Yongwook/N-7413-2016; Avestimehr, Amir Salman/O-7864-2019		NSF [CCF-1408639, NETS-1419632]; ONR [N000141612189]; NSA grant; Defense Advanced Research Projects Agency (DARPA) [HR001117C0053]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); NSA grant; Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work is in part supported by NSF grants CCF-1408639, NETS-1419632, ONR award N000141612189, NSA grant, and a research gift from Intel. This material is based upon work supported by Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0053. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.	[Anonymous], 2016, ARXIV160901690; [Anonymous], 2015, ARXIV151202673; Baktir S., 2006, P 44 ANN S EAST REGI, P549; Dean J., 2004, 6 USENIX S OP SYST D; Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794; Didier F., 1886, ARXIV09011886; Dutta S., 2017, ARXIV170503875; HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475; JOU JY, 1986, P IEEE, V74, P732, DOI 10.1109/PROC.1986.13535; Kedlaya KS, 2011, SIAM J COMPUT, V40, P1767, DOI 10.1137/08073408X; Lee K, 2017, IEEE INT SYMP INFO, P2418, DOI 10.1109/ISIT.2017.8006963; Li SS, 2015, PROCEDIA ENGINEER, V119, P53, DOI 10.1016/j.proeng.2015.08.853; Li SZ, 2018, IEEE T INFORM THEORY, V64, P109, DOI 10.1109/TIT.2017.2756959; Reisizadehmobarakeh A., 2017, ARXIV170105973; Roth R., 2006, IET COMMUN; SINGLETON RC, 1964, IEEE T INFORM THEORY, V10, P116, DOI 10.1109/TIT.1964.1053661; Tandon R., 2016, ARXIV161203301; Wan FY, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P160, DOI 10.1109/ICPHM.2017.7998322; Yu Q., 2018, ARXIV180107487; Zaharia M, 2010, HOTCLOUD, P10, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?; Zaharia Matei, 2008, OSDI, V8, P7	21	1	1	4	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404046
C	Zhang, ZT; Li, QJ; Huang, ZJ; Wu, JJ; Tenenbaum, JB; Freeman, WT		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhang, Zhoutong; Li, Qiujia; Huang, Zhengjia; Wu, Jiajun; Tenenbaum, Joshua B.; Freeman, William T.			Shape and Material from Sound	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Hearing an object falling onto the ground, humans can recover rich information including its rough shape, material, and falling height. In this paper, we build machines to approximate such competency. We first mimic human knowledge of the physical world by building an efficient, physics-based simulation engine. Then, we present an analysis-by-synthesis approach to infer properties of the falling object. We further accelerate the process by learning a mapping from a sound wave to object properties, and using the predicted values to initialize the inference. This mapping can be viewed as an approximation of human commonsense learned from past experience. Our model performs well on both synthetic audio clips and real recordings without requiring any annotated data. We conduct behavior studies to compare human responses with ours on estimating object shape, material, and falling height from sound. Our model achieves near-human performance.	[Zhang, Zhoutong; Wu, Jiajun; Tenenbaum, Joshua B.; Freeman, William T.] MIT, Cambridge, MA 02139 USA; [Li, Qiujia] Univ Cambridge, Cambridge, England; [Huang, Zhengjia] ShanghaiTech Univ, Shanghai, Peoples R China; [Freeman, William T.] Google Res, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); University of Cambridge; ShanghaiTech University; Google Incorporated	Zhang, ZT (corresponding author), MIT, Cambridge, MA 02139 USA.		Wu, JiaJun/GQH-7885-2022; Jeong, Yongwook/N-7413-2016		NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Toyota Research Institute; Samsung; Shell; Center for Brain, Minds and Machines (NSF STC award) [CCF-1231216]	NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); Toyota Research Institute; Samsung(Samsung); Shell(Royal Dutch Shell); Center for Brain, Minds and Machines (NSF STC award)	The authors would like to thank Changxi Zheng, Eitan Grinspun, and Josh H. McDermott for helpful discussions. This work is supported by NSF #1212849 and #1447476, ONR MURI N00014-16-1-2007, Toyota Research Institute, Samsung, Shell, and the Center for Brain, Minds and Machines (NSF STC award CCF-1231216).	[Anonymous], 2016, CVPR; [Anonymous], 2016, ECCV; [Anonymous], 2015, NEURIPS; Aytar Y., 2016, NIPS; Battaglia P, 2016, NIPS; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Bever TG, 2010, BIOLINGUISTICS, V4, P174; Bonneel N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360623; Chang MB., 2017, 5 INT C LEARN REPR I; Collobert R., 2011, BIGLEARN NIPS WORKSH; Coumans Erwin, 2010, BULLET PHYS ENGINE; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; James DL, 2006, ACM T GRAPHIC, V25, P987, DOI 10.1145/1141911.1141983; Klatzky RL, 2000, PRESENCE-TELEOP VIRT, V9, P399, DOI 10.1162/105474600566907; Kunkler-Peck AJ, 2000, J EXP PSYCHOL HUMAN, V26, P279, DOI 10.1037/0096-1523.26.1.279; McDermott JH, 2013, NAT NEUROSCI, V16, P493, DOI 10.1038/nn.3347; O'Brien James F, 2002, SCA; O'Brien James F, 2001, SIGGRAPH; Rocchesso D., 2003, THE SOUNDING OBJECT; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sanborn AN, 2013, PSYCHOL REV, V120, P411, DOI 10.1037/a0031912; Siegel M., 2014, COGSCI; van den Doel K, 1998, PRESENCE-TELEOP VIRT, V7, P382, DOI 10.1162/105474698565794; Wu J., 2017, NIPS; Wu Jiajun, 2016, BMVC; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zhang Zhoutong, 2017, ICCV; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018; Zwicker E., 2013, PSYCHOACOUSTICS FACT, V22	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401031
